Topic,Count,Name,Representation,Aspect1,Representative_Docs
-1,292,-1_learning_models_neural_dataset,"['learning', 'models', 'neural', 'dataset', 'prediction', 'datasets', 'deep', 'ai', 'classification', 'networks']","['data', 'learning', 'model', 'problem', 'methods', 'time', 'models', 'approach', 'results', 'new']","[""  Modelling the impact of a material's mesostructure on device level\nperformance typically requires access to 3D image data containing all the\nrelevant information to define the geometry of the simulation domain. This\nimage data must include sufficient contrast between phases to distinguish each\nmaterial, be of high enough resolution to capture the key details, but also\nhave a large enough field-of-view to be representative of the material in\ngeneral. It is rarely possible to obtain data with all of these properties from\na single imaging technique. In this paper, we present a method for combining\ninformation from pairs of distinct but complementary imaging techniques in\norder to accurately reconstruct the desired multi-phase, high resolution,\nrepresentative, 3D images. Specifically, we use deep convolutional generative\nadversarial networks to implement super-resolution, style transfer and\ndimensionality expansion. To demonstrate the widespread applicability of this\ntool, two pairs of datasets are used to validate the quality of the volumes\ngenerated by fusing the information from paired imaging techniques. Three key\nmesostructural metrics are calculated in each case to show the accuracy of this\nmethod. Having confidence in the accuracy of our method, we then demonstrate\nits power by applying to a real data pair from a lithium ion battery electrode,\nwhere the required 3D high resolution image data is not available anywhere in\nthe literature. We believe this approach is superior to previously reported\nstatistical material reconstruction methods both in terms of its fidelity and\nease of use. Furthermore, much of the data required to train this algorithm\nalready exists in the literature, waiting to be combined. As such, our\nopen-access code could precipitate a step change by generating the hard to\nobtain high quality image volumes necessary to simulate behaviour at the\nmesoscale.\n"", '  Modeling of non-rigid object launching and manipulation is complex\nconsidering the wide range of dynamics affecting trajectory, many of which may\nbe unknown. Using physics models can be inaccurate because they cannot account\nfor unknown factors and the effects of the deformation of the object as it is\nlaunched; moreover, deriving force coefficients for these models is not\npossible without extensive experimental testing. Recently, advancements in\ndata-powered artificial intelligence methods have allowed learnable models and\nsystems to emerge. It is desirable to train a model for launch prediction on a\nrobot, as deep neural networks can account for immeasurable dynamics. However,\nthe inability to collect large amounts of experimental data decreases\nperformance of deep neural networks. Through estimating force coefficients, the\naccepted physics models can be leveraged to produce adequate supplemental data\nto artificially increase the size of the training set, yielding improved neural\nnetworks. In this paper, we introduce a new framework for algorithmic\nestimation of force coefficients for non-rigid object launching, which can be\ngeneralized to other domains, in order to generate large datasets. We implement\na novel training algorithm and objective for our deep neural network to\naccurately model launch trajectory of non-rigid objects and predict whether\nthey will hit a series of targets. Our experimental results demonstrate the\neffectiveness of using simulated data from force coefficient estimation and\nshows the importance of simulated data for training an effective neural\nnetwork.\n', '  This paper is concerned with the statistical analysis of matrix-valued time\nseries. These are data collected over a network of sensors (typically a set of\nspatial locations) along time, where a vector of features is observed per time\ninstant per sensor. Thus each sensor is characterized by a vectorial time\nseries. We would like to identify the dependency structure among these sensors\nand represent it by a graph. When there is only one feature per sensor, the\nvector auto-regressive models have been widely adapted to infer the structure\nof Granger causality. The resulting graph is referred to as causal graph. Our\nfirst contribution is then extending VAR models to matrix-variate models to\nserve the purpose of graph learning. Secondly, we propose two online procedures\nrespectively in low and high dimensions, which can update quickly the estimates\nof coefficients when new samples arrive. In particular in high dimensional\nregime, a novel Lasso-type is introduced and we develop its homotopy algorithms\nfor the online learning. We also provide an adaptive tuning procedure for the\nregularization parameter. Lastly, we consider that, the application of AR\nmodels onto data usually requires detrending the raw data, however, this step\nis forbidden in online context. Therefore, we augment the proposed AR models by\nincorporating trend as extra parameter, and then adapt the online algorithms to\nthe augmented data models, which allow us to simultaneously learn the graph and\ntrend from streaming samples. In this work, we consider primarily the periodic\ntrend. Numerical experiments using both synthetic and real data are performed,\nwhose results support the effectiveness of the proposed methods.\n']"
0,113,0_nlp_corpus_semantic_textual,"['nlp', 'corpus', 'semantic', 'textual', 'linguistic', 'corpora', 'embeddings', 'text', 'bert', 'attention']","['language', 'text', 'model', 'languages', 'knowledge', 'models', 'word', 'task', 'performance', 'attention']","['  Recently, with the help of deep learning models, significant advances have\nbeen made in different Natural Language Processing (NLP) tasks. Unfortunately,\nstate-of-the-art models are vulnerable to noisy texts. We propose a new\ncontextual text denoising algorithm based on the ready-to-use masked language\nmodel. The proposed algorithm does not require retraining of the model and can\nbe integrated into any NLP system without additional training on paired\ncleaning training data. We evaluate our method under synthetic noise and\nnatural noise and show that the proposed algorithm can use context information\nto correct noise text and improve the performance of noisy inputs in several\ndownstream tasks.\n', '  Pre-training large-scale neural language models on raw texts has made a\nsignificant contribution to improving transfer learning in natural language\nprocessing (NLP). With the introduction of transformer-based language models,\nsuch as bidirectional encoder representations from transformers (BERT), the\nperformance of information extraction from a free text by NLP has significantly\nimproved for both the general domain and medical domain; however, it is\ndifficult to train specific BERT models that perform well for domains in which\nthere are few publicly available databases of high quality and large size. We\nhypothesized that this problem can be addressed by up-sampling a\ndomain-specific corpus and using it for pre-training with a larger corpus in a\nbalanced manner. Our proposed method consists of a single intervention with one\noption: simultaneous pre-training after up-sampling and amplified vocabulary.\nWe conducted three experiments and evaluated the resulting products. We\nconfirmed that our Japanese medical BERT outperformed conventional baselines\nand the other BERT models in terms of the medical document classification task\nand that our English BERT pre-trained using both the general and medical-domain\ncorpora performed sufficiently well for practical use in terms of the\nbiomedical language understanding evaluation (BLUE) benchmark. Moreover, our\nenhanced biomedical BERT model, in which clinical notes were not used during\npre-training, showed that both the clinical and biomedical scores of the BLUE\nbenchmark were 0.3 points above that of the ablation model trained without our\nproposed method. Well-balanced pre-training by up-sampling instances derived\nfrom a corpus appropriate for the target task allows us to construct a\nhigh-performance BERT model.\n', '  Existing technologies expand BERT from different perspectives, e.g. designing\ndifferent pre-training tasks, different semantic granularities, and different\nmodel architectures. Few models consider expanding BERT from different text\nformats. In this paper, we propose a heterogeneous knowledge language model\n(\\textbf{HKLM}), a unified pre-trained language model (PLM) for all forms of\ntext, including unstructured text, semi-structured text, and well-structured\ntext. To capture the corresponding relations among these multi-format\nknowledge, our approach uses masked language model objective to learn word\nknowledge, uses triple classification objective and title matching objective to\nlearn entity knowledge and topic knowledge respectively. To obtain the\naforementioned multi-format text, we construct a corpus in the tourism domain\nand conduct experiments on 5 tourism NLP datasets. The results show that our\napproach outperforms the pre-training of plain text using only 1/4 of the data.\nWe further pre-train the domain-agnostic HKLM and achieve performance gains on\nthe XNLI dataset.\n']"
1,58,1_reinforcement_learning_reward_regularization,"['reinforcement', 'learning', 'reward', 'regularization', 'planning', 'rewards', 'exploration', 'optimal', 'policy', 'training']","['policy', 'reinforcement', 'learning', 'action', 'reward', 'state', 'algorithm', 'gradient', 'function', 'control']","['  Agents trained with deep reinforcement learning algorithms are capable of\nperforming highly complex tasks including locomotion in continuous\nenvironments. We investigate transferring the learning acquired in one task to\na set of previously unseen tasks. Generalization and overfitting in deep\nreinforcement learning are not commonly addressed in current transfer learning\nresearch. Conducting a comparative analysis without an intermediate\nregularization step results in underperforming benchmarks and inaccurate\nalgorithm comparisons due to rudimentary assessments. In this study, we propose\nregularization techniques in deep reinforcement learning for continuous control\nthrough the application of sample elimination, early stopping and maximum\nentropy regularized adversarial learning. First, the importance of the\ninclusion of training iteration number to the hyperparameters in deep transfer\nreinforcement learning will be discussed. Because source task performance is\nnot indicative of the generalization capacity of the algorithm, we start by\nacknowledging the training iteration number as a hyperparameter. In line with\nthis, we introduce an additional step of resorting to earlier snapshots of\npolicy parameters to prevent overfitting to the source task. Then, to generate\nrobust policies, we discard the samples that lead to overfitting via a method\nwe call strict clipping. Furthermore, we increase the generalization capacity\nin widely used transfer learning benchmarks by using maximum entropy\nregularization, different critic methods, and curriculum learning in an\nadversarial setup. Subsequently, we propose maximum entropy adversarial\nreinforcement learning to increase the domain randomization. Finally, we\nevaluate the robustness of these methods on simulated robots in target\nenvironments where the morphology of the robot, gravity, and tangential\nfriction coefficient of the environment are altered.\n', '  Entropy regularization is an efficient technique for encouraging exploration\nand preventing a premature convergence of (vanilla) policy gradient methods in\nreinforcement learning (RL). However, the theoretical understanding of\nentropy-regularized RL algorithms has been limited. In this paper, we revisit\nthe classical entropy regularized policy gradient methods with the soft-max\npolicy parametrization, whose convergence has so far only been established\nassuming access to exact gradient oracles. To go beyond this scenario, we\npropose the first set of (nearly) unbiased stochastic policy gradient\nestimators with trajectory-level entropy regularization, with one being an\nunbiased visitation measure-based estimator and the other one being a nearly\nunbiased yet more practical trajectory-based estimator. We prove that although\nthe estimators themselves are unbounded in general due to the additional\nlogarithmic policy rewards introduced by the entropy term, the variances are\nuniformly bounded. We then propose a two-phase stochastic policy gradient (PG)\nalgorithm that uses a large batch size in the first phase to overcome the\nchallenge of the stochastic approximation due to the non-coercive landscape,\nand uses a small batch size in the second phase by leveraging the curvature\ninformation around the optimal policy. We establish a global optimality\nconvergence result and a sample complexity of\n$\\widetilde{\\mathcal{O}}(\\frac{1}{\\epsilon^2})$ for the proposed algorithm. Our\nresult is the first global convergence and sample complexity results for the\nstochastic entropy-regularized vanilla PG method.\n', '  We propose two policy gradient algorithms for solving the problem of control\nin an off-policy reinforcement learning (RL) context. Both algorithms\nincorporate a smoothed functional (SF) based gradient estimation scheme. The\nfirst algorithm is a straightforward combination of importance sampling-based\noff-policy evaluation with SF-based gradient estimation. The second algorithm,\ninspired by the stochastic variance-reduced gradient (SVRG) algorithm,\nincorporates variance reduction in the update iteration. For both algorithms,\nwe derive non-asymptotic bounds that establish convergence to an approximate\nstationary point. From these results, we infer that the first algorithm\nconverges at a rate that is comparable to the well-known REINFORCE algorithm in\nan off-policy RL context, while the second algorithm exhibits an improved rate\nof convergence.\n']"
2,54,2_bandits_bandit_optimal_greedy,"['bandits', 'bandit', 'optimal', 'greedy', 'reward', 'regret', 'rewards', 'strategy', 'stochastic', 'exploration']","['regret', 'algorithm', 'bandits', 'bandit', 'problem', 'optimal', 'arm', 'algorithms', 'feedback', 'armed']","[""  We consider a decentralized multi-agent Multi Armed Bandit (MAB) setup\nconsisting of $N$ agents, solving the same MAB instance to minimize individual\ncumulative regret. In our model, agents collaborate by exchanging messages\nthrough pairwise gossip style communications on an arbitrary connected graph.\nWe develop two novel algorithms, where each agent only plays from a subset of\nall the arms. Agents use the communication medium to recommend only arm-IDs\n(not samples), and thus update the set of arms from which they play. We\nestablish that, if agents communicate $\\Omega(\\log(T))$ times through any\nconnected pairwise gossip mechanism, then every agent's regret is a factor of\norder $N$ smaller compared to the case of no collaborations. Furthermore, we\nshow that the communication constraints only have a second order effect on the\nregret of our algorithm. We then analyze this second order term of the regret\nto derive bounds on the regret-communication tradeoffs. Finally, we empirically\nevaluate our algorithm and conclude that the insights are fundamental and not\nartifacts of our bounds. We also show a lower bound which gives that the regret\nscaling obtained by our algorithm cannot be improved even in the absence of any\ncommunication constraints. Our results thus demonstrate that even a minimal\nlevel of collaboration among agents greatly reduces regret for all agents.\n"", ""  We investigate a Bayesian $k$-armed bandit problem in the \\emph{many-armed}\nregime, where $k \\geq \\sqrt{T}$ and $T$ represents the time horizon. Initially,\nand aligned with recent literature on many-armed bandit problems, we observe\nthat subsampling plays a key role in designing optimal algorithms; the\nconventional UCB algorithm is sub-optimal, whereas a subsampled UCB (SS-UCB),\nwhich selects $\\Theta(\\sqrt{T})$ arms for execution under the UCB framework,\nachieves rate-optimality. However, despite SS-UCB's theoretical promise of\noptimal regret, it empirically underperforms compared to a greedy algorithm\nthat consistently chooses the empirically best arm. This observation extends to\ncontextual settings through simulations with real-world data. Our findings\nsuggest a new form of \\emph{free exploration} beneficial to greedy algorithms\nin the many-armed context, fundamentally linked to a tail event concerning the\nprior distribution of arm rewards. This finding diverges from the notion of\nfree exploration, which relates to covariate variation, as recently discussed\nin contextual bandit literature. Expanding upon these insights, we establish\nthat the subsampled greedy approach not only achieves rate-optimality for\nBernoulli bandits within the many-armed regime but also attains sublinear\nregret across broader distributions. Collectively, our research indicates that\nin the many-armed regime, practitioners might find greater value in adopting\ngreedy algorithms.\n"", ""  We consider a novel stochastic multi-armed bandit setting, where playing an\narm makes it unavailable for a fixed number of time slots thereafter. This\nmodels situations where reusing an arm too often is undesirable (e.g. making\nthe same product recommendation repeatedly) or infeasible (e.g. compute job\nscheduling on machines). We show that with prior knowledge of the rewards and\ndelays of all the arms, the problem of optimizing cumulative reward does not\nadmit any pseudo-polynomial time algorithm (in the number of arms) unless\nrandomized exponential time hypothesis is false, by mapping to the PINWHEEL\nscheduling problem. Subsequently, we show that a simple greedy algorithm that\nplays the available arm with the highest reward is asymptotically $(1-1/e)$\noptimal. When the rewards are unknown, we design a UCB based algorithm which is\nshown to have $c \\log T + o(\\log T)$ cumulative regret against the greedy\nalgorithm, leveraging the free exploration of arms due to the unavailability.\nFinally, when all the delays are equal the problem reduces to Combinatorial\nSemi-bandits providing us with a lower bound of $c' \\log T+ \\omega(\\log T)$.\n""]"
3,50,3_graphs_networks_graph_nodes,"['graphs', 'networks', 'graph', 'nodes', 'embeddings', 'neural', 'embedding', 'vertices', 'edges', 'deep']","['graph', 'node', 'graphs', 'networks', 'clustering', 'embedding', 'network', 'nodes', 'learning', 'neural']","['  Deep learning models for graphs have advanced the state of the art on many\ntasks. Despite their recent success, little is known about their robustness. We\ninvestigate training time attacks on graph neural networks for node\nclassification that perturb the discrete graph structure. Our core principle is\nto use meta-gradients to solve the bilevel problem underlying training-time\nattacks, essentially treating the graph as a hyperparameter to optimize. Our\nexperiments show that small graph perturbations consistently lead to a strong\ndecrease in performance for graph convolutional networks, and even transfer to\nunsupervised embeddings. Remarkably, the perturbations created by our algorithm\ncan misguide the graph neural networks such that they perform worse than a\nsimple baseline that ignores all relational information. Our attacks do not\nassume any knowledge about or access to the target classifiers.\n', '  Graph-based clustering plays an important role in the clustering area. Recent\nstudies about graph convolution neural networks have achieved impressive\nsuccess on graph type data. However, in general clustering tasks, the graph\nstructure of data does not exist such that the strategy to construct a graph is\ncrucial for performance. Therefore, how to extend graph convolution networks\ninto general clustering tasks is an attractive problem. In this paper, we\npropose a graph auto-encoder for general data clustering, which constructs the\ngraph adaptively according to the generative perspective of graphs. The\nadaptive process is designed to induce the model to exploit the high-level\ninformation behind data and utilize the non-Euclidean structure sufficiently.\nWe further design a novel mechanism with rigorous analysis to avoid the\ncollapse caused by the adaptive construction. Via combining the generative\nmodel for network embedding and graph-based clustering, a graph auto-encoder\nwith a novel decoder is developed such that it performs well in weighted graph\nused scenarios. Extensive experiments prove the superiority of our model.\n', '  In graph neural networks (GNNs), pooling operators compute local summaries of\ninput graphs to capture their global properties, and they are fundamental for\nbuilding deep GNNs that learn hierarchical representations. In this work, we\npropose the Node Decimation Pooling (NDP), a pooling operator for GNNs that\ngenerates coarser graphs while preserving the overall graph topology. During\ntraining, the GNN learns new node representations and fits them to a pyramid of\ncoarsened graphs, which is computed offline in a pre-processing stage. NDP\nconsists of three steps. First, a node decimation procedure selects the nodes\nbelonging to one side of the partition identified by a spectral algorithm that\napproximates the \\maxcut{} solution. Afterwards, the selected nodes are\nconnected with Kron reduction to form the coarsened graph. Finally, since the\nresulting graph is very dense, we apply a sparsification procedure that prunes\nthe adjacency matrix of the coarsened graph to reduce the computational cost in\nthe GNN. Notably, we show that it is possible to remove many edges without\nsignificantly altering the graph structure. Experimental results show that NDP\nis more efficient compared to state-of-the-art graph pooling operators while\nreaching, at the same time, competitive performance on a significant variety of\ngraph classification tasks.\n']"
4,43,4_cnn_neural_layers_deep,"['cnn', 'neural', 'layers', 'deep', 'networks', 'convolutional', 'images', 'learning', 'visual', 'layer']","['image', 'deep', 'neural', 'convolutional', 'networks', 'learning', 'network', 'layer', 'accuracy', 'images']","['  Convolutional Neural Networks (CNNs) are successfully used for the important\nautomotive visual perception tasks including object recognition, motion and\ndepth estimation, visual SLAM, etc. However, these tasks are typically\nindependently explored and modeled. In this paper, we propose a joint\nmulti-task network design for learning several tasks simultaneously. Our main\nmotivation is the computational efficiency achieved by sharing the expensive\ninitial convolutional layers between all tasks. Indeed, the main bottleneck in\nautomated driving systems is the limited processing power available on\ndeployment hardware. There is also some evidence for other benefits in\nimproving accuracy for some tasks and easing development effort. It also offers\nscalability to add more tasks leveraging existing features and achieving better\ngeneralization. We survey various CNN based solutions for visual perception\ntasks in automated driving. Then we propose a unified CNN model for the\nimportant tasks and discuss several advanced optimization and architecture\ndesign techniques to improve the baseline model. The paper is partly review and\npartly positional with demonstration of several preliminary results promising\nfor future research. We first demonstrate results of multi-stream learning and\nauxiliary learning which are important ingredients to scale to a large\nmulti-task model. Finally, we implement a two-stream three-task network which\nperforms better in many cases compared to their corresponding single-task\nmodels, while maintaining network size.\n', ""  One of the main challenges since the advancement of convolutional neural\nnetworks is how to connect the extracted feature map to the final\nclassification layer. VGG models used two sets of fully connected layers for\nthe classification part of their architectures, which significantly increased\nthe number of models' weights. ResNet and the next deep convolutional models\nused the Global Average Pooling (GAP) layer to compress the feature map and\nfeed it to the classification layer. Although using the GAP layer reduces the\ncomputational cost, but also causes losing spatial resolution of the feature\nmap, which results in decreasing learning efficiency. In this paper, we aim to\ntackle this problem by replacing the GAP layer with a new architecture called\nWise-SrNet. It is inspired by the depthwise convolutional idea and is designed\nfor processing spatial resolution while not increasing computational cost. We\nhave evaluated our method using three different datasets: Intel Image\nClassification Challenge, MIT Indoors Scenes, and a part of the ImageNet\ndataset. We investigated the implementation of our architecture on several\nmodels of the Inception, ResNet, and DenseNet families. Applying our\narchitecture has revealed a significant effect on increasing convergence speed\nand accuracy. Our Experiments on images with 224*224 resolution increased the\nTop-1 accuracy between 2% to 8% on different datasets and models. Running our\nmodels on 512*512 resolution images of the MIT Indoors Scenes dataset showed a\nnotable result of improving the Top-1 accuracy within 3% to 26%. We will also\ndemonstrate the GAP layer's disadvantage when the input images are large and\nthe number of classes is not few. In this circumstance, our proposed\narchitecture can do a great help in enhancing classification results. The code\nis shared at https://github.com/mr7495/image-classification-spatial.\n"", ""  Whereas conventional state-of-the-art image processing systems of recording\nand output devices almost exclusively utilize square arranged methods,\nbiological models, however, suggest an alternative, evolutionarily-based\nstructure. Inspired by the human visual perception system, hexagonal image\nprocessing in the context of machine learning offers a number of key advantages\nthat can benefit both researchers and users alike. The hexagonal deep learning\nframework Hexnet leveraged in this contribution serves therefore the generation\nof hexagonal images by utilizing hexagonal deep neural networks (H-DNN). As the\nresults of our created test environment show, the proposed models can surpass\ncurrent approaches of conventional image generation. While resulting in a\nreduction of the models' complexity in the form of trainable parameters, they\nfurthermore allow an increase of test rates in comparison to their square\ncounterparts.\n""]"
5,35,5_federated_learning_fedcluster_fedcg,"['federated', 'learning', 'fedcluster', 'fedcg', 'distributed', 'privacy', 'training', 'sharing', 'collaboratively', 'centralized']","['privacy', 'learning', 'local', 'communication', 'training', 'data', 'convergence', 'model', 'clients', 'framework']","[""  Federated learning (FL) aims to protect data privacy by enabling clients to\nbuild machine learning models collaboratively without sharing their private\ndata. Recent works demonstrate that information exchanged during FL is subject\nto gradient-based privacy attacks, and consequently, a variety of\nprivacy-preserving methods have been adopted to thwart such attacks. However,\nthese defensive methods either introduce orders of magnitude more computational\nand communication overheads (e.g., with homomorphic encryption) or incur\nsubstantial model performance losses in terms of prediction accuracy (e.g.,\nwith differential privacy). In this work, we propose $\\textsc{FedCG}$, a novel\nfederated learning method that leverages conditional generative adversarial\nnetworks to achieve high-level privacy protection while still maintaining\ncompetitive model performance. $\\textsc{FedCG}$ decomposes each client's local\nnetwork into a private extractor and a public classifier and keeps the\nextractor local to protect privacy. Instead of exposing extractors,\n$\\textsc{FedCG}$ shares clients' generators with the server for aggregating\nclients' shared knowledge, aiming to enhance the performance of each client's\nlocal networks. Extensive experiments demonstrate that $\\textsc{FedCG}$ can\nachieve competitive model performance compared with FL baselines, and privacy\nanalysis shows that $\\textsc{FedCG}$ has a high-level privacy-preserving\ncapability. Code is available at https://github.com/yankang18/FedCG\n"", ""  Federated Learning (FL) enables the multiple participating devices to\ncollaboratively contribute to a global neural network model while keeping the\ntraining data locally. Unlike the centralized training setting, the non-IID,\nimbalanced (statistical heterogeneity) and distribution shifted training data\nof FL is distributed in the federated network, which will increase the\ndivergences between the local models and the global model, further degrading\nperformance. In this paper, we propose a flexible clustered federated learning\n(CFL) framework named FlexCFL, in which we 1) group the training of clients\nbased on the similarities between the clients' optimization directions for\nlower training divergence; 2) implement an efficient newcomer device cold start\nmechanism for framework scalability and practicality; 3) flexibly migrate\nclients to meet the challenge of client-level data distribution shift. FlexCFL\ncan achieve improvements by dividing joint optimization into groups of\nsub-optimization and can strike a balance between accuracy and communication\nefficiency in the distribution shift environment. The convergence and\ncomplexity are analyzed to demonstrate the efficiency of FlexCFL. We also\nevaluate FlexCFL on several open datasets and made comparisons with related CFL\nframeworks. The results show that FlexCFL can significantly improve absolute\ntest accuracy by +10.6% on FEMNIST compared to FedAvg, +3.5% on FashionMNIST\ncompared to FedProx, +8.4% on MNIST compared to FeSEM. The experiment results\nshow that FlexCFL is also communication efficient in the distribution shift\nenvironment.\n"", ""  Federated Learning (FL) enables the multiple participating devices to\ncollaboratively contribute to a global neural network model while keeping the\ntraining data locally. Unlike the centralized training setting, the non-IID and\nimbalanced (statistical heterogeneity) training data of FL is distributed in\nthe federated network, which will increase the divergences between the local\nmodels and global model, further degrading performance. In this paper, we\npropose a novel clustered federated learning (CFL) framework FedGroup, in which\nwe 1) group the training of clients based on the similarities between the\nclients' optimization directions for high training performance; 2) construct a\nnew data-driven distance measure to improve the efficiency of the client\nclustering procedure. 3) implement a newcomer device cold start mechanism based\non the auxiliary global model for framework scalability and practicality.\n  FedGroup can achieve improvements by dividing joint optimization into groups\nof sub-optimization and can be combined with FL optimizer FedProx. The\nconvergence and complexity are analyzed to demonstrate the efficiency of our\nproposed framework. We also evaluate FedGroup and FedGrouProx (combined with\nFedProx) on several open datasets and made comparisons with related CFL\nframeworks. The results show that FedGroup can significantly improve absolute\ntest accuracy by +14.1% on FEMNIST compared to FedAvg. +3.4% on Sentiment140\ncompared to FedProx, +6.9% on MNIST compared to FeSEM.\n""]"
6,30,6_gradient_optimizers_optimization_learning,"['gradient', 'optimizers', 'optimization', 'learning', 'adaptive', 'sgd', 'minimax', 'neural', 'generalization', 'algorithms']","['gradient', 'convergence', 'stochastic', 'descent', 'optimization', 'nonconvex', 'convex', 'step', 'adaptive', 'problems']","['  It is known that adaptive optimization algorithms represent the key pillar\nbehind the rise of the Machine Learning field. In the Optimization literature\nnumerous studies have been devoted to accelerated gradient methods but only\nrecently adaptive iterative techniques were analyzed from a theoretical point\nof view. In the present paper we introduce new adaptive algorithms endowed with\nmomentum terms for stochastic non-convex optimization problems. Our purpose is\nto show a deep connection between accelerated methods endowed with different\ninertial steps and AMSGrad-type momentum methods. Our methodology is based on\nthe framework of stochastic and possibly non-convex objective mappings, along\nwith some assumptions that are often used in the investigation of adaptive\nalgorithms. In addition to discussing the finite-time horizon analysis in\nrelation to a certain final iteration and the almost sure convergence to\nstationary points, we shall also look at the worst-case iteration complexity.\nThis will be followed by an estimate for the expectation of the squared\nEuclidean norm of the gradient. Various computational simulations for the\ntraining of neural networks are being used to support the theoretical analysis.\nFor future research we emphasize that there are multiple possible extensions to\nour work, from which we mention the investigation regarding non-smooth\nobjective functions and the theoretical analysis of a more general formulation\nthat encompass our adaptive optimizers in a stochastic framework.\n', '  Adaptive gradient methods are workhorses in deep learning. However, the\nconvergence guarantees of adaptive gradient methods for nonconvex optimization\nhave not been thoroughly studied. In this paper, we provide a fine-grained\nconvergence analysis for a general class of adaptive gradient methods including\nAMSGrad, RMSProp and AdaGrad. For smooth nonconvex functions, we prove that\nadaptive gradient methods in expectation converge to a first-order stationary\npoint. Our convergence rate is better than existing results for adaptive\ngradient methods in terms of dimension. In addition, we also prove high\nprobability bounds on the convergence rates of AMSGrad, RMSProp as well as\nAdaGrad, which have not been established before. Our analyses shed light on\nbetter understanding the mechanism behind adaptive gradient methods in\noptimizing nonconvex objectives.\n', '  The training of machine learning models is typically carried out using some\nform of gradient descent, often with great success. However, non-asymptotic\nanalyses of first-order optimization algorithms typically employ a gradient\nsmoothness assumption (formally, Lipschitz continuity of the gradient) that is\ntoo strong to be applicable in the case of deep neural networks. To address\nthis, we propose an algorithm named duality structure gradient descent (DSGD)\nthat is amenable to non-asymptotic performance analysis, under mild assumptions\non the training set and network architecture. The algorithm can be viewed as a\nform of layer-wise coordinate descent, where at each iteration the algorithm\nchooses one layer of the network to update. The decision of what layer to\nupdate is done in a greedy fashion, based on a rigorous lower bound on the\nimprovement of the objective function for each choice of layer. In the\nanalysis, we bound the time required to reach approximate stationary points, in\nboth the deterministic and stochastic settings. The convergence is measured in\nterms of a parameter-dependent family of norms that is derived from the network\narchitecture and designed to confirm a smoothness-like property on the gradient\nof the training loss function. We empirically demonstrate the behavior of DSGD\nin several neural network training scenarios.\n']"
7,29,7_bayesian_models_estimation_learning,"['bayesian', 'models', 'estimation', 'learning', 'prediction', 'mcmc', 'posterior', 'stochastic', 'forecasting', 'model']","['models', 'data', 'model', 'method', 'parameters', 'process', 'processes', 'demand', 'diffusion', 'learning']","['  Bayesian optimization (BO) has become a popular strategy for global\noptimization of expensive real-world functions. Contrary to a common\nexpectation that BO is suited to optimizing black-box functions, it actually\nrequires domain knowledge about those functions to deploy BO successfully. Such\ndomain knowledge often manifests in Gaussian process (GP) priors that specify\ninitial beliefs on functions. However, even with expert knowledge, it is\nnon-trivial to quantitatively define a prior. This is especially true for\nhyperparameter tuning problems on complex machine learning models, where\nlandscapes of tuning objectives are often difficult to comprehend. We seek an\nalternative practice for setting these functional priors. In particular, we\nconsider the scenario where we have data from similar functions that allow us\nto pre-train a tighter distribution a priori. We detail what pre-training\nentails for GPs using a KL divergence based loss function, and propose a new\npre-training based BO framework named HyperBO. Theoretically, we show bounded\nposterior predictions and near-zero regrets for HyperBO without assuming the\n""ground truth"" GP prior is known. To verify our approach in realistic setups,\nwe collect a large multi-task hyperparameter tuning dataset by training tens of\nthousands of configurations of near-state-of-the-art deep learning models on\npopular image and text datasets, as well as a protein sequence dataset. Our\nresults show that on average, HyperBO is able to locate good hyperparameters at\nleast 3 times more efficiently than the best competing methods on both our new\ntuning dataset and existing multi-task BO benchmarks.\n', '  This paper develops mfEGRA, a multifidelity active learning method using\ndata-driven adaptively refined surrogates for failure boundary location in\nreliability analysis. This work addresses the issue of prohibitive cost of\nreliability analysis using Monte Carlo sampling for expensive-to-evaluate\nhigh-fidelity models by using cheaper-to-evaluate approximations of the\nhigh-fidelity model. The method builds on the Efficient Global Reliability\nAnalysis (EGRA) method, which is a surrogate-based method that uses adaptive\nsampling for refining Gaussian process surrogates for failure boundary location\nusing a single-fidelity model. Our method introduces a two-stage adaptive\nsampling criterion that uses a multifidelity Gaussian process surrogate to\nleverage multiple information sources with different fidelities. The method\ncombines expected feasibility criterion from EGRA with one-step lookahead\ninformation gain to refine the surrogate around the failure boundary. The\ncomputational savings from mfEGRA depends on the discrepancy between the\ndifferent models, and the relative cost of evaluating the different models as\ncompared to the high-fidelity model. We show that accurate estimation of\nreliability using mfEGRA leads to computational savings of $\\sim$46% for an\nanalytic multimodal test problem and 24% for a three-dimensional acoustic horn\nproblem, when compared to single-fidelity EGRA. We also show the effect of\nusing a priori drawn Monte Carlo samples in the implementation for the acoustic\nhorn problem, where mfEGRA leads to computational savings of 45% for the\nthree-dimensional case and 48% for a rarer event four-dimensional case as\ncompared to single-fidelity EGRA.\n', '  Gaussian processes~(Kriging) are interpolating data-driven models that are\nfrequently applied in various disciplines. Often, Gaussian processes are\ntrained on datasets and are subsequently embedded as surrogate models in\noptimization problems. These optimization problems are nonconvex and global\noptimization is desired. However, previous literature observed computational\nburdens limiting deterministic global optimization to Gaussian processes\ntrained on few data points. We propose a reduced-space formulation for\ndeterministic global optimization with trained Gaussian processes embedded. For\noptimization, the branch-and-bound solver branches only on the degrees of\nfreedom and McCormick relaxations are propagated through explicit Gaussian\nprocess models. The approach also leads to significantly smaller and\ncomputationally cheaper subproblems for lower and upper bounding. To further\naccelerate convergence, we derive envelopes of common covariance functions for\nGPs and tight relaxations of acquisition functions used in Bayesian\noptimization including expected improvement, probability of improvement, and\nlower confidence bound. In total, we reduce computational time by orders of\nmagnitude compared to state-of-the-art methods, thus overcoming previous\ncomputational burdens. We demonstrate the performance and scaling of the\nproposed method and apply it to Bayesian optimization with global optimization\nof the acquisition function and chance-constrained programming. The Gaussian\nprocess models, acquisition functions, and training scripts are available\nopen-source within the ""MeLOn - Machine Learning Models for Optimization""\ntoolbox~(https://git.rwth-aachen.de/avt.svt/public/MeLOn).\n']"
8,26,8_boosting_classification_ensemble_features,"['boosting', 'classification', 'ensemble', 'features', 'feature', 'prediction', 'predictive', 'predict', 'forests', 'models']","['decision', 'machine', 'models', 'trees', 'forests', 'learning', 'data', 'performance', 'predictive', 'random']","['  Decision forests, including Random Forests and Gradient Boosting Trees, have\nrecently demonstrated state-of-the-art performance in a variety of machine\nlearning settings. Decision forests are typically ensembles of axis-aligned\ndecision trees; that is, trees that split only along feature dimensions. In\ncontrast, many recent extensions to decision forests are based on axis-oblique\nsplits. Unfortunately, these extensions forfeit one or more of the favorable\nproperties of decision forests based on axis-aligned splits, such as robustness\nto many noise dimensions, interpretability, or computational efficiency. We\nintroduce yet another decision forest, called ""Sparse Projection Oblique\nRandomer Forests"" (SPORF). SPORF uses very sparse random projections, i.e.,\nlinear combinations of a small subset of features. SPORF significantly improves\naccuracy over existing state-of-the-art algorithms on a standard benchmark\nsuite for classification with >100 problems of varying dimension, sample size,\nand number of classes. To illustrate how SPORF addresses the limitations of\nboth axis-aligned and existing oblique decision forest methods, we conduct\nextensive simulated experiments. SPORF typically yields improved performance\nover existing decision forests, while mitigating computational efficiency and\nscalability and maintaining interpretability. SPORF can easily be incorporated\ninto other ensemble methods such as boosting to obtain potentially similar\ngains.\n', '  In machine learning (ML), ensemble methods such as bagging, boosting, and\nstacking are widely-established approaches that regularly achieve top-notch\npredictive performance. Stacking (also called ""stacked generalization"") is an\nensemble method that combines heterogeneous base models, arranged in at least\none layer, and then employs another metamodel to summarize the predictions of\nthose models. Although it may be a highly-effective approach for increasing the\npredictive performance of ML, generating a stack of models from scratch can be\na cumbersome trial-and-error process. This challenge stems from the enormous\nspace of available solutions, with different sets of data instances and\nfeatures that could be used for training, several algorithms to choose from,\nand instantiations of these algorithms using diverse parameters (i.e., models)\nthat perform differently according to various metrics. In this work, we present\na knowledge generation model, which supports ensemble learning with the use of\nvisualization, and a visual analytics system for stacked generalization. Our\nsystem, StackGenVis, assists users in dynamically adapting performance metrics,\nmanaging data instances, selecting the most important features for a given data\nset, choosing a set of top-performant and diverse algorithms, and measuring the\npredictive performance. In consequence, our proposed tool helps users to decide\nbetween distinct models and to reduce the complexity of the resulting stack by\nremoving overpromising and underperforming models. The applicability and\neffectiveness of StackGenVis are demonstrated with two use cases: a real-world\nhealthcare data set and a collection of data related to sentiment/stance\ndetection in texts. Finally, the tool has been evaluated through interviews\nwith three ML experts.\n', ""  Problem definition. In retailing, discrete choice models (DCMs) are commonly\nused to capture the choice behavior of customers when offered an assortment of\nproducts. When estimating DCMs using transaction data, flexible models (such as\nmachine learning models or nonparametric models) are typically not\ninterpretable and hard to estimate, while tractable models (such as the\nmultinomial logit model) tend to misspecify the complex behavior represeted in\nthe data. Methodology/results. In this study, we use a forest of binary\ndecision trees to represent DCMs. This approach is based on random forests, a\npopular machine learning algorithm. The resulting model is interpretable: the\ndecision trees can explain the decision-making process of customers during the\npurchase. We show that our approach can predict the choice probability of any\nDCM consistently and thus never suffers from misspecification. Moreover, our\nalgorithm predicts assortments unseen in the training data. The mechanism and\nerrors can be theoretically analyzed. We also prove that the random forest can\nrecover preference rankings of customers thanks to the splitting criterion such\nas the Gini index and information gain ratio. Managerial implications. The\nframework has unique practical advantages. It can capture customers' behavioral\npatterns such as irrationality or sequential searches when purchasing a\nproduct. It handles nonstandard formats of training data that result from\naggregation. It can measure product importance based on how frequently a random\ncustomer would make decisions depending on the presence of the product. It can\nalso incorporate price information and customer features. Our numerical\nexperiments using synthetic and real data show that using random forests to\nestimate customer choices can outperform existing methods.\n""]"
9,25,9_logics_semantics_relational_logic,"['logics', 'semantics', 'relational', 'logic', 'ontological', 'reasoning', 'notions', 'inference', 'satisfiability', 'boolean']","['logic', 'relational', 'semantics', 'logics', 'expressiveness', 'algebraic', 'proportions', 'complexity', 'constraints', 'analogical']","[""  We study Boolean classification problems over relational background\nstructures in the logical framework introduced by Grohe and Tur\\'an (TOCS\n2004). It is known (Grohe and Ritzert, LICS 2017) that classifiers definable in\nfirst-order logic over structures of polylogarithmic degree can be learned in\nsublinear time, where the degree of the structure and the running time are\nmeasured in terms of the size of the structure. We generalise the results to\nthe first-order logic with counting FOCN, which was introduced by Kuske and\nSchweikardt (LICS 2017) as an expressive logic generalising various other\ncounting logics. Specifically, we prove that classifiers definable in FOCN over\nclasses of structures of polylogarithmic degree can be consistently learned in\nsublinear time. This can be seen as a first step towards extending the learning\nframework to include numerical aspects of machine learning. We extend the\nresult to agnostic probably approximately correct (PAC) learning for classes of\nstructures of degree at most $(\\log \\log n)^c$ for some constant $c$. Moreover,\nwe show that bounding the degree is crucial to obtain sublinear-time learning\nalgorithms. That is, we prove that, for structures of unbounded degree,\nlearning is not possible in sublinear time, even for classifiers definable in\nplain first-order logic.\n"", '  This survey explores the integration of learning and reasoning in two\ndifferent fields of artificial intelligence: neurosymbolic and statistical\nrelational artificial intelligence. Neurosymbolic artificial intelligence\n(NeSy) studies the integration of symbolic reasoning and neural networks, while\nstatistical relational artificial intelligence (StarAI) focuses on integrating\nlogic with probabilistic graphical models. This survey identifies seven shared\ndimensions between these two subfields of AI. These dimensions can be used to\ncharacterize different NeSy and StarAI systems. They are concerned with (1) the\napproach to logical inference, whether model or proof-based; (2) the syntax of\nthe used logical theories; (3) the logical semantics of the systems and their\nextensions to facilitate learning; (4) the scope of learning, encompassing\neither parameter or structure learning; (5) the presence of symbolic and\nsubsymbolic representations; (6) the degree to which systems capture the\noriginal logic, probabilistic, and neural paradigms; and (7) the classes of\nlearning tasks the systems are applied to. By positioning various NeSy and\nStarAI systems along these dimensions and pointing out similarities and\ndifferences between them, this survey contributes fundamental concepts for\nunderstanding the integration of learning and reasoning.\n', ""  Analogy-making is at the core of human and artificial intelligence and\ncreativity with applications to such diverse tasks as proving mathematical\ntheorems and building mathematical theories, common sense reasoning, learning,\nlanguage acquisition, and story telling. This paper introduces from first\nprinciples an abstract algebraic framework of analogical proportions of the\nform `$a$ is to $b$ what $c$ is to $d$' in the general setting of universal\nalgebra. This enables us to compare mathematical objects possibly across\ndifferent domains in a uniform way which is crucial for AI-systems. It turns\nout that our notion of analogical proportions has appealing mathematical\nproperties. As we construct our model from first principles using only\nelementary concepts of universal algebra, and since our model questions some\nbasic properties of analogical proportions presupposed in the literature, to\nconvince the reader of the plausibility of our model we show that it can be\nnaturally embedded into first-order logic via model-theoretic types and prove\nfrom that perspective that analogical proportions are compatible with\nstructure-preserving mappings. This provides conceptual evidence for its\napplicability. In a broader sense, this paper is a first step towards a theory\nof analogical reasoning and learning systems with potential applications to\nfundamental AI-problems like common sense reasoning and computational learning\nand creativity.\n""]"
10,24,10_adversarial_adversary_robustness_gans,"['adversarial', 'adversary', 'robustness', 'gans', 'robust', 'attacks', 'evasion', 'security', 'malicious', 'defending']","['adversarial', 'robustness', 'attacks', 'attack', 'examples', 'defense', 'malware', 'neural', 'security', 'perturbations']","['  Deep Neural Networks (DNNs) are vulnerable to invisible perturbations on the\nimages generated by adversarial attacks, which raises researches on the\nadversarial robustness of DNNs. A series of methods represented by the\nadversarial training and its variants have proven as one of the most effective\ntechniques in enhancing the DNN robustness. Generally, adversarial training\nfocuses on enriching the training data by involving perturbed data. Such data\naugmentation effect of the involved perturbed data in adversarial training does\nnot contribute to the robustness of DNN itself and usually suffers from clean\naccuracy drop. Towards the robustness of DNN itself, we in this paper propose a\nnovel defense that aims at augmenting the model in order to learn features that\nare adaptive to diverse inputs, including adversarial examples. More\nspecifically, to augment the model, multiple paths are embedded into the\nnetwork, and an orthogonality constraint is imposed on these paths to guarantee\nthe diversity among them. A margin-maximization loss is then designed to\nfurther boost such DIversity via Orthogonality (DIO). In this way, the proposed\nDIO augments the model and enhances the robustness of DNN itself as the learned\nfeatures can be corrected by these mutually-orthogonal paths. Extensive\nempirical results on various data sets, structures and attacks verify the\nstronger adversarial robustness of the proposed DIO utilizing model\naugmentation. Besides, DIO can also be flexibly combined with different data\naugmentation techniques (e.g., TRADES and DDPM), further promoting robustness\ngains.\n', '  Previous works have shown that automatic speaker verification (ASV) is\nseriously vulnerable to malicious spoofing attacks, such as replay, synthetic\nspeech, and recently emerged adversarial attacks. Great efforts have been\ndedicated to defending ASV against replay and synthetic speech; however, only a\nfew approaches have been explored to deal with adversarial attacks. All the\nexisting approaches to tackle adversarial attacks for ASV require the knowledge\nfor adversarial samples generation, but it is impractical for defenders to know\nthe exact attack algorithms that are applied by the in-the-wild attackers. This\nwork is among the first to perform adversarial defense for ASV without knowing\nthe specific attack algorithms. Inspired by self-supervised learning models\n(SSLMs) that possess the merits of alleviating the superficial noise in the\ninputs and reconstructing clean samples from the interrupted ones, this work\nregards adversarial perturbations as one kind of noise and conducts adversarial\ndefense for ASV by SSLMs. Specifically, we propose to perform adversarial\ndefense from two perspectives: 1) adversarial perturbation purification and 2)\nadversarial perturbation detection. Experimental results show that our\ndetection module effectively shields the ASV by detecting adversarial samples\nwith an accuracy of around 80%. Moreover, since there is no common metric for\nevaluating the adversarial defense performance for ASV, this work also\nformalizes evaluation metrics for adversarial defense considering both\npurification and detection based approaches into account. We sincerely\nencourage future works to benchmark their approaches based on the proposed\nevaluation framework.\n', '  Deep neural networks have been successfully applied in various machine\nlearning tasks. However, studies show that neural networks are susceptible to\nadversarial attacks. This exposes a potential threat to neural network-based\nintelligent systems. We observe that the probability of the correct result\noutputted by the neural network increases by applying small first-order\nperturbations generated for non-predicted class labels to adversarial examples.\nBased on this observation, we propose a method for counteracting adversarial\nperturbations to improve adversarial robustness. In the proposed method, we\nrandomly select a number of class labels and generate small first-order\nperturbations for these selected labels. The generated perturbations are added\ntogether and then clamped onto a specified space. The obtained perturbation is\nfinally added to the adversarial example to counteract the adversarial\nperturbation contained in the example. The proposed method is applied at\ninference time and does not require retraining or finetuning the model. We\nexperimentally validate the proposed method on CIFAR-10 and CIFAR-100. The\nresults demonstrate that our method effectively improves the defense\nperformance of several transformation-based defense methods, especially against\nstrong adversarial examples generated using more iterations.\n']"
11,24,11_regularization_lasso_regularized_minimization,"['regularization', 'lasso', 'regularized', 'minimization', 'matrix', 'predictors', 'matrices', 'pca', 'penalized', 'completion']","['matrix', 'rank', 'lasso', 'regression', 'linear', 'screening', 'squares', 'low', 'rule', 'completion']","['  Predictor screening rules, which discard predictors before fitting a model,\nhave had considerable impact on the speed with which sparse regression\nproblems, such as the lasso, can be solved. In this paper we present a new\nscreening rule for solving the lasso path: the Hessian Screening Rule. The rule\nuses second-order information from the model to provide both effective\nscreening, particularly in the case of high correlation, as well as accurate\nwarm starts. The proposed rule outperforms all alternatives we study on\nsimulated data sets with both low and high correlation for $\\ell_1$-regularized\nleast-squares (the lasso) and logistic regression. It also performs best in\ngeneral on the real data sets that we examine.\n', '  In this paper, we develop a relative error bound for nuclear norm regularized\nmatrix completion, with the focus on the completion of full-rank matrices.\nUnder the assumption that the top eigenspaces of the target matrix are\nincoherent, we derive a relative upper bound for recovering the best low-rank\napproximation of the unknown matrix. Although multiple works have been devoted\nto analyzing the recovery error of full-rank matrix completion, their error\nbounds are usually additive, making it impossible to obtain the perfect\nrecovery case and more generally difficult to leverage the skewed distribution\nof eigenvalues. Our analysis is built upon the optimality condition of the\nregularized formulation and existing guarantees for low-rank matrix completion.\nTo the best of our knowledge, this is the first relative bound that has been\nproved for the regularized formulation of matrix completion.\n', '  This paper considers the problem of estimating a low-rank matrix from the\nobservation of all or a subset of its entries in the presence of Poisson noise.\nWhen we observe all entries, this is a problem of matrix denoising; when we\nobserve only a subset of the entries, this is a problem of matrix completion.\nIn both cases, we exploit an assumption that the underlying matrix is low-rank.\nSpecifically, we analyze several estimators, including a constrained\nnuclear-norm minimization program, nuclear-norm regularized least squares, and\na nonconvex constrained low-rank optimization problem. We show that for all\nthree estimators, with high probability, we have an upper error bound (in the\nFrobenius norm error metric) that depends on the matrix rank, the fraction of\nthe elements observed, and maximal row and column sums of the true matrix. We\nfurthermore show that the above results are minimax optimal (within a universal\nconstant) in classes of matrices with low rank and bounded row and column sums.\nWe also extend these results to handle the case of matrix multinomial denoising\nand completion.\n']"
12,21,12_kernels_dimensionality_distributions_kernel,"['kernels', 'dimensionality', 'distributions', 'kernel', 'gaussian', 'outliers', 'sampling', 'dimensional', 'metric', 'divergence']","['kernel', 'test', 'distance', 'data', 'dimensional', 'guarantees', 'log', 'maximum', 'sample', 'distributions']","[""  We introduce kernel thinning, a new procedure for compressing a distribution\n$\\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given\na suitable reproducing kernel $\\mathbf{k}_{\\star}$ and $O(n^2)$ time, kernel\nthinning compresses an $n$-point approximation to $\\mathbb{P}$ into a\n$\\sqrt{n}$-point approximation with comparable worst-case integration error\nacross the associated reproducing kernel Hilbert space. The maximum discrepancy\nin integration error is $O_d(n^{-1/2}\\sqrt{\\log n})$ in probability for\ncompactly supported $\\mathbb{P}$ and $O_d(n^{-\\frac{1}{2}} (\\log\nn)^{(d+1)/2}\\sqrt{\\log\\log n})$ for sub-exponential $\\mathbb{P}$ on\n$\\mathbb{R}^d$. In contrast, an equal-sized i.i.d. sample from $\\mathbb{P}$\nsuffers $\\Omega(n^{-1/4})$ integration error. Our sub-exponential guarantees\nresemble the classical quasi-Monte Carlo error rates for uniform $\\mathbb{P}$\non $[0,1]^d$ but apply to general distributions on $\\mathbb{R}^d$ and a wide\nrange of common kernels. Moreover, the same construction delivers near-optimal\n$L^\\infty$ coresets in $O(n^2)$ time. We use our results to derive explicit\nnon-asymptotic maximum mean discrepancy bounds for Gaussian, Mat\\'ern, and\nB-spline kernels and present two vignettes illustrating the practical benefits\nof kernel thinning over i.i.d. sampling and standard Markov chain Monte Carlo\nthinning, in dimensions $d=2$ through $100$.\n"", '  We present a study of a kernel-based two-sample test statistic related to the\nMaximum Mean Discrepancy (MMD) in the manifold data setting, assuming that\nhigh-dimensional observations are close to a low-dimensional manifold. We\ncharacterize the test level and power in relation to the kernel bandwidth, the\nnumber of samples, and the intrinsic dimensionality of the manifold.\nSpecifically, when data densities $p$ and $q$ are supported on a\n$d$-dimensional sub-manifold ${M}$ embedded in an $m$-dimensional space and are\nH\\""older with order $\\beta$ (up to 2) on ${M}$, we prove a guarantee of the\ntest power for finite sample size $n$ that exceeds a threshold depending on\n$d$, $\\beta$, and $\\Delta_2$ the squared $L^2$-divergence between $p$ and $q$\non the manifold, and with a properly chosen kernel bandwidth $\\gamma$. For\nsmall density departures, we show that with large $n$ they can be detected by\nthe kernel test when $\\Delta_2$ is greater than $n^{- { 2 \\beta/( d + 4 \\beta )\n}}$ up to a certain constant and $\\gamma$ scales as $n^{-1/(d+4\\beta)}$. The\nanalysis extends to cases where the manifold has a boundary and the data\nsamples contain high-dimensional additive noise. Our results indicate that the\nkernel two-sample test has no curse-of-dimensionality when the data lie on or\nnear a low-dimensional manifold. We validate our theory and the properties of\nthe kernel test for manifold data through a series of numerical experiments.\n', ""  The kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) compresses a\nprobability distribution more effectively than independent sampling by\ntargeting a reproducing kernel Hilbert space (RKHS) and leveraging a less\nsmooth square-root kernel. Here we provide four improvements. First, we show\nthat KT applied directly to the target RKHS yields tighter, dimension-free\nguarantees for any kernel, any distribution, and any fixed function in the\nRKHS. Second, we show that, for analytic kernels like Gaussian, inverse\nmultiquadric, and sinc, target KT admits maximum mean discrepancy (MMD)\nguarantees comparable to or better than those of square-root KT without making\nexplicit use of a square-root kernel. Third, we prove that KT with a fractional\npower kernel yields better-than-Monte-Carlo MMD guarantees for non-smooth\nkernels, like Laplace and Mat\\'ern, that do not have square-roots. Fourth, we\nestablish that KT applied to a sum of the target and power kernels (a procedure\nwe call KT+) simultaneously inherits the improved MMD guarantees of power KT\nand the tighter individual function guarantees of target KT. In our experiments\nwith target KT and KT+, we witness significant improvements in integration\nerror even in $100$ dimensions and when compressing challenging differential\nequation posteriors.\n""]"
13,21,13_generative_gans_gan_adversarial,"['generative', 'gans', 'gan', 'adversarial', 'codegan', 'cgan', 'neural', 'deep', 'asymmetricgan', 'supervised']","['generative', 'image', 'latent', 'training', 'generation', 'quality', 'disentanglement', 'model', 'distribution', 'learning']","['  We study the problem of self-supervised structured representation learning\nusing autoencoders for downstream tasks such as generative modeling. Unlike\nmost methods which rely on matching an arbitrary, relatively unstructured,\nprior distribution for sampling, we propose a sampling technique that relies\nsolely on the independence of latent variables, thereby avoiding the trade-off\nbetween reconstruction quality and generative performance typically observed in\nVAEs. We design a novel autoencoder architecture capable of learning a\nstructured representation without the need for aggressive regularization. Our\nstructural decoders learn a hierarchy of latent variables, thereby ordering the\ninformation without any additional regularization or supervision. We\ndemonstrate how these models learn a representation that improves results in a\nvariety of downstream tasks including generation, disentanglement, and\nextrapolation using several challenging and natural image datasets.\n', '  Existing models for unsupervised image translation with Generative\nAdversarial Networks (GANs) can learn the mapping from the source domain to the\ntarget domain using a cycle-consistency loss. However, these methods always\nadopt a symmetric network architecture to learn both forward and backward\ncycles. Because of the task complexity and cycle input difference between the\nsource and target domains, the inequality in bidirectional forward-backward\ncycle translations is significant and the amount of information between two\ndomains is different. In this paper, we analyze the limitation of existing\nsymmetric GANs in asymmetric translation tasks, and propose an AsymmetricGAN\nmodel with both translation and reconstruction generators of unequal sizes and\ndifferent parameter-sharing strategy to adapt to the asymmetric need in both\nunsupervised and supervised image translation tasks. Moreover, the training\nstage of existing methods has the common problem of model collapse that\ndegrades the quality of the generated images, thus we explore different\noptimization losses for better training of AsymmetricGAN, making image\ntranslation with higher consistency and better stability. Extensive experiments\non both supervised and unsupervised generative tasks with 8 datasets show that\nAsymmetricGAN achieves superior model capacity and better generation\nperformance compared with existing GANs. To the best of our knowledge, we are\nthe first to investigate the asymmetric GAN structure on both unsupervised and\nsupervised image translation tasks.\n', '  Current autoencoder-based disentangled representation learning methods\nachieve disentanglement by penalizing the (aggregate) posterior to encourage\nstatistical independence of the latent factors. This approach introduces a\ntrade-off between disentangled representation learning and reconstruction\nquality since the model does not have enough capacity to learn correlated\nlatent variables that capture detail information present in most image data. To\novercome this trade-off, we present a novel multi-stage modeling approach where\nthe disentangled factors are first learned using a penalty-based disentangled\nrepresentation learning method; then, the low-quality reconstruction is\nimproved with another deep generative model that is trained to model the\nmissing correlated latent variables, adding detail information while\nmaintaining conditioning on the previously learned disentangled factors. Taken\ntogether, our multi-stage modelling approach results in a single, coherent\nprobabilistic model that is theoretically justified by the principal of\nD-separation and can be realized with a variety of model classes including\nlikelihood-based models such as variational autoencoders, implicit models such\nas generative adversarial networks, and tractable models like normalizing flows\nor mixtures of Gaussians. We demonstrate that our multi-stage model has higher\nreconstruction quality than current state-of-the-art methods with equivalent\ndisentanglement performance across multiple standard benchmarks. In addition,\nwe apply the multi-stage model to generate synthetic tabular datasets,\nshowcasing an enhanced performance over benchmark models across a variety of\nmetrics. The interpretability analysis further indicates that the multi-stage\nmodel can effectively uncover distinct and meaningful features of variations\nfrom which the original distribution can be recovered.\n']"
14,21,14_causal_interventions_estimating_predictive,"['causal', 'interventions', 'estimating', 'predictive', 'inference', 'counterfactual', 'observational', 'predictor', 'regression', 'models']","['causal', 'effects', 'effect', 'covariates', 'parametric', 'inference', 'data', 'non', 'outcome', 'interventions']","['  This paper presents a new optimization approach to causal estimation. Given\ndata that contains covariates and an outcome, which covariates are causes of\nthe outcome, and what is the strength of the causality? In classical machine\nlearning (ML), the goal of optimization is to maximize predictive accuracy.\nHowever, some covariates might exhibit a non-causal association with the\noutcome. Such spurious associations provide predictive power for classical ML,\nbut they prevent us from causally interpreting the result. This paper proposes\nCoCo, an optimization algorithm that bridges the gap between pure prediction\nand causal inference. CoCo leverages the recently-proposed idea of\nenvironments, datasets of covariates/response where the causal relationships\nremain invariant but where the distribution of the covariates changes from\nenvironment to environment. Given datasets from multiple environments-and ones\nthat exhibit sufficient heterogeneity-CoCo maximizes an objective for which the\nonly solution is the causal solution. We describe the theoretical foundations\nof this approach and demonstrate its effectiveness on simulated and real\ndatasets. Compared to classical ML and existing methods, CoCo provides more\naccurate estimates of the causal model and more accurate predictions under\ninterventions.\n', '  What is the difference of a prediction that is made with a causal model and a\nnon-causal model? Suppose we intervene on the predictor variables or change the\nwhole environment. The predictions from a causal model will in general work as\nwell under interventions as for observational data. In contrast, predictions\nfrom a non-causal model can potentially be very wrong if we actively intervene\non variables. Here, we propose to exploit this invariance of a prediction under\na causal model for causal inference: given different experimental settings (for\nexample various interventions) we collect all models that do show invariance in\ntheir predictive accuracy across settings and interventions. The causal model\nwill be a member of this set of models with high probability. This approach\nyields valid confidence intervals for the causal relationships in quite general\nscenarios. We examine the example of structural equation models in more detail\nand provide sufficient assumptions under which the set of causal predictors\nbecomes identifiable. We further investigate robustness properties of our\napproach under model misspecification and discuss possible extensions. The\nempirical properties are studied for various data sets, including large-scale\ngene perturbation experiments.\n', '  Causal discovery, i.e., inferring underlying causal relationships from\nobservational data, has been shown to be highly challenging for AI systems. In\ntime series modeling context, traditional causal discovery methods mainly\nconsider constrained scenarios with fully observed variables and/or data from\nstationary time-series. We develop a causal discovery approach to handle a wide\nclass of non-stationary time-series that are conditionally stationary, where\nthe non-stationary behaviour is modeled as stationarity conditioned on a set of\n(possibly hidden) state variables. Named State-Dependent Causal Inference\n(SDCI), our approach is able to recover the underlying causal dependencies,\nprovably with fully-observed states and empirically with hidden states. The\nlatter is confirmed by experiments on synthetic linear system and nonlinear\nparticle interaction data, where SDCI achieves superior performance over\nbaseline causal discovery methods. Improved results over non-causal RNNs on\nmodeling NBA player movements demonstrate the potential of our method and\nmotivate the use of causality-driven methods for forecasting.\n']"
15,19,15_tensorization_neural_networks_tensor,"['tensorization', 'neural', 'networks', 'tensor', 'approximating', 'approximation', 'generalization', 'layers', 'smoothness', 'deepening']","['networks', 'approximation', 'neural', 'functions', 'spaces', 'layer', 'tensor', 'deep', 'margin', 'network']","['  We study the approximation of multivariate functions with tensor networks\n(TNs). The main conclusion of this work is an answer to the following two\nquestions: ``What are the approximation capabilities of TNs?"" and ""What is an\nappropriate model class of functions that can be approximated with TNs?""\n  To answer the former, we show that TNs can (near to) optimally replicate\n$h$-uniform and $h$-adaptive approximation, for any smoothness order of the\ntarget function. Tensor networks thus exhibit universal expressivity w.r.t.\nisotropic, anisotropic and mixed smoothness spaces that is comparable with more\ngeneral neural networks families such as deep rectified linear unit (ReLU)\nnetworks. Put differently, TNs have the capacity to (near to) optimally\napproximate many function classes -- without being adapted to the particular\nclass in question.\n  To answer the latter, as a candidate model class we consider approximation\nclasses of TNs and show that these are (quasi-)Banach spaces, that many types\nof classical smoothness spaces are continuously embedded into said\napproximation classes and that TN approximation classes are themselves not\nembedded in any classical smoothness space.\n', '  We study the approximation by tensor networks (TNs) of functions from\nclassical smoothness classes. The considered approximation tool combines a\ntensorization of functions in $L^p([0,1))$, which allows to identify a\nunivariate function with a multivariate function (or tensor), and the use of\ntree tensor networks (the tensor train format) for exploiting low-rank\nstructures of multivariate functions. The resulting tool can be interpreted as\na feed-forward neural network, with first layers implementing the\ntensorization, interpreted as a particular featuring step, followed by a\nsum-product network with sparse architecture. In part I of this work, we\npresented several approximation classes associated with different measures of\ncomplexity of tensor networks and studied their properties. In this work (part\nII), we show how classical approximation tools, such as polynomials or splines\n(with fixed or free knots), can be encoded as a tensor network with controlled\ncomplexity. We use this to derive direct (Jackson) inequalities for the\napproximation spaces of tensor networks. This is then utilized to show that\nBesov spaces are continuously embedded into these approximation spaces. In\nother words, we show that arbitrary Besov functions can be approximated with\noptimal or near to optimal rate. We also show that an arbitrary function in the\napproximation class possesses no Besov smoothness, unless one limits the depth\nof the tensor network.\n', '  We study the approximation of functions by tensor networks (TNs). We show\nthat Lebesgue $L^p$-spaces in one dimension can be identified with tensor\nproduct spaces of arbitrary order through tensorization. We use this tensor\nproduct structure to define subsets of $L^p$ of rank-structured functions of\nfinite representation complexity. These subsets are then used to define\ndifferent approximation classes of tensor networks, associated with different\nmeasures of complexity. These approximation classes are shown to be\nquasi-normed linear spaces. We study some elementary properties and\nrelationships of said spaces. In part II of this work, we will show that\nclassical smoothness (Besov) spaces are continuously embedded into these\napproximation classes. We will also show that functions in these approximation\nclasses do not possess any Besov smoothness, unless one restricts the depth of\nthe tensor networks. The results of this work are both an analysis of the\napproximation spaces of TNs and a study of the expressivity of a particular\ntype of neural networks (NN) -- namely feed-forward sum-product networks with\nsparse architecture. The input variables of this network result from the\ntensorization step, interpreted as a particular featuring step which can also\nbe implemented with a neural network with a specific architecture. We point out\ninteresting parallels to recent results on the expressivity of rectified linear\nunit (ReLU) networks -- currently one of the most popular type of NNs.\n']"
16,19,16_supervised_imaging_ai_classification,"['supervised', 'imaging', 'ai', 'classification', 'segmentation', 'scans', 'learning', 'images', 'lung', 'deep']","['medical', 'segmentation', 'learning', 'cancer', 'deep', 'imaging', 'image', 'scans', 'images', 'models']","['  Coronavirus, or COVID-19, is a hazardous disease that has endangered the\nhealth of many people around the world by directly affecting the lungs.\nCOVID-19 is a medium-sized, coated virus with a single-stranded RNA, and also\nhas one of the largest RNA genomes and is approximately 120 nm. The X-Ray and\ncomputed tomography (CT) imaging modalities are widely used to obtain a fast\nand accurate medical diagnosis. Identifying COVID-19 from these medical images\nis extremely challenging as it is time-consuming and prone to human errors.\nHence, artificial intelligence (AI) methodologies can be used to obtain\nconsistent high performance. Among the AI methods, deep learning (DL) networks\nhave gained popularity recently compared to conventional machine learning (ML).\nUnlike ML, all stages of feature extraction, feature selection, and\nclassification are accomplished automatically in DL models. In this paper, a\ncomplete survey of studies on the application of DL techniques for COVID-19\ndiagnostic and segmentation of lungs is discussed, concentrating on works that\nused X-Ray and CT images. Additionally, a review of papers on the forecasting\nof coronavirus prevalence in different parts of the world with DL is presented.\nLastly, the challenges faced in the detection of COVID-19 using DL techniques\nand directions for future research are discussed.\n', '  Learning from noisy labels is an important concern in plenty of real-world\nscenarios. Various approaches for this concern first make corrections\ncorresponding to potentially noisy-labeled instances, and then update\npredictive model with information of the made corrections. However, in specific\nareas, such as medical histopathology whole slide image analysis (MHWSIA), it\nis often difficult or impossible for experts to manually achieve the noisy-free\nground-truth labels which leads to labels with complex noise. This situation\nraises two more difficult problems: 1) the methodology of approaches making\ncorrections corresponding to potentially noisy-labeled instances has\nlimitations due to the complex noise existing in labels; and 2) the appropriate\nevaluation strategy for validation/testing is unclear because of the great\ndifficulty in collecting the noisy-free ground-truth labels. For the problem\n1), we present one-step abductive multi-target learning (OSAMTL) that imposes a\none-step logical reasoning upon machine learning via a multi-target learning\nprocedure to constrain the predictions of the learning model to be subject to\nour prior knowledge about the true target. For the problem 2), we propose a\nlogical assessment formula (LAF) that evaluates the logical rationality of the\noutputs of an approach by estimating the consistencies between the predictions\nof the learning model and the logical facts narrated from the results of the\none-step logical reasoning of OSAMTL. Based on the Helicobacter pylori (H.\npylori) segmentation task in MHWSIA, we show that OSAMTL enables the machine\nlearning model achieving logically more rational predictions, which is beyond\nvarious state-of-the-art approaches in handling complex noisy labels.\n', '  Recent studies have demonstrated the effectiveness of the combination of\nmachine learning and logical reasoning, including data-driven logical\nreasoning, knowledge driven machine learning and abductive learning, in\ninventing advanced technologies for different artificial intelligence\napplications. One-step abductive multi-target learning (OSAMTL), an approach\ninspired by abductive learning, via simply combining machine learning and\nlogical reasoning in a one-step balanced multi-target learning way, has as well\nshown its effectiveness in handling complex noisy labels of a single noisy\nsample in medical histopathology whole slide image analysis (MHWSIA). However,\nOSAMTL is not suitable for the situation where diverse noisy samples (DiNS) are\nprovided for a learning task. In this paper, giving definition of DiNS, we\npropose one-step abductive multi-target learning with DiNS (OSAMTL-DiNS) to\nexpand the original OSAMTL to handle complex noisy labels of DiNS. Applying\nOSAMTL-DiNS to tumour segmentation for breast cancer in MHWSIA, we show that\nOSAMTL-DiNS is able to enable various state-of-the-art approaches for learning\nfrom noisy labels to achieve more rational predictions. We released a model\npre-trained with OSAMTL-DiNS for tumour segmentation in HE-stained\npre-treatment biopsy images in breast cancer, which has been successfully\napplied as a pre-processing tool to extract tumour-associated stroma\ncompartment for predicting the pathological complete response to neoadjuvant\nchemotherapy in breast cancer.\n']"
17,19,17_cluster_clusters_clustering_dbscan,"['cluster', 'clusters', 'clustering', 'dbscan', 'dendrogram', 'groupings', 'algorithms', 'grouping', 'similarity', 'algorithm']","['clustering', 'clusters', 'means', 'kernel', 'density', 'cluster', 'data', 'distance', 'algorithm', 'method']","['  A recent proposal of data dependent similarity called Isolation\nKernel/Similarity has enabled SVM to produce better classification accuracy. We\nidentify shortcomings of using a tree method to implement Isolation Similarity;\nand propose a nearest neighbour method instead. We formally prove the\ncharacteristic of Isolation Similarity with the use of the proposed method. The\nimpact of Isolation Similarity on density-based clustering is studied here. We\nshow for the first time that the clustering performance of the classic\ndensity-based clustering algorithm DBSCAN can be significantly uplifted to\nsurpass that of the recent density-peak clustering algorithm DP. This is\nachieved by simply replacing the distance measure with the proposed\nnearest-neighbour-induced Isolation Similarity in DBSCAN, leaving the rest of\nthe procedure unchanged. A new type of clusters called mass-connected clusters\nis formally defined. We show that DBSCAN, which detects density-connected\nclusters, becomes one which detects mass-connected clusters, when the distance\nmeasure is replaced with the proposed similarity. We also provide the condition\nunder which mass-connected clusters can be detected, while density-connected\nclusters cannot.\n', '  Clustering non-Euclidean data is difficult, and one of the most used\nalgorithms besides hierarchical clustering is the popular algorithm\nPartitioning Around Medoids (PAM), also simply referred to as k-medoids. In\nEuclidean geometry the mean-as used in k-means-is a good estimator for the\ncluster center, but this does not hold for arbitrary dissimilarities. PAM uses\nthe medoid instead, the object with the smallest dissimilarity to all others in\nthe cluster. This notion of centrality can be used with any (dis-)similarity,\nand thus is of high relevance to many domains such as biology that require the\nuse of Jaccard, Gower, or more complex distances.\n  A key issue with PAM is its high run time cost. We propose modifications to\nthe PAM algorithm to achieve an O(k)-fold speedup in the second SWAP phase of\nthe algorithm, but will still find the same results as the original PAM\nalgorithm. If we slightly relax the choice of swaps performed (at comparable\nquality), we can further accelerate the algorithm by performing up to k swaps\nin each iteration. With the substantially faster SWAP, we can now also explore\nalternative strategies for choosing the initial medoids. We also show how the\nCLARA and CLARANS algorithms benefit from these modifications. It can easily be\ncombined with earlier approaches to use PAM and CLARA on big data (some of\nwhich use PAM as a subroutine, hence can immediately benefit from these\nimprovements), where the performance with high k becomes increasingly\nimportant.\n  In experiments on real data with k=100, we observed a 200-fold speedup\ncompared to the original PAM SWAP algorithm, making PAM applicable to larger\ndata sets as long as we can afford to compute a distance matrix, and in\nparticular to higher k (at k=2, the new SWAP was only 1.5 times faster, as the\nspeedup is expected to increase with k).\n', '  Clustering non-Euclidean data is difficult, and one of the most used\nalgorithms besides hierarchical clustering is the popular algorithm\nPartitioning Around Medoids (PAM), also simply referred to as k-medoids\nclustering. In Euclidean geometry the mean-as used in k-means-is a good\nestimator for the cluster center, but this does not exist for arbitrary\ndissimilarities. PAM uses the medoid instead, the object with the smallest\ndissimilarity to all others in the cluster. This notion of centrality can be\nused with any (dis-)similarity, and thus is of high relevance to many domains\nand applications. A key issue with PAM is its high run time cost. We propose\nmodifications to the PAM algorithm that achieve an O(k)-fold speedup in the\nsecond (""SWAP"") phase of the algorithm, but will still find the same results as\nthe original PAM algorithm. If we relax the choice of swaps performed (while\nretaining comparable quality), we can further accelerate the algorithm by\neagerly performing additional swaps in each iteration. With the substantially\nfaster SWAP, we can now explore faster initialization strategies, because (i)\nthe classic (""BUILD"") initialization now becomes the bottleneck, and (ii) our\nswap is fast enough to compensate for worse starting conditions. We also show\nhow the CLARA and CLARANS algorithms benefit from the proposed modifications.\nWhile we do not study the parallelization of our approach in this work, it can\neasily be combined with earlier approaches to use PAM and CLARA on big data\n(some of which use PAM as a subroutine, hence can immediately benefit from\nthese improvements), where the performance with high k becomes increasingly\nimportant. In experiments on real data with k=100,200, we observed a 458x\nrespectively 1191x speedup compared to the original PAM SWAP algorithm, making\nPAM applicable to larger data sets, and in particular to higher k.\n']"
18,19,18_neural_learning_networks_nonlinear,"['neural', 'learning', 'networks', 'nonlinear', 'pinn', 'gradient', 'pdes', 'odes', 'pinns', 'dpinn']","['equations', 'differential', 'neural', 'physics', 'state', 'solution', 'control', 'networks', 'network', 'equation']","['  Physics-Informed Neural Networks (PINNs) have emerged recently as a promising\napplication of deep neural networks to the numerical solution of nonlinear\npartial differential equations (PDEs). However, it has been recognized that\nadaptive procedures are needed to force the neural network to fit accurately\nthe stubborn spots in the solution of ""stiff"" PDEs. In this paper, we propose a\nfundamentally new way to train PINNs adaptively, where the adaptation weights\nare fully trainable and applied to each training point individually, so the\nneural network learns autonomously which regions of the solution are difficult\nand is forced to focus on them. The self-adaptation weights specify a soft\nmultiplicative soft attention mask, which is reminiscent of similar mechanisms\nused in computer vision. The basic idea behind these SA-PINNs is to make the\nweights increase as the corresponding losses increase, which is accomplished by\ntraining the network to simultaneously minimize the losses and maximize the\nweights. In addition, we show how to build a continuous map of self-adaptive\nweights using Gaussian Process regression, which allows the use of stochastic\ngradient descent in problems where conventional gradient descent is not enough\nto produce accurate solutions. Finally, we derive the Neural Tangent Kernel\nmatrix for SA-PINNs and use it to obtain a heuristic understanding of the\neffect of the self-adaptive weights on the dynamics of training in the limiting\ncase of infinitely-wide PINNs, which suggests that SA-PINNs work by producing a\nsmooth equalization of the eigenvalues of the NTK matrix corresponding to the\ndifferent loss terms. In numerical experiments with several linear and\nnonlinear benchmark problems, the SA-PINN outperformed other state-of-the-art\nPINN algorithm in L2 error, while using a smaller number of training epochs.\n', '  Recently, physics-informed neural networks (PINNs) have offered a powerful\nnew paradigm for solving problems relating to differential equations. Compared\nto classical numerical methods PINNs have several advantages, for example their\nability to provide mesh-free solutions of differential equations and their\nability to carry out forward and inverse modelling within the same optimisation\nproblem. Whilst promising, a key limitation to date is that PINNs have\nstruggled to accurately and efficiently solve problems with large domains\nand/or multi-scale solutions, which is crucial for their real-world\napplication. Multiple significant and related factors contribute to this issue,\nincluding the increasing complexity of the underlying PINN optimisation problem\nas the problem size grows and the spectral bias of neural networks. In this\nwork we propose a new, scalable approach for solving large problems relating to\ndifferential equations called Finite Basis PINNs (FBPINNs). FBPINNs are\ninspired by classical finite element methods, where the solution of the\ndifferential equation is expressed as the sum of a finite set of basis\nfunctions with compact support. In FBPINNs neural networks are used to learn\nthese basis functions, which are defined over small, overlapping subdomains.\nFBINNs are designed to address the spectral bias of neural networks by using\nseparate input normalisation over each subdomain, and reduce the complexity of\nthe underlying optimisation problem by using many smaller neural networks in a\nparallel divide-and-conquer approach. Our numerical experiments show that\nFBPINNs are effective in solving both small and larger, multi-scale problems,\noutperforming standard PINNs in both accuracy and computational resources\nrequired, potentially paving the way to the application of PINNs on large,\nreal-world problems.\n', '  Physics-informed neural networks (PINNs) impose known physical laws into the\nlearning of deep neural networks, making sure they respect the physics of the\nprocess while decreasing the demand of labeled data. For systems represented by\nOrdinary Differential Equations (ODEs), the conventional PINN has a continuous\ntime input variable and outputs the solution of the corresponding ODE. In their\noriginal form, PINNs do not allow control inputs, neither can they simulate for\nvariable long-range intervals without serious degradation in their predictions.\nIn this context, this work presents a new framework called Physics-Informed\nNeural Nets for Control (PINC), which proposes a novel PINN-based architecture\nthat is amenable to control problems and able to simulate for longer-range time\nhorizons that are not fixed beforehand, making it a very flexible framework\nwhen compared to traditional PINNs. Furthermore, this long-range time\nsimulation of differential equations is faster than numerical methods since it\nrelies only on signal propagation through the network, making it less\ncomputationally costly and, thus, a better alternative for simulation of models\nin Model Predictive Control. We showcase our proposal in the control of two\nnonlinear dynamic systems: the Van der Pol oscillator and the four-tank system.\n']"
19,18,19_quantum_quantumnas_qubit_qnns,"['quantum', 'quantumnas', 'qubit', 'qnns', 'pqc', 'qml', 'unitary', 'nisq', 'hamiltonians', 'learning']","['quantum', 'noise', 'classical', 'gate', 'gates', 'ansatz', 'computers', 'variational', 'complexity', 'networks']","['  Quantum noise is the key challenge in Noisy Intermediate-Scale Quantum (NISQ)\ncomputers. Previous work for mitigating noise has primarily focused on\ngate-level or pulse-level noise-adaptive compilation. However, limited research\nefforts have explored a higher level of optimization by making the quantum\ncircuits themselves resilient to noise.\n  We propose QuantumNAS, a comprehensive framework for noise-adaptive co-search\nof the variational circuit and qubit mapping. Variational quantum circuits are\na promising approach for constructing QML and quantum simulation. However,\nfinding the best variational circuit and its optimal parameters is challenging\ndue to the large design space and parameter training cost. We propose to\ndecouple the circuit search and parameter training by introducing a novel\nSuperCircuit. The SuperCircuit is constructed with multiple layers of\npre-defined parameterized gates and trained by iteratively sampling and\nupdating the parameter subsets (SubCircuits) of it. It provides an accurate\nestimation of SubCircuits performance trained from scratch. Then we perform an\nevolutionary co-search of SubCircuit and its qubit mapping. The SubCircuit\nperformance is estimated with parameters inherited from SuperCircuit and\nsimulated with real device noise models. Finally, we perform iterative gate\npruning and finetuning to remove redundant gates.\n  Extensively evaluated with 12 QML and VQE benchmarks on 14 quantum computers,\nQuantumNAS significantly outperforms baselines. For QML, QuantumNAS is the\nfirst to demonstrate over 95% 2-class, 85% 4-class, and 32% 10-class\nclassification accuracy on real QC. It also achieves the lowest eigenvalue for\nVQE tasks on H2, H2O, LiH, CH4, BeH2 compared with UCCSD. We also open-source\nTorchQuantum (https://github.com/mit-han-lab/torchquantum) for fast training of\nparameterized quantum circuits to facilitate future research.\n', '  Quantum machine learning -- and specifically Variational Quantum Algorithms\n(VQAs) -- offers a powerful, flexible paradigm for programming near-term\nquantum computers, with applications in chemistry, metrology, materials\nscience, data science, and mathematics. Here, one trains an ansatz, in the form\nof a parameterized quantum circuit, to accomplish a task of interest. However,\nchallenges have recently emerged suggesting that deep ansatzes are difficult to\ntrain, due to flat training landscapes caused by randomness or by hardware\nnoise. This motivates our work, where we present a variable structure approach\nto build ansatzes for VQAs. Our approach, called VAns (Variable Ansatz),\napplies a set of rules to both grow and (crucially) remove quantum gates in an\ninformed manner during the optimization. Consequently, VAns is ideally suited\nto mitigate trainability and noise-related issues by keeping the ansatz\nshallow. We employ VAns in the variational quantum eigensolver for condensed\nmatter and quantum chemistry applications, in the quantum autoencoder for data\ncompression and in unitary compilation problems showing successful results in\nall cases.\n', '  Parameterized Quantum Circuits (PQC) are promising towards quantum advantage\non near-term quantum hardware. However, due to the large quantum noises\n(errors), the performance of PQC models has a severe degradation on real\nquantum devices. Take Quantum Neural Network (QNN) as an example, the accuracy\ngap between noise-free simulation and noisy results on IBMQ-Yorktown for\nMNIST-4 classification is over 60%. Existing noise mitigation methods are\ngeneral ones without leveraging unique characteristics of PQC; on the other\nhand, existing PQC work does not consider noise effect. To this end, we present\nQuantumNAT, a PQC-specific framework to perform noise-aware optimizations in\nboth training and inference stages to improve robustness. We experimentally\nobserve that the effect of quantum noise to PQC measurement outcome is a linear\nmap from noise-free outcome with a scaling and a shift factor. Motivated by\nthat, we propose post-measurement normalization to mitigate the feature\ndistribution differences between noise-free and noisy scenarios. Furthermore,\nto improve the robustness against noise, we propose noise injection to the\ntraining process by inserting quantum error gates to PQC according to realistic\nnoise models of quantum hardware. Finally, post-measurement quantization is\nintroduced to quantize the measurement outcomes to discrete values, achieving\nthe denoising effect. Extensive experiments on 8 classification tasks using 6\nquantum devices demonstrate that QuantumNAT improves accuracy by up to 43%, and\nachieves over 94% 2-class, 80% 4-class, and 34% 10-class classification\naccuracy measured on real quantum computers. The code for construction and\nnoise-aware training of PQC is available in the TorchQuantum library.\n']"
20,14,20_activity_monitoring_activities_wearable,"['activity', 'monitoring', 'activities', 'wearable', 'sensor', 'sensors', 'iot', 'ecg', 'classification', 'signals']","['activity', 'sensor', 'sleep', 'quality', 'heart', 'signals', 'recognition', 'signal', 'leads', 'smart']","['  Human Activity Recognition from body-worn sensor data poses an inherent\nchallenge in capturing spatial and temporal dependencies of time-series\nsignals. In this regard, the existing recurrent or convolutional or their\nhybrid models for activity recognition struggle to capture spatio-temporal\ncontext from the feature space of sensor reading sequence. To address this\ncomplex problem, we propose a self-attention based neural network model that\nforegoes recurrent architectures and utilizes different types of attention\nmechanisms to generate higher dimensional feature representation used for\nclassification. We performed extensive experiments on four popular publicly\navailable HAR datasets: PAMAP2, Opportunity, Skoda and USC-HAD. Our model\nachieve significant performance improvement over recent state-of-the-art models\nin both benchmark test subjects and Leave-one-subject-out evaluation. We also\nobserve that the sensor attention maps produced by our model is able capture\nthe importance of the modality and placement of the sensors in predicting the\ndifferent activity classes.\n', '  Wearable sensor based human activity recognition is a challenging problem due\nto difficulty in modeling spatial and temporal dependencies of sensor signals.\nRecognition models in closed-set assumption are forced to yield members of\nknown activity classes as prediction. However, activity recognition models can\nencounter an unseen activity due to body-worn sensor malfunction or disability\nof the subject performing the activities. This problem can be addressed through\nmodeling solution according to the assumption of open-set recognition. Hence,\nthe proposed self attention based approach combines data hierarchically from\ndifferent sensor placements across time to classify closed-set activities and\nit obtains notable performance improvement over state-of-the-art models on five\npublicly available datasets. The decoder in this autoencoder architecture\nincorporates self-attention based feature representations from encoder to\ndetect unseen activity classes in open-set recognition setting. Furthermore,\nattention maps generated by the hierarchical model demonstrate explainable\nselection of features in activity recognition. We conduct extensive leave one\nsubject out validation experiments that indicate significantly improved\nrobustness to noise and subject specific variability in body-worn sensor\nsignals. The source code is available at:\ngithub.com/saif-mahmud/hierarchical-attention-HAR\n', ""  The quality of sleep has a deep impact on people's physical and mental\nhealth. People with insufficient sleep are more likely to report physical and\nmental distress, activity limitation, anxiety, and pain. Moreover, in the past\nfew years, there has been an explosion of applications and devices for activity\nmonitoring and health tracking. Signals collected from these wearable devices\ncan be used to study and improve sleep quality. In this paper, we utilize the\nrelationship between physical activity and sleep quality to find ways of\nassisting people improve their sleep using machine learning techniques. People\nusually have several behavior modes that their bio-functions can be divided\ninto. Performing time series clustering on activity data, we find cluster\ncenters that would correlate to the most evident behavior modes for a specific\nsubject. Activity recipes are then generated for good sleep quality for each\nbehavior mode within each cluster. These activity recipes are supplied to an\nactivity recommendation engine for suggesting a mix of relaxed to intense\nactivities to subjects during their daily routines. The recommendations are\nfurther personalized based on the subjects' lifestyle constraints, i.e. their\nage, gender, body mass index (BMI), resting heart rate, etc, with the objective\nof the recommendation being the improvement of that night's quality of sleep.\nThis would in turn serve a longer-term health objective, like lowering heart\nrate, improving the overall quality of sleep, etc.\n""]"
21,13,21_classification_supervised_classifications_classifier,"['classification', 'supervised', 'classifications', 'classifier', 'labeled', 'labels', 'classified', 'label', 'learning', 'unlabeled']","['label', 'labels', 'hierarchical', 'semi', 'supervised', 'data', 'loss', 'information', 'classes', 'learning']","['  Classification is a major tool of statistics and machine learning. A\nclassification method first processes a training set of objects with given\nclasses (labels), with the goal of afterward assigning new objects to one of\nthese classes. When running the resulting prediction method on the training\ndata or on test data, it can happen that an object is predicted to lie in a\nclass that differs from its given label. This is sometimes called label bias,\nand raises the question whether the object was mislabeled. The proposed class\nmap reflects the probability that an object belongs to an alternative class,\nhow far it is from the other objects in its given class, and whether some\nobjects lie far from all classes. The goal is to visualize aspects of the\nclassification results to obtain insight in the data. The display is\nconstructed for discriminant analysis, the k-nearest neighbor classifier,\nsupport vector machines, logistic regression, and coupling pairwise\nclassifications. It is illustrated on several benchmark datasets, including\nsome about images and texts.\n', '  In semi-supervised learning, information from unlabeled examples is used to\nimprove the model learned from labeled examples. In some learning problems,\npartial label information can be inferred from otherwise unlabeled examples and\nused to further improve the model. In particular, partial label information\nexists when subsets of training examples are known to have the same label, even\nthough the label itself is missing. By encouraging the model to give the same\nlabel to all such examples through contrastive learning objectives, we can\npotentially improve its performance. We call this encouragement Nullspace\nTuning because the difference vector between any pair of examples with the same\nlabel should lie in the nullspace of a linear model. In this paper, we\ninvestigate the benefit of using partial label information using a careful\ncomparison framework over well-characterized public datasets. We show that the\nadditional information provided by partial labels reduces test error over good\nsemi-supervised methods usually by a factor of 2, up to a factor of 5.5 in the\nbest case. We also show that adding Nullspace Tuning to the newer and\nstate-of-the-art MixMatch method decreases its test error by up to a factor of\n1.8.\n', '  Semi-supervised learning is a model training method that uses both labeled\nand unlabeled data. This paper proposes a fully Bayes semi-supervised learning\nalgorithm that can be applied to any multi-category classification problem. We\nassume the labels are missing at random when using unlabeled data in a\nsemi-supervised setting. Suppose we have $K$ classes in the data. We assume\nthat the observations follow $K$ multivariate normal distributions depending on\ntheir true class labels after some common unknown transformation is applied to\neach component of the observation vector. The function is expanded in a\nB-splines series, and a prior is added to the coefficients. We consider a\nnormal prior on the coefficients and constrain the values to meet the normality\nand identifiability constraints requirement. The precision matrices of the\nGaussian distributions are given a conjugate Wishart prior, while the means are\ngiven the improper uniform prior. The resulting posterior is still\nconditionally conjugate, and the Gibbs sampler aided by a data-augmentation\ntechnique can thus be adopted. An extensive simulation study compares the\nproposed method with several other available methods. The proposed method is\nalso applied to real datasets on diagnosing breast cancer and classification of\nsignals. We conclude that the proposed method has a better prediction accuracy\nin various cases.\n']"
22,13,22_robotic_articulated_grasp_robot,"['robotic', 'articulated', 'grasp', 'robot', 'manipulators', 'robots', 'objects', 'motions', 'movements', 'motion']","['object', 'manipulation', 'robot', 'objects', 'grasp', 'motion', 'intent', 'dynamic', 'control', 'apex']","['  Pushing is an essential non-prehensile manipulation skill used for tasks\nranging from pre-grasp manipulation to scene rearrangement, reasoning about\nobject relations in the scene, and thus pushing actions have been widely\nstudied in robotics. The effective use of pushing actions often requires an\nunderstanding of the dynamics of the manipulated objects and adaptation to the\ndiscrepancies between prediction and reality. For this reason, effect\nprediction and parameter estimation with pushing actions have been heavily\ninvestigated in the literature. However, current approaches are limited because\nthey either model systems with a fixed number of objects or use image-based\nrepresentations whose outputs are not very interpretable and quickly accumulate\nerrors. In this paper, we propose a graph neural network based framework for\neffect prediction and parameter estimation of pushing actions by modeling\nobject relations based on contacts or articulations. Our framework is validated\nboth in real and simulated environments containing different shaped multi-part\nobjects connected via different types of joints and objects with different\nmasses, and it outperforms image-based representations on physics prediction.\nOur approach enables the robot to predict and adapt the effect of a pushing\naction as it observes the scene. It can also be used for tool manipulation with\nnever-seen tools. Further, we demonstrate 6D effect prediction in the lever-up\naction in the context of robot-based hard-disk disassembly.\n', '  A kitchen assistant needs to operate human-scale objects, such as cabinets\nand ovens, in unmapped environments with dynamic obstacles. Autonomous\ninteractions in such environments require integrating dexterous manipulation\nand fluid mobility. While mobile manipulators in different form factors provide\nan extended workspace, their real-world adoption has been limited. Executing a\nhigh-level task for general objects requires a perceptual understanding of the\nobject as well as adaptive whole-body control among dynamic obstacles. In this\npaper, we propose a two-stage architecture for autonomous interaction with\nlarge articulated objects in unknown environments. The first stage,\nobject-centric planner, only focuses on the object to provide an\naction-conditional sequence of states for manipulation using RGB-D data. The\nsecond stage, agent-centric planner, formulates the whole-body motion control\nas an optimal control problem that ensures safe tracking of the generated plan,\neven in scenes with moving obstacles. We show that the proposed pipeline can\nhandle complex static and dynamic kitchen settings for both wheel-based and\nlegged mobile manipulators. Compared to other agent-centric planners, our\nproposed planner achieves a higher success rate and a lower execution time. We\nalso perform hardware tests on a legged mobile manipulator to interact with\nvarious articulated objects in a kitchen. For additional material, please\ncheck: www.pair.toronto.edu/articulated-mm/.\n', ""  This paper presents INVIGORATE, a robot system that interacts with human\nthrough natural language and grasps a specified object in clutter. The objects\nmay occlude, obstruct, or even stack on top of one another. INVIGORATE embodies\nseveral challenges: (i) infer the target object among other occluding objects,\nfrom input language expressions and RGB images, (ii) infer object blocking\nrelationships (OBRs) from the images, and (iii) synthesize a multi-step plan to\nask questions that disambiguate the target object and to grasp it successfully.\nWe train separate neural networks for object detection, for visual grounding,\nfor question generation, and for OBR detection and grasping. They allow for\nunrestricted object categories and language expressions, subject to the\ntraining datasets. However, errors in visual perception and ambiguity in human\nlanguages are inevitable and negatively impact the robot's performance. To\novercome these uncertainties, we build a partially observable Markov decision\nprocess (POMDP) that integrates the learned neural network modules. Through\napproximate POMDP planning, the robot tracks the history of observations and\nasks disambiguation questions in order to achieve a near-optimal sequence of\nactions that identify and grasp the target object. INVIGORATE combines the\nbenefits of model-based POMDP planning and data-driven deep learning.\nPreliminary experiments with INVIGORATE on a Fetch robot show significant\nbenefits of this integrated approach to object grasping in clutter with natural\nlanguage interactions. A demonstration video is available at\nhttps://youtu.be/zYakh80SGcU.\n""]"
