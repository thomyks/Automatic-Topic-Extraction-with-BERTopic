Unnamed: 0,Topic,Count,Name,Representation,Aspect1,Representative_Docs,Human_Readable_Topic,Higher_Topic_Label,Highest_Topic_Label
0,0,222,0_hypergraph_subgraph_graphs_hypergraphs,"['hypergraph', 'subgraph', 'graphs', 'hypergraphs', 'graph', 'adjacency', 'networks', 'nodes', 'vertices', 'embeddings']","['graph', 'node', 'graphs', 'nodes', 'hypergraph', 'clustering', 'structural', 'hypergraphs', 'structure', 'representation']","['  Graph-based clustering plays an important role in the clustering area. Recent\nstudies about graph convolution neural networks have achieved impressive\nsuccess on graph type data. However, in general clustering tasks, the graph\nstructure of data does not exist such that the strategy to construct a graph is\ncrucial for performance. Therefore, how to extend graph convolution networks\ninto general clustering tasks is an attractive problem. In this paper, we\npropose a graph auto-encoder for general data clustering, which constructs the\ngraph adaptively according to the generative perspective of graphs. The\nadaptive process is designed to induce the model to exploit the high-level\ninformation behind data and utilize the non-Euclidean structure sufficiently.\nWe further design a novel mechanism with rigorous analysis to avoid the\ncollapse caused by the adaptive construction. Via combining the generative\nmodel for network embedding and graph-based clustering, a graph auto-encoder\nwith a novel decoder is developed such that it performs well in weighted graph\nused scenarios. Extensive experiments prove the superiority of our model.\n', '  Clustering holds profound significance in data mining. In recent years, graph\nconvolutional network (GCN) has emerged as a powerful tool for deep clustering,\nintegrating both graph structural information and node attributes. However,\nmost existing methods ignore the higher-order structural information of the\ngraph. Evidently, nodes within the same cluster can establish distant\nconnections. Besides, recent deep clustering methods usually apply a\nself-supervised module to monitor the training process of their model, focusing\nsolely on node attributes without paying attention to graph structure. In this\npaper, we propose a novel graph clustering network to make full use of graph\nstructural information. To capture the higher-order structural information, we\ndesign a graph mutual infomax module, effectively maximizing mutual information\nbetween graph-level and node-level representations, and employ a trinary\nself-supervised module that includes modularity as a structural constraint. Our\nproposed model outperforms many state-of-the-art methods on various datasets,\ndemonstrating its superiority.\n', '  Hypergraphs, with their capacity to depict high-order relationships, have\nemerged as a significant extension of traditional graphs. Although Graph Neural\nNetworks (GNNs) have remarkable performance in graph representation learning,\ntheir extension to hypergraphs encounters challenges due to their intricate\nstructures. Furthermore, current hypergraph transformers, a special variant of\nGNN, utilize semantic feature-based self-attention, ignoring topological\nattributes of nodes and hyperedges. To address these challenges, we propose a\nTopology-guided Hypergraph Transformer Network (THTN). In this model, we first\nformulate a hypergraph from a graph while retaining its structural essence to\nlearn higher-order relations within the graph. Then, we design a simple yet\neffective structural and spatial encoding module to incorporate the topological\nand spatial information of the nodes into their representation. Further, we\npresent a structure-aware self-attention mechanism that discovers the important\nnodes and hyperedges from both semantic and structural viewpoints. By\nleveraging these two modules, THTN crafts an improved node representation,\ncapturing both local and global topological expressions. Extensive experiments\nconducted on node classification tasks demonstrate that the performance of the\nproposed model consistently exceeds that of the existing approaches.\n']",Graph Clustering and Representation Learning,Graph Neural Networks and Deep Learning on Graph-Structured Data,Graph Representation Learning and Neural Networks
1,1,198,1_federated_privacy_learning_datasets,"['federated', 'privacy', 'learning', 'datasets', 'collaborative', 'distributed', 'trained', 'sharing', 'fedllm', 'collaboratively']","['clients', 'client', 'privacy', 'local', 'heterogeneity', 'communication', 'global', 'server', 'data', 'tuning']","[""  Federated learning, a distributed learning paradigm, utilizes multiple\nclients to build a robust global model. In real-world applications, local\nclients often operate within their limited domains, leading to a `domain shift'\nacross clients. Privacy concerns limit each client's learning to its own domain\ndata, which increase the risk of overfitting. Moreover, the process of\naggregating models trained on own limited domain can be potentially lead to a\nsignificant degradation in the global model performance. To deal with these\nchallenges, we introduce the concept of federated feature diversification. Each\nclient diversifies the own limited domain data by leveraging global feature\nstatistics, i.e., the aggregated average statistics over all participating\nclients, shared through the global model's parameters. This data\ndiversification helps local models to learn client-invariant representations\nwhile preserving privacy. Our resultant global model shows robust performance\non unseen test domain data. To enhance performance further, we develop an\ninstance-adaptive inference approach tailored for test domain data. Our\nproposed instance feature adapter dynamically adjusts feature statistics to\nalign with the test input, thereby reducing the domain gap between the test and\ntraining domains. We show that our method achieves state-of-the-art performance\non several domain generalization benchmarks within a federated learning\nsetting.\n"", ""  Random forests are considered a cornerstone in machine learning for their\nrobustness and versatility. Despite these strengths, their conventional\ncentralized training is ill-suited for the modern landscape of data that is\noften distributed, sensitive, and subject to privacy concerns. Federated\nlearning (FL) provides a compelling solution to this problem, enabling models\nto be trained across a group of clients while maintaining the privacy of each\nclient's data. However, adapting tree-based methods like random forests to\nfederated settings introduces significant challenges, particularly when it\ncomes to non-identically distributed (non-IID) data across clients, which is a\ncommon scenario in real-world applications. This paper presents a federated\nrandom forest approach that employs a novel ensemble construction method aimed\nat improving performance under non-IID data. Instead of growing trees\nindependently in each client, our approach ensures each decision tree in the\nensemble is iteratively and collectively grown across clients. To preserve the\nprivacy of the client's data, we confine the information stored in the leaf\nnodes to the majority class label identified from the samples of the client's\nlocal data that reach each node. This limited disclosure preserves the\nconfidentiality of the underlying data distribution of clients, thereby\nenhancing the privacy of the federated learning process. Furthermore, our\ncollaborative ensemble construction strategy allows the ensemble to better\nreflect the data's heterogeneity across different clients, enhancing its\nperformance on non-IID data, as our experimental results confirm.\n"", '  Federated Learning (FL) is a recent model training paradigm in which client\ndevices collaboratively train a model without ever aggregating their data.\nCrucially, this scheme offers users potential privacy and security benefits by\nonly ever communicating updates to the model weights to a central server as\nopposed to traditional machine learning (ML) training which directly\ncommunicates and aggregates data. However, FL training suffers from statistical\nheterogeneity as clients may have differing local data distributions. Large\nlanguage models (LLMs) offer a potential solution to this issue of\nheterogeneity given that they have consistently been shown to be able to learn\non vast amounts of noisy data. While LLMs are a promising development for\nresolving the consistent issue of non-I.I.D. Clients in federated settings\nexacerbate two other bottlenecks in FL: limited local computing and expensive\ncommunication. This thesis aims to develop efficient training methods for LLMs\nin FL. To this end, we employ two critical techniques in enabling efficient\ntraining. First, we use low-rank adaptation (LoRA) to reduce the computational\nload of local model training. Second, we communicate sparse updates throughout\ntraining to significantly cut down on communication costs. Taken together, our\nmethod reduces communication costs by up to 10x over vanilla LoRA and up to 5x\nover more complex sparse LoRA baselines while achieving greater utility. We\nemphasize the importance of carefully applying sparsity and picking effective\nrank and sparsity configurations for federated LLM training.\n']",Federated Learning for Distributed Data Privacy,Federated Learning for Privacy-Preserving Machine Learning,Machine Learning and Data Privacy
2,2,196,2_graphs_subgraph_nodes_networks,"['graphs', 'subgraph', 'nodes', 'networks', 'graph', 'subgraphs', 'supervised', 'node', 'learning', 'labeled']","['graph', 'node', 'graphs', 'nodes', 'subgraph', 'subgraphs', 'classification', 'networks', 'distribution', 'label']","['  In this work, we study semi-supervised multi-label node classification\nproblem in attributed graphs. Classic solutions to multi-label node\nclassification follow two steps, first learn node embedding and then build a\nnode classifier on the learned embedding. To improve the discriminating power\nof the node embedding, we propose a novel collaborative graph walk, named\nMulti-Label-Graph-Walk, to finely tune node representations with the available\nlabel assignments in attributed graphs via reinforcement learning. The proposed\nmethod formulates the multi-label node classification task as simultaneous\ngraph walks conducted by multiple label-specific agents. Furthermore, policies\nof the label-wise graph walks are learned in a cooperative way to capture first\nthe predictive relation between node labels and structural attributes of\ngraphs; and second, the correlation among the multiple label-specific\nclassification tasks. A comprehensive experimental study demonstrates that the\nproposed method can achieve significantly better multi-label classification\nperformance than the state-of-the-art approaches and conduct more efficient\ngraph exploration.\n', '  Graph learning plays a pivotal role and has gained significant attention in\nvarious application scenarios, from social network analysis to recommendation\nsystems, for its effectiveness in modeling complex data relations represented\nby graph structural data. In reality, the real-world graph data typically show\ndynamics over time, with changing node attributes and edge structure, leading\nto the severe graph data distribution shift issue. This issue is compounded by\nthe diverse and complex nature of distribution shifts, which can significantly\nimpact the performance of graph learning methods in degraded generalization and\nadaptation capabilities, posing a substantial challenge to their effectiveness.\nIn this survey, we provide a comprehensive review and summary of the latest\napproaches, strategies, and insights that address distribution shifts within\nthe context of graph learning. Concretely, according to the observability of\ndistributions in the inference stage and the availability of sufficient\nsupervision information in the training stage, we categorize existing graph\nlearning methods into several essential scenarios, including graph domain\nadaptation learning, graph out-of-distribution learning, and graph continual\nlearning. For each scenario, a detailed taxonomy is proposed, with specific\ndescriptions and discussions of existing progress made in distribution-shifted\ngraph learning. Additionally, we discuss the potential applications and future\ndirections for graph learning under distribution shifts with a systematic\nanalysis of the current state in this field. The survey is positioned to\nprovide general guidance for the development of effective graph learning\nalgorithms in handling graph distribution shifts, and to stimulate future\nresearch and advancements in this area.\n', ""  Graph learning has become indispensable for interpreting and harnessing\nrelational data in diverse fields, ranging from recommendation systems to\nsocial network analysis. In this context, a variety of GNNs have emerged as\npromising methodologies for encoding the structural information of graphs. By\neffectively capturing the graph's underlying structure, these GNNs have shown\ngreat potential in enhancing performance in graph learning tasks, such as link\nprediction and node classification. However, despite their successes, a\nsignificant challenge persists: these advanced methods often face difficulties\nin generalizing to unseen graph data that significantly differs from the\ntraining instances. In this work, our aim is to advance the graph learning\nparadigm by developing a general graph foundation model. This model is designed\nto understand the complex topological patterns present in diverse graph data,\nenabling it to excel in zero-shot graph learning tasks across different\ndownstream datasets. To achieve this goal, we address several key technical\nchallenges in our OpenGraph model. Firstly, we propose a unified graph\ntokenizer to adapt our graph model to generalize well on unseen graph data,\neven when the underlying graph properties differ significantly from those\nencountered during training. Secondly, we develop a scalable graph transformer\nas the foundational encoder, which effectively captures node-wise dependencies\nwithin the global topological context. Thirdly, we introduce a data\naugmentation mechanism enhanced by a LLM to alleviate the limitations of data\nscarcity in real-world scenarios. Extensive experiments validate the\neffectiveness of our framework. By adapting our OpenGraph to new graph\ncharacteristics and comprehending the nuances of diverse graphs, our approach\nachieves remarkable zero-shot graph learning performance across various\nsettings and domains.\n""]",Graph Learning and Node Classification,Graph Neural Networks and Deep Learning on Graph-Structured Data,Graph Representation Learning and Neural Networks
3,3,178,3_captioning_multimodal_embeddings_recognition,"['captioning', 'multimodal', 'embeddings', 'recognition', 'captions', 'encoder', 'embedding', 'retrieval', 'visual', 'text']","['visual', 'image', 'text', 'vision', 'modal', 'captions', 'images', 'object', 'textual', 'caption']","['  Composed Image Retrieval (CIR) aims to retrieve a target image based on a\nreference image and conditioning text, enabling controllable searches. Due to\nthe expensive dataset construction cost for CIR triplets, a zero-shot (ZS) CIR\nsetting has been actively studied to eliminate the need for human-collected\ntriplet datasets. The mainstream of ZS-CIR employs an efficient projection\nmodule that projects a CLIP image embedding to the CLIP text token embedding\nspace, while fixing the CLIP encoders. Using the projected image embedding,\nthese methods generate image-text composed features by using the pre-trained\ntext encoder. However, their CLIP image and text encoders suffer from the task\ndiscrepancy between the pre-training task (text $\\leftrightarrow$ image) and\nthe target CIR task (image + text $\\leftrightarrow$ image). Conceptually, we\nneed expensive triplet samples to reduce the discrepancy, but we use cheap text\ntriplets instead and update the text encoder. To that end, we introduce the\nReducing Task Discrepancy of text encoders for Composed Image Retrieval (RTD),\na plug-and-play training scheme for the text encoder that enhances its\ncapability using a novel target-anchored text contrastive learning. We also\npropose two additional techniques to improve the proposed learning scheme: a\nhard negatives-based refined batch sampling strategy and a sophisticated\nconcatenation scheme. Integrating RTD into the state-of-the-art\nprojection-based ZS-CIR methods significantly improves performance across\nvarious datasets and backbones, demonstrating its efficiency and\ngeneralizability.\n', ""  CLIP models perform remarkably well on zero-shot classification and retrieval\ntasks. But recent studies have shown that learnt representations in CLIP are\nnot well suited for dense prediction tasks like object detection, semantic\nsegmentation or depth estimation. More recently, multi-stage training methods\nfor CLIP models was introduced to mitigate the weak performance of CLIP on\ndownstream tasks. In this work, we find that simply improving the quality of\ncaptions in image-text datasets improves the quality of CLIP's visual\nrepresentations, resulting in significant improvement on downstream dense\nprediction vision tasks. In fact, we find that CLIP pretraining with good\nquality captions can surpass recent supervised, self-supervised and weakly\nsupervised pretraining methods. We show that when CLIP model with ViT-B/16 as\nimage encoder is trained on well aligned image-text pairs it obtains 12.1%\nhigher mIoU and 11.5% lower RMSE on semantic segmentation and depth estimation\ntasks over recent state-of-the-art Masked Image Modeling (MIM) pretraining\nmethods like Masked Autoencoder (MAE). We find that mobile architectures also\nbenefit significantly from CLIP pretraining. A recent mobile vision\narchitecture, MCi2, with CLIP pretraining obtains similar performance as\nSwin-L, pretrained on ImageNet-22k for semantic segmentation task while being\n6.1$\\times$ smaller. Moreover, we show that improving caption quality results\nin $10\\times$ data efficiency when finetuning for dense prediction tasks.\n"", '  Treating texts as images, combining prompts with textual labels for prompt\ntuning, and leveraging the alignment properties of CLIP have been successfully\napplied in zero-shot multi-label image recognition. Nonetheless, relying solely\non textual labels to store visual information is insufficient for representing\nthe diversity of visual objects. In this paper, we propose reversing the\ntraining process of CLIP and introducing the concept of Pseudo Visual Prompts.\nThese prompts are initialized for each object category and pre-trained on\nlarge-scale, low-cost sentence data generated by large language models. This\nprocess mines the aligned visual information in CLIP and stores it in\nclass-specific visual prompts. We then employ contrastive learning to transfer\nthe stored visual information to the textual labels, enhancing their visual\nrepresentation capacity. Additionally, we introduce a dual-adapter module that\nsimultaneously leverages knowledge from the original CLIP and new learning\nknowledge derived from downstream datasets. Benefiting from the pseudo visual\nprompts, our method surpasses the state-of-the-art not only on clean annotated\ntext data but also on pseudo text data generated by large language models.\n']",Multimodal Image Retrieval and Captioning,Multimodal Learning and Applications,Multimodal Learning and Applications
4,4,165,4_learns_reinforcement_robotic_robotics,"['learns', 'reinforcement', 'robotic', 'robotics', 'robot', 'learning', 'robots', 'exploration', 'learned', 'imitation']","['robot', 'demonstrations', 'reinforcement', 'robotic', 'policy', 'imitation', 'skills', 'policies', 'skill', 'environment']","['  Imitation learning (IL), aiming to learn optimal control policies from expert\ndemonstrations, has been an effective method for robot manipulation tasks.\nHowever, previous IL methods either only use expensive expert demonstrations\nand omit imperfect demonstrations or rely on interacting with the environment\nand learning from online experiences. In the context of robotic manipulation,\nwe aim to conquer the above two challenges and propose a novel framework named\nSimilarity Weighted Behavior Transformer (SWBT). SWBT effectively learn from\nboth expert and imperfect demonstrations without interaction with environments.\nWe reveal that the easy-to-get imperfect demonstrations, such as forward and\ninverse dynamics, significantly enhance the network by learning fruitful\ninformation. To the best of our knowledge, we are the first to attempt to\nintegrate imperfect demonstrations into the offline imitation learning setting\nfor robot manipulation tasks. Extensive experiments on the ManiSkill2 benchmark\nbuilt on the high-fidelity Sapien simulator and real-world robotic manipulation\ntasks demonstrated that the proposed method can extract better features and\nimprove the success rates for all tasks. Our code will be released upon\nacceptance of the paper.\n', ""  Reinforcement Learning (RL) has achieved great success in sequential\ndecision-making problems, but often at the cost of a large number of\nagent-environment interactions. To improve sample efficiency, methods like\nReinforcement Learning from Expert Demonstrations (RLED) introduce external\nexpert demonstrations to facilitate agent exploration during the learning\nprocess. In practice, these demonstrations, which are often collected from\nhuman users, are costly and hence often constrained to a limited amount. How to\nselect the best set of human demonstrations that is most beneficial for\nlearning therefore becomes a major concern. This paper presents EARLY (Episodic\nActive Learning from demonstration querY), an algorithm that enables a learning\nagent to generate optimized queries of expert demonstrations in a\ntrajectory-based feature space. Based on a trajectory-level estimate of\nuncertainty in the agent's current policy, EARLY determines the optimized\ntiming and content for feature-based queries. By querying episodic\ndemonstrations as opposed to isolated state-action pairs, EARLY improves the\nhuman teaching experience and achieves better learning performance. We validate\nthe effectiveness of our method in three simulated navigation tasks of\nincreasing difficulty. The results show that our method is able to achieve\nexpert-level performance for all three tasks with convergence over 30\\% faster\nthan other baseline methods when demonstrations are generated by simulated\noracle policies. The results of a follow-up pilot user study (N=18) further\nvalidate that our method can still maintain a significantly better convergence\nin the case of human expert demonstrators while achieving a better user\nexperience in perceived task load and consuming significantly less human time.\n"", '  Most reinforcement learning (RL) methods focus on learning optimal policies\nover low-level action spaces. While these methods can perform well in their\ntraining environments, they lack the flexibility to transfer to new tasks.\nInstead, RL agents that can act over useful, temporally extended skills rather\nthan low-level actions can learn new tasks more easily. Prior work in\nskill-based RL either requires expert supervision to define useful skills,\nwhich is hard to scale, or learns a skill-space from offline data with\nheuristics that limit the adaptability of the skills, making them difficult to\ntransfer during downstream RL. Our approach, EXTRACT, instead utilizes\npre-trained vision language models to extract a discrete set of semantically\nmeaningful skills from offline data, each of which is parameterized by\ncontinuous arguments, without human supervision. This skill parameterization\nallows robots to learn new tasks by only needing to learn when to select a\nspecific skill and how to modify its arguments for the specific task. We\ndemonstrate through experiments in sparse-reward, image-based, robot\nmanipulation environments that EXTRACT can more quickly learn new tasks than\nprior works, with major gains in sample efficiency and performance over prior\nskill-based RL. Website at https://www.jessezhang.net/projects/extract/.\n']",Robotics and Imitation Learning,Imitation Learning and Robotics,Robotics and Artificial Intelligence
5,5,164,5_mri_imaging_neuroimaging_tomography,"['mri', 'imaging', 'neuroimaging', 'tomography', 'fmri', 'segmentation', 'dataset', 'deep', 'images', 'brain']","['brain', 'imaging', 'segmentation', 'cardiac', 'medical', 'images', 'scans', 'fetal', 'clinical', 'diagnosis']","['  Computed tomography (CT) segmentation models frequently include classes that\nare not currently supported by magnetic resonance imaging (MRI) segmentation\nmodels. In this study, we show that a simple image inversion technique can\nsignificantly improve the segmentation quality of CT segmentation models on MRI\ndata, by using the TotalSegmentator model, applied to T1-weighted MRI images,\nas example. Image inversion is straightforward to implement and does not\nrequire dedicated graphics processing units (GPUs), thus providing a quick\nalternative to complex deep modality-transfer models for generating\nsegmentation masks for MRI data.\n', '  Cardiac magnetic resonance imaging (MRI) has emerged as a clinically\ngold-standard technique for diagnosing cardiac diseases, thanks to its ability\nto provide diverse information with multiple modalities and anatomical views.\nAccelerated cardiac MRI is highly expected to achieve time-efficient and\npatient-friendly imaging, and then advanced image reconstruction approaches are\nrequired to recover high-quality, clinically interpretable images from\nundersampled measurements. However, the lack of publicly available cardiac MRI\nk-space dataset in terms of both quantity and diversity has severely hindered\nsubstantial technological progress, particularly for data-driven artificial\nintelligence. Here, we provide a standardized, diverse, and high-quality\nCMRxRecon2024 dataset to facilitate the technical development, fair evaluation,\nand clinical transfer of cardiac MRI reconstruction approaches, towards\npromoting the universal frameworks that enable fast and robust reconstructions\nacross different cardiac MRI protocols in clinical practice. To the best of our\nknowledge, the CMRxRecon2024 dataset is the largest and most diverse publicly\navailable cardiac k-space dataset. It is acquired from 330 healthy volunteers,\ncovering commonly used modalities, anatomical views, and acquisition\ntrajectories in clinical cardiac MRI workflows. Besides, an open platform with\ntutorials, benchmarks, and data processing tools is provided to facilitate data\nusage, advanced method development, and fair performance evaluation.\n', ""  Brain disorders are a major challenge to global health, causing millions of\ndeaths each year. Accurate diagnosis of these diseases relies heavily on\nadvanced medical imaging techniques such as Magnetic Resonance Imaging (MRI)\nand Computed Tomography (CT). However, the scarcity of annotated data poses a\nsignificant challenge in deploying machine learning models for medical\ndiagnosis. To address this limitation, deep learning techniques have shown\nconsiderable promise. Domain adaptation techniques enhance a model's ability to\ngeneralize across imaging modalities by transferring knowledge from one domain\n(e.g., CT images) to another (e.g., MRI images). Such cross-modality adaptation\nis essential to improve the ability of models to consistently generalize across\ndifferent imaging modalities. This study collected relevant resources from the\nKaggle website and employed the Maximum Mean Difference (MMD) method - a\npopular domain adaptation method - to reduce the differences between imaging\ndomains. By combining MMD with Convolutional Neural Networks (CNNs), the\naccuracy and utility of the model is obviously enhanced. The excellent\nexperimental results highlight the great potential of data-driven domain\nadaptation techniques to improve diagnostic accuracy and efficiency, especially\nin resource-limited environments. By bridging the gap between different imaging\nmodalities, the study aims to provide clinicians with more reliable diagnostic\ntools.\n""]",Medical Imaging Segmentation and Analysis,Medical Image Analysis,Medical Imaging and Reporting
6,6,160,6_attention_rnns_memory_nlp,"['attention', 'rnns', 'memory', 'nlp', 'learn', 'recurrent', 'examples', 'context', 'language', 'transformers']","['transformers', 'transformer', 'context', 'tokens', 'attention', 'language', 'layers', 'hidden', 'sequence', 'input']","[""  Recently, the mysterious In-Context Learning (ICL) ability exhibited by\nTransformer architectures, especially in large language models (LLMs), has\nsparked significant research interest. However, the resilience of Transformers'\nin-context learning capabilities in the presence of noisy samples, prevalent in\nboth training corpora and prompt demonstrations, remains underexplored. In this\npaper, inspired by prior research that studies ICL ability using simple\nfunction classes, we take a closer look at this problem by investigating the\nrobustness of Transformers against noisy labels. Specifically, we first conduct\na thorough evaluation and analysis of the robustness of Transformers against\nnoisy labels during in-context learning and show that they exhibit notable\nresilience against diverse types of noise in demonstration labels. Furthermore,\nwe delve deeper into this problem by exploring whether introducing noise into\nthe training set, akin to a form of data augmentation, enhances such robustness\nduring inference, and find that such noise can indeed improve the robustness of\nICL. Overall, our fruitful analysis and findings provide a comprehensive\nunderstanding of the resilience of Transformer models against label noises\nduring ICL and provide valuable insights into the research on Transformers in\nnatural language processing. Our code is available at\nhttps://github.com/InezYu0928/in-context-learning.\n"", '  Large language models (LLM) have emerged as a powerful tool for AI, with the\nkey ability of in-context learning (ICL), where they can perform well on unseen\ntasks based on a brief series of task examples without necessitating any\nadjustments to the model parameters. One recent interesting mysterious\nobservation is that models of different scales may have different ICL\nbehaviors: larger models tend to be more sensitive to noise in the test\ncontext. This work studies this observation theoretically aiming to improve the\nunderstanding of LLM and ICL. We analyze two stylized settings: (1) linear\nregression with one-layer single-head linear transformers and (2) parity\nclassification with two-layer multiple attention heads transformers (non-linear\ndata and non-linear model). In both settings, we give closed-form optimal\nsolutions and find that smaller models emphasize important hidden features\nwhile larger ones cover more hidden features; thus, smaller models are more\nrobust to noise while larger ones are more easily distracted, leading to\ndifferent ICL behaviors. This sheds light on where transformers pay attention\nto and how that affects ICL. Preliminary experimental results on large base and\nchat models provide positive support for our analysis.\n', '  The incredible success of transformers on sequence modeling tasks can be\nlargely attributed to the self-attention mechanism, which allows information to\nbe transferred between different parts of a sequence. Self-attention allows\ntransformers to encode causal structure which makes them particularly suitable\nfor sequence modeling. However, the process by which transformers learn such\ncausal structure via gradient-based training algorithms remains poorly\nunderstood. To better understand this process, we introduce an in-context\nlearning task that requires learning latent causal structure. We prove that\ngradient descent on a simplified two-layer transformer learns to solve this\ntask by encoding the latent causal graph in the first attention layer. The key\ninsight of our proof is that the gradient of the attention matrix encodes the\nmutual information between tokens. As a consequence of the data processing\ninequality, the largest entries of this gradient correspond to edges in the\nlatent causal graph. As a special case, when the sequences are generated from\nin-context Markov chains, we prove that transformers learn an induction head\n(Olsson et al., 2022). We confirm our theoretical findings by showing that\ntransformers trained on our in-context learning task are able to recover a wide\nvariety of causal structures.\n']",In-Context Learning with Transformers,In-Context Learning with Transformers,Advances in Sequence Modeling and Learning
6,6,160,6_attention_rnns_memory_nlp,"['attention', 'rnns', 'memory', 'nlp', 'learn', 'recurrent', 'examples', 'context', 'language', 'transformers']","['transformers', 'transformer', 'context', 'tokens', 'attention', 'language', 'layers', 'hidden', 'sequence', 'input']","[""  Recently, the mysterious In-Context Learning (ICL) ability exhibited by\nTransformer architectures, especially in large language models (LLMs), has\nsparked significant research interest. However, the resilience of Transformers'\nin-context learning capabilities in the presence of noisy samples, prevalent in\nboth training corpora and prompt demonstrations, remains underexplored. In this\npaper, inspired by prior research that studies ICL ability using simple\nfunction classes, we take a closer look at this problem by investigating the\nrobustness of Transformers against noisy labels. Specifically, we first conduct\na thorough evaluation and analysis of the robustness of Transformers against\nnoisy labels during in-context learning and show that they exhibit notable\nresilience against diverse types of noise in demonstration labels. Furthermore,\nwe delve deeper into this problem by exploring whether introducing noise into\nthe training set, akin to a form of data augmentation, enhances such robustness\nduring inference, and find that such noise can indeed improve the robustness of\nICL. Overall, our fruitful analysis and findings provide a comprehensive\nunderstanding of the resilience of Transformer models against label noises\nduring ICL and provide valuable insights into the research on Transformers in\nnatural language processing. Our code is available at\nhttps://github.com/InezYu0928/in-context-learning.\n"", '  Large language models (LLM) have emerged as a powerful tool for AI, with the\nkey ability of in-context learning (ICL), where they can perform well on unseen\ntasks based on a brief series of task examples without necessitating any\nadjustments to the model parameters. One recent interesting mysterious\nobservation is that models of different scales may have different ICL\nbehaviors: larger models tend to be more sensitive to noise in the test\ncontext. This work studies this observation theoretically aiming to improve the\nunderstanding of LLM and ICL. We analyze two stylized settings: (1) linear\nregression with one-layer single-head linear transformers and (2) parity\nclassification with two-layer multiple attention heads transformers (non-linear\ndata and non-linear model). In both settings, we give closed-form optimal\nsolutions and find that smaller models emphasize important hidden features\nwhile larger ones cover more hidden features; thus, smaller models are more\nrobust to noise while larger ones are more easily distracted, leading to\ndifferent ICL behaviors. This sheds light on where transformers pay attention\nto and how that affects ICL. Preliminary experimental results on large base and\nchat models provide positive support for our analysis.\n', '  The incredible success of transformers on sequence modeling tasks can be\nlargely attributed to the self-attention mechanism, which allows information to\nbe transferred between different parts of a sequence. Self-attention allows\ntransformers to encode causal structure which makes them particularly suitable\nfor sequence modeling. However, the process by which transformers learn such\ncausal structure via gradient-based training algorithms remains poorly\nunderstood. To better understand this process, we introduce an in-context\nlearning task that requires learning latent causal structure. We prove that\ngradient descent on a simplified two-layer transformer learns to solve this\ntask by encoding the latent causal graph in the first attention layer. The key\ninsight of our proof is that the gradient of the attention matrix encodes the\nmutual information between tokens. As a consequence of the data processing\ninequality, the largest entries of this gradient correspond to edges in the\nlatent causal graph. As a special case, when the sequences are generated from\nin-context Markov chains, we prove that transformers learn an induction head\n(Olsson et al., 2022). We confirm our theoretical findings by showing that\ntransformers trained on our in-context learning task are able to recover a wide\nvariety of causal structures.\n']",In-Context Learning with Transformers,In-Context Learning with Transformers,Advances in Sequence Modeling and Learning
7,7,156,7_audio_microphone_encoder_recording,"['audio', 'microphone', 'encoder', 'recording', 'acoustic', 'supervised', 'acoustics', 'wavlm', 'recognition', 'auditory']","['audio', 'speech', 'acoustic', 'sound', 'speaker', 'recognition', 'supervised', 'self', 'spectrogram', 'transformer']","['  The goal of universal audio representation learning is to obtain foundational\nmodels that can be used for a variety of downstream tasks involving speech,\nmusic and environmental sounds. To approach this problem, methods inspired by\nworks on self-supervised learning for NLP, like BERT, or computer vision, like\nmasked autoencoders (MAE), are often adapted to the audio domain. In this work,\nwe propose masking representations of the audio signal, and training a MAE to\nreconstruct the masked segments. The reconstruction is done by predicting the\ndiscrete units generated by EnCodec, a neural audio codec, from the unmasked\ninputs. We evaluate this approach, which we call EnCodecMAE, on a wide range of\ntasks involving speech, music and environmental sounds. Our best model\noutperforms various state-of-the-art audio representation models in terms of\nglobal performance. Additionally, we evaluate the resulting representations in\nthe challenging task of automatic speech recognition (ASR), obtaining decent\nresults and paving the way for a universal audio representation.\n', '  Audio tagging is an important task of mapping audio samples to their\ncorresponding categories. Recently endeavours that exploit transformer models\nin this field have achieved great success. However, the quadratic\nself-attention cost limits the scaling of audio transformer models and further\nconstrains the development of more universal audio models. In this paper, we\nattempt to solve this problem by proposing Audio Mamba, a self-attention-free\napproach that captures long audio spectrogram dependency with state space\nmodels. Our experimental results on two audio-tagging datasets demonstrate the\nparameter efficiency of Audio Mamba, it achieves comparable results to SOTA\naudio spectrogram transformers with one third parameters.\n', '  Audio self-supervised learning (SSL) pre-training, which aims to learn good\nrepresentations from unlabeled audio, has made remarkable progress. However,\nthe extensive computational demands during pre-training pose a significant\nbarrier to the potential application and optimization of audio SSL models. In\nthis paper, inspired by the success of data2vec 2.0 in image modality and\nAudio-MAE in audio modality, we introduce Efficient Audio Transformer (EAT) to\nfurther improve the effectiveness and efficiency in audio SSL. The proposed EAT\nadopts the bootstrap self-supervised training paradigm to the audio domain. A\nnovel Utterance-Frame Objective (UFO) is designed to enhance the modeling\ncapability of acoustic events. Furthermore, we reveal that the masking strategy\nis critical in audio SSL pre-training, and superior audio representations can\nbe obtained with large inverse block masks. Experiment results demonstrate that\nEAT achieves state-of-the-art (SOTA) performance on a range of audio-related\ntasks, including AudioSet (AS-2M, AS-20K), ESC-50, and SPC-2, along with a\nsignificant pre-training speedup up to ~15x compared to existing audio SSL\nmodels.\n']",Audio Representation Learning,Audio and Speech Processing,Speech and Audio Processing
8,8,152,8_adversarial_malicious_vulnerabilities_attacker,"['adversarial', 'malicious', 'vulnerabilities', 'attacker', 'threats', 'vulnerability', 'security', 'attacks', 'attackers', 'vulnerable']","['attacks', 'attack', 'adversarial', 'defense', 'harmful', 'malicious', 'safety', 'security', 'vulnerabilities', 'prompts']","['  As Large Language Models (LLMs) increasingly become key components in various\nAI applications, understanding their security vulnerabilities and the\neffectiveness of defense mechanisms is crucial. This survey examines the\nsecurity challenges of LLMs, focusing on two main areas: Prompt Hacking and\nAdversarial Attacks, each with specific types of threats. Under Prompt Hacking,\nwe explore Prompt Injection and Jailbreaking Attacks, discussing how they work,\ntheir potential impacts, and ways to mitigate them. Similarly, we analyze\nAdversarial Attacks, breaking them down into Data Poisoning Attacks and\nBackdoor Attacks. This structured examination helps us understand the\nrelationships between these vulnerabilities and the defense strategies that can\nbe implemented. The survey highlights these security challenges and discusses\nrobust defensive frameworks to protect LLMs against these threats. By detailing\nthese security issues, the survey contributes to the broader discussion on\ncreating resilient AI systems that can resist sophisticated attacks.\n', '  Large Language Models (LLMs) have performed exceptionally in various\ntext-generative tasks, including question answering, translation, code\ncompletion, etc. However, the over-assistance of LLMs has raised the challenge\nof ""jailbreaking"", which induces the model to generate malicious responses\nagainst the usage policy and society by designing adversarial prompts. With the\nemergence of jailbreak attack methods exploiting different vulnerabilities in\nLLMs, the corresponding safety alignment measures are also evolving. In this\npaper, we propose a comprehensive and detailed taxonomy of jailbreak attack and\ndefense methods. For instance, the attack methods are divided into black-box\nand white-box attacks based on the transparency of the target model. Meanwhile,\nwe classify defense methods into prompt-level and model-level defenses.\nAdditionally, we further subdivide these attack and defense methods into\ndistinct sub-classes and present a coherent diagram illustrating their\nrelationships. We also conduct an investigation into the current evaluation\nmethods and compare them from different perspectives. Our findings aim to\ninspire future research and practical implementations in safeguarding LLMs\nagainst adversarial attacks. Above all, although jailbreak remains a\nsignificant concern within the community, we believe that our work enhances the\nunderstanding of this domain and provides a foundation for developing more\nsecure LLMs.\n', ""  Although safely enhanced Large Language Models (LLMs) have achieved\nremarkable success in tackling various complex tasks in a zero-shot manner,\nthey remain susceptible to jailbreak attacks, particularly the unknown\njailbreak attack. To enhance LLMs' generalized defense capabilities, we propose\na two-stage adversarial tuning framework, which generates adversarial prompts\nto explore worst-case scenarios by optimizing datasets containing pairs of\nadversarial prompts and their safe responses. In the first stage, we introduce\nthe hierarchical meta-universal adversarial prompt learning to efficiently and\neffectively generate token-level adversarial prompts. In the second stage, we\npropose the automatic adversarial prompt learning to iteratively refine\nsemantic-level adversarial prompts, further enhancing LLM's defense\ncapabilities. We conducted comprehensive experiments on three widely used\njailbreak datasets, comparing our framework with six defense baselines under\nfive representative attack scenarios. The results underscore the superiority of\nour proposed methods. Furthermore, our adversarial tuning framework exhibits\nempirical generalizability across various attack strategies and target LLMs,\nhighlighting its potential as a transferable defense mechanism.\n""]",Adversarial Attacks on Large Language Models,Adversarial Attacks and Defenses in AI and Machine Learning Models,Adversarial Attacks and Vulnerabilities in AI and Machine Learning
8,8,152,8_adversarial_malicious_vulnerabilities_attacker,"['adversarial', 'malicious', 'vulnerabilities', 'attacker', 'threats', 'vulnerability', 'security', 'attacks', 'attackers', 'vulnerable']","['attacks', 'attack', 'adversarial', 'defense', 'harmful', 'malicious', 'safety', 'security', 'vulnerabilities', 'prompts']","['  As Large Language Models (LLMs) increasingly become key components in various\nAI applications, understanding their security vulnerabilities and the\neffectiveness of defense mechanisms is crucial. This survey examines the\nsecurity challenges of LLMs, focusing on two main areas: Prompt Hacking and\nAdversarial Attacks, each with specific types of threats. Under Prompt Hacking,\nwe explore Prompt Injection and Jailbreaking Attacks, discussing how they work,\ntheir potential impacts, and ways to mitigate them. Similarly, we analyze\nAdversarial Attacks, breaking them down into Data Poisoning Attacks and\nBackdoor Attacks. This structured examination helps us understand the\nrelationships between these vulnerabilities and the defense strategies that can\nbe implemented. The survey highlights these security challenges and discusses\nrobust defensive frameworks to protect LLMs against these threats. By detailing\nthese security issues, the survey contributes to the broader discussion on\ncreating resilient AI systems that can resist sophisticated attacks.\n', '  Large Language Models (LLMs) have performed exceptionally in various\ntext-generative tasks, including question answering, translation, code\ncompletion, etc. However, the over-assistance of LLMs has raised the challenge\nof ""jailbreaking"", which induces the model to generate malicious responses\nagainst the usage policy and society by designing adversarial prompts. With the\nemergence of jailbreak attack methods exploiting different vulnerabilities in\nLLMs, the corresponding safety alignment measures are also evolving. In this\npaper, we propose a comprehensive and detailed taxonomy of jailbreak attack and\ndefense methods. For instance, the attack methods are divided into black-box\nand white-box attacks based on the transparency of the target model. Meanwhile,\nwe classify defense methods into prompt-level and model-level defenses.\nAdditionally, we further subdivide these attack and defense methods into\ndistinct sub-classes and present a coherent diagram illustrating their\nrelationships. We also conduct an investigation into the current evaluation\nmethods and compare them from different perspectives. Our findings aim to\ninspire future research and practical implementations in safeguarding LLMs\nagainst adversarial attacks. Above all, although jailbreak remains a\nsignificant concern within the community, we believe that our work enhances the\nunderstanding of this domain and provides a foundation for developing more\nsecure LLMs.\n', ""  Although safely enhanced Large Language Models (LLMs) have achieved\nremarkable success in tackling various complex tasks in a zero-shot manner,\nthey remain susceptible to jailbreak attacks, particularly the unknown\njailbreak attack. To enhance LLMs' generalized defense capabilities, we propose\na two-stage adversarial tuning framework, which generates adversarial prompts\nto explore worst-case scenarios by optimizing datasets containing pairs of\nadversarial prompts and their safe responses. In the first stage, we introduce\nthe hierarchical meta-universal adversarial prompt learning to efficiently and\neffectively generate token-level adversarial prompts. In the second stage, we\npropose the automatic adversarial prompt learning to iteratively refine\nsemantic-level adversarial prompts, further enhancing LLM's defense\ncapabilities. We conducted comprehensive experiments on three widely used\njailbreak datasets, comparing our framework with six defense baselines under\nfive representative attack scenarios. The results underscore the superiority of\nour proposed methods. Furthermore, our adversarial tuning framework exhibits\nempirical generalizability across various attack strategies and target LLMs,\nhighlighting its potential as a transferable defense mechanism.\n""]",Adversarial Attacks on Large Language Models,Adversarial Attacks and Defenses in AI and Machine Learning Models,Adversarial Attacks and Vulnerabilities in AI and Machine Learning
9,9,150,9_utterances_utterance_transcription_speech,"['utterances', 'utterance', 'transcription', 'speech', 'dialogue', 'voice', 'transcripts', 'multilingual', 'conversational', 'decoder']","['speech', 'spoken', 'audio', 'text', 'recognition', 'acoustic', 'translation', 'end', 'automatic', 'word']","['  In this work, we introduce a framework for speech summarization that\nleverages the processing and reasoning capabilities of large language models\n(LLMs). We propose an end-to-end system that combines an instruction-tuned LLM\nwith an audio encoder that converts speech into token representations that the\nLLM can interpret. Using a dataset with paired speech-text data, the overall\nsystem is trained to generate consistent responses to prompts with the same\nsemantic information regardless of the input modality. The resulting framework\nallows the LLM to process speech inputs in the same way as text, enabling\nspeech summarization by simply prompting the LLM. Unlike prior approaches, our\nmethod is able to summarize spoken content from any arbitrary domain, and it\ncan produce summaries in different styles by varying the LLM prompting\nstrategy. Experiments demonstrate that our approach outperforms a cascade\nbaseline of speech recognition followed by LLM text processing.\n', '  While recent work shows promising results in expanding the capabilities of\nlarge language models (LLM) to directly understand and synthesize speech, an\nLLM-based strategy for modeling spoken dialogs remains elusive and calls for\nfurther investigation. This work proposes an extensive speech-text LLM\nframework, named the Unified Spoken Dialog Model (USDM), to generate coherent\nspoken responses with organic prosodic features relevant to the given input\nspeech without relying on automatic speech recognition (ASR) or text-to-speech\n(TTS) solutions. Our approach employs a multi-step speech-text inference scheme\nthat leverages chain-of-reasoning capabilities exhibited by the underlying LLM.\nWe also propose a generalized speech-text pretraining scheme that helps with\ncapturing cross-modal semantics. Automatic and human evaluations show that the\nproposed approach is effective in generating natural-sounding spoken responses,\noutperforming both prior and cascaded baselines. Detailed comparative studies\nreveal that, despite the cascaded approach being stronger in individual\ncomponents, the joint speech-text modeling improves robustness against\nrecognition errors and speech quality. Demo is available at\nhttps://unifiedsdm.github.io.\n', '  The emergence of large language models (LLMs) has sparked significant\ninterest in extending their remarkable language capabilities to speech.\nHowever, modality alignment between speech and text still remains an open\nproblem. Current solutions can be categorized into two strategies. One is a\ncascaded approach where outputs (tokens or states) of a separately trained\nspeech recognition system are used as inputs for LLMs, which limits their\npotential in modeling alignment between speech and text. The other is an\nend-to-end approach that relies on speech instruction data, which is very\ndifficult to collect in large quantities. In this paper, we address these\nissues and propose the BLSP approach that Bootstraps Language-Speech\nPre-training via behavior alignment of continuation writing. We achieve this by\nlearning a lightweight modality adapter between a frozen speech encoder and an\nLLM, ensuring that the LLM exhibits the same generation behavior regardless of\nthe modality of input: a speech segment or its transcript. The training process\ncan be divided into two steps. The first step prompts an LLM to generate texts\nwith speech transcripts as prefixes, obtaining text continuations. In the\nsecond step, these continuations are used as supervised signals to train the\nmodality adapter in an end-to-end manner. We demonstrate that this\nstraightforward process can extend the capabilities of LLMs to speech, enabling\nspeech recognition, speech translation, spoken language understanding, and\nspeech conversation, even in zero-shot cross-lingual scenarios.\n']",Speech-to-Text Processing with Large Language Models,Speech and Language Processing,Speech and Audio Processing
10,10,136,10_reinforcement_reward_rewards_learning,"['reinforcement', 'reward', 'rewards', 'learning', 'agents', 'agent', 'learned', 'automaton', 'autonomous', 'tasks']","['reward', 'agent', 'reinforcement', 'agents', 'policies', 'rewards', 'environment', 'policy', 'learning', 'action']","['  Reinforcement Learning (RL) has made significant strides in enabling\nartificial agents to learn diverse behaviors. However, learning an effective\npolicy often requires a large number of environment interactions. To mitigate\nsample complexity issues, recent approaches have used high-level task\nspecifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward\nMachines (RM), to guide the learning progress of the agent. In this work, we\npropose a novel approach, called Logical Specifications-guided Dynamic Task\nSampling (LSTS), that learns a set of RL policies to guide an agent from an\ninitial state to a goal state based on a high-level task specification, while\nminimizing the number of environmental interactions. Unlike previous work, LSTS\ndoes not assume information about the environment dynamics or the Reward\nMachine, and dynamically samples promising tasks that lead to successful goal\npolicies. We evaluate LSTS on a gridworld and show that it achieves improved\ntime-to-threshold performance on complex sequential decision-making problems\ncompared to state-of-the-art RM and Automaton-guided RL baselines, such as\nQ-Learning for Reward Machines and Compositional RL from logical Specifications\n(DIRL). Moreover, we demonstrate that our method outperforms RM and\nAutomaton-guided RL baselines in terms of sample-efficiency, both in a\npartially observable robotic task and in a continuous control robotic\nmanipulation task.\n', ""  Explanation is a key component for the adoption of reinforcement learning\n(RL) in many real-world decision-making problems. In the literature, the\nexplanation is often provided by saliency attribution to the features of the RL\nagent's state. In this work, we propose a complementary approach to these\nexplanations, particularly for offline RL, where we attribute the policy\ndecisions of a trained RL agent to the trajectories encountered by it during\ntraining. To do so, we encode trajectories in offline training data\nindividually as well as collectively (encoding a set of trajectories). We then\nattribute policy decisions to a set of trajectories in this encoded space by\nestimating the sensitivity of the decision with respect to that set. Further,\nwe demonstrate the effectiveness of the proposed approach in terms of quality\nof attributions as well as practical scalability in diverse environments that\ninvolve both discrete and continuous state and action spaces such as\ngrid-worlds, video games (Atari) and continuous control (MuJoCo). We also\nconduct a human study on a simple navigation task to observe how their\nunderstanding of the task compares with data attributed for a trained RL\npolicy. Keywords -- Explainable AI, Verifiability of AI Decisions, Explainable\nRL.\n"", '  Properly defining a reward signal to efficiently train a reinforcement\nlearning (RL) agent is a challenging task. Designing balanced objective\nfunctions from which a desired behavior can emerge requires expert knowledge,\nespecially for complex environments. Learning rewards from human feedback or\nusing large language models (LLMs) to directly provide rewards are promising\nalternatives, allowing non-experts to specify goals for the agent. However,\nblack-box reward models make it difficult to debug the reward. In this work, we\npropose Object-Centric Assessment with Language Models (OCALM) to derive\ninherently interpretable reward functions for RL agents from natural language\ntask descriptions. OCALM uses the extensive world-knowledge of LLMs while\nleveraging the object-centric nature common to many environments to derive\nreward functions focused on relational concepts, providing RL agents with the\nability to derive policies from task descriptions.\n']",Reinforcement Learning with Logical Specifications and Explainability,Reinforcement Learning Methods and Applications,Reinforcement Learning
10,10,136,10_reinforcement_reward_rewards_learning,"['reinforcement', 'reward', 'rewards', 'learning', 'agents', 'agent', 'learned', 'automaton', 'autonomous', 'tasks']","['reward', 'agent', 'reinforcement', 'agents', 'policies', 'rewards', 'environment', 'policy', 'learning', 'action']","['  Reinforcement Learning (RL) has made significant strides in enabling\nartificial agents to learn diverse behaviors. However, learning an effective\npolicy often requires a large number of environment interactions. To mitigate\nsample complexity issues, recent approaches have used high-level task\nspecifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward\nMachines (RM), to guide the learning progress of the agent. In this work, we\npropose a novel approach, called Logical Specifications-guided Dynamic Task\nSampling (LSTS), that learns a set of RL policies to guide an agent from an\ninitial state to a goal state based on a high-level task specification, while\nminimizing the number of environmental interactions. Unlike previous work, LSTS\ndoes not assume information about the environment dynamics or the Reward\nMachine, and dynamically samples promising tasks that lead to successful goal\npolicies. We evaluate LSTS on a gridworld and show that it achieves improved\ntime-to-threshold performance on complex sequential decision-making problems\ncompared to state-of-the-art RM and Automaton-guided RL baselines, such as\nQ-Learning for Reward Machines and Compositional RL from logical Specifications\n(DIRL). Moreover, we demonstrate that our method outperforms RM and\nAutomaton-guided RL baselines in terms of sample-efficiency, both in a\npartially observable robotic task and in a continuous control robotic\nmanipulation task.\n', ""  Explanation is a key component for the adoption of reinforcement learning\n(RL) in many real-world decision-making problems. In the literature, the\nexplanation is often provided by saliency attribution to the features of the RL\nagent's state. In this work, we propose a complementary approach to these\nexplanations, particularly for offline RL, where we attribute the policy\ndecisions of a trained RL agent to the trajectories encountered by it during\ntraining. To do so, we encode trajectories in offline training data\nindividually as well as collectively (encoding a set of trajectories). We then\nattribute policy decisions to a set of trajectories in this encoded space by\nestimating the sensitivity of the decision with respect to that set. Further,\nwe demonstrate the effectiveness of the proposed approach in terms of quality\nof attributions as well as practical scalability in diverse environments that\ninvolve both discrete and continuous state and action spaces such as\ngrid-worlds, video games (Atari) and continuous control (MuJoCo). We also\nconduct a human study on a simple navigation task to observe how their\nunderstanding of the task compares with data attributed for a trained RL\npolicy. Keywords -- Explainable AI, Verifiability of AI Decisions, Explainable\nRL.\n"", '  Properly defining a reward signal to efficiently train a reinforcement\nlearning (RL) agent is a challenging task. Designing balanced objective\nfunctions from which a desired behavior can emerge requires expert knowledge,\nespecially for complex environments. Learning rewards from human feedback or\nusing large language models (LLMs) to directly provide rewards are promising\nalternatives, allowing non-experts to specify goals for the agent. However,\nblack-box reward models make it difficult to debug the reward. In this work, we\npropose Object-Centric Assessment with Language Models (OCALM) to derive\ninherently interpretable reward functions for RL agents from natural language\ntask descriptions. OCALM uses the extensive world-knowledge of LLMs while\nleveraging the object-centric nature common to many environments to derive\nreward functions focused on relational concepts, providing RL agents with the\nability to derive policies from task descriptions.\n']",Reinforcement Learning with Logical Specifications and Explainability,Reinforcement Learning Methods and Applications,Reinforcement Learning
11,11,135,11_sparse_lasso_optimal_iterative,"['sparse', 'lasso', 'optimal', 'iterative', 'optimization', 'algorithms', 'matrix', 'matrices', 'spectral', 'complexity']","['matrix', 'sparse', 'matrices', 'rank', 'subspace', 'tensor', 'regression', 'linear', 'problem', 'random']","['  We give a stochastic optimization algorithm that solves a dense $n\\times n$\nreal-valued linear system $Ax=b$, returning $\\tilde x$ such that $\\|A\\tilde\nx-b\\|\\leq \\epsilon\\|b\\|$ in time: $$\\tilde\nO((n^2+nk^{\\omega-1})\\log1/\\epsilon),$$ where $k$ is the number of singular\nvalues of $A$ larger than $O(1)$ times its smallest positive singular value,\n$\\omega < 2.372$ is the matrix multiplication exponent, and $\\tilde O$ hides a\npoly-logarithmic in $n$ factor. When $k=O(n^{1-\\theta})$ (namely, $A$ has a\nflat-tailed spectrum, e.g., due to noisy data or regularization), this improves\non both the cost of solving the system directly, as well as on the cost of\npreconditioning an iterative method such as conjugate gradient. In particular,\nour algorithm has an $\\tilde O(n^2)$ runtime when $k=O(n^{0.729})$. We further\nadapt this result to sparse positive semidefinite matrices and least squares\nregression.\n  Our main algorithm can be viewed as a randomized block coordinate descent\nmethod, where the key challenge is simultaneously ensuring good convergence and\nfast per-iteration time. In our analysis, we use theory of majorization for\nelementary symmetric polynomials to establish a sharp convergence guarantee\nwhen coordinate blocks are sampled using a determinantal point process. We then\nuse a Markov chain coupling argument to show that similar convergence can be\nattained with a cheaper sampling scheme, and accelerate the block coordinate\ndescent update via matrix sketching.\n', '  The problem of structured matrix estimation has been studied mostly under\nstrong noise dependence assumptions. This paper considers a general framework\nof noisy low-rank-plus-sparse matrix recovery, where the noise matrix may come\nfrom any joint distribution with arbitrary dependence across entries. We\npropose an incoherent-constrained least-square estimator and prove its\ntightness both in the sense of deterministic lower bound and matching minimax\nrisks under various noise distributions. To attain this, we establish a novel\nresult asserting that the difference between two arbitrary low-rank incoherent\nmatrices must spread energy out across its entries, in other words cannot be\ntoo sparse, which sheds light on the structure of incoherent low-rank matrices\nand may be of independent interest. We then showcase the applications of our\nframework to several important statistical machine learning problems. In the\nproblem of estimating a structured Markov transition kernel, the proposed\nmethod achieves the minimax optimality and the result can be extended to\nestimating the conditional mean operator, a crucial component in reinforcement\nlearning. The applications to multitask regression and structured covariance\nestimation are also presented. We propose an alternating minimization algorithm\nto approximately solve the potentially hard optimization problem. Numerical\nresults corroborate the effectiveness of our method which typically converges\nin a few steps.\n', '  We consider sketching algorithms which first compress data by multiplication\nwith a random sketch matrix, and then apply the sketch to quickly solve an\noptimization problem, e.g., low-rank approximation and regression. In the\nlearning-based sketching paradigm proposed by~\\cite{indyk2019learning}, the\nsketch matrix is found by choosing a random sparse matrix, e.g., CountSketch,\nand then the values of its non-zero entries are updated by running gradient\ndescent on a training data set. Despite the growing body of work on this\nparadigm, a noticeable omission is that the locations of the non-zero entries\nof previous algorithms were fixed, and only their values were learned. In this\nwork, we propose the first learning-based algorithms that also optimize the\nlocations of the non-zero entries. Our first proposed algorithm is based on a\ngreedy algorithm. However, one drawback of the greedy algorithm is its slower\ntraining time. We fix this issue and propose approaches for learning a\nsketching matrix for both low-rank approximation and Hessian approximation for\nsecond order optimization. The latter is helpful for a range of constrained\noptimization problems, such as LASSO and matrix estimation with a nuclear norm\nconstraint. Both approaches achieve good accuracy with a fast running time.\nMoreover, our experiments suggest that our algorithm can still reduce the error\nsignificantly even if we only have a very limited number of training matrices.\n']",Optimization Algorithms for Matrix Problems,Optimization Methods and Algorithms,Optimization and Design
12,12,132,12_forecast_forecasts_forecasting_meteorological,"['forecast', 'forecasts', 'forecasting', 'meteorological', 'predicting', 'spatiotemporal', 'predict', 'climate', 'prediction', 'weather']","['forecasting', 'weather', 'climate', 'soil', 'precipitation', 'spatial', 'meteorological', 'spatiotemporal', 'forecasts', 'temporal']","['  Accurate wind speed and direction forecasting is paramount across many\nsectors, spanning agriculture, renewable energy generation, and bushfire\nmanagement. However, conventional forecasting models encounter significant\nchallenges in precisely predicting wind conditions at high spatial resolutions\nfor individual locations or small geographical areas (< 20 km2) and capturing\nmedium to long-range temporal trends and comprehensive spatio-temporal\npatterns. This study focuses on a spatial temporal approach for high-resolution\ngridded wind forecasting at the height of 3 and 10 metres across large areas of\nthe Southwest of Western Australia to overcome these challenges. The model\nutilises the data that covers a broad geographic area and harnesses a diverse\narray of meteorological factors, including terrain characteristics, air\npressure, 10-metre wind forecasts from the European Centre for Medium-Range\nWeather Forecasts, and limited observation data from sparsely distributed\nweather stations (such as 3-metre wind profiles, humidity, and temperature),\nthe model demonstrates promising advancements in wind forecasting accuracy and\nreliability across the entire region of interest. This paper shows the\npotential of our machine learning model for wind forecasts across various\nprediction horizons and spatial coverage. It can help facilitate more informed\ndecision-making and enhance resilience across critical sectors.\n', '  Satellite images have become increasingly valuable for modelling regional\nclimate change effects. Earth surface forecasting represents one such task that\nintegrates satellite images with meteorological data to capture the joint\nevolution of regional climate change effects. However, understanding the\ncomplex relationship between specific meteorological variables and land surface\nevolution poses a significant challenge. In light of this challenge, our paper\nintroduces a pipeline that integrates principles from both perturbation-based\nexplainability techniques like LIME and global marginal explainability\ntechniques like PDP, besides addressing the constraints of using such\ntechniques when applying them to high-dimensional spatiotemporal deep models.\nThe proposed pipeline simplifies the undertaking of diverse investigative\nanalyses, such as marginal sensitivity analysis, marginal correlation analysis,\nlag analysis, etc., on complex land surface forecasting models In this study we\nutilised Convolutional Long Short-Term Memory (ConvLSTM) as the surface\nforecasting model and did analyses on the Normalized Difference Vegetation\nIndex (NDVI) of the surface forecasts, since meteorological variables like\ntemperature, pressure, and precipitation significantly influence it. The study\narea encompasses various regions in Europe. Our analyses show that\nprecipitation exhibits the highest sensitivity in the study area, followed by\ntemperature and pressure. Pressure has little to no direct effect on NDVI.\nAdditionally, interesting nonlinear correlations between meteorological\nvariables and NDVI have been uncovered.\n', '  The majority of real-world processes are spatiotemporal, and the data\ngenerated by them exhibits both spatial and temporal evolution. Weather is one\nof the most essential processes in this domain, and weather forecasting has\nbecome a crucial part of our daily routine. Weather data analysis is considered\nthe most complex and challenging task. Although numerical weather prediction\nmodels are currently state-of-the-art, they are resource-intensive and\ntime-consuming. Numerous studies have proposed time series-based models as a\nviable alternative to numerical forecasts. Recent research in the area of time\nseries analysis indicates significant advancements, particularly regarding the\nuse of state-space-based models (white box) and, more recently, the integration\nof machine learning and deep neural network-based models (black box). The most\nfamous examples of such models are RNNs and transformers. These models have\ndemonstrated remarkable results in the field of time-series analysis and have\ndemonstrated effectiveness in modelling temporal correlations. It is crucial to\ncapture both temporal and spatial correlations for a spatiotemporal process, as\nthe values at nearby locations and time affect the values of a spatiotemporal\nprocess at a specific point. This self-contained paper explores various\nregional data-driven weather forecasting methods, i.e., forecasting over\nmultiple latitude-longitude points (matrix-shaped spatial grid) to capture\nspatiotemporal correlations. The results showed that spatiotemporal prediction\nmodels reduced computational costs while improving accuracy. In particular, the\nproposed tensor train dynamic mode decomposition-based forecasting model has\ncomparable accuracy to the state-of-the-art models without the need for\ntraining. We provide convincing numerical experiments to show that the proposed\napproach is practical.\n']",Spatiotemporal Weather Forecasting and Climate Prediction,Climate and Weather Modeling and Prediction using Machine Learning and NLP,Environmental Modeling and Prediction using Advanced Computing and AI
13,13,131,13_reinforcement_planning_learning_exploration,"['reinforcement', 'planning', 'learning', 'exploration', 'actions', 'rewards', 'learned', 'reward', 'critic', 'control']","['policy', 'reinforcement', 'control', 'environment', 'action', 'dynamics', 'continuous', 'learning', 'critic', 'exploration']","[""  Deploying controllers trained with Reinforcement Learning (RL) on real robots\ncan be challenging: RL relies on agents' policies being modeled as Markov\nDecision Processes (MDPs), which assume an inherently discrete passage of time.\nThe use of MDPs results in that nearly all RL-based control systems employ a\nfixed-rate control strategy with a period (or time step) typically chosen based\non the developer's experience or specific characteristics of the application\nenvironment. Unfortunately, the system should be controlled at the highest,\nworst-case frequency to ensure stability, which can demand significant\ncomputational and energy resources and hinder the deployability of the\ncontroller on onboard hardware. Adhering to the principles of reactive\nprogramming, we surmise that applying control actions only when necessary\nenables the use of simpler hardware and helps reduce energy consumption. We\nchallenge the fixed frequency assumption by proposing a variant of RL with\nvariable control rate. In this approach, the policy decides the action the\nagent should take as well as the duration of the time step associated with that\naction. In our new setting, we expand Soft Actor-Critic (SAC) to compute the\noptimal policy with a variable control rate, introducing the Soft Elastic\nActor-Critic (SEAC) algorithm. We show the efficacy of SEAC through a\nproof-of-concept simulation driving an agent with Newtonian kinematics. Our\nexperiments show higher average returns, shorter task completion times, and\nreduced computational resources when compared to fixed rate policies.\n"", ""  Traditional reinforcement learning (RL) methods typically employ a fixed\ncontrol loop, where each cycle corresponds to an action. This rigidity poses\nchallenges in practical applications, as the optimal control frequency is\ntask-dependent. A suboptimal choice can lead to high computational demands and\nreduced exploration efficiency. Variable Time Step Reinforcement Learning\n(VTS-RL) addresses these issues by using adaptive frequencies for the control\nloop, executing actions only when necessary. This approach, rooted in reactive\nprogramming principles, reduces computational load and extends the action space\nby including action durations. However, VTS-RL's implementation is often\ncomplicated by the need to tune multiple hyperparameters that govern\nexploration in the multi-objective action-duration space (i.e., balancing task\nperformance and number of time steps to achieve a goal). To overcome these\nchallenges, we introduce the Multi-Objective Soft Elastic Actor-Critic (MOSEAC)\nmethod. This method features an adaptive reward scheme that adjusts\nhyperparameters based on observed trends in task rewards during training. This\nscheme reduces the complexity of hyperparameter tuning, requiring a single\nhyperparameter to guide exploration, thereby simplifying the learning process\nand lowering deployment costs. We validate the MOSEAC method through\nsimulations in a Newtonian kinematics environment, demonstrating high task and\ntraining performance with fewer time steps, ultimately lowering energy\nconsumption. This validation shows that MOSEAC streamlines RL algorithm\ndeployment by automatically tuning the agent control loop frequency using a\nsingle parameter. Its principles can be applied to enhance any RL algorithm,\nmaking it a versatile solution for various applications.\n"", ""  Reinforcement learning (RL) on high-dimensional and complex problems relies\non abstraction for improved efficiency and generalization. In this paper, we\nstudy abstraction in the continuous-control setting, and extend the definition\nof Markov decision process (MDP) homomorphisms to the setting of continuous\nstate and action spaces. We derive a policy gradient theorem on the abstract\nMDP for both stochastic and deterministic policies. Our policy gradient results\nallow for leveraging approximate symmetries of the environment for policy\noptimization. Based on these theorems, we propose a family of actor-critic\nalgorithms that are able to learn the policy and the MDP homomorphism map\nsimultaneously, using the lax bisimulation metric. Finally, we introduce a\nseries of environments with continuous symmetries to further demonstrate the\nability of our algorithm for action abstraction in the presence of such\nsymmetries. We demonstrate the effectiveness of our method on our environments,\nas well as on challenging visual control tasks from the DeepMind Control Suite.\nOur method's ability to utilize MDP homomorphisms for representation learning\nleads to improved performance, and the visualizations of the latent space\nclearly demonstrate the structure of the learned abstraction.\n""]",Reinforcement Learning with Adaptive Control and Exploration,Reinforcement Learning Applications and Methodologies,Reinforcement Learning
14,14,129,14_reinforcement_optimality_learning_optimal,"['reinforcement', 'optimality', 'learning', 'optimal', 'critic', 'policies', 'policy', 'markovian', 'reward', 'exploration']","['policy', 'reinforcement', 'control', 'gradient', 'policies', 'function', 'value', 'critic', 'actor', 'algorithm']","['  Entropy regularization is an efficient technique for encouraging exploration\nand preventing a premature convergence of (vanilla) policy gradient methods in\nreinforcement learning (RL). However, the theoretical understanding of\nentropy-regularized RL algorithms has been limited. In this paper, we revisit\nthe classical entropy regularized policy gradient methods with the soft-max\npolicy parametrization, whose convergence has so far only been established\nassuming access to exact gradient oracles. To go beyond this scenario, we\npropose the first set of (nearly) unbiased stochastic policy gradient\nestimators with trajectory-level entropy regularization, with one being an\nunbiased visitation measure-based estimator and the other one being a nearly\nunbiased yet more practical trajectory-based estimator. We prove that although\nthe estimators themselves are unbounded in general due to the additional\nlogarithmic policy rewards introduced by the entropy term, the variances are\nuniformly bounded. We then propose a two-phase stochastic policy gradient (PG)\nalgorithm that uses a large batch size in the first phase to overcome the\nchallenge of the stochastic approximation due to the non-coercive landscape,\nand uses a small batch size in the second phase by leveraging the curvature\ninformation around the optimal policy. We establish a global optimality\nconvergence result and a sample complexity of\n$\\widetilde{\\mathcal{O}}(\\frac{1}{\\epsilon^2})$ for the proposed algorithm. Our\nresult is the first global convergence and sample complexity results for the\nstochastic entropy-regularized vanilla PG method.\n', '  Since the objective functions of reinforcement learning problems are\ntypically highly nonconvex, it is desirable that policy gradient, the most\npopular algorithm, escapes saddle points and arrives at second-order stationary\npoints. Existing results only consider vanilla policy gradient algorithms with\nunbiased gradient estimators, but practical implementations under the\ninfinite-horizon discounted reward setting are biased due to finite-horizon\nsampling. Moreover, actor-critic methods, whose second-order convergence has\nnot yet been established, are also biased due to the critic approximation of\nthe value function. We provide a novel second-order analysis of biased policy\ngradient methods, including the vanilla gradient estimator computed from\nMonte-Carlo sampling of trajectories as well as the double-loop actor-critic\nalgorithm, where in the inner loop the critic improves the approximation of the\nvalue function via TD(0) learning. Separately, we also establish the\nconvergence of TD(0) on Markov chains irrespective of initial state\ndistribution.\n', '  The infinite horizon setting is widely adopted for problems of reinforcement\nlearning (RL). These invariably result in stationary policies that are optimal.\nIn many situations, finite horizon control problems are of interest and for\nsuch problems, the optimal policies are time-varying in general. Another\nsetting that has become popular in recent times is of Constrained Reinforcement\nLearning, where the agent maximizes its rewards while it also aims to satisfy\nsome given constraint criteria. However, this setting has only been studied in\nthe context of infinite horizon MDPs where stationary policies are optimal. We\npresent an algorithm for constrained RL in the Finite Horizon Setting where the\nhorizon terminates after a fixed (finite) time. We use function approximation\nin our algorithm which is essential when the state and action spaces are large\nor continuous and use the policy gradient method to find the optimal policy.\nThe optimal policy that we obtain depends on the stage and so is non-stationary\nin general. To the best of our knowledge, our paper presents the first policy\ngradient algorithm for the finite horizon setting with constraints. We show the\nconvergence of our algorithm to a constrained optimal policy. We also compare\nand analyze the performance of our algorithm through experiments and show that\nour algorithm performs better than some other well known algorithms.\n']",Reinforcement Learning with Policy Gradient Methods,Reinforcement Learning Methods and Applications,Reinforcement Learning
14,14,129,14_reinforcement_optimality_learning_optimal,"['reinforcement', 'optimality', 'learning', 'optimal', 'critic', 'policies', 'policy', 'markovian', 'reward', 'exploration']","['policy', 'reinforcement', 'control', 'gradient', 'policies', 'function', 'value', 'critic', 'actor', 'algorithm']","['  Entropy regularization is an efficient technique for encouraging exploration\nand preventing a premature convergence of (vanilla) policy gradient methods in\nreinforcement learning (RL). However, the theoretical understanding of\nentropy-regularized RL algorithms has been limited. In this paper, we revisit\nthe classical entropy regularized policy gradient methods with the soft-max\npolicy parametrization, whose convergence has so far only been established\nassuming access to exact gradient oracles. To go beyond this scenario, we\npropose the first set of (nearly) unbiased stochastic policy gradient\nestimators with trajectory-level entropy regularization, with one being an\nunbiased visitation measure-based estimator and the other one being a nearly\nunbiased yet more practical trajectory-based estimator. We prove that although\nthe estimators themselves are unbounded in general due to the additional\nlogarithmic policy rewards introduced by the entropy term, the variances are\nuniformly bounded. We then propose a two-phase stochastic policy gradient (PG)\nalgorithm that uses a large batch size in the first phase to overcome the\nchallenge of the stochastic approximation due to the non-coercive landscape,\nand uses a small batch size in the second phase by leveraging the curvature\ninformation around the optimal policy. We establish a global optimality\nconvergence result and a sample complexity of\n$\\widetilde{\\mathcal{O}}(\\frac{1}{\\epsilon^2})$ for the proposed algorithm. Our\nresult is the first global convergence and sample complexity results for the\nstochastic entropy-regularized vanilla PG method.\n', '  Since the objective functions of reinforcement learning problems are\ntypically highly nonconvex, it is desirable that policy gradient, the most\npopular algorithm, escapes saddle points and arrives at second-order stationary\npoints. Existing results only consider vanilla policy gradient algorithms with\nunbiased gradient estimators, but practical implementations under the\ninfinite-horizon discounted reward setting are biased due to finite-horizon\nsampling. Moreover, actor-critic methods, whose second-order convergence has\nnot yet been established, are also biased due to the critic approximation of\nthe value function. We provide a novel second-order analysis of biased policy\ngradient methods, including the vanilla gradient estimator computed from\nMonte-Carlo sampling of trajectories as well as the double-loop actor-critic\nalgorithm, where in the inner loop the critic improves the approximation of the\nvalue function via TD(0) learning. Separately, we also establish the\nconvergence of TD(0) on Markov chains irrespective of initial state\ndistribution.\n', '  The infinite horizon setting is widely adopted for problems of reinforcement\nlearning (RL). These invariably result in stationary policies that are optimal.\nIn many situations, finite horizon control problems are of interest and for\nsuch problems, the optimal policies are time-varying in general. Another\nsetting that has become popular in recent times is of Constrained Reinforcement\nLearning, where the agent maximizes its rewards while it also aims to satisfy\nsome given constraint criteria. However, this setting has only been studied in\nthe context of infinite horizon MDPs where stationary policies are optimal. We\npresent an algorithm for constrained RL in the Finite Horizon Setting where the\nhorizon terminates after a fixed (finite) time. We use function approximation\nin our algorithm which is essential when the state and action spaces are large\nor continuous and use the policy gradient method to find the optimal policy.\nThe optimal policy that we obtain depends on the stage and so is non-stationary\nin general. To the best of our knowledge, our paper presents the first policy\ngradient algorithm for the finite horizon setting with constraints. We show the\nconvergence of our algorithm to a constrained optimal policy. We also compare\nand analyze the performance of our algorithm through experiments and show that\nour algorithm performs better than some other well known algorithms.\n']",Reinforcement Learning with Policy Gradient Methods,Reinforcement Learning Methods and Applications,Reinforcement Learning
15,15,125,15_learns_reinforcement_learning_planning,"['learns', 'reinforcement', 'learning', 'planning', 'supervised', 'learned', 'trained', 'reward', 'rewards', 'learn']","['reinforcement', 'environments', 'offline', 'actions', 'agent', 'action', 'tasks', 'reward', 'visual', 'agents']","['  Recent works have shown the remarkable superiority of transformer models in\nreinforcement learning (RL), where the decision-making problem is formulated as\nsequential generation. Transformer-based agents could emerge with\nself-improvement in online environments by providing task contexts, such as\nmultiple trajectories, called in-context RL. However, due to the quadratic\ncomputation complexity of attention in transformers, current in-context RL\nmethods suffer from huge computational costs as the task horizon increases. In\ncontrast, the Mamba model is renowned for its efficient ability to process\nlong-term dependencies, which provides an opportunity for in-context RL to\nsolve tasks that require long-term memory. To this end, we first implement\nDecision Mamba (DM) by replacing the backbone of Decision Transformer (DT).\nThen, we propose a Decision Mamba-Hybrid (DM-H) with the merits of transformers\nand Mamba in high-quality prediction and long-term memory. Specifically, DM-H\nfirst generates high-value sub-goals from long-term memory through the Mamba\nmodel. Then, we use sub-goals to prompt the transformer, establishing\nhigh-quality predictions. Experimental results demonstrate that DM-H achieves\nstate-of-the-art in long and short-term tasks, such as D4RL, Grid World, and\nTmaze benchmarks. Regarding efficiency, the online testing of DM-H in the\nlong-term task is 28$\\times$ times faster than the transformer-based baselines.\n', '  Real-world reinforcement learning (RL) environments, whether in robotics or\nindustrial settings, often involve non-visual observations and require not only\nefficient but also reliable and thus interpretable and flexible RL approaches.\nTo improve efficiency, agents that perform state representation learning with\nauxiliary tasks have been widely studied in visual observation contexts.\nHowever, for real-world problems, dedicated representation learning modules\nthat are decoupled from RL agents are more suited to meet requirements. This\nstudy compares common auxiliary tasks based on, to the best of our knowledge,\nthe only decoupled representation learning method for low-dimensional\nnon-visual observations. We evaluate potential improvements in sample\nefficiency and returns for environments ranging from a simple pendulum to a\ncomplex simulated robotics task. Our findings show that representation learning\nwith auxiliary tasks only provides performance gains in sufficiently complex\nenvironments and that learning environment dynamics is preferable to predicting\nrewards. These insights can inform future development of interpretable\nrepresentation learning approaches for non-visual observations and advance the\nuse of RL solutions in real-world scenarios.\n', '  Unsupervised and self-supervised objectives, such as next token prediction,\nhave enabled pre-training generalist models from large amounts of unlabeled\ndata. In reinforcement learning (RL), however, finding a truly general and\nscalable unsupervised pre-training objective for generalist policies from\noffline data remains a major open question. While a number of methods have been\nproposed to enable generic self-supervised RL, based on principles such as\ngoal-conditioned RL, behavioral cloning, and unsupervised skill learning, such\nmethods remain limited in terms of either the diversity of the discovered\nbehaviors, the need for high-quality demonstration data, or the lack of a clear\nadaptation mechanism for downstream tasks. In this work, we propose a novel\nunsupervised framework to pre-train generalist policies that capture diverse,\noptimal, long-horizon behaviors from unlabeled offline data such that they can\nbe quickly adapted to any arbitrary new tasks in a zero-shot manner. Our key\ninsight is to learn a structured representation that preserves the temporal\nstructure of the underlying environment, and then to span this learned latent\nspace with directional movements, which enables various zero-shot policy\n""prompting"" schemes for downstream tasks. Through our experiments on simulated\nrobotic locomotion and manipulation benchmarks, we show that our unsupervised\npolicies can solve goal-conditioned and general RL tasks in a zero-shot\nfashion, even often outperforming prior methods designed specifically for each\nsetting. Our code and videos are available at\nhttps://seohong.me/projects/hilp/.\n']",Reinforcement Learning Methods,Reinforcement Learning Methods and Applications,Reinforcement Learning
15,15,125,15_learns_reinforcement_learning_planning,"['learns', 'reinforcement', 'learning', 'planning', 'supervised', 'learned', 'trained', 'reward', 'rewards', 'learn']","['reinforcement', 'environments', 'offline', 'actions', 'agent', 'action', 'tasks', 'reward', 'visual', 'agents']","['  Recent works have shown the remarkable superiority of transformer models in\nreinforcement learning (RL), where the decision-making problem is formulated as\nsequential generation. Transformer-based agents could emerge with\nself-improvement in online environments by providing task contexts, such as\nmultiple trajectories, called in-context RL. However, due to the quadratic\ncomputation complexity of attention in transformers, current in-context RL\nmethods suffer from huge computational costs as the task horizon increases. In\ncontrast, the Mamba model is renowned for its efficient ability to process\nlong-term dependencies, which provides an opportunity for in-context RL to\nsolve tasks that require long-term memory. To this end, we first implement\nDecision Mamba (DM) by replacing the backbone of Decision Transformer (DT).\nThen, we propose a Decision Mamba-Hybrid (DM-H) with the merits of transformers\nand Mamba in high-quality prediction and long-term memory. Specifically, DM-H\nfirst generates high-value sub-goals from long-term memory through the Mamba\nmodel. Then, we use sub-goals to prompt the transformer, establishing\nhigh-quality predictions. Experimental results demonstrate that DM-H achieves\nstate-of-the-art in long and short-term tasks, such as D4RL, Grid World, and\nTmaze benchmarks. Regarding efficiency, the online testing of DM-H in the\nlong-term task is 28$\\times$ times faster than the transformer-based baselines.\n', '  Real-world reinforcement learning (RL) environments, whether in robotics or\nindustrial settings, often involve non-visual observations and require not only\nefficient but also reliable and thus interpretable and flexible RL approaches.\nTo improve efficiency, agents that perform state representation learning with\nauxiliary tasks have been widely studied in visual observation contexts.\nHowever, for real-world problems, dedicated representation learning modules\nthat are decoupled from RL agents are more suited to meet requirements. This\nstudy compares common auxiliary tasks based on, to the best of our knowledge,\nthe only decoupled representation learning method for low-dimensional\nnon-visual observations. We evaluate potential improvements in sample\nefficiency and returns for environments ranging from a simple pendulum to a\ncomplex simulated robotics task. Our findings show that representation learning\nwith auxiliary tasks only provides performance gains in sufficiently complex\nenvironments and that learning environment dynamics is preferable to predicting\nrewards. These insights can inform future development of interpretable\nrepresentation learning approaches for non-visual observations and advance the\nuse of RL solutions in real-world scenarios.\n', '  Unsupervised and self-supervised objectives, such as next token prediction,\nhave enabled pre-training generalist models from large amounts of unlabeled\ndata. In reinforcement learning (RL), however, finding a truly general and\nscalable unsupervised pre-training objective for generalist policies from\noffline data remains a major open question. While a number of methods have been\nproposed to enable generic self-supervised RL, based on principles such as\ngoal-conditioned RL, behavioral cloning, and unsupervised skill learning, such\nmethods remain limited in terms of either the diversity of the discovered\nbehaviors, the need for high-quality demonstration data, or the lack of a clear\nadaptation mechanism for downstream tasks. In this work, we propose a novel\nunsupervised framework to pre-train generalist policies that capture diverse,\noptimal, long-horizon behaviors from unlabeled offline data such that they can\nbe quickly adapted to any arbitrary new tasks in a zero-shot manner. Our key\ninsight is to learn a structured representation that preserves the temporal\nstructure of the underlying environment, and then to span this learned latent\nspace with directional movements, which enables various zero-shot policy\n""prompting"" schemes for downstream tasks. Through our experiments on simulated\nrobotic locomotion and manipulation benchmarks, we show that our unsupervised\npolicies can solve goal-conditioned and general RL tasks in a zero-shot\nfashion, even often outperforming prior methods designed specifically for each\nsetting. Our code and videos are available at\nhttps://seohong.me/projects/hilp/.\n']",Reinforcement Learning Methods,Reinforcement Learning Methods and Applications,Reinforcement Learning
16,16,125,16_bias_biases_stereotypes_attitudes,"['bias', 'biases', 'stereotypes', 'attitudes', 'stereotyping', 'perceptions', 'stereotypical', 'language', 'demographics', 'culturally']","['biases', 'bias', 'cultural', 'social', 'persona', 'stereotypes', 'personas', 'groups', 'responses', 'human']","[""  Given the rapid advancement of large-scale language models, artificial\nintelligence (AI) models, like ChatGPT, are playing an increasingly prominent\nrole in human society. However, to ensure that artificial intelligence models\nbenefit human society, we must first fully understand the similarities and\ndifferences between the human-like characteristics exhibited by artificial\nintelligence models and real humans, as well as the cultural stereotypes and\nbiases that artificial intelligence models may exhibit in the process of\ninteracting with humans. This study first measured ChatGPT in 84 dimensions of\npsychological characteristics, revealing differences between ChatGPT and human\nnorms in most dimensions as well as in high-dimensional psychological\nrepresentations. Additionally, through the measurement of ChatGPT in 13\ndimensions of cultural values, it was revealed that ChatGPT's cultural value\npatterns are dissimilar to those of various countries/regions worldwide.\nFinally, an analysis of ChatGPT's performance in eight decision-making tasks\ninvolving interactions with humans from different countries/regions revealed\nthat ChatGPT exhibits clear cultural stereotypes in most decision-making tasks\nand shows significant cultural bias in third-party punishment and ultimatum\ngames. The findings indicate that, compared to humans, ChatGPT exhibits a\ndistinct psychological profile and cultural value orientation, and it also\nshows cultural biases and stereotypes in interpersonal decision-making. Future\nresearch endeavors should emphasize enhanced technical oversight and augmented\ntransparency in the database and algorithmic training procedures to foster more\nefficient cross-cultural communication and mitigate social disparities.\n"", ""  Warning: This paper contains examples of stereotypes and biases. Large\nLanguage Models (LLMs) exhibit considerable social biases, and various studies\nhave tried to evaluate and mitigate these biases accurately. Previous studies\nuse downstream tasks as prompts to examine the degree of social biases for\nevaluation and mitigation. While LLMs' output highly depends on prompts,\nprevious studies evaluating and mitigating bias have often relied on a limited\nvariety of prompts. In this paper, we investigate the sensitivity of LLMs when\nchanging prompt variations (task instruction and prompt, few-shot examples,\ndebias-prompt) by analyzing task performance and social bias of LLMs. Our\nexperimental results reveal that LLMs are highly sensitive to prompts to the\nextent that the ranking of LLMs fluctuates when comparing models for task\nperformance and social bias. Additionally, we show that LLMs have tradeoffs\nbetween performance and social bias caused by the prompts. Less bias from\nprompt setting may result in reduced performance. Moreover, the ambiguity of\ninstances is one of the reasons for this sensitivity to prompts in advanced\nLLMs, leading to various outputs. We recommend using diverse prompts, as in\nthis study, to compare the effects of prompts on social bias in LLMs.\n"", ""  Large language models (LLMs) can pass explicit social bias tests but still\nharbor implicit biases, similar to humans who endorse egalitarian beliefs yet\nexhibit subtle biases. Measuring such implicit biases can be a challenge: as\nLLMs become increasingly proprietary, it may not be possible to access their\nembeddings and apply existing bias measures; furthermore, implicit biases are\nprimarily a concern if they affect the actual decisions that these systems\nmake. We address both challenges by introducing two new measures of bias: LLM\nImplicit Bias, a prompt-based method for revealing implicit bias; and LLM\nDecision Bias, a strategy to detect subtle discrimination in decision-making\ntasks. Both measures are based on psychological research: LLM Implicit Bias\nadapts the Implicit Association Test, widely used to study the automatic\nassociations between concepts held in human minds; and LLM Decision Bias\noperationalizes psychological results indicating that relative evaluations\nbetween two candidates, not absolute evaluations assessing each independently,\nare more diagnostic of implicit biases. Using these measures, we found\npervasive stereotype biases mirroring those in society in 8 value-aligned\nmodels across 4 social categories (race, gender, religion, health) in 21\nstereotypes (such as race and criminality, race and weapons, gender and\nscience, age and negativity). Our prompt-based LLM Implicit Bias measure\ncorrelates with existing language model embedding-based bias methods, but\nbetter predicts downstream behaviors measured by LLM Decision Bias. These new\nprompt-based measures draw from psychology's long history of research into\nmeasuring stereotype biases based on purely observable behavior; they expose\nnuanced biases in proprietary value-aligned LLMs that appear unbiased according\nto standard benchmarks.\n""]",Language Models and Social Biases,Bias and Fairness in Large Language Models,Fairness and Bias in Artificial Intelligence
17,17,125,17_privacy_adversarial_private_confidential,"['privacy', 'adversarial', 'private', 'confidential', 'obfuscation', 'datasets', 'unlearning', 'deep', 'secure', 'learning']","['privacy', 'private', 'unlearning', 'sensitive', 'attacks', 'leakage', 'security', 'synthetic', 'utility', 'protection']","['  We present an approach for generating differentially private synthetic text\nusing large language models (LLMs), via private prediction. In the private\nprediction framework, we only require the output synthetic data to satisfy\ndifferential privacy guarantees. This is in contrast to approaches that train a\ngenerative model on potentially sensitive user-supplied source data and seek to\nensure the model itself is safe to release.\n  We prompt a pretrained LLM with source data, but ensure that next-token\npredictions are made with differential privacy guarantees. Previous work in\nthis paradigm reported generating a small number of examples (<10) at\nreasonable privacy levels, an amount of data that is useful only for downstream\nin-context learning or prompting. In contrast, we make changes that allow us to\ngenerate thousands of high-quality synthetic data points, greatly expanding the\nset of potential applications. Our improvements come from an improved privacy\nanalysis and a better private selection mechanism, which makes use of the\nequivalence between the softmax layer for sampling tokens in LLMs and the\nexponential mechanism. Furthermore, we introduce a novel use of public\npredictions via the sparse vector technique, in which we do not pay privacy\ncosts for tokens that are predictable without sensitive data; we find this to\nbe particularly effective for structured data.\n', '  Deep learning holds immense promise for aiding radiologists in breast cancer\ndetection. However, achieving optimal model performance is hampered by\nlimitations in availability and sharing of data commonly associated to patient\nprivacy concerns. Such concerns are further exacerbated, as traditional deep\nlearning models can inadvertently leak sensitive training information. This\nwork addresses these challenges exploring and quantifying the utility of\nprivacy-preserving deep learning techniques, concretely, (i) differentially\nprivate stochastic gradient descent (DP-SGD) and (ii) fully synthetic training\ndata generated by our proposed malignancy-conditioned generative adversarial\nnetwork. We assess these methods via downstream malignancy classification of\nmammography masses using a transformer model. Our experimental results depict\nthat synthetic data augmentation can improve privacy-utility tradeoffs in\ndifferentially private model training. Further, model pretraining on synthetic\ndata achieves remarkable performance, which can be further increased with\nDP-SGD fine-tuning across all privacy guarantees. With this first in-depth\nexploration of privacy-preserving deep learning in breast imaging, we address\ncurrent and emerging clinical privacy requirements and pave the way towards the\nadoption of private high-utility deep diagnostic models. Our reproducible\ncodebase is publicly available at https://github.com/RichardObi/mammo_dp.\n', '  Differentially private training algorithms like DP-SGD protect sensitive\ntraining data by ensuring that trained models do not reveal private\ninformation. An alternative approach, which this paper studies, is to use a\nsensitive dataset to generate synthetic data that is differentially private\nwith respect to the original data, and then non-privately training a model on\nthe synthetic data. Doing so has several advantages: synthetic data can be\nreused for other tasks (including for hyper parameter tuning), retained\nindefinitely, and shared with third parties without sacrificing privacy.\nHowever, generating private synthetic data is much harder than training a\nprivate model. To improve performance on text data, recent work has utilized\npublic data by starting with a pre-trained generative language model and\nprivately fine-tuning it on sensitive data. This model can be used to sample a\nDP synthetic dataset. While this strategy seems straightforward, executing it\nhas proven problematic. Previous approaches either show significant performance\nloss, or have, as we show, critical design flaws. In this paper we demonstrate\nthat a proper training objective along with tuning fewer parameters results in\nexcellent DP synthetic data quality. Our approach is competitive with direct\nDP-training of downstream classifiers in terms of performance on downstream\ntasks. Further, we demonstrate that our DP synthetic data is not only useful\nfor downstream classifier training, but also to tune those same models.\n']",Differentially Private Deep Learning and Synthetic Data Generation,Differential Privacy in Machine Learning,Machine Learning and Data Privacy
18,18,124,18_graphs_networks_graph_graphgpt,"['graphs', 'networks', 'graph', 'graphgpt', 'adjacency', 'neural', 'nodes', 'node', 'edges', 'network']","['graph', 'graphs', 'topological', 'node', 'simplicial', 'message', 'networks', 'neural', 'molecular', 'positional']","['  Graph Neural Networks (GNNs) have excelled in predicting graph properties in\nvarious applications ranging from identifying trends in social networks to drug\ndiscovery and malware detection. With the abundance of new architectures and\nincreased complexity, GNNs are becoming highly specialized when tested on a few\nwell-known datasets. However, how the performance of GNNs depends on the\ntopological and features properties of graphs is still an open question. In\nthis work, we introduce a comprehensive benchmarking framework for graph\nmachine learning, focusing on the performance of GNNs across varied network\nstructures. Utilizing the geometric soft configuration model in hyperbolic\nspace, we generate synthetic networks with realistic topological properties and\nnode feature vectors. This approach enables us to assess the impact of network\nproperties, such as topology-feature correlation, degree distributions, local\ndensity of triangles (or clustering), and homophily, on the effectiveness of\ndifferent GNN architectures. Our results highlight the dependency of model\nperformance on the interplay between network structure and node features,\nproviding insights for model selection in various scenarios. This study\ncontributes to the field by offering a versatile tool for evaluating GNNs,\nthereby assisting in developing and selecting suitable models based on specific\ndata characteristics.\n', ""  Recently, transformer architectures for graphs emerged as an alternative to\nestablished techniques for machine learning with graphs, such as\n(message-passing) graph neural networks. So far, they have shown promising\nempirical results, e.g., on molecular prediction datasets, often attributed to\ntheir ability to circumvent graph neural networks' shortcomings, such as\nover-smoothing and over-squashing. Here, we derive a taxonomy of graph\ntransformer architectures, bringing some order to this emerging field. We\noverview their theoretical properties, survey structural and positional\nencodings, and discuss extensions for important graph classes, e.g., 3D\nmolecular graphs. Empirically, we probe how well graph transformers can recover\nvarious graph properties, how well they can deal with heterophilic graphs, and\nto what extent they prevent over-squashing. Further, we outline open challenges\nand research direction to stimulate future work. Our code is available at\nhttps://github.com/luis-mueller/probing-graph-transformers.\n"", ""  The irreducible complexity of natural phenomena has led Graph Neural Networks\nto be employed as a standard model to perform representation learning tasks on\ngraph-structured data. While their capacity to capture local and global\npatterns is remarkable, the implications associated with long-range and\nhigher-order dependencies pose considerable challenges to such models. This\nwork starts with a theoretical framework to reveal the impact of network's\nwidth, depth, and graph topology on the over-squashing phenomena in\nmessage-passing neural networks. Then, the work drifts towards, higher-order\ninteractions and multi-relational inductive biases via Topological Neural\nNetworks. Such models propagate messages through higher-dimensional structures,\nproviding shortcuts or additional routes for information flow. With this\nconstruction, the underlying computational graph is no longer coupled with the\ninput graph structure, thus mitigating the aforementioned bottlenecks while\naccounting also for higher-order interactions. Inspired by Graph Attention\nNetworks, two topological attention networks are proposed: Simplicial and Cell\nAttention Networks. The rationale behind these architecture is to leverage the\nextended notion of neighbourhoods provided by the arrangement of groups of\nnodes within a simplicial or cell complex to design anisotropic aggregations\nable to measure the importance of the information coming from different regions\nof the domain. By doing so, they capture dependencies that conventional Graph\nNeural Networks might miss. Finally, a multi-way communication scheme is\nintroduced with Enhanced Cellular Isomorphism Networks, which augment\ntopological message passing schemes to enable a direct interactions among\ngroups of nodes arranged in ring-like structures.\n""]",Graph Neural Networks and Architectures,Graph Neural Networks and Deep Learning on Graph-Structured Data,Graph Representation Learning and Neural Networks
19,19,123,19_visual_multimodal_recognition_vision,"['visual', 'multimodal', 'recognition', 'vision', 'embedding', 'language', 'linguistic', 'concepts', 'semantic', 'vlm']","['vision', 'visual', 'multimodal', 'language', 'modal', 'image', 'concepts', 'text', 'models', 'vlm']","['  Following the recent popularity of Large Language Models (LLMs), several\nattempts have been made to extend them to the visual domain. From having a\nvisual assistant that could guide us through unfamiliar environments to\ngenerative models that produce images using only a high-level text description,\nthe vision-language model (VLM) applications will significantly impact our\nrelationship with technology. However, there are many challenges that need to\nbe addressed to improve the reliability of those models. While language is\ndiscrete, vision evolves in a much higher dimensional space in which concepts\ncannot always be easily discretized. To better understand the mechanics behind\nmapping vision to language, we present this introduction to VLMs which we hope\nwill help anyone who would like to enter the field. First, we introduce what\nVLMs are, how they work, and how to train them. Then, we present and discuss\napproaches to evaluate VLMs. Although this work primarily focuses on mapping\nimages to language, we also discuss extending VLMs to videos.\n', ""  Vision-language models (VLMs) excel in zero-shot recognition but their\nperformance varies greatly across different visual concepts. For example,\nalthough CLIP achieves impressive accuracy on ImageNet (60-80%), its\nperformance drops below 10% for more than ten concepts like night snake,\npresumably due to their limited presence in the pretraining data. However,\nmeasuring the frequency of concepts in VLMs' large-scale datasets is\nchallenging. We address this by using large language models (LLMs) to count the\nnumber of pretraining texts that contain synonyms of these concepts. Our\nanalysis confirms that popular datasets, such as LAION, exhibit a long-tailed\nconcept distribution, yielding biased performance in VLMs. We also find that\ndownstream applications of VLMs, including visual chatbots (e.g., GPT-4V) and\ntext-to-image models (e.g., Stable Diffusion), often fail to recognize or\ngenerate images of rare concepts identified by our method. To mitigate the\nimbalanced performance of zero-shot VLMs, we propose REtrieval-Augmented\nLearning (REAL). First, instead of prompting VLMs using the original class\nnames, REAL uses their most frequent synonyms found in pretraining texts. This\nsimple change already outperforms costly human-engineered and LLM-enriched\nprompts over nine benchmark datasets. Second, REAL trains a linear classifier\non a small yet balanced set of pretraining data retrieved using concept\nsynonyms. REAL surpasses the previous zero-shot SOTA, using 400x less storage\nand 10,000x less training time!\n"", '  Vision language models (VLMs) have drastically changed the computer vision\nmodel landscape in only a few years, opening an exciting array of new\napplications from zero-shot image classification, over to image captioning, and\nvisual question answering. Unlike pure vision models, they offer an intuitive\nway to access visual content through language prompting. The wide applicability\nof such models encourages us to ask whether they also align with human vision -\nspecifically, how far they adopt human-induced visual biases through multimodal\nfusion, or whether they simply inherit biases from pure vision models. One\nimportant visual bias is the texture vs. shape bias, or the dominance of local\nover global information. In this paper, we study this bias in a wide range of\npopular VLMs. Interestingly, we find that VLMs are often more shape-biased than\ntheir vision encoders, indicating that visual biases are modulated to some\nextent through text in multimodal models. If text does indeed influence visual\nbiases, this suggests that we may be able to steer visual biases not just\nthrough visual input but also through language: a hypothesis that we confirm\nthrough extensive experiments. For instance, we are able to steer shape bias\nfrom as low as 49% to as high as 72% through prompting alone. For now, the\nstrong human bias towards shape (96%) remains out of reach for all tested VLMs.\n']",Vision-Language Models (VLMs),Vision-Language Models and Multimodal Learning,Multimodal Learning and Vision-Language Models
20,20,121,20_federated_privacy_distributed_cloud,"['federated', 'privacy', 'distributed', 'cloud', 'collaborative', 'shared', 'cluster', 'sharing', 'networks', 'fedmap']","['federated', 'clients', 'client', 'server', 'communication', 'devices', 'heterogeneity', 'privacy', 'local', 'decentralized']","['  Federated Learning(FL) is a privacy-preserving machine learning paradigm\nwhere a global model is trained in-situ across a large number of distributed\nedge devices. These systems are often comprised of millions of user devices and\nonly a subset of available devices can be used for training in each epoch.\nDesigning a device selection strategy is challenging, given that devices are\nhighly heterogeneous in both their system resources and training data. This\nheterogeneity makes device selection very crucial for timely model convergence\nand sufficient model accuracy. To tackle the FL client heterogeneity problem,\nvarious client selection algorithms have been developed, showing promising\nperformance improvement in terms of model coverage and accuracy. In this work,\nwe study the overhead of client selection algorithms in a large scale FL\nenvironment. Then we propose an efficient data distribution summary calculation\nalgorithm to reduce the overhead in a real-world large scale FL environment.\nThe evaluation shows that our proposed solution could achieve up to 30x\nreduction in data summary time, and up to 360x reduction in clustering time.\n', '  Federated Learning (FL) is a promising paradigm that offers significant\nadvancements in privacy-preserving, decentralized machine learning by enabling\ncollaborative training of models across distributed devices without\ncentralizing data. However, the practical deployment of FL systems faces a\nsignificant bottleneck: the communication overhead caused by frequently\nexchanging large model updates between numerous devices and a central server.\nThis communication inefficiency can hinder training speed, model performance,\nand the overall feasibility of real-world FL applications. In this survey, we\ninvestigate various strategies and advancements made in communication-efficient\nFL, highlighting their impact and potential to overcome the communication\nchallenges inherent in FL systems. Specifically, we define measures for\ncommunication efficiency, analyze sources of communication inefficiency in FL\nsystems, and provide a taxonomy and comprehensive review of state-of-the-art\ncommunication-efficient FL methods. Additionally, we discuss promising future\nresearch directions for enhancing the communication efficiency of FL systems.\nBy addressing the communication bottleneck, FL can be effectively applied and\nenable scalable and practical deployment across diverse applications that\nrequire privacy-preserving, decentralized machine learning, such as IoT,\nhealthcare, or finance.\n', '  Federated learning (FL), as an emerging collaborative learning paradigm, has\ngarnered significant attention due to its capacity to preserve privacy within\ndistributed learning systems. In these systems, clients collaboratively train a\nunified neural network model using their local datasets and share model\nparameters rather than raw data, enhancing privacy. Predominantly, FL systems\nare designed for mobile and edge computing environments where training\ntypically occurs over wireless networks. Consequently, as model sizes increase,\nthe conventional FL frameworks increasingly consume substantial communication\nresources. To address this challenge and improve communication efficiency, this\npaper introduces a novel hierarchical FL framework that integrates the benefits\nof clustered FL and model compression. We present an adaptive clustering\nalgorithm that identifies a core client and dynamically organizes clients into\nclusters. Furthermore, to enhance transmission efficiency, each core client\nimplements a local aggregation with compression (LC aggregation) algorithm\nafter collecting compressed models from other clients within the same cluster.\nSimulation results affirm that our proposed algorithms not only maintain\ncomparable predictive accuracy but also significantly reduce energy consumption\nrelative to existing FL mechanisms.\n']",Federated Learning for Distributed Systems,Federated Learning,Machine Learning and Data Privacy
21,21,121,21_graphnas_networks_graphs_subgraph,"['graphnas', 'networks', 'graphs', 'subgraph', 'graphstorm', 'nodes', 'graph', 'gnn', 'gnns', 'neural']","['graph', 'neural', 'networks', 'graphs', 'node', 'architecture', 'message', 'isomorphism', 'subgraph', 'training']","['  Subgraph isomorphism counting is an important problem on graphs, as many\ngraph-based tasks exploit recurring subgraph patterns. Classical methods\nusually boil down to a backtracking framework that needs to navigate a huge\nsearch space with prohibitive computational costs. Some recent studies resort\nto graph neural networks (GNNs) to learn a low-dimensional representation for\nboth the query and input graphs, in order to predict the number of subgraph\nisomorphisms on the input graph. However, typical GNNs employ a node-centric\nmessage passing scheme that receives and aggregates messages on nodes, which is\ninadequate in complex structure matching for isomorphism counting. Moreover, on\nan input graph, the space of possible query graphs is enormous, and different\nparts of the input graph will be triggered to match different queries. Thus,\nexpecting a fixed representation of the input graph to match diversely\nstructured query graphs is unrealistic. In this paper, we propose a novel GNN\ncalled Count-GNN for subgraph isomorphism counting, to deal with the above\nchallenges. At the edge level, given that an edge is an atomic unit of encoding\ngraph structures, we propose an edge-centric message passing scheme, where\nmessages on edges are propagated and aggregated based on the edge adjacency to\npreserve fine-grained structural information. At the graph level, we modulate\nthe input graph representation conditioned on the query, so that the input\ngraph can be adapted to each query individually to improve their matching.\nFinally, we conduct extensive experiments on a number of benchmark datasets to\ndemonstrate the superior performance of Count-GNN.\n', ""  GNN-to-MLP distillation aims to utilize knowledge distillation (KD) to learn\ncomputationally-efficient multi-layer perceptron (student MLP) on graph data by\nmimicking the output representations of teacher GNN. Existing methods mainly\nmake the MLP to mimic the GNN predictions over a few class labels. However, the\nclass space may not be expressive enough for covering numerous diverse local\ngraph structures, thus limiting the performance of knowledge transfer from GNN\nto MLP. To address this issue, we propose to learn a new powerful graph\nrepresentation space by directly labeling nodes' diverse local structures for\nGNN-to-MLP distillation. Specifically, we propose a variant of VQ-VAE to learn\na structure-aware tokenizer on graph data that can encode each node's local\nsubstructure as a discrete code. The discrete codes constitute a codebook as a\nnew graph representation space that is able to identify different local graph\nstructures of nodes with the corresponding code indices. Then, based on the\nlearned codebook, we propose a new distillation target, namely soft code\nassignments, to directly transfer the structural knowledge of each node from\nGNN to MLP. The resulting framework VQGraph achieves new state-of-the-art\nperformance on GNN-to-MLP distillation in both transductive and inductive\nsettings across seven graph datasets. We show that VQGraph with better\nperformance infers faster than GNNs by 828x, and also achieves accuracy\nimprovement over GNNs and stand-alone MLPs by 3.90% and 28.05% on average,\nrespectively. Code: https://github.com/YangLing0818/VQGraph.\n"", '  Graph Neural Networks (GNNs) are deep-learning architectures designed for\ngraph-type data, where understanding relationships among individual\nobservations is crucial. However, achieving promising GNN performance,\nespecially on unseen data, requires comprehensive hyperparameter tuning and\nmeticulous training. Unfortunately, these processes come with high\ncomputational costs and significant human effort. Additionally, conventional\nsearching algorithms such as grid search may result in overfitting on\nvalidation data, diminishing generalization accuracy. To tackle these\nchallenges, we propose a graph conditional latent diffusion framework\n(GNN-Diff) to generate high-performing GNNs directly by learning from\ncheckpoints saved during a light-tuning coarse search. Our method: (1)\nunleashes GNN training from heavy tuning and complex search space design; (2)\nproduces GNN parameters that outperform those obtained through comprehensive\ngrid search; and (3) establishes higher-quality generation for GNNs compared to\ndiffusion frameworks designed for general neural networks.\n']",Graph Neural Networks (GNNs) for Graph Data Analysis,Graph Neural Networks (GNNs) and Graph Data Analysis,Graph Representation Learning and Neural Networks
22,22,121,22_kernels_kernel_approximations_rkhs,"['kernels', 'kernel', 'approximations', 'rkhs', 'gaussian', 'hessian', 'approximation', 'neural', 'spectral', 'dimensional']","['kernel', 'kernels', 'approximation', 'gaussian', 'dimensional', 'functional', 'regression', 'spaces', 'functions', 'infinite']","['  The generalization error curve of certain kernel regression method aims at\ndetermining the exact order of generalization error with various source\ncondition, noise level and choice of the regularization parameter rather than\nthe minimax rate. In this work, under mild assumptions, we rigorously provide a\nfull characterization of the generalization error curves of the kernel gradient\ndescent method (and a large class of analytic spectral algorithms) in kernel\nregression. Consequently, we could sharpen the near inconsistency of kernel\ninterpolation and clarify the saturation effects of kernel regression\nalgorithms with higher qualification, etc. Thanks to the neural tangent kernel\ntheory, these results greatly improve our understanding of the generalization\nbehavior of training the wide neural networks. A novel technical contribution,\nthe analytic functional argument, might be of independent interest.\n', '  Various methods in statistical learning build on kernels considered in\nreproducing kernel Hilbert spaces. In applications, the kernel is often\nselected based on characteristics of the problem and the data. This kernel is\nthen employed to infer response variables at points, where no explanatory data\nwere observed. The data considered here are located in compact sets in higher\ndimensions and the paper addresses approximations of the kernel itself. The new\napproach considers Taylor series approximations of radial kernel functions. For\nthe Gauss kernel on the unit cube, the paper establishes an upper bound of the\nassociated eigenfunctions, which grows only polynomially with respect to the\nindex. The novel approach substantiates smaller regularization parameters than\nconsidered in the literature, overall leading to better approximations. This\nimprovement confirms low rank approximation methods such as the Nystr\\""om\nmethod.\n', '  For the past 30 years or so, machine learning has stimulated a great deal of\nresearch in the study of approximation capabilities (expressive power) of a\nmultitude of processes, such as approximation by shallow or deep neural\nnetworks, radial basis function networks, and a variety of kernel based\nmethods. Motivated by applications such as invariant learning, transfer\nlearning, and synthetic aperture radar imaging, we initiate in this paper a\ngeneral approach to study the approximation capabilities of kernel based\nnetworks using non-symmetric kernels. While singular value decomposition is a\nnatural instinct to study such kernels, we consider a more general approach to\ninclude the use of a family of kernels, such as generalized translation\nnetworks (which include neural networks and translation invariant kernels as\nspecial cases) and rotated zonal function kernels. Naturally, unlike\ntraditional kernel based approximation, we cannot require the kernels to be\npositive definite. In particular, we obtain estimates on the accuracy of\nuniform approximation of functions in a ($L^2$)-Sobolev class by ReLU$^r$\nnetworks when $r$ is not necessarily an integer. Our general results apply to\nthe approximation of functions with small smoothness compared to the dimension\nof the input space.\n']",Kernel Methods for Regression and Approximation,Kernel Methods for Machine Learning and Statistical Analysis,Machine Learning and Artificial Intelligence
23,23,117,23_parallelization_fpgas_cnns_fpga,"['parallelization', 'fpgas', 'cnns', 'fpga', 'throughput', 'accelerator', 'accelerators', 'hardware', 'memory', 'microarchitecture']","['hardware', 'parallelism', 'memory', 'accelerators', 'latency', 'device', 'energy', 'devices', 'throughput', 'inference']","['  The relentless advancement of artificial intelligence (AI) and machine\nlearning (ML) applications necessitates the development of specialized hardware\naccelerators capable of handling the increasing complexity and computational\ndemands. Traditional computing architectures, based on the von Neumann model,\nare being outstripped by the requirements of contemporary AI/ML algorithms,\nleading to a surge in the creation of accelerators like the Graphcore\nIntelligence Processing Unit (IPU), Sambanova Reconfigurable Dataflow Unit\n(RDU), and enhanced GPU platforms. These hardware accelerators are\ncharacterized by their innovative data-flow architectures and other design\noptimizations that promise to deliver superior performance and energy\nefficiency for AI/ML tasks.\n  This research provides a preliminary evaluation and comparison of these\ncommercial AI/ML accelerators, delving into their hardware and software design\nfeatures to discern their strengths and unique capabilities. By conducting a\nseries of benchmark evaluations on common DNN operators and other AI/ML\nworkloads, we aim to illuminate the advantages of data-flow architectures over\nconventional processor designs and offer insights into the performance\ntrade-offs of each platform. The findings from our study will serve as a\nvaluable reference for the design and performance expectations of research\nprototypes, thereby facilitating the development of next-generation hardware\naccelerators tailored for the ever-evolving landscape of AI/ML applications.\nThrough this analysis, we aspire to contribute to the broader understanding of\ncurrent accelerator technologies and to provide guidance for future innovations\nin the field.\n', ""  With the recent growth in demand for large-scale deep neural networks,\ncompute in-memory (CiM) has come up as a prominent solution to alleviate\nbandwidth and on-chip interconnect bottlenecks that constrain Von-Neuman\narchitectures. However, the construction of CiM hardware poses a challenge as\nany specific memory hierarchy in terms of cache sizes and memory bandwidth at\ndifferent interfaces may not be ideally matched to any neural network's\nattributes such as tensor dimension and arithmetic intensity, thus leading to\nsuboptimal and under-performing systems. Despite the success of neural\narchitecture search (NAS) techniques in yielding efficient sub-networks for a\ngiven hardware metric budget (e.g., DNN execution time or latency), it assumes\nthe hardware configuration to be frozen, often yielding sub-optimal\nsub-networks for a given budget. In this paper, we present CiMNet, a framework\nthat jointly searches for optimal sub-networks and hardware configurations for\nCiM architectures creating a Pareto optimal frontier of downstream task\naccuracy and execution metrics (e.g., latency). The proposed framework can\ncomprehend the complex interplay between a sub-network's performance and the\nCiM hardware configuration choices including bandwidth, processing element\nsize, and memory size. Exhaustive experiments on different model architectures\nfrom both CNN and Transformer families demonstrate the efficacy of the CiMNet\nin finding co-optimized sub-networks and CiM hardware configurations.\nSpecifically, for similar ImageNet classification accuracy as baseline ViT-B,\noptimizing only the model architecture increases performance (or reduces\nworkload execution time) by 1.7x while optimizing for both the model\narchitecture and hardware configuration increases it by 3.1x.\n"", '  Deep neural networks (DNNs) have been widely used in many artificial\nintelligence (AI) tasks. However, deploying them brings significant challenges\ndue to the huge cost of memory, energy, and computation. To address these\nchallenges, researchers have developed various model compression techniques\nsuch as model quantization and model pruning. Recently, there has been a surge\nin research of compression methods to achieve model efficiency while retaining\nthe performance. Furthermore, more and more works focus on customizing the DNN\nhardware accelerators to better leverage the model compression techniques. In\naddition to efficiency, preserving security and privacy is critical for\ndeploying DNNs. However, the vast and diverse body of related works can be\noverwhelming. This inspires us to conduct a comprehensive survey on recent\nresearch toward the goal of high-performance, cost-efficient, and safe\ndeployment of DNNs. Our survey first covers the mainstream model compression\ntechniques such as model quantization, model pruning, knowledge distillation,\nand optimizations of non-linear operations. We then introduce recent advances\nin designing hardware accelerators that can adapt to efficient model\ncompression approaches. Additionally, we discuss how homomorphic encryption can\nbe integrated to secure DNN deployment. Finally, we discuss several issues,\nsuch as hardware evaluation, generalization, and integration of various\ncompression approaches. Overall, we aim to provide a big picture of efficient\nDNNs, from algorithm to hardware accelerators and security perspectives.\n']",AI/ML Hardware Accelerators and Optimizations,Hardware Design and Acceleration for AI and ML Applications,Artificial Intelligence and Machine Learning Hardware Acceleration and Optimization
24,24,117,24_cloud_edge_federated_networks,"['cloud', 'edge', 'federated', 'networks', 'servers', 'bandwidth', 'iot', 'serverless', 'network', 'distributed']","['edge', 'devices', 'federated', 'resource', 'computing', 'communication', 'server', 'device', 'resources', 'wireless']","['  Federated learning (FL) enables edge nodes to collaboratively contribute to\nconstructing a global model without sharing their data. This is accomplished by\ndevices computing local, private model updates that are then aggregated by a\nserver. However, computational resource constraints and network communication\ncan become a severe bottleneck for larger model sizes typical for deep learning\napplications. Edge nodes tend to have limited hardware resources (RAM, CPU),\nand the network bandwidth and reliability at the edge is a concern for scaling\nfederated fleet applications. In this paper, we propose and evaluate a FL\nstrategy inspired by transfer learning in order to reduce resource utilization\non devices, as well as the load on the server and network in each global\ntraining round. For each local model update, we randomly select layers to\ntrain, freezing the remaining part of the model. In doing so, we can reduce\nboth server load and communication costs per round by excluding all untrained\nlayer weights from being transferred to the server. The goal of this study is\nto empirically explore the potential trade-off between resource utilization on\ndevices and global model convergence under the proposed strategy. We implement\nthe approach using the federated learning framework FEDn. A number of\nexperiments were carried out over different datasets (CIFAR-10, CASA, and\nIMDB), performing different tasks using different deep-learning model\narchitectures. Our results show that training the model partially can\naccelerate the training process, efficiently utilizes resources on-device, and\nreduce the data transmission by around 75% and 53% when we train 25%, and 50%\nof the model layers, respectively, without harming the resulting global model\naccuracy.\n', '  Federated Learning (FL) plays a critical role in distributed systems. In\nthese systems, data privacy and confidentiality hold paramount importance,\nparticularly within edge-based data processing systems such as IoT devices\ndeployed in smart homes. FL emerges as a privacy-enforcing sub-domain of\nmachine learning that enables model training on client devices, eliminating the\nnecessity to share private data with a central server. While existing research\nhas predominantly addressed challenges pertaining to data heterogeneity, there\nremains a current gap in addressing issues such as varying device capabilities\nand efficient communication. These unaddressed issues raise a number of\nimplications in resource-constrained environments. In particular, the practical\nimplementation of FL-based IoT or edge systems is extremely inefficient. In\nthis paper, we propose ""Resource-Efficient Federated Training Framework for\nHeterogeneous and Resource-Constrained Environments (REFT),"" a novel approach\nspecifically devised to address these challenges in resource-limited devices.\nOur proposed method uses Variable Pruning to optimize resource utilization by\nadapting pruning strategies to the computational capabilities of each client.\nFurthermore, our proposed REFT technique employs knowledge distillation to\nminimize the need for continuous bidirectional client-server communication.\nThis achieves a significant reduction in communication bandwidth, thereby\nenhancing the overall resource efficiency. We conduct experiments for an image\nclassification task, and the results demonstrate the effectiveness of our\napproach in resource-limited settings. Our technique not only preserves data\nprivacy and performance standards but also accommodates heterogeneous model\narchitectures, facilitating the participation of a broader array of diverse\nclient devices in the training process, all while consuming minimal bandwidth.\n', '  To enable large-scale and efficient deployment of artificial intelligence\n(AI), the combination of AI and edge computing has spawned Edge Intelligence,\nwhich leverages the computing and communication capabilities of end devices and\nedge servers to process data closer to where it is generated. A key technology\nfor edge intelligence is the privacy-protecting machine learning paradigm known\nas Federated Learning (FL), which enables data owners to train models without\nhaving to transfer raw data to third-party servers. However, FL networks are\nexpected to involve thousands of heterogeneous distributed devices. As a\nresult, communication efficiency remains a key bottleneck. To reduce node\nfailures and device exits, a Hierarchical Federated Learning (HFL) framework is\nproposed, where a designated cluster leader supports the data owner through\nintermediate model aggregation. Therefore, based on the improvement of edge\nserver resource utilization, this paper can effectively make up for the\nlimitation of cache capacity. In order to mitigate the impact of soft clicks on\nthe quality of user experience (QoE), the authors model the user QoE as a\ncomprehensive system cost. To solve the formulaic problem, the authors propose\na decentralized caching algorithm with federated deep reinforcement learning\n(DRL) and federated learning (FL), where multiple agents learn and make\ndecisions independently\n']",Federated Learning for Edge Computing and IoT,Federated Learning,Machine Learning and Data Privacy
25,25,116,25_federated_distributed_learning_synchronization,"['federated', 'distributed', 'learning', 'synchronization', 'adaptive', 'sgd', 'stochastic', 'centralized', 'asynchronous', 'gradient']","['communication', 'federated', 'convergence', 'clients', 'decentralized', 'client', 'local', 'server', 'convex', 'heterogeneity']","['  Federated Learning (FL) is a distributed machine learning paradigm that\nallows clients to train models on their data while preserving their privacy. FL\nalgorithms, such as Federated Averaging (FedAvg) and its variants, have been\nshown to converge well in many scenarios. However, these methods require\nclients to upload their local updates to the server in a synchronous manner,\nwhich can be slow and unreliable in realistic FL settings. To address this\nissue, researchers have developed asynchronous FL methods that allow clients to\ncontinue training on their local data using a stale global model. However, most\nof these methods simply aggregate all of the received updates without\nconsidering their relative contributions, which can slow down convergence. In\nthis paper, we propose a contribution-aware asynchronous FL method that takes\ninto account the staleness and statistical heterogeneity of the received\nupdates. Our method dynamically adjusts the contribution of each update based\non these factors, which can speed up convergence compared to existing methods.\n', ""  Federated learning (FL) algorithms usually sample a fraction of clients in\neach round (partial participation) when the number of participants is large and\nthe server's communication bandwidth is limited. Recent works on the\nconvergence analysis of FL have focused on unbiased client sampling, e.g.,\nsampling uniformly at random, which suffers from slow wall-clock time for\nconvergence due to high degrees of system heterogeneity and statistical\nheterogeneity. This paper aims to design an adaptive client sampling algorithm\nfor FL over wireless networks that tackles both system and statistical\nheterogeneity to minimize the wall-clock convergence time. We obtain a new\ntractable convergence bound for FL algorithms with arbitrary client sampling\nprobability. Based on the bound, we analytically establish the relationship\nbetween the total learning time and sampling probability with an adaptive\nbandwidth allocation scheme, which results in a non-convex optimization\nproblem. We design an efficient algorithm for learning the unknown parameters\nin the convergence bound and develop a low-complexity algorithm to\napproximately solve the non-convex problem. Our solution reveals the impact of\nsystem and statistical heterogeneity parameters on the optimal client sampling\ndesign. Moreover, our solution shows that as the number of sampled clients\nincreases, the total convergence time first decreases and then increases\nbecause a larger sampling number reduces the number of rounds for convergence\nbut results in a longer expected time per-round due to limited wireless\nbandwidth. Experimental results from both hardware prototype and simulation\ndemonstrate that our proposed sampling scheme significantly reduces the\nconvergence time compared to several baseline sampling schemes.\n"", '  Federated learning (FL) was recently proposed to securely train models with\ndata held over multiple locations (""clients"") under the coordination of a\ncentral server. Two major challenges hindering the performance of FL algorithms\nare long training times caused by straggling clients, and a decline in model\naccuracy under non-iid local data distributions (""client drift""). In this work,\nwe propose and analyze Asynchronous Exact Averaging (AREA), a new stochastic\n(sub)gradient algorithm that utilizes asynchronous communication to speed up\nconvergence and enhance scalability, and employs client memory to correct the\nclient drift caused by variations in client update frequencies. Moreover, AREA\nis, to the best of our knowledge, the first method that is guaranteed to\nconverge under arbitrarily long delays, without the use of delay-adaptive\nstepsizes, and (i) for strongly convex, smooth functions, asymptotically\nconverges to an error neighborhood whose size depends only on the variance of\nthe stochastic gradients used with respect to the number of iterations, and\n(ii) for convex, non-smooth functions, matches the convergence rate of the\ncentralized stochastic subgradient method up to a constant factor, which\ndepends on the average of the individual client update frequencies instead of\ntheir minimum (or maximum). Our numerical results validate our theoretical\nanalysis and indicate AREA outperforms state-of-the-art methods when local data\nare highly non-iid, especially as the number of clients grows.\n']",Federated Learning with Asynchronous and Adaptive Methods,Federated Learning,Machine Learning and Data Privacy
26,26,114,26_prognosis_lung_cancer_ai,"['prognosis', 'lung', 'cancer', 'ai', 'cnn', 'predict', 'radiology', 'oncology', 'prediction', 'melanoma']","['cancer', 'clinical', 'diagnosis', 'medical', 'patients', 'treatment', 'chest', 'patient', 'imaging', 'survival']","[""  Accurately predicting the survival rate of cancer patients is crucial for\naiding clinicians in planning appropriate treatment, reducing cancer-related\nmedical expenses, and significantly enhancing patients' quality of life.\nMultimodal prediction of cancer patient survival offers a more comprehensive\nand precise approach. However, existing methods still grapple with challenges\nrelated to missing multimodal data and information interaction within\nmodalities. This paper introduces SELECTOR, a heterogeneous graph-aware network\nbased on convolutional mask encoders for robust multimodal prediction of cancer\npatient survival. SELECTOR comprises feature edge reconstruction, convolutional\nmask encoder, feature cross-fusion, and multimodal survival prediction modules.\nInitially, we construct a multimodal heterogeneous graph and employ the\nmeta-path method for feature edge reconstruction, ensuring comprehensive\nincorporation of feature information from graph edges and effective embedding\nof nodes. To mitigate the impact of missing features within the modality on\nprediction accuracy, we devised a convolutional masked autoencoder (CMAE) to\nprocess the heterogeneous graph post-feature reconstruction. Subsequently, the\nfeature cross-fusion module facilitates communication between modalities,\nensuring that output features encompass all features of the modality and\nrelevant information from other modalities. Extensive experiments and analysis\non six cancer datasets from TCGA demonstrate that our method significantly\noutperforms state-of-the-art methods in both modality-missing and\nintra-modality information-confirmed cases. Our codes are made available at\nhttps://github.com/panliangrui/Selector.\n"", ""  The COVID-19 pandemic has strained global public health, necessitating\naccurate diagnosis and intervention to control disease spread and reduce\nmortality rates. This paper introduces an interpretable deep survival\nprediction model designed specifically for improved understanding and trust in\nCOVID-19 prognosis using chest X-ray (CXR) images. By integrating a large-scale\npretrained image encoder, Risk-specific Grad-CAM, and anatomical region\ndetection techniques, our approach produces regional interpretable outcomes\nthat effectively capture essential disease features while focusing on rare but\ncritical abnormal regions. Our model's predictive results provide enhanced\nclarity and transparency through risk area localization, enabling clinicians to\nmake informed decisions regarding COVID-19 diagnosis with better understanding\nof prognostic insights. We evaluate the proposed method on a multi-center\nsurvival dataset and demonstrate its effectiveness via quantitative and\nqualitative assessments, achieving superior C-indexes (0.764 and 0.727) and\ntime-dependent AUCs (0.799 and 0.691). These results suggest that our\nexplainable deep survival prediction model surpasses traditional survival\nanalysis methods in risk prediction, improving interpretability for clinical\ndecision making and enhancing AI system trustworthiness.\n"", ""  At present, the incidence and fatality rate of lung cancer in China rank\nfirst among all malignant tumors. Despite the continuous development and\nimprovement of China's medical level, the overall 5-year survival rate of lung\ncancer patients is still lower than 20% and is staged. A number of studies have\nconfirmed that early diagnosis and treatment of early stage lung cancer is of\ngreat significance to improve the prognosis of patients. In recent years,\nartificial intelligence technology has gradually begun to be applied in\noncology. ai is used in cancer screening, clinical diagnosis, radiation therapy\n(image acquisition, at-risk organ segmentation, image calibration and delivery)\nand other aspects of rapid development. However, whether medical ai can be\nsocialized depends on the public's attitude and acceptance to a certain extent.\nHowever, at present, there are few studies on the diagnosis of early lung\ncancer by AI technology combined with SCT scanning. In view of this, this study\napplied the combined method in early lung cancer screening, aiming to find a\nsafe and efficient screening mode and provide a reference for clinical\ndiagnosis and treatment.\n""]",Cancer Prognosis Prediction using AI and Imaging,Artificial Intelligence in Medical Diagnosis and Prognosis,Artificial Intelligence Applications and Implications
27,27,110,27_sentiment_corpus_tweets_linguistic,"['sentiment', 'corpus', 'tweets', 'linguistic', 'annotators', 'annotations', 'twitter', 'lexicon', 'annotation', 'textual']","['news', 'sentiment', 'media', 'social', 'political', 'emoji', 'languages', 'tweets', 'emojis', 'headlines']","[""  The paper presents a new training dataset of sentences in 7 languages,\nmanually annotated for sentiment, which are used in a series of experiments\nfocused on training a robust sentiment identifier for parliamentary\nproceedings. The paper additionally introduces the first domain-specific\nmultilingual transformer language model for political science applications,\nwhich was additionally pre-trained on 1.72 billion words from parliamentary\nproceedings of 27 European parliaments. We present experiments demonstrating\nhow the additional pre-training on parliamentary data can significantly improve\nthe model downstream performance, in our case, sentiment identification in\nparliamentary proceedings. We further show that our multilingual model performs\nvery well on languages not seen during fine-tuning, and that additional\nfine-tuning data from other languages significantly improves the target\nparliament's results. The paper makes an important contribution to multiple\ndisciplines inside the social sciences, and bridges them with computer science\nand computational linguistics. Lastly, the resulting fine-tuned language model\nsets up a more robust approach to sentiment analysis of political texts across\nlanguages, which allows scholars to study political sentiment from a\ncomparative perspective using standardized tools and techniques.\n"", '  Most previous research on moral frames has focused on social media short\ntexts, little work has explored moral sentiment within news articles. In news\narticles, authors often express their opinions or political stance through\nmoral judgment towards events, specifically whether the event is right or wrong\naccording to social moral rules. This paper initiates a new task to understand\nmoral opinions towards events in news articles. We have created a new dataset,\nEMONA, and annotated event-level moral opinions in news articles. This dataset\nconsists of 400 news articles containing over 10k sentences and 45k events,\namong which 9,613 events received moral foundation labels. Extracting event\nmorality is a challenging task, as moral judgment towards events can be very\nimplicit. Baseline models were built for event moral identification and\nclassification. In addition, we also conduct extrinsic evaluations to integrate\nevent-level moral opinions into three downstream tasks. The statistical\nanalysis and experiments show that moral opinions of events can serve as\ninformative features for identifying ideological bias or subjective events.\n', '  Understanding the writing frame of news articles is vital for addressing\nsocial issues, and thus has attracted notable attention in the fields of\ncommunication studies. Yet, assessing such news article frames remains a\nchallenge due to the absence of a concrete and unified standard dataset that\nconsiders the comprehensive nuances within news content.\n  To address this gap, we introduce an extended version of a large labeled news\narticle dataset with 16,687 new labeled pairs. Leveraging the pairwise\ncomparison of news articles, our method frees the work of manual identification\nof frame classes in traditional news frame analysis studies. Overall we\nintroduce the most extensive cross-lingual news article similarity dataset\navailable to date with 26,555 labeled news article pairs across 10 languages.\nEach data point has been meticulously annotated according to a codebook\ndetailing eight critical aspects of news content, under a human-in-the-loop\nframework. Application examples demonstrate its potential in unearthing country\ncommunities within global news coverage, exposing media bias among news\noutlets, and quantifying the factors related to news creation. We envision that\nthis news similarity dataset will broaden our understanding of the media\necosystem in terms of news coverage of events and perspectives across\ncountries, locations, languages, and other social constructs. By doing so, it\ncan catalyze advancements in social science research and applied methodologies,\nthereby exerting a profound impact on our society.\n']",Sentiment Analysis in Textual Data,Sentiment Analysis,Business and Marketing Analytics
28,28,110,28_ranking_evaluations_rankings_evaluation,"['ranking', 'evaluations', 'rankings', 'evaluation', 'language', 'assessment', 'responses', 'bias', 'questioner', 'prompts']","['evaluation', 'feedback', 'responses', 'questions', 'human', 'ranking', 'choice', 'language', 'rankings', 'pairwise']","[""  Multiple-choice questions (MCQ) are frequently used to assess large language\nmodels (LLMs). Typically, an LLM is given a question and selects the answer\ndeemed most probable after adjustments for factors like length. Unfortunately,\nLLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to\ninherent biases of priori unbalanced probabilities, influencing the prediction\nof answers based on these IDs. Previous research has introduced methods to\nreduce this ''selection bias'' by simply permutating options on a few test\nsamples and applying to new ones. Another problem of MCQ is the lottery ticket\nchoice by ''random guessing''. The LLM does not learn particular knowledge, but\nthe option is guessed correctly. This situation is especially serious for those\nsmall-scale LLMs. To address them, a more thorough approach involves shifting\nfrom MCQ to open-style questions, which can fundamentally eliminate selection\nbias and random guessing issues. However, transitioning causes its own set of\nchallenges in (1) identifying suitable open-style questions and (2) validating\nthe correctness of LLM open-style responses against human-annotated\nground-truths. This work aims to tackle these significant difficulties, and\nestablish a new LLM evaluation benchmark through entirely open-style questions.\nConsequently, we introduce the Open-LLM-Leaderboard to track various LLMs'\nperformance and reflect true capability of them, such as GPT-4o/4/3.5, Claude\n3, Gemini, etc. Our code and dataset are available at\nhttps://github.com/VILA-Lab/Open-LLM-Leaderboard.\n"", '  The rapid evolution of language models has necessitated the development of\nmore challenging benchmarks. Current static benchmarks often struggle to\nconsistently distinguish between the capabilities of different models and fail\nto align with real-world user preferences. On the other hand, live\ncrowd-sourced platforms like the Chatbot Arena collect a wide range of natural\nprompts and user feedback. However, these prompts vary in sophistication and\nthe feedback cannot be applied offline to new models. In order to ensure that\nbenchmarks keep up with the pace of LLM development, we address how one can\nevaluate benchmarks on their ability to confidently separate models and their\nalignment with human preference. Under these principles, we developed\nBenchBuilder, a living benchmark that filters high-quality prompts from live\ndata sources to enable offline evaluation on fresh, challenging prompts.\nBenchBuilder identifies seven indicators of a high-quality prompt, such as the\nrequirement for domain knowledge, and utilizes an LLM annotator to select a\nhigh-quality subset of prompts from various topic clusters. The LLM evaluation\nprocess employs an LLM judge to ensure a fully automated, high-quality, and\nconstantly updating benchmark. We apply BenchBuilder on prompts from the\nChatbot Arena to create Arena-Hard-Auto v0.1: 500 challenging user prompts from\na wide range of tasks. Arena-Hard-Auto v0.1 offers 3x tighter confidence\nintervals than MT-Bench and achieves a state-of-the-art 89.1% agreement with\nhuman preference rankings, all at a cost of only $25 and without human\nlabelers. The BenchBuilder pipeline enhances evaluation benchmarks and provides\na valuable tool for developers, enabling them to extract high-quality\nbenchmarks from extensive data with minimal effort.\n', '  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious applications, fundamentally reshaping the landscape of natural language\nprocessing (NLP) research. However, recent evaluation frameworks often rely on\nthe output probabilities of LLMs for predictions, primarily due to\ncomputational constraints, diverging from real-world LLM usage scenarios. While\nwidely employed, the efficacy of these probability-based evaluation strategies\nremains an open research question. This study aims to scrutinize the validity\nof such probability-based evaluation methods within the context of using LLMs\nfor Multiple Choice Questions (MCQs), highlighting their inherent limitations.\nOur empirical investigation reveals that the prevalent probability-based\nevaluation method inadequately aligns with generation-based prediction.\nFurthermore, current evaluation frameworks typically assess LLMs through\npredictive tasks based on output probabilities rather than directly generating\nresponses, owing to computational limitations. We illustrate that these\nprobability-based approaches do not effectively correspond with generative\npredictions. The outcomes of our study can enhance the understanding of LLM\nevaluation methodologies and provide insights for future research in this\ndomain.\n']",Evaluating Large Language Models,Evaluating Large Language Models,Large Language Models
28,28,110,28_ranking_evaluations_rankings_evaluation,"['ranking', 'evaluations', 'rankings', 'evaluation', 'language', 'assessment', 'responses', 'bias', 'questioner', 'prompts']","['evaluation', 'feedback', 'responses', 'questions', 'human', 'ranking', 'choice', 'language', 'rankings', 'pairwise']","[""  Multiple-choice questions (MCQ) are frequently used to assess large language\nmodels (LLMs). Typically, an LLM is given a question and selects the answer\ndeemed most probable after adjustments for factors like length. Unfortunately,\nLLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to\ninherent biases of priori unbalanced probabilities, influencing the prediction\nof answers based on these IDs. Previous research has introduced methods to\nreduce this ''selection bias'' by simply permutating options on a few test\nsamples and applying to new ones. Another problem of MCQ is the lottery ticket\nchoice by ''random guessing''. The LLM does not learn particular knowledge, but\nthe option is guessed correctly. This situation is especially serious for those\nsmall-scale LLMs. To address them, a more thorough approach involves shifting\nfrom MCQ to open-style questions, which can fundamentally eliminate selection\nbias and random guessing issues. However, transitioning causes its own set of\nchallenges in (1) identifying suitable open-style questions and (2) validating\nthe correctness of LLM open-style responses against human-annotated\nground-truths. This work aims to tackle these significant difficulties, and\nestablish a new LLM evaluation benchmark through entirely open-style questions.\nConsequently, we introduce the Open-LLM-Leaderboard to track various LLMs'\nperformance and reflect true capability of them, such as GPT-4o/4/3.5, Claude\n3, Gemini, etc. Our code and dataset are available at\nhttps://github.com/VILA-Lab/Open-LLM-Leaderboard.\n"", '  The rapid evolution of language models has necessitated the development of\nmore challenging benchmarks. Current static benchmarks often struggle to\nconsistently distinguish between the capabilities of different models and fail\nto align with real-world user preferences. On the other hand, live\ncrowd-sourced platforms like the Chatbot Arena collect a wide range of natural\nprompts and user feedback. However, these prompts vary in sophistication and\nthe feedback cannot be applied offline to new models. In order to ensure that\nbenchmarks keep up with the pace of LLM development, we address how one can\nevaluate benchmarks on their ability to confidently separate models and their\nalignment with human preference. Under these principles, we developed\nBenchBuilder, a living benchmark that filters high-quality prompts from live\ndata sources to enable offline evaluation on fresh, challenging prompts.\nBenchBuilder identifies seven indicators of a high-quality prompt, such as the\nrequirement for domain knowledge, and utilizes an LLM annotator to select a\nhigh-quality subset of prompts from various topic clusters. The LLM evaluation\nprocess employs an LLM judge to ensure a fully automated, high-quality, and\nconstantly updating benchmark. We apply BenchBuilder on prompts from the\nChatbot Arena to create Arena-Hard-Auto v0.1: 500 challenging user prompts from\na wide range of tasks. Arena-Hard-Auto v0.1 offers 3x tighter confidence\nintervals than MT-Bench and achieves a state-of-the-art 89.1% agreement with\nhuman preference rankings, all at a cost of only $25 and without human\nlabelers. The BenchBuilder pipeline enhances evaluation benchmarks and provides\na valuable tool for developers, enabling them to extract high-quality\nbenchmarks from extensive data with minimal effort.\n', '  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious applications, fundamentally reshaping the landscape of natural language\nprocessing (NLP) research. However, recent evaluation frameworks often rely on\nthe output probabilities of LLMs for predictions, primarily due to\ncomputational constraints, diverging from real-world LLM usage scenarios. While\nwidely employed, the efficacy of these probability-based evaluation strategies\nremains an open research question. This study aims to scrutinize the validity\nof such probability-based evaluation methods within the context of using LLMs\nfor Multiple Choice Questions (MCQs), highlighting their inherent limitations.\nOur empirical investigation reveals that the prevalent probability-based\nevaluation method inadequately aligns with generation-based prediction.\nFurthermore, current evaluation frameworks typically assess LLMs through\npredictive tasks based on output probabilities rather than directly generating\nresponses, owing to computational limitations. We illustrate that these\nprobability-based approaches do not effectively correspond with generative\npredictions. The outcomes of our study can enhance the understanding of LLM\nevaluation methodologies and provide insights for future research in this\ndomain.\n']",Evaluating Large Language Models,Evaluating Large Language Models,Large Language Models
29,29,110,29_adversarial_teaming_threat_unsafe,"['adversarial', 'teaming', 'threat', 'unsafe', 'attacker', 'attackers', 'attacking', 'vulnerability', 'vulnerabilities', 'attacks']","['safety', 'red', 'teaming', 'adversarial', 'attack', 'vulnerabilities', 'malicious', 'harmful', 'attacks', 'prompts']","[""  AI safety training and red-teaming of large language models (LLMs) are\nmeasures to mitigate the generation of unsafe content. Our work exposes the\ninherent cross-lingual vulnerability of these safety mechanisms, resulting from\nthe linguistic inequality of safety training data, by successfully\ncircumventing GPT-4's safeguard through translating unsafe English inputs into\nlow-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe\ntranslated inputs and provides actionable items that can get the users towards\ntheir harmful goals 79% of the time, which is on par with or even surpassing\nstate-of-the-art jailbreaking attacks. Other high-/mid-resource languages have\nsignificantly lower attack success rate, which suggests that the cross-lingual\nvulnerability mainly applies to low-resource languages. Previously, limited\ntraining on low-resource languages primarily affects speakers of those\nlanguages, causing technological disparities. However, our work highlights a\ncrucial shift: this deficiency now poses a risk to all LLMs users. Publicly\navailable translation APIs enable anyone to exploit LLMs' safety\nvulnerabilities. Therefore, our work calls for a more holistic red-teaming\nefforts to develop robust multilingual safeguards with wide language coverage.\n"", '  Manual Red teaming is a commonly-used method to identify vulnerabilities in\nlarge language models (LLMs), which, is costly and unscalable. In contrast,\nautomated red teaming uses a Red LLM to automatically generate adversarial\nprompts to the Target LLM, offering a scalable way for safety vulnerability\ndetection. However, the difficulty of building a powerful automated Red LLM\nlies in the fact that the safety vulnerabilities of the Target LLM are\ndynamically changing with the evolution of the Target LLM. To mitigate this\nissue, we propose a Deep Adversarial Automated Red Teaming (DART) framework in\nwhich the Red LLM and Target LLM are deeply and dynamically interacting with\neach other in an iterative manner. In each iteration, in order to generate\nsuccessful attacks as many as possible, the Red LLM not only takes into account\nthe responses from the Target LLM, but also adversarially adjust its attacking\ndirections by monitoring the global diversity of generated attacks across\nmultiple iterations. Simultaneously, to explore dynamically changing safety\nvulnerabilities of the Target LLM, we allow the Target LLM to enhance its\nsafety via an active learning based data selection mechanism. Experimential\nresults demonstrate that DART significantly reduces the safety risk of the\ntarget LLM. For human evaluation on Anthropic Harmless dataset, compared to the\ninstruction-tuning target LLM, DART eliminates the violation risks by 53.4\\%.\nWe will release the datasets and codes of DART soon.\n', '  Red-teaming, or identifying prompts that elicit harmful responses, is a\ncritical step in ensuring the safe and responsible deployment of large language\nmodels (LLMs). Developing effective protection against many modes of attack\nprompts requires discovering diverse attacks. Automated red-teaming typically\nuses reinforcement learning to fine-tune an attacker language model to generate\nprompts that elicit undesirable responses from a target LLM, as measured, for\nexample, by an auxiliary toxicity classifier. We show that even with explicit\nregularization to favor novelty and diversity, existing approaches suffer from\nmode collapse or fail to generate effective attacks. As a flexible and\nprobabilistically principled alternative, we propose to use GFlowNet\nfine-tuning, followed by a secondary smoothing phase, to train the attacker\nmodel to generate diverse and effective attack prompts. We find that the\nattacks generated by our method are effective against a wide range of target\nLLMs, both with and without safety tuning, and transfer well between target\nLLMs. Finally, we demonstrate that models safety-tuned using a dataset of\nred-teaming prompts generated by our method are robust to attacks from other\nRL-based red-teaming approaches.\n']",Adversarial Attacks on Large Language Models,Adversarial Attacks and Defenses in AI and Machine Learning Models,Adversarial Attacks and Vulnerabilities in AI and Machine Learning
29,29,110,29_adversarial_teaming_threat_unsafe,"['adversarial', 'teaming', 'threat', 'unsafe', 'attacker', 'attackers', 'attacking', 'vulnerability', 'vulnerabilities', 'attacks']","['safety', 'red', 'teaming', 'adversarial', 'attack', 'vulnerabilities', 'malicious', 'harmful', 'attacks', 'prompts']","[""  AI safety training and red-teaming of large language models (LLMs) are\nmeasures to mitigate the generation of unsafe content. Our work exposes the\ninherent cross-lingual vulnerability of these safety mechanisms, resulting from\nthe linguistic inequality of safety training data, by successfully\ncircumventing GPT-4's safeguard through translating unsafe English inputs into\nlow-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe\ntranslated inputs and provides actionable items that can get the users towards\ntheir harmful goals 79% of the time, which is on par with or even surpassing\nstate-of-the-art jailbreaking attacks. Other high-/mid-resource languages have\nsignificantly lower attack success rate, which suggests that the cross-lingual\nvulnerability mainly applies to low-resource languages. Previously, limited\ntraining on low-resource languages primarily affects speakers of those\nlanguages, causing technological disparities. However, our work highlights a\ncrucial shift: this deficiency now poses a risk to all LLMs users. Publicly\navailable translation APIs enable anyone to exploit LLMs' safety\nvulnerabilities. Therefore, our work calls for a more holistic red-teaming\nefforts to develop robust multilingual safeguards with wide language coverage.\n"", '  Manual Red teaming is a commonly-used method to identify vulnerabilities in\nlarge language models (LLMs), which, is costly and unscalable. In contrast,\nautomated red teaming uses a Red LLM to automatically generate adversarial\nprompts to the Target LLM, offering a scalable way for safety vulnerability\ndetection. However, the difficulty of building a powerful automated Red LLM\nlies in the fact that the safety vulnerabilities of the Target LLM are\ndynamically changing with the evolution of the Target LLM. To mitigate this\nissue, we propose a Deep Adversarial Automated Red Teaming (DART) framework in\nwhich the Red LLM and Target LLM are deeply and dynamically interacting with\neach other in an iterative manner. In each iteration, in order to generate\nsuccessful attacks as many as possible, the Red LLM not only takes into account\nthe responses from the Target LLM, but also adversarially adjust its attacking\ndirections by monitoring the global diversity of generated attacks across\nmultiple iterations. Simultaneously, to explore dynamically changing safety\nvulnerabilities of the Target LLM, we allow the Target LLM to enhance its\nsafety via an active learning based data selection mechanism. Experimential\nresults demonstrate that DART significantly reduces the safety risk of the\ntarget LLM. For human evaluation on Anthropic Harmless dataset, compared to the\ninstruction-tuning target LLM, DART eliminates the violation risks by 53.4\\%.\nWe will release the datasets and codes of DART soon.\n', '  Red-teaming, or identifying prompts that elicit harmful responses, is a\ncritical step in ensuring the safe and responsible deployment of large language\nmodels (LLMs). Developing effective protection against many modes of attack\nprompts requires discovering diverse attacks. Automated red-teaming typically\nuses reinforcement learning to fine-tune an attacker language model to generate\nprompts that elicit undesirable responses from a target LLM, as measured, for\nexample, by an auxiliary toxicity classifier. We show that even with explicit\nregularization to favor novelty and diversity, existing approaches suffer from\nmode collapse or fail to generate effective attacks. As a flexible and\nprobabilistically principled alternative, we propose to use GFlowNet\nfine-tuning, followed by a secondary smoothing phase, to train the attacker\nmodel to generate diverse and effective attack prompts. We find that the\nattacks generated by our method are effective against a wide range of target\nLLMs, both with and without safety tuning, and transfer well between target\nLLMs. Finally, we demonstrate that models safety-tuned using a dataset of\nred-teaming prompts generated by our method are robust to attacks from other\nRL-based red-teaming approaches.\n']",Adversarial Attacks on Large Language Models,Adversarial Attacks and Defenses in AI and Machine Learning Models,Adversarial Attacks and Vulnerabilities in AI and Machine Learning
30,30,110,30_pretraining_trained_unlearning_pretrained,"['pretraining', 'trained', 'unlearning', 'pretrained', 'training', 'benchmarks', 'tuning', 'datasets', 'dataset', 'tuned']","['tuning', 'fine', 'instruction', 'training', 'tokens', 'token', 'language', 'tasks', 'context', 'parameters']","[""  Fine-tuning on task-specific question-answer pairs is a predominant method\nfor enhancing the performance of instruction-tuned large language models (LLMs)\non downstream tasks. However, in certain specialized domains, such as\nhealthcare or harmless content generation, it is nearly impossible to obtain a\nlarge volume of high-quality data that matches the downstream distribution. To\nimprove the performance of LLMs in data-scarce domains with domain-mismatched\ndata, we re-evaluated the Transformer architecture and discovered that not all\nparameter updates during fine-tuning contribute positively to downstream\nperformance. Our analysis reveals that within the self-attention and\nfeed-forward networks, only the fine-tuned attention parameters are\nparticularly beneficial when the training set's distribution does not fully\nalign with the test set. Based on this insight, we propose an effective\ninference-time intervention method: Training All parameters but Inferring with\nonly Attention (\\trainallInfAttn). We empirically validate \\trainallInfAttn\nusing two general instruction-tuning datasets and evaluate it on seven\ndownstream tasks involving math, reasoning, and knowledge understanding across\nLLMs of different parameter sizes and fine-tuning techniques. Our comprehensive\nexperiments demonstrate that \\trainallInfAttn achieves superior improvements\ncompared to both the fully fine-tuned model and the base model in most\nscenarios, with significant performance gains. The high tolerance of\n\\trainallInfAttn to data mismatches makes it resistant to jailbreaking tuning\nand enhances specialized tasks using general data.\n"", '  This work focuses on leveraging and selecting from vast, unlabeled, open data\nto pre-fine-tune a pre-trained language model. The goal is to minimize the need\nfor costly domain-specific data for subsequent fine-tuning while achieving\ndesired performance levels. While many data selection algorithms have been\ndesigned for small-scale applications, rendering them unsuitable for our\ncontext, some emerging methods do cater to language data scales. However, they\noften prioritize data that aligns with the target distribution. While this\nstrategy may be effective when training a model from scratch, it can yield\nlimited results when the model has already been pre-trained on a different\ndistribution. Differing from prior work, our key idea is to select data that\nnudges the pre-training distribution closer to the target distribution. We show\nthe optimality of this approach for fine-tuning tasks under certain conditions.\nWe demonstrate the efficacy of our methodology across a diverse array of tasks\n(NLU, NLG, zero-shot) with models up to 2.7B, showing that it consistently\nsurpasses other selection methods. Moreover, our proposed method is\nsignificantly faster than existing techniques, scaling to millions of samples\nwithin a single GPU hour. Our code is open-sourced (Code repository:\nhttps://anonymous.4open.science/r/DV4LLM-D761/ ). While fine-tuning offers\nsignificant potential for enhancing performance across diverse tasks, its\nassociated costs often limit its widespread adoption; with this work, we hope\nto lay the groundwork for cost-effective fine-tuning, making its benefits more\naccessible.\n', '  In the domain of large language models (LLMs), arXiv:2305.16938 showed that\nfew-shot full-model fine-tuning -- namely Vanilla Fine Tuning (FT) and\nPattern-Based Fine Tuning (PBFT) --, and In-Context Learning (ICL) generalize\nsimilarly on Out-Of-Domain (OOD) datasets, but vary in terms of task\nadaptation. However, they both pose challenges, especially in term of memory\nrequirements. In this paper, we further try to push the understanding of\ndifferent fine-tuning strategies for LLM and aim to bring a myriad of these on\nthe same pedestal for an elaborate comparison with full-model fine-tuning on\ntwo diverse datasets. To that end, we conducted a series of experiments,\nbeginning with state-of-the-art methods like vanilla fine-tuning and\nPattern-Based Fine-Tuning (PBFT) on pre-trained models across two datasets,\nCOLA and MNLI. We then investigate adaptive fine-tuning and the efficiency of\nLoRA adapters in a few-shot setting. Finally, we also compare an alternative\napproach that has gained recent popularity -- context distillation -- with the\nvanilla FT and PBFT with and without few-shot setup.\n  Our findings suggest that these alternative strategies that we explored can\nexhibit out-of-domain generalization comparable to that of vanilla FT and PBFT.\nPBFT under-performs Vanilla FT on out-of-domain (OOD) data, emphasizing the\nneed for effective prompts. Further, our adaptive-fine tuning and LoRA\nexperiments perform comparable or slightly worse than the standard fine-tunings\nas anticipated, since standard fine-tunings involve tuning the entire model.\nFinally, our context distillation experiments out-perform the standard\nfine-tuning methods. These findings underscore that eventually the choice of an\nappropriate fine-tuning method depends on the available resources (memory,\ncompute, data) and task adaptability.\n']",Fine-Tuning Strategies for Large Language Models,Advances in Large Language Models,Large Language Models
30,30,110,30_pretraining_trained_unlearning_pretrained,"['pretraining', 'trained', 'unlearning', 'pretrained', 'training', 'benchmarks', 'tuning', 'datasets', 'dataset', 'tuned']","['tuning', 'fine', 'instruction', 'training', 'tokens', 'token', 'language', 'tasks', 'context', 'parameters']","[""  Fine-tuning on task-specific question-answer pairs is a predominant method\nfor enhancing the performance of instruction-tuned large language models (LLMs)\non downstream tasks. However, in certain specialized domains, such as\nhealthcare or harmless content generation, it is nearly impossible to obtain a\nlarge volume of high-quality data that matches the downstream distribution. To\nimprove the performance of LLMs in data-scarce domains with domain-mismatched\ndata, we re-evaluated the Transformer architecture and discovered that not all\nparameter updates during fine-tuning contribute positively to downstream\nperformance. Our analysis reveals that within the self-attention and\nfeed-forward networks, only the fine-tuned attention parameters are\nparticularly beneficial when the training set's distribution does not fully\nalign with the test set. Based on this insight, we propose an effective\ninference-time intervention method: Training All parameters but Inferring with\nonly Attention (\\trainallInfAttn). We empirically validate \\trainallInfAttn\nusing two general instruction-tuning datasets and evaluate it on seven\ndownstream tasks involving math, reasoning, and knowledge understanding across\nLLMs of different parameter sizes and fine-tuning techniques. Our comprehensive\nexperiments demonstrate that \\trainallInfAttn achieves superior improvements\ncompared to both the fully fine-tuned model and the base model in most\nscenarios, with significant performance gains. The high tolerance of\n\\trainallInfAttn to data mismatches makes it resistant to jailbreaking tuning\nand enhances specialized tasks using general data.\n"", '  This work focuses on leveraging and selecting from vast, unlabeled, open data\nto pre-fine-tune a pre-trained language model. The goal is to minimize the need\nfor costly domain-specific data for subsequent fine-tuning while achieving\ndesired performance levels. While many data selection algorithms have been\ndesigned for small-scale applications, rendering them unsuitable for our\ncontext, some emerging methods do cater to language data scales. However, they\noften prioritize data that aligns with the target distribution. While this\nstrategy may be effective when training a model from scratch, it can yield\nlimited results when the model has already been pre-trained on a different\ndistribution. Differing from prior work, our key idea is to select data that\nnudges the pre-training distribution closer to the target distribution. We show\nthe optimality of this approach for fine-tuning tasks under certain conditions.\nWe demonstrate the efficacy of our methodology across a diverse array of tasks\n(NLU, NLG, zero-shot) with models up to 2.7B, showing that it consistently\nsurpasses other selection methods. Moreover, our proposed method is\nsignificantly faster than existing techniques, scaling to millions of samples\nwithin a single GPU hour. Our code is open-sourced (Code repository:\nhttps://anonymous.4open.science/r/DV4LLM-D761/ ). While fine-tuning offers\nsignificant potential for enhancing performance across diverse tasks, its\nassociated costs often limit its widespread adoption; with this work, we hope\nto lay the groundwork for cost-effective fine-tuning, making its benefits more\naccessible.\n', '  In the domain of large language models (LLMs), arXiv:2305.16938 showed that\nfew-shot full-model fine-tuning -- namely Vanilla Fine Tuning (FT) and\nPattern-Based Fine Tuning (PBFT) --, and In-Context Learning (ICL) generalize\nsimilarly on Out-Of-Domain (OOD) datasets, but vary in terms of task\nadaptation. However, they both pose challenges, especially in term of memory\nrequirements. In this paper, we further try to push the understanding of\ndifferent fine-tuning strategies for LLM and aim to bring a myriad of these on\nthe same pedestal for an elaborate comparison with full-model fine-tuning on\ntwo diverse datasets. To that end, we conducted a series of experiments,\nbeginning with state-of-the-art methods like vanilla fine-tuning and\nPattern-Based Fine-Tuning (PBFT) on pre-trained models across two datasets,\nCOLA and MNLI. We then investigate adaptive fine-tuning and the efficiency of\nLoRA adapters in a few-shot setting. Finally, we also compare an alternative\napproach that has gained recent popularity -- context distillation -- with the\nvanilla FT and PBFT with and without few-shot setup.\n  Our findings suggest that these alternative strategies that we explored can\nexhibit out-of-domain generalization comparable to that of vanilla FT and PBFT.\nPBFT under-performs Vanilla FT on out-of-domain (OOD) data, emphasizing the\nneed for effective prompts. Further, our adaptive-fine tuning and LoRA\nexperiments perform comparable or slightly worse than the standard fine-tunings\nas anticipated, since standard fine-tunings involve tuning the entire model.\nFinally, our context distillation experiments out-perform the standard\nfine-tuning methods. These findings underscore that eventually the choice of an\nappropriate fine-tuning method depends on the available resources (memory,\ncompute, data) and task adaptability.\n']",Fine-Tuning Strategies for Large Language Models,Advances in Large Language Models,Large Language Models
30,30,110,30_pretraining_trained_unlearning_pretrained,"['pretraining', 'trained', 'unlearning', 'pretrained', 'training', 'benchmarks', 'tuning', 'datasets', 'dataset', 'tuned']","['tuning', 'fine', 'instruction', 'training', 'tokens', 'token', 'language', 'tasks', 'context', 'parameters']","[""  Fine-tuning on task-specific question-answer pairs is a predominant method\nfor enhancing the performance of instruction-tuned large language models (LLMs)\non downstream tasks. However, in certain specialized domains, such as\nhealthcare or harmless content generation, it is nearly impossible to obtain a\nlarge volume of high-quality data that matches the downstream distribution. To\nimprove the performance of LLMs in data-scarce domains with domain-mismatched\ndata, we re-evaluated the Transformer architecture and discovered that not all\nparameter updates during fine-tuning contribute positively to downstream\nperformance. Our analysis reveals that within the self-attention and\nfeed-forward networks, only the fine-tuned attention parameters are\nparticularly beneficial when the training set's distribution does not fully\nalign with the test set. Based on this insight, we propose an effective\ninference-time intervention method: Training All parameters but Inferring with\nonly Attention (\\trainallInfAttn). We empirically validate \\trainallInfAttn\nusing two general instruction-tuning datasets and evaluate it on seven\ndownstream tasks involving math, reasoning, and knowledge understanding across\nLLMs of different parameter sizes and fine-tuning techniques. Our comprehensive\nexperiments demonstrate that \\trainallInfAttn achieves superior improvements\ncompared to both the fully fine-tuned model and the base model in most\nscenarios, with significant performance gains. The high tolerance of\n\\trainallInfAttn to data mismatches makes it resistant to jailbreaking tuning\nand enhances specialized tasks using general data.\n"", '  This work focuses on leveraging and selecting from vast, unlabeled, open data\nto pre-fine-tune a pre-trained language model. The goal is to minimize the need\nfor costly domain-specific data for subsequent fine-tuning while achieving\ndesired performance levels. While many data selection algorithms have been\ndesigned for small-scale applications, rendering them unsuitable for our\ncontext, some emerging methods do cater to language data scales. However, they\noften prioritize data that aligns with the target distribution. While this\nstrategy may be effective when training a model from scratch, it can yield\nlimited results when the model has already been pre-trained on a different\ndistribution. Differing from prior work, our key idea is to select data that\nnudges the pre-training distribution closer to the target distribution. We show\nthe optimality of this approach for fine-tuning tasks under certain conditions.\nWe demonstrate the efficacy of our methodology across a diverse array of tasks\n(NLU, NLG, zero-shot) with models up to 2.7B, showing that it consistently\nsurpasses other selection methods. Moreover, our proposed method is\nsignificantly faster than existing techniques, scaling to millions of samples\nwithin a single GPU hour. Our code is open-sourced (Code repository:\nhttps://anonymous.4open.science/r/DV4LLM-D761/ ). While fine-tuning offers\nsignificant potential for enhancing performance across diverse tasks, its\nassociated costs often limit its widespread adoption; with this work, we hope\nto lay the groundwork for cost-effective fine-tuning, making its benefits more\naccessible.\n', '  In the domain of large language models (LLMs), arXiv:2305.16938 showed that\nfew-shot full-model fine-tuning -- namely Vanilla Fine Tuning (FT) and\nPattern-Based Fine Tuning (PBFT) --, and In-Context Learning (ICL) generalize\nsimilarly on Out-Of-Domain (OOD) datasets, but vary in terms of task\nadaptation. However, they both pose challenges, especially in term of memory\nrequirements. In this paper, we further try to push the understanding of\ndifferent fine-tuning strategies for LLM and aim to bring a myriad of these on\nthe same pedestal for an elaborate comparison with full-model fine-tuning on\ntwo diverse datasets. To that end, we conducted a series of experiments,\nbeginning with state-of-the-art methods like vanilla fine-tuning and\nPattern-Based Fine-Tuning (PBFT) on pre-trained models across two datasets,\nCOLA and MNLI. We then investigate adaptive fine-tuning and the efficiency of\nLoRA adapters in a few-shot setting. Finally, we also compare an alternative\napproach that has gained recent popularity -- context distillation -- with the\nvanilla FT and PBFT with and without few-shot setup.\n  Our findings suggest that these alternative strategies that we explored can\nexhibit out-of-domain generalization comparable to that of vanilla FT and PBFT.\nPBFT under-performs Vanilla FT on out-of-domain (OOD) data, emphasizing the\nneed for effective prompts. Further, our adaptive-fine tuning and LoRA\nexperiments perform comparable or slightly worse than the standard fine-tunings\nas anticipated, since standard fine-tunings involve tuning the entire model.\nFinally, our context distillation experiments out-perform the standard\nfine-tuning methods. These findings underscore that eventually the choice of an\nappropriate fine-tuning method depends on the available resources (memory,\ncompute, data) and task adaptability.\n']",Fine-Tuning Strategies for Large Language Models,Advances in Large Language Models,Large Language Models
31,31,109,31_modeling_simulations_learning_neural,"['modeling', 'simulations', 'learning', 'neural', 'dynamics', 'networks', 'nonlinear', 'pdes', 'simulation', 'computational']","['physics', 'physical', 'equations', 'fluid', 'neural', 'simulations', 'differential', 'inverse', 'problems', 'partial']","[""  Multiscale phenomena manifest across various scientific domains, presenting a\nubiquitous challenge in accurately and effectively simulating multiscale\ndynamics in complex systems. In this paper, a novel decoupling solving paradigm\nis proposed through modelling large-scale dynamics independently and treating\nsmall-scale dynamics as a slaved system. A Spectral Physics-informed Neural\nNetwork (PINN) is developed to characterize the small-scale system in an\nefficient and accurate way, addressing the challenges posed by the\nrepresentation of multiscale dynamics in neural networks. The effectiveness of\nthe method is demonstrated through extensive numerical experiments, including\none-dimensional Kuramot-Sivashinsky equation, two- and three-dimensional\nNavier-Stokes equations, showcasing its versatility in addressing problems of\nfluid dynamics. Furthermore, we also delve into the application of the proposed\napproach to more complex problems, including non-uniform meshes, complex\ngeometries, large-scale data with noise, and high-dimensional small-scale\ndynamics. The discussions about these scenarios contribute to a comprehensive\nunderstanding of the method's capabilities and limitations. By enabling the\nacquisition of large-scale data with minimal computational demands, coupled\nwith the efficient and accurate characterization of small-scale dynamics via\nSpectral PINN, our approach offers a valuable and promising approach for\nresearchers seeking to tackle multiscale phenomena effectively.\n"", '  Finding the distribution of the velocities and pressures of a fluid by\nsolving the Navier-Stokes equations is a principal task in the chemical,\nenergy, and pharmaceutical industries, as well as in mechanical engineering and\nthe design of pipeline systems. With existing solvers, such as OpenFOAM and\nAnsys, simulations of fluid dynamics in intricate geometries are\ncomputationally expensive and require re-simulation whenever the geometric\nparameters or the initial and boundary conditions are altered. Physics-informed\nneural networks are a promising tool for simulating fluid flows in complex\ngeometries, as they can adapt to changes in the geometry and mesh definitions,\nallowing for generalization across fluid parameters and transfer learning\nacross different shapes. We present a hybrid quantum physics-informed neural\nnetwork that simulates laminar fluid flows in 3D Y-shaped mixers. Our approach\ncombines the expressive power of a quantum model with the flexibility of a\nphysics-informed neural network, resulting in a 21% higher accuracy compared to\na purely classical neural network. Our findings highlight the potential of\nmachine learning approaches, and in particular hybrid quantum physics-informed\nneural network, for complex shape optimization tasks in computational fluid\ndynamics. By improving the accuracy of fluid simulations in complex geometries,\nour research using hybrid quantum models contributes to the development of more\nefficient and reliable fluid dynamics solvers.\n', '  Scientific discovery and engineering design are currently limited by the time\nand cost of physical experiments, selected mostly through trial-and-error and\nintuition that require deep domain expertise. Numerical simulations present an\nalternative to physical experiments but are usually infeasible for complex\nreal-world domains due to the computational requirements of existing numerical\nmethods. Artificial intelligence (AI) presents a potential paradigm shift by\ndeveloping fast data-driven surrogate models. In particular, an AI framework,\nknown as Neural Operators, presents a principled framework for learning\nmappings between functions defined on continuous domains, e.g., spatiotemporal\nprocesses and partial differential equations (PDE). They can extrapolate and\npredict solutions at new locations unseen during training, i.e., perform\nzero-shot super-resolution. Neural Operators can augment or even replace\nexisting simulators in many applications, such as computational fluid dynamics,\nweather forecasting, and material modeling, while being 4-5 orders of magnitude\nfaster. Further, Neural Operators can be integrated with physics and other\ndomain constraints enforced at finer resolutions to obtain high-fidelity\nsolutions and good generalization. Since Neural Operators are differentiable,\nthey can directly optimize parameters for inverse design and other inverse\nproblems. We believe that Neural Operators present a transformative approach to\nsimulation and design, enabling rapid research and development.\n']",Physics-Informed Neural Networks for Fluid Dynamics,Physics-Informed Machine Learning for Differential Equations and Fluid Dynamics,Machine Learning for Dynamical Systems and Differential Equations
32,32,109,32_dynamics_learning_dynamical_neural,"['dynamics', 'learning', 'dynamical', 'neural', 'modeling', 'odes', 'networks', 'dynamic', 'nonlinear', 'hamiltonian']","['dynamical', 'dynamics', 'equations', 'systems', 'physics', 'equation', 'neural', 'differential', 'physical', 'multigrid']","[""  Neural ordinary differential equations (Neural ODEs) is a class of machine\nlearning models that approximate the time derivative of hidden states using a\nneural network. They are powerful tools for modeling continuous-time dynamical\nsystems, enabling the analysis and prediction of complex temporal behaviors.\nHowever, how to improve the model's stability and physical interpretability\nremains a challenge. This paper introduces new conservation relations in Neural\nODEs using Lie symmetries in both the hidden state dynamics and the back\npropagation dynamics. These conservation laws are then incorporated into the\nloss function as additional regularization terms, potentially enhancing the\nphysical interpretability and generalizability of the model. To illustrate this\nmethod, the paper derives Lie symmetries and conservation laws in a simple\nNeural ODE designed to monitor charged particles in a sinusoidal electric\nfield. New loss functions are constructed from these conservation relations,\ndemonstrating the applicability symmetry-regularized Neural ODE in typical\nmodeling tasks, such as data-driven discovery of dynamical systems.\n"", ""  Differential equations are a ubiquitous tool to study dynamics, ranging from\nphysical systems to complex systems, where a large number of agents interact\nthrough a graph with non-trivial topological features. Data-driven\napproximations of differential equations present a promising alternative to\ntraditional methods for uncovering a model of dynamical systems, especially in\ncomplex systems that lack explicit first principles. A recently employed\nmachine learning tool for studying dynamics is neural networks, which can be\nused for data-driven solution finding or discovery of differential equations.\nSpecifically for the latter task, however, deploying deep learning models in\nunfamiliar settings - such as predicting dynamics in unobserved state space\nregions or on novel graphs - can lead to spurious results. Focusing on complex\nsystems whose dynamics are described with a system of first-order differential\nequations coupled through a graph, we show that extending the model's\ngeneralizability beyond traditional statistical learning theory limits is\nfeasible. However, achieving this advanced level of generalization requires\nneural network models to conform to fundamental assumptions about the dynamical\nmodel. Additionally, we propose a statistical significance test to assess\nprediction quality during inference, enabling the identification of a neural\nnetwork's confidence level in its predictions.\n"", '  Recent advances in deep learning for physics have focused on discovering\nshared representations of target systems by incorporating physics priors or\ninductive biases into neural networks. While effective, these methods are\nlimited to the system domain, where the type of system remains consistent and\nthus cannot ensure the adaptation to new, or unseen physical systems governed\nby different laws. For instance, a neural network trained on a mass-spring\nsystem cannot guarantee accurate predictions for the behavior of a two-body\nsystem or any other system with different physical laws. In this work, we take\na significant leap forward by targeting cross domain generalization within the\nfield of Hamiltonian dynamics. We model our system with a graph neural network\n(GNN) and employ a meta learning algorithm to enable the model to gain\nexperience over a distribution of systems and make it adapt to new physics. Our\napproach aims to learn a unified Hamiltonian representation that is\ngeneralizable across multiple system domains, thereby overcoming the\nlimitations of system-specific models. We demonstrate that the meta-trained\nmodel captures the generalized Hamiltonian representation that is consistent\nacross different physical domains. Overall, through the use of meta learning,\nwe offer a framework that achieves cross domain generalization, providing a\nstep towards a unified model for understanding a wide array of dynamical\nsystems via deep learning.\n']",Neural Networks for Dynamical Systems Modeling,Neural Networks for Modeling and Control of Dynamical Systems,Machine Learning for Dynamical Systems and Differential Equations
33,33,108,33_editing_gans_generative_gan,"['editing', 'gans', 'generative', 'gan', 'blur', 'denoising', 'attention', 'diffusion', 'images', 'pixel']","['diffusion', 'image', 'editing', 'images', 'resolution', 'quality', 'text', 'generation', 'denoising', 'texture']","['  Recently, there has been a significant advancement in text-to-image diffusion\nmodels, leading to groundbreaking performance in 2D image generation. These\nadvancements have been extended to 3D models, enabling the generation of novel\n3D objects from textual descriptions. This has evolved into NeRF editing\nmethods, which allow the manipulation of existing 3D objects through textual\nconditioning. However, existing NeRF editing techniques have faced limitations\nin their performance due to slow training speeds and the use of loss functions\nthat do not adequately consider editing. To address this, here we present a\nnovel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding\nreal-world scenes into the latent space of the latent diffusion model (LDM)\nthrough a unique refinement layer. This approach enables us to obtain a NeRF\nbackbone that is not only faster but also more amenable to editing compared to\ntraditional image space NeRF editing. Furthermore, we propose an improved loss\nfunction tailored for editing by migrating the delta denoising score (DDS)\ndistillation loss, originally used in 2D image editing to the three-dimensional\ndomain. This novel loss function surpasses the well-known score distillation\nsampling (SDS) loss in terms of suitability for editing purposes. Our\nexperimental results demonstrate that ED-NeRF achieves faster editing speed\nwhile producing improved output quality compared to state-of-the-art 3D editing\nmodels.\n', '  With the remarkable advent of text-to-image diffusion models, image editing\nmethods have become more diverse and continue to evolve. A promising recent\napproach in this realm is Delta Denoising Score (DDS) - an image editing\ntechnique based on Score Distillation Sampling (SDS) framework that leverages\nthe rich generative prior of text-to-image diffusion models. However, relying\nsolely on the difference between scoring functions is insufficient for\npreserving specific structural elements from the original image, a crucial\naspect of image editing. To address this, here we present an embarrassingly\nsimple yet very powerful modification of DDS, called Contrastive Denoising\nScore (CDS), for latent diffusion models (LDM). Inspired by the similarities\nand differences between DDS and the contrastive learning for unpaired\nimage-to-image translation(CUT), we introduce a straightforward approach using\nCUT loss within the DDS framework. Rather than employing auxiliary networks as\nin the original CUT approach, we leverage the intermediate features of LDM,\nspecifically those from the self-attention layers, which possesses rich spatial\ninformation. Our approach enables zero-shot image-to-image translation and\nneural radiance field (NeRF) editing, achieving structural correspondence\nbetween the input and output while maintaining content controllability.\nQualitative results and comparisons demonstrates the effectiveness of our\nproposed method. Project page: https://hyelinnam.github.io/CDS/\n', '  Large-scale Text-to-Image (T2I) diffusion models have revolutionized image\ngeneration over the last few years. Although owning diverse and high-quality\ngeneration capabilities, translating these abilities to fine-grained image\nediting remains challenging. In this paper, we propose DiffEditor to rectify\ntwo weaknesses in existing diffusion-based image editing: (1) in complex\nscenarios, editing results often lack editing accuracy and exhibit unexpected\nartifacts; (2) lack of flexibility to harmonize editing operations, e.g.,\nimagine new content. In our solution, we introduce image prompts in\nfine-grained image editing, cooperating with the text prompt to better describe\nthe editing content. To increase the flexibility while maintaining content\nconsistency, we locally combine stochastic differential equation (SDE) into the\nordinary differential equation (ODE) sampling. In addition, we incorporate\nregional score-based gradient guidance and a time travel strategy into the\ndiffusion sampling, further improving the editing quality. Extensive\nexperiments demonstrate that our method can efficiently achieve\nstate-of-the-art performance on various fine-grained image editing tasks,\nincluding editing within a single image (e.g., object moving, resizing, and\ncontent dragging) and across images (e.g., appearance replacing and object\npasting). Our source code is released at\nhttps://github.com/MC-E/DragonDiffusion.\n']",Diffusion-based Image Editing,Diffusion Models for Image and Video Generation and Manipulation,Diffusion Models for Generative Tasks
34,34,108,34_factorization_dimensionality_supervised_autoencoders,"['factorization', 'dimensionality', 'supervised', 'autoencoders', 'regularization', 'features', 'discriminative', 'dimensional', 'matrix', 'feature']","['matrix', 'clustering', 'factorization', 'reduction', 'dimensionality', 'dimensional', 'kernel', 'subspace', 'feature', 'graphical']","['  By removing irrelevant and redundant features, feature selection aims to find\na good representation of the original features. With the prevalence of\nunlabeled data, unsupervised feature selection has been proven effective in\nalleviating the so-called curse of dimensionality. Most existing matrix\nfactorization-based unsupervised feature selection methods are built upon\nsubspace learning, but they have limitations in capturing nonlinear structural\ninformation among features. It is well-known that kernel techniques can capture\nnonlinear structural information. In this paper, we construct a model by\nintegrating kernel functions and kernel alignment, which can be equivalently\ncharacterized as a matrix factorization problem. However, such an extension\nraises another issue: the algorithm performance heavily depends on the choice\nof kernel, which is often unknown a priori. Therefore, we further propose a\nmultiple kernel-based learning method. By doing so, our model can learn both\nlinear and nonlinear similarity information and automatically generate the most\nappropriate kernel. Experimental analysis on real-world data demonstrates that\nthe two proposed methods outperform other classic and state-of-the-art\nunsupervised feature selection methods in terms of clustering results and\nredundancy reduction in almost all datasets tested.\n', '  Unlike typical visual scene recognition domains, in which massive datasets\nare accessible to deep neural networks, medical image interpretations are often\nobstructed by the paucity of data. In this paper, we investigate the\neffectiveness of data-based few-shot learning in medical imaging by exploring\ndifferent data attribute representations in a low-dimensional space. We\nintroduce different types of non-negative matrix factorization (NMF) in\nfew-shot learning, addressing the data scarcity issue in medical image\nclassification. Extensive empirical studies are conducted in terms of\nvalidating the effectiveness of NMF, especially its supervised variants (e.g.,\ndiscriminative NMF, and supervised and constrained NMF with sparseness), and\nthe comparison with principal component analysis (PCA), i.e., the collaborative\nrepresentation-based dimensionality reduction technique derived from\neigenvectors. With 14 different datasets covering 11 distinct illness\ncategories, thorough experimental results and comparison with related\ntechniques demonstrate that NMF is a competitive alternative to PCA for\nfew-shot learning in medical imaging, and the supervised NMF algorithms are\nmore discriminative in the subspace with greater effectiveness. Furthermore, we\nshow that the part-based representation of NMF, especially its supervised\nvariants, is dramatically impactful in detecting lesion areas in medical\nimaging with limited samples.\n', '  Dimensionality Reduction plays a pivotal role in improving feature learning\naccuracy and reducing training time by eliminating redundant features, noise,\nand irrelevant data. Nonnegative Matrix Factorization (NMF) has emerged as a\npopular and powerful method for dimensionality reduction. Despite its extensive\nuse, there remains a need for a comprehensive analysis of NMF in the context of\ndimensionality reduction. To address this gap, this paper presents a\ncomprehensive survey of NMF, focusing on its applications in both feature\nextraction and feature selection. We introduce a classification of\ndimensionality reduction, enhancing understanding of the underlying concepts.\nSubsequently, we delve into a thorough summary of diverse NMF approaches used\nfor feature extraction and selection. Furthermore, we discuss the latest\nresearch trends and potential future directions of NMF in dimensionality\nreduction, aiming to highlight areas that need further exploration and\ndevelopment.\n']",Unsupervised Feature Selection and Dimensionality Reduction,Dimensionality Reduction and Data Visualization Techniques,Data Analysis and Visualization
35,35,108,35_recommender_recommendation_recommendations_personalized,"['recommender', 'recommendation', 'recommendations', 'personalized', 'popularity', 'preference', 'spotify', 'preferences', 'playlists', 'rankings']","['recommendation', 'recommender', 'user', 'ranking', 'music', 'items', 'item', 'users', 'recommendations', 'preferences']","[""  Recommender systems usually learn user interests from various user behaviors,\nincluding clicks and post-click behaviors (e.g., like and favorite). However,\nthese behaviors inevitably exhibit popularity bias, leading to some unfairness\nissues: 1) for items with similar quality, more popular ones get more exposure;\nand 2) even worse the popular items with lower popularity might receive more\nexposure. Existing work on mitigating popularity bias blindly eliminates the\nbias and usually ignores the effect of item quality. We argue that the\nrelationships between different user behaviors (e.g., conversion rate) actually\nreflect the item quality. Therefore, to handle the unfairness issues, we\npropose to mitigate the popularity bias by considering multiple user behaviors.\n  In this work, we examine causal relationships behind the interaction\ngeneration procedure in multi-behavior recommendation. Specifically, we find\nthat: 1) item popularity is a confounder between the exposed items and users'\npost-click interactions, leading to the first unfairness; and 2) some hidden\nconfounders (e.g., the reputation of item producers) affect both item\npopularity and quality, resulting in the second unfairness. To alleviate these\nconfounding issues, we propose a causal framework to estimate the causal\neffect, which leverages backdoor adjustment to block the backdoor paths caused\nby the confounders. In the inference stage, we remove the negative effect of\npopularity and utilize the good effect of quality for recommendation.\nExperiments on two real-world datasets validate the effectiveness of our\nproposed framework, which enhances fairness without sacrificing recommendation\naccuracy.\n"", ""  Recommender systems can automatically recommend users with items that they\nprobably like. The goal of them is to model the user-item interaction by\neffectively representing the users and items. Existing methods have primarily\nlearned the user's preferences and item's features with vectorized embeddings,\nand modeled the user's general preferences to items by the interaction of them.\nIn fact, users have their specific preferences to item attributes and different\npreferences are usually related. Therefore, exploring the fine-grained\npreferences as well as modeling the relationships among user's different\npreferences could improve the recommendation performance. Toward this end, we\npropose a dual preference distribution learning framework (DUPLE), which aims\nto jointly learn a general preference distribution and a specific preference\ndistribution for a given user, where the former corresponds to the user's\ngeneral preference to items and the latter refers to the user's specific\npreference to item attributes. Notably, the mean vector of each Gaussian\ndistribution can capture the user's preferences, and the covariance matrix can\nlearn their relationship. Moreover, we can summarize a preferred attribute\nprofile for each user, depicting his/her preferred item attributes. We then can\nprovide the explanation for each recommended item by checking the overlap\nbetween its attributes and the user's preferred attribute profile. Extensive\nquantitative and qualitative experiments on six public datasets demonstrate the\neffectiveness and explainability of the DUPLE method.\n"", ""  Recommendation systems are widespread, and through customized\nrecommendations, promise to match users with options they will like. To that\nend, data on engagement is collected and used. Most recommendation systems are\nranking-based, where they rank and recommend items based on their predicted\nengagement. However, the engagement signals are often only a crude proxy for\nutility, as data on the latter is rarely collected or available. This paper\nexplores the following question: By optimizing for measurable proxies, are\nrecommendation systems at risk of significantly under-delivering on utility? If\nso, how can one improve utility which is seldom measured? To study these\nquestions, we introduce a model of repeated user consumption in which, at each\ninteraction, users select between an outside option and the best option from a\nrecommendation set. Our model accounts for user heterogeneity, with the\nmajority preferring ``popular'' content, and a minority favoring ``niche''\ncontent. The system initially lacks knowledge of individual user preferences\nbut can learn them through observations of users' choices over time. Our\ntheoretical and numerical analysis demonstrate that optimizing for engagement\ncan lead to significant utility losses. Instead, we propose a utility-aware\npolicy that initially recommends a mix of popular and niche content. As the\nplatform becomes more forward-looking, our utility-aware policy achieves the\nbest of both worlds: near-optimal utility and near-optimal engagement\nsimultaneously. Our study elucidates an important feature of recommendation\nsystems; given the ability to suggest multiple items, one can perform\nsignificant exploration without incurring significant reductions in engagement.\nBy recommending high-risk, high-reward items alongside popular items, systems\ncan enhance discovery of high utility items without significantly affecting\nengagement.\n""]",Recommender Systems and Personalized Recommendations,Advances in Recommender Systems,Recommender Systems and Personalization
36,36,107,36_transportation_traffic_routes_ridesharing,"['transportation', 'traffic', 'routes', 'ridesharing', 'travelers', 'travel', 'trips', 'transport', 'mobility', 'passengers']","['mobility', 'transportation', 'travel', 'traffic', 'urban', 'delivery', 'transit', 'route', 'vehicle', 'vehicles']","['  Mobility service route design requires demand information to operate in a\nservice region. Transit planners and operators can access various data sources\nincluding household travel survey data and mobile device location logs.\nHowever, when implementing a mobility system with emerging technologies,\nestimating demand becomes harder because of limited data resulting in\nuncertainty. This study proposes an artificial intelligence-driven algorithm\nthat combines sequential transit network design with optimal learning to\naddress the operation under limited data. An operator gradually expands its\nroute system to avoid risks from inconsistency between designed routes and\nactual travel demand. At the same time, observed information is archived to\nupdate the knowledge that the operator currently uses. Three learning policies\nare compared within the algorithm: multi-armed bandit, knowledge gradient, and\nknowledge gradient with correlated beliefs. For validation, a new route system\nis designed on an artificial network based on public use microdata areas in New\nYork City. Prior knowledge is reproduced from the regional household travel\nsurvey data. The results suggest that exploration considering correlations can\nachieve better performance compared to greedy choices in general. In future\nwork, the problem may incorporate more complexities such as demand elasticity\nto travel time, no limitations to the number of transfers, and costs for\nexpansion.\n', '  This paper leverages macroscopic models and multi-source spatiotemporal data\ncollected from automatic traffic counters and probe vehicles to accurately\nestimate traffic flow and travel time in links where these measurements are\nunavailable. This problem is critical in transportation planning applications\nwhere the sensor coverage is low and the planned interventions have\nnetwork-wide impacts. The proposed model, named the Macroscopic Traffic\nEstimator (MaTE), can perform network-wide estimations of traffic flow and\ntravel time only using the set of observed measurements of these quantities.\nBecause MaTE is grounded in macroscopic flow theory, all parameters and\nvariables are interpretable. The estimated traffic flow satisfies fundamental\nflow conservation constraints and exhibits an increasing monotonic relationship\nwith the estimated travel time. Using logit-based stochastic traffic assignment\nas the principle for routing flow behavior makes the model fully differentiable\nwith respect to the model parameters. This property facilitates the application\nof computational graphs to learn parameters from vast amounts of spatiotemporal\ndata. We also integrate neural networks and polynomial kernel functions to\ncapture link flow interactions and enrich the mapping of traffic flows into\ntravel times. MaTE also adds a destination choice model and a trip generation\nmodel that uses historical data on the number of trips generated by location.\nExperiments on synthetic data show that the model can accurately estimate\ntravel time and traffic flow in out-of-sample links. Results obtained using\nreal-world multi-source data from a large-scale transportation network suggest\nthat MaTE outperforms data-driven benchmarks, especially in travel time\nestimation. The estimated parameters of MaTE are also informative about the\nhourly change in travel demand and supply characteristics of the transportation\nnetwork.\n', '  Mobility analysis is a crucial element in the research area of transportation\nsystems. Forecasting traffic information offers a viable solution to address\nthe conflict between increasing transportation demands and the limitations of\ntransportation infrastructure. Predicting human travel is significant in aiding\nvarious transportation and urban management tasks, such as taxi dispatch and\nurban planning. Machine learning and deep learning methods are favored for\ntheir flexibility and accuracy. Nowadays, with the advent of large language\nmodels (LLMs), many researchers have combined these models with previous\ntechniques or applied LLMs to directly predict future traffic information and\nhuman travel behaviors. However, there is a lack of comprehensive studies on\nhow LLMs can contribute to this field. This survey explores existing approaches\nusing LLMs for mobility forecasting problems. We provide a literature review\nconcerning the forecasting applications within transportation systems,\nelucidating how researchers utilize LLMs, showcasing recent state-of-the-art\nadvancements, and identifying the challenges that must be overcome to fully\nleverage LLMs in this domain.\n']",Transportation Systems and Mobility Analysis,Transportation and Mobility Analysis,Transportation Systems and Environmental Analytics
37,37,106,37_optimality_reinforcement_optimal_bandit,"['optimality', 'reinforcement', 'optimal', 'bandit', 'mdps', 'learning', 'mdp', 'policies', 'exploration', 'policy']","['policy', 'reward', 'offline', 'reinforcement', 'optimal', 'policies', 'sample', 'rewards', 'decision', 'action']","['  In offline reinforcement learning (RL), the absence of active exploration\ncalls for attention on the model robustness to tackle the sim-to-real gap,\nwhere the discrepancy between the simulated and deployed environments can\nsignificantly undermine the performance of the learned policy. To endow the\nlearned policy with robustness in a sample-efficient manner in the presence of\nhigh-dimensional state-action space, this paper considers the sample complexity\nof distributionally robust linear Markov decision processes (MDPs) with an\nuncertainty set characterized by the total variation distance using offline\ndata. We develop a pessimistic model-based algorithm and establish its sample\ncomplexity bound under minimal data coverage assumptions, which outperforms\nprior art by at least $\\widetilde{O}(d)$, where $d$ is the feature dimension.\nWe further improve the performance guarantee of the proposed algorithm by\nincorporating a carefully-designed variance estimator.\n', '  We study offline reinforcement learning (RL) with linear MDPs under the\ninfinite-horizon discounted setting which aims to learn a policy that maximizes\nthe expected discounted cumulative reward using a pre-collected dataset.\nExisting algorithms for this setting either require a uniform data coverage\nassumptions or are computationally inefficient for finding an\n$\\epsilon$-optimal policy with $O(\\epsilon^{-2})$ sample complexity. In this\npaper, we propose a primal dual algorithm for offline RL with linear MDPs in\nthe infinite-horizon discounted setting. Our algorithm is the first\ncomputationally efficient algorithm in this setting that achieves sample\ncomplexity of $O(\\epsilon^{-2})$ with partial data coverage assumption. Our\nwork is an improvement upon a recent work that requires $O(\\epsilon^{-4})$\nsamples. Moreover, we extend our algorithm to work in the offline constrained\nRL setting that enforces constraints on additional reward signals.\n', '  Hybrid Reinforcement Learning (RL), where an agent learns from both an\noffline dataset and online explorations in an unknown environment, has garnered\nsignificant recent interest. A crucial question posed by Xie et al. (2022) is\nwhether hybrid RL can improve upon the existing lower bounds established in\npurely offline and purely online RL without relying on the single-policy\nconcentrability assumption. While Li et al. (2023) provided an affirmative\nanswer to this question in the tabular PAC RL case, the question remains\nunsettled for both the regret-minimizing RL case and the non-tabular case.\n  In this work, building upon recent advancements in offline RL and\nreward-agnostic exploration, we develop computationally efficient algorithms\nfor both PAC and regret-minimizing RL with linear function approximation,\nwithout single-policy concentrability. We demonstrate that these algorithms\nachieve sharper error or regret bounds that are no worse than, and can improve\non, the optimal sample complexity in offline RL (the first algorithm, for PAC\nRL) and online RL (the second algorithm, for regret-minimizing RL) in linear\nMarkov decision processes (MDPs), regardless of the quality of the behavior\npolicy. To our knowledge, this work establishes the tightest theoretical\nguarantees currently available for hybrid RL in linear MDPs.\n']",Offline Reinforcement Learning in MDPs,Reinforcement Learning Methods and Applications,Reinforcement Learning
37,37,106,37_optimality_reinforcement_optimal_bandit,"['optimality', 'reinforcement', 'optimal', 'bandit', 'mdps', 'learning', 'mdp', 'policies', 'exploration', 'policy']","['policy', 'reward', 'offline', 'reinforcement', 'optimal', 'policies', 'sample', 'rewards', 'decision', 'action']","['  In offline reinforcement learning (RL), the absence of active exploration\ncalls for attention on the model robustness to tackle the sim-to-real gap,\nwhere the discrepancy between the simulated and deployed environments can\nsignificantly undermine the performance of the learned policy. To endow the\nlearned policy with robustness in a sample-efficient manner in the presence of\nhigh-dimensional state-action space, this paper considers the sample complexity\nof distributionally robust linear Markov decision processes (MDPs) with an\nuncertainty set characterized by the total variation distance using offline\ndata. We develop a pessimistic model-based algorithm and establish its sample\ncomplexity bound under minimal data coverage assumptions, which outperforms\nprior art by at least $\\widetilde{O}(d)$, where $d$ is the feature dimension.\nWe further improve the performance guarantee of the proposed algorithm by\nincorporating a carefully-designed variance estimator.\n', '  We study offline reinforcement learning (RL) with linear MDPs under the\ninfinite-horizon discounted setting which aims to learn a policy that maximizes\nthe expected discounted cumulative reward using a pre-collected dataset.\nExisting algorithms for this setting either require a uniform data coverage\nassumptions or are computationally inefficient for finding an\n$\\epsilon$-optimal policy with $O(\\epsilon^{-2})$ sample complexity. In this\npaper, we propose a primal dual algorithm for offline RL with linear MDPs in\nthe infinite-horizon discounted setting. Our algorithm is the first\ncomputationally efficient algorithm in this setting that achieves sample\ncomplexity of $O(\\epsilon^{-2})$ with partial data coverage assumption. Our\nwork is an improvement upon a recent work that requires $O(\\epsilon^{-4})$\nsamples. Moreover, we extend our algorithm to work in the offline constrained\nRL setting that enforces constraints on additional reward signals.\n', '  Hybrid Reinforcement Learning (RL), where an agent learns from both an\noffline dataset and online explorations in an unknown environment, has garnered\nsignificant recent interest. A crucial question posed by Xie et al. (2022) is\nwhether hybrid RL can improve upon the existing lower bounds established in\npurely offline and purely online RL without relying on the single-policy\nconcentrability assumption. While Li et al. (2023) provided an affirmative\nanswer to this question in the tabular PAC RL case, the question remains\nunsettled for both the regret-minimizing RL case and the non-tabular case.\n  In this work, building upon recent advancements in offline RL and\nreward-agnostic exploration, we develop computationally efficient algorithms\nfor both PAC and regret-minimizing RL with linear function approximation,\nwithout single-policy concentrability. We demonstrate that these algorithms\nachieve sharper error or regret bounds that are no worse than, and can improve\non, the optimal sample complexity in offline RL (the first algorithm, for PAC\nRL) and online RL (the second algorithm, for regret-minimizing RL) in linear\nMarkov decision processes (MDPs), regardless of the quality of the behavior\npolicy. To our knowledge, this work establishes the tightest theoretical\nguarantees currently available for hybrid RL in linear MDPs.\n']",Offline Reinforcement Learning in MDPs,Reinforcement Learning Methods and Applications,Reinforcement Learning
38,38,106,38_diffusion_models_denoising_generative,"['diffusion', 'models', 'denoising', 'generative', 'stochastic', 'denoisers', 'denoiser', 'priors', 'brownian', 'sampling']","['diffusion', 'sampling', 'score', 'denoising', 'generative', 'noise', 'steps', 'process', 'samples', 'distribution']","['  Fourier analysis has been an instrumental tool in the development of signal\nprocessing. This leads us to wonder whether this framework could similarly\nbenefit generative modelling. In this paper, we explore this question through\nthe scope of time series diffusion models. More specifically, we analyze\nwhether representing time series in the frequency domain is a useful inductive\nbias for score-based diffusion models. By starting from the canonical SDE\nformulation of diffusion in the time domain, we show that a dual diffusion\nprocess occurs in the frequency domain with an important nuance: Brownian\nmotions are replaced by what we call mirrored Brownian motions, characterized\nby mirror symmetries among their components. Building on this insight, we show\nhow to adapt the denoising score matching approach to implement diffusion\nmodels in the frequency domain. This results in frequency diffusion models,\nwhich we compare to canonical time diffusion models. Our empirical evaluation\non real-world datasets, covering various domains like healthcare and finance,\nshows that frequency diffusion models better capture the training distribution\nthan time diffusion models. We explain this observation by showing that time\nseries from these datasets tend to be more localized in the frequency domain\nthan in the time domain, which makes them easier to model in the former case.\nAll our observations point towards impactful synergies between Fourier analysis\nand diffusion models.\n', '  Diffusion models are gaining widespread use in cutting-edge image, video, and\naudio generation. Score-based diffusion models stand out among these methods,\nnecessitating the estimation of score function of the input data distribution.\nIn this study, we present a theoretical framework to analyze two-layer neural\nnetwork-based diffusion models by reframing score matching and denoising score\nmatching as convex optimization. We prove that training shallow neural networks\nfor score prediction can be done by solving a single convex program. Although\nmost analyses of diffusion models operate in the asymptotic setting or rely on\napproximations, we characterize the exact predicted score function and\nestablish convergence results for neural network-based diffusion models with\nfinite data. Our results provide a precise characterization of what neural\nnetwork-based diffusion models learn in non-asymptotic settings.\n', '  Diffusion models, a powerful and universal generative AI technology, have\nachieved tremendous success in computer vision, audio, reinforcement learning,\nand computational biology. In these applications, diffusion models provide\nflexible high-dimensional data modeling, and act as a sampler for generating\nnew samples under active guidance towards task-desired properties. Despite the\nsignificant empirical success, theory of diffusion models is very limited,\npotentially slowing down principled methodological innovations for further\nharnessing and improving diffusion models. In this paper, we review emerging\napplications of diffusion models, understanding their sample generation under\nvarious controls. Next, we overview the existing theories of diffusion models,\ncovering their statistical properties and sampling capabilities. We adopt a\nprogressive routine, beginning with unconditional diffusion models and\nconnecting to conditional counterparts. Further, we review a new avenue in\nhigh-dimensional structured optimization through conditional diffusion models,\nwhere searching for solutions is reformulated as a conditional sampling problem\nand solved by diffusion models. Lastly, we discuss future directions about\ndiffusion models. The purpose of this paper is to provide a well-rounded\ntheoretical exposure for stimulating forward-looking theories and methods of\ndiffusion models.\n']",Diffusion Models for Generative Time Series Analysis,Diffusion Models for Generative Tasks,Diffusion Models for Generative Tasks
39,39,105,39_bert_encoder_encoders_encode,"['bert', 'encoder', 'encoders', 'encode', 'embeddings', 'tokenizer', 'tokenization', 'decoder', 'embedding', 'representations']","['tokens', 'embeddings', 'token', 'word', 'sentence', 'language', 'encoder', 'transformer', 'pre', 'representations']","[""  Background/introduction: Pre-trained transformer models shine in many natural\nlanguage processing tasks and therefore are expected to bear the representation\nof the input sentence or text meaning. These sentence-level embeddings are also\nimportant in retrieval-augmented generation. But do commonly used plain\naveraging or prompt templates surface it enough?\n  Methods: Given 110M parameters BERT's hidden representations from multiple\nlayers and multiple tokens we tried various ways to extract optimal sentence\nrepresentations. We tested various token aggregation and representation\npost-processing techniques. We also tested multiple ways of using a general\nWikitext dataset to complement BERTs sentence representations. All methods were\ntested on 8 Semantic Textual Similarity (STS), 6 short text clustering, and 12\nclassification tasks. We also evaluated our representation-shaping techniques\non other static models, including random token representations.\n  Results: Proposed representation extraction methods improved the performance\non STS and clustering tasks for all models considered. Very high improvements\nfor static token-based models, especially random embeddings for STS tasks\nalmost reach the performance of BERT-derived representations.\n  Conclusions: Our work shows that for multiple tasks simple baselines with\nrepresentation shaping techniques reach or even outperform more complex\nBERT-based models or are able to contribute to their performance.\n"", ""  BERT (Bidirectional Encoder Representations from Transformers) has\nrevolutionized the field of natural language processing through its exceptional\nperformance on numerous tasks. Yet, the majority of researchers have mainly\nconcentrated on enhancements related to the model structure, such as relative\nposition embedding and more efficient attention mechanisms. Others have delved\ninto pretraining tricks associated with Masked Language Modeling, including\nwhole word masking. DeBERTa introduced an enhanced decoder adapted for BERT's\nencoder model for pretraining, proving to be highly effective. We argue that\nthe design and research around enhanced masked language modeling decoders have\nbeen underappreciated. In this paper, we propose several designs of enhanced\ndecoders and introduce BPDec (BERT Pretraining Decoder), a novel method for\nmodeling training. Typically, a pretrained BERT model is fine-tuned for\nspecific Natural Language Understanding (NLU) tasks. In our approach, we\nutilize the original BERT model as the encoder, making only changes to the\ndecoder without altering the encoder. This approach does not necessitate\nextensive modifications to the encoder architecture and can be seamlessly\nintegrated into existing fine-tuning pipelines and services, offering an\nefficient and effective enhancement strategy. Compared to other methods, while\nwe also incur a moderate training cost for the decoder during the pretraining\nprocess, our approach does not introduce additional training costs during the\nfine-tuning phase. We test multiple enhanced decoder structures after\npretraining and evaluate their performance on the GLUE tasks and SQuAD tasks.\nOur results demonstrate that BPDec, having only undergone subtle refinements to\nthe model structure during pretraining, significantly enhances model\nperformance without escalating the finetuning cost, inference time and serving\nbudget.\n"", ""  Recent years have witnessed a substantial increase in the use of deep\nlearning to solve various natural language processing (NLP) problems. Early\ndeep learning models were constrained by their sequential or unidirectional\nnature, such that they struggled to capture the contextual relationships across\ntext inputs. The introduction of bidirectional encoder representations from\ntransformers (BERT) leads to a robust encoder for the transformer model that\ncan understand the broader context and deliver state-of-the-art performance\nacross various NLP tasks. This has inspired researchers and practitioners to\napply BERT to practical problems, such as information retrieval (IR). A survey\nthat focuses on a comprehensive analysis of prevalent approaches that apply\npretrained transformer encoders like BERT to IR can thus be useful for academia\nand the industry. In light of this, we revisit a variety of BERT-based methods\nin this survey, cover a wide range of techniques of IR, and group them into six\nhigh-level categories: (i) handling long documents, (ii) integrating semantic\ninformation, (iii) balancing effectiveness and efficiency, (iv) predicting the\nweights of terms, (v) query expansion, and (vi) document expansion. We also\nprovide links to resources, including datasets and toolkits, for BERT-based IR\nsystems. A key highlight of our survey is the comparison between BERT's\nencoder-based models and the latest generative Large Language Models (LLMs),\nsuch as ChatGPT, which rely on decoders. Despite the popularity of LLMs, we\nfind that for specific tasks, finely tuned BERT encoders still outperform, and\nat a lower deployment cost. Finally, we summarize the comprehensive outcomes of\nthe survey and suggest directions for future research in the area.\n""]",BERT-based NLP Models and Techniques,Natural Language Processing (NLP) Techniques and Applications,Natural Language Processing
40,40,104,40_privacy_adversary_security_attacks,"['privacy', 'adversary', 'security', 'attacks', 'secure', 'federated', 'confidentiality', 'malicious', 'attacker', 'protect']","['privacy', 'private', 'attacks', 'secure', 'federated', 'attack', 'server', 'data', 'encryption', 'protection']","[""  In this paper, we initiate the study of local model reconstruction attacks\nfor federated learning, where a honest-but-curious adversary eavesdrops the\nmessages exchanged between a targeted client and the server, and then\nreconstructs the local/personalized model of the victim. The local model\nreconstruction attack allows the adversary to trigger other classical attacks\nin a more effective way, since the local model only depends on the client's\ndata and can leak more private information than the global model learned by the\nserver. Additionally, we propose a novel model-based attribute inference attack\nin federated learning leveraging the local model reconstruction attack. We\nprovide an analytical lower-bound for this attribute inference attack.\nEmpirical results using real world datasets confirm that our local\nreconstruction attack works well for both regression and classification tasks.\nMoreover, we benchmark our novel attribute inference attack against the\nstate-of-the-art attacks in federated learning. Our attack results in higher\nreconstruction accuracy especially when the clients' datasets are\nheterogeneous. Our work provides a new angle for designing powerful and\nexplainable attacks to effectively quantify the privacy risk in FL.\n"", ""  Federated Learning (FL) has garnered significant attention for its potential\nto protect user privacy while enhancing model training efficiency. For that\nreason, FL has found its use in various domains, from healthcare to industrial\nengineering, especially where data cannot be easily exchanged due to sensitive\ninformation or privacy laws. However, recent research has demonstrated that FL\nprotocols can be easily compromised by active reconstruction attacks executed\nby dishonest servers. These attacks involve the malicious modification of\nglobal model parameters, allowing the server to obtain a verbatim copy of\nusers' private data by inverting their gradient updates. Tackling this class of\nattack remains a crucial challenge due to the strong threat model. In this\npaper, we propose a defense mechanism, namely OASIS, based on image\naugmentation that effectively counteracts active reconstruction attacks while\npreserving model performance. We first uncover the core principle of gradient\ninversion that enables these attacks and theoretically identify the main\nconditions by which the defense can be robust regardless of the attack\nstrategies. We then construct our defense with image augmentation showing that\nit can undermine the attack principle. Comprehensive evaluations demonstrate\nthe efficacy of the defense mechanism highlighting its feasibility as a\nsolution.\n"", '  Deep learning has shown incredible potential across a vast array of tasks and\naccompanying this growth has been an insatiable appetite for data. However, a\nlarge amount of data needed for enabling deep learning is stored on personal\ndevices and recent concerns on privacy have further highlighted challenges for\naccessing such data. As a result, federated learning (FL) has emerged as an\nimportant privacy-preserving technology enabling collaborative training of\nmachine learning models without the need to send the raw, potentially\nsensitive, data to a central server. However, the fundamental premise that\nsending model updates to a server is privacy-preserving only holds if the\nupdates cannot be ""reverse engineered"" to infer information about the private\ntraining data. It has been shown under a wide variety of settings that this\npremise for privacy does {\\em not} hold.\n  In this survey paper, we provide a comprehensive literature review of the\ndifferent privacy attacks and defense methods in FL. We identify the current\nlimitations of these attacks and highlight the settings in which FL client\nprivacy can be broken. We dissect some of the successful industry applications\nof FL and draw lessons for future successful adoption. We survey the emerging\nlandscape of privacy regulation for FL. We conclude with future directions for\ntaking FL toward the cherished goal of generating accurate models while\npreserving the privacy of the data from its participants.\n']",Federated Learning Security and Privacy Attacks,Federated Learning for Privacy-Preserving Machine Learning,Machine Learning and Data Privacy
41,41,104,41_answerability_answering_questions_annotators,"['answerability', 'answering', 'questions', 'annotators', 'retrieval', 'comprehension', 'responses', 'answers', 'questionnaires', 'texts']","['questions', 'answers', 'question', 'answer', 'comprehension', 'answering', 'hallucinations', 'evaluation', 'retrieval', 'hallucination']","['  Long-form question answering (LFQA) aims to provide thorough and in-depth\nanswers to complex questions, enhancing comprehension. However, such detailed\nresponses are prone to hallucinations and factual inconsistencies, challenging\ntheir faithful evaluation. This work introduces HaluQuestQA, the first\nhallucination dataset with localized error annotations for human-written and\nmodel-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 4.7k\nspan-level error annotations for five different error types by expert\nannotators, along with preference judgments. Using our collected data, we\nthoroughly analyze the shortcomings of long-form answers and find that they\nlack comprehensiveness and provide unhelpful references. We train an automatic\nfeedback model on this dataset that predicts error spans with incomplete\ninformation and provides associated explanations. Finally, we propose a\nprompt-based approach, Error-informed refinement, that uses signals from the\nlearned feedback model to refine generated answers, which we show reduces\nhallucination and improves answer quality. Furthermore, humans find answers\ngenerated by our approach comprehensive and highly prefer them (84%) over the\nbaseline answers.\n', '  Document-based Question-Answering (QA) tasks are crucial for precise\ninformation retrieval. While some existing work focus on evaluating large\nlanguage models performance on retrieving and answering questions from\ndocuments, assessing the LLMs performance on QA types that require exact answer\nselection from predefined options and numerical extraction is yet to be fully\nassessed. In this paper, we specifically focus on this underexplored context\nand conduct empirical analysis of LLMs (GPT-4 and GPT-3.5) on question types,\nincluding single-choice, yes-no, multiple-choice, and number extraction\nquestions from documents in zero-shot setting. We use the CogTale dataset for\nevaluation, which provide human expert-tagged responses, offering a robust\nbenchmark for precision and factual grounding. We found that LLMs, particularly\nGPT-4, can precisely answer many single-choice and yes-no questions given\nrelevant context, demonstrating their efficacy in information retrieval tasks.\nHowever, their performance diminishes when confronted with multiple-choice and\nnumber extraction formats, lowering the overall performance of the model on\nthis task, indicating that these models may not yet be sufficiently reliable\nfor the task. This limits the applications of LLMs on applications demanding\nprecise information extraction from documents, such as meta-analysis tasks.\nThese findings hinge on the assumption that the retrievers furnish pertinent\ncontext necessary for accurate responses, emphasizing the need for further\nresearch. Our work offers a framework for ongoing dataset evaluation, ensuring\nthat LLM applications for information retrieval and document analysis continue\nto meet evolving standards.\n', ""  Using questions in written text is an effective strategy to enhance\nreadability. However, what makes an active reading question good, what the\nlinguistic role of these questions is, and what is their impact on human\nreading remains understudied. We introduce GuidingQ, a dataset of 10K in-text\nquestions from textbooks and scientific articles. By analyzing the dataset, we\npresent a comprehensive understanding of the use, distribution, and linguistic\ncharacteristics of these questions. Then, we explore various approaches to\ngenerate such questions using language models. Our results highlight the\nimportance of capturing inter-question relationships and the challenge of\nquestion position identification in generating these questions. Finally, we\nconduct a human study to understand the implication of such questions on\nreading comprehension. We find that the generated questions are of high quality\nand are almost as effective as human-written questions in terms of improving\nreaders' memorization and comprehension.\n""]",Question Answering and Comprehension,Question Answering and Reasoning Systems,Artificial Intelligence and Reasoning Systems
42,42,103,42_tweets_twitter_sentiment_reddit,"['tweets', 'twitter', 'sentiment', 'reddit', 'hashtags', 'communities', 'sentiments', 'news', 'epidemic', 'content']","['media', 'social', 'posts', 'misinformation', 'communities', 'sentiment', 'news', 'tweets', 'online', 'discussions']","[""  The Covid-19 pandemic has sparked renewed attention on the prevalence of\nmisinformation online, whether intentional or not, underscoring the potential\nrisks posed to individuals' quality of life associated with the dissemination\nof misconceptions and enduring myths on health-related subjects. In this study,\nwe analyze 6 years (2016-2021) of Italian vaccine debate across diverse social\nmedia platforms (Facebook, Instagram, Twitter, YouTube), encompassing all major\nnews sources - both questionable and reliable. We first use the symbolic\ntransfer entropy analysis of news production time-series to dynamically\ndetermine which category of sources, questionable or reliable, causally drives\nthe agenda on vaccines. Then, leveraging deep learning models capable to\naccurately classify vaccine-related content based on the conveyed stance and\ndiscussed topic, respectively, we evaluate the focus on various topics by news\nsources promoting opposing views and compare the resulting user engagement.\nAside from providing valuable resources for further investigation of\nvaccine-related misinformation, particularly in a language (Italian) that\nreceives less attention in scientific research compared to languages like\nEnglish, our study uncovers misinformation not as a parasite of the news\necosystem that merely opposes the perspectives offered by mainstream media, but\nas an autonomous force capable of even overwhelming the production of\nvaccine-related content from the latter. While the pervasiveness of\nmisinformation is evident in the significantly higher engagement of\nquestionable sources compared to reliable ones, our findings underscore the\nimportance of consistent and thorough pro-vax coverage. This is especially\ncrucial in addressing the most sensitive topics where the risk of\nmisinformation spreading and potentially exacerbating negative attitudes toward\nvaccines among the users involved is higher.\n"", ""  With the advent of social media, an increasing number of netizens are sharing\nand reading posts and news online. However, the huge volumes of misinformation\n(e.g., fake news and rumors) that flood the internet can adversely affect\npeople's lives, and have resulted in the emergence of rumor and fake news\ndetection as a hot research topic. The emotions and sentiments of netizens, as\nexpressed in social media posts and news, constitute important factors that can\nhelp to distinguish fake news from genuine news and to understand the spread of\nrumors. This article comprehensively reviews emotion-based methods for\nmisinformation detection. We begin by explaining the strong links between\nemotions and misinformation. We subsequently provide a detailed analysis of a\nrange of misinformation detection methods that employ a variety of emotion,\nsentiment and stance-based features, and describe their strengths and\nweaknesses. Finally, we discuss a number of ongoing challenges in emotion-based\nmisinformation detection based on large language models and suggest future\nresearch directions, including data collection (multi-platform, multilingual),\nannotation, benchmark, multimodality, and interpretability.\n"", '  Social media is now the predominant source of information due to the\navailability of immediate public response. As a result, social media data has\nbecome a valuable resource for comprehending public sentiments. Studies have\nshown that it can amplify ideas and influence public sentiments. This study\nanalyzes the public perception of climate change and the environment over a\ndecade from 2014 to 2023. Using the Pointwise Mutual Information (PMI)\nalgorithm, we identify sentiment and explore prevailing emotions expressed\nwithin environmental tweets across various social media platforms, namely\nTwitter, Reddit, and YouTube. Accuracy on a human-annotated dataset was 0.65,\nhigher than Vader score but lower than that of an expert rater (0.90). Our\nfindings suggest that negative environmental tweets are far more common than\npositive or neutral ones. Climate change, air quality, emissions, plastic, and\nrecycling are the most discussed topics on all social media platforms,\nhighlighting its huge global concern. The most common emotions in environmental\ntweets are fear, trust, and anticipation, demonstrating public reactions wide\nand complex nature. By identifying patterns and trends in opinions related to\nthe environment, we hope to provide insights that can help raise awareness\nregarding environmental issues, inform the development of interventions, and\nadapt further actions to meet environmental challenges.\n']",Social Media Misinformation and Sentiment Analysis,Social Media and Information Dynamics,Information Dynamics and Network Influence
43,43,102,43_captioning_captions_multimodal_caption,"['captioning', 'captions', 'multimodal', 'caption', 'multilingual', 'visual', 'translation', 'textual', 'text', 'benchmark']","['captions', 'captioning', 'image', 'multimodal', 'visual', 'video', 'translation', 'text', 'story', 'caption']","['  We propose Wolf, a WOrLd summarization Framework for accurate video\ncaptioning. Wolf is an automated captioning framework that adopts a\nmixture-of-experts approach, leveraging complementary strengths of Vision\nLanguage Models (VLMs). By utilizing both image and video models, our framework\ncaptures different levels of information and summarizes them efficiently. Our\napproach can be applied to enhance video understanding, auto-labeling, and\ncaptioning. To evaluate caption quality, we introduce CapScore, an LLM-based\nmetric to assess the similarity and quality of generated captions compared to\nthe ground truth captions. We further build four human-annotated datasets in\nthree domains: autonomous driving, general scenes, and robotics, to facilitate\ncomprehensive comparisons. We show that Wolf achieves superior captioning\nperformance compared to state-of-the-art approaches from the research community\n(VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For\ninstance, in comparison with GPT-4V, Wolf improves CapScore both quality-wise\nby 55.6% and similarity-wise by 77.4% on challenging driving videos. Finally,\nwe establish a benchmark for video captioning and introduce a leaderboard,\naiming to accelerate advancements in video understanding, captioning, and data\nalignment. Leaderboard: https://wolfv0.github.io/leaderboard.html.\n', '  A good evaluation framework should evaluate multimodal machine translation\n(MMT) models by measuring 1) their use of visual information to aid in the\ntranslation task and 2) their ability to translate complex sentences such as\ndone for text-only machine translation. However, most current work in MMT is\nevaluated against the Multi30k testing sets, which do not measure these\nproperties. Namely, the use of visual information by the MMT model cannot be\nshown directly from the Multi30k test set results and the sentences in Multi30k\nare are image captions, i.e., short, descriptive sentences, as opposed to\ncomplex sentences that typical text-only machine translation models are\nevaluated against.\n  Therefore, we propose that MMT models be evaluated using 1) the CoMMuTE\nevaluation framework, which measures the use of visual information by MMT\nmodels, 2) the text-only WMT news translation task test sets, which evaluates\ntranslation performance against complex sentences, and 3) the Multi30k test\nsets, for measuring MMT model performance against a real MMT dataset. Finally,\nwe evaluate recent MMT models trained solely against the Multi30k dataset\nagainst our proposed evaluation framework and demonstrate the dramatic drop\nperformance against text-only testing sets compared to recent text-only MT\nmodels.\n', '  News image captioning requires model to generate an informative caption rich\nin entities, with the news image and the associated news article. Though\nMultimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in addressing various vision-language tasks, our research finds\nthat current MLLMs still bear limitations in handling entity information on\nnews image captioning task. Besides, while MLLMs have the ability to process\nlong inputs, generating high-quality news image captions still requires a\ntrade-off between sufficiency and conciseness of textual input information. To\nexplore the potential of MLLMs and address problems we discovered, we propose :\nan Entity-Aware Multimodal Alignment based approach for news image captioning.\nOur approach first aligns the MLLM through Balance Training Strategy with two\nextra alignment tasks: Entity-Aware Sentence Selection task and Entity\nSelection task, together with News Image Captioning task, to enhance its\ncapability in handling multimodal entity information. The aligned MLLM will\nutilizes the additional entity-related information it explicitly extracts to\nsupplement its textual input while generating news image captions. Our approach\nachieves better results than all previous models in CIDEr score on GoodNews\ndataset (72.33 -> 88.39) and NYTimes800k dataset (70.83 -> 85.61).\n']",Multimodal Captioning and Translation,Multimodal Vision-Language Understanding and Generation,Multimodal Learning and Vision-Language Models
44,44,102,44_speechtokenizer_voice_vocalizations_voices,"['speechtokenizer', 'voice', 'vocalizations', 'voices', 'utterances', 'audio', 'speaker', 'speech', 'vocal', 'acoustic']","['speech', 'speaker', 'voice', 'audio', 'prosody', 'style', 'emotion', 'synthesis', 'conversion', 'emotional']","[""  This paper presents a method for end-to-end cross-lingual text-to-speech\n(TTS) which aims to preserve the target language's pronunciation regardless of\nthe original speaker's language. The model used is based on a non-attentive\nTacotron architecture, where the decoder has been replaced with a normalizing\nflow network conditioned on the speaker identity, allowing both TTS and voice\nconversion (VC) to be performed by the same model due to the inherent\nlinguistic content and speaker identity disentanglement. When used in a\ncross-lingual setting, acoustic features are initially produced with a native\nspeaker of the target language and then voice conversion is applied by the same\nmodel in order to convert these features to the target speaker's voice. We\nverify through objective and subjective evaluations that our method can have\nbenefits compared to baseline cross-lingual synthesis. By including speakers\naveraging 7.5 minutes of speech, we also present positive results on\nlow-resource scenarios.\n"", ""  Recently, zero-shot text-to-speech (TTS) systems, capable of synthesizing any\nspeaker's voice from a short audio prompt, have made rapid advancements.\nHowever, the quality of the generated speech significantly deteriorates when\nthe audio prompt contains noise, and limited research has been conducted to\naddress this issue. In this paper, we explored various strategies to enhance\nthe quality of audio generated from noisy audio prompts within the context of\nflow-matching-based zero-shot TTS. Our investigation includes comprehensive\ntraining strategies: unsupervised pre-training with masked speech denoising,\nmulti-speaker detection and DNSMOS-based data filtering on the pre-training\ndata, and fine-tuning with random noise mixing. The results of our experiments\ndemonstrate significant improvements in intelligibility, speaker similarity,\nand overall audio quality compared to the approach of applying speech\nenhancement to the audio prompt.\n"", '  In speech synthesis, modeling of rich emotions and prosodic variations\npresent in human voice are crucial to synthesize natural speech. Although\nspeaker embeddings have been widely used in personalized speech synthesis as\nconditioning inputs, they are designed to lose variation to optimize speaker\nrecognition accuracy. Thus, they are suboptimal for speech synthesis in terms\nof modeling the rich variations at the output speech distribution. In this\nwork, we propose a novel speaker embedding network which utilizes multiple\nclass centers in the speaker classification training rather than a single class\ncenter as traditional embeddings. The proposed approach introduces variations\nin the speaker embedding while retaining the speaker recognition performance\nsince model does not have to map all of the utterances of a speaker into a\nsingle class center. We apply our proposed embedding in voice conversion task\nand show that our method provides better naturalness and prosody in synthesized\nspeech.\n']",Speech Synthesis and Voice Conversion,Speech and Voice Synthesis,Speech and Audio Processing
45,45,101,45_reinforcement_reward_rewards_learning,"['reinforcement', 'reward', 'rewards', 'learning', 'rl', 'offline', 'learned', 'learner', 'optimal', 'exploration']","['reward', 'offline', 'policy', 'reinforcement', 'rewards', 'feedback', 'expert', 'preferences', 'inverse', 'function']","[""  In preference-based reinforcement learning (PbRL), a reward function is\nlearned from a type of human feedback called preference. To expedite preference\ncollection, recent works have leveraged \\emph{offline preferences}, which are\npreferences collected for some offline data. In this scenario, the learned\nreward function is fitted on the offline data. If a learning agent exhibits\nbehaviors that do not overlap with the offline data, the learned reward\nfunction may encounter generalizability issues. To address this problem, the\npresent study introduces a framework that consolidates offline preferences and\n\\emph{virtual preferences} for PbRL, which are comparisons between the agent's\nbehaviors and the offline data. Critically, the reward function can track the\nagent's behaviors using the virtual preferences, thereby offering well-aligned\nguidance to the agent. Through experiments on continuous control tasks, this\nstudy demonstrates the effectiveness of incorporating the virtual preferences\nin PbRL.\n"", ""  Inverse Reinforcement Learning (IRL) is a powerful framework for learning\ncomplex behaviors from expert demonstrations. However, it traditionally\nrequires repeatedly solving a computationally expensive reinforcement learning\n(RL) problem in its inner loop. It is desirable to reduce the exploration\nburden by leveraging expert demonstrations in the inner-loop RL. As an example,\nrecent work resets the learner to expert states in order to inform the learner\nof high-reward expert states. However, such an approach is infeasible in the\nreal world. In this work, we consider an alternative approach to speeding up\nthe RL subroutine in IRL: \\emph{pessimism}, i.e., staying close to the expert's\ndata distribution, instantiated via the use of offline RL algorithms. We\nformalize a connection between offline RL and IRL, enabling us to use an\narbitrary offline RL algorithm to improve the sample efficiency of IRL. We\nvalidate our theory experimentally by demonstrating a strong correlation\nbetween the efficacy of an offline RL algorithm and how well it works as part\nof an IRL procedure. By using a strong offline RL algorithm as part of an IRL\nprocedure, we are able to find policies that match expert performance\nsignificantly more efficiently than the prior art.\n"", '  Offline reinforcement learning has become one of the most practical RL\nsettings. A recent success story has been RLHF, offline preference-based RL\n(PBRL) with preference from humans. However, most existing works on offline RL\nfocus on the standard setting with scalar reward feedback. It remains unknown\nhow to universally transfer the existing rich understanding of offline RL from\nthe reward-based to the preference-based setting. In this work, we propose a\ngeneral framework to bridge this gap. Our key insight is transforming\npreference feedback to scalar rewards via optimal reward labeling (ORL), and\nthen any reward-based offline RL algorithms can be applied to the dataset with\nthe reward labels. We theoretically show the connection between several recent\nPBRL techniques and our framework combined with specific offline RL algorithms\nin terms of how they utilize the preference signals. By combining reward\nlabeling with different algorithms, our framework can lead to new and\npotentially more efficient offline PBRL algorithms. We empirically test our\nframework on preference datasets based on the standard D4RL benchmark. When\ncombined with a variety of efficient reward-based offline RL algorithms, the\nlearning result achieved under our framework is comparable to training the same\nalgorithm on the dataset with actual rewards in many cases and better than the\nrecent PBRL baselines in most cases.\n']",Offline Reinforcement Learning with Preferences,Reinforcement Learning Methods and Applications,Reinforcement Learning
45,45,101,45_reinforcement_reward_rewards_learning,"['reinforcement', 'reward', 'rewards', 'learning', 'rl', 'offline', 'learned', 'learner', 'optimal', 'exploration']","['reward', 'offline', 'policy', 'reinforcement', 'rewards', 'feedback', 'expert', 'preferences', 'inverse', 'function']","[""  In preference-based reinforcement learning (PbRL), a reward function is\nlearned from a type of human feedback called preference. To expedite preference\ncollection, recent works have leveraged \\emph{offline preferences}, which are\npreferences collected for some offline data. In this scenario, the learned\nreward function is fitted on the offline data. If a learning agent exhibits\nbehaviors that do not overlap with the offline data, the learned reward\nfunction may encounter generalizability issues. To address this problem, the\npresent study introduces a framework that consolidates offline preferences and\n\\emph{virtual preferences} for PbRL, which are comparisons between the agent's\nbehaviors and the offline data. Critically, the reward function can track the\nagent's behaviors using the virtual preferences, thereby offering well-aligned\nguidance to the agent. Through experiments on continuous control tasks, this\nstudy demonstrates the effectiveness of incorporating the virtual preferences\nin PbRL.\n"", ""  Inverse Reinforcement Learning (IRL) is a powerful framework for learning\ncomplex behaviors from expert demonstrations. However, it traditionally\nrequires repeatedly solving a computationally expensive reinforcement learning\n(RL) problem in its inner loop. It is desirable to reduce the exploration\nburden by leveraging expert demonstrations in the inner-loop RL. As an example,\nrecent work resets the learner to expert states in order to inform the learner\nof high-reward expert states. However, such an approach is infeasible in the\nreal world. In this work, we consider an alternative approach to speeding up\nthe RL subroutine in IRL: \\emph{pessimism}, i.e., staying close to the expert's\ndata distribution, instantiated via the use of offline RL algorithms. We\nformalize a connection between offline RL and IRL, enabling us to use an\narbitrary offline RL algorithm to improve the sample efficiency of IRL. We\nvalidate our theory experimentally by demonstrating a strong correlation\nbetween the efficacy of an offline RL algorithm and how well it works as part\nof an IRL procedure. By using a strong offline RL algorithm as part of an IRL\nprocedure, we are able to find policies that match expert performance\nsignificantly more efficiently than the prior art.\n"", '  Offline reinforcement learning has become one of the most practical RL\nsettings. A recent success story has been RLHF, offline preference-based RL\n(PBRL) with preference from humans. However, most existing works on offline RL\nfocus on the standard setting with scalar reward feedback. It remains unknown\nhow to universally transfer the existing rich understanding of offline RL from\nthe reward-based to the preference-based setting. In this work, we propose a\ngeneral framework to bridge this gap. Our key insight is transforming\npreference feedback to scalar rewards via optimal reward labeling (ORL), and\nthen any reward-based offline RL algorithms can be applied to the dataset with\nthe reward labels. We theoretically show the connection between several recent\nPBRL techniques and our framework combined with specific offline RL algorithms\nin terms of how they utilize the preference signals. By combining reward\nlabeling with different algorithms, our framework can lead to new and\npotentially more efficient offline PBRL algorithms. We empirically test our\nframework on preference datasets based on the standard D4RL benchmark. When\ncombined with a variety of efficient reward-based offline RL algorithms, the\nlearning result achieved under our framework is comparable to training the same\nalgorithm on the dataset with actual rewards in many cases and better than the\nrecent PBRL baselines in most cases.\n']",Offline Reinforcement Learning with Preferences,Reinforcement Learning Methods and Applications,Reinforcement Learning
46,46,101,46_variational_dynamical_neural_stochastic,"['variational', 'dynamical', 'neural', 'stochastic', 'dynamics', 'kalman', 'nonlinear', 'ensemble', 'spatiotemporal', 'simulations']","['dynamical', 'assimilation', 'series', 'equations', 'time', 'differential', 'uncertainty', 'latent', 'dynamics', 'observations']","['  Modeling dynamical systems, e.g. in climate and engineering sciences, often\nnecessitates solving partial differential equations. Neural operators are deep\nneural networks designed to learn nontrivial solution operators of such\ndifferential equations from data. As for all statistical models, the\npredictions of these models are imperfect and exhibit errors. Such errors are\nparticularly difficult to spot in the complex nonlinear behaviour of dynamical\nsystems. We introduce a new framework for approximate Bayesian uncertainty\nquantification in neural operators using function-valued Gaussian processes.\nOur approach can be interpreted as a probabilistic analogue of the concept of\ncurrying from functional programming and provides a practical yet theoretically\nsound way to apply the linearized Laplace approximation to neural operators. In\na case study on Fourier neural operators, we show that, even for a discretized\ninput, our method yields a Gaussian closure--a structured Gaussian process\nposterior capturing the uncertainty in the output function of the neural\noperator, which can be evaluated at an arbitrary set of points. The method adds\nminimal prediction overhead, can be applied post-hoc without retraining the\nneural operator, and scales to large models and datasets. We showcase the\nefficacy of our approach through applications to different types of partial\ndifferential equations.\n', '  With the increasing availability of large scale datasets, computational power\nand tools like automatic differentiation and expressive neural network\narchitectures, sequential data are now often treated in a data-driven way, with\na dynamical model trained from the observation data. While neural networks are\noften seen as uninterpretable black-box architectures, they can still benefit\nfrom physical priors on the data and from mathematical knowledge. In this\npaper, we use a neural network architecture which leverages the long-known\nKoopman operator theory to embed dynamical systems in latent spaces where their\ndynamics can be described linearly, enabling a number of appealing features. We\nintroduce methods that enable to train such a model for long-term continuous\nreconstruction, even in difficult contexts where the data comes in\nirregularly-sampled time series. The potential for self-supervised learning is\nalso demonstrated, as we show the promising use of trained dynamical models as\npriors for variational data assimilation techniques, with applications to e.g.\ntime series interpolation and forecasting.\n', '  Incorporating unstructured data into physical models is a challenging problem\nthat is emerging in data assimilation. Traditional approaches focus on\nwell-defined observation operators whose functional forms are typically assumed\nto be known. This prevents these methods from achieving a consistent model-data\nsynthesis in configurations where the mapping from data-space to model-space is\nunknown. To address these shortcomings, in this paper we develop a\nphysics-informed dynamical variational autoencoder ($\\Phi$-DVAE) to embed\ndiverse data streams into time-evolving physical systems described by\ndifferential equations. Our approach combines a standard, possibly nonlinear,\nfilter for the latent state-space model and a VAE, to assimilate the\nunstructured data into the latent dynamical system. Unstructured data, in our\nexample systems, comes in the form of video data and velocity field\nmeasurements, however the methodology is suitably generic to allow for\narbitrary unknown observation operators. A variational Bayesian framework is\nused for the joint estimation of the encoding, latent states, and unknown\nsystem parameters. To demonstrate the method, we provide case studies with the\nLorenz-63 ordinary differential equation, and the advection and Korteweg-de\nVries partial differential equations. Our results, with synthetic data, show\nthat $\\Phi$-DVAE provides a data efficient dynamics encoding methodology which\nis competitive with standard approaches. Unknown parameters are recovered with\nuncertainty quantification, and unseen data are accurately predicted.\n']",Neural Operators for Dynamical Systems,Neural Networks for Modeling and Control of Dynamical Systems,Machine Learning for Dynamical Systems and Differential Equations
47,47,101,47_diffusion_denoising_denoiser_gans,"['diffusion', 'denoising', 'denoiser', 'gans', 'denoised', 'generative', 'imagenet', 'images', 'modeling', 'gan']","['diffusion', 'denoising', 'image', 'resolution', 'generative', 'conditional', 'images', 'noise', 'quality', 'generation']","['  Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.\n', '  Image super-resolution is a fundamentally ill-posed problem because multiple\nvalid high-resolution images exist for one low-resolution image.\nSuper-resolution methods based on diffusion probabilistic models can deal with\nthe ill-posed nature by learning the distribution of high-resolution images\nconditioned on low-resolution images, avoiding the problem of blurry images in\nPSNR-oriented methods. However, existing diffusion-based super-resolution\nmethods have high time consumption with the use of iterative sampling, while\nthe quality and consistency of generated images are less than ideal due to\nproblems like color shifting. In this paper, we propose Efficient Conditional\nDiffusion Model with Probability Flow Sampling (ECDP) for image\nsuper-resolution. To reduce the time consumption, we design a continuous-time\nconditional diffusion model for image super-resolution, which enables the use\nof probability flow sampling for efficient generation. Additionally, to improve\nthe consistency of generated images, we propose a hybrid parametrization for\nthe denoiser network, which interpolates between the data-predicting\nparametrization and the noise-predicting parametrization for different noise\nscales. Moreover, we design an image quality loss as a complement to the score\nmatching loss of diffusion models, further improving the consistency and\nquality of super-resolution. Extensive experiments on DIV2K, ImageNet, and\nCelebA demonstrate that our method achieves higher super-resolution quality\nthan existing diffusion-based image super-resolution methods while having lower\ntime consumption. Our code is available at https://github.com/Yuan-Yutao/ECDP.\n', '  Deep generative models have garnered significant attention in low-level\nvision tasks due to their generative capabilities. Among them, diffusion\nmodel-based solutions, characterized by a forward diffusion process and a\nreverse denoising process, have emerged as widely acclaimed for their ability\nto produce samples of superior quality and diversity. This ensures the\ngeneration of visually compelling results with intricate texture information.\nDespite their remarkable success, a noticeable gap exists in a comprehensive\nsurvey that amalgamates these pioneering diffusion model-based works and\norganizes the corresponding threads. This paper proposes the comprehensive\nreview of diffusion model-based techniques. We present three generic diffusion\nmodeling frameworks and explore their correlations with other deep generative\nmodels, establishing the theoretical foundation. Following this, we introduce a\nmulti-perspective categorization of diffusion models, considering both the\nunderlying framework and the target task. Additionally, we summarize extended\ndiffusion models applied in other tasks, including medical, remote sensing, and\nvideo scenarios. Moreover, we provide an overview of commonly used benchmarks\nand evaluation metrics. We conduct a thorough evaluation, encompassing both\nperformance and efficiency, of diffusion model-based techniques in three\nprominent tasks. Finally, we elucidate the limitations of current diffusion\nmodels and propose seven intriguing directions for future research. This\ncomprehensive examination aims to facilitate a profound understanding of the\nlandscape surrounding denoising diffusion models in the context of low-level\nvision tasks. A curated list of diffusion model-based techniques in over 20\nlow-level vision tasks can be found at\nhttps://github.com/ChunmingHe/awesome-diffusion-models-in-low-level-vision.\n']",Diffusion Models for Image Restoration,Image Restoration and Denoising Techniques,Image and Video Processing
48,48,99,48_segmentation_imaging_supervised_mri,"['segmentation', 'imaging', 'supervised', 'mri', 'microscopy', 'tomography', 'images', 'datasets', 'unsupervised', 'detection']","['segmentation', 'imaging', 'images', 'medical', 'image', 'ray', 'microscopy', 'cell', 'reconstruction', 'brain']","['  Diffusion probabilistic models (DPMs) have exhibited significant\neffectiveness in computer vision tasks, particularly in image generation.\nHowever, their notable performance heavily relies on labelled datasets, which\nlimits their application in medical images due to the associated high-cost\nannotations. Current DPM-related methods for lesion detection in medical\nimaging, which can be categorized into two distinct approaches, primarily rely\non image-level annotations. The first approach, based on anomaly detection,\ninvolves learning reference healthy brain representations and identifying\nanomalies based on the difference in inference results. In contrast, the second\napproach, resembling a segmentation task, employs only the original brain\nmulti-modalities as prior information for generating pixel-level annotations.\nIn this paper, our proposed model - discrepancy distribution medical diffusion\n(DDMD) - for lesion detection in brain MRI introduces a novel framework by\nincorporating distinctive discrepancy features, deviating from the conventional\ndirect reliance on image-level annotations or the original brain modalities. In\nour method, the inconsistency in image-level annotations is translated into\ndistribution discrepancies among heterogeneous samples while preserving\ninformation within homogeneous samples. This property retains pixel-wise\nuncertainty and facilitates an implicit ensemble of segmentation, ultimately\nenhancing the overall detection performance. Thorough experiments conducted on\nthe BRATS2020 benchmark dataset containing multimodal MRI scans for brain\ntumour detection demonstrate the great performance of our approach in\ncomparison to state-of-the-art methods.\n', '  Understanding the morphological structure of medical images and precisely\nsegmenting the region of interest or abnormality is an important task that can\nassist in diagnosis. However, the unique properties of medical imaging make\nclear segmentation difficult, and the high cost and time-consuming task of\nlabeling leads to a coarse-grained representation of ground truth. Facing with\nthese problems, we propose a novel Diffusion Transformer Segmentation (DTS)\nmodel for robust segmentation in the presence of noise. We propose an\nalternative to the dominant Denoising U-Net encoder through experiments\napplying a transformer architecture, which captures global dependency through\nself-attention. Additionally, we propose k-neighbor label smoothing, reverse\nboundary attention, and self-supervised learning with morphology-driven\nlearning to improve the ability to identify complex structures. Our model,\nwhich analyzes the morphological representation of images, shows better results\nthan the previous models in various medical imaging modalities, including CT,\nMRI, and lesion images.\n', '  Image segmentation, the process of partitioning an image into meaningful\nregions, plays a pivotal role in computer vision and medical imaging\napplications. Unsupervised segmentation, particularly in the absence of labeled\ndata, remains a challenging task due to the inter-class similarity and\nvariations in intensity and resolution. In this study, we extract high-level\nfeatures of the input image using pretrained vision transformer. Subsequently,\nthe proposed method leverages the underlying graph structures of the images,\nseeking to discover and delineate meaningful boundaries using graph neural\nnetworks and modularity based optimization criteria without relying on\npre-labeled training data. Experimental results on benchmark datasets\ndemonstrate the effectiveness and versatility of the proposed approach,\nshowcasing competitive performance compared to the state-of-the-art\nunsupervised segmentation methods. This research contributes to the broader\nfield of unsupervised medical imaging and computer vision by presenting an\ninnovative methodology for image segmentation that aligns with real-world\nchallenges. The proposed method holds promise for diverse applications,\nincluding medical imaging, remote sensing, and object recognition, where\nlabeled data may be scarce or unavailable. The github repository of the code is\navailable on [https://github.com/ksgr5566/unseggnet]\n']",Medical Image Segmentation and Detection,Medical Image Analysis,Medical Imaging and Reporting
49,49,99,49_attention_memory_sparse_recurrent,"['attention', 'memory', 'sparse', 'recurrent', 'recall', 'tasks', 'efficient', 'language', 'chunk', 'longlora']","['attention', 'memory', 'transformer', 'tokens', 'length', 'long', 'sparse', 'layers', 'sequence', 'language']","[""  The transformer architecture has driven breakthroughs in recent years on\ntasks which require modeling pairwise relationships between sequential\nelements, as is the case in natural language understanding. However, long\nseqeuences pose a problem due to the quadratic complexity of the attention\noperation. Previous research has aimed to lower the complexity by sparsifying\nor linearly approximating the attention matrix. Yet, these approaches cannot\nstraightforwardly distill knowledge from a teacher's attention matrix and often\nrequire complete retraining from scratch. Furthermore, previous sparse and\nlinear approaches lose interpretability if they cannot produce full attention\nmatrices. To address these challenges, we propose SEA: Sparse linear attention\nwith an Estimated Attention mask. SEA estimates the attention matrix with\nlinear complexity via kernel-based linear attention, then subsequently creates\na sparse attention matrix with a top-k selection to perform a sparse attention\noperation. For language modeling tasks (Wikitext2), previous linear and sparse\nattention methods show roughly two-fold worse perplexity scores over the\nquadratic OPT-1.3B baseline, while SEA achieves better perplexity than\nOPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable\nattention matrix. We believe that our work will have a large practical impact,\nas it opens the possibility of running large transformers on resource-limited\ndevices with less memory.\n"", '  Extending the functionality of the Transformer model to accommodate longer\nsequence lengths has become a critical challenge. This extension is crucial not\nonly for improving tasks such as language translation and long-context\nprocessing but also for enabling novel applications like chatbots, code\ngeneration, and multimedia content creation. The primary obstacle is the\nself-attention mechanism, which scales quadratically with sequence length in\nterms of computation time and memory requirements. LongLoRA proposed shifted\nsparse attention (S\\(^2\\)-Attn), effectively enabling context extension and\nleading to non-trivial computation savings with similar performance to\nfine-tuning with vanilla attention. However, LongLoRA is still not as efficient\nas vanilla attention, reaching only 39\\% of the perplexity improvement compared\nto full attention. This inefficiency is due to the cyclic shift applied within\ndifferent attention head patterns, causing either chaos in the attention head\nstructure or unnecessary information exchange between token groups. To address\nthese issues, We propose \\textbf{SinkLoRA}, which features better work\npartitioning. Specifically, (1) we developed SF-Attn with a segmentation and\nreassembly algorithm to proportionally return cyclically shifted groups of\nattention heads to their un-shifted state together with global attention of\n""sink attention tokens"", achieving 92\\% of the perplexity improvement compared\nto full attention after fine tuning, and (2) applied a SOTA KV cache\ncompression algorithm H$_2$O to accelerate inference. Furthermore, We conducted\nsupervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus\ndataset. All our code, models, datasets, and demos are available at\n\\url{https://github.com/Dexter-GT-86/SinkLoRA}.\n', ""  In modern large language models (LLMs), increasing sequence lengths is a\ncrucial challenge for enhancing their comprehension and coherence in handling\ncomplex tasks such as multi-modal question answering. However, handling long\ncontext sequences with LLMs is prohibitively costly due to the conventional\nattention mechanism's quadratic time and space complexity, and the context\nwindow size is limited by the GPU memory. Although recent works have proposed\nlinear and sparse attention mechanisms to address this issue, their real-world\napplicability is often limited by the need to re-train pre-trained models. In\nresponse, we propose a novel approach, Hierarchically Pruned Attention (HiP),\nwhich simultaneously reduces the training and inference time complexity from\n$O(T^2)$ to $O(T \\log T)$ and the space complexity from $O(T^2)$ to $O(T)$. To\nthis end, we devise a dynamic sparse attention mechanism that generates an\nattention mask through a novel tree-search-like algorithm for a given query on\nthe fly. HiP is training-free as it only utilizes the pre-trained attention\nscores to spot the positions of the top-$k$ most significant elements for each\nquery. Moreover, it ensures that no token is overlooked, unlike the sliding\nwindow-based sub-quadratic attention methods, such as StreamingLLM. Extensive\nexperiments on diverse real-world benchmarks demonstrate that HiP significantly\nreduces prompt (i.e., prefill) and decoding latency and memory usage while\nmaintaining high generation performance with little or no degradation. As HiP\nallows pretrained LLMs to scale to millions of tokens on commodity GPUs with no\nadditional engineering due to its easy plug-and-play deployment, we believe\nthat our work will have a large practical impact, opening up the possibility to\nmany long-context LLM applications previously infeasible.\n""]",Efficient Attention Mechanisms for Long Sequences,Efficient Attention Mechanisms for Large Language Models,Large Language Models
50,50,98,50_utterances_phonetics_phonetic_speech,"['utterances', 'phonetics', 'phonetic', 'speech', 'phonetically', 'voice', 'corpus', 'asr', 'pronunciation', 'multilingual']","['speech', 'whisper', 'automatic', 'speakers', 'spoken', 'gender', 'recognition', 'voice', 'phonetic', 'transcriptions']","['  Whisper is a multitask and multilingual speech model covering 99 languages.\nIt yields commendable automatic speech recognition (ASR) results in a subset of\nits covered languages, but the model still underperforms on a non-negligible\nnumber of under-represented languages, a problem exacerbated in smaller model\nversions. In this work, we examine its limitations, demonstrating the presence\nof speaker-related (gender, age) and model-related (resourcefulness and model\nsize) bias. Despite that, we show that only model-related bias are amplified by\nquantization, impacting more low-resource languages and smaller models.\nSearching for a better compression approach, we propose DistilWhisper, an\napproach that is able to bridge the performance gap in ASR for these languages\nwhile retaining the advantages of multitask and multilingual capabilities. Our\napproach involves two key strategies: lightweight modular ASR fine-tuning of\nwhisper-small using language-specific experts, and knowledge distillation from\nwhisper-large-v2. This dual approach allows us to effectively boost ASR\nperformance while keeping the robustness inherited from the multitask and\nmultilingual pre-training. Results demonstrate that our approach is more\neffective than standard fine-tuning or LoRA adapters, boosting performance in\nthe targeted languages for both in- and out-of-domain test sets, while\nintroducing only a negligible parameter overhead at inference.\n', ""  One of the central skills that language learners need to practice is speaking\nthe language. Currently, students in school do not get enough speaking\nopportunities and lack conversational practice. Recent advances in speech\ntechnology and natural language processing allow for the creation of novel\ntools to practice their speaking skills. In this work, we tackle the first\ncomponent of such a pipeline, namely, the automated speech recognition module\n(ASR), which faces a number of challenges: first, state-of-the-art ASR models\nare often trained on adult read-aloud data by native speakers and do not\ntransfer well to young language learners' speech. Second, most ASR systems\ncontain a powerful language model, which smooths out errors made by the\nspeakers. To give corrective feedback, which is a crucial part of language\nlearning, the ASR systems in our setting need to preserve the errors made by\nthe language learners. In this work, we build an ASR system that satisfies\nthese requirements: it works on spontaneous speech by young language learners\nand preserves their errors. For this, we collected a corpus containing around\n85 hours of English audio spoken by learners in Switzerland from grades 4 to 6\non different language learning tasks, which we used to train an ASR model. Our\nexperiments show that our model benefits from direct fine-tuning on children's\nvoices and has a much higher error preservation rate than other models.\n"", '  Automatic reading diagnosis systems can benefit both teachers for more\nefficient scoring of reading exercises and students for accessing reading\nexercises with feedback more easily. However, there are limited studies on\nAutomatic Speech Recognition (ASR) for child speech in languages other than\nEnglish, and limited research on ASR-based reading diagnosis systems. This\nstudy investigates how efficiently state-of-the-art (SOTA) pretrained ASR\nmodels recognize Dutch native children speech and manage to detect reading\nmiscues. We found that Hubert Large finetuned on Dutch speech achieves SOTA\nphoneme-level child speech recognition (PER at 23.1\\%), while Whisper (Faster\nWhisper Large-v2) achieves SOTA word-level performance (WER at 9.8\\%). Our\nfindings suggest that Wav2Vec2 Large and Whisper are the two best ASR models\nfor reading miscue detection. Specifically, Wav2Vec2 Large shows the highest\nrecall at 0.83, whereas Whisper exhibits the highest precision at 0.52 and an\nF1 score of 0.52.\n']",Automatic Speech Recognition (ASR) for Multilingual and Child Speech,Speech Processing and Recognition Systems,Speech and Audio Processing
51,51,97,51_multimodal_embedding_supervised_modality,"['multimodal', 'embedding', 'supervised', 'modality', 'ehr', 'ehrs', 'health', 'medical', 'structured', 'unstructured']","['clinical', 'series', 'pain', 'patient', 'healthcare', 'disease', 'multimodal', 'medical', 'patients', 'time']","[""  Electronic health record (EHR) systems contain a wealth of multimodal\nclinical data including structured data like clinical codes and unstructured\ndata such as clinical notes. However, many existing EHR-focused studies has\ntraditionally either concentrated on an individual modality or merged different\nmodalities in a rather rudimentary fashion. This approach often results in the\nperception of structured and unstructured data as separate entities, neglecting\nthe inherent synergy between them. Specifically, the two important modalities\ncontain clinically relevant, inextricably linked and complementary health\ninformation. A more complete picture of a patient's medical history is captured\nby the joint analysis of the two modalities of data. Despite the great success\nof multimodal contrastive learning on vision-language, its potential remains\nunder-explored in the realm of multimodal EHR, particularly in terms of its\ntheoretical understanding. To accommodate the statistical analysis of\nmultimodal EHR data, in this paper, we propose a novel multimodal feature\nembedding generative model and design a multimodal contrastive loss to obtain\nthe multimodal EHR feature representation. Our theoretical analysis\ndemonstrates the effectiveness of multimodal learning compared to\nsingle-modality learning and connects the solution of the loss function to the\nsingular value decomposition of a pointwise mutual information matrix. This\nconnection paves the way for a privacy-preserving algorithm tailored for\nmultimodal EHR feature representation learning. Simulation studies show that\nthe proposed algorithm performs well under a variety of configurations. We\nfurther validate the clinical utility of the proposed algorithm in real-world\nEHR data.\n"", '  The integration of multimodal Electronic Health Records (EHR) data has\nsignificantly improved clinical predictive capabilities. Leveraging clinical\nnotes and multivariate time-series EHR, existing models often lack the medical\ncontext relevent to clinical tasks, prompting the incorporation of external\nknowledge, particularly from the knowledge graph (KG). Previous approaches with\nKG knowledge have primarily focused on structured knowledge extraction,\nneglecting unstructured data modalities and semantic high dimensional medical\nknowledge. In response, we propose REALM, a Retrieval-Augmented Generation\n(RAG) driven framework to enhance multimodal EHR representations that address\nthese limitations. Firstly, we apply Large Language Model (LLM) to encode long\ncontext clinical notes and GRU model to encode time-series EHR data. Secondly,\nwe prompt LLM to extract task-relevant medical entities and match entities in\nprofessionally labeled external knowledge graph (PrimeKG) with corresponding\nmedical knowledge. By matching and aligning with clinical standards, our\nframework eliminates hallucinations and ensures consistency. Lastly, we propose\nan adaptive multimodal fusion network to integrate extracted knowledge with\nmultimodal EHR data. Our extensive experiments on MIMIC-III mortality and\nreadmission tasks showcase the superior performance of our REALM framework over\nbaselines, emphasizing the effectiveness of each module. REALM framework\ncontributes to refining the use of multimodal EHR data in healthcare and\nbridging the gap with nuanced medical context essential for informed clinical\npredictions.\n', ""  The integration of multimodal Electronic Health Records (EHR) data has\nnotably advanced clinical predictive capabilities. However, current models that\nutilize clinical notes and multivariate time-series EHR data often lack the\nnecessary medical context for precise clinical tasks. Previous methods using\nknowledge graphs (KGs) primarily focus on structured knowledge extraction. To\naddress this, we propose EMERGE, a Retrieval-Augmented Generation (RAG) driven\nframework aimed at enhancing multimodal EHR predictive modeling. Our approach\nextracts entities from both time-series data and clinical notes by prompting\nLarge Language Models (LLMs) and aligns them with professional PrimeKG to\nensure consistency. Beyond triplet relationships, we include entities'\ndefinitions and descriptions to provide richer semantics. The extracted\nknowledge is then used to generate task-relevant summaries of patients' health\nstatuses. These summaries are fused with other modalities utilizing an adaptive\nmultimodal fusion network with cross-attention. Extensive experiments on the\nMIMIC-III and MIMIC-IV datasets for in-hospital mortality and 30-day\nreadmission tasks demonstrate the superior performance of the EMERGE framework\ncompared to baseline models. Comprehensive ablation studies and analyses\nunderscore the efficacy of each designed module and the framework's robustness\nto data sparsity. EMERGE significantly enhances the use of multimodal EHR data\nin healthcare, bridging the gap with nuanced medical contexts crucial for\ninformed clinical predictions.\n""]",Multimodal EHR Representation Learning,Multimodal Learning and Model Integration for Healthcare and Multitask Applications,Multimodal Learning and Applications
52,52,96,52_privacy_federated_private_adversarial,"['privacy', 'federated', 'private', 'adversarial', 'distributed', 'sharing', 'learning', 'secret', 'security', 'decentralized']","['privacy', 'private', 'clients', 'differential', 'server', 'client', 'sharing', 'local', 'attacks', 'secure']","['  Federated learning (FL) is gaining increasing popularity in the medical\ndomain for analyzing medical images, which is considered an effective technique\nto safeguard sensitive patient data and comply with privacy regulations.\nHowever, several recent studies have revealed that the default settings of FL\nmay leak private training data under privacy attacks. Thus, it is still unclear\nwhether and to what extent such privacy risks of FL exist in the medical\ndomain, and if so, ""how to mitigate such risks?"". In this paper, first, we\npropose a holistic framework for Medical data Privacy risk analysis and\nmitigation in Federated Learning (MedPFL) to analyze privacy risks and develop\neffective mitigation strategies in FL for protecting private medical data.\nSecond, we demonstrate the substantial privacy risks of using FL to process\nmedical images, where adversaries can easily perform privacy attacks to\nreconstruct private medical images accurately. Third, we show that the defense\napproach of adding random noises may not always work effectively to protect\nmedical images against privacy attacks in FL, which poses unique and pressing\nchallenges associated with medical data for privacy protection.\n', ""  Federated Learning (FL) is a decentralized machine learning (ML) approach\nthat keeps data localized and often incorporates Differential Privacy (DP) to\nenhance privacy guarantees. Similar to previous work on DP in ML, we observed\nthat differentially private federated learning (DPFL) introduces performance\ndisparities, particularly affecting minority groups. Recent work has attempted\nto address performance fairness in vanilla FL through clustering, but this\nmethod remains sensitive and prone to errors, which are further exacerbated by\nthe DP noise in DPFL. To fill this gap, in this paper, we propose a novel\nclustered DPFL algorithm designed to effectively identify clients' clusters in\nhighly heterogeneous settings while maintaining high accuracy with DP\nguarantees. To this end, we propose to cluster clients based on both their\nmodel updates and training loss values. Our proposed approach also addresses\nthe server's uncertainties in clustering clients' model updates by employing\nlarger batch sizes along with Gaussian Mixture Model (GMM) to alleviate the\nimpact of noise and potential clustering errors, especially in\nprivacy-sensitive scenarios. We provide theoretical analysis of the\neffectiveness of our proposed approach. We also extensively evaluate our\napproach across diverse data distributions and privacy budgets and show its\neffectiveness in mitigating the disparate impact of DP in FL settings with a\nsmall computational cost.\n"", '  Despite recent progress in enhancing the privacy of federated learning (FL)\nvia differential privacy (DP), the trade-off of DP between privacy protection\nand performance is still underexplored for real-world medical scenario. In this\npaper, we propose to optimize the trade-off under the context of client-level\nDP, which focuses on privacy during communications. However, FL for medical\nimaging involves typically much fewer participants (hospitals) than other\ndomains (e.g., mobile devices), thus ensuring clients be differentially private\nis much more challenging. To tackle this problem, we propose an adaptive\nintermediary strategy to improve performance without harming privacy.\nSpecifically, we theoretically find splitting clients into sub-clients, which\nserve as intermediaries between hospitals and the server, can mitigate the\nnoises introduced by DP without harming privacy. Our proposed approach is\nempirically evaluated on both classification and segmentation tasks using two\npublic datasets, and its effectiveness is demonstrated with significant\nperformance improvements and comprehensive analytical studies. Code is\navailable at: https://github.com/med-air/Client-DP-FL.\n']",Federated Learning for Medical Data Privacy,Federated Learning for Privacy-Preserving Machine Learning,Machine Learning and Data Privacy
53,53,96,53_seismic_waveform_deep_inversion,"['seismic', 'waveform', 'deep', 'inversion', 'predicting', 'forecasting', 'neural', 'prediction', 'deeponet', 'wave']","['seismic', 'wave', 'subsurface', 'physics', 'material', 'mechanical', 'vibration', 'physical', 'properties', 'hydraulic']","['  Full waveform inversion (FWI) infers the subsurface structure information\nfrom seismic waveform data by solving a non-convex optimization problem.\nData-driven FWI has been increasingly studied with various neural network\narchitectures to improve accuracy and computational efficiency. Nevertheless,\nthe applicability of pre-trained neural networks is severely restricted by\npotential discrepancies between the source function used in the field survey\nand the one utilized during training. Here, we develop a Fourier-enhanced deep\noperator network (Fourier-DeepONet) for FWI with the generalization of seismic\nsources, including the frequencies and locations of sources. Specifically, we\nemploy the Fourier neural operator as the decoder of DeepONet, and we utilize\nsource parameters as one input of Fourier-DeepONet, facilitating the resolution\nof FWI with variable sources. To test Fourier-DeepONet, we develop three new\nand realistic FWI benchmark datasets (FWI-F, FWI-L, and FWI-FL) with varying\nsource frequencies, locations, or both. Our experiments demonstrate that\ncompared with existing data-driven FWI methods, Fourier-DeepONet obtains more\naccurate predictions of subsurface structures in a wide range of source\nparameters. Moreover, the proposed Fourier-DeepONet exhibits superior\nrobustness when handling data with Gaussian noise or missing traces and sources\nwith Gaussian noise, paving the way for more reliable and accurate subsurface\nimaging across diverse real conditions.\n', '  Full-waveform inversion (FWI) plays a vital role in geoscience to explore the\nsubsurface. It utilizes the seismic wave to image the subsurface velocity map.\nAs the machine learning (ML) technique evolves, the data-driven approaches\nusing ML for FWI tasks have emerged, offering enhanced accuracy and reduced\ncomputational cost compared to traditional physics-based methods. However, a\ncommon challenge in geoscience, the unprivileged data, severely limits ML\neffectiveness. The issue becomes even worse during model pruning, a step\nessential in geoscience due to environmental complexities. To tackle this, we\nintroduce the EdGeo toolkit, which employs a diffusion-based model guided by\nphysics principles to generate high-fidelity velocity maps. The toolkit uses\nthe acoustic wave equation to generate corresponding seismic waveform data,\nfacilitating the fine-tuning of pruned ML models. Our results demonstrate\nsignificant improvements in SSIM scores and reduction in both MAE and MSE\nacross various pruning ratios. Notably, the ML model fine-tuned using data\ngenerated by EdGeo yields superior quality of velocity maps, especially in\nrepresenting unprivileged features, outperforming other existing methods.\n', '  Full-Waveform Inversion (FWI) is a nonlinear iterative seismic imaging\ntechnique that, by reducing the misfit between recorded and predicted seismic\nwaveforms, can produce detailed estimates of subsurface geophysical properties.\nNevertheless, the strong nonlinearity of FWI can trap the optimization in local\nminima. This issue arises due to factors such as improper initial values, the\nabsence of low frequencies in the measurements, noise, and other related\nconsiderations. To address this challenge and with the advent of advanced\nmachine-learning techniques, data-driven methods, such as deep learning, have\nattracted significantly increasing attention in the geophysical community.\nFurthermore, the elastic wave equation should be included in FWI to represent\nelastic effects accurately. The intersection of data-driven techniques and\nelastic scattering theories presents opportunities and challenges. In this\npaper, by using the knowledge of elastic scattering (Physics of problem) and\nintegrating it with deep learning techniques, we propose methods for the\nsolution of time-harmonic FWI to enhance accuracy compared to pure data-driven\napproaches. Moreover, by modifying the structure of the Variational\nAutoencoder, we introduce a probabilistic deep learning method based on the\nphysics of the problem that enables us to explore the uncertainties of the\nsolution. According to the limited availability of datasets in this field and\nto assess the performance and accuracy of the proposed methods, we create a\ncomprehensive dataset close to reality and conduct a comparative analysis of\nthe presented approaches to it.\n']",Seismic Waveform Inversion with Deep Learning,Deep Learning for Geophysical and Structural Analysis,Deep Learning Applications in Engineering and Computer Vision
54,54,96,54_caching_cache_caches_memory,"['caching', 'cache', 'caches', 'memory', 'cachedattention', 'quantization', 'storage', 'attention', 'compression', 'efficient']","['cache', 'memory', 'rank', 'compression', 'quantization', 'latency', 'throughput', 'low', 'inference', 'fine']","['  KV cache stores key and value states from previous tokens to avoid\nre-computation, yet it demands substantial storage space, especially for long\nsequences. Adaptive KV cache compression seeks to discern the saliency of\ntokens, preserving vital information while aggressively compressing those of\nless importance. However, previous methods of this approach exhibit significant\nperformance degradation at high compression ratios due to inaccuracies in\nidentifying salient tokens. In this paper, we present ZipCache, an accurate and\nefficient KV cache quantization method for LLMs. First, we construct a strong\nbaseline for quantizing KV cache. Through the proposed channel-separable\ntokenwise quantization scheme, the memory overhead of quantization parameters\nare substantially reduced compared to fine-grained groupwise quantization. To\nenhance the compression ratio, we propose normalized attention score as an\neffective metric for identifying salient tokens by considering the lower\ntriangle characteristics of the attention matrix. Moreover, we develop an\nefficient approximation method that decouples the saliency metric from full\nattention scores, enabling compatibility with fast attention implementations\nlike FlashAttention. Extensive experiments demonstrate that ZipCache achieves\nsuperior compression ratios, fast generation speed and minimal performance\nlosses compared with previous KV cache compression methods. For instance, when\nevaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of\ncompressing the KV cache by $4.98\\times$, with only a $0.38\\%$ drop in\naccuracy. In terms of efficiency, ZipCache also showcases a $37.3\\%$ reduction\nin prefill-phase latency, a $56.9\\%$ reduction in decoding-phase latency, and a\n$19.8\\%$ reduction in GPU memory usage when evaluating LLaMA3-8B model with a\ninput length of $4096$.\n', '  Key-Value (KV) Caching has become an essential technique for accelerating the\ninference speed and throughput of generative Large Language Models~(LLMs).\nHowever, the memory footprint of the KV cache poses a critical bottleneck in\nLLM deployment as the cache size grows with batch size and sequence length,\noften surpassing even the size of the model itself. Although recent methods\nwere proposed to select and evict unimportant KV pairs from the cache to reduce\nmemory consumption, the potential ramifications of eviction on the generative\nprocess are yet to be thoroughly examined. In this paper, we examine the\ndetrimental impact of cache eviction and observe that unforeseen risks arise as\nthe information contained in the KV pairs is exhaustively discarded, resulting\nin safety breaches, hallucinations, and context loss. Surprisingly, we find\nthat preserving even a small amount of information contained in the evicted KV\npairs via reduced precision quantization substantially recovers the incurred\ndegradation. On the other hand, we observe that the important KV pairs must be\nkept at a relatively higher precision to safeguard the generation quality.\nMotivated by these observations, we propose \\textit{Mixed-precision KV\ncache}~(MiKV), a reliable cache compression method that simultaneously\npreserves the context details by retaining the evicted KV pairs in\nlow-precision and ensure generation quality by keeping the important KV pairs\nin high-precision. Experiments on diverse benchmarks and LLM backbones show\nthat our proposed method offers a state-of-the-art trade-off between\ncompression ratio and performance, compared to other baselines.\n', '  Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.\n']",Efficient Cache Compression for Large Language Models,Optimization and Efficiency of Large Language Models,Large Language Models
55,55,95,55_speaker_utterances_diarization_voice,"['speaker', 'utterances', 'diarization', 'voice', 'voices', 'speech', 'audio', 'microphone', 'corpus', 'transcription']","['speaker', 'diarization', 'speech', 'speakers', 'voice', 'recognition', 'singing', 'audio', 'vocal', 'voices']","['  Speaker diarization provides the answer to the question ""who spoke when?"" for\nan audio file. This information can be used to complete audio transcripts for\nfurther processing steps. Most speaker diarization systems assume that the\naudio file is available as a whole. However, there are scenarios in which the\nspeaker labels are needed immediately after the arrival of an audio segment.\nSpeaker diarization with a correspondingly low latency is referred to as online\nspeaker diarization. This paper provides an overview. First the history of\nonline speaker diarization is briefly presented. Next a taxonomy and datasets\nfor training and evaluation are given. In the sections that follow, online\ndiarization methods and systems are discussed in detail. This paper concludes\nwith the presentation of challenges that still need to be solved by future\nresearch in the field of online speaker diarization.\n', '  Speech foundation models, trained on vast datasets, have opened unique\nopportunities in addressing challenging low-resource speech understanding, such\nas child speech. In this work, we explore the capabilities of speech foundation\nmodels on child-adult speaker diarization. We show that exemplary foundation\nmodels can achieve 39.5% and 62.3% relative reductions in Diarization Error\nRate and Speaker Confusion Rate, respectively, compared to previous speaker\ndiarization methods. In addition, we benchmark and evaluate the speaker\ndiarization results of the speech foundation models with varying the input\naudio window size, speaker demographics, and training data ratio. Our results\nhighlight promising pathways for understanding and adopting speech foundation\nmodels to facilitate child speech understanding.\n', ""  Speaker diarization has gained considerable attention within speech\nprocessing research community. Mainstream speaker diarization rely primarily on\nspeakers' voice characteristics extracted from acoustic signals and often\noverlook the potential of semantic information. Considering the fact that\nspeech signals can efficiently convey the content of a speech, it is of our\ninterest to fully exploit these semantic cues utilizing language models. In\nthis work we propose a novel approach to effectively leverage semantic\ninformation in clustering-based speaker diarization systems. Firstly, we\nintroduce spoken language understanding modules to extract speaker-related\nsemantic information and utilize these information to construct pairwise\nconstraints. Secondly, we present a novel framework to integrate these\nconstraints into the speaker diarization pipeline, enhancing the performance of\nthe entire system. Extensive experiments conducted on the public dataset\ndemonstrate the consistent superiority of our proposed approach over\nacoustic-only speaker diarization systems.\n""]",Speaker Diarization in Audio Files,Audio and Speech Processing,Speech and Audio Processing
56,56,95,56_optimal_allocation_equilibrium_incentive,"['optimal', 'allocation', 'equilibrium', 'incentive', 'bandits', 'equilibria', 'auctions', 'games', 'markets', 'bandit']","['regret', 'facility', 'agents', 'bandit', 'optimal', 'mechanism', 'algorithm', 'allocation', 'matching', 'problem']","[""  Two-sided matching markets have been widely studied in the literature due to\ntheir rich applications. Since participants are usually uncertain about their\npreferences, online algorithms have recently been adopted to learn them through\niterative interactions. An existing work initiates the study of this problem in\na many-to-one setting with responsiveness. However, their results are far from\noptimal and lack guarantees of incentive compatibility. We first extend an\nexisting algorithm for the one-to-one setting to this more general setting and\nshow it achieves a near-optimal bound for player-optimal regret. Nevertheless,\ndue to the substantial requirement for collaboration, a single player's\ndeviation could lead to a huge increase in its own cumulative rewards and a\nlinear regret for others. In this paper, we aim to enhance the regret bound in\nmany-to-one markets while ensuring incentive compatibility. We first propose\nthe adaptively explore-then-deferred-acceptance (AETDA) algorithm for\nresponsiveness setting and derive an upper bound for player-optimal stable\nregret while demonstrating its guarantee of incentive compatibility. To the\nbest of our knowledge, it constitutes the first polynomial player-optimal\nguarantee in matching markets that offers such robust assurances without known\n$\\Delta$, where $\\Delta$ is some preference gap among players and arms. We also\nconsider broader substitutable preferences, one of the most general conditions\nto ensure the existence of a stable matching and cover responsiveness. We\ndevise an online DA (ODA) algorithm and establish an upper bound for the\nplayer-pessimal stable regret for this setting.\n"", ""  We consider non-cooperative facility location games where both facilities and\nclients act strategically and heavily influence each other. This contrasts\nestablished game-theoretic facility location models with non-strategic clients\nthat simply select the closest opened facility. In our model, every facility\nlocation has a set of attracted clients and each client has a set of shopping\nlocations and a weight that corresponds to her spending capacity. Facility\nagents selfishly select a location for opening their facility to maximize the\nattracted total spending capacity, whereas clients strategically decide how to\ndistribute their spending capacity among the opened facilities in their\nshopping range. We focus on a natural client behavior similar to classical load\nbalancing: our selfish clients aim for a distribution that minimizes their\nmaximum waiting times for getting serviced, where a facility's waiting time\ncorresponds to its total attracted client weight.\n  We show that subgame perfect equilibria exist and give almost tight constant\nbounds on the Price of Anarchy and the Price of Stability, which even hold for\na broader class of games with arbitrary client behavior. Since facilities and\nclients influence each other, it is crucial for the facilities to anticipate\nthe selfish clients' behavior when selecting their location. For this, we\nprovide an efficient algorithm that also implies an efficient check for\nequilibrium. Finally, we show that computing a socially optimal facility\nplacement is NP-hard and that this result holds for all feasible client weight\ndistributions.\n"", '  We study a non-cooperative two-sided facility location game in which\nfacilities and clients behave strategically. This is in contrast to many other\nfacility location games in which clients simply visit their closest facility.\nFacility agents select a location on a graph to open a facility to attract as\nmuch purchasing power as possible, while client agents choose which facilities\nto patronize by strategically distributing their purchasing power in order to\nminimize their total waiting time. Here, the waiting time of a facility depends\non its received total purchasing power. We show that our client stage is an\natomic splittable congestion game, which implies existence, uniqueness and\nefficient computation of a client equilibrium. Therefore, facility agents can\nefficiently predict client behavior and make strategic decisions accordingly.\nDespite that, we prove that subgame perfect equilibria do not exist in all\ninstances of this game and that their existence is NP-hard to decide. On the\npositive side, we provide a simple and efficient algorithm to compute\n3-approximate subgame perfect equilibria.\n']",Mechanism Design for Strategic Markets and Games,Game Theory and Strategic Decision Making,Decision Making and Optimization under Uncertainty
57,57,94,57_corpus_zeroshot_annotation_annotate,"['corpus', 'zeroshot', 'annotation', 'annotate', 'annotated', 'nlp', 'classification', 'shot', 'nlg', 'sentences']","['shot', 'language', 'extraction', 'languages', 'corpus', 'datasets', 'text', 'task', 'classification', 'sentences']","['  In recent years, few-shot and zero-shot learning, which learn to predict\nlabels with limited annotated instances, have garnered significant attention.\nTraditional approaches often treat frequent-shot (freq-shot; labels with\nabundant instances), few-shot, and zero-shot learning as distinct challenges,\noptimizing systems for just one of these scenarios. Yet, in real-world\nsettings, label occurrences vary greatly. Some of them might appear thousands\nof times, while others might only appear sporadically or not at all. For\npractical deployment, it is crucial that a system can adapt to any label\noccurrence. We introduce a novel classification challenge: X-shot, reflecting a\nreal-world context where freq-shot, few-shot, and zero-shot labels co-occur\nwithout predefined limits. Here, X can span from 0 to positive infinity. The\ncrux of X-shot centers on open-domain generalization and devising a system\nversatile enough to manage various label scenarios. To solve X-shot, we propose\nBinBin (Binary INference Based on INstruction following) that leverages the\nIndirect Supervision from a large collection of NLP tasks via instruction\nfollowing, bolstered by Weak Supervision provided by large language models.\nBinBin surpasses previous state-of-the-art techniques on three benchmark\ndatasets across multiple domains. To our knowledge, this is the first work\naddressing X-shot learning, where X remains variable.\n', ""  Large language models (LLMs) offer impressive performance in various\nzero-shot and few-shot tasks. However, their success in zero-shot and few-shot\nsettings may be affected by task contamination, a potential limitation that has\nnot been thoroughly examined. This paper investigates how zero-shot and\nfew-shot performance of LLMs has changed chronologically over time. Utilizing\nGPT-3 series models and several other recent open-sourced LLMs, and controlling\nfor dataset difficulty, we find that on datasets released before the LLM\ntraining data creation date, LLMs perform surprisingly better than on datasets\nreleased after. This strongly indicates that, for many LLMs, there exists task\ncontamination on zero-shot and few-shot evaluation for datasets released prior\nto the LLMs' training data creation date. Additionally, we utilize training\ndata inspection, task example extraction, and a membership inference attack,\nwhich reveal further evidence of task contamination. Importantly, we find that\nfor classification tasks with no possibility of task contamination, LLMs rarely\ndemonstrate statistically significant improvements over simple majority\nbaselines, in both zero and few-shot settings.\n"", '  Recent foundational language models have shown state-of-the-art performance\nin many NLP tasks in zero- and few-shot settings. An advantage of these models\nover more standard approaches based on fine-tuning is the ability to understand\ninstructions written in natural language (prompts), which helps them generalise\nbetter to different tasks and domains without the need for specific training\ndata. This makes them suitable for addressing text classification problems for\ndomains with limited amounts of annotated instances. However, existing research\nis limited in scale and lacks understanding of how text generation models\ncombined with prompting techniques compare to more established methods for text\nclassification such as fine-tuning masked language models. In this paper, we\naddress this research gap by performing a large-scale evaluation study for 16\ntext classification datasets covering binary, multiclass, and multilabel\nproblems. In particular, we compare zero- and few-shot approaches of large\nlanguage models to fine-tuning smaller language models. We also analyse the\nresults by prompt, classification type, domain, and number of labels. In\ngeneral, the results show how fine-tuning smaller and more efficient language\nmodels can still outperform few-shot approaches of larger language models,\nwhich have room for improvement when it comes to text classification.\n']",Few-shot and Zero-shot Learning in NLP,Meta-Learning and Few-Shot Learning in Machine Learning and NLP,Machine Learning Methodologies
58,58,94,58_wasserstein_gradient_variational_divergence,"['wasserstein', 'gradient', 'variational', 'divergence', 'divergences', 'generative', 'flow', 'minimization', 'stochastic', 'langevin']","['gradient', 'flow', 'convergence', 'flows', 'transport', 'probability', 'variational', 'distribution', 'distributions', 'functional']","['  We consider the optimization problem of minimizing a functional defined over\na family of probability distributions, where the objective functional is\nassumed to possess a variational form. Such a distributional optimization\nproblem arises widely in machine learning and statistics, with Monte-Carlo\nsampling, variational inference, policy optimization, and generative\nadversarial network as examples. For this problem, we propose a novel\nparticle-based algorithm, dubbed as variational transport, which approximately\nperforms Wasserstein gradient descent over the manifold of probability\ndistributions via iteratively pushing a set of particles. Specifically, we\nprove that moving along the geodesic in the direction of functional gradient\nwith respect to the second-order Wasserstein distance is equivalent to applying\na pushforward mapping to a probability distribution, which can be approximated\naccurately by pushing a set of particles. Specifically, in each iteration of\nvariational transport, we first solve the variational problem associated with\nthe objective functional using the particles, whose solution yields the\nWasserstein gradient direction. Then we update the current distribution by\npushing each particle along the direction specified by such a solution. By\ncharacterizing both the statistical error incurred in estimating the\nWasserstein gradient and the progress of the optimization algorithm, we prove\nthat when the objective function satisfies a functional version of the\nPolyak-\\L{}ojasiewicz (PL) (Polyak, 1963) and smoothness conditions,\nvariational transport converges linearly to the global minimum of the objective\nfunctional up to a certain statistical error, which decays to zero sublinearly\nas the number of particles goes to infinity.\n', ""  We study the convergence of gradient flow for the training of deep neural\nnetworks. If Residual Neural Networks are a popular example of very deep\narchitectures, their training constitutes a challenging optimization problem\ndue notably to the non-convexity and the non-coercivity of the objective. Yet,\nin applications, those tasks are successfully solved by simple optimization\nalgorithms such as gradient descent. To better understand this phenomenon, we\nfocus here on a ``mean-field'' model of infinitely deep and arbitrarily wide\nResNet, parameterized by probability measures over the product set of layers\nand parameters and with constant marginal on the set of layers. Indeed, in the\ncase of shallow neural networks, mean field models have proven to benefit from\nsimplified loss-landscapes and good theoretical guarantees when trained with\ngradient flow for the Wasserstein metric on the set of probability measures.\nMotivated by this approach, we propose to train our model with gradient flow\nw.r.t. the conditional Optimal Transport distance: a restriction of the\nclassical Wasserstein distance which enforces our marginal condition. Relying\non the theory of gradient flows in metric spaces we first show the\nwell-posedness of the gradient flow equation and its consistency with the\ntraining of ResNets at finite width. Performing a local Polyak-\\L{}ojasiewicz\nanalysis, we then show convergence of the gradient flow for well-chosen\ninitializations: if the number of features is finite but sufficiently large and\nthe risk is sufficiently small at initialization, the gradient flow converges\ntowards a global minimizer. This is the first result of this type for\ninfinitely deep and arbitrarily wide ResNets.\n"", '  Recently, optimization on the Riemannian manifold has provided new insights\nto the optimization community. In this regard, the manifold taken as the\nprobability measure metric space equipped with the second-order Wasserstein\ndistance is of particular interest, since optimization on it can be linked to\npractical sampling processes. In general, the standard (continuous)\noptimization method on Wasserstein space is Riemannian gradient flow (i.e.,\nLangevin dynamics when minimizing KL divergence). In this paper, we aim to\nenrich the continuous optimization methods in the Wasserstein space, by\nextending the gradient flow on it into the stochastic gradient descent (SGD)\nflow and stochastic variance reduction gradient (SVRG) flow. The two flows in\nEuclidean space are standard continuous stochastic methods, while their\nRiemannian counterparts are unexplored. By leveraging the property of\nWasserstein space, we construct stochastic differential equations (SDEs) to\napproximate the corresponding discrete dynamics of desired Riemannian\nstochastic methods in Euclidean space. Then, our probability measures flows are\nobtained by the Fokker-Planck equation. Finally, the convergence rates of our\nRiemannian stochastic flows are proven, which match the results in Euclidean\nspace.\n']",Wasserstein Gradient Flows for Optimization,Optimal Transport and Wasserstein Metrics for Machine Learning and Optimization,Machine Learning and Optimization
59,59,94,59_biomarkers_ai_diagnoses_diagnosis,"['biomarkers', 'ai', 'diagnoses', 'diagnosis', 'diagnostic', 'predicting', 'classification', 'predictive', 'interpretability', 'prediction']","['cancer', 'medical', 'patients', 'diagnostic', 'diagnosis', 'patient', 'clinical', 'mpox', 'clinicians', 'health']","['  Although data-driven artificial intelligence (AI) in medical image diagnosis\nhas shown impressive performance in silico, the lack of interpretability makes\nit difficult to incorporate the ""black box"" into clinicians\' workflows. To make\nthe diagnostic patterns learned from data understandable by clinicians, we\ndevelop an interpretable model, knowledge-guided diagnosis model (KGDM), that\nprovides a visualized reasoning process containing AI-based biomarkers and\nretrieved cases that with the same diagnostic patterns. It embraces clinicians\'\nprompts into the interpreted reasoning through human-AI interaction, leading to\npotentially enhanced safety and more accurate predictions. This study\ninvestigates the performance, interpretability, and clinical utility of KGDM in\nthe diagnosis of infectious keratitis (IK), which is the leading cause of\ncorneal blindness. The classification performance of KGDM is evaluated on a\nprospective validation dataset, an external testing dataset, and an publicly\navailable testing dataset. The diagnostic odds ratios (DOR) of the interpreted\nAI-based biomarkers are effective, ranging from 3.011 to 35.233 and exhibit\nconsistent diagnostic patterns with clinic experience. Moreover, a human-AI\ncollaborative diagnosis test is conducted and the participants with\ncollaboration achieved a performance exceeding that of both humans and AI. By\nsynergistically integrating interpretability and interaction, this study\nfacilitates the convergence of clinicians\' expertise and data-driven\nintelligence. The promotion of inexperienced ophthalmologists with the aid of\nAI-based biomarkers, as well as increased AI prediction by intervention from\nexperienced ones, demonstrate a promising diagnostic paradigm for infectious\nkeratitis using KGDM, which holds the potential for extension to other diseases\nwhere experienced medical practitioners are limited and the safety of AI is\nconcerned.\n', ""  Thyroid cancer, the most prevalent endocrine cancer, has gained significant\nglobal attention due to its impact on public health. Extensive research efforts\nhave been dedicated to leveraging artificial intelligence (AI) methods for the\nearly detection of this disease, aiming to reduce its morbidity rates. However,\na comprehensive understanding of the structured organization of research\napplications in this particular field remains elusive. To address this\nknowledge gap, we conducted a systematic review and developed a comprehensive\ntaxonomy of machine learning-based applications in thyroid cancer pathogenesis,\ndiagnosis, and prognosis. Our primary objective was to facilitate the research\ncommunity's ability to stay abreast of technological advancements and\npotentially lead the emerging trends in this field. This survey presents a\ncoherent literature review framework for interpreting the advanced techniques\nused in thyroid cancer research. A total of 758 related studies were identified\nand scrutinized. To the best of our knowledge, this is the first review that\nprovides an in-depth analysis of the various aspects of AI applications\nemployed in the context of thyroid cancer. Furthermore, we highlight key\nchallenges encountered in this domain and propose future research opportunities\nfor those interested in studying the latest trends or exploring\nless-investigated aspects of thyroid cancer research. By presenting this\ncomprehensive review and taxonomy, we contribute to the existing knowledge in\nthe field, while providing valuable insights for researchers, clinicians, and\nstakeholders in advancing the understanding and management of this disease.\n"", '  Thyroid cancer is an increasing global health concern that requires advanced\ndiagnostic methods. The application of AI and radiomics to thyroid cancer\ndiagnosis is examined in this review. A review of multiple databases was\nconducted in compliance with PRISMA guidelines until October 2023. A\ncombination of keywords led to the discovery of an English academic publication\non thyroid cancer and related subjects. 267 papers were returned from the\noriginal search after 109 duplicates were removed. Relevant studies were\nselected according to predetermined criteria after 124 articles were eliminated\nbased on an examination of their abstract and title. After the comprehensive\nanalysis, an additional six studies were excluded. Among the 28 included\nstudies, radiomics analysis, which incorporates ultrasound (US) images,\ndemonstrated its effectiveness in diagnosing thyroid cancer. Various results\nwere noted, some of the studies presenting new strategies that outperformed the\nstatus quo. The literature has emphasized various challenges faced by AI\nmodels, including interpretability issues, dataset constraints, and operator\ndependence. The synthesized findings of the 28 included studies mentioned the\nneed for standardization efforts and prospective multicenter studies to address\nthese concerns. Furthermore, approaches to overcome these obstacles were\nidentified, such as advances in explainable AI technology and personalized\nmedicine techniques. The review focuses on how AI and radiomics could transform\nthe diagnosis and treatment of thyroid cancer. Despite challenges, future\nresearch on multidisciplinary cooperation, clinical applicability validation,\nand algorithm improvement holds the potential to improve patient outcomes and\ndiagnostic precision in the treatment of thyroid cancer.\n']",AI in Medical Diagnosis and Predictive Analytics,Artificial Intelligence in Medical Diagnosis and Prognosis,Artificial Intelligence Applications and Implications
60,60,93,60_boltzmann_molecule_molecules_molecular,"['boltzmann', 'molecule', 'molecules', 'molecular', 'simulations', 'sampling', 'kinetic', 'atomnet', 'particles', 'flow']","['molecular', 'protein', 'flow', 'molecules', 'dynamics', 'coarse', 'proteins', 'sampling', 'energy', 'simulations']","['  The generation of equilibrium samples of molecular systems has been a\nlong-standing problem in statistical physics. Boltzmann Generators are a\ngenerative machine learning method that addresses this issue by learning a\ntransformation via a normalizing flow from a simple prior distribution to the\ntarget Boltzmann distribution of interest. Recently, flow matching has been\nemployed to train Boltzmann Generators for small molecular systems in Cartesian\ncoordinates. We extend this work and propose a first framework for Boltzmann\nGenerators that are transferable across chemical space, such that they predict\nzero-shot Boltzmann distributions for test molecules without being retrained\nfor these systems. These transferable Boltzmann Generators allow approximate\nsampling from the target distribution of unseen systems, as well as efficient\nreweighting to the target Boltzmann distribution. The transferability of the\nproposed framework is evaluated on dipeptides, where we show that it\ngeneralizes efficiently to unseen systems. Furthermore, we demonstrate that our\nproposed architecture enhances the efficiency of Boltzmann Generators trained\non single molecular systems.\n', ""  Sampling all possible transition paths between two 3D states of a molecular\nsystem has various applications ranging from catalyst design to drug discovery.\nCurrent approaches to sample transition paths use Markov chain Monte Carlo and\nrely on time-intensive molecular dynamics simulations to find new paths. Our\napproach operates in the latent space of a normalizing flow that maps from the\nmolecule's Boltzmann distribution to a Gaussian, where we propose new paths\nwithout requiring molecular simulations. Using alanine dipeptide, we explore\nMetropolis-Hastings acceptance criteria in the latent space for exact sampling\nand investigate different latent proposal mechanisms.\n"", '  Efficient sampling of the Boltzmann distribution of molecular systems is a\nlong-standing challenge. Recently, instead of generating long molecular\ndynamics simulations, generative machine learning methods such as normalizing\nflows have been used to learn the Boltzmann distribution directly, without\nsamples. However, this approach is susceptible to mode collapse and thus often\ndoes not explore the full configurational space. In this work, we address this\nchallenge by separating the problem into two levels, the fine-grained and\ncoarse-grained degrees of freedom. A normalizing flow conditioned on the\ncoarse-grained space yields a probabilistic connection between the two levels.\nTo explore the configurational space, we employ coarse-grained simulations with\nactive learning which allows us to update the flow and make all-atom potential\nenergy evaluations only when necessary. Using alanine dipeptide as an example,\nwe show that our methods obtain a speedup to molecular dynamics simulations of\napproximately 15.9 to 216.2 compared to the speedup of 4.5 of the current\nstate-of-the-art machine learning approach.\n']",Boltzmann Generators for Molecular Systems,Machine Learning for Molecular Simulation and Sampling,Computational Biology and Chemistry
61,61,93,61_annotated_instruction_responses_language,"['annotated', 'instruction', 'responses', 'language', 'metacognitive', 'feedback', 'tasks', 'examples', 'reinforcement', 'text']","['self', 'instruction', 'instructions', 'human', 'tuning', 'feedback', 'fine', 'responses', 'language', 'reasoning']","['  Large Language Models (LLMs) have achieved remarkable success, where\ninstruction tuning is the critical step in aligning LLMs with user intentions.\nIn this work, we investigate how the instruction tuning adjusts pre-trained\nmodels with a focus on intrinsic changes. Specifically, we first develop\nseveral local and global explanation methods, including a gradient-based method\nfor input-output attribution, and techniques for interpreting patterns and\nconcepts in self-attention and feed-forward layers. The impact of instruction\ntuning is then studied by comparing the explanations derived from the\npre-trained and instruction-tuned models. This approach provides an internal\nperspective of the model shifts on a human-comprehensible level. Our findings\nreveal three significant impacts of instruction tuning: 1) It empowers LLMs to\nrecognize the instruction parts of user prompts, and promotes the response\ngeneration constantly conditioned on the instructions. 2) It encourages the\nself-attention heads to capture more word-word relationships about instruction\nverbs. 3) It encourages the feed-forward networks to rotate their pre-trained\nknowledge toward user-oriented tasks. These insights contribute to a more\ncomprehensive understanding of instruction tuning and lay the groundwork for\nfuture work that aims at explaining and optimizing LLMs for various\napplications. Our code and data are publicly available at\nhttps://github.com/JacksonWuxs/Interpret_Instruction_Tuning_LLMs.\n', '  Large language models (LLMs) have shown tremendous success in following user\ninstructions and generating helpful responses. Nevertheless, their robustness\nis still far from optimal, as they may generate significantly inconsistent\nresponses due to minor changes in the verbalized instructions. Recent\nliterature has explored this inconsistency issue, highlighting the importance\nof continued improvement in the robustness of response generation. However,\nsystematic analysis and solutions are still lacking. In this paper, we\nquantitatively define the inconsistency problem and propose a two-stage\ntraining framework consisting of instruction-augmented supervised fine-tuning\nand consistency alignment training. The first stage helps a model generalize on\nfollowing instructions via similar instruction augmentations. In the second\nstage, we improve the diversity and help the model understand which responses\nare more aligned with human expectations by differentiating subtle differences\nin similar responses. The training process is accomplished by self-rewards\ninferred from the trained model at the first stage without referring to\nexternal human preference resources. We conduct extensive experiments on recent\npublicly available LLMs on instruction-following tasks and demonstrate the\neffectiveness of our training framework.\n', ""  Large Language Models (LLMs) have demonstrated remarkable versatility across\nvarious domains. To further advance LLMs, we propose 'SELF' (Self-Evolution\nwith Language Feedback), a novel approach that enables LLMs to self-improve\nthrough self-reflection, akin to human learning processes. SELF initiates with\na meta-skill learning process that equips the LLMs with capabilities for\nself-feedback and self-refinement. Subsequently, the model undergoes an\niterative process of self-evolution. In each iteration, it utilizes an\nunlabeled dataset of instructions to generate initial responses. These\nresponses are enhanced through self-feedback and self-refinement. The model is\nthen fine-tuned using this enhanced data. The model undergoes progressive\nimprovement through this iterative self-evolution process. Moreover, the SELF\nframework enables the model to apply self-refinement during inference, which\nfurther improves response quality. Our experiments in mathematics and general\ntasks demonstrate that SELF can enhance the capabilities of LLMs without human\nintervention. The SELF framework indicates a promising direction for the\nautonomous evolution of LLMs, transitioning them from passive information\nreceivers to active participants in their development.\n""]",Instruction Tuning for Large Language Models,Advances in Large Language Models,Large Language Models
61,61,93,61_annotated_instruction_responses_language,"['annotated', 'instruction', 'responses', 'language', 'metacognitive', 'feedback', 'tasks', 'examples', 'reinforcement', 'text']","['self', 'instruction', 'instructions', 'human', 'tuning', 'feedback', 'fine', 'responses', 'language', 'reasoning']","['  Large Language Models (LLMs) have achieved remarkable success, where\ninstruction tuning is the critical step in aligning LLMs with user intentions.\nIn this work, we investigate how the instruction tuning adjusts pre-trained\nmodels with a focus on intrinsic changes. Specifically, we first develop\nseveral local and global explanation methods, including a gradient-based method\nfor input-output attribution, and techniques for interpreting patterns and\nconcepts in self-attention and feed-forward layers. The impact of instruction\ntuning is then studied by comparing the explanations derived from the\npre-trained and instruction-tuned models. This approach provides an internal\nperspective of the model shifts on a human-comprehensible level. Our findings\nreveal three significant impacts of instruction tuning: 1) It empowers LLMs to\nrecognize the instruction parts of user prompts, and promotes the response\ngeneration constantly conditioned on the instructions. 2) It encourages the\nself-attention heads to capture more word-word relationships about instruction\nverbs. 3) It encourages the feed-forward networks to rotate their pre-trained\nknowledge toward user-oriented tasks. These insights contribute to a more\ncomprehensive understanding of instruction tuning and lay the groundwork for\nfuture work that aims at explaining and optimizing LLMs for various\napplications. Our code and data are publicly available at\nhttps://github.com/JacksonWuxs/Interpret_Instruction_Tuning_LLMs.\n', '  Large language models (LLMs) have shown tremendous success in following user\ninstructions and generating helpful responses. Nevertheless, their robustness\nis still far from optimal, as they may generate significantly inconsistent\nresponses due to minor changes in the verbalized instructions. Recent\nliterature has explored this inconsistency issue, highlighting the importance\nof continued improvement in the robustness of response generation. However,\nsystematic analysis and solutions are still lacking. In this paper, we\nquantitatively define the inconsistency problem and propose a two-stage\ntraining framework consisting of instruction-augmented supervised fine-tuning\nand consistency alignment training. The first stage helps a model generalize on\nfollowing instructions via similar instruction augmentations. In the second\nstage, we improve the diversity and help the model understand which responses\nare more aligned with human expectations by differentiating subtle differences\nin similar responses. The training process is accomplished by self-rewards\ninferred from the trained model at the first stage without referring to\nexternal human preference resources. We conduct extensive experiments on recent\npublicly available LLMs on instruction-following tasks and demonstrate the\neffectiveness of our training framework.\n', ""  Large Language Models (LLMs) have demonstrated remarkable versatility across\nvarious domains. To further advance LLMs, we propose 'SELF' (Self-Evolution\nwith Language Feedback), a novel approach that enables LLMs to self-improve\nthrough self-reflection, akin to human learning processes. SELF initiates with\na meta-skill learning process that equips the LLMs with capabilities for\nself-feedback and self-refinement. Subsequently, the model undergoes an\niterative process of self-evolution. In each iteration, it utilizes an\nunlabeled dataset of instructions to generate initial responses. These\nresponses are enhanced through self-feedback and self-refinement. The model is\nthen fine-tuned using this enhanced data. The model undergoes progressive\nimprovement through this iterative self-evolution process. Moreover, the SELF\nframework enables the model to apply self-refinement during inference, which\nfurther improves response quality. Our experiments in mathematics and general\ntasks demonstrate that SELF can enhance the capabilities of LLMs without human\nintervention. The SELF framework indicates a promising direction for the\nautonomous evolution of LLMs, transitioning them from passive information\nreceivers to active participants in their development.\n""]",Instruction Tuning for Large Language Models,Advances in Large Language Models,Large Language Models
61,61,93,61_annotated_instruction_responses_language,"['annotated', 'instruction', 'responses', 'language', 'metacognitive', 'feedback', 'tasks', 'examples', 'reinforcement', 'text']","['self', 'instruction', 'instructions', 'human', 'tuning', 'feedback', 'fine', 'responses', 'language', 'reasoning']","['  Large Language Models (LLMs) have achieved remarkable success, where\ninstruction tuning is the critical step in aligning LLMs with user intentions.\nIn this work, we investigate how the instruction tuning adjusts pre-trained\nmodels with a focus on intrinsic changes. Specifically, we first develop\nseveral local and global explanation methods, including a gradient-based method\nfor input-output attribution, and techniques for interpreting patterns and\nconcepts in self-attention and feed-forward layers. The impact of instruction\ntuning is then studied by comparing the explanations derived from the\npre-trained and instruction-tuned models. This approach provides an internal\nperspective of the model shifts on a human-comprehensible level. Our findings\nreveal three significant impacts of instruction tuning: 1) It empowers LLMs to\nrecognize the instruction parts of user prompts, and promotes the response\ngeneration constantly conditioned on the instructions. 2) It encourages the\nself-attention heads to capture more word-word relationships about instruction\nverbs. 3) It encourages the feed-forward networks to rotate their pre-trained\nknowledge toward user-oriented tasks. These insights contribute to a more\ncomprehensive understanding of instruction tuning and lay the groundwork for\nfuture work that aims at explaining and optimizing LLMs for various\napplications. Our code and data are publicly available at\nhttps://github.com/JacksonWuxs/Interpret_Instruction_Tuning_LLMs.\n', '  Large language models (LLMs) have shown tremendous success in following user\ninstructions and generating helpful responses. Nevertheless, their robustness\nis still far from optimal, as they may generate significantly inconsistent\nresponses due to minor changes in the verbalized instructions. Recent\nliterature has explored this inconsistency issue, highlighting the importance\nof continued improvement in the robustness of response generation. However,\nsystematic analysis and solutions are still lacking. In this paper, we\nquantitatively define the inconsistency problem and propose a two-stage\ntraining framework consisting of instruction-augmented supervised fine-tuning\nand consistency alignment training. The first stage helps a model generalize on\nfollowing instructions via similar instruction augmentations. In the second\nstage, we improve the diversity and help the model understand which responses\nare more aligned with human expectations by differentiating subtle differences\nin similar responses. The training process is accomplished by self-rewards\ninferred from the trained model at the first stage without referring to\nexternal human preference resources. We conduct extensive experiments on recent\npublicly available LLMs on instruction-following tasks and demonstrate the\neffectiveness of our training framework.\n', ""  Large Language Models (LLMs) have demonstrated remarkable versatility across\nvarious domains. To further advance LLMs, we propose 'SELF' (Self-Evolution\nwith Language Feedback), a novel approach that enables LLMs to self-improve\nthrough self-reflection, akin to human learning processes. SELF initiates with\na meta-skill learning process that equips the LLMs with capabilities for\nself-feedback and self-refinement. Subsequently, the model undergoes an\niterative process of self-evolution. In each iteration, it utilizes an\nunlabeled dataset of instructions to generate initial responses. These\nresponses are enhanced through self-feedback and self-refinement. The model is\nthen fine-tuned using this enhanced data. The model undergoes progressive\nimprovement through this iterative self-evolution process. Moreover, the SELF\nframework enables the model to apply self-refinement during inference, which\nfurther improves response quality. Our experiments in mathematics and general\ntasks demonstrate that SELF can enhance the capabilities of LLMs without human\nintervention. The SELF framework indicates a promising direction for the\nautonomous evolution of LLMs, transitioning them from passive information\nreceivers to active participants in their development.\n""]",Instruction Tuning for Large Language Models,Advances in Large Language Models,Large Language Models
62,62,92,62_pose_poses_camera_cameras,"['pose', 'poses', 'camera', 'cameras', 'scenes', '3d', 'articulated', 'vision', 'scene', 'slam']","['pose', 'camera', 'scene', 'flow', 'motion', 'object', 'estimation', 'point', 'optical', 'objects']","[""  In this work, we introduce a novel method for calculating the 6DoF pose of an\nobject using a single RGB-D image. Unlike existing methods that either directly\npredict objects' poses or rely on sparse keypoints for pose recovery, our\napproach addresses this challenging task using dense correspondence, i.e., we\nregress the object coordinates for each visible pixel. Our method leverages\nexisting object detection methods. We incorporate a re-projection mechanism to\nadjust the camera's intrinsic matrix to accommodate cropping in RGB-D images.\nMoreover, we transform the 3D object coordinates into a residual\nrepresentation, which can effectively reduce the output space and yield\nsuperior performance. We conducted extensive experiments to validate the\nefficacy of our approach for 6D pose estimation. Our approach outperforms most\nprevious methods, especially in occlusion scenarios, and demonstrates notable\nimprovements over the state-of-the-art methods. Our code is available on\nhttps://github.com/AI-Application-and-Integration-Lab/RDPN6D.\n"", '  Pose regression networks predict the camera pose of a query image relative to\na known environment. Within this family of methods, absolute pose regression\n(APR) has recently shown promising accuracy in the range of a few centimeters\nin position error. APR networks encode the scene geometry implicitly in their\nweights. To achieve high accuracy, they require vast amounts of training data\nthat, realistically, can only be created using novel view synthesis in a\ndays-long process. This process has to be repeated for each new scene again and\nagain. We present a new approach to pose regression, map-relative pose\nregression (marepo), that satisfies the data hunger of the pose regression\nnetwork in a scene-agnostic fashion. We condition the pose regressor on a\nscene-specific map representation such that its pose predictions are relative\nto the scene map. This allows us to train the pose regressor across hundreds of\nscenes to learn the generic relation between a scene-specific map\nrepresentation and the camera pose. Our map-relative pose regressor can be\napplied to new map representations immediately or after mere minutes of\nfine-tuning for the highest accuracy. Our approach outperforms previous pose\nregression methods by far on two public datasets, indoor and outdoor. Code is\navailable: https://nianticlabs.github.io/marepo\n', '  Reconstructing 4D scenes from video inputs is a crucial yet challenging task.\nConventional methods usually rely on the assumptions of multi-view video\ninputs, known camera parameters, or static scenes, all of which are typically\nabsent under in-the-wild scenarios. In this paper, we relax all these\nconstraints and tackle a highly ambitious but practical task, which we termed\nas AnyV4D: we assume only one monocular video is available without any camera\nparameters as input, and we aim to recover the dynamic 4D world alongside the\ncamera poses. To this end, we introduce GFlow, a new framework that utilizes\nonly 2D priors (depth and optical flow) to lift a video (3D) to a 4D explicit\nrepresentation, entailing a flow of Gaussian splatting through space and time.\nGFlow first clusters the scene into still and moving parts, then applies a\nsequential optimization process that optimizes camera poses and the dynamics of\n3D Gaussian points based on 2D priors and scene clustering, ensuring fidelity\namong neighboring points and smooth movement across frames. Since dynamic\nscenes always introduce new content, we also propose a new pixel-wise\ndensification strategy for Gaussian points to integrate new visual content.\nMoreover, GFlow transcends the boundaries of mere 4D reconstruction; it also\nenables tracking of any points across frames without the need for prior\ntraining and segments moving objects from the scene in an unsupervised way.\nAdditionally, the camera poses of each frame can be derived from GFlow,\nallowing for rendering novel views of a video scene through changing camera\npose. By employing the explicit representation, we may readily conduct\nscene-level or object-level editing as desired, underscoring its versatility\nand power. Visit our project website at: https://littlepure2333.github.io/GFlow\n']",6D Pose Estimation and Scene Reconstruction,3D Pose Estimation and Scene Understanding,Computer Vision and 3D Scene Understanding
63,63,92,63_visual_supervised_learning_multimodal,"['visual', 'supervised', 'learning', 'multimodal', 'prompts', 'captioning', 'embeddings', 'trained', 'captions', 'prompt']","['prompt', 'visual', 'vision', 'image', 'prompts', 'contrastive', 'concepts', 'downstream', 'tuning', 'pre']","['  Visual prompt tuning (VPT) is a promising solution incorporating learnable\nprompt tokens to customize pre-trained models for downstream tasks. However,\nVPT and its variants often encounter challenges like prompt initialization,\nprompt length, and subpar performance in self-supervised pretraining, hindering\nsuccessful contextual adaptation. This study commences by exploring the\ncorrelation evolvement between prompts and patch tokens during proficient\ntraining. Inspired by the observation that the prompt tokens tend to share high\nmutual information with patch tokens, we propose initializing prompts with\ndownstream token prototypes. The strategic initialization, a stand-in for the\nprevious initialization, substantially improves performance in fine-tuning. To\nrefine further, we optimize token construction with a streamlined pipeline that\nmaintains excellent performance with almost no increase in computational\nexpenses compared to VPT. Exhaustive experiments show our proposed approach\noutperforms existing methods by a remarkable margin. For instance, it surpasses\nfull fine-tuning in 19 out of 24 tasks, using less than 0.4% of learnable\nparameters on the FGVC and VTAB-1K benchmarks. Notably, our method\nsignificantly advances the adaptation for self-supervised pretraining,\nachieving impressive task performance gains of at least 10% to 30%. Besides,\nthe experimental results demonstrate the proposed SPT is robust to prompt\nlengths and scales well with model capacity and training data size. We finally\nprovide an insightful exploration into the amount of target data facilitating\nthe adaptation of pre-trained models to downstream tasks. The code is available\nat https://github.com/WangYZ1608/Self-Prompt-Tuning.\n', '  Data-Free Knowledge Distillation (DFKD) has shown great potential in creating\na compact student model while alleviating the dependency on real training data\nby synthesizing surrogate data. However, prior arts are seldom discussed under\ndistribution shifts, which may be vulnerable in real-world applications. Recent\nVision-Language Foundation Models, e.g., CLIP, have demonstrated remarkable\nperformance in zero-shot out-of-distribution generalization, yet consuming\nheavy computation resources. In this paper, we discuss the extension of DFKD to\nVision-Language Foundation Models without access to the billion-level\nimage-text datasets. The objective is to customize a student model for\ndistribution-agnostic downstream tasks with given category concepts, inheriting\nthe out-of-distribution generalization capability from the pre-trained\nfoundation models. In order to avoid generalization degradation, the primary\nchallenge of this task lies in synthesizing diverse surrogate images driven by\ntext prompts. Since not only category concepts but also style information are\nencoded in text prompts, we propose three novel Prompt Diversification methods\nto encourage image synthesis with diverse styles, namely Mix-Prompt,\nRandom-Prompt, and Contrastive-Prompt. Experiments on out-of-distribution\ngeneralization datasets demonstrate the effectiveness of the proposed methods,\nwith Contrastive-Prompt performing the best.\n', ""  In recent years, soft prompt learning methods have been proposed to fine-tune\nlarge-scale vision-language pre-trained models for various downstream tasks.\nThese methods typically combine learnable textual tokens with class tokens as\ninput for models with frozen parameters. However, they often employ a single\nprompt to describe class contexts, failing to capture categories' diverse\nattributes adequately. This study introduces the Partitioned Multi-modal Prompt\n(PMPO), a multi-modal prompting technique that extends the soft prompt from a\nsingle learnable prompt to multiple prompts. Our method divides the visual\nencoder depths and connects learnable prompts to the separated visual depths,\nenabling different prompts to capture the hierarchical contextual depths of\nvisual representations. Furthermore, to maximize the advantages of multi-prompt\nlearning, we incorporate prior information from manually designed templates and\nlearnable multi-prompts, thus improving the generalization capabilities of our\napproach. We evaluate the effectiveness of our approach on three challenging\ntasks: new class generalization, cross-dataset evaluation, and domain\ngeneralization. For instance, our method achieves a $79.28$ harmonic mean,\naveraged over 11 diverse image recognition datasets ($+7.62$ compared to CoOp),\ndemonstrating significant competitiveness compared to state-of-the-art\nprompting methods.\n""]",Multimodal Prompt Learning for Vision-Language Tasks,Vision-Language Models and Multimodal Learning,Multimodal Learning and Vision-Language Models
64,64,92,64_bandits_bandit_optimal_regret,"['bandits', 'bandit', 'optimal', 'regret', 'optimization', 'reward', 'games', 'guarantees', 'exploration', 'rewards']","['regret', 'bandit', 'games', 'bandits', 'online', 'algorithms', 'bound', 'bounds', 'monotone', 'convex']","['  In this work, we study potential games and Markov potential games under\nstochastic cost and bandit feedback. We propose a variant of the Frank-Wolfe\nalgorithm with sufficient exploration and recursive gradient estimation, which\nprovably converges to the Nash equilibrium while attaining sublinear regret for\neach individual player. Our algorithm simultaneously achieves a Nash regret and\na regret bound of $O(T^{4/5})$ for potential games, which matches the best\navailable result, without using additional projection steps. Through carefully\nbalancing the reuse of past samples and exploration of new samples, we then\nextend the results to Markov potential games and improve the best available\nNash regret from $O(T^{5/6})$ to $O(T^{4/5})$. Moreover, our algorithm requires\nno knowledge of the game, such as the distribution mismatch coefficient, which\nprovides more flexibility in its practical implementation. Experimental results\ncorroborate our theoretical findings and underscore the practical effectiveness\nof our method.\n', ""  Large language models (LLMs) have been increasingly employed for\n(interactive) decision-making, via the development of LLM-based autonomous\nagents. Despite their emerging successes, the performance of LLM agents in\ndecision-making has not been fully investigated through quantitative metrics,\nespecially in the multi-agent setting when they interact with each other, a\ntypical scenario in real-world LLM-agent applications. To better understand the\nlimits of LLM agents in these interactive environments, we propose to study\ntheir interactions in benchmark decision-making settings in online learning and\ngame theory, through the performance metric of \\emph{regret}. We first\nempirically study the {no-regret} behaviors of LLMs in canonical\n(non-stationary) online learning problems, as well as the emergence of\nequilibria when LLM agents interact through playing repeated games. We then\nprovide some theoretical insights into the no-regret behaviors of LLM agents,\nunder certain assumptions on the supervised pre-training and the rationality\nmodel of human decision-makers who generate the data. Notably, we also identify\n(simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To\npromote the no-regret behaviors, we propose a novel \\emph{unsupervised}\ntraining loss of \\emph{regret-loss}, which, in contrast to the supervised\npre-training loss, does not require the labels of (optimal) actions. We then\nestablish the statistical guarantee of generalization bound for regret-loss\nminimization, followed by the optimization guarantee that minimizing such a\nloss may automatically lead to known no-regret learning algorithms. Our further\nexperiments demonstrate the effectiveness of our regret-loss, especially in\naddressing the above ``regrettable'' cases.\n"", ""  We consider online no-regret learning in unknown games with bandit feedback,\nwhere each player can only observe its reward at each time -- determined by all\nplayers' current joint action -- rather than its gradient. We focus on the\nclass of \\textit{smooth and strongly monotone} games and study optimal\nno-regret learning therein. Leveraging self-concordant barrier functions, we\nfirst construct a new bandit learning algorithm and show that it achieves the\nsingle-agent optimal regret of $\\tilde{\\Theta}(n\\sqrt{T})$ under smooth and\nstrongly concave reward functions ($n \\geq 1$ is the problem dimension). We\nthen show that if each player applies this no-regret learning algorithm in\nstrongly monotone games, the joint action converges in the \\textit{last\niterate} to the unique Nash equilibrium at a rate of\n$\\tilde{\\Theta}(nT^{-1/2})$. Prior to our work, the best-known convergence rate\nin the same class of games is $\\tilde{O}(n^{2/3}T^{-1/3})$ (achieved by a\ndifferent algorithm), thus leaving open the problem of optimal no-regret\nlearning algorithms (since the known lower bound is $\\Omega(nT^{-1/2})$). Our\nresults thus settle this open problem and contribute to the broad landscape of\nbandit game-theoretical learning by identifying the first doubly optimal bandit\nlearning algorithm, in that it achieves (up to log factors) both optimal regret\nin the single-agent learning and optimal last-iterate convergence rate in the\nmulti-agent learning. We also present preliminary numerical results on several\napplication problems to demonstrate the efficacy of our algorithm in terms of\niteration count.\n""]",Bandit Learning and Regret Optimization in Games,Optimization and Learning in Bandit and Game-Theoretic Settings,Decision Making and Optimization under Uncertainty
65,65,92,65_graphs_graphon_graph_adjacency,"['graphs', 'graphon', 'graph', 'adjacency', 'spectral', 'nodes', 'networks', 'vertex', 'edges', 'clustering']","['graph', 'graphs', 'spectral', 'clustering', 'filters', 'node', 'distance', 'edges', 'signal', 'nodes']","['  Time-evolving graphs arise frequently when modeling complex dynamical systems\nsuch as social networks, traffic flow, and biological processes. Developing\ntechniques to identify and analyze communities in these time-varying graph\nstructures is an important challenge. In this work, we generalize existing\nspectral clustering algorithms from static to dynamic graphs using canonical\ncorrelation analysis (CCA) to capture the temporal evolution of clusters. Based\non this extended canonical correlation framework, we define the dynamic graph\nLaplacian and investigate its spectral properties. We connect these concepts to\ndynamical systems theory via transfer operators, and illustrate the advantages\nof our method on benchmark graphs by comparison with existing methods. We show\nthat the dynamic graph Laplacian allows for a clear interpretation of cluster\nstructure evolution over time for directed and undirected graphs.\n', '  Graph signal processing (GSP) is a prominent framework for analyzing signals\non non-Euclidean domains. The graph Fourier transform (GFT) uses the\ncombinatorial graph Laplacian matrix to reveal the spectral decomposition of\nsignals in the graph frequency domain. However, a common challenge in applying\nGSP methods is that in many scenarios the underlying graph of a system is\nunknown. A solution in such cases is to construct the unobserved graph from\navailable data, which is commonly referred to as graph or network inference.\nAlthough different graph inference methods exist, these are restricted to\nlearning from either smooth graph signals or simple additive Gaussian noise.\nOther types of noisy data, such as discrete counts or binary digits, are rather\ncommon in real-world applications, yet are underexplored in graph inference. In\nthis paper, we propose a versatile graph inference framework for learning from\ngraph signals corrupted by exponential family noise. Our framework generalizes\nprevious methods from continuous smooth graph signals to various data types. We\npropose an alternating algorithm that jointly estimates the graph Laplacian and\nthe unobserved smooth representation from the noisy signals. We also extend our\napproach to a variational form to account for the inherent stochasticity of the\nlatent smooth representation. Finally, since real-world graph signals are\nfrequently non-independent and temporally correlated, we further adapt our\noriginal setting to a time-vertex formulation. We demonstrate on synthetic and\nreal-world data that our new algorithms outperform competing Laplacian\nestimation methods that suffer from noise model mismatch.\n', '  Many real-world systems can be represented as graphs where the different\nentities in the system are presented by nodes and their interactions by edges.\nAn important task in studying large datasets with graphical structure is graph\nclustering. While there has been a lot of work on graph clustering using the\nconnectivity between the nodes, many real-world networks also have node\nattributes. Clustering attributed graphs requires joint modeling of graph\nstructure and node attributes. Recent work has focused on combining these two\ncomplementary sources of information through graph convolutional networks and\ngraph filtering. However, these methods are mostly limited to lowpass filtering\nand do not explicitly learn the filter parameters for the clustering task. In\nthis paper, we introduce a graph signal processing based approach, where we\nlearn the parameters of Finite Impulse Response (FIR) and Autoregressive Moving\nAverage (ARMA) graph filters optimized for clustering. The proposed approach is\nformulated as a two-step iterative optimization problem, focusing on learning\ninterpretable graph filters that are optimal for the given data and that\nmaximize the separation between different clusters. The proposed approach is\nevaluated on attributed networks and compared to the state-of-the-art methods.\n']",Graph Clustering and Signal Processing,Graph Analysis and Processing Techniques,Data Analysis and Pattern Discovery
66,66,92,66_reinforcement_agents_games_agent,"['reinforcement', 'agents', 'games', 'agent', 'rewards', 'cooperation', 'reward', 'opponents', 'strategies', 'cooperative']","['agent', 'games', 'game', 'agents', 'multi', 'reinforcement', 'cooperative', 'policy', 'player', 'policies']","['  Recent advances in reinforcement learning (RL) heavily rely on a variety of\nwell-designed benchmarks, which provide environmental platforms and consistent\ncriteria to evaluate existing and novel algorithms. Specifically, in\nmulti-agent RL (MARL), a plethora of benchmarks based on cooperative games have\nspurred the development of algorithms that improve the scalability of\ncooperative multi-agent systems. However, for the competitive setting, a\nlightweight and open-sourced benchmark with challenging gaming dynamics and\nvisual inputs has not yet been established. In this work, we present\nFightLadder, a real-time fighting game platform, to empower competitive MARL\nresearch. Along with the platform, we provide implementations of\nstate-of-the-art MARL algorithms for competitive games, as well as a set of\nevaluation metrics to characterize the performance and exploitability of\nagents. We demonstrate the feasibility of this platform by training a general\nagent that consistently defeats 12 built-in characters in single-player mode,\nand expose the difficulty of training a non-exploitable agent without human\nknowledge and demonstrations in two-player mode. FightLadder provides\nmeticulously designed environments to address critical challenges in\ncompetitive MARL research, aiming to catalyze a new era of discovery and\nadvancement in the field. Videos and code at\nhttps://sites.google.com/view/fightladder/home.\n', '  Multi-agent reinforcement learning (MARL) methods, while effective in\nzero-sum or positive-sum games, often yield suboptimal outcomes in general-sum\ngames where cooperation is essential for achieving globally optimal outcomes.\nMatrix game social dilemmas, which abstract key aspects of general-sum\ninteractions, such as cooperation, risk, and trust, fail to model the temporal\nand spatial dynamics characteristic of real-world scenarios. In response, our\nstudy extends matrix game social dilemmas into more complex, higher-dimensional\nMARL environments. We adapt a gridworld implementation of the Stag Hunt dilemma\nto more closely match the decision-space of a one-shot matrix game while also\nintroducing variable environment complexity. Our findings indicate that as\ncomplexity increases, MARL agents trained in these environments converge to\nsuboptimal strategies, consistent with the risk-dominant Nash equilibria\nstrategies found in matrix games. Our work highlights the impact of environment\ncomplexity on achieving optimal outcomes in higher-dimensional game-theoretic\nMARL environments.\n', '  Measuring the contribution of individual agents is challenging in cooperative\nmulti-agent reinforcement learning (MARL). In cooperative MARL, team\nperformance is typically inferred from a single shared global reward. Arguably,\namong the best current approaches to effectively measure individual agent\ncontributions is to use Shapley values. However, calculating these values is\nexpensive as the computational complexity grows exponentially with respect to\nthe number of agents. In this paper, we adapt difference rewards into an\nefficient method for quantifying the contribution of individual agents,\nreferred to as Agent Importance, offering a linear computational complexity\nrelative to the number of agents. We show empirically that the computed values\nare strongly correlated with the true Shapley values, as well as the true\nunderlying individual agent rewards, used as the ground truth in environments\nwhere these are available. We demonstrate how Agent Importance can be used to\nhelp study MARL systems by diagnosing algorithmic failures discovered in prior\nMARL benchmarking work. Our analysis illustrates Agent Importance as a valuable\nexplainability component for future MARL benchmarks.\n']",Multi-Agent Reinforcement Learning,Multi-Agent Decision Making and Reinforcement Learning,Multi-Agent Systems and Artificial Intelligence
66,66,92,66_reinforcement_agents_games_agent,"['reinforcement', 'agents', 'games', 'agent', 'rewards', 'cooperation', 'reward', 'opponents', 'strategies', 'cooperative']","['agent', 'games', 'game', 'agents', 'multi', 'reinforcement', 'cooperative', 'policy', 'player', 'policies']","['  Recent advances in reinforcement learning (RL) heavily rely on a variety of\nwell-designed benchmarks, which provide environmental platforms and consistent\ncriteria to evaluate existing and novel algorithms. Specifically, in\nmulti-agent RL (MARL), a plethora of benchmarks based on cooperative games have\nspurred the development of algorithms that improve the scalability of\ncooperative multi-agent systems. However, for the competitive setting, a\nlightweight and open-sourced benchmark with challenging gaming dynamics and\nvisual inputs has not yet been established. In this work, we present\nFightLadder, a real-time fighting game platform, to empower competitive MARL\nresearch. Along with the platform, we provide implementations of\nstate-of-the-art MARL algorithms for competitive games, as well as a set of\nevaluation metrics to characterize the performance and exploitability of\nagents. We demonstrate the feasibility of this platform by training a general\nagent that consistently defeats 12 built-in characters in single-player mode,\nand expose the difficulty of training a non-exploitable agent without human\nknowledge and demonstrations in two-player mode. FightLadder provides\nmeticulously designed environments to address critical challenges in\ncompetitive MARL research, aiming to catalyze a new era of discovery and\nadvancement in the field. Videos and code at\nhttps://sites.google.com/view/fightladder/home.\n', '  Multi-agent reinforcement learning (MARL) methods, while effective in\nzero-sum or positive-sum games, often yield suboptimal outcomes in general-sum\ngames where cooperation is essential for achieving globally optimal outcomes.\nMatrix game social dilemmas, which abstract key aspects of general-sum\ninteractions, such as cooperation, risk, and trust, fail to model the temporal\nand spatial dynamics characteristic of real-world scenarios. In response, our\nstudy extends matrix game social dilemmas into more complex, higher-dimensional\nMARL environments. We adapt a gridworld implementation of the Stag Hunt dilemma\nto more closely match the decision-space of a one-shot matrix game while also\nintroducing variable environment complexity. Our findings indicate that as\ncomplexity increases, MARL agents trained in these environments converge to\nsuboptimal strategies, consistent with the risk-dominant Nash equilibria\nstrategies found in matrix games. Our work highlights the impact of environment\ncomplexity on achieving optimal outcomes in higher-dimensional game-theoretic\nMARL environments.\n', '  Measuring the contribution of individual agents is challenging in cooperative\nmulti-agent reinforcement learning (MARL). In cooperative MARL, team\nperformance is typically inferred from a single shared global reward. Arguably,\namong the best current approaches to effectively measure individual agent\ncontributions is to use Shapley values. However, calculating these values is\nexpensive as the computational complexity grows exponentially with respect to\nthe number of agents. In this paper, we adapt difference rewards into an\nefficient method for quantifying the contribution of individual agents,\nreferred to as Agent Importance, offering a linear computational complexity\nrelative to the number of agents. We show empirically that the computed values\nare strongly correlated with the true Shapley values, as well as the true\nunderlying individual agent rewards, used as the ground truth in environments\nwhere these are available. We demonstrate how Agent Importance can be used to\nhelp study MARL systems by diagnosing algorithmic failures discovered in prior\nMARL benchmarking work. Our analysis illustrates Agent Importance as a valuable\nexplainability component for future MARL benchmarks.\n']",Multi-Agent Reinforcement Learning,Multi-Agent Decision Making and Reinforcement Learning,Multi-Agent Systems and Artificial Intelligence
67,67,91,67_adversarial_gans_watermarking_steganalysis,"['adversarial', 'gans', 'watermarking', 'steganalysis', 'copyright', 'steganography', 'deepfacegen', 'watermark', 'spoofing', 'impersonation']","['images', 'copyright', 'fingerprint', 'image', 'face', 'diffusion', 'attack', 'generative', 'steganalysis', 'detection']","['  The commercialization of text-to-image diffusion models (DMs) brings forth\npotential copyright concerns. Despite numerous attempts to protect DMs from\ncopyright issues, the vulnerabilities of these solutions are underexplored. In\nthis study, we formalized the Copyright Infringement Attack on generative AI\nmodels and proposed a backdoor attack method, SilentBadDiffusion, to induce\ncopyright infringement without requiring access to or control over training\nprocesses. Our method strategically embeds connections between pieces of\ncopyrighted information and text references in poisoning data while carefully\ndispersing that information, making the poisoning data inconspicuous when\nintegrated into a clean dataset. Our experiments show the stealth and efficacy\nof the poisoning data. When given specific text prompts, DMs trained with a\npoisoning ratio of 0.20% can produce copyrighted images. Additionally, the\nresults reveal that the more sophisticated the DMs are, the easier the success\nof the attack becomes. These findings underline potential pitfalls in the\nprevailing copyright protection strategies and underscore the necessity for\nincreased scrutiny to prevent the misuse of DMs.\n', '  Diffusion Models (DMs) have shown remarkable capabilities in various\nimage-generation tasks. However, there are growing concerns that DMs could be\nused to imitate unauthorized creations and thus raise copyright issues. To\naddress this issue, we propose a novel framework that embeds personal\nwatermarks in the generation of adversarial examples. Such examples can force\nDMs to generate images with visible watermarks and prevent DMs from imitating\nunauthorized images. We construct a generator based on conditional adversarial\nnetworks and design three losses (adversarial loss, GAN loss, and perturbation\nloss) to generate adversarial examples that have subtle perturbation but can\neffectively attack DMs to prevent copyright violations. Training a generator\nfor a personal watermark by our method only requires 5-10 samples within 2-3\nminutes, and once the generator is trained, it can generate adversarial\nexamples with that watermark significantly fast (0.2s per image). We conduct\nextensive experiments in various conditional image-generation scenarios.\nCompared to existing methods that generate images with chaotic textures, our\nmethod adds visible watermarks on the generated images, which is a more\nstraightforward way to indicate copyright violations. We also observe that our\nadversarial examples exhibit good transferability across unknown generative\nmodels. Therefore, this work provides a simple yet powerful way to protect\ncopyright from DM-based imitation.\n', '  Diffusion models excel in many generative modeling tasks, notably in creating\nimages from text prompts, a task referred to as text-to-image (T2I) generation.\nDespite the ability to generate high-quality images, these models often\nreplicate elements from their training data, leading to increasing copyright\nconcerns in real applications in recent years. In response to this raising\nconcern about copyright infringement, recent studies have studied the copyright\nbehavior of diffusion models when using direct, copyrighted prompts. Our\nresearch extends this by examining subtler forms of infringement, where even\nindirect prompts can trigger copyright issues. Specifically, we introduce a\ndata generation pipeline to systematically produce data for studying copyright\nin diffusion models. Our pipeline enables us to investigate copyright\ninfringement in a more practical setting, involving replicating visual features\nrather than entire works using seemingly irrelevant prompts for T2I generation.\nWe generate data using our proposed pipeline to test various diffusion models,\nincluding the latest Stable Diffusion XL. Our findings reveal a widespread\ntendency that these models tend to produce copyright-infringing content,\nhighlighting a significant challenge in this field.\n']",Copyright Protection in Text-to-Image Diffusion Models,"Diffusion Models for Text, Image, and Graph Generation",Diffusion Models for Generative Tasks
68,68,89,68_posterior_bayesian_generative_phylogenetic,"['posterior', 'bayesian', 'generative', 'phylogenetic', 'likelihood', 'inference', 'models', 'estimating', 'markov', 'monte']","['posterior', 'latent', 'likelihood', 'inference', 'variational', 'phylogenetic', 'tree', 'dynamics', 'variables', 'estimation']","[""  Bayesian Additive Regression Trees (BART) is a popular Bayesian\nnon-parametric regression model that is commonly used in causal inference and\nbeyond. Its strong predictive performance is supported by theoretical\nguarantees that its posterior distribution concentrates around the true\nregression function at optimal rates under various data generative settings and\nfor appropriate prior choices. In this paper, we show that the BART sampler\noften converges slowly, confirming empirical observations by other researchers.\nAssuming discrete covariates, we show that, while the BART posterior\nconcentrates on a set comprising all optimal tree structures (smallest bias and\ncomplexity), the Markov chain's hitting time for this set increases with $n$\n(training sample size), under several common data generative settings. As $n$\nincreases, the approximate BART posterior thus becomes increasingly different\nfrom the exact posterior (for the same number of MCMC samples), contrasting\nwith earlier concentration results on the exact posterior. This contrast is\nhighlighted by our simulations showing worsening frequentist undercoverage for\napproximate posterior intervals and a growing ratio between the MSE of the\napproximate posterior and that obtainable by artificially improving convergence\nvia averaging multiple sampler chains. Finally, based on our theoretical\ninsights, possibilities are discussed to improve the BART sampler convergence\nperformance.\n"", '  Bayesian phylogenetic inference is currently done via Markov chain Monte\nCarlo (MCMC) with simple proposal mechanisms. This hinders exploration\nefficiency and often requires long runs to deliver accurate posterior\nestimates. In this paper, we present an alternative approach: a variational\nframework for Bayesian phylogenetic analysis. We propose combining subsplit\nBayesian networks, an expressive graphical model for tree topology\ndistributions, and a structured amortization of the branch lengths over tree\ntopologies for a suitable variational family of distributions. We train the\nvariational approximation via stochastic gradient ascent and adopt gradient\nestimators for continuous and discrete variational parameters separately to\ndeal with the composite latent space of phylogenetic models. We show that our\nvariational approach provides competitive performance to MCMC, while requiring\nmuch fewer (though more costly) iterations due to a more efficient exploration\nmechanism enabled by variational inference. Experiments on a benchmark of\nchallenging real data Bayesian phylogenetic inference problems demonstrate the\neffectiveness and efficiency of our methods.\n', '  Likelihood-free inference methods based on neural conditional density\nestimation were shown to drastically reduce the simulation burden in comparison\nto classical methods such as ABC. When applied in the context of any latent\nvariable model, such as a Hidden Markov model (HMM), these methods are designed\nto only estimate the parameters, rather than the joint distribution of the\nparameters and the hidden states. Naive application of these methods to a HMM,\nignoring the inference of this joint posterior distribution, will thus produce\nan inaccurate estimate of the posterior predictive distribution, in turn\nhampering the assessment of goodness-of-fit. To rectify this problem, we\npropose a novel, sample-efficient likelihood-free method for estimating the\nhigh-dimensional hidden states of an implicit HMM. Our approach relies on\nlearning directly the intractable posterior distribution of the hidden states,\nusing an autoregressive-flow, by exploiting the Markov property. Upon\nevaluating our approach on some implicit HMMs, we found that the quality of the\nestimates retrieved using our method is comparable to what can be achieved\nusing a much more computationally expensive SMC algorithm.\n']",Bayesian Inference and Phylogenetic Models,Computational Methods in Historical Linguistics and Phylogenetics,Computational Methods in Evolutionary Analysis
69,69,89,69_dialogues_dialogue_dialog_conversation,"['dialogues', 'dialogue', 'dialog', 'conversation', 'conversational', 'chatbots', 'conversations', 'chatbot', 'chat', 'planning']","['dialogue', 'conversation', 'conversational', 'dialogues', 'conversations', 'negotiation', 'chatbots', 'agents', 'user', 'chatbot']","['  Dialogue systems, commonly known as chatbots, have gained escalating\npopularity in recent times due to their wide-spread applications in carrying\nout chit-chat conversations with users and task-oriented dialogues to\naccomplish various user tasks. Existing chatbots are usually trained from\npre-collected and manually-labeled data and/or written with handcrafted rules.\nMany also use manually-compiled knowledge bases (KBs). Their ability to\nunderstand natural language is still limited, and they tend to produce many\nerrors resulting in poor user satisfaction. Typically, they need to be\nconstantly improved by engineers with more labeled data and more manually\ncompiled knowledge. This book introduces the new paradigm of lifelong learning\ndialogue systems to endow chatbots the ability to learn continually by\nthemselves through their own self-initiated interactions with their users and\nworking environments to improve themselves. As the systems chat more and more\nwith users or learn more and more from external sources, they become more and\nmore knowledgeable and better and better at conversing. The book presents the\nlatest developments and techniques for building such continual learning\ndialogue systems that continuously learn new language expressions and lexical\nand factual knowledge during conversation from users and off conversation from\nexternal sources, acquire new training examples during conversation, and learn\nconversational skills. Apart from these general topics, existing works on\ncontinual learning of some specific aspects of dialogue systems are also\nsurveyed. The book concludes with a discussion of open challenges for future\nresearch.\n', '  Target-oriented proactive dialogue systems aim to lead conversations from a\ndialogue context toward a pre-determined target, such as making recommendations\non designated items or introducing new specific topics. To this end, it is\ncritical for such dialogue systems to plan reasonable actions to drive the\nconversation proactively, and meanwhile, to plan appropriate topics to move the\nconversation forward to the target topic smoothly. In this work, we mainly\nfocus on effective dialogue planning for target-oriented dialogue generation.\nInspired by decision-making theories in cognitive science, we propose a novel\ntarget-constrained bidirectional planning (TRIP) approach, which plans an\nappropriate dialogue path by looking ahead and looking back. By formulating the\nplanning as a generation task, our TRIP bidirectionally generates a dialogue\npath consisting of a sequence of <action, topic> pairs using two Transformer\ndecoders. They are expected to supervise each other and converge on consistent\nactions and topics by minimizing the decision gap and contrastive generation of\ntargets. Moreover, we propose a target-constrained decoding algorithm with a\nbidirectional agreement to better control the planning process. Subsequently,\nwe adopt the planned dialogue paths to guide dialogue generation in a pipeline\nmanner, where we explore two variants: prompt-based generation and\nplan-controlled generation. Extensive experiments are conducted on two\nchallenging dialogue datasets, which are re-purposed for exploring\ntarget-oriented dialogue. Our automatic and human evaluations demonstrate that\nthe proposed methods significantly outperform various baseline models.\n', '  Proactive dialogues serve as a practical yet challenging dialogue problem in\nthe era of large language models (LLMs), where the dialogue policy planning is\nthe key to improving the proactivity of LLMs. Most existing studies enable the\ndialogue policy planning of LLMs using various prompting schemes or iteratively\nenhance this capability in handling the given case with verbal AI feedback.\nHowever, these approaches are either bounded by the policy planning capability\nof the frozen LLMs or hard to be transferred to new cases. In this work, we\nintroduce a new dialogue policy planning paradigm to strategize LLMs for\nproactive dialogue problems with a tunable language model plug-in as a\nplug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a\nnovel training framework to facilitate supervised fine-tuning over available\nhuman-annotated data as well as reinforcement learning from goal-oriented AI\nfeedback with dynamic interaction data collected by the LLM-based self-play\nsimulation. In this manner, the LLM-powered dialogue agent can not only be\ngeneralized to different cases after the training, but also be applicable to\ndifferent applications by just substituting the learned plug-in. In addition,\nwe propose to evaluate the policy planning capability of dialogue systems under\nthe interactive setting. Experimental results demonstrate that PPDPP\nconsistently and substantially outperforms existing approaches on three\ndifferent proactive dialogue applications, including negotiation, emotional\nsupport, and tutoring dialogues.\n']",Dialogue Systems and Conversational AI,Conversational AI and Language Models,Conversational AI and Human-Computer Interaction
70,70,88,70_traffic_accidentgpt_driving_accidents,"['traffic', 'accidentgpt', 'driving', 'accidents', 'roads', 'lanes', 'crash', 'vehicles', 'predicting', 'prediction']","['traffic', 'road', 'trajectory', 'accident', 'vehicle', 'accidents', 'crash', 'trajectories', 'prediction', 'transportation']","['  Traffic accidents, being a significant contributor to both human casualties\nand property damage, have long been a focal point of research for many scholars\nin the field of traffic safety. However, previous studies, whether focusing on\nstatic environmental assessments or dynamic driving analyses, as well as\npre-accident predictions or post-accident rule analyses, have typically been\nconducted in isolation. There has been a lack of an effective framework for\ndeveloping a comprehensive understanding and application of traffic safety. To\naddress this gap, this paper introduces AccidentGPT, a comprehensive accident\nanalysis and prevention multi-modal large model. AccidentGPT establishes a\nmulti-modal information interaction framework grounded in multi-sensor\nperception, thereby enabling a holistic approach to accident analysis and\nprevention in the field of traffic safety. Specifically, our capabilities can\nbe categorized as follows: for autonomous driving vehicles, we provide\ncomprehensive environmental perception and understanding to control the vehicle\nand avoid collisions. For human-driven vehicles, we offer proactive long-range\nsafety warnings and blind-spot alerts while also providing safety driving\nrecommendations and behavioral norms through human-machine dialogue and\ninteraction. Additionally, for traffic police and management agencies, our\nframework supports intelligent and real-time analysis of traffic safety,\nencompassing pedestrian, vehicles, roads, and the environment through\ncollaborative perception from multiple vehicles and road testing devices. The\nsystem is also capable of providing a thorough analysis of accident causes and\nliability after vehicle collisions. Our framework stands as the first large\nmodel to integrate comprehensive scene understanding into traffic safety\nstudies. Project page: https://accidentgpt.github.io\n', '  We consider the problem of traffic accident analysis on a road network based\non road network connections and traffic volume. Previous works have designed\nvarious deep-learning methods using historical records to predict traffic\naccident occurrences. However, there is a lack of consensus on how accurate\nexisting methods are, and a fundamental issue is the lack of public accident\ndatasets for comprehensive evaluations. This paper constructs a large-scale,\nunified dataset of traffic accident records from official reports of various\nstates in the US, totaling 9 million records, accompanied by road networks and\ntraffic volume reports. Using this new dataset, we evaluate existing\ndeep-learning methods for predicting the occurrence of accidents on road\nnetworks. Our main finding is that graph neural networks such as GraphSAGE can\naccurately predict the number of accidents on roads with less than 22% mean\nabsolute error (relative to the actual count) and whether an accident will\noccur or not with over 87% AUROC, averaged over states. We achieve these\nresults by using multitask learning to account for cross-state variabilities\n(e.g., availability of accident labels) and transfer learning to combine\ntraffic volume with accident prediction. Ablation studies highlight the\nimportance of road graph-structural features, amongst other features. Lastly,\nwe discuss the implications of the analysis and develop a package for easily\nusing our new dataset.\n', '  The precise prediction of multi-scale traffic is a ubiquitous challenge in\nthe urbanization process for car owners, road administrators, and governments.\nIn the case of complex road networks, current and past traffic information from\nboth upstream and downstream roads are crucial since various road networks have\ndifferent semantic information about traffic. Rationalizing the utilization of\nsemantic information can realize short-term, long-term, and unseen road traffic\nprediction. As the demands of multi-scale traffic analysis increase, on-demand\ninteractions and visualizations are expected to be available for transportation\nparticipants. We have designed a multi-scale traffic generation system, namely\nTrafficGPT, using three AI agents to process multi-scale traffic data, conduct\nmulti-scale traffic analysis, and present multi-scale visualization results.\nTrafficGPT consists of three essential AI agents: 1) a text-to-demand agent\nthat is employed with Question & Answer AI to interact with users and extract\nprediction tasks through texts; 2) a traffic prediction agent that leverages\nmulti-scale traffic data to generate temporal features and similarity, and fuse\nthem with limited spatial features and similarity, to achieve accurate\nprediction of three tasks; and 3) a suggestion and visualization agent that\nuses the prediction results to generate suggestions and visualizations,\nproviding users with a comprehensive understanding of traffic conditions. Our\nTrafficGPT system focuses on addressing concerns about traffic prediction from\ntransportation participants, and conducted extensive experiments on five\nreal-world road datasets to demonstrate its superior predictive and interactive\nperformance\n']",Traffic Accident Analysis and Prediction,Environmental and Transportation Predictive Analytics,Transportation Systems and Environmental Analytics
71,71,88,71_cnn_cnns_efficientnetv2b1_memory,"['cnn', 'cnns', 'efficientnetv2b1', 'memory', 'optimized', 'networks', 'neural', 'deep', 'dnns', 'dnn']","['deep', 'inference', 'edge', 'hardware', 'accuracy', 'device', 'latency', 'training', 'resource', 'memory']","[""  Deploying Deep Neural Networks (DNNs) on microcontrollers (TinyML) is a\ncommon trend to process the increasing amount of sensor data generated at the\nedge, but in practice, resource and latency constraints make it difficult to\nfind optimal DNN candidates. Neural Architecture Search (NAS) is an excellent\napproach to automate this search and can easily be combined with DNN\ncompression techniques commonly used in TinyML. However, many NAS techniques\nare not only computationally expensive, especially hyperparameter optimization\n(HPO), but also often focus on optimizing only a single objective, e.g.,\nmaximizing accuracy, without considering additional objectives such as memory\nconsumption or computational complexity of a DNN, which are key to making\ndeployment at the edge feasible. In this paper, we propose a novel NAS strategy\nfor TinyML based on Multi-Objective Bayesian optimization (MOBOpt) and an\nensemble of competing parametric policies trained using Augmented Random Search\n(ARS) Reinforcement Learning (RL) agents. Our methodology aims at efficiently\nfinding tradeoffs between a DNN's predictive accuracy, memory consumption on a\ngiven target system, and computational complexity. Our experiments show that we\noutperform existing MOBOpt approaches consistently on different data sets and\narchitectures such as ResNet-18 and MobileNetV3.\n"", ""  While machine learning is traditionally a resource intensive task, embedded\nsystems, autonomous navigation, and the vision of the Internet of Things fuel\nthe interest in resource-efficient approaches. These approaches aim for a\ncarefully chosen trade-off between performance and resource consumption in\nterms of computation and energy. The development of such approaches is among\nthe major challenges in current machine learning research and key to ensure a\nsmooth transition of machine learning technology from a scientific environment\nwith virtually unlimited computing resources into everyday's applications. In\nthis article, we provide an overview of the current state of the art of machine\nlearning techniques facilitating these real-world requirements. In particular,\nwe focus on resource-efficient inference based on deep neural networks (DNNs),\nthe predominant machine learning models of the past decade. We give a\ncomprehensive overview of the vast literature that can be mainly split into\nthree non-mutually exclusive categories: (i) quantized neural networks, (ii)\nnetwork pruning, and (iii) structural efficiency. These techniques can be\napplied during training or as post-processing, and they are widely used to\nreduce the computational demands in terms of memory footprint, inference speed,\nand energy efficiency. We also briefly discuss different concepts of embedded\nhardware for DNNs and their compatibility with machine learning techniques as\nwell as potential for energy and latency reduction. We substantiate our\ndiscussion with experiments on well-known benchmark data sets using compression\ntechniques (quantization, pruning) for a set of resource-constrained embedded\nsystems, such as CPUs, GPUs and FPGAs. The obtained results highlight the\ndifficulty of finding good trade-offs between resource efficiency and\nprediction quality.\n"", ""  As artificial intelligence (AI) applications continue to expand, there is a\ngrowing need for deep neural network (DNN) models. Although DNN models deployed\nat the edge are promising to provide AI as a service with low latency, their\ncooperation is yet to be explored. In this paper, we consider the DNN service\nproviders share their computing resources as well as their models' parameters\nand allow other DNNs to offload their computations without mirroring. We\npropose a novel algorithm called coordinated DNNs on edge (\\textbf{CoDE}) that\nfacilitates coordination among DNN services by creating multi-task DNNs out of\nindividual models. CoDE aims to find the optimal path that results in the\nlowest possible cost, where the cost reflects the inference delay, model\naccuracy, and local computation workload. With CoDE, DNN models can make new\npaths for inference by using their own or other models' parameters. We then\nevaluate the performance of CoDE through numerical experiments. The results\ndemonstrate a $75\\%$ reduction in the local service computation workload while\ndegrading the accuracy by only $2\\%$ and having the same inference time in a\nbalanced load condition. Under heavy load, CoDE can further decrease the\ninference time by $30\\%$ while the accuracy is reduced by only $4\\%$.\n""]",Efficient Deep Neural Networks for Edge Deployment,Efficient Deep Learning Architectures and Acceleration Techniques,Artificial Intelligence and Machine Learning Hardware Acceleration and Optimization
72,72,87,72_supervised_classification_learning_adversarial,"['supervised', 'classification', 'learning', 'adversarial', 'labeling', 'deep', 'unlearning', 'trained', 'labeled', 'labels']","['medical', 'imaging', 'segmentation', 'disease', 'label', 'clinical', 'classification', 'image', 'images', 'diagnosis']","[""  The lack of annotated medical images limits the performance of deep learning\nmodels, which usually need large-scale labelled datasets. Few-shot learning\ntechniques can reduce data scarcity issues and enhance medical image analysis,\nespecially with meta-learning. This systematic review gives a comprehensive\noverview of few-shot learning in medical imaging. We searched the literature\nsystematically and selected 80 relevant articles published from 2018 to 2023.\nWe clustered the articles based on medical outcomes, such as tumour\nsegmentation, disease classification, and image registration; anatomical\nstructure investigated (i.e. heart, lung, etc.); and the meta-learning method\nused. For each cluster, we examined the papers' distributions and the results\nprovided by the state-of-the-art. In addition, we identified a generic pipeline\nshared among all the studies. The review shows that few-shot learning can\novercome data scarcity in most outcomes and that meta-learning is a popular\nchoice to perform few-shot learning because it can adapt to new tasks with few\nlabelled samples. In addition, following meta-learning, supervised learning and\nsemi-supervised learning stand out as the predominant techniques employed to\ntackle few-shot learning challenges in medical imaging and also best\nperforming. Lastly, we observed that the primary application areas\npredominantly encompass cardiac, pulmonary, and abdominal domains. This\nsystematic review aims to inspire further research to improve medical image\nanalysis and patient care.\n"", '  Objectives: Medical research faces substantial challenges from noisy labels\nattributed to factors like inter-expert variability and machine-extracted\nlabels. Despite this, the adoption of label noise management remains limited,\nand label noise is largely ignored. To this end, there is a critical need to\nconduct a scoping review focusing on the problem space. This scoping review\naims to comprehensively review label noise management in deep learning-based\nmedical prediction problems, which includes label noise detection, label noise\nhandling, and evaluation. Research involving label uncertainty is also\nincluded.\n  Methods: Our scoping review follows the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses (PRISMA) guidelines. We searched 4\ndatabases, including PubMed, IEEE Xplore, Google Scholar, and Semantic Scholar.\nOur search terms include ""noisy label AND medical / healthcare / clinical"",\n""un-certainty AND medical / healthcare / clinical"", and ""noise AND medical /\nhealthcare / clinical"".\n  Results: A total of 60 papers met inclusion criteria between 2016 and 2023. A\nseries of practical questions in medical research are investigated. These\ninclude the sources of label noise, the impact of label noise, the detection of\nlabel noise, label noise handling techniques, and their evaluation.\nCategorization of both label noise detection methods and handling techniques\nare provided.\n  Discussion: From a methodological perspective, we observe that the medical\ncommunity has been up to date with the broader deep-learning community, given\nthat most techniques have been evaluated on medical data. We recommend\nconsidering label noise as a standard element in medical research, even if it\nis not dedicated to handling noisy labels. Initial experiments can start with\neasy-to-implement methods, such as noise-robust loss functions, weighting, and\ncurriculum learning.\n', '  Black-box deep learning approaches have showcased significant potential in\nthe realm of medical image analysis. However, the stringent trustworthiness\nrequirements intrinsic to the medical field have catalyzed research into the\nutilization of Explainable Artificial Intelligence (XAI), with a particular\nfocus on concept-based methods. Existing concept-based methods predominantly\napply concept annotations from a single perspective (e.g., global level),\nneglecting the nuanced semantic relationships between sub-regions and concepts\nembedded within medical images. This leads to underutilization of the valuable\nmedical information and may cause models to fall short in harmoniously\nbalancing interpretability and performance when employing inherently\ninterpretable architectures such as Concept Bottlenecks. To mitigate these\nshortcomings, we propose a multi-modal explainable disease diagnosis framework\nthat meticulously aligns medical images and clinical-related concepts\nsemantically at multiple strata, encompassing the image level, token level, and\nconcept level. Moreover, our method allows for model intervention and offers\nboth textual and visual explanations in terms of human-interpretable concepts.\nExperimental results on three skin image datasets demonstrate that our method,\nwhile preserving model interpretability, attains high performance and label\nefficiency for concept detection and disease diagnosis.\n']",Medical Image Analysis with Deep Learning,Deep Learning for Medical Imaging and Analysis,Deep Learning for Medical Imaging and Analysis
73,73,87,73_normalization_regularization_neural_generalization,"['normalization', 'regularization', 'neural', 'generalization', 'backpropagation', 'convolutional', 'neuron', 'learning', 'networks', 'deep']","['normalization', 'weight', 'networks', 'neural', 'deep', 'gradient', 'network', 'backpropagation', 'loss', 'batch']","['  Batch Normalization (BN), a widely-used technique in neural networks,\nenhances generalization and expedites training by normalizing each mini-batch\nto the same mean and variance. However, its effectiveness diminishes when\nconfronted with diverse data distributions. To address this challenge, we\npropose Supervised Batch Normalization (SBN), a pioneering approach. We expand\nnormalization beyond traditional single mean and variance parameters, enabling\nthe identification of data modes prior to training. This ensures effective\nnormalization for samples sharing common features. We define contexts as modes,\ncategorizing data with similar characteristics. These contexts are explicitly\ndefined, such as domains in domain adaptation or modalities in multimodal\nsystems, or implicitly defined through clustering algorithms based on data\nsimilarity. We illustrate the superiority of our approach over BN and other\ncommonly employed normalization techniques through various experiments on both\nsingle and multi-task datasets. Integrating SBN with Vision Transformer results\nin a remarkable \\textit{15.13}\\% accuracy enhancement on CIFAR-100.\nAdditionally, in domain adaptation scenarios, employing AdaMatch demonstrates\nan impressive \\textit{22.25}\\% accuracy improvement on MNIST and SVHN compared\nto BN.\n', ""  Normalization is a pre-processing step that converts the data into a more\nusable representation. As part of the deep neural networks (DNNs), the batch\nnormalization (BN) technique uses normalization to address the problem of\ninternal covariate shift. It can be packaged as general modules, which have\nbeen extensively integrated into various DNNs, to stabilize and accelerate\ntraining, presumably leading to improved generalization. However, the effect of\nBN is dependent on the mini-batch size and it does not take into account any\ngroups or clusters that may exist in the dataset when estimating population\nstatistics. This study proposes a new normalization technique, called context\nnormalization, for image data. This approach adjusts the scaling of features\nbased on the characteristics of each sample, which improves the model's\nconvergence speed and performance by adapting the data values to the context of\nthe target task. The effectiveness of context normalization is demonstrated on\nvarious datasets, and its performance is compared to other standard\nnormalization techniques.\n"", '  Deep learning grapples with challenges in training neural networks, notably\ninternal covariate shift and label shift. Conventional normalization techniques\nlike Batch Normalization (BN) partially mitigate these issues but are hindered\nby constraints such as dependency on batch size and distribution assumptions.\nSimilarly, mixture normalization (MN) encounters computational barriers in\nhandling diverse Gaussian distributions. This paper introduces Cluster-based\nNormalization (CB-Norm), presenting two variants: Supervised Cluster-based\nNormalization (SCB-Norm) and Unsupervised Cluster-based Normalization\n(UCB-Norm), offering a pioneering single-step normalization strategy. CB-Norm\nemploys a Gaussian mixture model to address gradient stability and learning\nacceleration challenges. SCB-Norm utilizes predefined data partitioning, termed\nclusters, for supervised normalization, while UCB-Norm adaptively clusters\nneuron activations during training, eliminating reliance on predefined\npartitions. This approach simultaneously tackles clustering and resolution\ntasks within neural networks, reducing computational complexity compared to\nexisting methods. CB-Norm outperforms traditional techniques like BN and MN,\nenhancing neural network performance across diverse learning scenarios.\n']",Normalization Techniques for Deep Neural Networks,Deep Learning Optimization Techniques,Deep Learning Optimization and Training
74,74,87,74_classifiers_learning_classification_classifier,"['classifiers', 'learning', 'classification', 'classifier', 'generalization', 'regularization', 'optimal', 'memorization', 'minimax', 'distributionally']","['loss', 'distribution', 'risk', 'error', 'convex', 'bounds', 'classifier', 'losses', 'empirical', 'generalization']","[""  This paper introduces General Distribution Learning (GD learning), a novel\ntheoretical learning framework designed to address a comprehensive range of\nmachine learning and statistical tasks, including classification, regression,\nand parameter estimation. GD learning focuses on estimating the true underlying\nprobability distribution of dataset and using models to fit the estimated\nparameters of the distribution. The learning error in GD learning is thus\ndecomposed into two distinct categories: estimation error and fitting error.\nThe estimation error, which stems from the constraints of finite sampling,\nlimited prior knowledge, and the estimation algorithm's inherent limitations,\nquantifies the discrepancy between the true distribution and its estimate. The\nfitting error can be attributed to model's capacity limitation and the\nperformance limitation of the optimization algorithm, which evaluates the\ndeviation of the model output from the fitted objective. To address the\nchallenge of non-convexity in the optimization of learning error, we introduce\nthe standard loss function and demonstrate that, when employing this function,\nglobal optimal solutions in non-convex optimization can be approached by\nminimizing the gradient norm and the structural error. Moreover, we demonstrate\nthat the estimation error is determined by the uncertainty of the estimate $q$,\nand propose the minimum uncertainty principle to obtain an optimal estimate of\nthe true distribution. We further provide upper bounds for the estimation\nerror, fitting error, and learning error within the GD learning framework.\nUltimately, our findings are applied to offer theoretical explanations for\nseveral unanswered questions on deep learning, including overparameterization,\nnon-convex optimization, flat minima, dynamic isometry condition and other\ntechniques in deep learning.\n"", '  Deep neural networks (DNNs) trained with the logistic loss (i.e., the cross\nentropy loss) have made impressive advancements in various binary\nclassification tasks. However, generalization analysis for binary\nclassification with DNNs and logistic loss remains scarce. The unboundedness of\nthe target function for the logistic loss is the main obstacle to deriving\nsatisfactory generalization bounds. In this paper, we aim to fill this gap by\nestablishing a novel and elegant oracle-type inequality, which enables us to\ndeal with the boundedness restriction of the target function, and using it to\nderive sharp convergence rates for fully connected ReLU DNN classifiers trained\nwith logistic loss. In particular, we obtain optimal convergence rates (up to\nlog factors) only requiring the H\\""older smoothness of the conditional class\nprobability $\\eta$ of data. Moreover, we consider a compositional assumption\nthat requires $\\eta$ to be the composition of several vector-valued functions\nof which each component function is either a maximum value function or a\nH\\""older smooth function only depending on a small number of its input\nvariables. Under this assumption, we derive optimal convergence rates (up to\nlog factors) which are independent of the input dimension of data. This result\nexplains why DNN classifiers can perform well in practical high-dimensional\nclassification problems. Besides the novel oracle-type inequality, the sharp\nconvergence rates given in our paper also owe to a tight error bound for\napproximating the natural logarithm function near zero (where it is unbounded)\nby ReLU DNNs. In addition, we justify our claims for the optimality of rates by\nproving corresponding minimax lower bounds. All these results are new in the\nliterature and will deepen our theoretical understanding of classification with\nDNNs.\n', ""  Transfer learning, or domain adaptation, is concerned with machine learning\nproblems in which training and testing data come from possibly different\nprobability distributions. In this work, we give an information-theoretic\nanalysis of the generalization error and excess risk of transfer learning\nalgorithms. Our results suggest, perhaps as expected, that the Kullback-Leibler\n(KL) divergence $D(\\mu\\|\\mu')$ plays an important role in the characterizations\nwhere $\\mu$ and $\\mu'$ denote the distribution of the training data and the\ntesting data, respectively. Specifically, we provide generalization error and\nexcess risk upper bounds for learning algorithms where data from both\ndistributions are available in the training phase. Recognizing that the bounds\ncould be sub-optimal in general, we provide improved excess risk upper bounds\nfor a certain class of algorithms, including the empirical risk minimization\n(ERM) algorithm, by making stronger assumptions through the \\textit{central\ncondition}. To demonstrate the usefulness of the bounds, we further extend the\nanalysis to the Gibbs algorithm and the noisy stochastic gradient descent\nmethod. We then generalize the mutual information bound with other divergences\nsuch as $\\phi$-divergence and Wasserstein distance, which may lead to tighter\nbounds and can handle the case when $\\mu$ is not absolutely continuous with\nrespect to $\\mu'$. Several numerical results are provided to demonstrate our\ntheoretical findings. Lastly, to address the problem that the bounds are often\nnot directly applicable in practice due to the absence of the distributional\nknowledge of the data, we develop an algorithm (called InfoBoost) that\ndynamically adjusts the importance weights for both source and target data\nbased on certain information measures. The empirical results show the\neffectiveness of the proposed algorithm.\n""]",Machine Learning Theory and Generalization,Machine Learning Foundations and Generalization,Machine Learning and Artificial Intelligence
75,75,87,75_planning_robotics_planner_robotic,"['planning', 'robotics', 'planner', 'robotic', 'robot', 'planners', 'robots', 'motions', 'motion', 'trajectory']","['planning', 'diffusion', 'trajectory', 'motion', 'trajectories', 'collision', 'robot', 'planners', 'path', 'policy']","['  Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feedback through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain-specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nunderscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings.\n', '  To control how a robot moves, motion planning algorithms must compute paths\nin high-dimensional state spaces while accounting for physical constraints\nrelated to motors and joints, generating smooth and stable motions, avoiding\nobstacles, and preventing collisions. A motion planning algorithm must\ntherefore balance competing demands, and should ideally incorporate uncertainty\nto handle noise, model errors, and facilitate deployment in complex\nenvironments. To address these issues, we introduce a framework for robot\nmotion planning based on variational Gaussian processes, which unifies and\ngeneralizes various probabilistic-inference-based motion planning algorithms,\nand connects them with optimization-based planners. Our framework provides a\nprincipled and flexible way to incorporate equality-based, inequality-based,\nand soft motion-planning constraints during end-to-end training, is\nstraightforward to implement, and provides both interval-based and\nMonte-Carlo-based uncertainty estimates. We conduct experiments using different\nenvironments and robots, comparing against baseline approaches based on the\nfeasibility of the planned paths, and obstacle avoidance quality. Results show\nthat our proposed approach yields a good balance between success rates and path\nquality.\n', '  Learning priors on trajectory distributions can help accelerate robot motion\nplanning optimization. Given previously successful plans, learning trajectory\ngenerative models as priors for a new planning problem is highly desirable.\nPrior works propose several ways on utilizing this prior to bootstrapping the\nmotion planning problem. Either sampling the prior for initializations or using\nthe prior distribution in a maximum-a-posterior formulation for trajectory\noptimization. In this work, we propose learning diffusion models as priors. We\nthen can sample directly from the posterior trajectory distribution conditioned\non task goals, by leveraging the inverse denoising process of diffusion models.\nFurthermore, diffusion has been recently shown to effectively encode data\nmultimodality in high-dimensional settings, which is particularly well-suited\nfor large trajectory dataset. To demonstrate our method efficacy, we compare\nour proposed method - Motion Planning Diffusion - against several baselines in\nsimulated planar robot and 7-dof robot arm manipulator environments. To assess\nthe generalization capabilities of our method, we test it in environments with\npreviously unseen obstacles. Our experiments show that diffusion models are\nstrong priors to encode high-dimensional trajectory distributions of robot\nmotions.\n']",Robot Motion Planning and Task Planning,Robotics and Artificial Intelligence,Robotics and Artificial Intelligence
76,76,86,76_reservoir_deep_flow_flows,"['reservoir', 'deep', 'flow', 'flows', 'predicting', 'modeling', 'predict', 'simulations', 'neural', 'prediction']","['physics', 'simulations', 'flow', 'wave', 'fluid', 'physical', 'fidelity', 'neural', 'reservoir', 'grid']","[""  We developed a novel reservoir characterization workflow that addresses\nreservoir history matching by coupling a physics-informed neural operator\n(PINO) forward model with a mixture of experts' approach, termed cluster\nclassify regress (CCR). The inverse modelling is achieved via an adaptive\nRegularized Ensemble Kalman inversion (aREKI) method, ideal for rapid inverse\nuncertainty quantification during history matching. We parametrize unknown\npermeability and porosity fields for non-Gaussian posterior measures using a\nvariational convolution autoencoder and a denoising diffusion implicit model\n(DDIM) exotic priors. The CCR works as a supervised model with the PINO\nsurrogate to replicate nonlinear Peaceman well equations. The CCR's flexibility\nallows any independent machine-learning algorithm for each stage. The PINO\nreservoir surrogate's loss function is derived from supervised data loss and\nlosses from the initial conditions and residual of the governing black oil PDE.\nThe PINO-CCR surrogate outputs pressure, water, and gas saturations, along with\noil, water, and gas production rates. The methodology was compared to a\nstandard numerical black oil simulator for a waterflooding case on the Norne\nfield, showing similar outputs. This PINO-CCR surrogate was then used in the\naREKI history matching workflow, successfully recovering the unknown\npermeability, porosity and fault multiplier, with simulations up to 6000 times\nfaster than conventional methods. Training the PINO-CCR surrogate on an NVIDIA\nH100 with 80G memory takes about 5 hours for 100 samples of the Norne field.\nThis workflow is suitable for ensemble-based approaches, where posterior\ndensity sampling, given an expensive likelihood evaluation, is desirable for\nuncertainty quantification.\n"", '  Accurately predicting the long-term behavior of chaotic systems is crucial\nfor various applications such as climate modeling. However, achieving such\npredictions typically requires iterative computations over a dense\nspatiotemporal grid to account for the unstable nature of chaotic systems,\nwhich is expensive and impractical in many real-world situations. An\nalternative approach to such a full-resolved simulation is using a coarse grid\nand then correcting its errors through a \\textit{closure model}, which\napproximates the overall information from fine scales not captured in the\ncoarse-grid simulation. Recently, ML approaches have been used for closure\nmodeling, but they typically require a large number of training samples from\nexpensive fully-resolved simulations (FRS). In this work, we prove an even more\nfundamental limitation, i.e., the standard approach to learning closure models\nsuffers from a large approximation error for generic problems, no matter how\nlarge the model is, and it stems from the non-uniqueness of the mapping. We\npropose an alternative end-to-end learning approach using a physics-informed\nneural operator (PINO) that overcomes this limitation by not using a closure\nmodel or a coarse-grid solver. We first train the PINO model on data from a\ncoarse-grid solver and then fine-tune it with (a small amount of) FRS and\nphysics-based losses on a fine grid. The discretization-free nature of neural\noperators means that they do not suffer from the restriction of a coarse grid\nthat closure models face, and they can provably approximate the long-term\nstatistics of chaotic systems. In our experiments, our PINO model achieves a\n120x speedup compared to FRS with a relative error $\\sim 5\\%$. In contrast, the\nclosure model coupled with a coarse-grid solver is $58$x slower than PINO while\nhaving a much higher error $\\sim205\\%$ when the closure model is trained on the\nsame FRS dataset.\n', ""  High-resolution reconstruction of flow-field data from low-resolution and\nnoisy measurements is of interest due to the prevalence of such problems in\nexperimental fluid mechanics, where the measurement data are in general sparse,\nincomplete and noisy. Deep-learning approaches have been shown suitable for\nsuch super-resolution tasks. However, a high number of high-resolution examples\nis needed, which may not be available for many cases. Moreover, the obtained\npredictions may lack in complying with the physical principles, e.g. mass and\nmomentum conservation. Physics-informed deep learning provides frameworks for\nintegrating data and physical laws for learning. In this study, we apply\nphysics-informed neural networks (PINNs) for super-resolution of flow-field\ndata both in time and space from a limited set of noisy measurements without\nhaving any high-resolution reference data. Our objective is to obtain a\ncontinuous solution of the problem, providing a physically-consistent\nprediction at any point in the solution domain. We demonstrate the\napplicability of PINNs for the super-resolution of flow-field data in time and\nspace through three canonical cases: Burgers' equation, two-dimensional vortex\nshedding behind a circular cylinder and the minimal turbulent channel flow. The\nrobustness of the models is also investigated by adding synthetic Gaussian\nnoise. Furthermore, we show the capabilities of PINNs to improve the resolution\nand reduce the noise in a real experimental dataset consisting of\nhot-wire-anemometry measurements. Our results show the adequate capabilities of\nPINNs in the context of data augmentation for experiments in fluid mechanics.\n""]",Physics-Informed Neural Networks for Fluid Flow Modeling,Physics-Informed Machine Learning for Differential Equations and Fluid Dynamics,Machine Learning for Dynamical Systems and Differential Equations
77,77,86,77_saliency_neural_attention_recognition,"['saliency', 'neural', 'attention', 'recognition', 'visual', 'attentions', 'vision', 'representations', 'images', 'perception']","['object', 'objects', 'attention', 'visual', 'vision', 'image', 'saliency', 'class', 'classes', 'concepts']","[""  Advances in multi-modal embeddings, and in particular CLIP, have recently\ndriven several breakthroughs in Computer Vision (CV). CLIP has shown impressive\nperformance on a variety of tasks, yet, its inherently opaque architecture may\nhinder the application of models employing CLIP as backbone, especially in\nfields where trust and model explainability are imperative, such as in the\nmedical domain. Current explanation methodologies for CV models rely on\nSaliency Maps computed through gradient analysis or input perturbation.\nHowever, these Saliency Maps can only be computed to explain classes relevant\nto the end task, often smaller in scope than the backbone training classes. In\nthe context of models implementing CLIP as their vision backbone, a substantial\nportion of the information embedded within the learned representations is thus\nleft unexplained.\n  In this work, we propose Concept Visualization (ConVis), a novel saliency\nmethodology that explains the CLIP embedding of an image by exploiting the\nmulti-modal nature of the embeddings. ConVis makes use of lexical information\nfrom WordNet to compute task-agnostic Saliency Maps for any concept, not\nlimited to concepts the end model was trained on. We validate our use of\nWordNet via an out of distribution detection experiment, and test ConVis on an\nobject localization benchmark, showing that Concept Visualizations correctly\nidentify and localize the image's semantic content. Additionally, we perform a\nuser study demonstrating that our methodology can give users insight on the\nmodel's functioning.\n"", '  Deep learning algorithms lack human-interpretable accounts of how they\ntransform raw visual input into a robust semantic understanding, which impedes\ncomparisons between different architectures, training objectives, and the human\nbrain. In this work, we take inspiration from neuroscience and employ\nrepresentational approaches to shed light on how neural networks encode\ninformation at low (visual saliency) and high (semantic similarity) levels of\nabstraction. Moreover, we introduce a custom image dataset where we\nsystematically manipulate salient and semantic information. We find that\nResNets are more sensitive to saliency information than ViTs, when trained with\nobject classification objectives. We uncover that networks suppress saliency in\nearly layers, a process enhanced by natural language supervision (CLIP) in\nResNets. CLIP also enhances semantic encoding in both architectures. Finally,\nwe show that semantic encoding is a key factor in aligning AI with human visual\nperception, while saliency suppression is a non-brain-like strategy.\n', '  Humans judge the similarity of two objects not just based on their visual\nappearance but also based on their semantic relatedness. However, it remains\nunclear how humans learn about semantic relationships between objects and\ncategories. One important source of semantic knowledge is that semantically\nrelated objects frequently co-occur in the same context. For instance, forks\nand plates are perceived as similar, at least in part, because they are often\nexperienced together in a ``kitchen"" or ``eating\'\' context. Here, we\ninvestigate whether a bio-inspired learning principle exploiting such\nco-occurrence statistics suffices to learn a semantically structured object\nrepresentation {\\em de novo} from raw visual or combined visual and linguistic\ninput. To this end, we simulate temporal sequences of visual experience by\nbinding together short video clips of real-world scenes showing objects in\ndifferent contexts. A bio-inspired neural network model aligns close-in-time\nvisual representations while also aligning visual and category label\nrepresentations to simulate visuo-language alignment. Our results show that our\nmodel clusters object representations based on their context, e.g. kitchen or\nbedroom, in particular in high-level layers of the network, akin to humans. In\ncontrast, lower-level layers tend to better reflect object identity or\ncategory. To achieve this, the model exploits two distinct strategies: the\nvisuo-language alignment ensures that different objects of the same category\nare represented similarly, whereas the temporal alignment leverages that\nobjects from the same context are frequently seen in succession to make their\nrepresentations more similar. Overall, our work suggests temporal and\nvisuo-language alignment as plausible computational principles for explaining\nthe origins of certain forms of semantic knowledge in humans.\n']",Visual Saliency and Representation Learning,Explainable AI and Machine Learning,Artificial Intelligence and Machine Learning Interpretability and Explainability
78,78,86,78_stochastic_brownian_discretization_diffusion,"['stochastic', 'brownian', 'discretization', 'diffusion', 'gaussian', 'sde', 'drift', 'probabilistic', 'sdes', 'mcmc']","['equation', 'stochastic', 'normalizing', 'equations', 'flows', 'density', 'differential', 'diffusion', 'posterior', 'distribution']","[""  Recently, Gaussian processes have been used to model the vector field of\ncontinuous dynamical systems, referred to as GPODEs, which are characterized by\na probabilistic ODE equation. Bayesian inference for these models has been\nextensively studied and applied in tasks such as time series prediction.\nHowever, the use of standard GPs with basic kernels like squared exponential\nkernels has been common in GPODE research, limiting the model's ability to\nrepresent complex scenarios. To address this limitation, we introduce\nnormalizing flows to reparameterize the ODE vector field, resulting in a\ndata-driven prior distribution, thereby increasing flexibility and expressive\npower. We develop a data-driven variational learning algorithm that utilizes\nanalytically tractable probability density functions of normalizing flows,\nenabling simultaneous learning and inference of unknown continuous dynamics.\nAdditionally, we also apply normalizing flows to the posterior inference of GP\nODEs to resolve the issue of strong mean-field assumptions in posterior\ninference. By applying normalizing flows in both these ways, our model improves\naccuracy and uncertainty estimates for Bayesian Gaussian Process ODEs. We\nvalidate the effectiveness of our approach on simulated dynamical systems and\nreal-world human motion data, including time series prediction and missing data\nrecovery tasks. Experimental results show that our proposed method effectively\ncaptures model uncertainty while improving accuracy.\n"", ""  The Fokker-Planck (FP) equation is a foundational PDE in stochastic\nprocesses. However, curse of dimensionality (CoD) poses challenge when dealing\nwith high-dimensional FP PDEs. Although Monte Carlo and vanilla\nPhysics-Informed Neural Networks (PINNs) have shown the potential to tackle\nCoD, both methods exhibit numerical errors in high dimensions when dealing with\nthe probability density function (PDF) associated with Brownian motion. The\npoint-wise PDF values tend to decrease exponentially as dimension increases,\nsurpassing the precision of numerical simulations and resulting in substantial\nerrors. Moreover, due to its massive sampling, Monte Carlo fails to offer fast\nsampling. Modeling the logarithm likelihood (LL) via vanilla PINNs transforms\nthe FP equation into a difficult HJB equation, whose error grows rapidly with\ndimension. To this end, we propose a novel approach utilizing a score-based\nsolver to fit the score function in SDEs. The score function, defined as the\ngradient of the LL, plays a fundamental role in inferring LL and PDF and\nenables fast SDE sampling. Three fitting methods, Score Matching (SM), Sliced\nSM (SSM), and Score-PINN, are introduced. The proposed score-based SDE solver\noperates in two stages: first, employing SM, SSM, or Score-PINN to acquire the\nscore; and second, solving the LL via an ODE using the obtained score.\nComparative evaluations across these methods showcase varying trade-offs. The\nproposed method is evaluated across diverse SDEs, including anisotropic OU\nprocesses, geometric Brownian, and Brownian with varying eigenspace. We also\ntest various distributions, including Gaussian, Log-normal, Laplace, and\nCauchy. The numerical results demonstrate the score-based SDE solver's\nstability, speed, and performance across different settings, solidifying its\npotential as a solution to CoD for high-dimensional FP equations.\n"", ""  With the rapid increase of observational, experimental and simulated data for\nstochastic systems, tremendous efforts have been devoted to identifying\ngoverning laws underlying the evolution of these systems. Despite the broad\napplications of non-Gaussian fluctuations in numerous physical phenomena, the\ndata-driven approaches to extracting stochastic dynamics with L\\'{e}vy noise\nare relatively few. In this work, we propose a Weak Collocation Regression\n(WCR) to explicitly reveal unknown stochastic dynamical systems, i.e., the\nStochastic Differential Equation (SDE) with both $\\alpha$-stable L\\'{e}vy noise\nand Gaussian noise, from discrete aggregate data. This method utilizes the\nevolution equation of the probability distribution function, i.e., the\nFokker-Planck (FP) equation. With the weak form of the FP equation, the WCR\nconstructs a linear system of unknown parameters where all integrals are\nevaluated by Monte Carlo method with the observations. Then, the unknown\nparameters are obtained by a sparse linear regression. For a SDE with L\\'{e}vy\nnoise, the corresponding FP equation is a partial integro-differential equation\n(PIDE), which contains nonlocal terms, and is difficult to deal with. The weak\nform can avoid complicated multiple integrals. Our approach can simultaneously\ndistinguish mixed noise types, even in multi-dimensional problems. Numerical\nexperiments demonstrate that our method is accurate and computationally\nefficient.\n""]",Stochastic Differential Equations Modeling,Stochastic Methods for Sampling and Dynamics,Probabilistic Methods and Stochastic Processes
79,79,85,79_hippocampal_planning_hippocampus_exploration,"['hippocampal', 'planning', 'hippocampus', 'exploration', 'cortex', 'cognitive', 'environments', 'reinforcement', 'behaviors', 'adaptive']","['active', 'environment', 'navigation', 'reinforcement', 'swarm', 'robots', 'place', 'animals', 'maps', 'behaviour']","[""  The vertebrate hippocampus is believed to use recurrent connectivity in area\nCA3 to support episodic memory recall from partial cues. This brain area also\ncontains place cells, whose location-selective firing fields implement maps\nsupporting spatial memory. Here we show that place cells emerge in networks\ntrained to remember temporally continuous sensory episodes. We model CA3 as a\nrecurrent autoencoder that recalls and reconstructs sensory experiences from\nnoisy and partially occluded observations by agents traversing simulated rooms.\nThe agents move in realistic trajectories modeled from rodents and environments\nare modeled as high-dimensional sensory experience maps. Training our\nautoencoder to pattern-complete and reconstruct experiences with a constraint\non total activity causes spatially localized firing fields, i.e., place cells,\nto emerge in the encoding layer. The emergent place fields reproduce key\naspects of hippocampal phenomenology: a) remapping (maintenance of and\nreversion to distinct learned maps in different environments), implemented via\nrepositioning of experience manifolds in the network's hidden layer, b)\northogonality of spatial representations in different arenas, c) robust place\nfield emergence in differently shaped rooms, with single units showing multiple\nplace fields in large or complex spaces, and d) slow representational drift of\nplace fields. We argue that these results arise because continuous traversal of\nspace makes sensory experience temporally continuous. We make testable\npredictions: a) rapidly changing sensory context will disrupt place fields, b)\nplace fields will form even if recurrent connections are blocked, but reversion\nto previously learned representations upon remapping will be abolished, c) the\ndimension of temporally smooth experience sets the dimensionality of place\nfields, including during virtual navigation of abstract spaces.\n"", ""  Drawing inspiration from animal navigation strategies, we introduce a novel\ncomputational model for navigation and mapping, rooted in biologically inspired\nprinciples. Animals exhibit remarkable navigation abilities by efficiently\nusing memory, imagination, and strategic decision-making to navigate complex\nand aliased environments. Building on these insights, we integrate traditional\ncognitive mapping approaches with an Active Inference Framework (AIF) to learn\nan environment structure in a few steps. Through the incorporation of\ntopological mapping for long-term memory and AIF for navigation planning and\nstructure learning, our model can dynamically apprehend environmental\nstructures and expand its internal map with predicted beliefs during\nexploration. Comparative experiments with the Clone-Structured Graph (CSCG)\nmodel highlight our model's ability to rapidly learn environmental structures\nin a single episode, with minimal navigation overlap. this is achieved without\nprior knowledge of the dimensions of the environment or the type of\nobservations, showcasing its robustness and effectiveness in navigating\nambiguous environments.\n"", '  By dynamic planning, we refer to the ability of the human brain to infer and\nimpose motor trajectories related to cognitive decisions. A recent paradigm,\nactive inference, brings fundamental insights into the adaptation of biological\norganisms, constantly striving to minimize prediction errors to restrict\nthemselves to life-compatible states. Over the past years, many studies have\nshown how human and animal behavior could be explained in terms of an active\ninferential process - either as discrete decision-making or continuous motor\ncontrol - inspiring innovative solutions in robotics and artificial\nintelligence. Still, the literature lacks a comprehensive outlook on how to\neffectively plan actions in changing environments. Setting ourselves the goal\nof modeling tool use, we delve into the topic of dynamic planning in active\ninference, keeping in mind two crucial aspects of biological goal-directed\nbehavior: the capacity to understand and exploit affordances for object\nmanipulation, and to learn the hierarchical interactions between the self and\nthe environment, including other agents. We start from a simple unit and\ngradually describe more advanced structures, comparing recently proposed design\nchoices and providing basic examples for each section. This study distances\nitself from traditional views centered on neural networks and reinforcement\nlearning, and points toward a yet unexplored direction in active inference:\nhybrid representations in hierarchical models.\n']",Hippocampal-inspired models for navigation and planning,Robot Navigation and Locomotion,Robotics and Artificial Intelligence
80,80,85,80_reinforcement_objectives_optimal_critic,"['reinforcement', 'objectives', 'optimal', 'critic', 'learning', 'reward', 'objective', 'rewards', 'optimization', 'control']","['policy', 'reinforcement', 'actor', 'policies', 'objective', 'value', 'critic', 'learning', 'optimal', 'control']","['  Multi-objective reinforcement learning (MORL) is essential for addressing the\nintricacies of real-world RL problems, which often require trade-offs between\nmultiple utility functions. However, MORL is challenging due to unstable\nlearning dynamics with deep learning-based function approximators. The research\npath most taken has been to explore different value-based loss functions for\nMORL to overcome this issue. Our work empirically explores model-free policy\nlearning loss functions and the impact of different architectural choices. We\nintroduce two different approaches: Multi-objective Proximal Policy\nOptimization (MOPPO), which extends PPO to MORL, and Multi-objective Advantage\nActor Critic (MOA2C), which acts as a simple baseline in our ablations. Our\nproposed approach is straightforward to implement, requiring only small\nmodifications at the level of function approximator. We conduct comprehensive\nevaluations on the MORL Deep Sea Treasure, Minecart, and Reacher environments\nand show that MOPPO effectively captures the Pareto front. Our extensive\nablation studies and empirical analyses reveal the impact of different\narchitectural choices, underscoring the robustness and versatility of MOPPO\ncompared to popular MORL approaches like Pareto Conditioned Networks (PCN) and\nEnvelope Q-learning in terms of MORL metrics, including hypervolume and\nexpected utility.\n', '  Many real-world continuous control problems are in the dilemma of weighing\nthe pros and cons, multi-objective reinforcement learning (MORL) serves as a\ngeneric framework of learning control policies for different preferences over\nobjectives. However, the existing MORL methods either rely on multiple passes\nof explicit search for finding the Pareto front and therefore are not\nsample-efficient, or utilizes a shared policy network for coarse knowledge\nsharing among policies. To boost the sample efficiency of MORL, we propose\nQ-Pensieve, a policy improvement scheme that stores a collection of Q-snapshots\nto jointly determine the policy update direction and thereby enables data\nsharing at the policy level. We show that Q-Pensieve can be naturally\nintegrated with soft policy iteration with convergence guarantee. To\nsubstantiate this concept, we propose the technique of Q replay buffer, which\nstores the learned Q-networks from the past iterations, and arrive at a\npractical actor-critic implementation. Through extensive experiments and an\nablation study, we demonstrate that with much fewer samples, the proposed\nalgorithm can outperform the benchmark MORL methods on a variety of MORL\nbenchmark tasks.\n', '  For a control problem with multiple conflicting objectives, there exists a\nset of Pareto-optimal policies called the Pareto set instead of a single\noptimal policy. When a multi-objective control problem is continuous and\ncomplex, traditional multi-objective reinforcement learning (MORL) algorithms\nsearch for many Pareto-optimal deep policies to approximate the Pareto set,\nwhich is quite resource-consuming. In this paper, we propose a simple and\nresource-efficient MORL algorithm that learns a continuous representation of\nthe Pareto set in a high-dimensional policy parameter space using a single\nhypernet. The learned hypernet can directly generate various well-trained\npolicy networks for different user preferences. We compare our method with two\nstate-of-the-art MORL algorithms on seven multi-objective continuous robot\ncontrol problems. Experimental results show that our method achieves the best\noverall performance with the least training parameters. An interesting\nobservation is that the Pareto set is well approximated by a curved line or\nsurface in a high-dimensional parameter space. This observation will provide\ninsight for researchers to design new MORL algorithms.\n']",Multi-Objective Reinforcement Learning,Multi-Agent Decision Making and Reinforcement Learning,Multi-Agent Systems and Artificial Intelligence
81,81,83,81_textual_nlp_summarization_linguistic,"['textual', 'nlp', 'summarization', 'linguistic', 'texts', 'evaluation', 'text', 'generated', 'language', 'lingual']","['evaluation', 'prompt', 'language', 'metrics', 'prompts', 'writing', 'multilingual', 'generation', 'instruction', 'texts']","['  Instruction-tuned Large Language Models (LLMs) have recently showcased\nremarkable advancements in their ability to generate fitting responses to\nnatural language instructions. However, many current works rely on manual\nevaluation to judge the quality of generated responses. Since such manual\nevaluation is time-consuming, it does not easily scale to the evaluation of\nmultiple models and model variants. In this short paper, we propose a\nstraightforward but remarkably effective evaluation metric called SemScore, in\nwhich we directly compare model outputs to gold target responses using semantic\ntextual similarity (STS). We conduct a comparative evaluation of the model\noutputs of 12 prominent instruction-tuned LLMs using 8 widely-used evaluation\nmetrics for text generation. We find that our proposed SemScore metric\noutperforms all other, in many cases more complex, evaluation metrics in terms\nof correlation to human evaluation. These findings indicate the utility of our\nproposed metric for the evaluation of instruction-tuned LLMs.\n', ""  Natural Language Processing (NLP) is witnessing a remarkable breakthrough\ndriven by the success of Large Language Models (LLMs). LLMs have gained\nsignificant attention across academia and industry for their versatile\napplications in text generation, question answering, and text summarization. As\nthe landscape of NLP evolves with an increasing number of domain-specific LLMs\nemploying diverse techniques and trained on various corpus, evaluating\nperformance of these models becomes paramount. To quantify the performance,\nit's crucial to have a comprehensive grasp of existing metrics. Among the\nevaluation, metrics which quantifying the performance of LLMs play a pivotal\nrole. This paper offers a comprehensive exploration of LLM evaluation from a\nmetrics perspective, providing insights into the selection and interpretation\nof metrics currently in use. Our main goal is to elucidate their mathematical\nformulations and statistical interpretations. We shed light on the application\nof these metrics using recent Biomedical LLMs. Additionally, we offer a\nsuccinct comparison of these metrics, aiding researchers in selecting\nappropriate metrics for diverse tasks. The overarching goal is to furnish\nresearchers with a pragmatic guide for effective LLM evaluation and metric\nselection, thereby advancing the understanding and application of these large\nlanguage models.\n"", '  Automatic evaluation of generated textual content presents an ongoing\nchallenge within the field of NLP. Given the impressive capabilities of modern\nlanguage models (LMs) across diverse NLP tasks, there is a growing trend to\nemploy these models in creating innovative evaluation metrics for automated\nassessment of generation tasks. This paper investigates a pivotal question: Do\nlanguage model-driven evaluation metrics inherently exhibit bias favoring texts\ngenerated by the same underlying language model? Specifically, we assess\nwhether prominent LM-based evaluation metrics (e.g. BARTScore, T5Score, and\nGPTScore) demonstrate a favorable bias toward their respective underlying LMs\nin the context of summarization tasks. Our findings unveil a latent bias,\nparticularly pronounced when such evaluation metrics are used in a\nreference-free manner without leveraging gold summaries. These results\nunderscore that assessments provided by generative evaluation models can be\ninfluenced by factors beyond the inherent text quality, highlighting the\nnecessity of developing more reliable evaluation protocols in the future.\n']",Evaluating Large Language Models for NLP Tasks,Evaluating Large Language Models,Large Language Models
82,82,83,82_predicting_biomarkers_prediction_personalized,"['predicting', 'biomarkers', 'prediction', 'personalized', 'learning', 'lstm', 'biomedical', 'cancer', 'genomic', 'genome']","['drug', 'cancer', 'clinical', 'patient', 'cell', 'disease', 'treatment', 'gene', 'prediction', 'diseases']","['  The development of single-cell sequencing technology had promoted the\ngeneration of a large amount of single-cell transcriptional profiles, providing\nvaluable opportunities to explore drug-resistant cell subpopulations in a\ntumor. However, the drug sensitivity data in single-cell level is still scarce\nto date, pressing an urgent and highly challenging task for computational\nprediction of the drug sensitivity to individual cells. This paper proposed\nscAdaDrug, a multi-source adaptive weighting model to predict single-cell drug\nsensitivity. We used an autoencoder to extract domain-invariant features\nrelated to drug sensitivity from multiple source domains by exploiting\nadversarial domain adaptation. Especially, we introduced an adaptive weight\ngenerator to produce importance-aware and mutual independent weights, which\ncould adaptively modulate the embedding of each sample in dimension-level for\nboth source and target domains. Extensive experimental results showed that our\nmodel achieved state-of-the-art performance in predicting drug sensitivity on\nsinle-cell datasets, as well as on cell line and patient datasets.\n', ""  Cancer, a leading cause of death globally, occurs due to genomic changes and\nmanifests heterogeneously across patients. To advance research on personalized\ntreatment strategies, the effectiveness of various drugs on cells derived from\ncancers (`cell lines') is experimentally determined in laboratory settings.\nNevertheless, variations in the distribution of genomic data and drug responses\nbetween cell lines and humans arise due to biological and environmental\ndifferences. Moreover, while genomic profiles of many cancer patients are\nreadily available, the scarcity of corresponding drug response data limits the\nability to train machine learning models that can predict drug response in\npatients effectively. Recent cancer drug response prediction methods have\nlargely followed the paradigm of unsupervised domain-invariant representation\nlearning followed by a downstream drug response classification step.\nIntroducing supervision in both stages is challenging due to heterogeneous\npatient response to drugs and limited drug response data. This paper addresses\nthese challenges through a novel representation learning method in the first\nphase and weak supervision in the second. Experimental results on real patient\ndata demonstrate the efficacy of our method (WISER) over state-of-the-art\nalternatives on predicting personalized drug response.\n"", '  AI-driven precision oncology has the transformative potential to reshape\ncancer treatment by leveraging the power of AI models to analyze the\ninteraction between complex patient characteristics and their corresponding\ntreatment outcomes. New technological platforms have facilitated the timely\nacquisition of multimodal data on tumor biology at an unprecedented resolution,\nsuch as single-cell multi-omics data, making this quality and quantity of data\navailable for data-driven improved clinical decision-making. In this work, we\npropose a modular machine learning framework designed for personalized\ncounterfactual cancer treatment suggestions based on an ensemble of machine\nlearning experts trained on diverse multi-omics technologies. These specialized\ncounterfactual experts per technology are consistently aggregated into a more\npowerful expert with superior performance and can provide both confidence and\nan explanation of its decision. The framework is tailored to address critical\nchallenges inherent in data-driven cancer research, including the\nhigh-dimensional nature of the data, and the presence of treatment assignment\nbias in the retrospective observational data. The framework is showcased\nthrough comprehensive demonstrations using data from in-vitro and in-vivo\ntreatment responses from a cohort of patients with ovarian cancer. Our method\naims to empower clinicians with a reality-centric decision-support tool\nincluding probabilistic treatment suggestions with calibrated confidence and\npersonalized explanations for tailoring treatment strategies to multi-omics\ncharacteristics of individual cancer patients.\n']",Cancer Treatment Prediction using Genomic Data,Computational Methods for Cancer Genomics and Transcriptomics,Computational Biology and Chemistry
83,83,82,83_adversarial_privacy_security_anonymization,"['adversarial', 'privacy', 'security', 'anonymization', 'protected', 'adversary', 'secure', 'confidentiality', 'protection', 'attacks']","['attacks', 'privacy', 'security', 'adversarial', 'protection', 'face', 'anonymization', 'sensitive', 'attack', 'unlearning']","[""  The increasing prevalence of adversarial attacks on Artificial Intelligence\n(AI) systems has created a need for innovative security measures. However, the\ncurrent methods of defending against these attacks often come with a high\ncomputing cost and require back-end processing, making real-time defense\nchallenging. Fortunately, there have been remarkable advancements in\nedge-computing, which make it easier to deploy neural networks on edge devices.\nBuilding upon these advancements, we propose an edge framework design to enable\nuniversal and efficient detection of adversarial attacks. This framework\nincorporates an attention-based adversarial detection methodology and a\nlightweight detection network formation, making it suitable for a wide range of\nneural networks and can be deployed on edge devices. To assess the\neffectiveness of our proposed framework, we conducted evaluations on five\nneural networks. The results indicate an impressive 97.43% F-score can be\nachieved, demonstrating the framework's proficiency in detecting adversarial\nattacks. Moreover, our proposed framework also exhibits significantly reduced\ncomputing complexity and cost in comparison to previous detection methods. This\naspect is particularly beneficial as it ensures that the defense mechanism can\nbe efficiently implemented in real-time on-edge devices.\n"", '  As a booming research area in the past decade, deep learning technologies\nhave been driven by big data collected and processed on an unprecedented scale.\nHowever, privacy concerns arise due to the potential leakage of sensitive\ninformation from the training data. Recent research has revealed that deep\nlearning models are vulnerable to various privacy attacks, including membership\ninference attacks, attribute inference attacks, and gradient inversion attacks.\nNotably, the efficacy of these attacks varies from model to model. In this\npaper, we answer a fundamental question: Does model architecture affect model\nprivacy? By investigating representative model architectures from convolutional\nneural networks (CNNs) to Transformers, we demonstrate that Transformers\ngenerally exhibit higher vulnerability to privacy attacks than CNNs.\nAdditionally, we identify the micro design of activation layers, stem layers,\nand LN layers, as major factors contributing to the resilience of CNNs against\nprivacy attacks, while the presence of attention modules is another main factor\nthat exacerbates the privacy vulnerability of Transformers. Our discovery\nreveals valuable insights for deep learning models to defend against privacy\nattacks and inspires the research community to develop privacy-friendly model\narchitectures.\n', '  The training phase of deep neural networks requires substantial resources and\nas such is often performed on cloud servers. However, this raises privacy\nconcerns when the training dataset contains sensitive content, e.g., face\nimages. In this work, we propose a method to perform the training phase of a\ndeep learning model on both an edge device and a cloud server that prevents\nsensitive content being transmitted to the cloud while retaining the desired\ninformation. The proposed privacy-preserving method uses adversarial early\nexits to suppress the sensitive content at the edge and transmits the\ntask-relevant information to the cloud. This approach incorporates noise\naddition during the training phase to provide a differential privacy guarantee.\nWe extensively test our method on different facial datasets with diverse face\nattributes using various deep learning architectures, showcasing its\noutstanding performance. We also demonstrate the effectiveness of privacy\npreservation through successful defenses against different white-box and deep\nreconstruction attacks.\n']",Adversarial Attacks and Privacy Preservation in AI Systems,Security and Privacy in Artificial Intelligence Systems,Artificial Intelligence Applications and Implications
84,84,82,84_compression_compressed_memory_softmax,"['compression', 'compressed', 'memory', 'softmax', 'decoding', 'tokenizers', 'decoder', 'tokenization', 'tokenizer', 'attention']","['compression', 'context', 'tokens', 'long', 'language', 'tuning', 'length', 'pruning', 'token', 'tokenization']","[""  Large language models (LLMs) are increasingly deployed in real-world\nscenarios with the help of recent model compression techniques. Such momentum\ntowards local deployment means the use of compressed LLMs will widely impact a\nlarge population. However, prior analysis works often prioritize on preserving\nperplexity which is a direct analogy to training loss. The impact of\ncompression method on other critical aspects of model behavior, particularly\nsafety, still calls for a systematic assessment. To this end, we investigate\nthe impact of model compression on four dimensions: (1) degeneration harm,\ni.e., bias and toxicity in generation; (2) representational harm, i.e., biases\nin discriminative tasks; (3) dialect bias; (4) language modeling and downstream\ntask performance. We cover a wide spectrum of LLM compression techniques,\nincluding unstructured pruning, semi-structured pruning and quantization. Our\nanalysis reveals that compression can lead to unexpected consequences. Although\ncompression may unintentionally remedy LLMs' degeneration harm, it can still\nexacerbate on the representational harm axis. Although compression may\nunintentionally remedy LLMs' degeneration harm, it can still exacerbate on the\nrepresentational harm axis. Moreover, there is a divergent impact on different\nprotected groups as the compression rate grows. Finally, different compression\nmethods have drastically different safety impacts, e.g., quantization mostly\npreserves bias while pruning degrades quickly. Our findings underscore the\nimportance of integrating safety assessments into the development of compressed\nLLMs to ensure their reliability across real-world applications. Our full\nresults are available here:\n\\url{https://github.com/zhichaoxu-shufe/Beyond-Perplexity-Compression-Safety-Eval}\n"", '  We conceptualize the process of understanding as information compression, and\npropose a method for ranking large language models (LLMs) based on lossless\ndata compression. We demonstrate the equivalence of compression length under\narithmetic coding with cumulative negative log probabilities when using a large\nlanguage model as a prior, that is, the pre-training phase of the model is\nessentially the process of learning the optimal coding length. At the same\ntime, the evaluation metric compression ratio can be obtained without actual\ncompression, which greatly saves overhead. In this paper, we use five large\nlanguage models as priors for compression, then compare their performance on\nchallenging natural language processing tasks, including sentence completion,\nquestion answering, and coreference resolution. Experimental results show that\ncompression ratio and model performance are positively correlated, so it can be\nused as a general metric to evaluate large language models.\n', '  Long context inference presents challenges at the system level with increased\ncompute and memory requirements, as well as from an accuracy perspective in\nbeing able to reason over long contexts. Recently, several methods have been\nproposed to compress the prompt to reduce the context length. However, there\nhas been little work on comparing the different proposed methods across\ndifferent tasks through a standardized analysis. This has led to conflicting\nresults. To address this, here we perform a comprehensive characterization and\nevaluation of different prompt compression methods. In particular, we analyze\nextractive compression, summarization-based abstractive compression, and token\npruning methods. Surprisingly, we find that extractive compression often\noutperforms all the other approaches, and enables up to 10x compression with\nminimal accuracy degradation. Interestingly, we also find that despite several\nrecent claims, token pruning methods often lag behind extractive compression.\nWe only found marginal improvements on summarization tasks.\n']",Language Model Compression and Safety Evaluation,"Large Language Models: Safety, Security, and Ethics",Large Language Models
85,85,82,85_mcmc_bayesian_probabilistic_posteriors,"['mcmc', 'bayesian', 'probabilistic', 'posteriors', 'markov', 'posterior', 'stochastic', 'bayesmbar', 'monte', 'dmc']","['sampling', 'posterior', 'variational', 'inference', 'distributions', 'chain', 'distribution', 'gradient', 'samples', 'stochastic']","['  In this paper, we study sampling from a posterior derived from a neural\nnetwork. We propose a new probabilistic model consisting of adding noise at\nevery pre- and post-activation in the network, arguing that the resulting\nposterior can be sampled using an efficient Gibbs sampler. For small models,\nthe Gibbs sampler attains similar performances as the state-of-the-art Markov\nchain Monte Carlo (MCMC) methods, such as the Hamiltonian Monte Carlo (HMC) or\nthe Metropolis adjusted Langevin algorithm (MALA), both on real and synthetic\ndata. By framing our analysis in the teacher-student setting, we introduce a\nthermalization criterion that allows us to detect when an algorithm, when run\non data with synthetic labels, fails to sample from the posterior. The\ncriterion is based on the fact that in the teacher-student setting we can\ninitialize an algorithm directly at equilibrium.\n', '  Numerous applications in biology, statistics, science, and engineering\nrequire generating samples from high-dimensional probability distributions. In\nrecent years, the Hamiltonian Monte Carlo (HMC) method has emerged as a\nstate-of-the-art Markov chain Monte Carlo technique, exploiting the shape of\nsuch high-dimensional target distributions to efficiently generate samples.\nDespite its impressive empirical success and increasing popularity, its\nwide-scale adoption remains limited due to the high computational cost of\ngradient calculation. Moreover, applying this method is impossible when the\ngradient of the posterior cannot be computed (for example, with black-box\nsimulators). To overcome these challenges, we propose a novel two-stage\nHamiltonian Monte Carlo algorithm with a surrogate model. In this\nmulti-fidelity algorithm, the acceptance probability is computed in the first\nstage via a standard HMC proposal using an inexpensive differentiable surrogate\nmodel, and if the proposal is accepted, the posterior is evaluated in the\nsecond stage using the high-fidelity (HF) numerical solver. Splitting the\nstandard HMC algorithm into these two stages allows for approximating the\ngradient of the posterior efficiently, while producing accurate posterior\nsamples by using HF numerical solvers in the second stage. We demonstrate the\neffectiveness of this algorithm for a range of problems, including linear and\nnonlinear Bayesian inverse problems with in-silico data and experimental data.\nThe proposed algorithm is shown to seamlessly integrate with various\nlow-fidelity and HF models, priors, and datasets. Remarkably, our proposed\nmethod outperforms the traditional HMC algorithm in both computational and\nstatistical efficiency by several orders of magnitude, all while retaining or\nimproving the accuracy in computed posterior statistics.\n', '  This paper is intended to appear as a chapter for the Handbook of Markov\nChain Monte Carlo. The goal of this chapter is to unify various problems at the\nintersection of Markov chain Monte Carlo (MCMC) and machine\nlearning$\\unicode{x2014}$which includes black-box variational inference,\nadaptive MCMC, normalizing flow construction and transport-assisted MCMC,\nsurrogate-likelihood MCMC, coreset construction for MCMC with big data, Markov\nchain gradient descent, Markovian score climbing, and\nmore$\\unicode{x2014}$within one common framework. By doing so, the theory and\nmethods developed for each may be translated and generalized.\n']",Markov Chain Monte Carlo Methods,Probabilistic Methods for Sampling and Decision Making,Probabilistic Methods and Stochastic Processes
86,86,81,86_knowledge_contexts_language_entailment,"['knowledge', 'contexts', 'language', 'entailment', 'comprehension', 'memory', 'paraphrases', 'nlp', 'facts', 'retrieval']","['knowledge', 'facts', 'conflicts', 'factual', 'answering', 'question', 'questions', 'answer', 'entity', 'parametric']","[""  Knowledge-intensive language understanding tasks require Language Models\n(LMs) to integrate relevant context, mitigating their inherent weaknesses, such\nas incomplete or outdated knowledge. Nevertheless, studies indicate that LMs\noften ignore the provided context as it can conflict with the pre-existing LM's\nmemory learned during pre-training. Moreover, conflicting knowledge can already\nbe present in the LM's parameters, termed intra-memory conflict. Existing works\nhave studied the two types of knowledge conflicts only in isolation. We\nconjecture that the (degree of) intra-memory conflicts can in turn affect LM's\nhandling of context-memory conflicts. To study this, we introduce the DYNAMICQA\ndataset, which includes facts with a temporal dynamic nature where a fact can\nchange with a varying time frequency and disputable dynamic facts, which can\nchange depending on the viewpoint. DYNAMICQA is the first to include real-world\nknowledge conflicts and provide context to study the link between the different\ntypes of knowledge conflicts. With the proposed dataset, we assess the use of\nuncertainty for measuring the intra-memory conflict and introduce a novel\nCoherent Persuasion (CP) score to evaluate the context's ability to sway LM's\nsemantic output. Our extensive experiments reveal that static facts, which are\nunlikely to change, are more easily updated with additional context, relative\nto temporal and disputable facts.\n"", ""  When large language models are aligned via supervised fine-tuning, they may\nencounter new factual information that was not acquired through pre-training.\nIt is often conjectured that this can teach the model the behavior of\nhallucinating factually incorrect responses, as the model is trained to\ngenerate facts that are not grounded in its pre-existing knowledge. In this\nwork, we study the impact of such exposure to new knowledge on the capability\nof the fine-tuned model to utilize its pre-existing knowledge. To this end, we\ndesign a controlled setup, focused on closed-book QA, where we vary the\nproportion of the fine-tuning examples that introduce new knowledge. We\ndemonstrate that large language models struggle to acquire new factual\nknowledge through fine-tuning, as fine-tuning examples that introduce new\nknowledge are learned significantly slower than those consistent with the\nmodel's knowledge. However, we also find that as the examples with new\nknowledge are eventually learned, they linearly increase the model's tendency\nto hallucinate. Taken together, our results highlight the risk in introducing\nnew factual knowledge through fine-tuning, and support the view that large\nlanguage models mostly acquire factual knowledge through pre-training, whereas\nfine-tuning teaches them to use it more efficiently.\n"", '  Although Large Language Models (LLMs) are effective in performing various NLP\ntasks, they still struggle to handle tasks that require extensive, real-world\nknowledge, especially when dealing with long-tail facts (facts related to\nlong-tail entities). This limitation highlights the need to supplement LLMs\nwith non-parametric knowledge. To address this issue, we analysed the effects\nof different types of non-parametric knowledge, including textual passage and\nknowledge graphs (KGs). Since LLMs have probably seen the majority of factual\nquestion-answering datasets already, to facilitate our analysis, we proposed a\nfully automatic pipeline for creating a benchmark that requires knowledge of\nlong-tail facts for answering the involved questions. Using this pipeline, we\nintroduce the LTGen benchmark. We evaluate state-of-the-art LLMs in different\nknowledge settings using the proposed benchmark. Our experiments show that LLMs\nalone struggle with answering these questions, especially when the long-tail\nlevel is high or rich knowledge is required. Nonetheless, the performance of\nthe same models improved significantly when they were prompted with\nnon-parametric knowledge. We observed that, in most cases, prompting LLMs with\nKG triples surpasses passage-based prompting using a state-of-the-art\nretriever. In addition, while prompting LLMs with both KG triples and documents\ndoes not consistently improve knowledge coverage, it can dramatically reduce\nhallucinations in the generated content.\n']",Language Models and Knowledge Integration,Large Language Models and Cognitive Abilities,Large Language Models
87,87,81,87_opioids_opioid_nlp_annotated,"['opioids', 'opioid', 'nlp', 'annotated', 'tweets', 'drug', 'classification', 'text', 'addiction', 'posts']","['opioid', 'health', 'social', 'mental', 'patients', 'media', 'f1', 'posts', 'healthcare', 'tweets']","['  Background: Electronic health records (EHRs) are a data source for opioid\nresearch. Opioid use disorder is known to be under-coded as a diagnosis, yet\nproblematic opioid use can be documented in clinical notes.\n  Objectives: Our goals were 1) to identify problematic opioid use from a full\nrange of clinical notes; and 2) to compare the characteristics of patients\nidentified as having problematic opioid use, exclusively documented in clinical\nnotes, to those having documented ICD opioid use disorder diagnostic codes.\n  Materials and Methods: We developed and applied a natural language processing\n(NLP) tool to the clinical notes of a patient cohort (n=222,371) from two\nVeteran Affairs service regions to identify patients with problematic opioid\nuse. We also used a set of ICD diagnostic codes to identify patients with\nopioid use disorder from the same cohort. We compared the demographic and\nclinical characteristics of patients identified only through NLP, to those of\npatients identified through ICD codes.\n  Results: NLP exclusively identified 57,331 patients; 6,997 patients had\npositive ICD code identifications. Patients exclusively identified through NLP\nwere more likely to be women. Those identified through ICD codes were more\nlikely to be male, younger, have concurrent benzodiazepine prescriptions, more\ncomorbidities, more care encounters, and less likely to be married. Patients in\nthe NLP and ICD groups had substantially elevated comorbidity levels compared\nto patients not documented as experiencing problematic opioid use.\n  Conclusions: NLP is a feasible approach for identifying problematic opioid\nuse not otherwise recorded by ICD codes. Clinicians may be reluctant to code\nfor opioid use disorder. It is therefore incumbent on the healthcare team to\nsearch for documentation of opioid concerns within clinical notes.\n', '  The opioid epidemic, referring to the growing hospitalizations and deaths\nbecause of overdose of opioid usage and addiction, has become a severe health\nproblem in the United States. Many strategies have been developed by the\nfederal and local governments and health communities to combat this crisis.\nAmong them, improving our understanding of the epidemic through better health\nsurveillance is one of the top priorities. In addition to direct testing,\nmachine learning approaches may also allow us to detect opioid users by\nanalyzing data from social media because many opioid users may choose not to do\nthe tests but may share their experiences on social media anonymously. In this\npaper, we take advantage of recent advances in machine learning, collect and\nanalyze user posts from a popular social network Reddit with the goal to\nidentify opioid users. Posts from more than 1,000 users who have posted on\nthree sub-reddits over a period of one month have been collected. In addition\nto the ones that contain keywords such as opioid, opiate, or heroin, we have\nalso collected posts that contain slang words of opioid such as black or\nchocolate. We apply an attention-based bidirectional long short memory model to\nidentify opioid users. Experimental results show that the approaches\nsignificantly outperform competitive algorithms in terms of F1-score.\nFurthermore, the model allows us to extract most informative words, such as\nopiate, opioid, and black, from posts via the attention layer, which provides\nmore insights on how the machine learning algorithm works in distinguishing\ndrug users from non-drug users.\n', '  In the last decade, the United States has lost more than 500,000 people from\nan overdose involving prescription and illicit opioids making it a national\npublic health emergency (USDHHS, 2017). Medical practitioners require robust\nand timely tools that can effectively identify at-risk patients.\nCommunity-based social media platforms such as Reddit allow self-disclosure for\nusers to discuss otherwise sensitive drug-related behaviors. We present a\nmoderate size corpus of 2500 opioid-related posts from various subreddits\nlabeled with six different phases of opioid use: Medical Use, Misuse,\nAddiction, Recovery, Relapse, Not Using. For every post, we annotate span-level\nextractive explanations and crucially study their role both in annotation\nquality and model development. We evaluate several state-of-the-art models in a\nsupervised, few-shot, or zero-shot setting. Experimental results and error\nanalysis show that identifying the phases of opioid use disorder is highly\ncontextual and challenging. However, we find that using explanations during\nmodeling leads to a significant boost in classification accuracy demonstrating\ntheir beneficial role in a high-stakes domain such as studying the opioid use\ndisorder continuum.\n']",Opioid Use Disorder Detection using NLP,Machine Learning for Pharmaceutical and Healthcare Applications,Machine Learning and Data-Driven Applications
88,88,81,88_multimodal_modality_dialogues_visual,"['multimodal', 'modality', 'dialogues', 'visual', 'models', 'mllm', 'interactive', 'conversations', 'conversational', 'dialog']","['multimodal', 'modalities', 'visual', 'dialogue', 'text', 'modal', 'dialog', 'image', 'audio', 'images']","['  Multimodal Large Models (MLMs) are becoming a significant research focus,\ncombining powerful large language models with multimodal learning to perform\ncomplex tasks across different data modalities. This review explores the latest\ndevelopments and challenges in MLMs, emphasizing their potential in achieving\nartificial general intelligence and as a pathway to world models. We provide an\noverview of key techniques such as Multimodal Chain of Thought (M-COT),\nMultimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning\n(M-ICL). Additionally, we discuss both the fundamental and specific\ntechnologies of multimodal models, highlighting their applications,\ninput/output modalities, and design characteristics. Despite significant\nadvancements, the development of a unified multimodal model remains elusive. We\ndiscuss the integration of 3D generation and embodied intelligence to enhance\nworld simulation capabilities and propose incorporating external rule systems\nfor improved reasoning and decision-making. Finally, we outline future research\ndirections to address these challenges and advance the field.\n', ""  Multimodal foundation models that can holistically process text alongside\nimages, video, audio, and other sensory modalities are increasingly used in a\nvariety of real-world applications. However, it is challenging to characterize\nand study progress in multimodal foundation models, given the range of possible\nmodeling decisions, tasks, and domains. In this paper, we introduce Holistic\nEvaluation of Multimodal Models (HEMM) to systematically evaluate the\ncapabilities of multimodal foundation models across a set of 3 dimensions:\nbasic skills, information flow, and real-world use cases. Basic multimodal\nskills are internal abilities required to solve problems, such as learning\ninteractions across modalities, fine-grained alignment, multi-step reasoning,\nand the ability to handle external knowledge. Information flow studies how\nmultimodal content changes during a task through querying, translation,\nediting, and fusion. Use cases span domain-specific challenges introduced in\nreal-world multimedia, affective computing, natural sciences, healthcare, and\nhuman-computer interaction applications. Through comprehensive experiments\nacross the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g.,\nbasic skills, information flows, and use cases) that pose challenges to today's\nmodels, and (2) distill performance trends regarding how different modeling\ndimensions (e.g., scale, pre-training data, multimodal alignment, pre-training,\nand instruction tuning objectives) influence performance. Our conclusions\nregarding challenging multimodal interactions, use cases, and tasks requiring\nreasoning and external knowledge, the benefits of data and model scale, and the\nimpacts of instruction tuning yield actionable insights for future work in\nmultimodal foundation models.\n"", '  With the recent advancement in large language models (LLMs), there is a\ngrowing interest in combining LLMs with multimodal learning. Previous surveys\nof multimodal large language models (MLLMs) mainly focus on multimodal\nunderstanding. This survey elaborates on multimodal generation and editing\nacross various domains, comprising image, video, 3D, and audio. Specifically,\nwe summarize the notable advancements with milestone works in these fields and\ncategorize these studies into LLM-based and CLIP/T5-based methods. Then, we\nsummarize the various roles of LLMs in multimodal generation and exhaustively\ninvestigate the critical technical components behind these methods and the\nmultimodal datasets utilized in these studies. Additionally, we dig into\ntool-augmented multimodal agents that can leverage existing generative models\nfor human-computer interaction. Lastly, we discuss the advancements in the\ngenerative AI safety field, investigate emerging applications, and discuss\nfuture prospects. Our work provides a systematic and insightful overview of\nmultimodal generation and processing, which is expected to advance the\ndevelopment of Artificial Intelligence for Generative Content (AIGC) and world\nmodels. A curated list of all related papers can be found at\nhttps://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation\n']",Multimodal Large Language Models (MLLMs),Multimodal Large Language Models (MLLMs),Multimodal Learning and Vision-Language Models
88,88,81,88_multimodal_modality_dialogues_visual,"['multimodal', 'modality', 'dialogues', 'visual', 'models', 'mllm', 'interactive', 'conversations', 'conversational', 'dialog']","['multimodal', 'modalities', 'visual', 'dialogue', 'text', 'modal', 'dialog', 'image', 'audio', 'images']","['  Multimodal Large Models (MLMs) are becoming a significant research focus,\ncombining powerful large language models with multimodal learning to perform\ncomplex tasks across different data modalities. This review explores the latest\ndevelopments and challenges in MLMs, emphasizing their potential in achieving\nartificial general intelligence and as a pathway to world models. We provide an\noverview of key techniques such as Multimodal Chain of Thought (M-COT),\nMultimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning\n(M-ICL). Additionally, we discuss both the fundamental and specific\ntechnologies of multimodal models, highlighting their applications,\ninput/output modalities, and design characteristics. Despite significant\nadvancements, the development of a unified multimodal model remains elusive. We\ndiscuss the integration of 3D generation and embodied intelligence to enhance\nworld simulation capabilities and propose incorporating external rule systems\nfor improved reasoning and decision-making. Finally, we outline future research\ndirections to address these challenges and advance the field.\n', ""  Multimodal foundation models that can holistically process text alongside\nimages, video, audio, and other sensory modalities are increasingly used in a\nvariety of real-world applications. However, it is challenging to characterize\nand study progress in multimodal foundation models, given the range of possible\nmodeling decisions, tasks, and domains. In this paper, we introduce Holistic\nEvaluation of Multimodal Models (HEMM) to systematically evaluate the\ncapabilities of multimodal foundation models across a set of 3 dimensions:\nbasic skills, information flow, and real-world use cases. Basic multimodal\nskills are internal abilities required to solve problems, such as learning\ninteractions across modalities, fine-grained alignment, multi-step reasoning,\nand the ability to handle external knowledge. Information flow studies how\nmultimodal content changes during a task through querying, translation,\nediting, and fusion. Use cases span domain-specific challenges introduced in\nreal-world multimedia, affective computing, natural sciences, healthcare, and\nhuman-computer interaction applications. Through comprehensive experiments\nacross the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g.,\nbasic skills, information flows, and use cases) that pose challenges to today's\nmodels, and (2) distill performance trends regarding how different modeling\ndimensions (e.g., scale, pre-training data, multimodal alignment, pre-training,\nand instruction tuning objectives) influence performance. Our conclusions\nregarding challenging multimodal interactions, use cases, and tasks requiring\nreasoning and external knowledge, the benefits of data and model scale, and the\nimpacts of instruction tuning yield actionable insights for future work in\nmultimodal foundation models.\n"", '  With the recent advancement in large language models (LLMs), there is a\ngrowing interest in combining LLMs with multimodal learning. Previous surveys\nof multimodal large language models (MLLMs) mainly focus on multimodal\nunderstanding. This survey elaborates on multimodal generation and editing\nacross various domains, comprising image, video, 3D, and audio. Specifically,\nwe summarize the notable advancements with milestone works in these fields and\ncategorize these studies into LLM-based and CLIP/T5-based methods. Then, we\nsummarize the various roles of LLMs in multimodal generation and exhaustively\ninvestigate the critical technical components behind these methods and the\nmultimodal datasets utilized in these studies. Additionally, we dig into\ntool-augmented multimodal agents that can leverage existing generative models\nfor human-computer interaction. Lastly, we discuss the advancements in the\ngenerative AI safety field, investigate emerging applications, and discuss\nfuture prospects. Our work provides a systematic and insightful overview of\nmultimodal generation and processing, which is expected to advance the\ndevelopment of Artificial Intelligence for Generative Content (AIGC) and world\nmodels. A curated list of all related papers can be found at\nhttps://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation\n']",Multimodal Large Language Models (MLLMs),Multimodal Large Language Models (MLLMs),Multimodal Learning and Vision-Language Models
89,89,81,89_retrieval_embeddings_embedding_embed,"['retrieval', 'embeddings', 'embedding', 'embed', 'textual', 'multilingual', 'search', 'encoder', 'lingual', 'relevance']","['retrieval', 'embeddings', 'multilingual', 'dense', 'text', 'document', 'languages', 'clustering', 'contrastive', 'documents']","['  A dense passage retrieval system can serve as the initial stages of\ninformation retrieval, selecting the most relevant text passages for downstream\ntasks. In this work we conducted experiments with the goal of finding how much\nthe quality of a multilingual retrieval could be degraded if the query part of\na dual encoder is tuned on an English-only dataset (assuming scarcity of\ncross-lingual samples for the targeted domain or task). Specifically, starting\nwith a high quality multilingual embedding model, we observe that an\nEnglish-only tuning may not only preserve the original quality of the\nmultilingual retrieval, but even improve it.\n', '  Decoder-only large language model (LLM)-based embedding models are beginning\nto outperform BERT or T5-based embedding models in general-purpose text\nembedding tasks, including dense vector-based retrieval. In this work, we\nintroduce the NV-Embed model with a variety of architectural designs and\ntraining procedures to significantly enhance the performance of LLM as a\nversatile embedding model, while maintaining its simplicity and\nreproducibility. For model architecture, we propose a latent attention layer to\nobtain pooled embeddings, which consistently improves retrieval and downstream\ntask accuracy compared to mean pooling or using the last <EOS> token embedding\nfrom LLMs. To enhance representation learning, we remove the causal attention\nmask of LLMs during contrastive training. For model training, we introduce a\ntwo-stage contrastive instruction-tuning method. It first applies contrastive\ntraining with instructions on retrieval datasets, utilizing in-batch negatives\nand curated hard negative examples. At stage-2, it blends various non-retrieval\ndatasets into instruction tuning, which not only enhances non-retrieval task\naccuracy but also improves retrieval performance. Combining these techniques,\nour NV-Embed model, using only publicly available data, has achieved a\nrecord-high score of 69.32, ranking No. 1 on the Massive Text Embedding\nBenchmark (MTEB) (as of May 24, 2024), with 56 tasks, encompassing retrieval,\nreranking, classification, clustering, and semantic textual similarity tasks.\nNotably, our model also attains the highest score of 59.36 on 15 retrieval\ntasks in the MTEB benchmark (also known as BEIR). We will open-source the model\nat: https://huggingface.co/nvidia/NV-Embed-v1.\n', ""  Utilizing large language models (LLMs) for zero-shot document ranking is done\nin one of two ways: 1) prompt-based re-ranking methods, which require no\nfurther training but are only feasible for re-ranking a handful of candidate\ndocuments due to computational costs; and 2) unsupervised contrastive trained\ndense retrieval methods, which can retrieve relevant documents from the entire\ncorpus but require a large amount of paired text data for contrastive training.\nIn this paper, we propose PromptReps, which combines the advantages of both\ncategories: no need for training and the ability to retrieve from the whole\ncorpus. Our method only requires prompts to guide an LLM to generate query and\ndocument representations for effective document retrieval. Specifically, we\nprompt the LLMs to represent a given text using a single word, and then use the\nlast token's hidden states and the corresponding logits associated with the\nprediction of the next token to construct a hybrid document retrieval system.\nThe retrieval system harnesses both dense text embedding and sparse\nbag-of-words representations given by the LLM. We further explore variations of\nthis core idea that consider the generation of multiple words, and\nrepresentations that rely on multiple embeddings and sparse distributions. Our\nexperimental evaluation on the MSMARCO, TREC deep learning and BEIR zero-shot\ndocument retrieval datasets illustrates that this simple prompt-based LLM\nretrieval method can achieve a similar or higher retrieval effectiveness than\nstate-of-the-art LLM embedding methods that are trained with large amounts of\nunsupervised data, especially when using a larger LLM.\n""]",Multilingual Text Retrieval and Embeddings,Multilingual Natural Language Processing,Natural Language Processing
90,90,81,90_forgetting_continual_learning_memory,"['forgetting', 'continual', 'learning', 'memory', 'learned', 'forgotten', 'adaptive', 'regularization', 'incremental', 'trained']","['continual', 'forgetting', 'catastrophic', 'incremental', 'replay', 'old', 'knowledge', 'new', 'memory', 'domain']","['  Recent works demonstrate a remarkable ability to customize text-to-image\ndiffusion models while only providing a few example images. What happens if you\ntry to customize such models using multiple, fine-grained concepts in a\nsequential (i.e., continual) manner? In our work, we show that recent\nstate-of-the-art customization of text-to-image models suffer from catastrophic\nforgetting when new concepts arrive sequentially. Specifically, when adding a\nnew concept, the ability to generate high quality images of past, similar\nconcepts degrade. To circumvent this forgetting, we propose a new method,\nC-LoRA, composed of a continually self-regularized low-rank adaptation in cross\nattention layers of the popular Stable Diffusion model. Furthermore, we use\ncustomization prompts which do not include the word of the customized object\n(i.e., ""person"" for a human face dataset) and are initialized as completely\nrandom embeddings. Importantly, our method induces only marginal additional\nparameter costs and requires no storage of user data for replay. We show that\nC-LoRA not only outperforms several baselines for our proposed setting of\ntext-to-image continual customization, which we refer to as Continual\nDiffusion, but that we achieve a new state-of-the-art in the well-established\nrehearsal-free continual learning setting for image classification. The high\nachieving performance of C-LoRA in two separate domains positions it as a\ncompelling solution for a wide range of applications, and we believe it has\nsignificant potential for practical impact. Project page:\nhttps://jamessealesmith.github.io/continual-diffusion/\n', ""  Foundational vision-language models have shown impressive performance on\nvarious downstream tasks. Yet, there is still a pressing need to update these\nmodels later as new tasks or domains become available. Ongoing Continual\nLearning (CL) research provides techniques to overcome catastrophic forgetting\nof previous information when new knowledge is acquired. To date, CL techniques\nfocus only on the supervised training sessions. This results in significant\nforgetting yielding inferior performance to even the prior model zero shot\nperformance. In this work, we argue that test-time data hold great information\nthat can be leveraged in a self supervised manner to refresh the model's memory\nof previous learned tasks and hence greatly reduce forgetting at no extra\nlabelling cost. We study how unsupervised data can be employed online to\nimprove models' performance on prior tasks upon encountering representative\nsamples. We propose a simple yet effective student-teacher model with gradient\nbased sparse parameters updates and show significant performance improvements\nand reduction in forgetting, which could alleviate the role of an offline\nepisodic memory/experience replay buffer.\n"", '  The recent success of large language models (LLMs) trained on static,\npre-collected, general datasets has sparked numerous research directions and\napplications. One such direction addresses the non-trivial challenge of\nintegrating pre-trained LLMs into dynamic data distributions, task structures,\nand user preferences. Pre-trained LLMs, when tailored for specific needs, often\nexperience significant performance degradation in previous knowledge domains --\na phenomenon known as ""catastrophic forgetting"". While extensively studied in\nthe continual learning (CL) community, it presents new manifestations in the\nrealm of LLMs. In this survey, we provide a comprehensive overview of the\ncurrent research progress on LLMs within the context of CL. This survey is\nstructured into four main sections: we first describe an overview of\ncontinually learning LLMs, consisting of two directions of continuity: vertical\ncontinuity (or vertical continual learning), i.e., continual adaptation from\ngeneral to specific capabilities, and horizontal continuity (or horizontal\ncontinual learning), i.e., continual adaptation across time and domains\n(Section 3). We then summarize three stages of learning LLMs in the context of\nmodern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP),\nand Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of\nevaluation protocols for continual learning with LLMs, along with the current\navailable data sources (Section 5). Finally, we discuss intriguing questions\npertaining to continual learning for LLMs (Section 6). The full list of papers\nexamined in this survey is available at\nhttps://github.com/Wang-ML-Lab/llm-continual-learning-survey.\n']",Continual Learning and Forgetting in AI Models,Continual Learning and Catastrophic Forgetting,Machine Learning Adaptation and Forgetting
91,91,80,91_reinforcement_learning_optimizing_exploration,"['reinforcement', 'learning', 'optimizing', 'exploration', 'reward', 'rewards', 'autonomous', 'optimization', 'optimisation', 'strategies']","['reinforcement', 'diversity', 'evolutionary', 'crop', 'policy', 'algorithms', 'optimization', 'control', 'deep', 'policies']","['  Evolutionary search via the quality-diversity (QD) paradigm can discover\nhighly performing solutions in different behavioural niches, showing\nconsiderable potential in complex real-world scenarios such as evolutionary\nrobotics. Yet most QD methods only tackle static tasks that are fixed over\ntime, which is rarely the case in the real world. Unlike noisy environments,\nwhere the fitness of an individual changes slightly at every evaluation,\ndynamic environments simulate tasks where external factors at unknown and\nirregular intervals alter the performance of the individual with a severity\nthat is unknown a priori. Literature on optimisation in dynamic environments is\nextensive, yet such environments have not been explored in the context of QD\nsearch. This paper introduces a novel and generalisable Dynamic QD methodology\nthat aims to keep the archive of past solutions updated in the case of\nenvironment changes. Secondly, we present a novel characterisation of dynamic\nenvironments that can be easily applied to well-known benchmarks, with minor\ninterventions to move them from a static task to a dynamic one. Our Dynamic QD\nintervention is applied on MAP-Elites and CMA-ME, two powerful QD algorithms,\nand we test the dynamic variants on different dynamic tasks.\n', '  Training generally capable agents that thoroughly explore their environment\nand learn new and diverse skills is a long-term goal of robot learning. Quality\nDiversity Reinforcement Learning (QD-RL) is an emerging research area that\nblends the best aspects of both fields -- Quality Diversity (QD) provides a\nprincipled form of exploration and produces collections of behaviorally diverse\nagents, while Reinforcement Learning (RL) provides a powerful performance\nimprovement operator enabling generalization across tasks and dynamic\nenvironments. Existing QD-RL approaches have been constrained to sample\nefficient, deterministic off-policy RL algorithms and/or evolution strategies,\nand struggle with highly stochastic environments. In this work, we, for the\nfirst time, adapt on-policy RL, specifically Proximal Policy Optimization\n(PPO), to the Differentiable Quality Diversity (DQD) framework and propose\nadditional improvements over prior work that enable efficient optimization and\ndiscovery of novel skills on challenging locomotion tasks. Our new algorithm,\nProximal Policy Gradient Arborescence (PPGA), achieves state-of-the-art\nresults, including a 4x improvement in best reward over baselines on the\nchallenging humanoid domain.\n', '  In the past few years, a considerable amount of research has been dedicated\nto the exploitation of previous learning experiences and the design of Few-shot\nand Meta Learning approaches, in problem domains ranging from Computer Vision\nto Reinforcement Learning based control. A notable exception, where to the best\nof our knowledge, little to no effort has been made in this direction is\nQuality-Diversity (QD) optimization. QD methods have been shown to be effective\ntools in dealing with deceptive minima and sparse rewards in Reinforcement\nLearning. However, they remain costly due to their reliance on inherently\nsample inefficient evolutionary processes. We show that, given examples from a\ntask distribution, information about the paths taken by optimization in\nparameter space can be leveraged to build a prior population, which when used\nto initialize QD methods in unseen environments, allows for few-shot\nadaptation. Our proposed method does not require backpropagation. It is simple\nto implement and scale, and furthermore, it is agnostic to the underlying\nmodels that are being trained. Experiments carried in both sparse and dense\nreward settings using robotic manipulation and navigation benchmarks show that\nit considerably reduces the number of generations that are required for QD\noptimization in these environments.\n']",Quality-Diversity Optimization in Dynamic Environments,Optimization and Learning in Complex Systems,Complex Systems Analysis and Optimization
92,92,80,92_memorizing_memorization_context_examples,"['memorizing', 'memorization', 'context', 'examples', 'skills', 'exemplars', 'answering', 'knowledge', 'language', 'reasoning']","['reasoning', 'context', 'knowledge', 'skills', 'language', 'teacher', 'tasks', 'abilities', 'prompt', 'question']","['  We investigate how to elicit compositional generalization capabilities in\nlarge language models (LLMs). Compositional generalization empowers LLMs to\nsolve complex problems by combining foundational skills, a critical reasoning\nability akin to human intelligence. However, even the most advanced LLMs\ncurrently struggle with this form of reasoning. We examine this problem within\nthe framework of in-context learning and find that demonstrating both\nfoundational skills and compositional examples grounded in these skills within\nthe same prompt context is crucial. We refer to this prompt structure as\nskills-in-context (SKiC). With as few as two exemplars, this in-context\nlearning structure enables LLMs to tackle more challenging problems requiring\ninnovative skill combinations, achieving near-perfect systematic generalization\nacross a broad range of tasks. Intriguingly, SKiC also unlocks the latent\npotential of LLMs, allowing them to more actively utilize pre-existing internal\nskills acquired during earlier pretraining stages to solve complex reasoning\nproblems. The SKiC structure is robust across different skill constructions and\nexemplar choices and demonstrates strong transferability to new tasks. Finally,\ninspired by our in-context learning study, we show that fine-tuning LLMs with\nSKiC-style data can elicit zero-shot weak-to-strong generalization, enabling\nthe models to solve much harder problems directly with standard prompting.\n', ""  Chain-of-thought (CoT) prompting teaches large language models (LLMs) in\ncontext to reason over queries that require more than mere information\nretrieval. However, human experts are usually required to craft demonstrations\nfor in-context learning (ICL), which is expensive and has high variance. More\nimportantly, how to craft helpful reasoning exemplars for ICL remains unclear.\nIn this work, we investigate whether LLMs can be better in-context teachers for\nknowledge reasoning. We follow the ``encoding specificity'' hypothesis in\nhuman's memory retrieval to assume in-context exemplars at inference should\nmatch the encoding context in training data. We are thus motivated to propose\nSelf-Explain to use one LLM's self-elicited explanations as in-context\ndemonstrations for prompting it as they are generalized from the model's\ntraining examples. Self-Explain is shown to significantly outperform using\nhuman-crafted exemplars and other baselines. We further reveal that for\nin-context teaching, rationales by distinct teacher LLMs or human experts that\nmore resemble the student LLM's self-explanations are better demonstrations,\nwhich supports our encoding specificity hypothesis. We then propose Teach-Back\nthat aligns the teacher LLM with the student to enhance the in-context teaching\nperformance. For example, Teach-Back enables a 7B model to teach the much\nlarger GPT-3.5 in context, surpassing human teachers by around 5% in test\naccuracy on medical question answering.\n"", '  Large language models (LLMs) excel at few-shot in-context learning (ICL) --\nlearning from a few examples provided in context at inference, without any\nweight updates. Newly expanded context windows allow us to investigate ICL with\nhundreds or thousands of examples -- the many-shot regime. Going from few-shot\nto many-shot, we observe significant performance gains across a wide variety of\ngenerative and discriminative tasks. While promising, many-shot ICL can be\nbottlenecked by the available amount of human-generated examples. To mitigate\nthis limitation, we explore two new settings: Reinforced and Unsupervised ICL.\nReinforced ICL uses model-generated chain-of-thought rationales in place of\nhuman examples. Unsupervised ICL removes rationales from the prompt altogether,\nand prompts the model only with domain-specific questions. We find that both\nReinforced and Unsupervised ICL can be quite effective in the many-shot regime,\nparticularly on complex reasoning tasks. Finally, we demonstrate that, unlike\nfew-shot learning, many-shot learning is effective at overriding pretraining\nbiases, can learn high-dimensional functions with numerical inputs, and\nperforms comparably to fine-tuning. Our analysis also reveals the limitations\nof next-token prediction loss as an indicator of downstream ICL performance.\n']",In-Context Learning for Large Language Models,Advances in Large Language Models,Large Language Models
92,92,80,92_memorizing_memorization_context_examples,"['memorizing', 'memorization', 'context', 'examples', 'skills', 'exemplars', 'answering', 'knowledge', 'language', 'reasoning']","['reasoning', 'context', 'knowledge', 'skills', 'language', 'teacher', 'tasks', 'abilities', 'prompt', 'question']","['  We investigate how to elicit compositional generalization capabilities in\nlarge language models (LLMs). Compositional generalization empowers LLMs to\nsolve complex problems by combining foundational skills, a critical reasoning\nability akin to human intelligence. However, even the most advanced LLMs\ncurrently struggle with this form of reasoning. We examine this problem within\nthe framework of in-context learning and find that demonstrating both\nfoundational skills and compositional examples grounded in these skills within\nthe same prompt context is crucial. We refer to this prompt structure as\nskills-in-context (SKiC). With as few as two exemplars, this in-context\nlearning structure enables LLMs to tackle more challenging problems requiring\ninnovative skill combinations, achieving near-perfect systematic generalization\nacross a broad range of tasks. Intriguingly, SKiC also unlocks the latent\npotential of LLMs, allowing them to more actively utilize pre-existing internal\nskills acquired during earlier pretraining stages to solve complex reasoning\nproblems. The SKiC structure is robust across different skill constructions and\nexemplar choices and demonstrates strong transferability to new tasks. Finally,\ninspired by our in-context learning study, we show that fine-tuning LLMs with\nSKiC-style data can elicit zero-shot weak-to-strong generalization, enabling\nthe models to solve much harder problems directly with standard prompting.\n', ""  Chain-of-thought (CoT) prompting teaches large language models (LLMs) in\ncontext to reason over queries that require more than mere information\nretrieval. However, human experts are usually required to craft demonstrations\nfor in-context learning (ICL), which is expensive and has high variance. More\nimportantly, how to craft helpful reasoning exemplars for ICL remains unclear.\nIn this work, we investigate whether LLMs can be better in-context teachers for\nknowledge reasoning. We follow the ``encoding specificity'' hypothesis in\nhuman's memory retrieval to assume in-context exemplars at inference should\nmatch the encoding context in training data. We are thus motivated to propose\nSelf-Explain to use one LLM's self-elicited explanations as in-context\ndemonstrations for prompting it as they are generalized from the model's\ntraining examples. Self-Explain is shown to significantly outperform using\nhuman-crafted exemplars and other baselines. We further reveal that for\nin-context teaching, rationales by distinct teacher LLMs or human experts that\nmore resemble the student LLM's self-explanations are better demonstrations,\nwhich supports our encoding specificity hypothesis. We then propose Teach-Back\nthat aligns the teacher LLM with the student to enhance the in-context teaching\nperformance. For example, Teach-Back enables a 7B model to teach the much\nlarger GPT-3.5 in context, surpassing human teachers by around 5% in test\naccuracy on medical question answering.\n"", '  Large language models (LLMs) excel at few-shot in-context learning (ICL) --\nlearning from a few examples provided in context at inference, without any\nweight updates. Newly expanded context windows allow us to investigate ICL with\nhundreds or thousands of examples -- the many-shot regime. Going from few-shot\nto many-shot, we observe significant performance gains across a wide variety of\ngenerative and discriminative tasks. While promising, many-shot ICL can be\nbottlenecked by the available amount of human-generated examples. To mitigate\nthis limitation, we explore two new settings: Reinforced and Unsupervised ICL.\nReinforced ICL uses model-generated chain-of-thought rationales in place of\nhuman examples. Unsupervised ICL removes rationales from the prompt altogether,\nand prompts the model only with domain-specific questions. We find that both\nReinforced and Unsupervised ICL can be quite effective in the many-shot regime,\nparticularly on complex reasoning tasks. Finally, we demonstrate that, unlike\nfew-shot learning, many-shot learning is effective at overriding pretraining\nbiases, can learn high-dimensional functions with numerical inputs, and\nperforms comparably to fine-tuning. Our analysis also reveals the limitations\nof next-token prediction loss as an indicator of downstream ICL performance.\n']",In-Context Learning for Large Language Models,Advances in Large Language Models,Large Language Models
92,92,80,92_memorizing_memorization_context_examples,"['memorizing', 'memorization', 'context', 'examples', 'skills', 'exemplars', 'answering', 'knowledge', 'language', 'reasoning']","['reasoning', 'context', 'knowledge', 'skills', 'language', 'teacher', 'tasks', 'abilities', 'prompt', 'question']","['  We investigate how to elicit compositional generalization capabilities in\nlarge language models (LLMs). Compositional generalization empowers LLMs to\nsolve complex problems by combining foundational skills, a critical reasoning\nability akin to human intelligence. However, even the most advanced LLMs\ncurrently struggle with this form of reasoning. We examine this problem within\nthe framework of in-context learning and find that demonstrating both\nfoundational skills and compositional examples grounded in these skills within\nthe same prompt context is crucial. We refer to this prompt structure as\nskills-in-context (SKiC). With as few as two exemplars, this in-context\nlearning structure enables LLMs to tackle more challenging problems requiring\ninnovative skill combinations, achieving near-perfect systematic generalization\nacross a broad range of tasks. Intriguingly, SKiC also unlocks the latent\npotential of LLMs, allowing them to more actively utilize pre-existing internal\nskills acquired during earlier pretraining stages to solve complex reasoning\nproblems. The SKiC structure is robust across different skill constructions and\nexemplar choices and demonstrates strong transferability to new tasks. Finally,\ninspired by our in-context learning study, we show that fine-tuning LLMs with\nSKiC-style data can elicit zero-shot weak-to-strong generalization, enabling\nthe models to solve much harder problems directly with standard prompting.\n', ""  Chain-of-thought (CoT) prompting teaches large language models (LLMs) in\ncontext to reason over queries that require more than mere information\nretrieval. However, human experts are usually required to craft demonstrations\nfor in-context learning (ICL), which is expensive and has high variance. More\nimportantly, how to craft helpful reasoning exemplars for ICL remains unclear.\nIn this work, we investigate whether LLMs can be better in-context teachers for\nknowledge reasoning. We follow the ``encoding specificity'' hypothesis in\nhuman's memory retrieval to assume in-context exemplars at inference should\nmatch the encoding context in training data. We are thus motivated to propose\nSelf-Explain to use one LLM's self-elicited explanations as in-context\ndemonstrations for prompting it as they are generalized from the model's\ntraining examples. Self-Explain is shown to significantly outperform using\nhuman-crafted exemplars and other baselines. We further reveal that for\nin-context teaching, rationales by distinct teacher LLMs or human experts that\nmore resemble the student LLM's self-explanations are better demonstrations,\nwhich supports our encoding specificity hypothesis. We then propose Teach-Back\nthat aligns the teacher LLM with the student to enhance the in-context teaching\nperformance. For example, Teach-Back enables a 7B model to teach the much\nlarger GPT-3.5 in context, surpassing human teachers by around 5% in test\naccuracy on medical question answering.\n"", '  Large language models (LLMs) excel at few-shot in-context learning (ICL) --\nlearning from a few examples provided in context at inference, without any\nweight updates. Newly expanded context windows allow us to investigate ICL with\nhundreds or thousands of examples -- the many-shot regime. Going from few-shot\nto many-shot, we observe significant performance gains across a wide variety of\ngenerative and discriminative tasks. While promising, many-shot ICL can be\nbottlenecked by the available amount of human-generated examples. To mitigate\nthis limitation, we explore two new settings: Reinforced and Unsupervised ICL.\nReinforced ICL uses model-generated chain-of-thought rationales in place of\nhuman examples. Unsupervised ICL removes rationales from the prompt altogether,\nand prompts the model only with domain-specific questions. We find that both\nReinforced and Unsupervised ICL can be quite effective in the many-shot regime,\nparticularly on complex reasoning tasks. Finally, we demonstrate that, unlike\nfew-shot learning, many-shot learning is effective at overriding pretraining\nbiases, can learn high-dimensional functions with numerical inputs, and\nperforms comparably to fine-tuning. Our analysis also reveals the limitations\nof next-token prediction loss as an indicator of downstream ICL performance.\n']",In-Context Learning for Large Language Models,Advances in Large Language Models,Large Language Models
93,93,80,93_bias_biases_debiasing_biased,"['bias', 'biases', 'debiasing', 'biased', 'unbiased', 'debiased', 'debias', 'annotators', 'biasalert', 'nlp']","['bias', 'debiasing', 'biases', 'fairness', 'biased', 'sentiment', 'annotator', 'language', 'racial', 'disagreement']","['  Pretrained Language Models (PLMs) are widely used in NLP for various tasks.\nRecent studies have identified various biases that such models exhibit and have\nproposed methods to correct these biases. However, most of the works address a\nlimited set of bias dimensions independently such as gender, race, or religion.\nMoreover, the methods typically involve finetuning the full model to maintain\nthe performance on the downstream task. In this work, we aim to modularly\ndebias a pretrained language model across multiple dimensions. Previous works\nextensively explored debiasing PLMs using limited US-centric counterfactual\ndata augmentation (CDA). We use structured knowledge and a large generative\nmodel to build a diverse CDA across multiple bias dimensions in a\nsemi-automated way. We highlight how existing debiasing methods do not consider\ninteractions between multiple societal biases and propose a debiasing model\nthat exploits the synergy amongst various societal biases and enables\nmulti-bias debiasing simultaneously. An extensive evaluation on multiple tasks\nand languages demonstrates the efficacy of our approach.\n', '  Large Language models (LLMs), while powerful, exhibit harmful social biases.\nDebiasing is often challenging due to computational costs, data constraints,\nand potential degradation of multi-task language capabilities. This work\nintroduces a novel approach utilizing ChatGPT to generate synthetic training\ndata, aiming to enhance the debiasing of LLMs. We propose two strategies:\nTargeted Prompting, which provides effective debiasing for known biases but\nnecessitates prior specification of bias in question; and General Prompting,\nwhich, while slightly less effective, offers debiasing across various\ncategories. We leverage resource-efficient LLM debiasing using adapter tuning\nand compare the effectiveness of our synthetic data to existing debiasing\ndatasets. Our results reveal that: (1) ChatGPT can efficiently produce\nhigh-quality training data for debiasing other LLMs; (2) data produced via our\napproach surpasses existing datasets in debiasing performance while also\npreserving internal knowledge of a pre-trained LLM; and (3) synthetic data\nexhibits generalizability across categories, effectively mitigating various\nbiases, including intersectional ones. These findings underscore the potential\nof synthetic data in advancing the fairness of LLMs with minimal retraining\ncost.\n', '  Language models are the new state-of-the-art natural language processing\n(NLP) models and they are being increasingly used in many NLP tasks. Even\nthough there is evidence that language models are biased, the impact of that\nbias on the fairness of downstream NLP tasks is still understudied.\nFurthermore, despite that numerous debiasing methods have been proposed in the\nliterature, the impact of bias removal methods on the fairness of NLP tasks is\nalso understudied. In this work, we investigate three different sources of bias\nin NLP models, i.e. representation bias, selection bias and overamplification\nbias, and examine how they impact the fairness of the downstream task of\ntoxicity detection. Moreover, we investigate the impact of removing these\nbiases using different bias removal techniques on the fairness of toxicity\ndetection. Results show strong evidence that downstream sources of bias,\nespecially overamplification bias, are the most impactful types of bias on the\nfairness of the task of toxicity detection. We also found strong evidence that\nremoving overamplification bias by fine-tuning the language models on a dataset\nwith balanced contextual representations and ratios of positive examples\nbetween different identity groups can improve the fairness of the task of\ntoxicity detection. Finally, we build on our findings and introduce a list of\nguidelines to ensure the fairness of the task of toxicity detection.\n']",Debiasing Language Models,Bias and Fairness in Large Language Models,Fairness and Bias in Artificial Intelligence
94,94,79,94_cnn_neural_backpropagation_imagenet,"['cnn', 'neural', 'backpropagation', 'imagenet', 'layers', 'layer', 'convolutional', 'neurons', 'networks', 'memory']","['activation', 'layers', 'deep', 'layer', 'networks', 'neural', 'residual', 'memory', 'training', 'architectures']","[""  Deep neural networks are typically trained using global error signals that\nbackpropagate (BP) end-to-end, which is not only biologically implausible but\nalso suffers from the update locking problem and requires huge memory\nconsumption. Local learning, which updates each layer independently with a\ngradient-isolated auxiliary network, offers a promising alternative to address\nthe above problems. However, existing local learning methods are confronted\nwith a large accuracy gap with the BP counterpart, particularly for large-scale\nnetworks. This is due to the weak coupling between local layers and their\nsubsequent network layers, as there is no gradient communication across layers.\nTo tackle this issue, we put forward an augmented local learning method, dubbed\nAugLocal. AugLocal constructs each hidden layer's auxiliary network by\nuniformly selecting a small subset of layers from its subsequent network layers\nto enhance their synergy. We also propose to linearly reduce the depth of\nauxiliary networks as the hidden layer goes deeper, ensuring sufficient network\ncapacity while reducing the computational cost of auxiliary networks. Our\nextensive experiments on four image classification datasets (i.e., CIFAR-10,\nSVHN, STL-10, and ImageNet) demonstrate that AugLocal can effectively scale up\nto tens of local layers with a comparable accuracy to BP-trained networks while\nreducing GPU memory usage by around 40%. The proposed AugLocal method,\ntherefore, opens up a myriad of opportunities for training high-performance\ndeep neural networks on resource-constrained platforms.Code is available at\nhttps://github.com/ChenxiangMA/AugLocal.\n"", '  Different activation functions work best for different deep learning models.\nTo exploit this, we leverage recent advancements in gradient-based search\ntechniques for neural architectures to efficiently identify high-performing\nactivation functions for a given application. We propose a fine-grained search\ncell that combines basic mathematical operations to model activation functions,\nallowing for the exploration of novel activations. Our approach enables the\nidentification of specialized activations, leading to improved performance in\nevery model we tried, from image classification to language models. Moreover,\nthe identified activations exhibit strong transferability to larger models of\nthe same type, as well as new datasets. Importantly, our automated process for\ncreating customized activation functions is orders of magnitude more efficient\nthan previous approaches. It can easily be applied on top of arbitrary deep\nlearning pipelines and thus offers a promising practical avenue for enhancing\ndeep learning architectures.\n', '  Fine-tuning pretrained large models to downstream tasks is an important\nproblem, which however suffers from huge memory overhead due to large-scale\nparameters. This work strives to reduce memory overhead in fine-tuning from\nperspectives of activation function and layer normalization. To this end, we\npropose the Approximate Backpropagation (Approx-BP) theory, which provides the\ntheoretical feasibility of decoupling the forward and backward passes. We apply\nour Approx-BP theory to backpropagation training and derive memory-efficient\nalternatives of GELU and SiLU activation functions, which use derivative\nfunctions of ReLUs in the backward pass while keeping their forward pass\nunchanged. In addition, we introduce a Memory-Sharing Backpropagation strategy,\nwhich enables the activation memory to be shared by two adjacent layers,\nthereby removing activation memory usage redundancy. Our method neither induces\nextra computation nor reduces training efficiency. We conduct extensive\nexperiments with pretrained vision and language models, and the results\ndemonstrate that our proposal can reduce up to $\\sim$$30\\%$ of the peak memory\nusage. Our code is released at https://github.com/yyyyychen/LowMemoryBP.\n']",Efficient Deep Learning with Local Learning and Memory Reduction,Efficient Deep Learning Architectures and Acceleration Techniques,Artificial Intelligence and Machine Learning Hardware Acceleration and Optimization
95,95,78,95_workflows_workflow_automate_automation,"['workflows', 'workflow', 'automate', 'automation', 'apis', 'tools', 'interpreter', 'flowmind', 'software', 'pipelines']","['workflow', 'workflows', 'tools', 'agents', 'software', 'management', 'data', 'automation', 'code', 'science']","['  Automating enterprise workflows could unlock $4 trillion/year in productivity\ngains. Despite being of interest to the data management community for decades,\nthe ultimate vision of end-to-end workflow automation has remained elusive.\nCurrent solutions rely on process mining and robotic process automation (RPA),\nin which a bot is hard-coded to follow a set of predefined rules for completing\na workflow. Through case studies of a hospital and large B2B enterprise, we\nfind that the adoption of RPA has been inhibited by high set-up costs (12-18\nmonths), unreliable execution (60% initial accuracy), and burdensome\nmaintenance (requiring multiple FTEs). Multimodal foundation models (FMs) such\nas GPT-4 offer a promising new approach for end-to-end workflow automation\ngiven their generalized reasoning and planning abilities. To study these\ncapabilities we propose ECLAIR, a system to automate enterprise workflows with\nminimal human supervision. We conduct initial experiments showing that\nmultimodal FMs can address the limitations of traditional RPA with (1)\nnear-human-level understanding of workflows (93% accuracy on a workflow\nunderstanding task) and (2) instant set-up with minimal technical barrier\n(based solely on a natural language description of a workflow, ECLAIR achieves\nend-to-end completion rates of 40%). We identify human-AI collaboration,\nvalidation, and self-improvement as open challenges, and suggest ways they can\nbe solved with data management techniques. Code is available at:\nhttps://github.com/HazyResearch/eclair-agents\n', '  Data science and engineering workflows often span multiple stages, from\nwarehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As\nvision language models (VLMs) advance in multimodal understanding and code\ngeneration, VLM-based agents could potentially automate these workflows by\ngenerating SQL queries, Python code, and GUI operations. This automation can\nimprove the productivity of experts while democratizing access to large-scale\ndata analysis. In this paper, we introduce Spider2-V, the first multimodal\nagent benchmark focusing on professional data science and engineering\nworkflows, featuring 494 real-world tasks in authentic computer environments\nand incorporating 20 enterprise-level professional applications. These tasks,\nderived from real-world use cases, evaluate the ability of a multimodal agent\nto perform data-related tasks by writing code and managing the GUI in\nenterprise data software systems. To balance realistic simulation with\nevaluation simplicity, we devote significant effort to developing automatic\nconfigurations for task setup and carefully crafting evaluation metrics for\neach task. Furthermore, we supplement multimodal agents with comprehensive\ndocuments of these enterprise data software systems. Our empirical evaluation\nreveals that existing state-of-the-art LLM/VLM-based agents do not reliably\nautomate full data workflows (14.0% success). Even with step-by-step guidance,\nthese agents still underperform in tasks that require fine-grained,\nknowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted\nworkspaces (10.6%). We hope that Spider2-V paves the way for autonomous\nmultimodal agents to transform the automation of data science and engineering\nworkflow. Our code and data are available at https://spider2-v.github.io.\n', ""  Machine Learning (ML) has become ubiquitous, fueling data-driven applications\nacross various organizations. Contrary to the traditional perception of ML in\nresearch, ML workflows can be complex, resource-intensive, and time-consuming.\nExpanding an ML workflow to encompass a wider range of data infrastructure and\ndata types may lead to larger workloads and increased deployment costs.\nCurrently, numerous workflow engines are available (with over ten being widely\nrecognized). This variety poses a challenge for end-users in terms of mastering\ndifferent engine APIs. While efforts have primarily focused on optimizing ML\nOperations (MLOps) for a specific workflow engine, current methods largely\noverlook workflow optimization across different engines.\n  In this work, we design and implement Couler, a system designed for unified\nML workflow optimization in the cloud. Our main insight lies in the ability to\ngenerate an ML workflow using natural language (NL) descriptions. We integrate\nLarge Language Models (LLMs) into workflow generation, and provide a unified\nprogramming interface for various workflow engines. This approach alleviates\nthe need to understand various workflow engines' APIs. Moreover, Couler\nenhances workflow computation efficiency by introducing automated caching at\nmultiple stages, enabling large workflow auto-parallelization and automatic\nhyperparameters tuning. These enhancements minimize redundant computational\ncosts and improve fault tolerance during deep learning workflow training.\nCouler is extensively deployed in real-world production scenarios at Ant Group,\nhandling approximately 22k workflows daily, and has successfully improved the\nCPU/Memory utilization by more than 15% and the workflow completion rate by\naround 17%.\n""]",Automating Workflows with AI and ML,Artificial Intelligence in Technology and Engineering,Artificial Intelligence and Cognitive Systems
96,96,78,96_quantization_compression_softmax_compressed,"['quantization', 'compression', 'softmax', 'compressed', 'quantize', 'compressing', 'quantized', 'memory', 'pruning', 'efficient']","['quantization', 'compression', 'transformers', 'pruning', 'hardware', 'bit', 'transformer', 'softmax', 'accuracy', 'lossy']","['  The increasing size of deep neural networks necessitates effective model\ncompression to improve computational efficiency and reduce their memory\nfootprint. Sparsity and quantization are two prominent compression methods that\nhave individually demonstrated significant reduction in computational and\nmemory footprints while preserving model accuracy. While effective, the\ninterplay between these two methods remains an open question. In this paper, we\ninvestigate the interaction between these two methods and assess whether their\ncombination impacts final model accuracy. We mathematically prove that applying\nsparsity before quantization is the optimal sequence for these operations,\nminimizing error in computation. Our empirical studies across a wide range of\nmodels, including OPT and Llama model families (125M-8B) and ViT corroborate\nthese theoretical findings. In addition, through rigorous analysis, we\ndemonstrate that sparsity and quantization are not orthogonal; their\ninteraction can significantly harm model accuracy, with quantization error\nplaying a dominant role in this degradation. Our findings extend to the\nefficient deployment of large models in resource-limited compute platforms and\nreduce serving cost, offering insights into best practices for applying these\ncompression methods to maximize efficacy without compromising accuracy.\n', ""  Motivated by the huge success of Transformers in the field of natural\nlanguage processing (NLP), Vision Transformers (ViTs) have been rapidly\ndeveloped and achieved remarkable performance in various computer vision tasks.\nHowever, their huge model sizes and intensive computations hinder ViTs'\ndeployment on embedded devices, calling for effective model compression\nmethods, such as quantization. Unfortunately, due to the existence of\nhardware-unfriendly and quantization-sensitive non-linear operations,\nparticularly {Softmax}, it is non-trivial to completely quantize all operations\nin ViTs, yielding either significant accuracy drops or non-negligible hardware\ncosts. In response to challenges associated with \\textit{standard ViTs}, we\nfocus our attention towards the quantization and acceleration for\n\\textit{efficient ViTs}, which not only eliminate the troublesome Softmax but\nalso integrate linear attention with low computational complexity, and propose\n\\emph{Trio-ViT} accordingly. Specifically, at the algorithm level, we develop a\n{tailored post-training quantization engine} taking the unique activation\ndistributions of Softmax-free efficient ViTs into full consideration, aiming to\nboost quantization accuracy. Furthermore, at the hardware level, we build an\naccelerator dedicated to the specific Convolution-Transformer hybrid\narchitecture of efficient ViTs, thereby enhancing hardware efficiency.\nExtensive experimental results consistently prove the effectiveness of our\nTrio-ViT framework. {Particularly, we can gain up to\n$\\uparrow$$\\mathbf{7.2}\\times$ and $\\uparrow$$\\mathbf{14.6}\\times$ FPS under\ncomparable accuracy over state-of-the-art ViT accelerators, as well as\n$\\uparrow$$\\mathbf{5.9}\\times$ and $\\uparrow$$\\mathbf{2.0}\\times$ DSP\nefficiency.} Codes will be released publicly upon acceptance.\n"", '  Model quantization and compression is widely used techniques to reduce usage\nof computing resource at inference time. While state-of-the-art works have been\nachieved reasonable accuracy with higher bit such as 4bit or 8bit, but still it\nis challenging to quantize/compress a model further, e.g., 1bit or 2bit. To\novercome the challenge, we focus on outliers in weights of a pre-trained model\nwhich disrupt effective lower bit quantization and compression. In this work,\nwe propose Range Restriction Loss (R2-Loss) for building lower bit quantization\nand compression friendly models by removing outliers from weights during\npre-training. By effectively restricting range of weights, we mold the overall\ndistribution into a tight shape to ensure high quantization bit resolution,\ntherefore allowing model compression and quantization techniques can to utilize\ntheir limited numeric representation powers better. We introduce three\ndifferent, L-inf R2-Loss, its extension Margin R2-Loss and a new\nSoft-Min-MaxR2-Loss to be used as an auxiliary loss during full-precision model\ntraining. These R2-Loss can be used in different cases such as L-inf and Margin\nR2-Loss would be effective for symmetric quantization, while Soft-Min-Max\nR2-Loss shows better performance for model compression. In our experiment,\nR2-Loss improves lower bit quantization accuracy with state-of-the-art\npost-training quantization (PTQ), quantization-aware training (QAT), and model\ncompression techniques. With R2-Loss, MobileNet-V2 2bit weight and 8bit\nactivation PTQ, MobileNet-V1 2bit weight and activation QAT, ResNet18 1bit\nweight compression are improved to 59.49% from 50.66%, 59.05% from 55.96%, and\n52.58% from 45.54%, respectively.\n']",Model Compression and Quantization Techniques,Quantization and Compression Techniques for Deep Learning Models,Deep Learning Optimization and Security
97,97,77,97_reinforcement_optimal_regret_adaptive,"['reinforcement', 'optimal', 'regret', 'adaptive', 'learning', 'optimization', 'guarantees', 'exploration', 'control', 'reward']","['regret', 'algorithm', 'control', 'bound', 'online', 'linear', 'horizon', 'policy', 'optimal', 'quadratic']","['  Recent advancement in online optimization and control has provided novel\ntools to study online linear quadratic regulator (LQR) problems, where cost\nmatrices are varying adversarially over time. However, the controller\nparameterization of existing works may not satisfy practical conditions like\nsparsity due to physical connections. In this work, we study online linear\nquadratic Gaussian problems with a given linear constraint imposed on the\ncontroller. Inspired by the recent work of [1] which proposed, for a linearly\nconstrained policy optimization of an offline LQR, a second order method\nequipped with a Riemannian metric that emerges naturally in the context of\noptimal control problems, we propose online optimistic Newton on manifold\n(OONM) which provides an online controller based on the prediction on the first\nand second order information of the function sequence. To quantify the proposed\nalgorithm, we leverage the notion of regret defined as the sub-optimality of\nits cumulative cost to that of a (locally) minimizing controller sequence and\nprovide the regret bound in terms of the path-length of the minimizer sequence.\nSimulation results are also provided to verify the property of OONM.\n', '  In this paper, we propose Posterior Sampling Reinforcement Learning for\nZero-sum Stochastic Games (PSRL-ZSG), the first online learning algorithm that\nachieves Bayesian regret bound of $O(HS\\sqrt{AT})$ in the infinite-horizon\nzero-sum stochastic games with average-reward criterion. Here $H$ is an upper\nbound on the span of the bias function, $S$ is the number of states, $A$ is the\nnumber of joint actions and $T$ is the horizon. We consider the online setting\nwhere the opponent can not be controlled and can take any arbitrary\ntime-adaptive history-dependent strategy. Our regret bound improves on the best\nexisting regret bound of $O(\\sqrt[3]{DS^2AT^2})$ by Wei et al. (2017) under the\nsame assumption and matches the theoretical lower bound in $T$.\n', '  We consider the online control problem with an unknown linear dynamical\nsystem in the presence of adversarial perturbations and adversarial convex loss\nfunctions. Although the problem is widely studied in model-based control, it\nremains unclear whether data-driven approaches, which bypass the system\nidentification step, can solve the problem. In this work, we present a novel\ndata-driven online adaptive control algorithm to address this online control\nproblem. Our algorithm leverages the behavioral systems theory to learn a\nnon-parametric system representation and then adopts a perturbation-based\ncontroller updated by online gradient descent. We prove that our algorithm\nguarantees an $\\tmO(T^{2/3})$ regret bound with high probability, which matches\nthe best-known regret bound for this problem. Furthermore, we extend our\nalgorithm and performance guarantee to the cases with output feedback.\n']",Online Learning and Control with Regret Guarantees,Optimization and Learning in Bandit and Game-Theoretic Settings,Decision Making and Optimization under Uncertainty
98,98,76,98_recognition_vision_detection_supervised,"['recognition', 'vision', 'detection', 'supervised', 'detectors', 'objectness', 'attention', 'yolov8', 'objects', '3d']","['driving', 'detection', 'object', 'autonomous', 'segmentation', 'crowd', 'road', 'active', 'semantic', 'objects']","['  Since the introduction of the self-attention mechanism and the adoption of\nthe Transformer architecture for Computer Vision tasks, the Vision\nTransformer-based architectures gained a lot of popularity in the field, being\nused for tasks such as image classification, object detection and image\nsegmentation. However, efficiently leveraging the attention mechanism in vision\ntransformers for the Monocular 3D Object Detection task remains an open\nquestion. In this paper, we present LAM3D, a framework that Leverages\nself-Attention mechanism for Monocular 3D object Detection. To do so, the\nproposed method is built upon a Pyramid Vision Transformer v2 (PVTv2) as\nfeature extraction backbone and 2D/3D detection machinery. We evaluate the\nproposed method on the KITTI 3D Object Detection Benchmark, proving the\napplicability of the proposed solution in the autonomous driving domain and\noutperforming reference methods. Moreover, due to the usage of self-attention,\nLAM3D is able to systematically outperform the equivalent architecture that\ndoes not employ self-attention.\n', ""  Although accuracy and other common metrics can provide a useful window into\nthe performance of an object detection model, they lack a deeper view of the\nmodel's decision process. Regardless of the quality of the training data and\nprocess, the features that an object detection model learns cannot be\nguaranteed. A model may learn a relationship between certain background\ncontext, i.e., scene level objects, and the presence of the labeled classes.\nFurthermore, standard performance verification and metrics would not identify\nthis phenomenon. This paper presents a new black box explainability method for\nadditional verification of object detection models by finding the impact of\nscene level objects on the identification of the objects within the image. By\ncomparing the accuracies of a model on test data with and without certain scene\nlevel objects, the contributions of these objects to the model's performance\nbecomes clearer. The experiment presented here will assess the impact of\nbuildings and people in image context on the detection of emergency road\nvehicles by a fine-tuned YOLOv8 model. A large increase in accuracy in the\npresence of a scene level object will indicate the model's reliance on that\nobject to make its detections. The results of this research lead to providing a\nquantitative explanation of the object detection model's decision process,\nenabling a deeper understanding of the model's performance.\n"", '  This survey explores the adaptation of visual transformer models in\nAutonomous Driving, a transition inspired by their success in Natural Language\nProcessing. Surpassing traditional Recurrent Neural Networks in tasks like\nsequential image processing and outperforming Convolutional Neural Networks in\nglobal context capture, as evidenced in complex scene recognition, Transformers\nare gaining traction in computer vision. These capabilities are crucial in\nAutonomous Driving for real-time, dynamic visual scene processing. Our survey\nprovides a comprehensive overview of Vision Transformer applications in\nAutonomous Driving, focusing on foundational concepts such as self-attention,\nmulti-head attention, and encoder-decoder architecture. We cover applications\nin object detection, segmentation, pedestrian detection, lane detection, and\nmore, comparing their architectural merits and limitations. The survey\nconcludes with future research directions, highlighting the growing role of\nVision Transformers in Autonomous Driving.\n']",Vision Transformers for Object Detection and Recognition,Computer Vision and Object Recognition,Computer Vision
99,99,76,99_inferences_answering_reasoning_questions,"['inferences', 'answering', 'reasoning', 'questions', 'retrieval', 'knowledge', 'answers', 'hop', 'language', 'contexts']","['reasoning', 'hop', 'commonsense', 'question', 'questions', 'answering', 'knowledge', 'answer', 'multi', 'answers']","['  In response to the increasing use of interactive artificial intelligence, the\ndemand for the capacity to handle complex questions has increased. Multi-hop\nquestion generation aims to generate complex questions that requires multi-step\nreasoning over several documents. Previous studies have predominantly utilized\nend-to-end models, wherein questions are decoded based on the representation of\ncontext documents. However, these approaches lack the ability to explain the\nreasoning process behind the generated multi-hop questions. Additionally, the\nquestion rewriting approach, which incrementally increases the question\ncomplexity, also has limitations due to the requirement of labeling data for\nintermediate-stage questions. In this paper, we introduce an end-to-end\nquestion rewriting model that increases question complexity through sequential\nrewriting. The proposed model has the advantage of training with only the final\nmulti-hop questions, without intermediate questions. Experimental results\ndemonstrate the effectiveness of our model in generating complex questions,\nparticularly 3- and 4-hop questions, which are appropriately paired with input\nanswers. We also prove that our model logically and incrementally increases the\ncomplexity of questions, and the generated multi-hop questions are also\nbeneficial for training question answering models.\n', ""  Large language models (LLMs) demonstrate strong reasoning abilities when\nprompted to generate chain-of-thought (CoT) explanations alongside answers.\nHowever, previous research on evaluating LLMs has solely focused on answer\naccuracy, neglecting the correctness of the generated CoT. In this paper, we\ndelve deeper into the CoT reasoning capabilities of LLMs in multi-hop question\nanswering by utilizing knowledge graphs (KGs). We propose a novel\ndiscriminative and generative CoT evaluation paradigm to assess LLMs' knowledge\nof reasoning and the accuracy of the generated CoT. Through experiments\nconducted on 5 different families of LLMs across 2 multi-hop question-answering\ndatasets, we find that LLMs possess sufficient knowledge to perform reasoning.\nHowever, there exists a significant disparity between answer accuracy and\nfaithfulness of the CoT reasoning generated by LLMs, indicating that they often\narrive at correct answers through incorrect reasoning.\n"", '  Most existing multi-hop datasets are extractive answer datasets, where the\nanswers to the questions can be extracted directly from the provided context.\nThis often leads models to use heuristics or shortcuts instead of performing\ntrue multi-hop reasoning. In this paper, we propose a new multi-hop dataset,\nMoreHopQA, which shifts from extractive to generative answers. Our dataset is\ncreated by utilizing three existing multi-hop datasets: HotpotQA,\n2WikiMultihopQA, and MuSiQue. Instead of relying solely on factual reasoning,\nwe enhance the existing multi-hop questions by adding another layer of\nquestioning that involves one, two, or all three of the following types of\nreasoning: commonsense, arithmetic, and symbolic. Our dataset is created\nthrough a semi-automated process, resulting in a dataset with 1,118 samples\nthat have undergone human verification. We then use our dataset to evaluate\nfive different large language models: Mistral 7B, Gemma 7B, Llama 3 (8B and\n70B), and GPT-4. We also design various cases to analyze the reasoning steps in\nthe question-answering process. Our results show that models perform well on\ninitial multi-hop questions but struggle with our extended questions,\nindicating that our dataset is more challenging than previous ones. Our\nanalysis of question decomposition reveals that although models can correctly\nanswer questions, only a portion - 38.7% for GPT-4 and 33.4% for Llama3-70B -\nachieve perfect reasoning, where all corresponding sub-questions are answered\ncorrectly. Evaluation code and data are available at\nhttps://github.com/Alab-NII/morehopqa\n']",Multi-Hop Question Answering and Reasoning,Question Answering and Reasoning Systems,Artificial Intelligence and Reasoning Systems
100,100,76,100_judgments_evaluations_bias_evaluation,"['judgments', 'evaluations', 'bias', 'evaluation', 'annotators', 'biases', 'evaluating', 'judge', 'annotations', 'assessing']","['bias', 'judge', 'evaluation', 'human', 'preference', 'evaluators', 'ratings', 'biases', 'judgments', 'preferences']","['  With the rising human-like precision of Large Language Models (LLMs) in\nnumerous tasks, their utilization in a variety of real-world applications is\nbecoming more prevalent. Several studies have shown that LLMs excel on many\nstandard NLP benchmarks. However, it is challenging to evaluate LLMs due to\ntest dataset contamination and the limitations of traditional metrics. Since\nhuman evaluations are difficult to collect, there is a growing interest in the\ncommunity to use LLMs themselves as reference-free evaluators for subjective\nmetrics. However, past work has shown that LLM-based evaluators can exhibit\nbias and have poor alignment with human judgments. In this study, we propose a\nframework for an end-to-end assessment of LLMs as evaluators in multilingual\nscenarios. We create a carefully curated dataset, covering 10 languages\ncontaining native speaker judgments for the task of summarization. This dataset\nis created specifically to evaluate LLM-based evaluators, which we refer to as\nmeta-evaluation (METAL). We compare the performance of LLM-based evaluators\ncreated using GPT-3.5-Turbo, GPT-4, and PaLM2. Our results indicate that\nLLM-based evaluators based on GPT-4 perform the best across languages, while\nGPT-3.5-Turbo performs poorly. Additionally, we perform an analysis of the\nreasoning provided by LLM-based evaluators and find that it often does not\nmatch the reasoning provided by human judges.\n', '  LLM-as-a-Judge offers a promising alternative to human judges across various\ntasks, yet inherent biases, particularly position bias - a systematic\npreference for answers based on their position in the prompt - compromise its\neffectiveness. Our study investigates this issue by developing a framework to\nsystematically study and quantify position bias using metrics such as\nrepetitional consistency, positional consistency, and positional fairness. We\nconduct experiments with 9 judge models across 22 tasks from the MTBench and\nDevBench benchmarks and nearly 40 answer-generating models, generating\napproximately 80,000 evaluation instances. This comprehensive assessment\nreveals significant variations in bias across judges and tasks. Although GPT-4\noften excels in positional consistency and fairness, some more cost-effective\nmodels perform comparably or even better in specific tasks, highlighting\nessential trade-offs between consistency, fairness, and cost. Our results also\ndemonstrate high consistency of judgment across repetitions, confirming that\nposition bias is not due to random variations. This research significantly\ncontributes to the field by introducing new concepts for understanding position\nbias and providing a multi-dimensional framework for evaluation. These insights\nguide the selection of optimal judge models, enhance benchmark design, and lay\nthe foundation for future research into effective debiasing strategies,\nultimately enhancing the reliability of LLM evaluators.\n', '  Large language models (LLMs) have shown promising abilities as cost-effective\nand reference-free evaluators for assessing language generation quality. In\nparticular, pairwise LLM evaluators, which compare two generated texts and\ndetermine the preferred one, have been employed in a wide range of\napplications. However, LLMs exhibit preference biases and worrying sensitivity\nto prompt designs. In this work, we first reveal that the predictive preference\nof LLMs can be highly brittle and skewed, even with semantically equivalent\ninstructions. We find that fairer predictive preferences from LLMs consistently\nlead to judgments that are better aligned with humans. Motivated by this\nphenomenon, we propose an automatic Zero-shot Evaluation-oriented Prompt\nOptimization framework, ZEPO, which aims to produce fairer preference decisions\nand improve the alignment of LLM evaluators with human judgments. To this end,\nwe propose a zero-shot learning objective based on the preference decision\nfairness. ZEPO demonstrates substantial performance improvements over\nstate-of-the-art LLM evaluators, without requiring labeled data, on\nrepresentative meta-evaluation benchmarks. Our findings underscore the critical\ncorrelation between preference fairness and human alignment, positioning ZEPO\nas an efficient prompt optimizer for bridging the gap between LLM evaluators\nand human judgments.\n']",Evaluating Large Language Models for Fairness and Bias,Bias and Fairness in Large Language Models,Fairness and Bias in Artificial Intelligence
101,101,76,101_navigate_navigating_navigation_planning,"['navigate', 'navigating', 'navigation', 'planning', 'robotics', 'robot', 'terrain', 'slam', 'obstacle', 'exploration']","['navigation', 'robot', 'robots', 'obstacles', 'planning', 'environments', 'legged', 'path', 'robotic', 'obstacle']","['  Autonomous robots exploring unknown environments face a significant\nchallenge: navigating effectively without prior maps and with limited external\nfeedback. This challenge intensifies in sparse reward environments, where\ntraditional exploration techniques often fail. In this paper, we present\nTopoNav, a novel topological navigation framework that integrates active\nmapping, hierarchical reinforcement learning, and intrinsic motivation to\nenable efficient goal-oriented exploration and navigation in sparse-reward\nsettings. TopoNav dynamically constructs a topological map of the environment,\ncapturing key locations and pathways. A two-level hierarchical policy\narchitecture, comprising a high-level graph traversal policy and low-level\nmotion control policies, enables effective navigation and obstacle avoidance\nwhile maintaining focus on the overall goal. Additionally, TopoNav incorporates\nintrinsic motivation to guide exploration toward relevant regions and frontier\nnodes in the topological map, addressing the challenges of sparse extrinsic\nrewards. We evaluate TopoNav both in the simulated and real-world off-road\nenvironments using a Clearpath Jackal robot, across three challenging\nnavigation scenarios: goal-reaching, feature-based navigation, and navigation\nin complex terrains. We observe an increase in exploration coverage by 7- 20%,\nin success rates by 9-19%, and reductions in navigation times by 15-36% across\nvarious scenarios, compared to state-of-the-art methods\n', ""  Recent results suggest that splitting topological navigation into\nrobot-independent and robot-specific components improves navigation performance\nby enabling the robot-independent part to be trained with data collected by\nrobots of different types. However, the navigation methods' performance is\nstill limited by the scarcity of suitable training data and they suffer from\npoor computational scaling.\n  In this work, we present PlaceNav, subdividing the robot-independent part\ninto navigation-specific and generic computer vision components. We utilize\nvisual place recognition for the subgoal selection of the topological\nnavigation pipeline. This makes subgoal selection more efficient and enables\nleveraging large-scale datasets from non-robotics sources, increasing training\ndata availability. Bayesian filtering, enabled by place recognition, further\nimproves navigation performance by increasing the temporal consistency of\nsubgoals. Our experimental results verify the design and the new method obtains\na 76% higher success rate in indoor and 23% higher in outdoor navigation tasks\nwith higher computational efficiency.\n"", '  Legged navigation is typically examined within open-world, off-road, and\nchallenging environments. In these scenarios, estimating external disturbances\nrequires a complex synthesis of multi-modal information. This underlines a\nmajor limitation in existing works that primarily focus on avoiding obstacles.\nIn this work, we propose TOP-Nav, a novel legged navigation framework that\nintegrates a comprehensive path planner with Terrain awareness, Obstacle\navoidance and close-loop Proprioception. TOP-Nav underscores the synergies\nbetween vision and proprioception in both path and motion planning. Within the\npath planner, we present and integrate a terrain estimator that enables the\nrobot to select waypoints on terrains with higher traversability while\neffectively avoiding obstacles. In the motion planning level, we not only\nimplement a locomotion controller to track the navigation commands, but also\nconstruct a proprioception advisor to provide motion evaluations for the path\nplanner. Based on the close-loop motion feedback, we make online corrections\nfor the vision-based terrain and obstacle estimations. Consequently, TOP-Nav\nachieves open-world navigation that the robot can handle terrains or\ndisturbances beyond the distribution of prior knowledge and overcomes\nconstraints imposed by visual conditions. Building upon extensive experiments\nconducted in both simulation and real-world environments, TOP-Nav demonstrates\nsuperior performance in open-world navigation compared to existing methods.\n']",Robot Navigation and Exploration in Unknown Environments,Robot Navigation and Locomotion,Robotics and Artificial Intelligence
102,102,76,102_recourses_recourse_algorithmic_unfairness,"['recourses', 'recourse', 'algorithmic', 'unfairness', 'algorithms', 'decisions', 'predictive', 'bias', 'allocation', 'fairness']","['fairness', 'recourse', 'algorithmic', 'fair', 'decisions', 'decision', 'unfairness', 'group', 'hiring', 'risk']","['  Algorithmic recourse provides explanations that help users overturn an\nunfavorable decision by a machine learning system. But so far very little\nattention has been paid to whether providing recourse is beneficial or not. We\nintroduce an abstract learning-theoretic framework that compares the risks\n(i.e., expected losses) for classification with and without algorithmic\nrecourse. This allows us to answer the question of when providing recourse is\nbeneficial or harmful at the population level. Surprisingly, we find that there\nare many plausible scenarios in which providing recourse turns out to be\nharmful, because it pushes users to regions of higher class uncertainty and\ntherefore leads to more mistakes. We further study whether the party deploying\nthe classifier has an incentive to strategize in anticipation of having to\nprovide recourse, and we find that sometimes they do, to the detriment of their\nusers. Providing algorithmic recourse may therefore also be harmful at the\nsystemic level. We confirm our theoretical findings in experiments on simulated\nand real-world data. All in all, we conclude that the current concept of\nalgorithmic recourse is not reliably beneficial, and therefore requires\nrethinking.\n', '  With the growing use of machine learning (ML) models in critical domains such\nas finance and healthcare, the need to offer recourse for those adversely\naffected by the decisions of ML models has become more important; individuals\nought to be provided with recommendations on actions to take for improving\ntheir situation and thus receiving a favorable decision. Prior work on\nsequential algorithmic recourse -- which recommends a series of changes --\nfocuses on action feasibility and uses the proximity of feature changes to\ndetermine action costs. However, the uncertainties of feature changes and the\nrisk of higher than average costs in recourse have not been considered. It is\nundesirable if a recourse could (with some probability) result in a worse\nsituation from which recovery requires an extremely high cost. It is essential\nto incorporate risks when computing and evaluating recourse. We call the\nrecourse computed with such risk considerations as Safe Algorithmic Recourse\n(SafeAR). The objective is to empower people to choose a recourse based on\ntheir risk tolerance. In this work, we discuss and show how existing recourse\ndesiderata can fail to capture the risk of higher costs. We present a method to\ncompute recourse policies that consider variability in cost and connect\nalgorithmic recourse literature with risk-sensitive reinforcement learning. We\nalso adopt measures ""Value at Risk"" and ""Conditional Value at Risk"" from the\nfinancial literature to summarize risk concisely. We apply our method to two\nreal-world datasets and compare policies with different risk-aversion levels\nusing risk measures and recourse desiderata (sparsity and proximity).\n', ""  Algorithmic recourse -- providing recommendations to those affected\nnegatively by the outcome of an algorithmic system on how they can take action\nand change that outcome -- has gained attention as a means of giving persons\nagency in their interactions with artificial intelligence (AI) systems. Recent\nwork has shown that even if an AI decision-making classifier is ``fair''\n(according to some reasonable criteria), recourse itself may be unfair due to\ndifferences in the initial circumstances of individuals, compounding\ndisparities for marginalized populations and requiring them to exert more\neffort than others. There is a need to define more methods and metrics for\nevaluating fairness in recourse that span a range of normative views of the\nworld, and specifically those that take into account time. Time is a critical\nelement in recourse because the longer it takes an individual to act, the more\nthe setting may change due to model or data drift.\n  This paper seeks to close this research gap by proposing two notions of\nfairness in recourse that are in normative alignment with substantive equality\nof opportunity, and that consider time. The first considers the (often\nrepeated) effort individuals exert per successful recourse event, and the\nsecond considers time per successful recourse event. Building upon an\nagent-based framework for simulating recourse, this paper demonstrates how much\neffort is needed to overcome disparities in initial circumstances. We then\nproposes an intervention to improve the fairness of recourse by rewarding\neffort, and compare it to existing strategies.\n""]",Algorithmic Recourse and Fairness,Fairness in Artificial Intelligence and Machine Learning,Fairness and Bias in Artificial Intelligence
103,103,75,103_biases_bias_biased_partisan,"['biases', 'bias', 'biased', 'partisan', 'debates', 'sentiment', 'propaganda', 'biasscanner', 'politically', 'political']","['political', 'news', 'media', 'bias', 'stance', 'biases', 'social', 'discourse', 'articles', 'detection']","[""  With the growth of online news over the past decade, empirical studies on\npolitical discourse and news consumption have focused on the phenomenon of\nfilter bubbles and echo chambers. Yet recently, scholars have revealed limited\nevidence around the impact of such phenomenon, leading some to argue that\npartisan segregation across news audiences cannot be fully explained by online\nnews consumption alone and that the role of traditional legacy media may be as\nsalient in polarizing public discourse around current events. In this work, we\nexpand the scope of analysis to include both online and more traditional media\nby investigating the relationship between broadcast news media language and\nsocial media discourse. By analyzing a decade's worth of closed captions (2\nmillion speaker turns) from CNN and Fox News along with topically corresponding\ndiscourse from Twitter, we provide a novel framework for measuring semantic\npolarization between America's two major broadcast networks to demonstrate how\nsemantic polarization between these outlets has evolved (Study 1), peaked\n(Study 2) and influenced partisan discussions on Twitter (Study 3) across the\nlast decade. Our results demonstrate a sharp increase in polarization in how\ntopically important keywords are discussed between the two channels, especially\nafter 2016, with overall highest peaks occurring in 2020. The two stations\ndiscuss identical topics in drastically distinct contexts in 2020, to the\nextent that there is barely any linguistic overlap in how identical keywords\nare contextually discussed. Further, we demonstrate at scale, how such partisan\ndivision in broadcast media language significantly shapes semantic polarity\ntrends on Twitter (and vice-versa), empirically linking for the first time, how\nonline discussions are influenced by televised media.\n"", '  We propose to measure political bias in LLMs by analyzing both the content\nand style of their generated content regarding political issues. Existing\nbenchmarks and measures focus on gender and racial biases. However, political\nbias exists in LLMs and can lead to polarization and other harms in downstream\napplications. In order to provide transparency to users, we advocate that there\nshould be fine-grained and explainable measures of political biases generated\nby LLMs. Our proposed measure looks at different political issues such as\nreproductive rights and climate change, at both the content (the substance of\nthe generation) and the style (the lexical polarity) of such bias. We measured\nthe political bias in eleven open-sourced LLMs and showed that our proposed\nframework is easily scalable to other topics and is explainable.\n', ""  Considerable efforts are currently underway to mitigate the negative impacts\nof echo chambers, such as increased susceptibility to fake news and resistance\ntowards accepting scientific evidence. Prior research has presented the\ndevelopment of computer systems that support the consumption of news\ninformation from diverse political perspectives to mitigate the echo chamber\neffect. However, existing studies still lack the ability to effectively support\nthe key processes of news information consumption and quantitatively identify a\npolitical stance towards the information. In this paper, we present HearHere,\nan AI-based web system designed to help users accommodate information and\nopinions from diverse perspectives. HearHere facilitates the key processes of\nnews information consumption through two visualizations. Visualization 1\nprovides political news with quantitative political stance information, derived\nfrom our graph-based political classification model, and users can experience\ndiverse perspectives (Hear). Visualization 2 allows users to express their\nopinions on specific political issues in a comment form and observe the\nposition of their own opinions relative to pro-liberal and pro-conservative\ncomments presented on a map interface (Here). Through a user study with 94\nparticipants, we demonstrate the feasibility of HearHere in supporting the\nconsumption of information from various perspectives. Our findings highlight\nthe importance of providing political stance information and quantifying users'\npolitical status as a means to mitigate political polarization. In addition, we\npropose design implications for system development, including the consideration\nof demographics such as political interest and providing users with\ninitiatives.\n""]",Media Bias and Political Polarization,Social Media and Information Dynamics,Information Dynamics and Network Influence
104,104,75,104_headlines_corpus_headline_nlp,"['headlines', 'corpus', 'headline', 'nlp', 'texts', 'newswire', 'newsserow', 'annotated', 'multilingual', 'corpora']","['news', 'articles', 'languages', 'headline', 'headlines', 'event', 'corpus', 'topic', 'summarization', 'entities']","['  With the rise of social media and online news sources, fake news has become a\nsignificant issue globally. However, the detection of fake news in low resource\nlanguages like Bengali has received limited attention in research. In this\npaper, we propose a methodology consisting of four distinct approaches to\nclassify fake news articles in Bengali using summarization and augmentation\ntechniques with five pre-trained language models. Our approach includes\ntranslating English news articles and using augmentation techniques to curb the\ndeficit of fake news articles. Our research also focused on summarizing the\nnews to tackle the token length limitation of BERT based models. Through\nextensive experimentation and rigorous evaluation, we show the effectiveness of\nsummarization and augmentation in the case of Bengali fake news detection. We\nevaluated our models using three separate test datasets. The BanglaBERT Base\nmodel, when combined with augmentation techniques, achieved an impressive\naccuracy of 96% on the first test dataset. On the second test dataset, the\nBanglaBERT model, trained with summarized augmented news articles achieved 97%\naccuracy. Lastly, the mBERT Base model achieved an accuracy of 86% on the third\ntest dataset which was reserved for generalization performance evaluation. The\ndatasets and implementations are available at\nhttps://github.com/arman-sakif/Bengali-Fake-News-Detection\n', '  Embedding news articles is a crucial tool for multiple fields, such as media\nbias detection, identifying fake news, and making news recommendations.\nHowever, existing news embedding methods are not optimized to capture the\nlatent context of news events. Most embedding methods rely on full-text\ninformation and neglect time-relevant embedding generation. In this paper, we\npropose a novel lightweight method that optimizes news embedding generation by\nfocusing on entities and themes mentioned in articles and their historical\nconnections to specific events. We suggest a method composed of three stages.\nFirst, we process and extract events, entities, and themes from the given news\narticles. Second, we generate periodic time embeddings for themes and entities\nby training time-separated GloVe models on current and historical data. Lastly,\nwe concatenate the news embeddings generated by two distinct approaches: Smooth\nInverse Frequency (SIF) for article-level vectors and Siamese Neural Networks\nfor embeddings with nuanced event-related information. We leveraged over\n850,000 news articles and 1,000,000 events from the GDELT project to test and\nevaluate our method. We conducted a comparative analysis of different news\nembedding generation methods for validation. Our experiments demonstrate that\nour approach can both improve and outperform state-of-the-art methods on shared\nevent detection tasks.\n', '  In this work, we introduce L3Cube-IndicNews, a multilingual text\nclassification corpus aimed at curating a high-quality dataset for Indian\nregional languages, with a specific focus on news headlines and articles. We\nhave centered our work on 10 prominent Indic languages, including Hindi,\nBengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and\nPunjabi. Each of these news datasets comprises 10 or more classes of news\narticles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle\ndifferent document lengths that are classified as: Short Headlines\nClassification (SHC) dataset containing the news headline and news category,\nLong Document Classification (LDC) dataset containing the whole news article\nand the news category, and Long Paragraph Classification (LPC) containing\nsub-articles of the news and the news category. We maintain consistent labeling\nacross all 3 datasets for in-depth length-based analysis. We evaluate each of\nthese Indic language datasets using 4 different models including monolingual\nBERT, multilingual Indic Sentence BERT (IndicSBERT), and IndicBERT. This\nresearch contributes significantly to expanding the pool of available text\nclassification datasets and also makes it possible to develop topic\nclassification models for Indian regional languages. This also serves as an\nexcellent resource for cross-lingual analysis owing to the high overlap of\nlabels among languages. The datasets and models are shared publicly at\nhttps://github.com/l3cube-pune/indic-nlp\n']",Multilingual News Classification and Fake News Detection,Misinformation and Disinformation Detection,Information Verification and Validation
105,105,75,105_planning_planner_pomdps_pomdp,"['planning', 'planner', 'pomdps', 'pomdp', 'reinforcement', 'rpomdps', 'markov', 'stochastic', 'optimal', 'pomcp']","['decision', 'planning', 'tree', 'observable', 'search', 'belief', 'value', 'policy', 'action', 'policies']","[""  Many sequential decision problems involve optimizing one objective function\nwhile imposing constraints on other objectives. Constrained Partially\nObservable Markov Decision Processes (C-POMDP) model this case with transition\nuncertainty and partial observability. In this work, we first show that\nC-POMDPs violate the optimal substructure property over successive decision\nsteps and thus may exhibit behaviors that are undesirable for some (e.g.,\nsafety critical) applications. Additionally, online re-planning in C-POMDPs is\noften ineffective due to the inconsistency resulting from this violation. To\naddress these drawbacks, we introduce the Recursively-Constrained POMDP\n(RC-POMDP), which imposes additional history-dependent cost constraints on the\nC-POMDP. We show that, unlike C-POMDPs, RC-POMDPs always have deterministic\noptimal policies and that optimal policies obey Bellman's principle of\noptimality. We also present a point-based dynamic programming algorithm for\nRC-POMDPs. Evaluations on benchmark problems demonstrate the efficacy of our\nalgorithm and show that policies for RC-POMDPs produce more desirable behaviors\nthan policies for C-POMDPs.\n"", '  To plan safely in uncertain environments, agents must balance utility with\nsafety constraints. Safe planning problems can be modeled as a\nchance-constrained partially observable Markov decision process (CC-POMDP) and\nsolutions often use expensive rollouts or heuristics to estimate the optimal\nvalue and action-selection policy. This work introduces the ConstrainedZero\npolicy iteration algorithm that solves CC-POMDPs in belief space by learning\nneural network approximations of the optimal value and policy with an\nadditional network head that estimates the failure probability given a belief.\nThis failure probability guides safe action selection during online Monte Carlo\ntree search (MCTS). To avoid overemphasizing search based on the failure\nestimates, we introduce $\\Delta$-MCTS, which uses adaptive conformal inference\nto update the failure threshold during planning. The approach is tested on a\nsafety-critical POMDP benchmark, an aircraft collision avoidance system, and\nthe sustainability problem of safe CO$_2$ storage. Results show that by\nseparating safety constraints from the objective we can achieve a target level\nof safety without optimizing the balance between rewards and costs.\n', '  Real-world planning problems, including autonomous driving and sustainable\nenergy applications like carbon storage and resource exploration, have recently\nbeen modeled as partially observable Markov decision processes (POMDPs) and\nsolved using approximate methods. To solve high-dimensional POMDPs in practice,\nstate-of-the-art methods use online planning with problem-specific heuristics\nto reduce planning horizons and make the problems tractable. Algorithms that\nlearn approximations to replace heuristics have recently found success in\nlarge-scale fully observable domains. The key insight is the combination of\nonline Monte Carlo tree search with offline neural network approximations of\nthe optimal policy and value function. In this work, we bring this insight to\npartially observable domains and propose BetaZero, a belief-state planning\nalgorithm for high-dimensional POMDPs. BetaZero learns offline approximations\nthat replace heuristics to enable online decision making in long-horizon\nproblems. We address several challenges inherent in large-scale partially\nobservable domains; namely challenges of transitioning in stochastic\nenvironments, prioritizing action branching with a limited search budget, and\nrepresenting beliefs as input to the network. To formalize the use of all\nlimited search information, we train against a novel $Q$-weighted visit counts\npolicy. We test BetaZero on various well-established POMDP benchmarks found in\nthe literature and a real-world problem of critical mineral exploration.\nExperiments show that BetaZero outperforms state-of-the-art POMDP solvers on a\nvariety of tasks.\n']",Planning under Uncertainty in POMDPs,Decision Making under Uncertainty in Markov Decision Processes,Decision Making and Optimization under Uncertainty
106,106,74,106_memorization_pretrained_tuning_trained,"['memorization', 'pretrained', 'tuning', 'trained', 'language', 'tuned', 'training', 'generating', 'tasks', 'examples']","['tuning', 'fine', 'instruction', 'prompt', 'language', 'tasks', 'performance', 'task', 'models', 'large']","['  High-quality instruction data is critical for aligning large language models\n(LLMs). Although some models, such as Llama-3-Instruct, have open weights,\ntheir alignment data remain private, which hinders the democratization of AI.\nHigh human labor costs and a limited, predefined scope for prompting prevent\nexisting open-source data creation methods from scaling effectively,\npotentially limiting the diversity and quality of public alignment datasets. Is\nit possible to synthesize high-quality instruction data at scale by extracting\nit directly from an aligned LLM? We present a self-synthesis method for\ngenerating large-scale alignment data named Magpie. Our key observation is that\naligned LLMs like Llama-3-Instruct can generate a user query when we input only\nthe left-side templates up to the position reserved for user messages, thanks\nto their auto-regressive nature. We use this method to prompt Llama-3-Instruct\nand generate 4 million instructions along with their corresponding responses.\nWe perform a comprehensive analysis of the extracted data and select 300K\nhigh-quality instances. To compare Magpie data with other public instruction\ndatasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the\nperformance of the fine-tuned models. Our results indicate that in some tasks,\nmodels fine-tuned with Magpie perform comparably to the official\nLlama-3-8B-Instruct, despite the latter being enhanced with 10 million data\npoints through supervised fine-tuning (SFT) and subsequent feedback learning.\nWe also show that using Magpie solely for SFT can surpass the performance of\nprevious public datasets utilized for both SFT and preference optimization,\nsuch as direct preference optimization with UltraFeedback. This advantage is\nevident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.\n', ""  Pretrained large language models (LLMs) are general purpose problem solvers\napplicable to a diverse set of tasks with prompts. They can be further improved\ntowards a specific task by fine-tuning on a specialized dataset. However,\nfine-tuning usually makes the model narrowly specialized on this dataset with\nreduced general in-context learning performances, which is undesirable whenever\nthe fine-tuned model needs to handle additional tasks where no fine-tuning data\nis available. In this work, we first demonstrate that fine-tuning on a single\ntask indeed decreases LLMs' general in-context learning performance. We\ndiscover one important cause of such forgetting, format specialization, where\nthe model overfits to the format of the fine-tuned task.We further show that\nformat specialization happens at the very beginning of fine-tuning. To solve\nthis problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet\neffective two-stage fine-tuning framework that reduces format specialization\nand improves generalization.ProMoT offloads task-specific format learning into\nadditional and removable parameters by first doing prompt tuning and then\nfine-tuning the model itself with this soft prompt attached. With experiments\non several fine-tuning tasks and 8 in-context evaluation tasks, we show that\nProMoT achieves comparable performance on fine-tuned tasks to standard\nfine-tuning, but with much less loss of in-context learning performances across\na board range of out-of-domain evaluation tasks. More importantly, ProMoT can\neven enhance generalization on in-context learning tasks that are semantically\nrelated to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly\nimproves performance on other language pairs, and ProMoT on NLI improves\nperformance on summarization. Experiments also show that ProMoT can improve the\ngeneralization performance of multi-task training.\n"", ""  While Large Language Models (LLMs) have demonstrated exceptional multitasking\nabilities, fine-tuning these models on downstream, domain-specific datasets is\noften necessary to yield superior performance on test sets compared to their\ncounterparts without fine-tuning. However, the comprehensive effects of\nfine-tuning on the LLMs' generalization ability are not fully understood. This\npaper delves into the differences between original, unmodified LLMs and their\nfine-tuned variants. Our primary investigation centers on whether fine-tuning\naffects the generalization ability intrinsic to LLMs. To elaborate on this, we\nconduct extensive experiments across five distinct language tasks on various\ndatasets. Our main findings reveal that models fine-tuned on generation and\nclassification tasks exhibit dissimilar behaviors in generalizing to different\ndomains and tasks. Intriguingly, we observe that integrating the in-context\nlearning strategy during fine-tuning on generation tasks can enhance the\nmodel's generalization ability. Through this systematic investigation, we aim\nto contribute valuable insights into the evolving landscape of fine-tuning\npractices for LLMs.\n""]",Fine-Tuning Large Language Models,Advances in Large Language Models,Large Language Models
106,106,74,106_memorization_pretrained_tuning_trained,"['memorization', 'pretrained', 'tuning', 'trained', 'language', 'tuned', 'training', 'generating', 'tasks', 'examples']","['tuning', 'fine', 'instruction', 'prompt', 'language', 'tasks', 'performance', 'task', 'models', 'large']","['  High-quality instruction data is critical for aligning large language models\n(LLMs). Although some models, such as Llama-3-Instruct, have open weights,\ntheir alignment data remain private, which hinders the democratization of AI.\nHigh human labor costs and a limited, predefined scope for prompting prevent\nexisting open-source data creation methods from scaling effectively,\npotentially limiting the diversity and quality of public alignment datasets. Is\nit possible to synthesize high-quality instruction data at scale by extracting\nit directly from an aligned LLM? We present a self-synthesis method for\ngenerating large-scale alignment data named Magpie. Our key observation is that\naligned LLMs like Llama-3-Instruct can generate a user query when we input only\nthe left-side templates up to the position reserved for user messages, thanks\nto their auto-regressive nature. We use this method to prompt Llama-3-Instruct\nand generate 4 million instructions along with their corresponding responses.\nWe perform a comprehensive analysis of the extracted data and select 300K\nhigh-quality instances. To compare Magpie data with other public instruction\ndatasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the\nperformance of the fine-tuned models. Our results indicate that in some tasks,\nmodels fine-tuned with Magpie perform comparably to the official\nLlama-3-8B-Instruct, despite the latter being enhanced with 10 million data\npoints through supervised fine-tuning (SFT) and subsequent feedback learning.\nWe also show that using Magpie solely for SFT can surpass the performance of\nprevious public datasets utilized for both SFT and preference optimization,\nsuch as direct preference optimization with UltraFeedback. This advantage is\nevident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.\n', ""  Pretrained large language models (LLMs) are general purpose problem solvers\napplicable to a diverse set of tasks with prompts. They can be further improved\ntowards a specific task by fine-tuning on a specialized dataset. However,\nfine-tuning usually makes the model narrowly specialized on this dataset with\nreduced general in-context learning performances, which is undesirable whenever\nthe fine-tuned model needs to handle additional tasks where no fine-tuning data\nis available. In this work, we first demonstrate that fine-tuning on a single\ntask indeed decreases LLMs' general in-context learning performance. We\ndiscover one important cause of such forgetting, format specialization, where\nthe model overfits to the format of the fine-tuned task.We further show that\nformat specialization happens at the very beginning of fine-tuning. To solve\nthis problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet\neffective two-stage fine-tuning framework that reduces format specialization\nand improves generalization.ProMoT offloads task-specific format learning into\nadditional and removable parameters by first doing prompt tuning and then\nfine-tuning the model itself with this soft prompt attached. With experiments\non several fine-tuning tasks and 8 in-context evaluation tasks, we show that\nProMoT achieves comparable performance on fine-tuned tasks to standard\nfine-tuning, but with much less loss of in-context learning performances across\na board range of out-of-domain evaluation tasks. More importantly, ProMoT can\neven enhance generalization on in-context learning tasks that are semantically\nrelated to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly\nimproves performance on other language pairs, and ProMoT on NLI improves\nperformance on summarization. Experiments also show that ProMoT can improve the\ngeneralization performance of multi-task training.\n"", ""  While Large Language Models (LLMs) have demonstrated exceptional multitasking\nabilities, fine-tuning these models on downstream, domain-specific datasets is\noften necessary to yield superior performance on test sets compared to their\ncounterparts without fine-tuning. However, the comprehensive effects of\nfine-tuning on the LLMs' generalization ability are not fully understood. This\npaper delves into the differences between original, unmodified LLMs and their\nfine-tuned variants. Our primary investigation centers on whether fine-tuning\naffects the generalization ability intrinsic to LLMs. To elaborate on this, we\nconduct extensive experiments across five distinct language tasks on various\ndatasets. Our main findings reveal that models fine-tuned on generation and\nclassification tasks exhibit dissimilar behaviors in generalizing to different\ndomains and tasks. Intriguingly, we observe that integrating the in-context\nlearning strategy during fine-tuning on generation tasks can enhance the\nmodel's generalization ability. Through this systematic investigation, we aim\nto contribute valuable insights into the evolving landscape of fine-tuning\npractices for LLMs.\n""]",Fine-Tuning Large Language Models,Advances in Large Language Models,Large Language Models
106,106,74,106_memorization_pretrained_tuning_trained,"['memorization', 'pretrained', 'tuning', 'trained', 'language', 'tuned', 'training', 'generating', 'tasks', 'examples']","['tuning', 'fine', 'instruction', 'prompt', 'language', 'tasks', 'performance', 'task', 'models', 'large']","['  High-quality instruction data is critical for aligning large language models\n(LLMs). Although some models, such as Llama-3-Instruct, have open weights,\ntheir alignment data remain private, which hinders the democratization of AI.\nHigh human labor costs and a limited, predefined scope for prompting prevent\nexisting open-source data creation methods from scaling effectively,\npotentially limiting the diversity and quality of public alignment datasets. Is\nit possible to synthesize high-quality instruction data at scale by extracting\nit directly from an aligned LLM? We present a self-synthesis method for\ngenerating large-scale alignment data named Magpie. Our key observation is that\naligned LLMs like Llama-3-Instruct can generate a user query when we input only\nthe left-side templates up to the position reserved for user messages, thanks\nto their auto-regressive nature. We use this method to prompt Llama-3-Instruct\nand generate 4 million instructions along with their corresponding responses.\nWe perform a comprehensive analysis of the extracted data and select 300K\nhigh-quality instances. To compare Magpie data with other public instruction\ndatasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the\nperformance of the fine-tuned models. Our results indicate that in some tasks,\nmodels fine-tuned with Magpie perform comparably to the official\nLlama-3-8B-Instruct, despite the latter being enhanced with 10 million data\npoints through supervised fine-tuning (SFT) and subsequent feedback learning.\nWe also show that using Magpie solely for SFT can surpass the performance of\nprevious public datasets utilized for both SFT and preference optimization,\nsuch as direct preference optimization with UltraFeedback. This advantage is\nevident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.\n', ""  Pretrained large language models (LLMs) are general purpose problem solvers\napplicable to a diverse set of tasks with prompts. They can be further improved\ntowards a specific task by fine-tuning on a specialized dataset. However,\nfine-tuning usually makes the model narrowly specialized on this dataset with\nreduced general in-context learning performances, which is undesirable whenever\nthe fine-tuned model needs to handle additional tasks where no fine-tuning data\nis available. In this work, we first demonstrate that fine-tuning on a single\ntask indeed decreases LLMs' general in-context learning performance. We\ndiscover one important cause of such forgetting, format specialization, where\nthe model overfits to the format of the fine-tuned task.We further show that\nformat specialization happens at the very beginning of fine-tuning. To solve\nthis problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet\neffective two-stage fine-tuning framework that reduces format specialization\nand improves generalization.ProMoT offloads task-specific format learning into\nadditional and removable parameters by first doing prompt tuning and then\nfine-tuning the model itself with this soft prompt attached. With experiments\non several fine-tuning tasks and 8 in-context evaluation tasks, we show that\nProMoT achieves comparable performance on fine-tuned tasks to standard\nfine-tuning, but with much less loss of in-context learning performances across\na board range of out-of-domain evaluation tasks. More importantly, ProMoT can\neven enhance generalization on in-context learning tasks that are semantically\nrelated to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly\nimproves performance on other language pairs, and ProMoT on NLI improves\nperformance on summarization. Experiments also show that ProMoT can improve the\ngeneralization performance of multi-task training.\n"", ""  While Large Language Models (LLMs) have demonstrated exceptional multitasking\nabilities, fine-tuning these models on downstream, domain-specific datasets is\noften necessary to yield superior performance on test sets compared to their\ncounterparts without fine-tuning. However, the comprehensive effects of\nfine-tuning on the LLMs' generalization ability are not fully understood. This\npaper delves into the differences between original, unmodified LLMs and their\nfine-tuned variants. Our primary investigation centers on whether fine-tuning\naffects the generalization ability intrinsic to LLMs. To elaborate on this, we\nconduct extensive experiments across five distinct language tasks on various\ndatasets. Our main findings reveal that models fine-tuned on generation and\nclassification tasks exhibit dissimilar behaviors in generalizing to different\ndomains and tasks. Intriguingly, we observe that integrating the in-context\nlearning strategy during fine-tuning on generation tasks can enhance the\nmodel's generalization ability. Through this systematic investigation, we aim\nto contribute valuable insights into the evolving landscape of fine-tuning\npractices for LLMs.\n""]",Fine-Tuning Large Language Models,Advances in Large Language Models,Large Language Models
107,107,74,107_ai_generative_openai_aied,"['ai', 'generative', 'openai', 'aied', 'creativity', 'intelligence', 'artificial', 'creators', 'software', 'openness']","['design', 'generative', 'developers', 'transparency', 'open', 'ecosystem', 'systems', 'practices', 'organizations', 'users']","['  Since late 2022, generative AI has taken the world by storm, with widespread\nuse of tools including ChatGPT, Gemini, and Claude. Generative AI and large\nlanguage model (LLM) applications are transforming how individuals find and\naccess data and knowledge. However, the intricate relationship between open\ndata and generative AI, and the vast potential it holds for driving innovation\nin this field remain underexplored areas. This white paper seeks to unpack the\nrelationship between open data and generative AI and explore possible\ncomponents of a new Fourth Wave of Open Data: Is open data becoming AI ready?\nIs open data moving towards a data commons approach? Is generative AI making\nopen data more conversational? Will generative AI improve open data quality and\nprovenance? Towards this end, we provide a new Spectrum of Scenarios framework.\nThis framework outlines a range of scenarios in which open data and generative\nAI could intersect and what is required from a data quality and provenance\nperspective to make open data ready for those specific scenarios. These\nscenarios include: pertaining, adaptation, inference and insight generation,\ndata augmentation, and open-ended exploration. Through this process, we found\nthat in order for data holders to embrace generative AI to improve open data\naccess and develop greater insights from open data, they first must make\nprogress around five key areas: enhance transparency and documentation, uphold\nquality and integrity, promote interoperability and standards, improve\naccessibility and useability, and address ethical considerations.\n', '  In the next few years, applications of Generative AI are expected to\nrevolutionize a number of different areas, ranging from science & medicine to\neducation. The potential for these seismic changes has triggered a lively\ndebate about potential risks and resulted in calls for tighter regulation, in\nparticular from some of the major tech companies who are leading in AI\ndevelopment. This regulation is likely to put at risk the budding field of\nopen-source Generative AI. We argue for the responsible open sourcing of\ngenerative AI models in the near and medium term. To set the stage, we first\nintroduce an AI openness taxonomy system and apply it to 40 current large\nlanguage models. We then outline differential benefits and risks of open versus\nclosed source AI and present potential risk mitigation, ranging from best\npractices to calls for technical and scientific contributions. We hope that\nthis report will add a much needed missing voice to the current public\ndiscourse on near to mid-term AI safety and other societal impact.\n', '  Generative AI applications present unique design challenges. As generative AI\ntechnologies are increasingly being incorporated into mainstream applications,\nthere is an urgent need for guidance on how to design user experiences that\nfoster effective and safe use. We present six principles for the design of\ngenerative AI applications that address unique characteristics of generative AI\nUX and offer new interpretations and extensions of known issues in the design\nof AI applications. Each principle is coupled with a set of design strategies\nfor implementing that principle via UX capabilities or through the design\nprocess. The principles and strategies were developed through an iterative\nprocess involving literature review, feedback from design practitioners,\nvalidation against real-world generative AI applications, and incorporation\ninto the design process of two generative AI applications. We anticipate the\nprinciples to usefully inform the design of generative AI applications by\ndriving actionable design recommendations.\n']",Generative AI and Open Data,Generative AI Applications and Implications,Generative Modeling and Artificial Intelligence
108,108,74,108_agents_reinforcement_agent_cooperative,"['agents', 'reinforcement', 'agent', 'cooperative', 'cooperation', 'learning', 'coordination', 'cooperate', 'collaborative', 'centralized']","['agent', 'agents', 'cooperative', 'decentralized', 'communication', 'graph', 'reinforcement', 'coordination', 'multi', 'cooperation']","['  Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research\nwithin the field of multi-agent systems. Several recent works have focused\nspecifically on the study of communication approaches in MARL. While multiple\ncommunication methods have been proposed, these might still be too complex and\nnot easily transferable to more practical contexts. One of the reasons for that\nis due to the use of the famous parameter sharing trick. In this paper, we\ninvestigate how independent learners in MARL that do not share parameters can\ncommunicate. We demonstrate that this setting might incur into some problems,\nto which we propose a new learning scheme as a solution. Our results show that,\ndespite the challenges, independent agents can still learn communication\nstrategies following our method. Additionally, we use this method to\ninvestigate how communication in MARL is affected by different network\ncapacities, both for sharing and not sharing parameters. We observe that\ncommunication may not always be needed and that the chosen agent network sizes\nneed to be considered when used together with communication in order to achieve\nefficient learning.\n', ""  Traditional multi-agent reinforcement learning algorithms are difficultly\napplied in a large-scale multi-agent environment. The introduction of mean\nfield theory has enhanced the scalability of multi-agent reinforcement learning\nin recent years. This paper considers partially observable multi-agent\nreinforcement learning (MARL), where each agent can only observe other agents\nwithin a fixed range. This partial observability affects the agent's ability to\nassess the quality of the actions of surrounding agents. This paper focuses on\ndeveloping a method to capture more effective information from local\nobservations in order to select more effective actions. Previous work in this\nfield employs probability distributions or weighted mean field to update the\naverage actions of neighborhood agents, but it does not fully consider the\nfeature information of surrounding neighbors and leads to a local optimum. In\nthis paper, we propose a novel multi-agent reinforcement learning algorithm,\nPartially Observable Mean Field Multi-Agent Reinforcement Learning based on\nGraph--Attention (GAMFQ) to remedy this flaw. GAMFQ uses a graph attention\nmodule and a mean field module to describe how an agent is influenced by the\nactions of other agents at each time step. This graph attention module consists\nof a graph attention encoder and a differentiable attention mechanism, and this\nmechanism outputs a dynamic graph to represent the effectiveness of\nneighborhood agents against central agents. The mean--field module approximates\nthe effect of a neighborhood agent on a central agent as the average effect of\neffective neighborhood agents. We evaluate GAMFQ on three challenging tasks in\nthe MAgents framework. Experiments show that GAMFQ outperforms baselines\nincluding the state-of-the-art partially observable mean-field reinforcement\nlearning algorithms.\n"", '  Achieving distributed reinforcement learning (RL) for large-scale cooperative\nmulti-agent systems (MASs) is challenging because: (i) each agent has access to\nonly limited information; (ii) issues on convergence or computational\ncomplexity emerge due to the curse of dimensionality. In this paper, we propose\na general computationally efficient distributed framework for cooperative\nmulti-agent reinforcement learning (MARL) by utilizing the structures of graphs\ninvolved in this problem. We introduce three coupling graphs describing three\ntypes of inter-agent couplings in MARL, namely, the state graph, the\nobservation graph and the reward graph. By further considering a communication\ngraph, we propose two distributed RL approaches based on local value-functions\nderived from the coupling graphs. The first approach is able to reduce sample\ncomplexity significantly under specific conditions on the aforementioned four\ngraphs. The second approach provides an approximate solution and can be\nefficient even for problems with dense coupling graphs. Here there is a\ntrade-off between minimizing the approximation error and reducing the\ncomputational complexity. Simulations show that our RL algorithms have a\nsignificantly improved scalability to large-scale MASs compared with\ncentralized and consensus-based distributed RL algorithms.\n']",Multi-Agent Reinforcement Learning,Multi-Agent Decision Making and Reinforcement Learning,Multi-Agent Systems and Artificial Intelligence
108,108,74,108_agents_reinforcement_agent_cooperative,"['agents', 'reinforcement', 'agent', 'cooperative', 'cooperation', 'learning', 'coordination', 'cooperate', 'collaborative', 'centralized']","['agent', 'agents', 'cooperative', 'decentralized', 'communication', 'graph', 'reinforcement', 'coordination', 'multi', 'cooperation']","['  Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research\nwithin the field of multi-agent systems. Several recent works have focused\nspecifically on the study of communication approaches in MARL. While multiple\ncommunication methods have been proposed, these might still be too complex and\nnot easily transferable to more practical contexts. One of the reasons for that\nis due to the use of the famous parameter sharing trick. In this paper, we\ninvestigate how independent learners in MARL that do not share parameters can\ncommunicate. We demonstrate that this setting might incur into some problems,\nto which we propose a new learning scheme as a solution. Our results show that,\ndespite the challenges, independent agents can still learn communication\nstrategies following our method. Additionally, we use this method to\ninvestigate how communication in MARL is affected by different network\ncapacities, both for sharing and not sharing parameters. We observe that\ncommunication may not always be needed and that the chosen agent network sizes\nneed to be considered when used together with communication in order to achieve\nefficient learning.\n', ""  Traditional multi-agent reinforcement learning algorithms are difficultly\napplied in a large-scale multi-agent environment. The introduction of mean\nfield theory has enhanced the scalability of multi-agent reinforcement learning\nin recent years. This paper considers partially observable multi-agent\nreinforcement learning (MARL), where each agent can only observe other agents\nwithin a fixed range. This partial observability affects the agent's ability to\nassess the quality of the actions of surrounding agents. This paper focuses on\ndeveloping a method to capture more effective information from local\nobservations in order to select more effective actions. Previous work in this\nfield employs probability distributions or weighted mean field to update the\naverage actions of neighborhood agents, but it does not fully consider the\nfeature information of surrounding neighbors and leads to a local optimum. In\nthis paper, we propose a novel multi-agent reinforcement learning algorithm,\nPartially Observable Mean Field Multi-Agent Reinforcement Learning based on\nGraph--Attention (GAMFQ) to remedy this flaw. GAMFQ uses a graph attention\nmodule and a mean field module to describe how an agent is influenced by the\nactions of other agents at each time step. This graph attention module consists\nof a graph attention encoder and a differentiable attention mechanism, and this\nmechanism outputs a dynamic graph to represent the effectiveness of\nneighborhood agents against central agents. The mean--field module approximates\nthe effect of a neighborhood agent on a central agent as the average effect of\neffective neighborhood agents. We evaluate GAMFQ on three challenging tasks in\nthe MAgents framework. Experiments show that GAMFQ outperforms baselines\nincluding the state-of-the-art partially observable mean-field reinforcement\nlearning algorithms.\n"", '  Achieving distributed reinforcement learning (RL) for large-scale cooperative\nmulti-agent systems (MASs) is challenging because: (i) each agent has access to\nonly limited information; (ii) issues on convergence or computational\ncomplexity emerge due to the curse of dimensionality. In this paper, we propose\na general computationally efficient distributed framework for cooperative\nmulti-agent reinforcement learning (MARL) by utilizing the structures of graphs\ninvolved in this problem. We introduce three coupling graphs describing three\ntypes of inter-agent couplings in MARL, namely, the state graph, the\nobservation graph and the reward graph. By further considering a communication\ngraph, we propose two distributed RL approaches based on local value-functions\nderived from the coupling graphs. The first approach is able to reduce sample\ncomplexity significantly under specific conditions on the aforementioned four\ngraphs. The second approach provides an approximate solution and can be\nefficient even for problems with dense coupling graphs. Here there is a\ntrade-off between minimizing the approximation error and reducing the\ncomputational complexity. Simulations show that our RL algorithms have a\nsignificantly improved scalability to large-scale MASs compared with\ncentralized and consensus-based distributed RL algorithms.\n']",Multi-Agent Reinforcement Learning,Multi-Agent Decision Making and Reinforcement Learning,Multi-Agent Systems and Artificial Intelligence
109,109,73,109_estimation_estimators_optimal_estimator,"['estimation', 'estimators', 'optimal', 'estimator', 'lasso', 'robust', 'outliers', 'outlier', 'unbiased', 'empirical']","['estimator', 'regression', 'sample', 'linear', 'asymptotic', 'complexity', 'estimation', 'statistical', 'log', 'estimators']","['  We consider the problem of linear regression with self-selection bias in the\nunknown-index setting, as introduced in recent work by Cherapanamjeri,\nDaskalakis, Ilyas, and Zampetakis [STOC 2023]. In this model, one observes $m$\ni.i.d. samples $(\\mathbf{x}_{\\ell},z_{\\ell})_{\\ell=1}^m$ where\n$z_{\\ell}=\\max_{i\\in [k]}\\{\\mathbf{x}_{\\ell}^T\\mathbf{w}_i+\\eta_{i,\\ell}\\}$,\nbut the maximizing index $i_{\\ell}$ is unobserved. Here, the\n$\\mathbf{x}_{\\ell}$ are assumed to be $\\mathcal{N}(0,I_n)$ and the noise\ndistribution $\\mathbf{\\eta}_{\\ell}\\sim \\mathcal{D}$ is centered and independent\nof $\\mathbf{x}_{\\ell}$. We provide a novel and near optimally sample-efficient\n(in terms of $k$) algorithm to recover $\\mathbf{w}_1,\\ldots,\\mathbf{w}_k\\in\n\\mathbb{R}^n$ up to additive $\\ell_2$-error $\\varepsilon$ with polynomial\nsample complexity $\\tilde{O}(n)\\cdot \\mathsf{poly}(k,1/\\varepsilon)$ and\nsignificantly improved time complexity\n$\\mathsf{poly}(n,k,1/\\varepsilon)+O(\\log(k)/\\varepsilon)^{O(k)}$. When\n$k=O(1)$, our algorithm runs in $\\mathsf{poly}(n,1/\\varepsilon)$ time,\ngeneralizing the polynomial guarantee of an explicit moment matching algorithm\nof Cherapanamjeri, et al. for $k=2$ and when it is known that\n$\\mathcal{D}=\\mathcal{N}(0,I_k)$. Our algorithm succeeds under significantly\nrelaxed noise assumptions, and therefore also succeeds in the related setting\nof max-linear regression where the added noise is taken outside the maximum.\nFor this problem, our algorithm is efficient in a much larger range of $k$ than\nthe state-of-the-art due to Ghosh, Pananjady, Guntuboyina, and Ramchandran\n[IEEE Trans. Inf. Theory 2022] for not too small $\\varepsilon$, and leads to\nimproved algorithms for any $\\varepsilon$ by providing a warm start for\nexisting local convergence methods.\n', ""  We study the problem of robust multivariate polynomial regression: let\n$p\\colon\\mathbb{R}^n\\to\\mathbb{R}$ be an unknown $n$-variate polynomial of\ndegree at most $d$ in each variable. We are given as input a set of random\nsamples $(\\mathbf{x}_i,y_i) \\in [-1,1]^n \\times \\mathbb{R}$ that are noisy\nversions of $(\\mathbf{x}_i,p(\\mathbf{x}_i))$. More precisely, each\n$\\mathbf{x}_i$ is sampled independently from some distribution $\\chi$ on\n$[-1,1]^n$, and for each $i$ independently, $y_i$ is arbitrary (i.e., an\noutlier) with probability at most $\\rho < 1/2$, and otherwise satisfies\n$|y_i-p(\\mathbf{x}_i)|\\leq\\sigma$. The goal is to output a polynomial\n$\\hat{p}$, of degree at most $d$ in each variable, within an\n$\\ell_\\infty$-distance of at most $O(\\sigma)$ from $p$.\n  Kane, Karmalkar, and Price [FOCS'17] solved this problem for $n=1$. We\ngeneralize their results to the $n$-variate setting, showing an algorithm that\nachieves a sample complexity of $O_n(d^n\\log d)$, where the hidden constant\ndepends on $n$, if $\\chi$ is the $n$-dimensional Chebyshev distribution. The\nsample complexity is $O_n(d^{2n}\\log d)$, if the samples are drawn from the\nuniform distribution instead. The approximation error is guaranteed to be at\nmost $O(\\sigma)$, and the run-time depends on $\\log(1/\\sigma)$. In the setting\nwhere each $\\mathbf{x}_i$ and $y_i$ are known up to $N$ bits of precision, the\nrun-time's dependence on $N$ is linear. We also show that our sample\ncomplexities are optimal in terms of $d^n$. Furthermore, we show that it is\npossible to have the run-time be independent of $1/\\sigma$, at the cost of a\nhigher sample complexity.\n"", '  We consider the multivariate max-linear regression problem where the model\nparameters\n$\\boldsymbol{\\beta}_{1},\\dotsc,\\boldsymbol{\\beta}_{k}\\in\\mathbb{R}^{p}$ need to\nbe estimated from $n$ independent samples of the (noisy) observations $y =\n\\max_{1\\leq j \\leq k} \\boldsymbol{\\beta}_{j}^{\\mathsf{T}} \\boldsymbol{x} +\n\\mathrm{noise}$. The max-linear model vastly generalizes the conventional\nlinear model, and it can approximate any convex function to an arbitrary\naccuracy when the number of linear models $k$ is large enough. However, the\ninherent nonlinearity of the max-linear model renders the estimation of the\nregression parameters computationally challenging. Particularly, no estimator\nbased on convex programming is known in the literature. We formulate and\nanalyze a scalable convex program given by anchored regression (AR) as the\nestimator for the max-linear regression problem. Under the standard Gaussian\nobservation setting, we present a non-asymptotic performance guarantee showing\nthat the convex program recovers the parameters with high probability. When the\n$k$ linear components are equally likely to achieve the maximum, our result\nshows a sufficient number of noise-free observations for exact recovery scales\nas {$k^{4}p$} up to a logarithmic factor. { This sample complexity coincides\nwith that by alternating minimization (Ghosh et al., {2021}). Moreover, the\nsame sample complexity applies when the observations are corrupted with\narbitrary deterministic noise. We provide empirical results that show that our\nmethod performs as our theoretical result predicts, and is competitive with the\nalternating minimization algorithm particularly in presence of multiplicative\nBernoulli noise. Furthermore, we also show empirically that a recursive\napplication of AR can significantly improve the estimation accuracy.}\n']",Robust Regression Estimation,Robust Regression Methods,Advanced Statistical and Machine Learning Methods
110,110,72,110_energy_buildings_forecasting_buildingsbench,"['energy', 'buildings', 'forecasting', 'buildingsbench', 'predicting', 'forecast', 'prediction', 'hvac', 'predict', 'forecasts']","['energy', 'buildings', 'forecasting', 'building', 'consumption', 'load', 'power', 'comfort', 'thermal', 'residential']","['  Energy prediction in buildings plays a crucial role in effective energy\nmanagement. Precise predictions are essential for achieving optimal energy\nconsumption and distribution within the grid. This paper introduces a Long\nShort-Term Memory (LSTM) model designed to forecast building energy consumption\nusing historical energy data, occupancy patterns, and weather conditions. The\nLSTM model provides accurate short, medium, and long-term energy predictions\nfor residential and commercial buildings compared to existing prediction\nmodels. We compare our LSTM model with established prediction methods,\nincluding linear regression, decision trees, and random forest. Encouragingly,\nthe proposed LSTM model emerges as the superior performer across all metrics.\nIt demonstrates exceptional prediction accuracy, boasting the highest R2 score\nof 0.97 and the most favorable mean absolute error (MAE) of 0.007. An\nadditional advantage of our developed model is its capacity to achieve\nefficient energy consumption forecasts even when trained on a limited dataset.\nWe address concerns about overfitting (variance) and underfitting (bias)\nthrough rigorous training and evaluation on real-world data. In summary, our\nresearch contributes to energy prediction by offering a robust LSTM model that\noutperforms alternative methods and operates with remarkable efficiency,\ngeneralizability, and reliability.\n', '  Buildings energy efficiency is a widely researched topic, which is rapidly\ngaining popularity due to rising environmental concerns and the need for energy\nindependence. In Northern Europe heating energy alone accounts for up to 70\npercent of the total building energy consumption. Industry 4.0 technologies\nsuch as IoT, big data, cloud computing and machine learning, along with the\ncreation of predictive and proactive digital twins, can help to reduce this\nnumber. However, buildings thermal dynamics is a very complex process that\ndepends on many variables. As a result, commonly used physics-based white box\nmodels are time-consuming and require vast expertise. On the contrary, black\nbox forecasting models, which rely primarily on building energy consumption\ndata, lack fundamental insights and hinder re-use. In this study we propose an\narchitecture to facilitate grey box modelling of building thermal dynamics\nwhile integrating real time IoT data with 3D representation of buildings. The\narchitecture is validated in a case study creating a digital twin platform that\nenables users to define the thermal dynamics of buildings based on physical\nlaws and real data, thus facilitating informed decision making for the best\nheating energy optimization strategy. Also, the created user interface enables\nstakeholders such as facility managers, energy providers or governing bodies to\nanalyse, compare and evaluate buildings thermal dynamics without extensive\nexpertise or time resources.\n', '  In the context of global sustainability, buildings are significant consumers\nof energy, emphasizing the necessity for innovative strategies to enhance\nefficiency and reduce environmental impact. This research leverages extensive\nraw data from building infrastructures to uncover energy consumption patterns\nand devise strategies for optimizing resource use. We investigate the factors\ninfluencing energy efficiency and cost reduction in buildings, utilizing Lasso\nRegression, Decision Tree, and Random Forest models for accurate energy use\nforecasting. Our study delves into the factors affecting energy utilization,\nfocusing on primary fuel and electrical energy, and discusses the potential for\nsubstantial cost savings and environmental benefits. Significantly, we apply\nmetaheuristic techniques to enhance the Decision Tree algorithm, resulting in\nimproved predictive precision. This enables a more nuanced understanding of the\ncharacteristics of buildings with high and low energy efficiency potential. Our\nfindings offer practical insights for reducing energy consumption and\noperational costs, contributing to the broader goals of sustainable development\nand cleaner production. By identifying key drivers of energy use in buildings,\nthis study provides a valuable framework for policymakers and industry\nstakeholders to implement cleaner and more sustainable energy practices.\n']",Energy Forecasting in Buildings,Energy Forecasting and Management,Predictive Modeling and Forecasting
111,111,72,111_optimizers_optimization_optimize_hyperparameters,"['optimizers', 'optimization', 'optimize', 'hyperparameters', 'minimization', 'hyperparameter', 'minimizing', 'optimal', 'objectives', 'constraints']","['optimization', 'problems', 'objective', 'objectives', 'convex', 'hyperparameter', 'constraints', 'constrained', 'algorithms', 'convergence']","['  Solving multi-objective optimization problems for large deep neural networks\nis a challenging task due to the complexity of the loss landscape and the\nexpensive computational cost of training and evaluating models. Efficient\nPareto front approximation of large models enables multi-objective optimization\nfor various tasks such as multi-task learning and trade-off analysis. Existing\nalgorithms for learning Pareto set, including (1) evolutionary, hypernetworks,\nand hypervolume-maximization methods, are computationally expensive and have\nrestricted scalability to large models; (2) Scalarization algorithms, where a\nseparate model is trained for each objective ray, which is inefficient for\nlearning the entire Pareto set and fails to capture the objective trade-offs\neffectively. Inspired by the recent success of model merging, we propose a\npractical and scalable approach to Pareto set learning problem via mixture of\nexperts (MoE) based model fusion. By ensembling the weights of specialized\nsingle-task models, the MoE module can effectively capture the trade-offs\nbetween multiple objectives and closely approximate the entire Pareto set of\nlarge neural networks. Once the routers are learned and a preference vector is\nset, the MoE module can be unloaded, thus no additional computational cost is\nintroduced during inference. We conduct extensive experiments on vision and\nlanguage tasks using large-scale models such as CLIP-ViT and GPT-2. The\nexperimental results demonstrate that our method efficiently approximates the\nentire Pareto front of large models. Using only hundreds of trainable\nparameters of the MoE routers, our method even has lower memory usage compared\nto linear scalarization and algorithms that learn a single Pareto optimal\nsolution, and are scalable to both the number of objectives and the size of the\nmodel.\n', ""  In the pursuit of efficient optimization of expensive-to-evaluate systems,\nthis paper investigates a novel approach to Bayesian multi-objective and\nmulti-fidelity (MOMF) optimization. Traditional optimization methods, while\neffective, often encounter prohibitively high costs in multi-dimensional\noptimizations of one or more objectives. Multi-fidelity approaches offer\npotential remedies by utilizing multiple, less costly information sources, such\nas low-resolution simulations. However, integrating these two strategies\npresents a significant challenge. We suggest the innovative use of a trust\nmetric to support simultaneous optimization of multiple objectives and data\nsources. Our method modifies a multi-objective optimization policy to\nincorporate the trust gain per evaluation cost as one objective in a Pareto\noptimization problem, enabling simultaneous MOMF at lower costs. We present and\ncompare two MOMF optimization methods: a holistic approach selecting both the\ninput parameters and the trust parameter jointly, and a sequential approach for\nbenchmarking. Through benchmarks on synthetic test functions, our approach is\nshown to yield significant cost reductions - up to an order of magnitude\ncompared to pure multi-objective optimization. Furthermore, we find that joint\noptimization of the trust and objective domains outperforms addressing them in\nsequential manner. We validate our results using the use case of optimizing\nlaser-plasma acceleration simulations, demonstrating our method's potential in\nPareto optimization of high-cost black-box functions. Implementing these\nmethods in existing Bayesian frameworks is simple, and they can be readily\nextended to batch optimization. With their capability to handle various\ncontinuous or discrete fidelity dimensions, our techniques offer broad\napplicability in solving simulation problems in fields such as plasma physics\nand fluid dynamics.\n"", '  This paper focuses on integrating the networks and adversarial training into\nconstrained optimization problems to develop a framework algorithm for\nconstrained optimization problems. For such problems, we first transform them\ninto minimax problems using the augmented Lagrangian method and then use two\n(or several) deep neural networks(DNNs) to represent the primal and dual\nvariables respectively. The parameters in the neural networks are then trained\nby an adversarial process. The proposed architecture is relatively insensitive\nto the scale of values of different constraints when compared to penalty based\ndeep learning methods. Through this type of training, the constraints are\nimposed better based on the augmented Lagrangian multipliers. Extensive\nexamples for optimization problems with scalar constraints, nonlinear\nconstraints, partial differential equation constraints, and inequality\nconstraints are considered to show the capability and robustness of the\nproposed method, with applications ranging from Ginzburg--Landau energy\nminimization problems, partition problems, fluid-solid topology optimization,\nto obstacle problems.\n']",Multi-Objective Optimization Methods,Multi-Objective Optimization Techniques,Optimization and Design
111,111,72,111_optimizers_optimization_optimize_hyperparameters,"['optimizers', 'optimization', 'optimize', 'hyperparameters', 'minimization', 'hyperparameter', 'minimizing', 'optimal', 'objectives', 'constraints']","['optimization', 'problems', 'objective', 'objectives', 'convex', 'hyperparameter', 'constraints', 'constrained', 'algorithms', 'convergence']","['  Solving multi-objective optimization problems for large deep neural networks\nis a challenging task due to the complexity of the loss landscape and the\nexpensive computational cost of training and evaluating models. Efficient\nPareto front approximation of large models enables multi-objective optimization\nfor various tasks such as multi-task learning and trade-off analysis. Existing\nalgorithms for learning Pareto set, including (1) evolutionary, hypernetworks,\nand hypervolume-maximization methods, are computationally expensive and have\nrestricted scalability to large models; (2) Scalarization algorithms, where a\nseparate model is trained for each objective ray, which is inefficient for\nlearning the entire Pareto set and fails to capture the objective trade-offs\neffectively. Inspired by the recent success of model merging, we propose a\npractical and scalable approach to Pareto set learning problem via mixture of\nexperts (MoE) based model fusion. By ensembling the weights of specialized\nsingle-task models, the MoE module can effectively capture the trade-offs\nbetween multiple objectives and closely approximate the entire Pareto set of\nlarge neural networks. Once the routers are learned and a preference vector is\nset, the MoE module can be unloaded, thus no additional computational cost is\nintroduced during inference. We conduct extensive experiments on vision and\nlanguage tasks using large-scale models such as CLIP-ViT and GPT-2. The\nexperimental results demonstrate that our method efficiently approximates the\nentire Pareto front of large models. Using only hundreds of trainable\nparameters of the MoE routers, our method even has lower memory usage compared\nto linear scalarization and algorithms that learn a single Pareto optimal\nsolution, and are scalable to both the number of objectives and the size of the\nmodel.\n', ""  In the pursuit of efficient optimization of expensive-to-evaluate systems,\nthis paper investigates a novel approach to Bayesian multi-objective and\nmulti-fidelity (MOMF) optimization. Traditional optimization methods, while\neffective, often encounter prohibitively high costs in multi-dimensional\noptimizations of one or more objectives. Multi-fidelity approaches offer\npotential remedies by utilizing multiple, less costly information sources, such\nas low-resolution simulations. However, integrating these two strategies\npresents a significant challenge. We suggest the innovative use of a trust\nmetric to support simultaneous optimization of multiple objectives and data\nsources. Our method modifies a multi-objective optimization policy to\nincorporate the trust gain per evaluation cost as one objective in a Pareto\noptimization problem, enabling simultaneous MOMF at lower costs. We present and\ncompare two MOMF optimization methods: a holistic approach selecting both the\ninput parameters and the trust parameter jointly, and a sequential approach for\nbenchmarking. Through benchmarks on synthetic test functions, our approach is\nshown to yield significant cost reductions - up to an order of magnitude\ncompared to pure multi-objective optimization. Furthermore, we find that joint\noptimization of the trust and objective domains outperforms addressing them in\nsequential manner. We validate our results using the use case of optimizing\nlaser-plasma acceleration simulations, demonstrating our method's potential in\nPareto optimization of high-cost black-box functions. Implementing these\nmethods in existing Bayesian frameworks is simple, and they can be readily\nextended to batch optimization. With their capability to handle various\ncontinuous or discrete fidelity dimensions, our techniques offer broad\napplicability in solving simulation problems in fields such as plasma physics\nand fluid dynamics.\n"", '  This paper focuses on integrating the networks and adversarial training into\nconstrained optimization problems to develop a framework algorithm for\nconstrained optimization problems. For such problems, we first transform them\ninto minimax problems using the augmented Lagrangian method and then use two\n(or several) deep neural networks(DNNs) to represent the primal and dual\nvariables respectively. The parameters in the neural networks are then trained\nby an adversarial process. The proposed architecture is relatively insensitive\nto the scale of values of different constraints when compared to penalty based\ndeep learning methods. Through this type of training, the constraints are\nimposed better based on the augmented Lagrangian multipliers. Extensive\nexamples for optimization problems with scalar constraints, nonlinear\nconstraints, partial differential equation constraints, and inequality\nconstraints are considered to show the capability and robustness of the\nproposed method, with applications ranging from Ginzburg--Landau energy\nminimization problems, partition problems, fluid-solid topology optimization,\nto obstacle problems.\n']",Multi-Objective Optimization Methods,Multi-Objective Optimization Techniques,Optimization and Design
112,112,71,112_multimodal_modality_attention_semantic,"['multimodal', 'modality', 'attention', 'semantic', 'modal', 'linking', 'semantics', 'visual', 'retrieval', 'linkage']","['multimodal', 'modal', 'modality', 'modalities', 'entity', 'retrieval', 'cross', 'text', 'grained', 'audio']","['  Semantic location prediction aims to derive meaningful location insights from\nmultimodal social media posts, offering a more contextual understanding of\ndaily activities than using GPS coordinates. This task faces significant\nchallenges due to the noise and modality heterogeneity in ""text-image"" posts.\nExisting methods are generally constrained by inadequate feature\nrepresentations and modal interaction, struggling to effectively reduce noise\nand modality heterogeneity. To address these challenges, we propose a\nSimilarity-Guided Multimodal Fusion Transformer (SG-MFT) for predicting the\nsemantic locations of users from their multimodal posts. First, we incorporate\nhigh-quality text and image representations by utilizing a pre-trained large\nvision-language model. Then, we devise a Similarity-Guided Interaction Module\n(SIM) to alleviate modality heterogeneity and noise interference by\nincorporating both coarse-grained and fine-grained similarity guidance for\nimproving modality interactions. Specifically, we propose a novel\nsimilarity-aware feature interpolation attention mechanism at the\ncoarse-grained level, leveraging modality-wise similarity to mitigate\nheterogeneity and reduce noise within each modality. At the fine-grained level,\nwe utilize a similarity-aware feed-forward block and element-wise similarity to\nfurther address the issue of modality heterogeneity. Finally, building upon\npre-processed features with minimal noise and modal interference, we devise a\nSimilarity-aware Fusion Module (SFM) to fuse two modalities with a\ncross-attention mechanism. Comprehensive experimental results clearly\ndemonstrate the superior performance of our proposed method.\n', '  Multimodal Entity Linking (MEL) is a crucial task that aims at linking\nambiguous mentions within multimodal contexts to the referent entities in a\nmultimodal knowledge base, such as Wikipedia. Existing methods focus heavily on\nusing complex mechanisms and extensive model tuning methods to model the\nmultimodal interaction on specific datasets. However, these methods\novercomplicate the MEL task and overlook the visual semantic information, which\nmakes them costly and hard to scale. Moreover, these methods can not solve the\nissues like textual ambiguity, redundancy, and noisy images, which severely\ndegrade their performance. Fortunately, the advent of Large Language Models\n(LLMs) with robust capabilities in text understanding and reasoning,\nparticularly Multimodal Large Language Models (MLLMs) that can process\nmultimodal inputs, provides new insights into addressing this challenge.\nHowever, how to design a universally applicable LLMs-based MEL approach remains\na pressing challenge. To this end, we propose UniMEL, a unified framework which\nestablishes a new paradigm to process multimodal entity linking tasks using\nLLMs. In this framework, we employ LLMs to augment the representation of\nmentions and entities individually by integrating textual and visual\ninformation and refining textual information. Subsequently, we employ the\nembedding-based method for retrieving and re-ranking candidate entities. Then,\nwith only ~0.26% of the model parameters fine-tuned, LLMs can make the final\nselection from the candidate entities. Extensive experiments on three public\nbenchmark datasets demonstrate that our solution achieves state-of-the-art\nperformance, and ablation studies verify the effectiveness of all modules. Our\ncode is available at https://anonymous.4open.science/r/UniMEL/.\n', '  Mining structured knowledge from tweets using named entity recognition (NER)\ncan be beneficial for many down stream applications such as recommendation and\nintention understanding. With tweet posts tending to be multimodal, multimodal\nnamed entity recognition (MNER) has attracted more attention. In this paper, we\npropose a novel approach, which can dynamically align the image and text\nsequence and achieve the multi-level cross-modal learning to augment textual\nword representation for MNER improvement. To be specific, our framework can be\nsplit into three main stages: the first stage focuses on intra-modality\nrepresentation learning to derive the implicit global and local knowledge of\neach modality, the second evaluates the relevance between the text and its\naccompanying image and integrates different grained visual information based on\nthe relevance, the third enforces semantic refinement via iterative cross-modal\ninteractions and co-attention. We conduct experiments on two open datasets, and\nthe results and detailed analysis demonstrate the advantage of our model.\n']",Multimodal Semantic Analysis and Linking,Multimodal Learning and Applications,Multimodal Learning and Applications
113,113,71,113_cnn_segmentation_objects_instance,"['cnn', 'segmentation', 'objects', 'instance', 'detection', 'vision', 'images', 'masks', 'detectors', 'mask']","['object', 'segmentation', 'detection', 'objects', 'detectors', 'bounding', 'images', 'instance', 'image', 'synthetic']","['  Multi-class multi-instance segmentation is the task of identifying masks for\nmultiple object classes and multiple instances of the same class within an\nimage. The foundational Segment Anything Model (SAM) is designed for promptable\nmulti-class multi-instance segmentation but tends to output part or sub-part\nmasks in the ""everything"" mode for various real-world applications. Whole\nobject segmentation masks play a crucial role for indoor scene understanding,\nespecially in robotics applications. We propose a new domain invariant\nReal-to-Simulation (Real-Sim) fine-tuning strategy for SAM. We use object\nimages and ground truth data collected from Ai2Thor simulator during\nfine-tuning (real-to-sim). To allow our Segment Any Object Model (SAOM) to work\nin the ""everything"" mode, we propose the novel nearest neighbour assignment\nmethod, updating point embeddings for each ground-truth mask. SAOM is evaluated\non our own dataset collected from Ai2Thor simulator. SAOM significantly\nimproves on SAM, with a 28% increase in mIoU and a 25% increase in mAcc for 54\nfrequently-seen indoor object classes. Moreover, our Real-to-Simulation\nfine-tuning strategy demonstrates promising generalization performance in real\nenvironments without being trained on the real-world data (sim-to-real). The\ndataset and the code will be released after publication.\n', ""  Instance segmentation is a form of image detection which has a range of\napplications, such as object refinement, medical image analysis, and\nimage/video editing, all of which demand a high degree of accuracy. However,\nthis precision is often beyond the reach of what even state-of-the-art, fully\nautomated instance segmentation algorithms can deliver. The performance gap\nbecomes particularly prohibitive for small and complex objects. Practitioners\ntypically resort to fully manual annotation, which can be a laborious process.\nIn order to overcome this problem, we propose a novel approach to enable more\nprecise predictions and generate higher-quality segmentation masks for\nhigh-curvature, complex and small-scale objects. Our human-assisted\nsegmentation model, HAISTA-NET, augments the existing Strong Mask R-CNN network\nto incorporate human-specified partial boundaries. We also present a dataset of\nhand-drawn partial object boundaries, which we refer to as human attention\nmaps. In addition, the Partial Sketch Object Boundaries (PSOB) dataset contains\nhand-drawn partial object boundaries which represent curvatures of an object's\nground truth mask with several pixels. Through extensive evaluation using the\nPSOB dataset, we show that HAISTA-NET outperforms state-of-the art methods such\nas Mask R-CNN, Strong Mask R-CNN, and Mask2Former, achieving respective\nincreases of +36.7, +29.6, and +26.5 points in AP-Mask metrics for these three\nmodels. We hope that our novel approach will set a baseline for future\nhuman-aided deep learning models by combining fully automated and interactive\ninstance segmentation architectures.\n"", '  Recently, foundation models trained on massive datasets to adapt to a wide\nrange of domains have attracted considerable attention and are actively being\nexplored within the computer vision community. Among these, the Segment\nAnything Model (SAM) stands out for its remarkable progress in generalizability\nand flexibility for image segmentation tasks, achieved through prompt-based\nobject mask generation. However, despite its strength, SAM faces two key\nlimitations when applied to customized instance segmentation that segments\nspecific objects or those in unique environments not typically present in the\ntraining data: 1) the ambiguity inherent in input prompts and 2) the necessity\nfor extensive additional training to achieve optimal segmentation. To address\nthese challenges, we propose a novel method, customized instance segmentation\nvia prompt learning tailored to SAM. Our method involves a prompt learning\nmodule (PLM), which adjusts input prompts into the embedding space to better\nalign with user intentions, thereby enabling more efficient training.\nFurthermore, we introduce a point matching module (PMM) to enhance the feature\nrepresentation for finer segmentation by ensuring detailed alignment with\nground truth boundaries. Experimental results on various customized instance\nsegmentation scenarios demonstrate the effectiveness of the proposed method.\n']",Instance Segmentation Models,Weakly Supervised Computer Vision,Computer Vision
114,114,71,114_chatbots_chatbot_ai_conversational,"['chatbots', 'chatbot', 'ai', 'conversational', 'intelligence', 'bots', 'communication', 'social', 'human', 'intentions']","['social', 'chatbots', 'consumers', 'impacts', 'chatbot', 'human', 'content', 'participants', 'ethical', 'media']","[""  Advances in generative AI (GenAI) have raised concerns about detecting and\ndiscerning AI-generated content from human-generated content. Most existing\nliterature assumes a paradigm where 'expert' organized disinformation creators\nand flawed AI models deceive 'ordinary' users. Based on longitudinal\nethnographic research with misinformation creators and consumers between\n2022-2023, we instead find that GenAI supports bricolage work, where\nnon-experts increasingly use GenAI to remix, repackage, and (re)produce content\nto meet their personal needs and desires. This research yielded four key\nfindings: First, participants primarily used GenAI for creation, rather than\ntruth-seeking. Second, a spreading 'influencer millionaire' narrative drove\nparticipants to become content creators, using GenAI as a productivity tool to\ngenerate a volume of (often misinformative) content. Third, GenAI lowered the\nbarrier to entry for content creation across modalities, enticing consumers to\nbecome creators and significantly increasing existing creators' output.\nFinally, participants used Gen AI to learn and deploy marketing tactics to\nexpand engagement and monetize their content. We argue for shifting analysis\nfrom the public as consumers of AI content to bricoleurs who use GenAI\ncreatively, often without a detailed understanding of its underlying\ntechnology. We analyze how these understudied emergent uses of GenAI produce\nnew or accelerated misinformation harms, and their implications for AI\nproducts, platforms and policies.\n"", '  Artificial intelligence based chatbots have brought unprecedented business\npotential. This study aims to explore consumers trust and response to a\ntext-based chatbot in ecommerce, involving the moderating effects of task\ncomplexity and chatbot identity disclosure. A survey method with 299 useable\nresponses was conducted in this research. This study adopted the ordinary least\nsquares regression to test the hypotheses. First, the consumers perception of\nboth the empathy and friendliness of the chatbot positively impacts their trust\nin it. Second, task complexity negatively moderates the relationship between\nfriendliness and consumers trust. Third, disclosure of the text based chatbot\nnegatively moderates the relationship between empathy and consumers trust,\nwhile it positively moderates the relationship between friendliness and\nconsumers trust. Fourth, consumers trust in the chatbot increases their\nreliance on the chatbot and decreases their resistance to the chatbot in future\ninteractions. Adopting the stimulus organism response framework, this study\nprovides important insights on consumers perception and response to the\ntext-based chatbot. The findings of this research also make suggestions that\ncan increase consumers positive responses to text based chatbots. Extant\nstudies have investigated the effects of automated bots attributes on consumers\nperceptions. However, the boundary conditions of these effects are largely\nignored. This research is one of the first attempts to provide a deep\nunderstanding of consumers responses to a chatbot.\n', ""  As artificial intelligence (AI) becomes more widespread, one question that\narises is how human-AI interaction might impact human-human interaction.\nChatbots, for example, are increasingly used as social companions, and while\nmuch is speculated, little is known empirically about how their use impacts\nhuman relationships. A common hypothesis is that relationships with companion\nchatbots are detrimental to social health by harming or replacing human\ninteraction, but this hypothesis may be too simplistic, especially considering\nthe social needs of users and the health of their preexisting human\nrelationships. To understand how relationships with companion chatbots impact\nsocial health, we studied people who regularly used companion chatbots and\npeople who did not use them. Contrary to expectations, companion chatbot users\nindicated that these relationships were beneficial to their social health,\nwhereas non-users viewed them as harmful. Another common assumption is that\npeople perceive conscious, humanlike AI as disturbing and threatening. Among\nboth users and non-users, however, we found the opposite: perceiving companion\nchatbots as more conscious and humanlike correlated with more positive opinions\nand more pronounced social health benefits. Detailed accounts from users\nsuggested that these humanlike chatbots may aid social health by supplying\nreliable and safe interactions, without necessarily harming human\nrelationships, but this may depend on users' preexisting social needs and how\nthey perceive both human likeness and mind in the chatbot.\n""]",Human-AI Interaction and Chatbots,Conversational AI and Chatbots,Conversational AI and Human-Computer Interaction
115,115,71,115_nonlinear_fractional_sparse_solvers,"['nonlinear', 'fractional', 'sparse', 'solvers', 'odes', 'differential', 'pdes', 'rnns', 'solver', 'numerical']","['differential', 'equations', 'fractional', 'neural', 'nonlinear', 'equation', 'numerical', 'operator', 'dynamical', 'partial']","['  This paper presents a novel operational matrix method to accelerate the\ntraining of fractional Physics-Informed Neural Networks (fPINNs). Our approach\ninvolves a non-uniform discretization of the fractional Caputo operator,\nfacilitating swift computation of fractional derivatives within Caputo-type\nfractional differential problems with $0<\\alpha<1$. In this methodology, the\noperational matrix is precomputed, and during the training phase, automatic\ndifferentiation is replaced with a matrix-vector product. While our methodology\nis compatible with any network, we particularly highlight its successful\nimplementation in PINNs, emphasizing the enhanced accuracy achieved when\nutilizing the Legendre Neural Block (LNB) architecture. LNB incorporates\nLegendre polynomials into the PINN structure, providing a significant boost in\naccuracy. The effectiveness of our proposed method is validated across diverse\ndifferential equations, including Delay Differential Equations (DDEs) and\nSystems of Differential Algebraic Equations (DAEs). To demonstrate its\nversatility, we extend the application of the method to systems of differential\nequations, specifically addressing nonlinear Pantograph fractional-order\nDDEs/DAEs. The results are supported by a comprehensive analysis of numerical\noutcomes.\n', '  The sparse identification of nonlinear dynamical systems (SINDy) is a\ndata-driven technique employed for uncovering and representing the fundamental\ndynamics of intricate systems based on observational data. However, a primary\nobstacle in the discovery of models for nonlinear partial differential\nequations (PDEs) lies in addressing the challenges posed by the curse of\ndimensionality and large datasets. Consequently, the strategic selection of the\nmost informative samples within a given dataset plays a crucial role in\nreducing computational costs and enhancing the effectiveness of SINDy-based\nalgorithms. To this aim, we employ a greedy sampling approach to the snapshot\nmatrix of a PDE to obtain its valuable samples, which are suitable to train a\ndeep neural network (DNN) in a SINDy framework. SINDy based algorithms often\nconsist of a data collection unit, constructing a dictionary of basis\nfunctions, computing the time derivative, and solving a sparse identification\nproblem which ends to regularised least squares minimization. In this paper, we\nextend the results of a SINDy based deep learning model discovery (DeePyMoD)\napproach by integrating greedy sampling technique in its data collection unit\nand new sparsity promoting algorithms in the least squares minimization unit.\nIn this regard we introduce the greedy sampling neural network in sparse\nidentification of nonlinear partial differential equations (GN-SINDy) which\nblends a greedy sampling method, the DNN, and the SINDy algorithm. In the\nimplementation phase, to show the effectiveness of GN-SINDy, we compare its\nresults with DeePyMoD by using a Python package that is prepared for this\npurpose on numerous PDE discovery\n', ""  We introduce an innovative approach for solving high-dimensional\nFokker-Planck-L\\'evy (FPL) equations in modeling non-Brownian processes across\ndisciplines such as physics, finance, and ecology. We utilize a fractional\nscore function and Physical-informed neural networks (PINN) to lift the curse\nof dimensionality (CoD) and alleviate numerical overflow from exponentially\ndecaying solutions with dimensions. The introduction of a fractional score\nfunction allows us to transform the FPL equation into a second-order partial\ndifferential equation without fractional Laplacian and thus can be readily\nsolved with standard physics-informed neural networks (PINNs). We propose two\nmethods to obtain a fractional score function: fractional score matching (FSM)\nand score-fPINN for fitting the fractional score function. While FSM is more\ncost-effective, it relies on known conditional distributions. On the other\nhand, score-fPINN is independent of specific stochastic differential equations\n(SDEs) but requires evaluating the PINN model's derivatives, which may be more\ncostly. We conduct our experiments on various SDEs and demonstrate numerical\nstability and effectiveness of our method in dealing with high-dimensional\nproblems, marking a significant advancement in addressing the CoD in FPL\nequations.\n""]",Numerical Solvers for Nonlinear Fractional Differential Equations,Nonlinear Dynamics and Differential Equations Analysis,Machine Learning for Dynamical Systems and Differential Equations
116,116,71,116_planning_reinforcement_planner_maze,"['planning', 'reinforcement', 'planner', 'maze', 'robotics', 'exploration', 'robot', 'autonomous', 'robotic', 'learning']","['planning', 'robot', 'policies', 'reinforcement', 'goal', 'environments', 'robots', 'path', 'policy', 'action']","['  The Value Iteration Network (VIN) is an end-to-end differentiable\narchitecture that performs value iteration on a latent MDP for planning in\nreinforcement learning (RL). However, VINs struggle to scale to long-term and\nlarge-scale planning tasks, such as navigating a $100\\times 100$ maze -- a task\nwhich typically requires thousands of planning steps to solve. We observe that\nthis deficiency is due to two issues: the representation capacity of the latent\nMDP and the planning module\'s depth. We address these by augmenting the latent\nMDP with a dynamic transition kernel, dramatically improving its\nrepresentational capacity, and, to mitigate the vanishing gradient problem,\nintroducing an ""adaptive highway loss"" that constructs skip connections to\nimprove gradient flow. We evaluate our method on both 2D maze navigation\nenvironments and the ViZDoom 3D navigation benchmark. We find that our new\nmethod, named Dynamic Transition VIN (DT-VIN), easily scales to 5000 layers and\ncasually solves challenging versions of the above tasks. Altogether, we believe\nthat DT-VIN represents a concrete step forward in performing long-term\nlarge-scale planning in RL environments.\n', ""  This work investigates the potential of Reinforcement Learning (RL) to tackle\nrobot motion planning challenges in the dynamic RoboCup Small Size League\n(SSL). Using a heuristic control approach, we evaluate RL's effectiveness in\nobstacle-free and single-obstacle path-planning environments. Ablation studies\nreveal significant performance improvements. Our method achieved a 60% time\ngain in obstacle-free environments compared to baseline algorithms.\nAdditionally, our findings demonstrated dynamic obstacle avoidance\ncapabilities, adeptly navigating around moving blocks. These findings highlight\nthe potential of RL to enhance robot motion planning in the challenging and\nunpredictable SSL environment.\n"", '  General-purpose agents require fine-grained controls and rich sensory inputs\nto perform a wide range of tasks. However, this complexity often leads to\nintractable decision-making. Traditionally, agents are provided with\ntask-specific action and observation spaces to mitigate this challenge, but\nthis reduces autonomy. Instead, agents must be capable of building state-action\nspaces at the correct abstraction level from their sensorimotor experiences. We\nleverage the structure of a given set of temporally-extended actions to learn\nabstract Markov decision processes (MDPs) that operate at a higher level of\ntemporal and state granularity. We characterize state abstractions necessary to\nensure that planning with these skills, by simulating trajectories in the\nabstract MDP, results in policies with bounded value loss in the original MDP.\nWe evaluate our approach in goal-based navigation environments that require\ncontinuous abstract states to plan successfully and show that abstract model\nlearning improves the sample efficiency of planning and learning.\n']",Reinforcement Learning for Planning and Robotics,Reinforcement Learning Methods and Applications,Reinforcement Learning
116,116,71,116_planning_reinforcement_planner_maze,"['planning', 'reinforcement', 'planner', 'maze', 'robotics', 'exploration', 'robot', 'autonomous', 'robotic', 'learning']","['planning', 'robot', 'policies', 'reinforcement', 'goal', 'environments', 'robots', 'path', 'policy', 'action']","['  The Value Iteration Network (VIN) is an end-to-end differentiable\narchitecture that performs value iteration on a latent MDP for planning in\nreinforcement learning (RL). However, VINs struggle to scale to long-term and\nlarge-scale planning tasks, such as navigating a $100\\times 100$ maze -- a task\nwhich typically requires thousands of planning steps to solve. We observe that\nthis deficiency is due to two issues: the representation capacity of the latent\nMDP and the planning module\'s depth. We address these by augmenting the latent\nMDP with a dynamic transition kernel, dramatically improving its\nrepresentational capacity, and, to mitigate the vanishing gradient problem,\nintroducing an ""adaptive highway loss"" that constructs skip connections to\nimprove gradient flow. We evaluate our method on both 2D maze navigation\nenvironments and the ViZDoom 3D navigation benchmark. We find that our new\nmethod, named Dynamic Transition VIN (DT-VIN), easily scales to 5000 layers and\ncasually solves challenging versions of the above tasks. Altogether, we believe\nthat DT-VIN represents a concrete step forward in performing long-term\nlarge-scale planning in RL environments.\n', ""  This work investigates the potential of Reinforcement Learning (RL) to tackle\nrobot motion planning challenges in the dynamic RoboCup Small Size League\n(SSL). Using a heuristic control approach, we evaluate RL's effectiveness in\nobstacle-free and single-obstacle path-planning environments. Ablation studies\nreveal significant performance improvements. Our method achieved a 60% time\ngain in obstacle-free environments compared to baseline algorithms.\nAdditionally, our findings demonstrated dynamic obstacle avoidance\ncapabilities, adeptly navigating around moving blocks. These findings highlight\nthe potential of RL to enhance robot motion planning in the challenging and\nunpredictable SSL environment.\n"", '  General-purpose agents require fine-grained controls and rich sensory inputs\nto perform a wide range of tasks. However, this complexity often leads to\nintractable decision-making. Traditionally, agents are provided with\ntask-specific action and observation spaces to mitigate this challenge, but\nthis reduces autonomy. Instead, agents must be capable of building state-action\nspaces at the correct abstraction level from their sensorimotor experiences. We\nleverage the structure of a given set of temporally-extended actions to learn\nabstract Markov decision processes (MDPs) that operate at a higher level of\ntemporal and state granularity. We characterize state abstractions necessary to\nensure that planning with these skills, by simulating trajectories in the\nabstract MDP, results in policies with bounded value loss in the original MDP.\nWe evaluate our approach in goal-based navigation environments that require\ncontinuous abstract states to plan successfully and show that abstract model\nlearning improves the sample efficiency of planning and learning.\n']",Reinforcement Learning for Planning and Robotics,Reinforcement Learning Methods and Applications,Reinforcement Learning
117,117,71,117_retrieval_conversational_conversation_chatbots,"['retrieval', 'conversational', 'conversation', 'chatbots', 'conversations', 'dialogue', 'chatbot', 'dialogues', 'dialog', 'assistant']","['dialogue', 'conversational', 'retrieval', 'user', 'conversation', 'memory', 'response', 'responses', 'generation', 'persona']","['  Conversational retrieval refers to an information retrieval system that\noperates in an iterative and interactive manner, requiring the retrieval of\nvarious external resources, such as persona, knowledge, and even response, to\neffectively engage with the user and successfully complete the dialogue.\nHowever, most previous work trained independent retrievers for each specific\nresource, resulting in sub-optimal performance and low efficiency. Thus, we\npropose a multi-task framework function as a universal retriever for three\ndominant retrieval tasks during the conversation: persona selection, knowledge\nselection, and response selection. To this end, we design a dual-encoder\narchitecture consisting of a context-adaptive dialogue encoder and a candidate\nencoder, aiming to attention to the relevant context from the long dialogue and\nretrieve suitable candidates by simply a dot product. Furthermore, we introduce\ntwo loss constraints to capture the subtle relationship between dialogue\ncontext and different candidates by regarding historically selected candidates\nas hard negatives. Extensive experiments and analysis establish\nstate-of-the-art retrieval quality both within and outside its training domain,\nrevealing the promising potential and generalization capability of our model to\nserve as a universal retriever for different candidate selection tasks\nsimultaneously.\n', ""  Despite the success of integrating large language models into the development\nof conversational systems, many studies have shown the effectiveness of\nretrieving and augmenting external knowledge for informative responses. Hence,\nmany existing studies commonly assume the always need for Retrieval Augmented\nGeneration (RAG) in a conversational system without explicit control. This\nraises a research question about such a necessity. In this study, we propose to\ninvestigate the need for each turn of system response to be augmented with\nexternal knowledge. In particular, by leveraging human judgements on the binary\nchoice of adaptive augmentation, we develop RAGate, a gating model, which\nmodels conversation context and relevant inputs to predict if a conversational\nsystem requires RAG for improved responses. We conduct extensive experiments on\ndevising and applying RAGate to conversational models and well-rounded analyses\nof different conversational scenarios. Our experimental results and analysis\nindicate the effective application of RAGate in RAG-based conversational\nsystems in identifying system responses for appropriate RAG with high-quality\nresponses and a high generation confidence. This study also identifies the\ncorrelation between the generation's confidence level and the relevance of the\naugmented knowledge.\n"", '  Retrieval-Augmented Generation (RAG) aims to generate more reliable and\naccurate responses, by augmenting large language models (LLMs) with the\nexternal vast and dynamic knowledge. Most previous work focuses on using RAG\nfor single-round question answering, while how to adapt RAG to the complex\nconversational setting wherein the question is interdependent on the preceding\ncontext is not well studied. In this paper, we propose a conversation-level RAG\napproach, which incorporates fine-grained retrieval augmentation and self-check\nfor conversational question answering (CQA). In particular, our approach\nconsists of three components, namely conversational question refiner,\nfine-grained retriever and self-check based response generator, which work\ncollaboratively for question understanding and relevant information acquisition\nin conversational settings. Extensive experiments demonstrate the great\nadvantages of our approach over the state-of-the-art baselines. Moreover, we\nalso release a Chinese CQA dataset with new features including reformulated\nquestion, extracted keyword, retrieved paragraphs and their helpfulness, which\nfacilitates further researches in RAG enhanced CQA.\n']",Conversational Retrieval and Response Generation,Conversational AI and Empathy in Human-Computer Interaction,Conversational AI and Human-Computer Interaction
118,118,70,118_dehazing_haze_blur_convolutional,"['dehazing', 'haze', 'blur', 'convolutional', 'encoder', 'vision', 'deblur', 'deep', 'hazy', 'res2net']","['dehazing', 'image', 'restoration', 'images', 'resolution', 'super', 'hexagonal', 'haze', 'convolutional', 'local']","['  Blind single image super-resolution (SISR) is a challenging task in image\nprocessing due to the ill-posed nature of the inverse problem. Complex\ndegradations present in real life images make it difficult to solve this\nproblem using na\\""ive deep learning approaches, where models are often trained\non synthetically generated image pairs. Most of the effort so far has been\nfocused on solving the inverse problem under some constraints, such as for a\nlimited space of blur kernels and/or assuming noise-free input images. Yet,\nthere is a gap in the literature to provide a well-generalized deep\nlearning-based solution that performs well on images with unknown and highly\ncomplex degradations. In this paper, we propose IKR-Net (Iterative Kernel\nReconstruction Network) for blind SISR. In the proposed approach, kernel and\nnoise estimation and high-resolution image reconstruction are carried out\niteratively using dedicated deep models. The iterative refinement provides\nsignificant improvement in both the reconstructed image and the estimated blur\nkernel even for noisy inputs. IKR-Net provides a generalized solution that can\nhandle any type of blur and level of noise in the input low-resolution image.\nIKR-Net achieves state-of-the-art results in blind SISR, especially for noisy\nimages with motion blur.\n', ""  Hazy images degrade visual quality, and dehazing is a crucial prerequisite\nfor subsequent processing tasks. Most current dehazing methods rely on neural\nnetworks and face challenges such as high computational parameter pressure and\nweak generalization capabilities. This paper introduces PriorNet--a novel,\nlightweight, and highly applicable dehazing network designed to significantly\nimprove the clarity and visual quality of hazy images while avoiding excessive\ndetail extraction issues. The core of PriorNet is the original\nMulti-Dimensional Interactive Attention (MIA) mechanism, which effectively\ncaptures a wide range of haze characteristics, substantially reducing the\ncomputational load and generalization difficulties associated with complex\nsystems. By utilizing a uniform convolutional kernel size and incorporating\nskip connections, we have streamlined the feature extraction process.\nSimplifying the number of layers and architecture not only enhances dehazing\nefficiency but also facilitates easier deployment on edge devices. Extensive\ntesting across multiple datasets has demonstrated PriorNet's exceptional\nperformance in dehazing and clarity restoration, maintaining image detail and\ncolor fidelity in single-image dehazing tasks. Notably, with a model size of\njust 18Kb, PriorNet showcases superior dehazing generalization capabilities\ncompared to other methods. Our research makes a significant contribution to\nadvancing image dehazing technology, providing new perspectives and tools for\nthe field and related domains, particularly emphasizing the importance of\nimproving universality and deployability.\n"", '  In recent years, as computer vision tasks have increasingly relied on\nhigh-quality image inputs, the task of image dehazing has received significant\nattention. Previously, many methods based on priors and deep learning have been\nproposed to address the task of image dehazing. Ignoring the domain gap between\ndifferent data, former de-hazing methods usually adopt multiple datasets for\nexplicit training, which often makes the methods themselves be violated. To\naddress this problem, we propose a novel method of internal and external data\naugmentation to improve the existing dehazing methodology. By using cross-data\nexternal augmentor. The dataset inherits samples from different domains that\nare firmly aligned, making the model learn more robust and generalizable\nfeatures. By using the internal data augmentation method, the model can fully\nexploit local information within the images, thereby obtaining more image\ndetails. To demonstrate the effectiveness of our proposed method, we conduct\ntraining on both the Natural Image Dataset (NID) and the Remote Sensing Image\nDataset (RSID). Experimental results show that our method clearly resolves the\ndomain gap in different dehazing datasets and presents a new pipeline for joint\ntraining in the dehazing task. Our approach significantly outperforms other\nadvanced methods in dehazing and produces dehazed images that are closest to\nreal haze-free images. The code will be available at:\nhttps://github.com/wengzp1/ScaleUpDehazing\n']",Image Dehazing and Super-Resolution,Image Processing and Enhancement,Image and Video Processing
119,119,70,119_programming_program_generate_pseudocode,"['programming', 'program', 'generate', 'pseudocode', 'abstractions', 'synthesis', 'algorithmic', 'programs', 'automata', 'solvers']","['program', 'programs', 'synthesis', 'logic', 'search', 'symbolic', 'programming', 'code', 'automata', 'reasoning']","[""  Program synthesis with language models (LMs) has unlocked a large set of\nreasoning abilities; code-tuned LMs have proven adept at generating programs\nthat solve a wide variety of algorithmic symbolic manipulation tasks (e.g. word\nconcatenation). However, not all reasoning tasks are easily expressible as\ncode, e.g. tasks involving commonsense reasoning, moral decision-making, and\nsarcasm understanding. Our goal is to extend an LM's program synthesis skills\nto such tasks and evaluate the results via pseudo-programs, namely Python\nprograms where some leaf function calls are left undefined. To that end, we\npropose, Code Generation and Emulated EXecution (CoGEX). CoGEX works by (1)\ntraining LMs to generate their own pseudo-programs, (2) teaching them to\nemulate their generated program's execution, including those leaf functions,\nallowing the LM's knowledge to fill in the execution gaps; and (3) using them\nto search over many programs to find an optimal one. To adapt the CoGEX model\nto a new task, we introduce a method for performing program search to find a\nsingle program whose pseudo-execution yields optimal performance when applied\nto all the instances of a given dataset. We show that our approach yields large\nimprovements compared to standard in-context learning approaches on a battery\nof tasks, both algorithmic and soft reasoning. This result thus demonstrates\nthat code synthesis can be applied to a much broader class of problems than\npreviously considered. Our released dataset, fine-tuned models, and\nimplementation can be found at \\url{https://github.com/nweir127/CoGEX}.\n"", '  Recently, program synthesis driven by large language models (LLMs) has become\nincreasingly popular. However, program synthesis for machine learning (ML)\ntasks still poses significant challenges. This paper explores a novel form of\nprogram synthesis, targeting ML programs, by combining LLMs and automated\nmachine learning (autoML). Specifically, our goal is to fully automate the\ngeneration and optimization of the code of the entire ML workflow, from data\npreparation to modeling and post-processing, utilizing only textual\ndescriptions of the ML tasks. To manage the length and diversity of ML\nprograms, we propose to break each ML program into smaller, manageable parts.\nEach part is generated separately by the LLM, with careful consideration of\ntheir compatibilities. To ensure compatibilities, we design a testing technique\nfor ML programs. Unlike traditional program synthesis, which typically relies\non binary evaluations (i.e., correct or incorrect), evaluating ML programs\nnecessitates more than just binary judgments. Therefore, we further assess ML\nprograms numerically and select the optimal programs from a range of candidates\nusing AutoML methods. In experiments across various ML tasks, our method\noutperforms existing methods in 10 out of 12 tasks for generating ML programs.\nIn addition, autoML significantly improves the performance of the generated ML\nprograms. In experiments, given the textual task description, our method,\nText-to-ML, generates the complete and optimized ML program in a fully\nautonomous process.\n', '  The Abstraction and Reasoning Corpus (ARC) is a general artificial\nintelligence benchmark that is currently unsolvable by any Machine Learning\nmethod, including Large Language Models (LLMs). It demands strong\ngeneralization and reasoning capabilities which are known to be weaknesses of\nNeural Network based systems. In this work, we propose a Program Synthesis\nsystem that uses Inductive Logic Programming (ILP), a branch of Symbolic AI, to\nsolve ARC. We have manually defined a simple Domain Specific Language (DSL)\nthat corresponds to a small set of object-centric abstractions relevant to ARC.\nThis is the Background Knowledge used by ILP to create Logic Programs that\nprovide reasoning capabilities to our system. The full system is capable of\ngeneralize to unseen tasks, since ILP can create Logic Program(s) from few\nexamples, in the case of ARC: pairs of Input-Output grids examples for each\ntask. These Logic Programs are able to generate Objects present in the Output\ngrid and the combination of these can form a complete program that transforms\nan Input grid into an Output grid. We randomly chose some tasks from ARC that\ndont require more than the small number of the Object primitives we implemented\nand show that given only these, our system can solve tasks that require each,\nsuch different reasoning.\n']",Program Synthesis and Generation with Language Models,Advances in Large Language Models,Large Language Models
119,119,70,119_programming_program_generate_pseudocode,"['programming', 'program', 'generate', 'pseudocode', 'abstractions', 'synthesis', 'algorithmic', 'programs', 'automata', 'solvers']","['program', 'programs', 'synthesis', 'logic', 'search', 'symbolic', 'programming', 'code', 'automata', 'reasoning']","[""  Program synthesis with language models (LMs) has unlocked a large set of\nreasoning abilities; code-tuned LMs have proven adept at generating programs\nthat solve a wide variety of algorithmic symbolic manipulation tasks (e.g. word\nconcatenation). However, not all reasoning tasks are easily expressible as\ncode, e.g. tasks involving commonsense reasoning, moral decision-making, and\nsarcasm understanding. Our goal is to extend an LM's program synthesis skills\nto such tasks and evaluate the results via pseudo-programs, namely Python\nprograms where some leaf function calls are left undefined. To that end, we\npropose, Code Generation and Emulated EXecution (CoGEX). CoGEX works by (1)\ntraining LMs to generate their own pseudo-programs, (2) teaching them to\nemulate their generated program's execution, including those leaf functions,\nallowing the LM's knowledge to fill in the execution gaps; and (3) using them\nto search over many programs to find an optimal one. To adapt the CoGEX model\nto a new task, we introduce a method for performing program search to find a\nsingle program whose pseudo-execution yields optimal performance when applied\nto all the instances of a given dataset. We show that our approach yields large\nimprovements compared to standard in-context learning approaches on a battery\nof tasks, both algorithmic and soft reasoning. This result thus demonstrates\nthat code synthesis can be applied to a much broader class of problems than\npreviously considered. Our released dataset, fine-tuned models, and\nimplementation can be found at \\url{https://github.com/nweir127/CoGEX}.\n"", '  Recently, program synthesis driven by large language models (LLMs) has become\nincreasingly popular. However, program synthesis for machine learning (ML)\ntasks still poses significant challenges. This paper explores a novel form of\nprogram synthesis, targeting ML programs, by combining LLMs and automated\nmachine learning (autoML). Specifically, our goal is to fully automate the\ngeneration and optimization of the code of the entire ML workflow, from data\npreparation to modeling and post-processing, utilizing only textual\ndescriptions of the ML tasks. To manage the length and diversity of ML\nprograms, we propose to break each ML program into smaller, manageable parts.\nEach part is generated separately by the LLM, with careful consideration of\ntheir compatibilities. To ensure compatibilities, we design a testing technique\nfor ML programs. Unlike traditional program synthesis, which typically relies\non binary evaluations (i.e., correct or incorrect), evaluating ML programs\nnecessitates more than just binary judgments. Therefore, we further assess ML\nprograms numerically and select the optimal programs from a range of candidates\nusing AutoML methods. In experiments across various ML tasks, our method\noutperforms existing methods in 10 out of 12 tasks for generating ML programs.\nIn addition, autoML significantly improves the performance of the generated ML\nprograms. In experiments, given the textual task description, our method,\nText-to-ML, generates the complete and optimized ML program in a fully\nautonomous process.\n', '  The Abstraction and Reasoning Corpus (ARC) is a general artificial\nintelligence benchmark that is currently unsolvable by any Machine Learning\nmethod, including Large Language Models (LLMs). It demands strong\ngeneralization and reasoning capabilities which are known to be weaknesses of\nNeural Network based systems. In this work, we propose a Program Synthesis\nsystem that uses Inductive Logic Programming (ILP), a branch of Symbolic AI, to\nsolve ARC. We have manually defined a simple Domain Specific Language (DSL)\nthat corresponds to a small set of object-centric abstractions relevant to ARC.\nThis is the Background Knowledge used by ILP to create Logic Programs that\nprovide reasoning capabilities to our system. The full system is capable of\ngeneralize to unseen tasks, since ILP can create Logic Program(s) from few\nexamples, in the case of ARC: pairs of Input-Output grids examples for each\ntask. These Logic Programs are able to generate Objects present in the Output\ngrid and the combination of these can form a complete program that transforms\nan Input grid into an Output grid. We randomly chose some tasks from ARC that\ndont require more than the small number of the Object primitives we implemented\nand show that given only these, our system can solve tasks that require each,\nsuch different reasoning.\n']",Program Synthesis and Generation with Language Models,Advances in Large Language Models,Large Language Models
119,119,70,119_programming_program_generate_pseudocode,"['programming', 'program', 'generate', 'pseudocode', 'abstractions', 'synthesis', 'algorithmic', 'programs', 'automata', 'solvers']","['program', 'programs', 'synthesis', 'logic', 'search', 'symbolic', 'programming', 'code', 'automata', 'reasoning']","[""  Program synthesis with language models (LMs) has unlocked a large set of\nreasoning abilities; code-tuned LMs have proven adept at generating programs\nthat solve a wide variety of algorithmic symbolic manipulation tasks (e.g. word\nconcatenation). However, not all reasoning tasks are easily expressible as\ncode, e.g. tasks involving commonsense reasoning, moral decision-making, and\nsarcasm understanding. Our goal is to extend an LM's program synthesis skills\nto such tasks and evaluate the results via pseudo-programs, namely Python\nprograms where some leaf function calls are left undefined. To that end, we\npropose, Code Generation and Emulated EXecution (CoGEX). CoGEX works by (1)\ntraining LMs to generate their own pseudo-programs, (2) teaching them to\nemulate their generated program's execution, including those leaf functions,\nallowing the LM's knowledge to fill in the execution gaps; and (3) using them\nto search over many programs to find an optimal one. To adapt the CoGEX model\nto a new task, we introduce a method for performing program search to find a\nsingle program whose pseudo-execution yields optimal performance when applied\nto all the instances of a given dataset. We show that our approach yields large\nimprovements compared to standard in-context learning approaches on a battery\nof tasks, both algorithmic and soft reasoning. This result thus demonstrates\nthat code synthesis can be applied to a much broader class of problems than\npreviously considered. Our released dataset, fine-tuned models, and\nimplementation can be found at \\url{https://github.com/nweir127/CoGEX}.\n"", '  Recently, program synthesis driven by large language models (LLMs) has become\nincreasingly popular. However, program synthesis for machine learning (ML)\ntasks still poses significant challenges. This paper explores a novel form of\nprogram synthesis, targeting ML programs, by combining LLMs and automated\nmachine learning (autoML). Specifically, our goal is to fully automate the\ngeneration and optimization of the code of the entire ML workflow, from data\npreparation to modeling and post-processing, utilizing only textual\ndescriptions of the ML tasks. To manage the length and diversity of ML\nprograms, we propose to break each ML program into smaller, manageable parts.\nEach part is generated separately by the LLM, with careful consideration of\ntheir compatibilities. To ensure compatibilities, we design a testing technique\nfor ML programs. Unlike traditional program synthesis, which typically relies\non binary evaluations (i.e., correct or incorrect), evaluating ML programs\nnecessitates more than just binary judgments. Therefore, we further assess ML\nprograms numerically and select the optimal programs from a range of candidates\nusing AutoML methods. In experiments across various ML tasks, our method\noutperforms existing methods in 10 out of 12 tasks for generating ML programs.\nIn addition, autoML significantly improves the performance of the generated ML\nprograms. In experiments, given the textual task description, our method,\nText-to-ML, generates the complete and optimized ML program in a fully\nautonomous process.\n', '  The Abstraction and Reasoning Corpus (ARC) is a general artificial\nintelligence benchmark that is currently unsolvable by any Machine Learning\nmethod, including Large Language Models (LLMs). It demands strong\ngeneralization and reasoning capabilities which are known to be weaknesses of\nNeural Network based systems. In this work, we propose a Program Synthesis\nsystem that uses Inductive Logic Programming (ILP), a branch of Symbolic AI, to\nsolve ARC. We have manually defined a simple Domain Specific Language (DSL)\nthat corresponds to a small set of object-centric abstractions relevant to ARC.\nThis is the Background Knowledge used by ILP to create Logic Programs that\nprovide reasoning capabilities to our system. The full system is capable of\ngeneralize to unseen tasks, since ILP can create Logic Program(s) from few\nexamples, in the case of ARC: pairs of Input-Output grids examples for each\ntask. These Logic Programs are able to generate Objects present in the Output\ngrid and the combination of these can form a complete program that transforms\nan Input grid into an Output grid. We randomly chose some tasks from ARC that\ndont require more than the small number of the Object primitives we implemented\nand show that given only these, our system can solve tasks that require each,\nsuch different reasoning.\n']",Program Synthesis and Generation with Language Models,Advances in Large Language Models,Large Language Models
120,120,70,120_wildfires_wildfire_disasters_flood,"['wildfires', 'wildfire', 'disasters', 'flood', 'fires', 'floodwater', 'disaster', 'rescue', 'emergency', 'emergencies']","['flood', 'disaster', 'wildfire', 'emergency', 'wildfires', 'damage', 'weather', 'spatial', 'urban', 'areas']","['  Over the past few years, wildfires have become a worldwide environmental\nemergency, resulting in substantial harm to natural habitats and playing a part\nin the acceleration of climate change. Wildfire management methods involve\nprevention, response, and recovery efforts. Despite improvements in detection\ntechniques, the rising occurrence of wildfires demands creative solutions for\nprompt identification and effective control. This research investigates\nproactive methods for detecting and handling wildfires in the United States,\nutilizing Artificial Intelligence (AI), Machine Learning (ML), and 5G\ntechnology. The specific objective of this research covers proactive detection\nand prevention of wildfires using advanced technology; Active monitoring and\nmapping with remote sensing and signaling leveraging on 5G technology; and\nAdvanced response mechanisms to wildfire using drones and IOT devices. This\nstudy was based on secondary data collected from government databases and\nanalyzed using descriptive statistics. In addition, past publications were\nreviewed through content analysis, and narrative synthesis was used to present\nthe observations from various studies. The results showed that developing new\ntechnology presents an opportunity to detect and manage wildfires proactively.\nUtilizing advanced technology could save lives and prevent significant economic\nlosses caused by wildfires. Various methods, such as AI-enabled remote sensing\nand 5G-based active monitoring, can enhance proactive wildfire detection and\nmanagement. In addition, super intelligent drones and IOT devices can be used\nfor safer responses to wildfires. This forms the core of the recommendation to\nthe fire Management Agencies and the government.\n', ""  Real-time flood forecasting plays a crucial role in enabling timely and\neffective emergency responses. However, a significant challenge lies in\nbridging the gap between complex numerical flood models and practical\ndecision-making. Decision-makers often rely on experts to interpret these\nmodels for optimizing flood mitigation strategies. And the public requires\ncomplex techniques to inquiry and understand socio-cultural and institutional\nfactors, often hinders the public's understanding of flood risks. To overcome\nthese challenges, our study introduces an innovative solution: a customized AI\nAssistant powered by the GPT-4 Large Language Model. This AI Assistant is\ndesigned to facilitate effective communication between decision-makers, the\ngeneral public, and flood forecasters, without the requirement of specialized\nknowledge. The new framework utilizes GPT-4's advanced natural language\nunderstanding and function calling capabilities to provide immediate flood\nalerts and respond to various flood-related inquiries. Our developed prototype\nintegrates real-time flood warnings with flood maps and social vulnerability\ndata. It also effectively translates complex flood zone information into\nactionable risk management advice. To assess its performance, we evaluated the\nprototype using six criteria within three main categories: relevance, error\nresilience, and understanding of context. Our research marks a significant step\ntowards a more accessible and user-friendly approach in flood risk management.\nThis study highlights the potential of advanced AI tools like GPT-4 in\ndemocratizing information and enhancing public engagement in critical social\nand environmental issues.\n"", '  Wildfires have emerged as one of the most destructive natural disasters\nworldwide, causing catastrophic losses in both human lives and forest wildlife.\nRecently, the use of Artificial Intelligence (AI) in wildfires, propelled by\nthe integration of Unmanned Aerial Vehicles (UAVs) and deep learning models,\nhas created an unprecedented momentum to implement and develop more effective\nwildfire management. Although some of the existing survey papers have explored\nvarious learning-based approaches, a comprehensive review emphasizing the\napplication of AI-enabled UAV systems and their subsequent impact on\nmulti-stage wildfire management is notably lacking. This survey aims to bridge\nthese gaps by offering a systematic review of the recent state-of-the-art\ntechnologies, highlighting the advancements of UAV systems and AI models from\npre-fire, through the active-fire stage, to post-fire management. To this aim,\nwe provide an extensive analysis of the existing remote sensing systems with a\nparticular focus on the UAV advancements, device specifications, and sensor\ntechnologies relevant to wildfire management. We also examine the pre-fire and\npost-fire management approaches, including fuel monitoring, prevention\nstrategies, as well as evacuation planning, damage assessment, and operation\nstrategies. Additionally, we review and summarize a wide range of computer\nvision techniques in active-fire management, with an emphasis on Machine\nLearning (ML), Reinforcement Learning (RL), and Deep Learning (DL) algorithms\nfor wildfire classification, segmentation, detection, and monitoring tasks.\nUltimately, we underscore the substantial advancement in wildfire modeling\nthrough the integration of cutting-edge AI techniques and UAV-based data,\nproviding novel insights and enhanced predictive capabilities to understand\ndynamic wildfire behavior.\n']",Wildfires and Floods: AI-Driven Disaster Management,Disaster Management and Response using AI and Data Analytics,AI and Data-Driven Approaches for Urban and Disaster Management
121,121,70,121_corpus_nlp_texts_sentences,"['corpus', 'nlp', 'texts', 'sentences', 'linguistic', 'language', 'structured', 'languages', 'text', 'writing']","['grammar', 'synthetic', 'language', 'generation', 'instruction', 'tuning', 'fine', 'text', 'data', 'quality']","['  Despite their impressive performance, large language models (LMs) still\nstruggle with reliably generating complex output structures when not finetuned\nto follow the required output format exactly. To address this issue,\ngrammar-constrained decoding (GCD) can be used to control the generation of\nLMs, guaranteeing that the output follows a given structure. Most existing GCD\nmethods are, however, limited to specific tasks, such as parsing or code\ngeneration. In this work, we demonstrate that formal grammars can describe the\noutput space for a much wider range of tasks and argue that GCD can serve as a\nunified framework for structured NLP tasks in general. For increased\nflexibility, we introduce input-dependent grammars, which allow the grammar to\ndepend on the input and thus enable the generation of different output\nstructures for different inputs. We then empirically demonstrate the power and\nflexibility of GCD-enhanced LMs on (1) information extraction, (2) entity\ndisambiguation, and (3) constituency parsing. Our results indicate that\ngrammar-constrained LMs substantially outperform unconstrained LMs or even beat\ntask-specific finetuned models. Grammar constraints thus hold great promise for\nharnessing off-the-shelf LMs for a wide range of structured NLP tasks,\nespecially where training data is scarce or finetuning is expensive. Code and\ndata: https://github.com/epfl-dlab/GCD.\n', '  Using Large Language Models (LLMs) to generate synthetic data for model\ntraining has become increasingly popular in recent years. While LLMs are\ncapable of producing realistic training data, the effectiveness of data\ngeneration is influenced by various factors, including the choice of prompt,\ntask complexity, and the quality, quantity, and diversity of the generated\ndata. In this work, we focus exclusively on using synthetic data for text\nclassification tasks. Specifically, we use natural language understanding (NLU)\nmodels trained on synthetic data to assess the quality of synthetic data from\ndifferent generation approaches. This work provides an empirical analysis of\nthe impact of these factors and offers recommendations for better data\ngeneration practices.\n', ""  We introduce Bonito, an open-source model for conditional task generation\nthat converts unannotated text into task-specific training datasets for\ninstruction tuning. We aim to enable zero-shot task adaptation of large\nlanguage models on users' specialized, private data. We train Bonito by\nfine-tuning a pretrained large language model on a new large-scale dataset with\n1.65M examples created by remixing existing instruction tuning datasets into\nmeta-templates. The meta-templates for a dataset produce training examples\nwhere the input is the unannotated text and the task attribute and the output\nconsists of the instruction and the response. We use Bonito to generate\nsynthetic tasks for seven datasets from specialized domains with unannotated\ntext across three task types -- yes-no question answering, extractive question\nanswering, and natural language inference -- and adapt language models. We show\nthat Bonito significantly improves the average performance of pretrained and\ninstruction tuned models over the de facto self supervised baseline. For\nexample, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral\nand Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1\npoints whereas the next word prediction objective undoes some of the benefits\nof instruction tuning and reduces the average performance by 0.8 F1 points. We\nconduct additional experiments with Bonito to understand the effects of the\ndomain, the size of the training set, and the choice of alternative synthetic\ntask generators. Overall, we show that learning with synthetic instruction\ntuning datasets is an effective way to adapt language models to new domains.\nThe model, dataset, and code are available at\nhttps://github.com/BatsResearch/bonito.\n""]",Structured NLP Tasks and Language Models,Natural Language Processing (NLP) Techniques and Applications,Natural Language Processing
122,122,69,122_urban_cities_city_infrastructure,"['urban', 'cities', 'city', 'infrastructure', 'spatial', 'data', 'features', 'geoshapley', 'areas', 'geographical']","['urban', 'city', 'vessel', 'cities', 'maritime', 'spatial', 'infrastructure', 'truck', 'prediction', 'smart']","['  Representing urban regions accurately and comprehensively is essential for\nvarious urban planning and analysis tasks. Recently, with the expansion of the\ncity, modeling long-range spatial dependencies with multiple data sources plays\nan important role in urban region representation. In this paper, we propose the\nAttentive Graph Enhanced Region Representation Learning (ATGRL) model, which\naims to capture comprehensive dependencies from multiple graphs and learn rich\nsemantic representations of urban regions. Specifically, we propose a\ngraph-enhanced learning module to construct regional graphs by incorporating\nmobility flow patterns, point of interests (POIs) functions, and check-in\nsemantics with noise filtering. Then, we present a multi-graph aggregation\nmodule to capture both local and global spatial dependencies between regions by\nintegrating information from multiple graphs. In addition, we design a\ndual-stage fusion module to facilitate information sharing between different\nviews and efficiently fuse multi-view representations for urban region\nembedding using an improved linear attention mechanism. Finally, extensive\nexperiments on real-world datasets for three downstream tasks demonstrate the\nsuperior performance of our model compared to state-of-the-art methods.\n', '  As cities continue to burgeon, Urban Computing emerges as a pivotal\ndiscipline for sustainable development by harnessing the power of cross-domain\ndata fusion from diverse sources (e.g., geographical, traffic, social media,\nand environmental data) and modalities (e.g., spatio-temporal, visual, and\ntextual modalities). Recently, we are witnessing a rising trend that utilizes\nvarious deep-learning methods to facilitate cross-domain data fusion in smart\ncities. To this end, we propose the first survey that systematically reviews\nthe latest advancements in deep learning-based data fusion methods tailored for\nurban computing. Specifically, we first delve into data perspective to\ncomprehend the role of each modality and data source. Secondly, we classify the\nmethodology into four primary categories: feature-based, alignment-based,\ncontrast-based, and generation-based fusion methods. Thirdly, we further\ncategorize multi-modal urban applications into seven types: urban planning,\ntransportation, economy, public safety, society, environment, and energy.\nCompared with previous surveys, we focus more on the synergy of deep learning\nmethods with urban computing applications. Furthermore, we shed light on the\ninterplay between Large Language Models (LLMs) and urban computing, postulating\nfuture research directions that could revolutionize the field. We firmly\nbelieve that the taxonomy, progress, and prospects delineated in our survey\nstand poised to significantly enrich the research community. The summary of the\ncomprehensive and up-to-date paper list can be found at\nhttps://github.com/yoshall/Awesome-Multimodal-Urban-Computing.\n', '  The digital transformation of modern cities by integrating advanced\ninformation, communication, and computing technologies has marked the epoch of\ndata-driven smart city applications for efficient and sustainable urban\nmanagement. Despite their effectiveness, these applications often rely on\nmassive amounts of high-dimensional and multi-domain data for monitoring and\ncharacterizing different urban sub-systems, presenting challenges in\napplication areas that are limited by data quality and availability, as well as\ncostly efforts for generating urban scenarios and design alternatives. As an\nemerging research area in deep learning, Generative Artificial Intelligence\n(AI) models have demonstrated their unique values in data and code generation.\nThis survey paper aims to explore the innovative integration of generative AI\ntechniques and urban digital twins to address challenges in the realm of smart\ncities in various urban sectors, such as transportation and mobility\nmanagement, energy system operations, building and infrastructure management,\nand urban design. The survey starts with the introduction of popular generative\nAI models with their application areas, followed by a structured review of the\nexisting urban science applications that leverage the autonomous capability of\nthe generative AI techniques to facilitate (a) data augmentation for promoting\nurban monitoring and predictive analytics, (b) synthetic data and scenario\ngeneration, (c) automated 3D city modeling, and (d) generative urban design and\noptimization. Based on the review, this survey discusses potential\nopportunities and technical strategies that integrate generative AI models into\nthe next-generation urban digital twins for more reliable, scalable, and\nautomated management of smart cities.\n']",Urban Region Representation and Analysis,Urban Planning and Analysis with AI and Data-Driven Approaches,AI and Data-Driven Approaches for Urban and Disaster Management
123,123,69,123_regularization_overfitting_gradients_minimization,"['regularization', 'overfitting', 'gradients', 'minimization', 'minimizing', 'overparameterization', 'generalization', 'gradient', 'learning', 'overparameterized']","['sharpness', 'descent', 'gradient', 'generalization', 'loss', 'minimization', 'overfitting', 'gradients', 'perturbation', 'double']","['  Recently, there has been a surge in interest in developing optimization\nalgorithms for overparameterized models as achieving generalization is believed\nto require algorithms with suitable biases. This interest centers on minimizing\nsharpness of the original loss function; the Sharpness-Aware Minimization (SAM)\nalgorithm has proven effective. However, most literature only considers a few\nsharpness measures, such as the maximum eigenvalue or trace of the training\nloss Hessian, which may not yield meaningful insights for non-convex\noptimization scenarios like neural networks. Additionally, many sharpness\nmeasures are sensitive to parameter invariances in neural networks, magnifying\nsignificantly under rescaling parameters. Motivated by these challenges, we\nintroduce a new class of sharpness measures in this paper, leading to new\nsharpness-aware objective functions. We prove that these measures are\n\\textit{universally expressive}, allowing any function of the training loss\nHessian matrix to be represented by appropriate hyperparameters. Furthermore,\nwe show that the proposed objective functions explicitly bias towards\nminimizing their corresponding sharpness measures, and how they allow\nmeaningful applications to models with parameter invariances (such as\nscale-invariances). Finally, as instances of our proposed general framework, we\npresent \\textit{Frob-SAM} and \\textit{Det-SAM}, which are specifically designed\nto minimize the Frobenius norm and the determinant of the Hessian of the\ntraining loss, respectively. We also demonstrate the advantages of our general\nframework through extensive experiments.\n', ""  Despite attaining high empirical generalization, the sharpness of models\ntrained with sharpness-aware minimization (SAM) do not always correlate with\ngeneralization error. Instead of viewing SAM as minimizing sharpness to improve\ngeneralization, our paper considers a new perspective based on SAM's training\ndynamics. We propose that perturbations in SAM perform perturbed forgetting,\nwhere they discard undesirable model biases to exhibit learning signals that\ngeneralize better. We relate our notion of forgetting to the information\nbottleneck principle, use it to explain observations like the better\ngeneralization of smaller perturbation batches, and show that perturbed\nforgetting can exhibit a stronger correlation with generalization than\nflatness. While standard SAM targets model biases exposed by the steepest\nascent directions, we propose a new perturbation that targets biases exposed\nthrough the model's outputs. Our output bias forgetting perturbations\noutperform standard SAM, GSAM, and ASAM on ImageNet, robustness benchmarks, and\ntransfer to CIFAR-{10,100}, while sometimes converging to sharper regions. Our\nresults suggest that the benefits of SAM can be explained by alternative\nmechanistic principles that do not require flatness of the loss surface.\n"", '  Sharpness-Aware Minimization (SAM) has been instrumental in improving deep\nneural network training by minimizing both training loss and loss sharpness.\nDespite the practical success, the mechanisms behind SAM\'s generalization\nenhancements remain elusive, limiting its progress in deep learning\noptimization. In this work, we investigate SAM\'s core components for\ngeneralization improvement and introduce ""Friendly-SAM"" (F-SAM) to further\nenhance SAM\'s generalization. Our investigation reveals the key role of\nbatch-specific stochastic gradient noise within the adversarial perturbation,\ni.e., the current minibatch gradient, which significantly influences SAM\'s\ngeneralization performance. By decomposing the adversarial perturbation in SAM\ninto full gradient and stochastic gradient noise components, we discover that\nrelying solely on the full gradient component degrades generalization while\nexcluding it leads to improved performance. The possible reason lies in the\nfull gradient component\'s increase in sharpness loss for the entire dataset,\ncreating inconsistencies with the subsequent sharpness minimization step solely\non the current minibatch data. Inspired by these insights, F-SAM aims to\nmitigate the negative effects of the full gradient component. It removes the\nfull gradient estimated by an exponentially moving average (EMA) of historical\nstochastic gradients, and then leverages stochastic gradient noise for improved\ngeneralization. Moreover, we provide theoretical validation for the EMA\napproximation and prove the convergence of F-SAM on non-convex problems.\nExtensive experiments demonstrate the superior generalization performance and\nrobustness of F-SAM over vanilla SAM. Code is available at\nhttps://github.com/nblt/F-SAM.\n']",Sharpness-Aware Minimization for Deep Learning,Deep Learning Optimization Techniques,Deep Learning Optimization and Training
124,124,68,124_learnability_classification_prediction_optimal,"['learnability', 'classification', 'prediction', 'optimal', 'deterministic', 'classifier', 'adversary', 'randomized', 'bandit', 'complexity']","['learner', 'strategic', 'online', 'regret', 'mistake', 'setting', 'learnability', 'bounds', 'multiclass', 'dimension']","['  In contrast with standard classification tasks, strategic classification\ninvolves agents strategically modifying their features in an effort to receive\nfavorable predictions. For instance, given a classifier determining loan\napproval based on credit scores, applicants may open or close their credit\ncards to fool the classifier. The learning goal is to find a classifier robust\nagainst strategic manipulations. Various settings, based on what and when\ninformation is known, have been explored in strategic classification. In this\nwork, we focus on addressing a fundamental question: the learnability gaps\nbetween strategic classification and standard learning.\n  We essentially show that any learnable class is also strategically learnable:\nwe first consider a fully informative setting, where the manipulation structure\n(which is modeled by a manipulation graph $G^\\star$) is known and during\ntraining time the learner has access to both the pre-manipulation data and\npost-manipulation data. We provide nearly tight sample complexity and regret\nbounds, offering significant improvements over prior results. Then, we relax\nthe fully informative setting by introducing two natural types of uncertainty.\nFirst, following Ahmadi et al. (2023), we consider the setting in which the\nlearner only has access to the post-manipulation data. We improve the results\nof Ahmadi et al. (2023) and close the gap between mistake upper bound and lower\nbound raised by them. Our second relaxation of the fully informative setting\nintroduces uncertainty to the manipulation structure. That is, we assume that\nthe manipulation graph is unknown but belongs to a known class of graphs. We\nprovide nearly tight bounds on the learning complexity in various unknown\nmanipulation graph settings. Notably, our algorithm in this setting is of\nindependent interest and can be applied to other problems such as multi-label\nlearning.\n', ""  We study the problem of online binary classification in settings where\nstrategic agents can modify their observable features to receive a positive\nclassification. We model the set of feasible manipulations by a directed graph\nover the feature space, and assume the learner only observes the manipulated\nfeatures instead of the original ones. We introduce the Strategic Littlestone\nDimension, a new combinatorial measure that captures the joint complexity of\nthe hypothesis class and the manipulation graph. We demonstrate that it\ncharacterizes the instance-optimal mistake bounds for deterministic learning\nalgorithms in the realizable setting. We also achieve improved regret in the\nagnostic setting by a refined agnostic-to-realizable reduction that accounts\nfor the additional challenge of not observing agents' original features.\nFinally, we relax the assumption that the learner knows the manipulation graph,\ninstead assuming their knowledge is captured by a family of graphs. We derive\nregret bounds in both the realizable setting where all agents manipulate\naccording to the same graph within the graph family, and the agnostic setting\nwhere the manipulation graphs are chosen adversarially and not consistently\nmodeled by a single graph in the family.\n"", ""  We study the problem of online binary classification where strategic agents\ncan manipulate their observable features in predefined ways, modeled by a\nmanipulation graph, in order to receive a positive classification. We show this\nsetting differs in fundamental ways from non-strategic online classification.\nFor instance, whereas in the non-strategic case, a mistake bound of $\\ln|H|$ is\nachievable via the halving algorithm when the target function belongs to a\nknown class $H$, we show that no deterministic algorithm can achieve a mistake\nbound $o(\\Delta)$ in the strategic setting, where $\\Delta$ is the maximum\ndegree of the manipulation graph (even when $|H|=O(\\Delta)$). We obtain an\nalgorithm achieving mistake bound $O(\\Delta\\ln|H|)$. We also extend this to the\nagnostic setting and obtain an algorithm with a $\\Delta$ multiplicative regret,\nand we show no deterministic algorithm can achieve $o(\\Delta)$ multiplicative\nregret.\n  Next, we study two randomized models based on whether the random choices are\nmade before or after agents respond, and show they exhibit fundamental\ndifferences. In the first model, at each round the learner deterministically\nchooses a probability distribution over classifiers inducing expected values on\neach vertex (probabilities of being classified as positive), which the\nstrategic agents respond to. We show that any learner in this model has to\nsuffer linear regret. On the other hand, in the second model, while the\nadversary who selects the next agent must respond to the learner's probability\ndistribution over classifiers, the agent then responds to the actual hypothesis\nclassifier drawn from this distribution. Surprisingly, we show this model is\nmore advantageous to the learner, and we design randomized algorithms that\nachieve sublinear regret bounds against both oblivious and adaptive\nadversaries.\n""]",Strategic Classification and Learnability,Game Theory and Strategic Decision Making,Decision Making and Optimization under Uncertainty
125,125,68,125_forecasting_inventory_forecast_forecasts,"['forecasting', 'inventory', 'forecast', 'forecasts', 'predicting', 'prediction', 'supply', 'planning', 'analytics', 'suppliers']","['supply', 'chain', 'retail', 'customer', 'forecasting', 'industry', 'prediction', 'predictive', 'pandemic', 'resilience']","['  Recent pandemics have highlighted vulnerabilities in our global economic\nsystems, especially supply chains. Possible future pandemic raises a dilemma\nfor businesses owners between short-term profitability and long-term supply\nchain resilience planning. In this study, we propose a novel agent-based\nsimulation model integrating extended Susceptible-Infected-Recovered (SIR)\nepidemiological model and supply and demand economic model to evaluate supply\nchain resilience strategies during pandemics. Using this model, we explore a\nrange of supply chain resilience strategies under pandemic scenarios using in\nsilico experiments. We find that a balanced approach to supply chain resilience\nperforms better in both pandemic and non-pandemic times compared to extreme\nstrategies, highlighting the importance of preparedness in the form of a better\nsupply chain resilience. However, our analysis shows that the exact supply\nchain resilience strategy is hard to obtain for each firm and is relatively\nsensitive to the exact profile of the pandemic and economic state at the\nbeginning of the pandemic. As such, we used a machine learning model that uses\nthe agent-based simulation to estimate a near-optimal supply chain resilience\nstrategy for a firm. The proposed model offers insights for policymakers and\nbusinesses to enhance supply chain resilience in the face of future pandemics,\ncontributing to understanding the trade-offs between short-term gains and\nlong-term sustainability in supply chain management before and during\npandemics.\n', '  This study tackles the complexities of global supply chains, which are\nincreasingly vulnerable to disruptions caused by port congestion, material\nshortages, and inflation. To address these challenges, we explore the\napplication of machine learning methods, which excel in predicting and\noptimizing solutions based on large datasets. Our focus is on enhancing supply\nchain security through fraud detection, maintenance prediction, and material\nbackorder forecasting. We introduce an automated machine learning framework\nthat streamlines data analysis, model construction, and hyperparameter\noptimization for these tasks. By automating these processes, our framework\nimproves the efficiency and effectiveness of supply chain security measures.\nOur research identifies key factors that influence machine learning\nperformance, including sampling methods, categorical encoding, feature\nselection, and hyperparameter optimization. We demonstrate the importance of\nconsidering these factors when applying machine learning to supply chain\nchallenges. Traditional mathematical programming models often struggle to cope\nwith the complexity of large-scale supply chain problems. Our study shows that\nmachine learning methods can provide a viable alternative, particularly when\ndealing with extensive datasets and complex patterns. The automated machine\nlearning framework presented in this study offers a novel approach to supply\nchain security, contributing to the existing body of knowledge in the field.\nIts comprehensive automation of machine learning processes makes it a valuable\ncontribution to the domain of supply chain management.\n', ""  Successful supply chain optimization must mitigate imbalances between supply\nand demand over time. While accurate demand prediction is essential for supply\nplanning, it alone does not suffice. The key to successful supply planning for\noptimal and viable execution lies in maximizing predictability for both demand\nand supply throughout an execution horizon. Therefore, enhancing the accuracy\nof supply predictions is imperative to create an attainable supply plan that\nmatches demand without overstocking or understocking. However, in complex\nsupply chain networks with numerous nodes and edges, accurate supply\npredictions are challenging due to dynamic node interactions, cascading supply\ndelays, resource availability, production and logistic capabilities.\nConsequently, supply executions often deviate from their initial plans. To\naddress this, we present the Graph-based Supply Prediction (GSP) probabilistic\nmodel. Our attention-based graph neural network (GNN) model predicts supplies,\ninventory, and imbalances using graph-structured historical data, demand\nforecasting, and original supply plan inputs. The experiments, conducted using\nhistorical data from a global consumer goods company's large-scale supply\nchain, demonstrate that GSP significantly improves supply and inventory\nprediction accuracy, potentially offering supply plan corrections to optimize\nexecutions.\n""]",Supply Chain Resilience and Optimization,Optimization and Learning in Complex Systems,Complex Systems Analysis and Optimization
126,126,68,126_scaling_generalization_neural_regularization,"['scaling', 'generalization', 'neural', 'regularization', 'sparse', 'sgd', 'networks', 'learned', 'small', 'empirically']","['scaling', 'laws', 'law', 'neural', 'gradient', 'width', 'generalization', 'loss', 'networks', 'size']","['  The population loss of trained deep neural networks often follows precise\npower-law scaling relations with either the size of the training dataset or the\nnumber of parameters in the network. We propose a theory that explains the\norigins of and connects these scaling laws. We identify variance-limited and\nresolution-limited scaling behavior for both dataset and model size, for a\ntotal of four scaling regimes. The variance-limited scaling follows simply from\nthe existence of a well-behaved infinite data or infinite width limit, while\nthe resolution-limited regime can be explained by positing that models are\neffectively resolving a smooth data manifold. In the large width limit, this\ncan be equivalently obtained from the spectrum of certain kernels, and we\npresent evidence that large width and large dataset resolution-limited scaling\nexponents are related by a duality. We exhibit all four scaling regimes in the\ncontrolled setting of large random feature and pretrained models and test the\npredictions empirically on a range of standard architectures and datasets. We\nalso observe several empirical relationships between datasets and scaling\nexponents under modifications of task and architecture aspect ratio. Our work\nprovides a taxonomy for classifying different scaling regimes, underscores that\nthere can be different mechanisms driving improvements in loss, and lends\ninsight into the microscopic origins of and relationships between scaling\nexponents.\n', '  Empirically, large-scale deep learning models often satisfy a neural scaling\nlaw: the test error of the trained model improves polynomially as the model\nsize and data size grow. However, conventional wisdom suggests the test error\nconsists of approximation, bias, and variance errors, where the variance error\nincreases with model size. This disagrees with the general form of neural\nscaling laws, which predict that increasing model size monotonically improves\nperformance.\n  We study the theory of scaling laws in an infinite dimensional linear\nregression setup. Specifically, we consider a model with $M$ parameters as a\nlinear function of sketched covariates. The model is trained by one-pass\nstochastic gradient descent (SGD) using $N$ data. Assuming the optimal\nparameter satisfies a Gaussian prior and the data covariance matrix has a\npower-law spectrum of degree $a>1$, we show that the reducible part of the test\nerror is $\\Theta(M^{-(a-1)} + N^{-(a-1)/a})$. The variance error, which\nincreases with $M$, is dominated by the other errors due to the implicit\nregularization of SGD, thus disappearing from the bound. Our theory is\nconsistent with the empirical neural scaling laws and verified by numerical\nsimulation.\n', '  On a variety of tasks, the performance of neural networks predictably\nimproves with training time, dataset size and model size across many orders of\nmagnitude. This phenomenon is known as a neural scaling law. Of fundamental\nimportance is the compute-optimal scaling law, which reports the performance as\na function of units of compute when choosing model sizes optimally. We analyze\na random feature model trained with gradient descent as a solvable model of\nnetwork training and generalization. This reproduces many observations about\nneural scaling laws. First, our model makes a prediction about why the scaling\nof performance with training time and with model size have different power law\nexponents. Consequently, the theory predicts an asymmetric compute-optimal\nscaling rule where the number of training steps are increased faster than model\nparameters, consistent with recent empirical observations. Second, it has been\nobserved that early in training, networks converge to their infinite-width\ndynamics at a rate $1/\\textit{width}$ but at late time exhibit a rate\n$\\textit{width}^{-c}$, where $c$ depends on the structure of the architecture\nand task. We show that our model exhibits this behavior. Lastly, our theory\nshows how the gap between training and test loss can gradually build up over\ntime due to repeated reuse of data.\n']",Neural Network Scaling Laws and Generalization,Deep Learning Theory and Foundations,Machine Learning and Artificial Intelligence
127,127,68,127_visual_3d_multimodal_vision,"['visual', '3d', 'multimodal', 'vision', 'spatial', 'interactive', 'reasoning', 'robot', 'tasks', 'robots']","['visual', 'reasoning', 'object', 'objects', 'diagrams', 'robot', 'spatial', 'vision', 'intention', 'gesture']","[""  Visual Language Models (VLMs) are essential for various tasks, particularly\nvisual reasoning tasks, due to their robust multi-modal information\nintegration, visual reasoning capabilities, and contextual awareness. However,\nexisting \\VLMs{}' visual spatial reasoning capabilities are often inadequate,\nstruggling even with basic tasks such as distinguishing left from right. To\naddress this, we propose the \\ours{} model, designed to enhance the visual\nspatial reasoning abilities of VLMS. ZeroVLM employs Zero-1-to-3, a 3D\nreconstruction model for obtaining different views of the input images and\nincorporates a prompting mechanism to further improve visual spatial reasoning.\nExperimental results on four visual spatial reasoning datasets show that our\n\\ours{} achieves up to 19.48% accuracy improvement, which indicates the\neffectiveness of the 3D reconstruction and prompting mechanisms of our ZeroVLM.\n"", '  Although great progress has been made in 3D visual grounding, current models\nstill rely on explicit textual descriptions for grounding and lack the ability\nto reason human intentions from implicit instructions. We propose a new task\ncalled 3D reasoning grounding and introduce a new benchmark ScanReason which\nprovides over 10K question-answer-location pairs from five reasoning types that\nrequire the synerization of reasoning and grounding. We further design our\napproach, ReGround3D, composed of the visual-centric reasoning module empowered\nby Multi-modal Large Language Model (MLLM) and the 3D grounding module to\nobtain accurate object locations by looking back to the enhanced geometry and\nfine-grained details from the 3D scenes. A chain-of-grounding mechanism is\nproposed to further boost the performance with interleaved reasoning and\ngrounding steps during inference. Extensive experiments on the proposed\nbenchmark validate the effectiveness of our proposed approach.\n', '  Visual representation learning has been a cornerstone in computer vision,\ninvolving typical forms such as visual embeddings, structural symbols, and\ntext-based representations. Despite the success of CLIP-type visual embeddings,\nthey often lack access to world knowledge critical for visual reasoning. In\nthis work, we propose Visual Table, a novel form of visual representation\ntailored for visual reasoning. Visual tables are constructed as hierarchical\ndescriptions of visual scenes, featuring a scene description and multiple\nobject-centric descriptions covering categories, attributes, and knowledge.\nThanks to the structural and textual formats, visual tables offer unique\nadvantages over mere visual embeddings, such as interpretability and\ncontrollable editing. Furthermore, they deliver instance-level world knowledge\nand detailed attributes that are essential for visual reasoning. To create\nvisual tables, we develop a generator trained on the dataset with collected,\nsmall-scale annotations. Extensive results on 11 visual reasoning benchmarks\ndemonstrate that the generated visual tables significantly outperform previous\nstructural and text-based representations. Moreover, they consistently enhance\nstate-of-the-art multimodal large language models across diverse benchmarks,\nshowcasing their potential for advancing visual reasoning tasks. Our code is\navailable at https://github.com/LaVi-Lab/Visual-Table.\n']",Visual Reasoning in Multimodal 3D Spaces,Visual and Spatial Reasoning in Artificial Intelligence,Artificial Intelligence and Cognitive Systems
128,128,67,128_attention_memory_softmax_decoder,"['attention', 'memory', 'softmax', 'decoder', 'layers', 'transformers', 'throughput', 'efficient', 'pruning', 'faster']","['attention', 'transformer', 'transformers', 'layers', 'sequence', 'layer', 'computation', 'memory', 'length', 'complexity']","['  We present Lightning Attention, the first linear attention implementation\nthat maintains a constant training speed for various sequence lengths under\nfixed memory consumption. Due to the issue with cumulative summation operations\n(cumsum), previous linear attention implementations cannot achieve their\ntheoretical advantage in a casual setting. However, this issue can be\neffectively solved by utilizing different attention calculation strategies to\ncompute the different parts of attention. Specifically, we split the attention\ncalculation into intra-blocks and inter-blocks and use conventional attention\ncomputation for intra-blocks and linear attention kernel tricks for\ninter-blocks. This eliminates the need for cumsum in the linear attention\ncalculation. Furthermore, a tiling technique is adopted through both forward\nand backward procedures to take full advantage of the GPU hardware. To enhance\naccuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new\narchitecture that is tailored to our lightning attention. We conduct rigorous\ntesting on standard and self-collected datasets with varying model sizes and\nsequence lengths. TNL is notably more efficient than other language models. In\naddition, benchmark results indicate that TNL performs on par with\nstate-of-the-art LLMs utilizing conventional transformer structures. The source\ncode is released at github.com/OpenNLPLab/TransnormerLLM.\n', '  Scaling Transformer-based large language models (LLMs) has demonstrated\npromising performance across various tasks. However, it also introduces\nredundant structures, posing challenges for real-world deployment. Despite some\nrecognition of redundancy in LLMs, the variability of redundancy across\ndifferent modules, such as MLP and Attention layers, is under-explored. In this\nwork, we investigate the varying redundancy across different modules within\nTransformers, including Blocks, MLP, and Attention layers, using a\nsimilarity-based metric. This metric operates on the premise that redundant\nstructures produce outputs highly similar to their inputs. Surprisingly, while\nattention layers are essential for transformers and distinguish them from other\nmainstream architectures, we found that a large proportion of attention layers\nexhibit excessively high similarity and can be safely pruned without degrading\nperformance, leading to reduced memory and computation costs. Additionally, we\nfurther propose a method that jointly drops Attention and MLP layers, achieving\nimproved performance and dropping ratios. Extensive experiments demonstrate the\neffectiveness of our methods, e.g., Llama-3-70B maintains comparable\nperformance even after pruning half of the attention layers. Our findings\nprovide valuable insights for future network architecture design. The code is\nreleased at: \\url{https://github.com/Shwai-He/LLM-Drop}.\n', '  Linear attention is an efficient attention mechanism that has recently\nemerged as a promising alternative to conventional softmax attention. With its\nability to process tokens in linear computational complexities, linear\nattention, in theory, can handle sequences of unlimited length without\nsacrificing speed, i.e., maintaining a constant training speed for various\nsequence lengths with a fixed memory consumption. However, due to the issue\nwith cumulative summation (cumsum), current linear attention algorithms cannot\ndemonstrate their theoretical advantage in a causal setting. In this paper, we\npresent Lightning Attention-2, the first linear attention implementation that\nenables linear attention to realize its theoretical computational benefits. To\nachieve this, we leverage the thought of tiling, separately handling the\nintra-block and inter-block components in linear attention calculation.\nSpecifically, we utilize the conventional attention computation mechanism for\nthe intra-blocks and apply linear attention kernel tricks for the inter-blocks.\nA tiling technique is adopted through both forward and backward procedures to\ntake full advantage of the GPU hardware. We implement our algorithm in Triton\nto make it IO-aware and hardware-friendly. Various experiments are conducted on\ndifferent model sizes and sequence lengths. Lightning Attention-2 retains\nconsistent training and inference speed regardless of input sequence length and\nis significantly faster than other attention mechanisms. The source code is\navailable at https://github.com/OpenNLPLab/lightning-attention.\n']",Efficient Attention Mechanisms for Transformers,Efficient Attention Mechanisms for Large Language Models,Large Language Models
129,129,67,129_iot_iotds_sensor_rscnet,"['iot', 'iotds', 'sensor', 'rscnet', 'cloud', 'sensing', 'sensors', 'networks', 'network', 'classification']","['iot', 'devices', 'device', 'energy', 'sensor', 'edge', 'wireless', 'sensing', 'smart', 'channel']","['  This article explores how to drive intelligent iot monitoring and control\nthrough cloud computing and machine learning. As iot and the cloud continue to\ngenerate large and diverse amounts of data as sensor devices in the network,\nthe collected data is sent to the cloud for statistical analysis, prediction,\nand data analysis to achieve business objectives. However, because the cloud\ncomputing model is limited by distance, it can be problematic in environments\nwhere the quality of the Internet connection is not ideal for critical\noperations. Therefore, edge computing, as a distributed computing architecture,\nmoves the location of processing applications, data and services from the\ncentral node of the network to the logical edge node of the network to reduce\nthe dependence on cloud processing and analysis of data, and achieve near-end\ndata processing and analysis. The combination of iot and edge computing can\nreduce latency, improve efficiency, and enhance security, thereby driving the\ndevelopment of intelligent systems. The paper also introduces the development\nof iot monitoring and control technology, the application of edge computing in\niot monitoring and control, and the role of machine learning in data analysis\nand fault detection. Finally, the application and effect of intelligent\nInternet of Things monitoring and control system in industry, agriculture,\nmedical and other fields are demonstrated through practical cases and\nexperimental studies.\n', ""  Deep learning models are increasingly deployed on edge Internet of Things\n(IoT) devices. However, these models typically operate under supervised\nconditions and fail to recognize unseen classes different from training. To\naddress this, zero-shot learning (ZSL) aims to classify data of unseen classes\nwith the help of semantic information. Foundation models (FMs) trained on\nweb-scale data have shown impressive ZSL capability in natural language\nprocessing and visual understanding. However, leveraging FMs' generalized\nknowledge for zero-shot IoT sensing using signals such as mmWave, IMU, and\nWi-Fi has not been fully investigated. In this work, we align the IoT data\nembeddings with the semantic embeddings generated by an FM's text encoder for\nzero-shot IoT sensing. To utilize the physics principles governing the\ngeneration of IoT sensor signals to derive more effective prompts for semantic\nembedding extraction, we propose to use cross-attention to combine a learnable\nsoft prompt that is optimized automatically on training data and an auxiliary\nhard prompt that encodes domain knowledge of the IoT sensing task. To address\nthe problem of IoT embeddings biasing to seen classes due to the lack of unseen\nclass data during training, we propose using data augmentation to synthesize\nunseen class IoT data for fine-tuning the IoT feature extractor and embedding\nprojector. We evaluate our approach on multiple IoT sensing tasks. Results show\nthat our approach achieves superior open-set detection and generalized\nzero-shot learning performance compared with various baselines. Our code is\navailable at https://github.com/schrodingho/FM\\_ZSL\\_IoT.\n"", ""  The Internet of Things (IoT) network integrating billions of smart physical\ndevices embedded with sensors, software, and communication technologies is a\ncritical and rapidly expanding component of our modern world. The IoT ecosystem\nprovides a rich source of real-world modalities such as motion, thermal,\ngeolocation, imaging, depth, sensors, and audio to recognize the states of\nhumans and physical objects. Machine learning presents a rich opportunity to\nautomatically process IoT data at scale, enabling efficient inference for\nunderstanding human wellbeing, controlling physical devices, and\ninterconnecting smart cities. To realize this potential, we introduce IoT-LM,\nan open-source large multisensory language model tailored for the IoT\necosystem. IoT-LM is enabled by two technical contributions: the first is\nMultiIoT, the most expansive unified IoT dataset to date, encompassing over\n1.15 million samples from 12 modalities and 8 tasks prepared for multisensory\npre-training and instruction-tuning. The second is a new multisensory multitask\nadapter layer to condition pre-trained large language models on multisensory\nIoT data. Not only does IoT-LM yield substantial improvements on 8 supervised\nIoT classification tasks, but it also demonstrates new interactive\nquestion-answering, reasoning, and dialog capabilities conditioned on IoT\nsensors. We release IoT-LM's data sources and new multisensory language\nmodeling framework.\n""]",IoT Monitoring and Control with Edge Computing and Machine Learning,Edge Computing and Artificial Intelligence for IoT and Mobile Networks,Edge Computing and Artificial Intelligence for IoT and Mobile Networks
130,130,66,130_vision_camera_cameras_3d,"['vision', 'camera', 'cameras', '3d', 'rgb', 'recognition', 'scenes', 'view', 'views', 'frames']","['event', 'motion', 'cameras', 'scene', 'object', 'view', 'stereo', 'matching', 'camera', 'feature']","['  One of the most critical factors in achieving sharp Novel View Synthesis\n(NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) is the quality of the training images. However,\nConventional RGB cameras are susceptible to motion blur. In contrast,\nneuromorphic cameras like event and spike cameras inherently capture more\ncomprehensive temporal information, which can provide a sharp representation of\nthe scene as additional training data. Recent methods have explored the\nintegration of event cameras to improve the quality of NVS. The event-RGB\napproaches have some limitations, such as high training costs and the inability\nto work effectively in the background. Instead, our study introduces a new\nmethod that uses the spike camera to overcome these limitations. By considering\ntexture reconstruction from spike streams as ground truth, we design the\nTexture from Spike (TfS) loss. Since the spike camera relies on temporal\nintegration instead of temporal differentiation used by event cameras, our\nproposed TfS loss maintains manageable training costs. It handles foreground\nobjects with backgrounds simultaneously. We also provide a real-world dataset\ncaptured with our spike-RGB camera system to facilitate future research\nendeavors. We conduct extensive experiments using synthetic and real-world\ndatasets to demonstrate that our design can enhance novel view synthesis across\nNeRF and 3DGS. The code and dataset will be made available for public access.\n', ""  Semantic segmentation in bird's eye view (BEV) plays a crucial role in\nautonomous driving. Previous methods usually follow an end-to-end pipeline,\ndirectly predicting the BEV segmentation map from monocular RGB inputs.\nHowever, the challenge arises when the RGB inputs and BEV targets from distinct\nperspectives, making the direct point-to-point predicting hard to optimize. In\nthis paper, we decompose the original BEV segmentation task into two stages,\nnamely BEV map reconstruction and RGB-BEV feature alignment. In the first\nstage, we train a BEV autoencoder to reconstruct the BEV segmentation maps\ngiven corrupted noisy latent representation, which urges the decoder to learn\nfundamental knowledge of typical BEV patterns. The second stage involves\nmapping RGB input images into the BEV latent space of the first stage, directly\noptimizing the correlations between the two views at the feature level. Our\napproach simplifies the complexity of combining perception and generation into\ndistinct steps, equipping the model to handle intricate and challenging scenes\neffectively. Besides, we propose to transform the BEV segmentation map from the\nCartesian to the polar coordinate system to establish the column-wise\ncorrespondence between RGB images and BEV maps. Moreover, our method requires\nneither multi-scale features nor camera intrinsic parameters for depth\nestimation and saves computational overhead. Extensive experiments on nuScenes\nand Argoverse show the effectiveness and efficiency of our method. Code is\navailable at https://github.com/happytianhao/TaDe.\n"", ""  Autonomous driving requires an accurate representation of the environment. A\nstrategy toward high accuracy is to fuse data from several sensors. Learned\nBird's-Eye View (BEV) encoders can achieve this by mapping data from individual\nsensors into one joint latent space. For cost-efficient camera-only systems,\nthis provides an effective mechanism to fuse data from multiple cameras with\ndifferent views. Accuracy can further be improved by aggregating sensor\ninformation over time. This is especially important in monocular camera systems\nto account for the lack of explicit depth and velocity measurements. Thereby,\nthe effectiveness of developed BEV encoders crucially depends on the operators\nused to aggregate temporal information and on the used latent representation\nspaces. We analyze BEV encoders proposed in the literature and compare their\neffectiveness, quantifying the effects of aggregation operators and latent\nrepresentations. While most existing approaches aggregate temporal information\neither in image or in BEV latent space, our analyses and performance\ncomparisons suggest that these latent representations exhibit complementary\nstrengths. Therefore, we develop a novel temporal BEV encoder, TempBEV, which\nintegrates aggregated temporal information from both latent spaces. We consider\nsubsequent image frames as stereo through time and leverage methods from\noptical flow estimation for temporal stereo encoding. Empirical evaluation on\nthe NuScenes dataset shows a significant improvement by TempBEV over the\nbaseline for 3D object detection and BEV segmentation. The ablation uncovers a\nstrong synergy of joint temporal aggregation in the image and BEV latent space.\nThese results indicate the overall effectiveness of our approach and make a\nstrong case for aggregating temporal information in both image and BEV latent\nspaces.\n""]",Computer Vision for 3D Scene Understanding,Multimodal Video and Image Understanding,Multimodal Learning and Applications
131,131,66,131_chatbots_chatbot_conversational_conversations,"['chatbots', 'chatbot', 'conversational', 'conversations', 'conversation', 'ai', 'dialogue', 'messages', 'dialogues', 'chat']","['mental', 'health', 'emotional', 'emotion', 'emotions', 'support', 'empathetic', 'chatbot', 'empathy', 'children']","['  Objective: This study aims to develop and validate an evaluation framework to\nensure the safety and reliability of mental health chatbots, which are\nincreasingly popular due to their accessibility, human-like interactions, and\ncontext-aware support. Materials and Methods: We created an evaluation\nframework with 100 benchmark questions and ideal responses, and five guideline\nquestions for chatbot responses. This framework, validated by mental health\nexperts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation\nmethods explored included large language model (LLM)-based scoring, an agentic\napproach using real-time data, and embedding models to compare chatbot\nresponses against ground truth standards. Results: The results highlight the\nimportance of guidelines and ground truth for improving LLM evaluation\naccuracy. The agentic method, dynamically accessing reliable information,\ndemonstrated the best alignment with human assessments. Adherence to a\nstandardized, expert-validated framework significantly enhanced chatbot\nresponse safety and reliability. Discussion: Our findings emphasize the need\nfor comprehensive, expert-tailored safety evaluation metrics for mental health\nchatbots. While LLMs have significant potential, careful implementation is\nnecessary to mitigate risks. The superior performance of the agentic approach\nunderscores the importance of real-time data access in enhancing chatbot\nreliability. Conclusion: The study validated an evaluation framework for mental\nhealth chatbots, proving its effectiveness in improving safety and reliability.\nFuture work should extend evaluations to accuracy, bias, empathy, and privacy\nto ensure holistic assessment and responsible integration into healthcare.\nStandardized evaluations will build trust among users and professionals,\nfacilitating broader adoption and improved mental health support through\ntechnology.\n', '  People experiencing severe distress increasingly use Large Language Model\n(LLM) chatbots as mental health support tools. Discussions on social media have\ndescribed how engagements were lifesaving for some, but evidence suggests that\ngeneral-purpose LLM chatbots also have notable risks that could endanger the\nwelfare of users if not designed responsibly. In this study, we investigate the\nlived experiences of people who have used LLM chatbots for mental health\nsupport. We build on interviews with 21 individuals from globally diverse\nbackgrounds to analyze how users create unique support roles for their\nchatbots, fill in gaps in everyday care, and navigate associated cultural\nlimitations when seeking support from chatbots. We ground our analysis in\npsychotherapy literature around effective support, and introduce the concept of\ntherapeutic alignment, or aligning AI with therapeutic values for mental health\ncontexts. Our study offers recommendations for how designers can approach the\nethical and effective use of LLM chatbots and other AI mental health support\ntools in mental health care.\n', '  Generative AI systems are increasingly capable of expressing emotions via\ntext and imagery. Effective emotional expression will likely play a major role\nin the efficacy of AI systems -- particularly those designed to support human\nmental health and wellbeing. This motivates our present research to better\nunderstand the alignment of AI expressed emotions with the human perception of\nemotions. When AI tries to express a particular emotion, how might we assess\nwhether they are successful? To answer this question, we designed a survey to\nmeasure the alignment between emotions expressed by generative AI and human\nperceptions. Three generative image models (DALL-E 2, DALL-E 3 and Stable\nDiffusion v1) were used to generate 240 examples of images, each of which was\nbased on a prompt designed to express five positive and five negative emotions\nacross both humans and robots. 24 participants recruited from the Prolific\nwebsite rated the alignment of AI-generated emotional expressions with a text\nprompt used to generate the emotion (i.e., ""A robot expressing the emotion\namusement""). The results of our evaluation suggest that generative AI models\nare indeed capable of producing emotional expressions that are well-aligned\nwith a range of human emotions; however, we show that the alignment\nsignificantly depends upon the AI model used and the emotion itself. We analyze\nvariations in the performance of these systems to identify gaps for future\nimprovement. We conclude with a discussion of the implications for future AI\nsystems designed to support mental health and wellbeing.\n']",Mental Health Chatbots and AI Support Systems,Conversational AI and Chatbots,Conversational AI and Human-Computer Interaction
132,132,66,132_automation_ai_architecture_agent,"['automation', 'ai', 'architecture', 'agent', 'agents', 'software', 'industrial', 'engineering', 'intelligence', 'artificial']","['smart', 'home', 'foundation', 'scientific', 'engineering', 'intelligence', 'manufacturing', 'automation', 'systems', 'software']","['  In this paper, we explore the transformative impact of Artificial\nIntelligence (AI) in the manufacturing sector, highlighting its potential to\nrevolutionize industry practices and enhance operational efficiency. We delve\ninto various applications of AI in manufacturing, with a particular emphasis on\nhuman-machine interfaces (HMI) and AI-powered milling machines, showcasing how\nthese technologies contribute to more intuitive operations and precision in\nproduction processes. Through rigorous market analysis, the paper presents\ninsightful data on AI adoption rates among German manufacturers, comparing\nthese figures with global trends and exploring the specific uses of AI in\nproduction, maintenance, customer service, and more. In addition, the paper\nexamines the emerging field of Generative AI and the potential applications of\nlarge language models in manufacturing processes. The findings indicate a\nsignificant increase in AI adoption from 6% in 2020 to 13.3% in 2023 among\nGerman companies, with a projection of substantial economic impact by 2030. The\nstudy also addresses the challenges faced by companies, such as data quality\nand integration hurdles, providing a balanced view of the opportunities and\nobstacles in AI implementation.\n', ""  Smart home systems are gaining popularity as homeowners strive to enhance\ntheir living and working environments while minimizing energy consumption.\nHowever, the adoption of artificial intelligence (AI)-enabled decision-making\nmodels in smart home systems faces challenges due to the complexity and\nblack-box nature of these systems, leading to concerns about explainability,\ntrust, transparency, accountability, and fairness. The emerging field of\nexplainable artificial intelligence (XAI) addresses these issues by providing\nexplanations for the models' decisions and actions. While state-of-the-art XAI\nmethods are beneficial for AI developers and practitioners, they may not be\neasily understood by general users, particularly household members. This paper\nadvocates for human-centered XAI methods, emphasizing the importance of\ndelivering readily comprehensible explanations to enhance user satisfaction and\ndrive the adoption of smart home systems. We review state-of-the-art XAI\nmethods and prior studies focusing on human-centered explanations for general\nusers in the context of smart home applications. Through experiments on two\nsmart home application scenarios, we demonstrate that explanations generated by\nprominent XAI techniques might not be effective in helping users understand and\nmake decisions. We thus argue for the necessity of a human-centric approach in\nrepresenting explanations in smart home systems and highlight relevant\nhuman-computer interaction (HCI) methodologies, including user studies,\nprototyping, technology probes analysis, and heuristic evaluation, that can be\nemployed to generate and present human-centered explanations to users.\n"", '  The rapid advancement of AI technology has led to widespread applications of\nagent systems across various domains. However, the need for detailed\narchitecture design poses significant challenges in designing and operating\nthese systems. This paper introduces a taxonomy focused on the architectures of\nfoundation-model-based agents, addressing critical aspects such as functional\ncapabilities and non-functional qualities. We also discuss the operations\ninvolved in both design-time and run-time phases, providing a comprehensive\nview of architectural design and operational characteristics. By unifying and\ndetailing these classifications, our taxonomy aims to improve the design of\nfoundation-model-based agents. Additionally, the paper establishes a decision\nmodel that guides critical design and runtime decisions, offering a structured\napproach to enhance the development of foundation-model-based agents. Our\ncontributions include providing a structured architecture design option and\nguiding the development process of foundation-model-based agents, thereby\naddressing current fragmentation in the field.\n']",Artificial Intelligence in Industrial Automation,Artificial Intelligence in Technology and Engineering,Artificial Intelligence and Cognitive Systems
133,133,66,133_artworks_artists_artistic_stylistic,"['artworks', 'artists', 'artistic', 'stylistic', 'styles', 'artness', 'artwork', 'generative', 'stylization', 'artist']","['style', 'artistic', 'image', 'artists', 'images', 'styles', 'artworks', 'aesthetic', 'text', 'stylization']","[""  Large-scale Text-to-Image (T2I) models have rapidly gained prominence across\ncreative fields, generating visually compelling outputs from textual prompts.\nHowever, controlling these models to ensure consistent style remains\nchallenging, with existing methods necessitating fine-tuning and manual\nintervention to disentangle content and style. In this paper, we introduce\nStyleAligned, a novel technique designed to establish style alignment among a\nseries of generated images. By employing minimal `attention sharing' during the\ndiffusion process, our method maintains style consistency across images within\nT2I models. This approach allows for the creation of style-consistent images\nusing a reference style through a straightforward inversion operation. Our\nmethod's evaluation across diverse styles and text prompts demonstrates\nhigh-quality synthesis and fidelity, underscoring its efficacy in achieving\nconsistent style across various inputs.\n"", '  Recent text-to-image generative models such as Stable Diffusion are extremely\nadept at mimicking and generating copyrighted content, raising concerns amongst\nartists that their unique styles may be improperly copied. Understanding how\ngenerative models copy ""artistic style"" is more complex than duplicating a\nsingle image, as style is comprised by a set of elements (or signature) that\nfrequently co-occurs across a body of work, where each individual work may vary\nsignificantly. In our paper, we first reformulate the problem of ""artistic\ncopyright infringement"" to a classification problem over image sets, instead of\nprobing image-wise similarities. We then introduce ArtSavant, a practical\n(i.e., efficient and easy to understand) tool to (i) determine the unique style\nof an artist by comparing it to a reference dataset of works from 372 artists\ncurated from WikiArt, and (ii) recognize if the identified style reappears in\ngenerated images. We leverage two complementary methods to perform artistic\nstyle classification over image sets, includingTagMatch, which is a novel\ninherently interpretable and attributable method, making it more suitable for\nbroader use by non-technical stake holders (artists, lawyers, judges, etc).\nLeveraging ArtSavant, we then perform a large-scale empirical study to provide\nquantitative insight on the prevalence of artistic style copying across 3\npopular text-to-image generative models. Namely, amongst a dataset of prolific\nartists (including many famous ones), only 20% of them appear to have their\nstyles be at a risk of copying via simple prompting of today\'s popular\ntext-to-image generative models.\n', ""  In this work we introduce a novel medical image style transfer method,\nStyleMapper, that can transfer medical scans to an unseen style with access to\nlimited training data. This is made possible by training our model on unlimited\npossibilities of simulated random medical imaging styles on the training set,\nmaking our work more computationally efficient when compared with other style\ntransfer methods. Moreover, our method enables arbitrary style transfer:\ntransferring images to styles unseen in training. This is useful for medical\nimaging, where images are acquired using different protocols and different\nscanner models, resulting in a variety of styles that data may need to be\ntransferred between. Methods: Our model disentangles image content from style\nand can modify an image's style by simply replacing the style encoding with one\nextracted from a single image of the target style, with no additional\noptimization required. This also allows the model to distinguish between\ndifferent styles of images, including among those that were unseen in training.\nWe propose a formal description of the proposed model. Results: Experimental\nresults on breast magnetic resonance images indicate the effectiveness of our\nmethod for style transfer. Conclusion: Our style transfer method allows for the\nalignment of medical images taken with different scanners into a single unified\nstyle dataset, allowing for the training of other downstream tasks on such a\ndataset for tasks such as classification, object detection and others.\n""]",Artistic Style Transfer and Generation,Artistic and Creative AI Applications,Artificial Intelligence in Creative Industries
134,134,65,134_multimodal_videos_visual_language,"['multimodal', 'videos', 'visual', 'language', 'tasks', 'answering', 'mllm', 'benchmark', 'mind', 'modal']","['multimodal', 'reasoning', 'visual', 'videos', 'instruction', 'video', 'understanding', 'event', 'question', 'language']","['  Multimodal Large Language Models (MLLMs) demonstrate exceptional\nproblem-solving capabilities, but there is limited research focusing on their\nability to generate data by converting unlabeled images into visual instruction\ntuning data. To this end, this paper is the first to explore the potential of\nempowering MLLM to generate data rather than prompting GPT-4. We introduce\nGenixer, a holistic data generation pipeline consisting of four key steps: (i)\ninstruction data collection, (ii) instruction template design, (iii) empowering\nMLLMs, and (iv) data generation and filtering. Additionally, we outline two\nmodes of data generation: task-agnostic and task-specific, enabling\ncontrollable output. We demonstrate that a synthetic VQA-like dataset trained\nwith LLaVA1.5 enhances performance on 10 out of 12 multimodal benchmarks.\nAdditionally, the grounding MLLM Shikra, when trained with a REC-like synthetic\ndataset, shows improvements on 7 out of 8 REC datasets. Through experiments and\nsynthetic data analysis, our findings are: (1) current MLLMs can serve as\nrobust data generators without assistance from GPT-4V; (2) MLLMs trained with\ntask-specific datasets can surpass GPT-4V in generating complex instruction\ntuning data; (3) synthetic datasets enhance performance across various\nmultimodal benchmarks and help mitigate model hallucinations. The data, code,\nand models can be found at https://github.com/zhaohengyuan1/Genixer.\n', ""  Can large multimodal models have a human-like ability for emotional and\nsocial reasoning, and if so, how does it work? Recent research has discovered\nemergent theory-of-mind (ToM) reasoning capabilities in large language models\n(LLMs). LLMs can reason about people's mental states by solving various\ntext-based ToM tasks that ask questions about the actors' ToM (e.g., human\nbelief, desire, intention). However, human reasoning in the wild is often\ngrounded in dynamic scenes across time. Thus, we consider videos a new medium\nfor examining spatio-temporal ToM reasoning ability. Specifically, we ask\nexplicit probing questions about videos with abundant social and emotional\nreasoning content. We develop a pipeline for multimodal LLM for ToM reasoning\nusing video and text. We also enable explicit ToM reasoning by retrieving key\nframes for answering a ToM question, which reveals how multimodal LLMs reason\nabout ToM.\n"", '  Multimodal large language models (LLMs) have achieved notable success across\nvarious domains, while research in the medical field has largely focused on\nunimodal images. Meanwhile, current general-domain multimodal models for videos\nstill lack the capabilities to understand and engage in conversations about\nsurgical videos. One major contributing factor is the absence of datasets in\nthe surgical field. In this paper, we create a new dataset, Surg-QA, consisting\nof 102,000 surgical video-instruction pairs, the largest of its kind so far. To\nbuild such a dataset, we propose a novel two-stage question-answer generation\npipeline with LLM to learn surgical knowledge in a structured manner from the\npublicly available surgical lecture videos. The pipeline breaks down the\ngeneration process into two stages to significantly reduce the task complexity,\nallowing us to use a more affordable, locally deployed open-source LLM than the\npremium paid LLM services. It also mitigates the risk of LLM hallucinations\nduring question-answer generation, thereby enhancing the overall quality of the\ngenerated data. We further train LLaVA-Surg, a novel vision-language\nconversational assistant capable of answering open-ended questions about\nsurgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations\non zero-shot surgical video question-answering tasks. We show that LLaVA-Surg\nsignificantly outperforms all previous general-domain models, demonstrating\nexceptional multimodal conversational skills in answering open-ended questions\nabout surgical videos. We will release our code, model, and the\ninstruction-tuning dataset.\n']",Multimodal Language Models for Vision and Language Tasks,Vision-Language Models and Multimodal Learning,Multimodal Learning and Vision-Language Models
135,135,65,135_articulated_pose_poses_portrait,"['articulated', 'pose', 'poses', 'portrait', '3d', 'animation', 'avatars', 'vr', 'avatar', 'camera']","['pose', 'hand', 'avatars', 'body', 'rendering', 'motion', 'mesh', 'facial', 'avatar', 'virtual']","['  Hand avatars play a pivotal role in a wide array of digital interfaces,\nenhancing user immersion and facilitating natural interaction within virtual\nenvironments. While previous studies have focused on photo-realistic hand\nrendering, little attention has been paid to reconstruct the hand geometry with\nfine details, which is essential to rendering quality. In the realms of\nextended reality and gaming, on-the-fly rendering becomes imperative. To this\nend, we introduce an expressive hand avatar, named XHand, that is designed to\ncomprehensively generate hand shape, appearance, and deformations in real-time.\nTo obtain fine-grained hand meshes, we make use of three feature embedding\nmodules to predict hand deformation displacements, albedo, and linear blending\nskinning weights, respectively. To achieve photo-realistic hand rendering on\nfine-grained meshes, our method employs a mesh-based neural renderer by\nleveraging mesh topological consistency and latent codes from embedding\nmodules. During training, a part-aware Laplace smoothing strategy is proposed\nby incorporating the distinct levels of regularization to effectively maintain\nthe necessary details and eliminate the undesired artifacts. The experimental\nevaluations on InterHand2.6M and DeepHandMesh datasets demonstrate the efficacy\nof XHand, which is able to recover high-fidelity geometry and texture for hand\nanimations across diverse poses in real-time. To reproduce our results, we will\nmake the full implementation publicly available at\nhttps://github.com/agnJason/XHand.\n', ""  We introduce VOODOO XP: a 3D-aware one-shot head reenactment method that can\ngenerate highly expressive facial expressions from any input driver video and a\nsingle 2D portrait. Our solution is real-time, view-consistent, and can be\ninstantly used without calibration or fine-tuning. We demonstrate our solution\non a monocular video setting and an end-to-end VR telepresence system for\ntwo-way communication. Compared to 2D head reenactment methods, 3D-aware\napproaches aim to preserve the identity of the subject and ensure\nview-consistent facial geometry for novel camera poses, which makes them\nsuitable for immersive applications. While various facial disentanglement\ntechniques have been introduced, cutting-edge 3D-aware neural reenactment\ntechniques still lack expressiveness and fail to reproduce complex and\nfine-scale facial expressions. We present a novel cross-reenactment\narchitecture that directly transfers the driver's facial expressions to\ntransformer blocks of the input source's 3D lifting module. We show that highly\neffective disentanglement is possible using an innovative multi-stage\nself-supervision approach, which is based on a coarse-to-fine strategy,\ncombined with an explicit face neutralization and 3D lifted frontalization\nduring its initial training stage. We further integrate our novel head\nreenactment solution into an accessible high-fidelity VR telepresence system,\nwhere any person can instantly build a personalized neural head avatar from any\nphoto and bring it to life using the headset. We demonstrate state-of-the-art\nperformance in terms of expressiveness and likeness preservation on a large set\nof diverse subjects and capture conditions.\n"", '  Recent advances in generative diffusion models have enabled the previously\nunfeasible capability of generating 3D assets from a single input image or a\ntext prompt. In this work, we aim to enhance the quality and functionality of\nthese models for the task of creating controllable, photorealistic human\navatars. We achieve this by integrating a 3D morphable model into the\nstate-of-the-art multi-view-consistent diffusion approach. We demonstrate that\naccurate conditioning of a generative pipeline on the articulated 3D model\nenhances the baseline model performance on the task of novel view synthesis\nfrom a single image. More importantly, this integration facilitates a seamless\nand accurate incorporation of facial expression and body pose control into the\ngeneration process. To the best of our knowledge, our proposed framework is the\nfirst diffusion model to enable the creation of fully 3D-consistent,\nanimatable, and photorealistic human avatars from a single image of an unseen\nsubject; extensive quantitative and qualitative evaluations demonstrate the\nadvantages of our approach over existing state-of-the-art avatar creation\nmodels on both novel view and novel expression synthesis tasks. The code for\nour project is publicly available.\n']",3D Avatar Generation and Animation,Computer-Generated 3D Content Creation,Artificial Intelligence for Creative Content Generation
136,136,65,136_programming_programmers_programmer_coding,"['programming', 'programmers', 'programmer', 'coding', 'code', 'program', 'developers', 'software', 'agilecoder', 'prompts']","['software', 'code', 'engineering', 'programming', 'prompt', 'students', 'development', 'agents', 'prompts', 'agent']","[""  The increasing demand for programming language education and growing class\nsizes require immediate and personalized feedback. However, traditional code\nreview methods have limitations in providing this level of feedback. As the\ncapabilities of Large Language Models (LLMs) like GPT for generating accurate\nsolutions and timely code reviews are verified, this research proposes a system\nthat employs GPT-4 to offer learner-friendly code reviews and minimize the risk\nof AI-assist cheating.\n  To provide learner-friendly code reviews, a dataset was collected from an\nonline judge system, and this dataset was utilized to develop and enhance the\nsystem's prompts. In addition, to minimize AI-assist cheating, the system flow\nwas designed to provide code reviews only for code submitted by a learner, and\na feature that highlights code lines to fix was added. After the initial system\nwas deployed on the web, software education experts conducted usability test.\nBased on the results, improvement strategies were developed to improve code\nreview and code correctness check module, thereby enhancing the system.\n  The improved system underwent evaluation by software education experts based\non four criteria: strict code correctness checks, response time, lower API call\ncosts, and the quality of code reviews. The results demonstrated a performance\nto accurately identify error types, shorten response times, lower API call\ncosts, and maintain high-quality code reviews without major issues. Feedback\nfrom participants affirmed the tool's suitability for teaching programming to\nprimary and secondary school students. Given these benefits, the system is\nanticipated to be a efficient learning tool in programming language learning\nfor educational settings.\n"", '  With the rise of large language models (LLMs), researchers are increasingly\nexploring their applications in var ious vertical domains, such as software\nengineering. LLMs have achieved remarkable success in areas including code\ngeneration and vulnerability detection. However, they also exhibit numerous\nlimitations and shortcomings. LLM-based agents, a novel tech nology with the\npotential for Artificial General Intelligence (AGI), combine LLMs as the core\nfor decision-making and action-taking, addressing some of the inherent\nlimitations of LLMs such as lack of autonomy and self-improvement. Despite\nnumerous studies and surveys exploring the possibility of using LLMs in\nsoftware engineering, it lacks a clear distinction between LLMs and LLM based\nagents. It is still in its early stage for a unified standard and benchmarking\nto qualify an LLM solution as an LLM-based agent in its domain. In this survey,\nwe broadly investigate the current practice and solutions for LLMs and\nLLM-based agents for software engineering. In particular we summarise six key\ntopics: requirement engineering, code generation, autonomous decision-making,\nsoftware design, test generation, and software maintenance. We review and\ndifferentiate the work of LLMs and LLM-based agents from these six topics,\nexamining their differences and similarities in tasks, benchmarks, and\nevaluation metrics. Finally, we discuss the models and benchmarks used,\nproviding a comprehensive analysis of their applications and effectiveness in\nsoftware engineering. We anticipate this work will shed some lights on pushing\nthe boundaries of LLM-based agents in software engineering for future research.\n', ""  Large Language Models (LLMs) represent a leap in artificial intelligence,\nexcelling in tasks using human language(s). Although the main focus of\ngeneral-purpose LLMs is not code generation, they have shown promising results\nin the domain. However, the usefulness of LLMs in an academic software\nengineering project has not been fully explored yet. In this study, we explore\nthe usefulness of LLMs for 214 students working in teams consisting of up to\nsix members. Notably, in the academic course through which this study is\nconducted, students were encouraged to integrate LLMs into their development\ntool-chain, in contrast to most other academic courses that explicitly prohibit\nthe use of LLMs.\n  In this paper, we analyze the AI-generated code, prompts used for code\ngeneration, and the human intervention levels to integrate the code into the\ncode base. We also conduct a perception study to gain insights into the\nperceived usefulness, influencing factors, and future outlook of LLM from a\ncomputer science student's perspective. Our findings suggest that LLMs can play\na crucial role in the early stages of software development, especially in\ngenerating foundational code structures, and helping with syntax and error\ndebugging. These insights provide us with a framework on how to effectively\nutilize LLMs as a tool to enhance the productivity of software engineering\nstudents, and highlight the necessity of shifting the educational focus toward\npreparing students for successful human-AI collaboration.\n""]",Large Language Models in Software Engineering and Education,Applications of Large Language Models,Large Language Models
137,137,65,137_cognition_biases_bias_judgments,"['cognition', 'biases', 'bias', 'judgments', 'cognitive', 'behavioral', 'behavior', 'intelligence', 'decisions', 'language']","['cognitive', 'human', 'humans', 'behavior', 'biases', 'people', 'decision', 'judgments', 'language', 'sycophancy']","['  The emergence of large language models (LLMs) has opened up exciting\npossibilities for simulating human behavior and cognitive processes, with\npotential applications in various domains, including marketing research and\nconsumer behavior analysis. However, the validity of utilizing LLMs as\nstand-ins for human subjects remains uncertain due to glaring divergences that\nsuggest fundamentally different underlying processes at play and the\nsensitivity of LLM responses to prompt variations. This paper presents a novel\napproach based on Shapley values from cooperative game theory to interpret LLM\nbehavior and quantify the relative contribution of each prompt component to the\nmodel\'s output. Through two applications-a discrete choice experiment and an\ninvestigation of cognitive biases-we demonstrate how the Shapley value method\ncan uncover what we term ""token noise"" effects, a phenomenon where LLM\ndecisions are disproportionately influenced by tokens providing minimal\ninformative content. This phenomenon raises concerns about the robustness and\ngeneralizability of insights obtained from LLMs in the context of human\nbehavior simulation. Our model-agnostic approach extends its utility to\nproprietary LLMs, providing a valuable tool for marketers and researchers to\nstrategically optimize prompts and mitigate apparent cognitive biases. Our\nfindings underscore the need for a more nuanced understanding of the factors\ndriving LLM responses before relying on them as substitutes for human subjects\nin research settings. We emphasize the importance of researchers reporting\nresults conditioned on specific prompt templates and exercising caution when\ndrawing parallels between human behavior and LLMs.\n', '  The observed similarities in the behavior of humans and Large Language Models\n(LLMs) have prompted researchers to consider the potential of using LLMs as\nmodels of human cognition. However, several significant challenges must be\naddressed before LLMs can be legitimately regarded as cognitive models. For\ninstance, LLMs are trained on far more data than humans typically encounter,\nand may have been directly trained on human data in specific cognitive tasks or\naligned with human preferences. Consequently, the origins of these behavioral\nsimilarities are not well understood. In this paper, we propose a novel way to\nenhance the utility of LLMs as cognitive models. This approach involves (i)\nleveraging computationally equivalent tasks that both an LLM and a rational\nagent need to master for solving a cognitive problem and (ii) examining the\nspecific task distributions required for an LLM to exhibit human-like\nbehaviors. We apply this approach to decision-making -- specifically risky and\nintertemporal choice -- where the key computationally equivalent task is the\narithmetic of expected value calculations. We show that an LLM pretrained on an\necologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts\nhuman behavior better than many traditional cognitive models. Pretraining LLMs\non ecologically valid arithmetic datasets is sufficient to produce a strong\ncorrespondence between these models and human decision-making. Our results also\nsuggest that LLMs used as cognitive models should be carefully investigated via\nablation studies of the pretraining data.\n', ""  In order for AI systems to communicate effectively with people, they must\nunderstand how we make decisions. However, people's decisions are not always\nrational, so the implicit internal models of human decision-making in Large\nLanguage Models (LLMs) must account for this. Previous empirical evidence seems\nto suggest that these implicit models are accurate -- LLMs offer believable\nproxies of human behavior, acting how we expect humans would in everyday\ninteractions. However, by comparing LLM behavior and predictions to a large\ndataset of human decisions, we find that this is actually not the case: when\nboth simulating and predicting people's choices, a suite of cutting-edge LLMs\n(GPT-4o & 4-Turbo, Llama-3-8B & 70B, Claude 3 Opus) assume that people are more\nrational than we really are. Specifically, these models deviate from human\nbehavior and align more closely with a classic model of rational choice --\nexpected value theory. Interestingly, people also tend to assume that other\npeople are rational when interpreting their behavior. As a consequence, when we\ncompare the inferences that LLMs and people draw from the decisions of others\nusing another psychological dataset, we find that these inferences are highly\ncorrelated. Thus, the implicit decision-making models of LLMs appear to be\naligned with the human expectation that other people will act rationally,\nrather than with how people actually act.\n""]",Cognitive Biases in Large Language Models,Bias and Fairness in Large Language Models,Fairness and Bias in Artificial Intelligence
138,138,65,138_language_models_lms_decoding,"['language', 'models', 'lms', 'decoding', 'llms', 'deepseek', 'llama2', 'llm', 'throughput', 'computational']","['inference', 'cost', 'tuning', 'performance', 'source', 'efficiency', 'smaller', 'chat', 'latency', 'open']","['  Large Language Models (LLMs) have achieved remarkable results, but their\nincreasing resource demand has become a major obstacle to the development of\npowerful and accessible super-human intelligence. This report introduces\nJetMoE-8B, a new LLM trained with less than $0.1 million, using 1.25T tokens\nfrom carefully mixed open-source corpora and 30,000 H100 GPU hours. Despite its\nlow cost, the JetMoE-8B demonstrates impressive performance, with JetMoE-8B\noutperforming the Llama2-7B model and JetMoE-8B-Chat surpassing the\nLlama2-13B-Chat model. These results suggest that LLM training can be much more\ncost-effective than generally thought. JetMoE-8B is based on an efficient\nSparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention\nand feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B\nto have 8B parameters while only activating 2B for each input token, reducing\ninference computation by about 70% compared to Llama2-7B. Moreover, JetMoE-8B\nis highly open and academia-friendly, using only public datasets and training\ncode. All training parameters and data mixtures have been detailed in this\nreport to facilitate future efforts in the development of open foundation\nmodels. This transparency aims to encourage collaboration and further\nadvancements in the field of accessible and efficient LLMs. The model weights\nare publicly available at https://github.com/myshell-ai/JetMoE.\n', '  In this paper, we present Paramanu-Ganita, a 208 million parameter novel Auto\nRegressive (AR) decoder based language model on mathematics. The model is\npretrained from scratch at context size of 4096 on our curated mixed\nmathematical corpus. We evaluate our model on both perplexity metric and GSM8k\nmathematical benchmark. Paramanu-Ganita despite being 35 times smaller than 7B\nLLMs, outperformed generalist LLMs such as LLaMa-1 7B by 28.4% points, LLaMa-2\n7B by 27.6% points, Falcon 7B by 32.6% points, PaLM 8B by 35.3% points, and\nmath specialised LLMs such as Minerva 8B by 23.2% points, and LLEMMA-7B by 3.0%\npoints in GSM8k test accuracy metric respectively. Paramanu-Ganita also\noutperformed giant LLMs like PaLM 62B by 6.4% points, Falcon 40B by 19.8%\npoints, LLaMa-1 33B by 3.8% points and Vicuna 13B by 11.8% points respectively.\nThe large significant margin improvement in performance of our math model over\nthe existing LLMs signifies that reasoning capabilities of language model are\njust not restricted to LLMs with humongous number of parameters.\nParamanu-Ganita took 146 hours of A100 training whereas math specialised LLM,\nLLEMMA 7B, was trained for 23,000 A100 hours of training equivalent. Thus, our\napproach of pretraining powerful domain specialised language models from\nscratch for domain adaptation is much more cost-effective than performing\ncontinual training of LLMs for domain adaptation. Hence, we conclude that for\nstrong mathematical reasoning abilities of language model, we do not need giant\nLLMs and immense computing power to our end. In the end, we want to point out\nthat we have only trained Paramanu-Ganita only on a part of our entire\nmathematical corpus and yet to explore the full potential of our model.\n', ""  Despite the general capabilities of large pretrained language models, they\nconsistently benefit from further adaptation to better achieve desired\nbehaviors. However, tuning these models has become increasingly\nresource-intensive, or impossible when model weights are private. We introduce\nproxy-tuning, a lightweight decoding-time algorithm that operates on top of\nblack-box LMs to achieve the same end as direct tuning, but by accessing only\nits predictions over the output vocabulary, not its parameters. Our method\ntunes a smaller LM, then applies the difference between the predictions of the\nsmall tuned and untuned LMs to shift the original predictions of the larger\nuntuned model in the direction of tuning, while retaining the benefits of\nlarger-scale pretraining. In experiments, when we apply proxy-tuning to\nLlama2-70B using proxies of only 7B size, we can close 88% of the gap between\nLlama2-70B and its truly-tuned chat version, when evaluated across knowledge,\nreasoning, and safety benchmarks. Interestingly, on TruthfulQA, proxy-tuned\nmodels are actually more truthful than directly tuned models, possibly because\ndecoding-time guidance better retains the model's factual knowledge. We then\ndemonstrate the generality of proxy-tuning by applying it to domain adaptation\non code, and task-specific finetuning on question-answering and math problems.\nFinally, we show how to proxy-tune a truly black-box LM, GPT-3.5, for temporal\nadaptation, increasing its knowledge about recent events. Our work demonstrates\nthe promise of using small tuned LMs to efficiently customize large,\npotentially proprietary LMs through decoding-time guidance.\n""]",Efficient Large Language Models (LLMs) and Decoding Techniques,Optimization and Efficiency of Large Language Models,Large Language Models
139,139,65,139_regularization_kernels_lasso_classification,"['regularization', 'kernels', 'lasso', 'classification', 'learning', 'classifier', 'regularized', 'kernel', 'generalization', 'svm']","['kernel', 'regularization', 'linear', 'sparse', 'regression', 'kernels', 'sparsity', 'lasso', 'function', 'convergence']","['  In inverse problems, it is widely recognized that the incorporation of a\nsparsity prior yields a regularization effect on the solution. This approach is\ngrounded on the a priori assumption that the unknown can be appropriately\nrepresented in a basis with a limited number of significant components, while\nmost coefficients are close to zero. This occurrence is frequently observed in\nreal-world scenarios, such as with piecewise smooth signals. In this study, we\npropose a probabilistic sparsity prior formulated as a mixture of degenerate\nGaussians, capable of modeling sparsity with respect to a generic basis. Under\nthis premise, we design a neural network that can be interpreted as the Bayes\nestimator for linear inverse problems. Additionally, we put forth both a\nsupervised and an unsupervised training strategy to estimate the parameters of\nthis network. To evaluate the effectiveness of our approach, we conduct a\nnumerical comparison with commonly employed sparsity-promoting regularization\ntechniques, namely LASSO, group LASSO, iterative hard thresholding, and sparse\ncoding/dictionary learning. Notably, our reconstructions consistently exhibit\nlower mean square error values across all $1$D datasets utilized for the\ncomparisons, even in cases where the datasets significantly deviate from a\nGaussian mixture model.\n', '  This paper presents new and effective algorithms for learning kernels. In\nparticular, as shown by our empirical results, these algorithms consistently\noutperform the so-called uniform combination solution that has proven to be\ndifficult to improve upon in the past, as well as other algorithms for learning\nkernels based on convex combinations of base kernels in both classification and\nregression. Our algorithms are based on the notion of centered alignment which\nis used as a similarity measure between kernels or kernel matrices. We present\na number of novel algorithmic, theoretical, and empirical results for learning\nkernels based on our notion of centered alignment. In particular, we describe\nefficient algorithms for learning a maximum alignment kernel by showing that\nthe problem can be reduced to a simple QP and discuss a one-stage algorithm for\nlearning both a kernel and a hypothesis based on that kernel using an\nalignment-based regularization. Our theoretical results include a novel\nconcentration bound for centered alignment between kernel matrices, the proof\nof the existence of effective predictors for kernels with high alignment, both\nfor classification and for regression, and the proof of stability-based\ngeneralization bounds for a broad family of algorithms for learning kernels\nbased on centered alignment. We also report the results of experiments with our\ncentered alignment-based algorithms in both classification and regression.\n', '  Recent advances in machine learning have inspired a surge of research into\nreconstructing specific quantities of interest from measurements that comply\nwith certain physical laws. These efforts focus on inverse problems that are\ngoverned by partial differential equations (PDEs). In this work, we develop an\nasymptotic Sobolev norm learning curve for kernel ridge(less) regression when\naddressing (elliptical) linear inverse problems. Our results show that the PDE\noperators in the inverse problem can stabilize the variance and even behave\nbenign overfitting for fixed-dimensional problems, exhibiting different\nbehaviors from regression problems. Besides, our investigation also\ndemonstrates the impact of various inductive biases introduced by minimizing\ndifferent Sobolev norms as a form of implicit regularization. For the\nregularized least squares estimator, we find that all considered inductive\nbiases can achieve the optimal convergence rate, provided the regularization\nparameter is appropriately chosen. The convergence rate is actually independent\nto the choice of (smooth enough) inductive bias for both ridge and ridgeless\nregression. Surprisingly, our smoothness requirement recovered the condition\nfound in Bayesian setting and extend the conclusion to the minimum norm\ninterpolation estimators.\n']",Machine Learning for Inverse Problems and Kernel Methods,Machine Learning for Inverse Problems,Machine Learning and Optimization
140,140,65,140_supervised_labeling_classification_imagenet,"['supervised', 'labeling', 'classification', 'imagenet', 'labeled', 'labels', 'embeddings', 'learning', 'embedding', 'label']","['supervised', 'contrastive', 'unlabeled', 'self', 'loss', 'class', 'label', 'hierarchical', 'clustering', 'augmentations']","['  In self-supervised learning (SSL), representations are learned via an\nauxiliary task without annotated labels. A common task is to classify\naugmentations or different modalities of the data, which share semantic content\n(e.g. an object in an image) but differ in style (e.g. the object\'s location).\nMany approaches to self-supervised learning have been proposed, e.g. SimCLR,\nCLIP, and VicREG, which have recently gained much attention for their\nrepresentations achieving downstream performance comparable to supervised\nlearning. However, a theoretical understanding of self-supervised methods\neludes. Addressing this, we present a generative latent variable model for\nself-supervised learning and show that several families of discriminative SSL,\nincluding contrastive methods, induce a comparable distribution over\nrepresentations, providing a unifying theoretical framework for these methods.\nThe proposed model also justifies connections drawn to mutual information and\nthe use of a ""projection head"". Learning representations by fitting the model\ngeneratively (termed SimVAE) improves performance over discriminative and other\nVAE-based methods on simple image benchmarks and significantly narrows the gap\nbetween generative and discriminative representation learning in more complex\nsettings. Importantly, as our analysis predicts, SimVAE outperforms\nself-supervised learning where style information is required, taking an\nimportant step toward understanding self-supervised methods and achieving\ntask-agnostic representations.\n', '  The rapid advancement in self-supervised learning (SSL) has highlighted its\npotential to leverage unlabeled data for learning rich visual representations.\nHowever, the existing SSL techniques, particularly those employing different\naugmentations of the same image, often rely on a limited set of simple\ntransformations that are not representative of real-world data variations. This\nconstrains the diversity and quality of samples, which leads to sub-optimal\nrepresentations. In this paper, we introduce a novel framework that enriches\nthe SSL paradigm by utilizing generative models to produce semantically\nconsistent image augmentations. By directly conditioning generative models on a\nsource image representation, our method enables the generation of diverse\naugmentations while maintaining the semantics of the source image, thus\noffering a richer set of data for self-supervised learning. Our extensive\nexperimental results on various SSL methods demonstrate that our framework\nsignificantly enhances the quality of learned visual representations by up to\n10\\% Top-1 accuracy in downstream tasks. This research demonstrates that\nincorporating generative models into the SSL workflow opens new avenues for\nexploring the potential of synthetic data. This development paves the way for\nmore robust and versatile representation learning techniques.\n', '  In this work, we propose a novel supervised contrastive loss that enables the\nintegration of taxonomic hierarchy information during the representation\nlearning process. A supervised contrastive loss operates by enforcing that\nimages with the same class label (positive samples) project closer to each\nother than images with differing class labels (negative samples). The advantage\nof this approach is that it directly penalizes the structure of the\nrepresentation space itself. This enables greater flexibility with respect to\nencoding semantic concepts. However, the standard supervised contrastive loss\nonly enforces semantic structure based on the downstream task (i.e. the class\nlabel). In reality, the class label is only one level of a \\emph{hierarchy of\ndifferent semantic relationships known as a taxonomy}. For example, the class\nlabel is oftentimes the species of an animal, but between different classes\nthere are higher order relationships such as all animals with wings being\n``birds"". We show that by explicitly accounting for these relationships with a\nweighting penalty in the contrastive loss we can out-perform the supervised\ncontrastive loss. Additionally, we demonstrate the adaptability of the notion\nof a taxonomy by integrating our loss into medical and noise-based settings\nthat show performance improvements by as much as 7%.\n']",Self-Supervised Learning and Representation,Self-Supervised Learning and Representation Learning,Self-Supervised Representation Learning
141,141,65,141_disinformation_misinformation_debunking_persuasive,"['disinformation', 'misinformation', 'debunking', 'persuasive', 'deception', 'fallacy', 'biases', 'fallacies', 'bias', 'headlines']","['misinformation', 'search', 'fairness', 'climate', 'disinformation', 'news', 'content', 'fact', 'claims', 'bias']","['  Scientific facts are often spun in the popular press with the intent to\ninfluence public opinion and action, as was evidenced during the COVID-19\npandemic. Automatic detection of misinformation in the scientific domain is\nchallenging because of the distinct styles of writing in these two media types\nand is still in its nascence. Most research on the validity of scientific\nreporting treats this problem as a claim verification challenge. In doing so,\nsignificant expert human effort is required to generate appropriate claims. Our\nsolution bypasses this step and addresses a more real-world scenario where such\nexplicit, labeled claims may not be available. The central research question of\nthis paper is whether it is possible to use large language models (LLMs) to\ndetect misinformation in scientific reporting. To this end, we first present a\nnew labeled dataset SciNews, containing 2.4k scientific news stories drawn from\ntrusted and untrustworthy sources, paired with related abstracts from the\nCORD-19 database. Our dataset includes both human-written and LLM-generated\nnews articles, making it more comprehensive in terms of capturing the growing\ntrend of using LLMs to generate popular press articles. Then, we identify\ndimensions of scientific validity in science news articles and explore how this\ncan be integrated into the automated detection of scientific misinformation. We\npropose several baseline architectures using LLMs to automatically detect false\nrepresentations of scientific findings in the popular press. For each of these\narchitectures, we use several prompt engineering strategies including\nzero-shot, few-shot, and chain-of-thought prompting. We also test these\narchitectures and prompting strategies on GPT-3.5, GPT-4, and Llama2-7B,\nLlama2-13B.\n', ""  The pervasive spread of misinformation and disinformation poses a significant\nthreat to society. Professional fact-checkers play a key role in addressing\nthis threat, but the vast scale of the problem forces them to prioritize their\nlimited resources. This prioritization may consider a range of factors, such as\nvarying risks of harm posed to specific groups of people. In this work, we\ninvestigate potential implications of using a large language model (LLM) to\nfacilitate such prioritization. Because fact-checking impacts a wide range of\ndiverse segments of society, it is important that diverse views are represented\nin the claim prioritization process. This paper examines whether a LLM can\nreflect the views of various groups when assessing the harms of misinformation,\nfocusing on gender as a primary variable. We pose two central questions: (1) To\nwhat extent do prompts with explicit gender references reflect gender\ndifferences in opinion in the United States on topics of social relevance? and\n(2) To what extent do gender-neutral prompts align with gendered viewpoints on\nthose topics? To analyze these questions, we present the TopicMisinfo dataset,\ncontaining 160 fact-checked claims from diverse topics, supplemented by nearly\n1600 human annotations with subjective perceptions and annotator demographics.\nAnalyzing responses to gender-specific and neutral prompts, we find that GPT\n3.5-Turbo reflects empirically observed gender differences in opinion but\namplifies the extent of these differences. These findings illuminate AI's\ncomplex role in moderating online communication, with implications for\nfact-checkers, algorithm designers, and the use of crowd-workers as annotators.\nWe also release the TopicMisinfo dataset to support continuing research in the\ncommunity.\n"", '  Misinformation about climate change is a complex societal issue requiring\nholistic, interdisciplinary solutions at the intersection between technology\nand psychology. One proposed solution is a ""technocognitive"" approach,\ninvolving the synthesis of psychological and computer science research.\nPsychological research has identified that interventions in response to\nmisinformation require both fact-based (e.g., factual explanations) and\ntechnique-based (e.g., explanations of misleading techniques) content. However,\nlittle progress has been made on documenting and detecting fallacies in climate\nmisinformation. In this study, we apply a previously developed critical\nthinking methodology for deconstructing climate misinformation, in order to\ndevelop a dataset mapping different types of climate misinformation to\nreasoning fallacies. This dataset is used to train a model to detect fallacies\nin climate misinformation. Our study shows F1 scores that are 2.5 to 3.5 better\nthan previous works. The fallacies that are easiest to detect include fake\nexperts and anecdotal arguments, while fallacies that require background\nknowledge, such as oversimplification, misrepresentation, and slothful\ninduction, are relatively more difficult to detect. This research lays the\ngroundwork for development of solutions where automatically detected climate\nmisinformation can be countered with generative technique-based corrections.\n']",Detecting Misinformation and Disinformation,Social Media and Information Dynamics,Information Dynamics and Network Influence
142,142,64,142_annotating_annotations_annotators_annotation,"['annotating', 'annotations', 'annotators', 'annotation', 'annotate', 'annotated', 'annotator', 'labeling', 'nlp', 'crowdsourcing']","['annotation', 'labels', 'annotators', 'annotations', 'label', 'annotator', 'text', 'human', 'labeling', 'classification']","['  Manual data annotation is an important NLP task but one that takes\nconsiderable amount of resources and effort. In spite of the costs, labeling\nand categorizing entities is essential for NLP tasks such as semantic\nevaluation. Even though annotation can be done by non-experts in most cases,\ndue to the fact that this requires human labor, the process is costly. Another\nmajor challenge encountered in data annotation is maintaining the annotation\nconsistency. Annotation efforts are typically carried out by teams of multiple\nannotators. The annotations need to maintain the consistency in relation to\nboth the domain truth and annotation format while reducing human errors.\nAnnotating a specialized domain that deviates significantly from the general\ndomain, such as fantasy literature, will see a lot of human error and annotator\ndisagreement. So it is vital that proper guidelines and error reduction\nmechanisms are enforced. One such way to enforce these constraints is using a\nspecialized application. Such an app can ensure that the notations are\nconsistent, and the labels can be pre-defined or restricted reducing the room\nfor errors. In this paper, we present SHADE, an annotation software that can be\nused to annotate entities in the high fantasy literature domain. Specifically\nin Dungeons and Dragons lore extracted from the Forgotten Realms Fandom Wiki.\n', '  Data annotation generally refers to the labeling or generating of raw data\nwith relevant information, which could be used for improving the efficacy of\nmachine learning models. The process, however, is labor-intensive and costly.\nThe emergence of advanced Large Language Models (LLMs), exemplified by GPT-4,\npresents an unprecedented opportunity to automate the complicated process of\ndata annotation. While existing surveys have extensively covered LLM\narchitecture, training, and general applications, we uniquely focus on their\nspecific utility for data annotation. This survey contributes to three core\naspects: LLM-Based Annotation Generation, LLM-Generated Annotations Assessment,\nand LLM-Generated Annotations Utilization. Furthermore, this survey includes an\nin-depth taxonomy of data types that LLMs can annotate, a comprehensive review\nof learning strategies for models utilizing LLM-generated annotations, and a\ndetailed discussion of the primary challenges and limitations associated with\nusing LLMs for data annotation. Serving as a key guide, this survey aims to\nassist researchers and practitioners in exploring the potential of the latest\nLLMs for data annotation, thereby fostering future advancements in this\ncritical field.\n', '  Counterfactual examples are frequently used for model development and\nevaluation in many natural language processing (NLP) tasks. Although methods\nfor automated counterfactual generation have been explored, such methods depend\non models such as pre-trained language models that are then fine-tuned on\nauxiliary, often task-specific datasets. Collecting and annotating such\ndatasets for counterfactual generation is labor intensive and therefore,\ninfeasible in practice. Therefore, in this work, we focus on a novel problem\nsetting: \\textit{zero-shot counterfactual generation}. To this end, we propose\na structured way to utilize large language models (LLMs) as general purpose\ncounterfactual example generators. We hypothesize that the\ninstruction-following and textual understanding capabilities of recent LLMs can\nbe effectively leveraged for generating high quality counterfactuals in a\nzero-shot manner, without requiring any training or fine-tuning. Through\ncomprehensive experiments on various downstream tasks in natural language\nprocessing (NLP), we demonstrate the efficacy of LLMs as zero-shot\ncounterfactual generators in evaluating and explaining black-box NLP models.\n']",Data Annotation in NLP,Natural Language Processing (NLP) Techniques and Applications,Natural Language Processing
143,143,64,143_hashing_imagenet_hash_cnn,"['hashing', 'imagenet', 'hash', 'cnn', 'cnns', 'retrieval', 'convolutional', 'images', 'features', 'embedding']","['hashing', 'distillation', 'student', 'hash', 'image', 'knowledge', 'category', 'quantization', 'semantic', 'pixel']","['  Deep hashing techniques have emerged as the predominant approach for\nefficient image retrieval. Traditionally, these methods utilize pre-trained\nconvolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature\nextractors. However, the increasing complexity of datasets poses challenges for\nthese backbone architectures in capturing meaningful features essential for\neffective image retrieval. In this study, we explore the efficacy of employing\nhigh-resolution features learned through state-of-the-art techniques for image\nretrieval tasks. Specifically, we propose a novel methodology that utilizes\nHigh-Resolution Networks (HRNets) as the backbone for the deep hashing task,\ntermed High-Resolution Hashing Network (HHNet). Our approach demonstrates\nsuperior performance compared to existing methods across all tested benchmark\ndatasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performance\nimprovement is more pronounced for complex datasets, which highlights the need\nto learn high-resolution features for intricate image retrieval tasks.\nFurthermore, we conduct a comprehensive analysis of different HRNet\nconfigurations and provide insights into the optimal architecture for the deep\nhashing task\n', ""  Previous knowledge distillation (KD) methods mostly focus on compressing\nnetwork architectures, which is not thorough enough in deployment as some costs\nlike transmission bandwidth and imaging equipment are related to the image\nsize. Therefore, we propose Pixel Distillation that extends knowledge\ndistillation into the input level while simultaneously breaking architecture\nconstraints. Such a scheme can achieve flexible cost control for deployment, as\nit allows the system to adjust both network architecture and image quality\naccording to the overall requirement of resources. Specifically, we first\npropose an input spatial representation distillation (ISRD) mechanism to\ntransfer spatial knowledge from large images to student's input module, which\ncan facilitate stable knowledge transfer between CNN and ViT. Then, a\nTeacher-Assistant-Student (TAS) framework is further established to disentangle\npixel distillation into the model compression stage and input compression\nstage, which significantly reduces the overall complexity of pixel distillation\nand the difficulty of distilling intermediate knowledge. Finally, we adapt\npixel distillation to object detection via an aligned feature for preservation\n(AFP) strategy for TAS, which aligns output dimensions of detectors at each\nstage by manipulating features and anchors of the assistant. Comprehensive\nexperiments on image classification and object detection demonstrate the\neffectiveness of our method. Code is available at\nhttps://github.com/gyguo/PixelDistillation.\n"", '  Unsupervised semantic hashing has emerged as an indispensable technique for\nfast image search, which aims to convert images into binary hash codes without\nrelying on labels. Recent advancements in the field demonstrate that employing\nlarge-scale backbones (e.g., ViT) in unsupervised semantic hashing models can\nyield substantial improvements. However, the inference delay has become\nincreasingly difficult to overlook. Knowledge distillation provides a means for\npractical model compression to alleviate this delay. Nevertheless, the\nprevailing knowledge distillation approaches are not explicitly designed for\nsemantic hashing. They ignore the unique search paradigm of semantic hashing,\nthe inherent necessities of the distillation process, and the property of hash\ncodes. In this paper, we propose an innovative Bit-mask Robust Contrastive\nknowledge Distillation (BRCD) method, specifically devised for the distillation\nof semantic hashing models. To ensure the effectiveness of two kinds of search\nparadigms in the context of semantic hashing, BRCD first aligns the semantic\nspaces between the teacher and student models through a contrastive knowledge\ndistillation objective. Additionally, to eliminate noisy augmentations and\nensure robust optimization, a cluster-based method within the knowledge\ndistillation process is introduced. Furthermore, through a bit-level analysis,\nwe uncover the presence of redundancy bits resulting from the bit independence\nproperty. To mitigate these effects, we introduce a bit mask mechanism in our\nknowledge distillation objective. Finally, extensive experiments not only\nshowcase the noteworthy performance of our BRCD method in comparison to other\nknowledge distillation methods but also substantiate the generality of our\nmethods across diverse semantic hashing models and backbones. The code for BRCD\nis available at https://github.com/hly1998/BRCD.\n']",Deep Hashing for Image Retrieval,Efficient Models for Information Retrieval,Information Retrieval and Knowledge Systems
144,144,64,144_llms_language_modelizer_llm,"['llms', 'language', 'modelizer', 'llm', 'benchmarks', 'expertise', 'models', 'knowledge', 'training', 'optimizers']","['expert', 'tuning', 'optimizer', 'cost', 'fine', 'language', 'tasks', 'models', 'optimization', 'fusion']","[""  Large language models (LLMs) hold the promise of solving diverse tasks when\nprovided with appropriate natural language prompts. However, prompting often\nleads models to make predictions with lower accuracy compared to finetuning a\nmodel with ample training data. On the other hand, while finetuning LLMs on\ntask-specific data generally improves their performance, abundant annotated\ndatasets are not available for all tasks. Previous work has explored generating\ntask-specific data from state-of-the-art LLMs and using this data to finetune\nsmaller models, but this approach requires access to a language model other\nthan the one being trained, which introduces cost, scalability challenges, and\nlegal hurdles associated with continuously relying on more powerful LLMs. In\nresponse to these, we propose SELF-GUIDE, a multi-stage mechanism in which we\nsynthesize task-specific input-output pairs from the student LLM, then use\nthese input-output pairs to finetune the student LLM itself. In our empirical\nevaluation of the Natural Instructions V2 benchmark, we find that SELF-GUIDE\nimproves the performance of LLM by a substantial margin. Specifically, we\nreport an absolute improvement of approximately 15% for classification tasks\nand 18% for generation tasks in the benchmark's metrics. This sheds light on\nthe promise of self-synthesized data guiding LLMs towards becoming\ntask-specific experts without any external learning signals.\n"", ""  Despite the strong performance of large language models (LLMs) across a wide\nrange of tasks, they still have reliability issues. Previous studies indicate\nthat strong LLMs like GPT-4-turbo excel in evaluating the reliability of\nresponses from LLMs, but face efficiency and local deployment issues. Thus, to\nenable weak LLMs to effectively assess the reliability of LLM responses, we\npropose a novel cross-query-comparison-based method called $\\textit{Meta\nRanking}$ (MR). Unlike previous few-shot methods that solely based on\nin-context learning capabilities in LLMs, MR assesses reliability by pairwisely\nranking the target query-response pair with multiple reference query-response\npairs. We found that MR is highly effective in error detection for LLM\nresponses, where weak LLMs, such as Phi-2, could surpass strong baselines like\nGPT-3.5-turbo, requiring only five reference samples and significantly\nimproving efficiency. We further demonstrate that MR can enhance strong LLMs'\nperformance in two practical applications: model cascading and instruction\ntuning. In model cascading, we combine open- and closed-source LLMs to achieve\nperformance comparable to GPT-4-turbo with lower costs. In instruction tuning,\nwe use MR for iterative training data filtering, significantly reducing data\nprocessing time and enabling LLaMA-7B and Phi-2 to surpass Alpaca-13B with\nfewer training tokens. These results underscore the high potential of MR in\nboth efficiency and effectiveness.\n"", '  While training large language models (LLMs) from scratch can indeed lead to\nmodels with distinct capabilities and strengths, it incurs substantial costs\nand may lead to redundancy in competencies. Knowledge fusion aims to integrate\nexisting LLMs of diverse architectures and capabilities into a more potent LLM\nthrough lightweight continual training, thereby reducing the need for costly\nLLM development. In this work, we propose a new framework for the knowledge\nfusion of chat LLMs through two main stages, resulting in FuseChat. Firstly, we\nconduct pairwise knowledge fusion on source chat LLMs of varying structures and\nscales to create multiple target LLMs with identical structure and size via\nlightweight fine-tuning. During this process, a statistics-based token\nalignment approach is introduced as the cornerstone for fusing LLMs with\ndifferent structures. Secondly, we merge these target LLMs within the parameter\nspace, where we propose a novel method for determining the merging coefficients\nbased on the magnitude of parameter updates before and after fine-tuning. We\nimplement and validate FuseChat using six prominent chat LLMs with diverse\narchitectures and scales, including OpenChat-3.5-7B, Starling-LM-7B-alpha,\nNH2-SOLAR-10.7B, InternLM2-Chat-20B, Mixtral-8x7B-Instruct, and\nQwen-1.5-Chat-72B. Experimental results on two instruction-following\nbenchmarks, AlpacaEval 2.0 and MT-Bench, demonstrate the superiority of\nFuseChat-7B over baselines of various sizes. Our model is even comparable to\nthe larger Mixtral-8x7B-Instruct and approaches GPT-3.5-Turbo-1106 on MT-Bench.\nOur code, model weights, and data are public at\n\\url{https://github.com/fanqiwan/FuseAI}.\n']",Large Language Models (LLMs) Training and Optimization,Optimization and Efficiency of Large Language Models,Large Language Models
145,145,64,145_aesthetics_aesthetic_multimodal_visual,"['aesthetics', 'aesthetic', 'multimodal', 'visual', 'attributes', 'generative', 'creative', 'ratings', 'colors', 'perception']","['aesthetic', 'preference', 'image', 'multimodal', 'human', 'aesthetics', 'visual', 'preferences', 'editing', 'images']","[""  Interior design is all about creating spaces that look and feel good.\nHowever, the subjective nature of aesthetic preferences presents a significant\nchallenge in defining and quantifying what makes an interior design visually\nappealing. The current paper addresses this gap by introducing a novel\nmethodology for quantifying and predicting aesthetic preferences in interior\ndesign. Our study combines fuzzy logic with image processing techniques. We\ncollected a dataset of interior design images from social media platforms,\nfocusing on essential visual attributes such as color harmony, lightness, and\ncomplexity. We integrate these features using weighted average to compute a\ngeneral aesthetic score. Our approach considers individual color preferences in\ncalculating the overall aesthetic preference. We initially gather user ratings\nfor primary colors like red, brown, and others to understand their preferences.\nThen, we use the pixel count of the top five dominant colors in the image to\nget the color scheme preference. The color scheme preference and the aesthetic\nscore are then passed as inputs to the fuzzy inference system to calculate an\noverall preference score. This score represents a comprehensive measure of the\nuser's preference for a particular interior design, considering their color\nchoices and general aesthetic appeal. We used the 2AFC (Two-Alternative Forced\nChoice) method to validate our methodology, achieving a notable hit rate of\n0.7. This study can help designers and professionals better understand and meet\npeople's interior design preferences, especially in a world that relies heavily\non digital media.\n"", ""  As people's aesthetic preferences for images are far from understood, image\naesthetic assessment is a challenging artificial intelligence task. The range\nof factors underlying this task is almost unlimited, but we know that some\naesthetic attributes affect those preferences. In this study, we present a\nmulti-task convolutional neural network that takes into account these\nattributes. The proposed neural network jointly learns the attributes along\nwith the overall aesthetic scores of images. This multi-task learning framework\nallows for effective generalization through the utilization of shared\nrepresentations. Our experiments demonstrate that the proposed method\noutperforms the state-of-the-art approaches in predicting overall aesthetic\nscores for images in one benchmark of image aesthetics. We achieve near-human\nperformance in terms of overall aesthetic scores when considering the\nSpearman's rank correlations. Moreover, our model pioneers the application of\nmulti-tasking in another benchmark, serving as a new baseline for future\nresearch. Notably, our approach achieves this performance while using fewer\nparameters compared to existing multi-task neural networks in the literature,\nand consequently makes our method more efficient in terms of computational\ncomplexity.\n"", ""  Modern vision models are trained on very large noisy datasets. While these\nmodels acquire strong capabilities, they may not follow the user's intent to\noutput the desired results in certain aspects, e.g., visual aesthetic,\npreferred style, and responsibility. In this paper, we target the realm of\nvisual aesthetics and aim to align vision models with human aesthetic standards\nin a retrieval system. Advanced retrieval systems usually adopt a cascade of\naesthetic models as re-rankers or filters, which are limited to low-level\nfeatures like saturation and perform poorly when stylistic, cultural or\nknowledge contexts are involved. We find that utilizing the reasoning ability\nof large language models (LLMs) to rephrase the search query and extend the\naesthetic expectations can make up for this shortcoming. Based on the above\nfindings, we propose a preference-based reinforcement learning method that\nfine-tunes the vision models to distill the knowledge from both LLMs reasoning\nand the aesthetic models to better align the vision models with human\naesthetics. Meanwhile, with rare benchmarks designed for evaluating retrieval\nsystems, we leverage large multi-modality model (LMM) to evaluate the aesthetic\nperformance with their strong abilities. As aesthetic assessment is one of the\nmost subjective tasks, to validate the robustness of LMM, we further propose a\nnovel dataset named HPIR to benchmark the alignment with human aesthetics.\nExperiments demonstrate that our method significantly enhances the aesthetic\nbehaviors of the vision models, under several metrics. We believe the proposed\nalgorithm can be a general practice for aligning vision models with human\nvalues.\n""]",Image Aesthetics Assessment,Image Processing and Enhancement,Image and Video Processing
146,146,63,146_tokenizers_language_datasets_nlp,"['tokenizers', 'language', 'datasets', 'nlp', 'models', 'feature', 'dataset', 'generating', 'trained', 'tokens']","['glitch', 'tokens', 'language', 'synthetic', 'data', 'feature', 'quality', 'open', 'datasets', 'research']","[""  The rapid advancement of large language models (LLMs) has sparked interest in\ndata synthesis techniques, aiming to generate diverse and high-quality\nsynthetic datasets. However, these synthetic datasets often suffer from a lack\nof diversity and added noise. In this paper, we present TarGEN, a multi-step\nprompting strategy for generating high-quality synthetic datasets utilizing a\nLLM. An advantage of TarGEN is its seedless nature; it does not require\nspecific task instances, broadening its applicability beyond task replication.\nWe augment TarGEN with a method known as self-correction empowering LLMs to\nrectify inaccurately labeled instances during dataset creation, ensuring\nreliable labels. To assess our technique's effectiveness, we emulate 8 tasks\nfrom the SuperGLUE benchmark and finetune various language models, including\nencoder-only, encoder-decoder, and decoder-only models on both synthetic and\noriginal training sets. Evaluation on the original test set reveals that models\ntrained on datasets generated by TarGEN perform approximately 1-2% points\nbetter than those trained on original datasets (82.84% via syn. vs. 81.12% on\nog. using Flan-T5). When incorporating instruction tuning, the performance\nincreases to 84.54% on synthetic data vs. 81.49% on original data by Flan-T5. A\ncomprehensive analysis of the synthetic dataset compared to the original\ndataset reveals that the synthetic dataset demonstrates similar or higher\nlevels of dataset complexity and diversity. Furthermore, the synthetic dataset\ndisplays a bias level that aligns closely with the original dataset. Finally,\nwhen pre-finetuned on our synthetic SuperGLUE dataset, T5-3B yields impressive\nresults on the OpenLLM leaderboard, surpassing the model trained on the\nSelf-Instruct dataset by 4.14% points. We hope that TarGEN can be helpful for\nquality data generation and reducing the human efforts to create complex\nbenchmarks.\n"", '  With the expanding application of Large Language Models (LLMs) in various\ndomains, it becomes imperative to comprehensively investigate their unforeseen\nbehaviors and consequent outcomes. In this study, we introduce and\nsystematically explore the phenomenon of ""glitch tokens"", which are anomalous\ntokens produced by established tokenizers and could potentially compromise the\nmodels\' quality of response. Specifically, we experiment on seven top popular\nLLMs utilizing three distinct tokenizers and involving a totally of 182,517\ntokens. We present categorizations of the identified glitch tokens and symptoms\nexhibited by LLMs when interacting with glitch tokens. Based on our observation\nthat glitch tokens tend to cluster in the embedding space, we propose\nGlitchHunter, a novel iterative clustering-based technique, for efficient\nglitch token detection. The evaluation shows that our approach notably\noutperforms three baseline methods on eight open-source LLMs. To the best of\nour knowledge, we present the first comprehensive study on glitch tokens. Our\nnew detection further provides valuable insights into mitigating\ntokenization-related errors in LLMs.\n', '  Large language models (LLMs) have achieved unprecedented success in the field\nof natural language processing. However, the black-box nature of their internal\nmechanisms has brought many concerns about their trustworthiness and\ninterpretability. Recent research has discovered a class of abnormal tokens in\nthe model\'s vocabulary space and named them ""glitch tokens"". Those tokens, once\nincluded in the input, may induce the model to produce incorrect, irrelevant,\nor even harmful results, drastically undermining the reliability and\npracticality of LLMs.\n  In this work, we aim to enhance the understanding of glitch tokens and\npropose techniques for their detection and mitigation. We first reveal the\ncharacteristic features induced by glitch tokens on LLMs, which are evidenced\nby significant deviations in the distributions of attention patterns and\ndynamic information from intermediate model layers. Based on the insights, we\ndevelop GlitchProber, a tool for efficient glitch token detection and\nmitigation. GlitchProber utilizes small-scale sampling, principal component\nanalysis for accelerated feature extraction, and a simple classifier for\nefficient vocabulary screening. Taking one step further, GlitchProber rectifies\nabnormal model intermediate layer values to mitigate the destructive effects of\nglitch tokens. Evaluated on five mainstream open-source LLMs, GlitchProber\ndemonstrates higher efficiency, precision, and recall compared to existing\napproaches, with an average F1 score of 0.86 and an average repair rate of\n50.06%. GlitchProber unveils a novel path to address the challenges posed by\nglitch tokens and inspires future research toward more robust and interpretable\nLLMs.\n']",Large Language Models and Dataset Generation,Advances in Large Language Models,Large Language Models
146,146,63,146_tokenizers_language_datasets_nlp,"['tokenizers', 'language', 'datasets', 'nlp', 'models', 'feature', 'dataset', 'generating', 'trained', 'tokens']","['glitch', 'tokens', 'language', 'synthetic', 'data', 'feature', 'quality', 'open', 'datasets', 'research']","[""  The rapid advancement of large language models (LLMs) has sparked interest in\ndata synthesis techniques, aiming to generate diverse and high-quality\nsynthetic datasets. However, these synthetic datasets often suffer from a lack\nof diversity and added noise. In this paper, we present TarGEN, a multi-step\nprompting strategy for generating high-quality synthetic datasets utilizing a\nLLM. An advantage of TarGEN is its seedless nature; it does not require\nspecific task instances, broadening its applicability beyond task replication.\nWe augment TarGEN with a method known as self-correction empowering LLMs to\nrectify inaccurately labeled instances during dataset creation, ensuring\nreliable labels. To assess our technique's effectiveness, we emulate 8 tasks\nfrom the SuperGLUE benchmark and finetune various language models, including\nencoder-only, encoder-decoder, and decoder-only models on both synthetic and\noriginal training sets. Evaluation on the original test set reveals that models\ntrained on datasets generated by TarGEN perform approximately 1-2% points\nbetter than those trained on original datasets (82.84% via syn. vs. 81.12% on\nog. using Flan-T5). When incorporating instruction tuning, the performance\nincreases to 84.54% on synthetic data vs. 81.49% on original data by Flan-T5. A\ncomprehensive analysis of the synthetic dataset compared to the original\ndataset reveals that the synthetic dataset demonstrates similar or higher\nlevels of dataset complexity and diversity. Furthermore, the synthetic dataset\ndisplays a bias level that aligns closely with the original dataset. Finally,\nwhen pre-finetuned on our synthetic SuperGLUE dataset, T5-3B yields impressive\nresults on the OpenLLM leaderboard, surpassing the model trained on the\nSelf-Instruct dataset by 4.14% points. We hope that TarGEN can be helpful for\nquality data generation and reducing the human efforts to create complex\nbenchmarks.\n"", '  With the expanding application of Large Language Models (LLMs) in various\ndomains, it becomes imperative to comprehensively investigate their unforeseen\nbehaviors and consequent outcomes. In this study, we introduce and\nsystematically explore the phenomenon of ""glitch tokens"", which are anomalous\ntokens produced by established tokenizers and could potentially compromise the\nmodels\' quality of response. Specifically, we experiment on seven top popular\nLLMs utilizing three distinct tokenizers and involving a totally of 182,517\ntokens. We present categorizations of the identified glitch tokens and symptoms\nexhibited by LLMs when interacting with glitch tokens. Based on our observation\nthat glitch tokens tend to cluster in the embedding space, we propose\nGlitchHunter, a novel iterative clustering-based technique, for efficient\nglitch token detection. The evaluation shows that our approach notably\noutperforms three baseline methods on eight open-source LLMs. To the best of\nour knowledge, we present the first comprehensive study on glitch tokens. Our\nnew detection further provides valuable insights into mitigating\ntokenization-related errors in LLMs.\n', '  Large language models (LLMs) have achieved unprecedented success in the field\nof natural language processing. However, the black-box nature of their internal\nmechanisms has brought many concerns about their trustworthiness and\ninterpretability. Recent research has discovered a class of abnormal tokens in\nthe model\'s vocabulary space and named them ""glitch tokens"". Those tokens, once\nincluded in the input, may induce the model to produce incorrect, irrelevant,\nor even harmful results, drastically undermining the reliability and\npracticality of LLMs.\n  In this work, we aim to enhance the understanding of glitch tokens and\npropose techniques for their detection and mitigation. We first reveal the\ncharacteristic features induced by glitch tokens on LLMs, which are evidenced\nby significant deviations in the distributions of attention patterns and\ndynamic information from intermediate model layers. Based on the insights, we\ndevelop GlitchProber, a tool for efficient glitch token detection and\nmitigation. GlitchProber utilizes small-scale sampling, principal component\nanalysis for accelerated feature extraction, and a simple classifier for\nefficient vocabulary screening. Taking one step further, GlitchProber rectifies\nabnormal model intermediate layer values to mitigate the destructive effects of\nglitch tokens. Evaluated on five mainstream open-source LLMs, GlitchProber\ndemonstrates higher efficiency, precision, and recall compared to existing\napproaches, with an average F1 score of 0.86 and an average repair rate of\n50.06%. GlitchProber unveils a novel path to address the challenges posed by\nglitch tokens and inspires future research toward more robust and interpretable\nLLMs.\n']",Large Language Models and Dataset Generation,Advances in Large Language Models,Large Language Models
146,146,63,146_tokenizers_language_datasets_nlp,"['tokenizers', 'language', 'datasets', 'nlp', 'models', 'feature', 'dataset', 'generating', 'trained', 'tokens']","['glitch', 'tokens', 'language', 'synthetic', 'data', 'feature', 'quality', 'open', 'datasets', 'research']","[""  The rapid advancement of large language models (LLMs) has sparked interest in\ndata synthesis techniques, aiming to generate diverse and high-quality\nsynthetic datasets. However, these synthetic datasets often suffer from a lack\nof diversity and added noise. In this paper, we present TarGEN, a multi-step\nprompting strategy for generating high-quality synthetic datasets utilizing a\nLLM. An advantage of TarGEN is its seedless nature; it does not require\nspecific task instances, broadening its applicability beyond task replication.\nWe augment TarGEN with a method known as self-correction empowering LLMs to\nrectify inaccurately labeled instances during dataset creation, ensuring\nreliable labels. To assess our technique's effectiveness, we emulate 8 tasks\nfrom the SuperGLUE benchmark and finetune various language models, including\nencoder-only, encoder-decoder, and decoder-only models on both synthetic and\noriginal training sets. Evaluation on the original test set reveals that models\ntrained on datasets generated by TarGEN perform approximately 1-2% points\nbetter than those trained on original datasets (82.84% via syn. vs. 81.12% on\nog. using Flan-T5). When incorporating instruction tuning, the performance\nincreases to 84.54% on synthetic data vs. 81.49% on original data by Flan-T5. A\ncomprehensive analysis of the synthetic dataset compared to the original\ndataset reveals that the synthetic dataset demonstrates similar or higher\nlevels of dataset complexity and diversity. Furthermore, the synthetic dataset\ndisplays a bias level that aligns closely with the original dataset. Finally,\nwhen pre-finetuned on our synthetic SuperGLUE dataset, T5-3B yields impressive\nresults on the OpenLLM leaderboard, surpassing the model trained on the\nSelf-Instruct dataset by 4.14% points. We hope that TarGEN can be helpful for\nquality data generation and reducing the human efforts to create complex\nbenchmarks.\n"", '  With the expanding application of Large Language Models (LLMs) in various\ndomains, it becomes imperative to comprehensively investigate their unforeseen\nbehaviors and consequent outcomes. In this study, we introduce and\nsystematically explore the phenomenon of ""glitch tokens"", which are anomalous\ntokens produced by established tokenizers and could potentially compromise the\nmodels\' quality of response. Specifically, we experiment on seven top popular\nLLMs utilizing three distinct tokenizers and involving a totally of 182,517\ntokens. We present categorizations of the identified glitch tokens and symptoms\nexhibited by LLMs when interacting with glitch tokens. Based on our observation\nthat glitch tokens tend to cluster in the embedding space, we propose\nGlitchHunter, a novel iterative clustering-based technique, for efficient\nglitch token detection. The evaluation shows that our approach notably\noutperforms three baseline methods on eight open-source LLMs. To the best of\nour knowledge, we present the first comprehensive study on glitch tokens. Our\nnew detection further provides valuable insights into mitigating\ntokenization-related errors in LLMs.\n', '  Large language models (LLMs) have achieved unprecedented success in the field\nof natural language processing. However, the black-box nature of their internal\nmechanisms has brought many concerns about their trustworthiness and\ninterpretability. Recent research has discovered a class of abnormal tokens in\nthe model\'s vocabulary space and named them ""glitch tokens"". Those tokens, once\nincluded in the input, may induce the model to produce incorrect, irrelevant,\nor even harmful results, drastically undermining the reliability and\npracticality of LLMs.\n  In this work, we aim to enhance the understanding of glitch tokens and\npropose techniques for their detection and mitigation. We first reveal the\ncharacteristic features induced by glitch tokens on LLMs, which are evidenced\nby significant deviations in the distributions of attention patterns and\ndynamic information from intermediate model layers. Based on the insights, we\ndevelop GlitchProber, a tool for efficient glitch token detection and\nmitigation. GlitchProber utilizes small-scale sampling, principal component\nanalysis for accelerated feature extraction, and a simple classifier for\nefficient vocabulary screening. Taking one step further, GlitchProber rectifies\nabnormal model intermediate layer values to mitigate the destructive effects of\nglitch tokens. Evaluated on five mainstream open-source LLMs, GlitchProber\ndemonstrates higher efficiency, precision, and recall compared to existing\napproaches, with an average F1 score of 0.86 and an average repair rate of\n50.06%. GlitchProber unveils a novel path to address the challenges posed by\nglitch tokens and inspires future research toward more robust and interpretable\nLLMs.\n']",Large Language Models and Dataset Generation,Advances in Large Language Models,Large Language Models
147,147,63,147_nlg_text_generate_texts,"['nlg', 'text', 'generate', 'texts', 'sentences', 'generation', 'generated', 'constraints', 'language', 'lexical']","['generation', 'text', 'decoding', 'attribute', 'diversity', 'control', 'sentences', 'language', 'constraint', 'generations']","[""  Recent approaches to controlled text generation (CTG) often involve\nmanipulating the weights or logits of base language models (LMs) at decoding\ntime. However, these methods are inapplicable to latest black-box LMs and\nineffective at preserving the core semantics of the base LM's original\ngenerations. In this work, we propose Locate&Edit(L&E), an efficient and\nflexible energy-based approach to CTG, which edits text outputs from a base LM\nusing off-the-shelf energy models. Given text outputs from the base LM, L&E\nfirst locates spans that are most relevant to constraints (e.g., toxicity)\nutilizing energy models, and then edits these spans by replacing them with more\nsuitable alternatives. Importantly, our method is compatible with black-box\nLMs, as it requires only the text outputs. Also, since L&E doesn't mandate\nspecific architecture for its component models, it can work with a diverse\ncombination of available off-the-shelf models. Moreover, L&E preserves the base\nLM's original generations, by selectively modifying constraint-related aspects\nof the texts and leaving others unchanged. These targeted edits also ensure\nthat L&E operates efficiently. Our experiments confirm that L&E achieves\nsuperior semantic preservation of the base LM generations and speed, while\nsimultaneously obtaining competitive or improved constraint satisfaction.\nFurthermore, we analyze how the granularity of energy distribution impacts CTG\nperformance and find that fine-grained, regression-based energy models improve\nconstraint satisfaction, compared to conventional binary classifier energy\nmodels.\n"", '  Controlled Text Generation (CTG) aims to produce texts that exhibit specific\ndesired attributes. In this study, we introduce a pluggable CTG framework for\nLarge Language Models (LLMs) named Dynamic Attribute Graphs-based controlled\ntext generation (DATG). This framework utilizes an attribute scorer to evaluate\nthe attributes of sentences generated by LLMs and constructs dynamic attribute\ngraphs. DATG modulates the occurrence of key attribute words and key\nanti-attribute words, achieving effective attribute control without\ncompromising the original capabilities of the model. We conduct experiments\nacross four datasets in two tasks: toxicity mitigation and sentiment\ntransformation, employing five LLMs as foundational models. Our findings\nhighlight a remarkable enhancement in control accuracy, achieving a peak\nimprovement of 19.29% over baseline methods in the most favorable task across\nfour datasets. Additionally, we observe a significant decrease in perplexity,\nmarkedly improving text fluency.\n', ""  Controllable text generation is a growing field within natural language\ngeneration (NLG) that focuses on producing text that meets specific constraints\nin real-world applications. Previous approaches, such as plug-and-play\ncontrollers (PPCs), aimed to steer the properties of generated text in a\nflexible manner. However, these methods often compromised the integrity of the\nlanguage model's decoding process, resulting in less smooth text generation.\nAlternatively, other techniques utilized multiple attribute prompts to align\nthe generated text with desired attributes, but this approach required prompt\ndesign for each attribute and was dependent on the size of the language model.\nThis paper introduces a novel method for flexible attribute control in text\ngeneration using pre-trained language models (PLMs). The proposed approach aims\nto enhance the fluency of generated text by guiding the generation process with\nPPCs. The key idea is to dynamically adjust the distribution of generated text\nby modifying prompts, effectively constraining the output space of the language\nmodel and influencing the desired attribute. To enable smooth cooperation\nbetween the PLM and the PPC, our work innovatively proposes a new model\nfine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback\n(RLDAF).This fine-tuning process adapts a small subset of the language model's\nparameters based on the generating actions taken during the PPC control\nprocess. The resulting harmonious collaboration between the PLM and PPC leads\nto improved smoothness in text generation during inference. Extensive\nexperiments were conducted on the SST2 dataset, and the proposed method\noutperformed previous approaches in various evaluation metrics, including text\nfluency and attribute consistency.\n""]",Controlled Text Generation,Text-Guided Visual Content Generation and Editing,Artificial Intelligence for Creative Content Generation
148,148,63,148_cnn_cnns_drilling_boulders,"['cnn', 'cnns', 'drilling', 'boulders', 'geological', 'neural', 'deep', 'networks', 'learning', 'drill']","['geological', 'melt', 'frequency', 'boulders', 'deep', 'neural', 'energy', 'logging', 'drilling', 'geoscience']","['  Deep subsurface exploration is important for mining, oil and gas industries,\nas well as in the assessment of geological units for the disposal of chemical\nor nuclear waste, or the viability of geothermal energy systems. Typically,\ndetailed examinations of subsurface formations or units are performed on\ncuttings or core materials extracted during drilling campaigns, as well as on\ngeophysical borehole data, which provide detailed information about the\npetrophysical properties of the rocks. Depending on the volume of rock samples\nand the analytical program, the laboratory analysis and diagnostics can be very\ntime-consuming. This study investigates the potential of utilizing machine\nlearning, specifically convolutional neural networks (CNN), to assess the\nlithology and mineral content solely from analysis of drill core images, aiming\nto support and expedite the subsurface geological exploration. The paper\noutlines a comprehensive methodology, encompassing data preprocessing, machine\nlearning methods, and transfer learning techniques. The outcome reveals a\nremarkable 96.7% accuracy in the classification of drill core segments into\ndistinct formation classes. Furthermore, a CNN model was trained for the\nevaluation of mineral content using a learning data set from multidimensional\nlog analysis data (silicate, total clay, carbonate). When benchmarked against\nlaboratory XRD measurements on samples from the cores, both the advanced\nmultidimensional log analysis model and the neural network approach developed\nhere provide equally good performance. This work demonstrates that deep\nlearning and particularly transfer learning can support extracting\npetrophysical properties, including mineral content and formation\nclassification, from drill core images, thus offering a road map for enhancing\nmodel performance and data set quality in image-based analysis of drill cores.\n', ""  Current rock engineering design in drill and blast tunnelling primarily\nrelies on engineers' observational assessments. Measure While Drilling (MWD)\ndata, a high-resolution sensor dataset collected during tunnel excavation, is\nunderutilised, mainly serving for geological visualisation. This study aims to\nautomate the translation of MWD data into actionable metrics for rock\nengineering. It seeks to link data to specific engineering actions, thus\nproviding critical decision support for geological challenges ahead of the\ntunnel face. Leveraging a large and geologically diverse dataset of 500,000\ndrillholes from 15 tunnels, the research introduces models for accurate rock\nmass quality classification in a real-world tunnelling context. Both\nconventional machine learning and image-based deep learning are explored to\nclassify MWD data into Q-classes and Q-values, examples of metrics describing\nthe stability of the rock mass, using both tabular and image data. The results\nindicate that the K-nearest neighbours algorithm in an ensemble with tree-based\nmodels using tabular data, effectively classifies rock mass quality. It\nachieves a cross-validated balanced accuracy of 0.86 in classifying rock mass\ninto the Q-classes A, B, C, D, E1, E2, and 0.95 for a binary classification\nwith E versus the rest. Classification using a CNN with MWD-images for each\nblasting round resulted in a balanced accuracy of 0.82 for binary\nclassification. Regressing the Q-value from tabular MWD-data achieved\ncross-validated R2 and MSE scores of 0.80 and 0.18 for a similar ensemble model\nas in classification. High performance in regression and classification boosts\nconfidence in automated rock mass assessment. Applying advanced modelling on a\nunique dataset demonstrates MWD data's value in improving rock mass\nclassification accuracy and advancing data-driven rock engineering design,\nreducing manual intervention.\n"", '  Rock mass classification systems are crucial for assessing stability and risk\nin underground construction globally and guiding support and excavation design.\nHowever, systems developed primarily in the 1970s lack access to modern\nhigh-resolution data and advanced statistical techniques, limiting their\neffectiveness as decision-support systems. Initially, we outline the\nlimitations observed in this context and later describe how a data-driven\nsystem, based on drilling data as detailed in this study, can overcome these\nlimitations. Using extracted statistical information from thousands of MWD-data\nvalues in one-meter sections of a full tunnel profile, thus working as a\nsignature of the rock mass, we have demonstrated that it is possible to form\nwell-defined clusters that can act as a foundational basis for various rock\nmass classification systems. We reduced the dimensionality of 48-value vectors\nusing nonlinear manifold learning techniques (UMAP) and linear principal\ncomponent analysis (PCA) to enhance clustering. Unsupervised machine learning\nmethods (HDBSCAN, Agglomerative Clustering, K-means) were employed to cluster\nthe data, with hyperparameters optimised through multi-objective Bayesian\noptimisation for effective clustering. Using domain knowledge, we experienced\nimproved clustering and system tuning opportunities in adding extra features to\ncore clusters of MWD-data. We structured and correlated these clusters with\nphysical rock mass properties, including labels of rock type and rock quality,\nand analysed cumulative distributions of key MWD-parameters for rock mass\nassessment to determine if clusters meaningfully differentiate rock masses. The\nability of MWD data to form distinct rock mass clusters suggests substantial\npotential for future classification systems grounded in this objective,\ndata-driven methodology, free from human bias.\n']",Geological Exploration using Deep Learning and Drilling Data,Deep Learning for Geophysical and Structural Analysis,Deep Learning Applications in Engineering and Computer Vision
149,149,63,149_ethical_ethics_ai_moral,"['ethical', 'ethics', 'ai', 'moral', 'agent', 'intelligence', 'algorithmic', 'agents', 'humanity', 'cooperation']","['ethical', 'moral', 'ethics', 'metaverse', 'norms', 'artificial', 'social', 'intelligence', 'society', 'agents']","['  Increasing interest in ensuring safety of next-generation Artificial\nIntelligence (AI) systems calls for novel approaches to embedding morality into\nautonomous agents. Traditionally, this has been done by imposing explicit\ntop-down rules or hard constraints on systems, for example by filtering system\noutputs through pre-defined ethical rules. Recently, instead, entirely\nbottom-up methods for learning implicit preferences from human behavior have\nbecome increasingly popular, such as those for training and fine-tuning Large\nLanguage Models. In this paper, we provide a systematization of existing\napproaches to the problem of introducing morality in machines - modeled as a\ncontinuum, and argue that the majority of popular techniques lie at the\nextremes - either being fully hard-coded, or entirely learned, where no\nexplicit statement of any moral principle is required. Given the relative\nstrengths and weaknesses of each type of methodology, we argue that more hybrid\nsolutions are needed to create adaptable and robust, yet more controllable and\ninterpretable agents.\n  In particular, we present three case studies of recent works which use\nlearning from experience (i.e., Reinforcement Learning) to explicitly provide\nmoral principles to learning agents - either as intrinsic rewards, moral\nlogical constraints or textual principles for language models. For example,\nusing intrinsic rewards in Social Dilemma games, we demonstrate how it is\npossible to represent classical moral frameworks for agents. We also present an\noverview of the existing work in this area in order to provide empirical\nevidence for the potential of this hybrid approach. We then discuss strategies\nfor evaluating the effectiveness of moral learning agents. Finally, we present\nopen research questions and implications for the future of AI safety and ethics\nwhich are emerging from this framework.\n', '  With the rise of individual and collaborative networks of autonomous agents,\nAI is deployed in more key reasoning and decision-making roles. For this\nreason, ethics-based audits play a pivotal role in the rapidly growing fields\nof AI safety and regulation. This paper undertakes an ethics-based audit to\nprobe the 8 leading commercial and open-source Large Language Models including\nGPT-4. We assess explicability and trustworthiness by a) establishing how well\ndifferent models engage in moral reasoning and b) comparing normative values\nunderlying models as ethical frameworks. We employ an experimental,\nevidence-based approach that challenges the models with ethical dilemmas in\norder to probe human-AI alignment. The ethical scenarios are designed to\nrequire a decision in which the particulars of the situation may or may not\nnecessitate deviating from normative ethical principles. A sophisticated\nethical framework was consistently elicited in one model, GPT-4. Nonetheless,\ntroubling findings include underlying normative frameworks with clear bias\ntowards particular cultural norms. Many models also exhibit disturbing\nauthoritarian tendencies. Code is available at\nhttps://github.com/jonchun/llm-sota-chatbots-ethics-based-audit.\n', '  This study addresses ethical issues surrounding Large Language Models (LLMs)\nwithin the field of artificial intelligence. It explores the common ethical\nchallenges posed by both LLMs and other AI systems, such as privacy and\nfairness, as well as ethical challenges uniquely arising from LLMs. It\nhighlights challenges such as hallucination, verifiable accountability, and\ndecoding censorship complexity, which are unique to LLMs and distinct from\nthose encountered in traditional AI systems. The study underscores the need to\ntackle these complexities to ensure accountability, reduce biases, and enhance\ntransparency in the influential role that LLMs play in shaping information\ndissemination. It proposes mitigation strategies and future directions for LLM\nethics, advocating for interdisciplinary collaboration. It recommends ethical\nframeworks tailored to specific domains and dynamic auditing systems adapted to\ndiverse contexts. This roadmap aims to guide responsible development and\nintegration of LLMs, envisioning a future where ethical considerations govern\nAI advancements in society.\n']",Ethics in Artificial Intelligence and Autonomous Agents,Artificial Intelligence Ethics and Governance,Artificial Intelligence Applications and Implications
150,150,63,150_linguistic_cognition_linguistics_language,"['linguistic', 'cognition', 'linguistics', 'language', 'cognitive', 'semantic', 'learners', 'knowledge', 'brain', 'minds']","['linguistic', 'cognitive', 'language', 'words', 'philosophical', 'meaning', 'competence', 'human', 'word', 'humans']","['  In a recent paper, Mandelkern & Linzen (2024) - henceforth M&L - address the\nquestion of whether language models\' (LMs) words refer. Their argument draws\nfrom the externalist tradition in philosophical semantics, which views\nreference as the capacity of words to ""achieve \'word-to-world\' connections"". In\nthe externalist framework, causally uninterrupted chains of usage, tracing\nevery occurrence of a name back to its bearer, guarantee that, for example,\n\'Peano\' refers to the individual Peano (Kripke 1980). This account is\nexternalist both because words pick out referents \'out there\' in the world, and\nbecause what determines reference are coordinated linguistic actions by members\nof a community, and not individual mental states. The ""central question to\nask"", for M&L, is whether LMs too belong to human linguistic communities, such\nthat words by LMs may also trace back causally to their bearers. Their answer\nis a cautious ""yes"": inputs to LMs are linguistic ""forms with particular\nhistories of referential use""; ""those histories ground the referents of those\nforms""; any occurrence of \'Peano\' in LM outputs is as causally connected to the\nindividual Peano as any other occurrence of the same proper name in human\nspeech or text; therefore, occurrences of \'Peano\' in LM outputs refer to Peano.\nIn this commentary, we first qualify M&L\'s claim as applying to a narrow class\nof natural language expressions. Thus qualified, their claim is valid, and we\nemphasise an additional motivation for that in Section 2. Next, we discuss the\nactual scope of their claim, and we suggest that the way they formulate it may\nlead to unwarranted generalisations about reference in LMs. Our critique is\nlikewise applicable to other externalist accounts of LMs (e.g., Lederman &\nMahowald 2024; Mollo & Milliere 2023). Lastly, we conclude with a comment on\nthe status of LMs as members of human linguistic communities.\n', ""  Although LLMs and other artificial intelligence systems demonstrate cognitive\nskills similar to humans, like concept learning and language acquisition, the\nway they process information fundamentally differs from biological cognition.\nTo better understand these differences this paper introduces Psychomatics, a\nmultidisciplinary framework bridging cognitive science, linguistics, and\ncomputer science. It aims to better understand the high-level functioning of\nLLMs, focusing specifically on how LLMs acquire, learn, remember, and use\ninformation to produce their outputs. To achieve this goal, Psychomatics will\nrely on a comparative methodology, starting from a theory-driven research\nquestion - is the process of language development and use different in humans\nand LLMs? - drawing parallels between LLMs and biological systems. Our analysis\nshows how LLMs can map and manipulate complex linguistic patterns in their\ntraining data. Moreover, LLMs can follow Grice's Cooperative Principle to\nprovide relevant and informative responses. However, human cognition draws from\nmultiple sources of meaning, including experiential, emotional, and imaginative\nfacets, which transcend mere language processing and are rooted in our social\nand developmental trajectories. Moreover, current LLMs lack physical\nembodiment, reducing their ability to make sense of the intricate interplay\nbetween perception, action, and cognition that shapes human understanding and\nexpression. Ultimately, Psychomatics holds the potential to yield\ntransformative insights into the nature of language, cognition, and\nintelligence, both artificial and biological. Moreover, by drawing parallels\nbetween LLMs and human cognitive processes, Psychomatics can inform the\ndevelopment of more robust and human-like AI systems.\n"", '  Large language models like GPT-4 have achieved remarkable proficiency in a\nbroad spectrum of language-based tasks, some of which are traditionally\nassociated with hallmarks of human intelligence. This has prompted ongoing\ndisagreements about the extent to which we can meaningfully ascribe any kind of\nlinguistic or cognitive competence to language models. Such questions have deep\nphilosophical roots, echoing longstanding debates about the status of\nartificial neural networks as cognitive models. This article -- the first part\nof two companion papers -- serves both as a primer on language models for\nphilosophers, and as an opinionated survey of their significance in relation to\nclassic debates in the philosophy cognitive science, artificial intelligence,\nand linguistics. We cover topics such as compositionality, language\nacquisition, semantic competence, grounding, world models, and the transmission\nof cultural knowledge. We argue that the success of language models challenges\nseveral long-held assumptions about artificial neural networks. However, we\nalso highlight the need for further empirical investigation to better\nunderstand their internal mechanisms. This sets the stage for the companion\npaper (Part II), which turns to novel empirical methods for probing the inner\nworkings of language models, and new philosophical questions prompted by their\nlatest developments.\n']",Language Models and Cognitive Linguistics,Large Language Models and Cognitive Abilities,Large Language Models
151,151,62,151_unlearning_memorization_forgetting_memorize,"['unlearning', 'memorization', 'forgetting', 'memorize', 'unlearned', 'dataset', 'overfitting', 'trained', 'forget', 'training']","['unlearning', 'membership', 'privacy', 'attack', 'attacks', 'memorization', 'access', 'forgetting', 'shadow', 'unlearned']","[""  With the increasing emphasis on data privacy, the significance of machine\nunlearning has grown substantially. Class unlearning, which involves enabling a\ntrained model to forget data belonging to a specific class learned before, is\nimportant as classification tasks account for the majority of today's machine\nlearning as a service (MLaaS). Retraining the model on the original data,\nexcluding the data to be forgotten (a.k.a forgetting data), is a common\napproach to class unlearning. However, the availability of original data during\nthe unlearning phase is not always guaranteed, leading to the exploration of\nclass unlearning with restricted data access. While current unlearning methods\nwith restricted data access usually generate proxy sample via the trained\nneural network classifier, they typically focus on training and forgetting\nbalanced data. However, the imbalanced original data can cause trouble for\nthese proxies and unlearning, particularly when the forgetting data consists\npredominantly of the majority class. To address this issue, we propose the\nGENerative Imbalanced Unlearning (GENIU) framework. GENIU utilizes a\nVariational Autoencoder (VAE) to concurrently train a proxy generator alongside\nthe original model. These generated proxies accurately represent each class and\nare leveraged in the unlearning phase, eliminating the reliance on the original\ntraining data. To further mitigate the performance degradation resulting from\nforgetting the majority class, we introduce an in-batch tuning strategy that\nworks with the generated proxies. GENIU is the first practical framework for\nclass unlearning in imbalanced data settings and restricted data access,\nensuring the preservation of essential information for future unlearning.\nExperimental results confirm the superiority of GENIU over existing methods,\nestablishing its effectiveness in empirical scenarios.\n"", '  With the implementation of personal data privacy regulations, the field of\nmachine learning (ML) faces the challenge of the ""right to be forgotten"".\nMachine unlearning has emerged to address this issue, aiming to delete data and\nreduce its impact on models according to user requests. Despite the widespread\ninterest in machine unlearning, comprehensive surveys on its latest\nadvancements, especially in the field of Large Language Models (LLMs) is\nlacking. This survey aims to fill this gap by providing an in-depth exploration\nof machine unlearning, including the definition, classification and evaluation\ncriteria, as well as challenges in different environments and their solutions.\nSpecifically, this paper categorizes and investigates unlearning on both\ntraditional models and LLMs, and proposes methods for evaluating the\neffectiveness and efficiency of unlearning, and standards for performance\nmeasurement. This paper reveals the limitations of current unlearning\ntechniques and emphasizes the importance of a comprehensive unlearning\nevaluation to avoid arbitrary forgetting. This survey not only summarizes the\nkey concepts of unlearning technology but also points out its prominent issues\nand feasible directions for future research, providing valuable guidance for\nscholars in the field.\n', ""  By adopting a more flexible definition of unlearning and adjusting the model\ndistribution to simulate training without the targeted data, approximate\nmachine unlearning provides a less resource-demanding alternative to the more\nlaborious exact unlearning methods. Yet, the unlearning completeness of target\nsamples-even when the approximate algorithms are executed faithfully without\nexternal threats-remains largely unexamined, raising questions about those\napproximate algorithms' ability to fulfill their commitment of unlearning\nduring the lifecycle.\n  In this paper, we introduce the task of Lifecycle Unlearning Commitment\nManagement (LUCM) for approximate unlearning and outline its primary\nchallenges. We propose an efficient metric designed to assess the sample-level\nunlearning completeness. Our empirical results demonstrate its superiority over\nmembership inference techniques in two key areas: the strong correlation of its\nmeasurements with unlearning completeness across various unlearning tasks, and\nits computational efficiency, making it suitable for real-time applications.\nAdditionally, we show that this metric is able to serve as a tool for\nmonitoring unlearning anomalies throughout the unlearning lifecycle, including\nboth under-unlearning and over-unlearning.\n  We apply this metric to evaluate the unlearning commitments of current\napproximate algorithms. Our analysis, conducted across multiple unlearning\nbenchmarks, reveals that these algorithms inconsistently fulfill their\nunlearning commitments due to two main issues: 1) unlearning new data can\nsignificantly affect the unlearning utility of previously requested data, and\n2) approximate algorithms fail to ensure equitable unlearning utility across\ndifferent groups. These insights emphasize the crucial importance of LUCM\nthroughout the unlearning lifecycle. We will soon open-source our newly\ndeveloped benchmark.\n""]",Machine Unlearning and Forgetting in AI,Machine Unlearning and Forgetting in Artificial Intelligence,Machine Learning Adaptation and Forgetting
152,152,62,152_bayesian_probabilistic_neural_ensembles,"['bayesian', 'probabilistic', 'neural', 'ensembles', 'optimization', 'posterior', 'deep', 'estimation', 'sbnn', 'networks']","['uncertainty', 'misspecification', 'neural', 'statistical', 'posterior', 'estimation', 'approximations', 'regression', 'deep', 'networks']","['  Bayesian Neural Networks (BNNs) have become one of the promising approaches\nfor uncertainty estimation due to the solid theorical foundations. However, the\nperformance of BNNs is affected by the ability of catching uncertainty. Instead\nof only seeking the distribution of neural network weights by in-distribution\n(ID) data, in this paper, we propose a new Bayesian Neural Network with an\nAttached structure (ABNN) to catch more uncertainty from out-of-distribution\n(OOD) data. We first construct a mathematical description for the uncertainty\nof OOD data according to the prior distribution, and then develop an attached\nBayesian structure to integrate the uncertainty of OOD data into the backbone\nnetwork. ABNN is composed of an expectation module and several distribution\nmodules. The expectation module is a backbone deep network which focuses on the\noriginal task, and the distribution modules are mini Bayesian structures which\nserve as attachments of the backbone. In particular, the distribution modules\naim at extracting the uncertainty from both ID and OOD data. We further provide\ntheoretical analysis for the convergence of ABNN, and experimentally validate\nits superiority by comparing with some state-of-the-art uncertainty estimation\nmethods Code will be made available.\n', '  Bayesian optimization is a highly efficient approach to optimizing objective\nfunctions which are expensive to query. These objectives are typically\nrepresented by Gaussian process (GP) surrogate models which are easy to\noptimize and support exact inference. While standard GP surrogates have been\nwell-established in Bayesian optimization, Bayesian neural networks (BNNs) have\nrecently become practical function approximators, with many benefits over\nstandard GPs such as the ability to naturally handle non-stationarity and learn\nrepresentations for high-dimensional data. In this paper, we study BNNs as\nalternatives to standard GP surrogates for optimization. We consider a variety\nof approximate inference procedures for finite-width BNNs, including\nhigh-quality Hamiltonian Monte Carlo, low-cost stochastic MCMC, and heuristics\nsuch as deep ensembles. We also consider infinite-width BNNs, linearized\nLaplace approximations, and partially stochastic models such as deep kernel\nlearning. We evaluate this collection of surrogate models on diverse problems\nwith varying dimensionality, number of objectives, non-stationarity, and\ndiscrete and continuous inputs. We find: (i) the ranking of methods is highly\nproblem dependent, suggesting the need for tailored inductive biases; (ii) HMC\nis the most successful approximate inference procedure for fully stochastic\nBNNs; (iii) full stochasticity may be unnecessary as deep kernel learning is\nrelatively competitive; (iv) deep ensembles perform relatively poorly; (v)\ninfinite-width BNNs are particularly promising, especially in high dimensions.\n', '  In the drug discovery process, where experiments can be costly and\ntime-consuming, computational models that predict drug-target interactions are\nvaluable tools to accelerate the development of new therapeutic agents.\nEstimating the uncertainty inherent in these neural network predictions\nprovides valuable information that facilitates optimal decision-making when\nrisk assessment is crucial. However, such models can be poorly calibrated,\nwhich results in unreliable uncertainty estimates that do not reflect the true\npredictive uncertainty. In this study, we compare different metrics, including\naccuracy and calibration scores, used for model hyperparameter tuning to\ninvestigate which model selection strategy achieves well-calibrated models.\nFurthermore, we propose to use a computationally efficient Bayesian uncertainty\nestimation method named Bayesian Linear Probing (BLP), which generates\nHamiltonian Monte Carlo (HMC) trajectories to obtain samples for the parameters\nof a Bayesian Logistic Regression fitted to the hidden layer of the baseline\nneural network. We report that BLP improves model calibration and achieves the\nperformance of common uncertainty quantification methods by combining the\nbenefits of uncertainty estimation and probability calibration methods.\nFinally, we show that combining post hoc calibration method with\nwell-performing uncertainty quantification approaches can boost model accuracy\nand calibration.\n']",Bayesian Neural Networks for Uncertainty Estimation,Bayesian Neural Networks,Probabilistic Machine Learning
153,153,62,153_videos_multimodal_visual_attention,"['videos', 'multimodal', 'visual', 'attention', 'video', 'clips', 'vision', 'frames', 'trained', 'recognition']","['video', 'videos', 'temporal', 'action', 'visual', 'frame', 'frames', 'objects', 'actions', 'egocentric']","[""  Amidst the advancements in image-based Large Vision-Language Models\n(image-LVLM), the transition to video-based models (video-LVLM) is hindered by\nthe limited availability of quality video data. This paper addresses the\nchallenge by leveraging the visual commonalities between images and videos to\nefficiently evolve image-LVLMs into video-LVLMs. We present a cost-effective\nvideo-LVLM that enhances model architecture, introduces innovative training\nstrategies, and identifies the most effective types of video instruction data.\nOur innovative weighted token sampler significantly compresses the visual token\nnumbers of each video frame, effectively cutting computational expenses. We\nalso find that judiciously using just 10% of the video data, compared to prior\nvideo-LVLMs, yields impressive results during various training phases.\nMoreover, we delve into the influence of video instruction data in\nlimited-resource settings, highlighting the significance of incorporating video\ntraining data that emphasizes temporal understanding to enhance model\nperformance. The resulting Fewer Tokens and Fewer Videos LVLM (FTFV-LVLM)\nexhibits exceptional performance across video and image benchmarks, validating\nour model's design and training approaches.\n"", '  The performance of Large Vision Language Models (LVLMs) is dependent on the\nsize and quality of their training datasets. Existing video instruction tuning\ndatasets lack diversity as they are derived by prompting large language models\nwith video captions to generate question-answer pairs, and are therefore mostly\ndescriptive. Meanwhile, many labeled video datasets with diverse labels and\nsupervision exist - however, we find that their integration into LVLMs is\nnon-trivial. Herein, we present Video Self-Training with augmented Reasoning\n(Video-STaR), the first video self-training approach. Video-STaR allows the\nutilization of any labeled video dataset for video instruction tuning. In\nVideo-STaR, an LVLM cycles between instruction generation and finetuning, which\nwe show (I) improves general video understanding and (II) adapts LVLMs to novel\ndownstream tasks with existing supervision. During generation, an LVLM is\nprompted to propose an answer. The answers are then filtered only to those that\ncontain the original video labels, and the LVLM is then re-trained on the\ngenerated dataset. By only training on generated answers that contain the\ncorrect video labels, Video-STaR utilizes these existing video labels as weak\nsupervision for video instruction tuning. Our results demonstrate that\nVideo-STaR-enhanced LVLMs exhibit improved performance in (I) general video QA,\nwhere TempCompass performance improved by 10%, and (II) on downstream tasks,\nwhere Video-STaR improved Kinetics700-QA accuracy by 20% and action quality\nassessment on FineDiving by 15%.\n', '  In light of recent advances in multimodal Large Language Models (LLMs), there\nis increasing attention to scaling them from image-text data to more\ninformative real-world videos. Compared to static images, video poses unique\nchallenges for effective large-scale pre-training due to the modeling of its\nspatiotemporal dynamics. In this paper, we address such limitations in\nvideo-language pre-training with an efficient video decomposition that\nrepresents each video as keyframes and temporal motions. These are then adapted\nto an LLM using well-designed tokenizers that discretize visual and temporal\ninformation as a few tokens, thus enabling unified generative pre-training of\nvideos, images, and text. At inference, the generated tokens from the LLM are\ncarefully recovered to the original continuous pixel space to create various\nvideo content. Our proposed framework is both capable of comprehending and\ngenerating image and video content, as demonstrated by its competitive\nperformance across 13 multimodal benchmarks in image and video understanding\nand generation. Our code and models are available at\nhttps://video-lavit.github.io.\n']",Multimodal Video Understanding and Generation,Multimodal Video and Image Understanding,Multimodal Learning and Applications
154,154,61,154_classifiers_classifier_classification_ensemble,"['classifiers', 'classifier', 'classification', 'ensemble', 'ensembles', 'boosting', 'datasets', 'outliers', 'outlier', 'unifier']","['ensemble', 'view', 'classifier', 'outlier', 'uncertainty', 'selection', 'ensembles', 'instances', 'detection', 'views']","['  Multi-view datasets offer diverse forms of data that can enhance prediction\nmodels by providing complementary information. However, the use of multi-view\ndata leads to an increase in high-dimensional data, which poses significant\nchallenges for the prediction models that can lead to poor generalization.\nTherefore, relevant feature selection from multi-view datasets is important as\nit not only addresses the poor generalization but also enhances the\ninterpretability of the models. Despite the success of traditional feature\nselection methods, they have limitations in leveraging intrinsic information\nacross modalities, lacking generalizability, and being tailored to specific\nclassification tasks. We propose a novel genetic algorithm strategy to overcome\nthese limitations of traditional feature selection methods for multi-view data.\nOur proposed approach, called the multi-view multi-objective feature selection\ngenetic algorithm (MMFS-GA), simultaneously selects the optimal subset of\nfeatures within a view and between views under a unified framework. The MMFS-GA\nframework demonstrates superior performance and interpretability for feature\nselection on multi-view datasets in both binary and multiclass classification\ntasks. The results of our evaluations on three benchmark datasets, including\nsynthetic and real data, show improvement over the best baseline methods. This\nwork provides a promising solution for multi-view feature selection and opens\nup new possibilities for further research in multi-view datasets.\n', ""  Multi-class ensemble classification remains a popular focus of investigation\nwithin the research community. The popularization of cloud services has sped up\ntheir adoption due to the ease of deploying large-scale machine-learning\nmodels. It has also drawn the attention of the industrial sector because of its\nability to identify common problems in production. However, there are\nchallenges to conform an ensemble classifier, namely a proper selection and\neffective training of the pool of classifiers, the definition of a proper\narchitecture for multi-class classification, and uncertainty quantification of\nthe ensemble classifier. The robustness and effectiveness of the ensemble\nclassifier lie in the selection of the pool of classifiers, as well as in the\nlearning process. Hence, the selection and the training procedure of the pool\nof classifiers play a crucial role. An (ensemble) classifier learns to detect\nthe classes that were used during the supervised training. However, when\ninjecting data with unknown conditions, the trained classifier will intend to\npredict the classes learned during the training. To this end, the uncertainty\nof the individual and ensemble classifier could be used to assess the learning\ncapability. We present a novel approach for novel detection using ensemble\nclassification and evidence theory. A pool selection strategy is presented to\nbuild a solid ensemble classifier. We present an architecture for multi-class\nensemble classification and an approach to quantify the uncertainty of the\nindividual classifiers and the ensemble classifier. We use uncertainty for the\nanomaly detection approach. Finally, we use the benchmark Tennessee Eastman to\nperform experiments to test the ensemble classifier's prediction and anomaly\ndetection capabilities.\n"", '  Out-of-distribution (OOD) detection methods have been developed to identify\nobjects that a model has not seen during training. The Outlier Exposure (OE)\nmethods use auxiliary datasets to train OOD detectors directly. However, the\ncollection and learning of representative OOD samples may pose challenges. To\ntackle these issues, we propose the Outlier Aware Metric Learning (OAML)\nframework. The main idea of our method is to use the k-NN algorithm and Stable\nDiffusion model to generate outliers for training at the feature level without\nmaking any distributional assumptions. To increase feature discrepancies in the\nsemantic space, we develop a mutual information-based contrastive learning\napproach for learning from OOD data effectively. Both theoretical and empirical\nresults confirm the effectiveness of this contrastive learning technique.\nFurthermore, we incorporate knowledge distillation into our learning framework\nto prevent degradation of in-distribution classification accuracy. The\ncombination of contrastive learning and knowledge distillation algorithms\nsignificantly enhances the performance of OOD detection. Experimental results\nacross various datasets show that our method significantly outperforms previous\nOE methods.\n']",Multi-View Classification and Ensemble Methods,Multi-View Learning and Ensemble Methods,Machine Learning Ensembles and Multi-View Methods
155,155,61,155_chemical_chemistry_chemists_chemist,"['chemical', 'chemistry', 'chemists', 'chemist', 'molecule', 'molecular', 'catalysts', 'chemicals', 'synthesis', 'molecules']","['chemical', 'reaction', 'chemistry', 'catalyst', 'reactions', 'molecular', 'discovery', 'molecules', 'chemists', 'catalysts']","[""  The task of chemical reaction predictions (CRPs) plays a pivotal role in\nadvancing drug discovery and material science. However, its effectiveness is\nconstrained by the vast and uncertain chemical reaction space and challenges in\ncapturing reaction selectivity, particularly due to existing methods'\nlimitations in exploiting the data's inherent knowledge. To address these\nchallenges, we introduce a data-curated self-feedback knowledge elicitation\napproach. This method starts from iterative optimization of molecular\nrepresentations and facilitates the extraction of knowledge on chemical\nreaction types (RTs). Then, we employ adaptive prompt learning to infuse the\nprior knowledge into the large language model (LLM). As a result, we achieve\nsignificant enhancements: a 14.2% increase in retrosynthesis prediction\naccuracy, a 74.2% rise in reagent prediction accuracy, and an expansion in the\nmodel's capability for handling multi-task chemical reactions. This research\noffers a novel paradigm for knowledge elicitation in scientific research and\nshowcases the untapped potential of LLMs in CRPs.\n"", '  Chemical reactions are the fundamental building blocks of drug design and\norganic chemistry research. In recent years, there has been a growing need for\na large-scale deep-learning framework that can efficiently capture the basic\nrules of chemical reactions. In this paper, we have proposed a unified\nframework that addresses both the reaction representation learning and molecule\ngeneration tasks, which allows for a more holistic approach. Inspired by the\norganic chemistry mechanism, we develop a novel pretraining framework that\nenables us to incorporate inductive biases into the model. Our framework\nachieves state-of-the-art results on challenging downstream tasks. By\npossessing chemical knowledge, our generative framework overcome the\nlimitations of current molecule generation models that rely on a small number\nof reaction templates. In the extensive experiments, our model generates\nsynthesizable drug-like structures of high quality. Overall, our work presents\na significant step toward a large-scale deep-learning framework for a variety\nof reaction-based applications.\n', '  Large language models (LLMs) have gained widespread interest due to their\nability to process human language and perform tasks on which they have not been\nexplicitly trained. This is relevant for the chemical sciences, which face the\nproblem of small and diverse datasets that are frequently in the form of text.\nLLMs have shown promise in addressing these issues and are increasingly being\nharnessed to predict chemical properties, optimize reactions, and even design\nand conduct experiments autonomously. However, we still have only a very\nlimited systematic understanding of the chemical reasoning capabilities of\nLLMs, which would be required to improve models and mitigate potential harms.\nHere, we introduce ""ChemBench,"" an automated framework designed to rigorously\nevaluate the chemical knowledge and reasoning abilities of state-of-the-art\nLLMs against the expertise of human chemists. We curated more than 7,000\nquestion-answer pairs for a wide array of subfields of the chemical sciences,\nevaluated leading open and closed-source LLMs, and found that the best models\noutperformed the best human chemists in our study on average. The models,\nhowever, struggle with some chemical reasoning tasks that are easy for human\nexperts and provide overconfident, misleading predictions, such as about\nchemicals\' safety profiles. These findings underscore the dual reality that,\nalthough LLMs demonstrate remarkable proficiency in chemical tasks, further\nresearch is critical to enhancing their safety and utility in chemical\nsciences. Our findings also indicate a need for adaptations to chemistry\ncurricula and highlight the importance of continuing to develop evaluation\nframeworks to improve safe and useful LLMs.\n']",Chemical Reaction Prediction and Synthesis,Artificial Intelligence in Organic Chemistry Synthesis,Artificial Intelligence in Data Generation and Chemical Synthesis
156,156,61,156_turbine_prediction_lstm_forecasting,"['turbine', 'prediction', 'lstm', 'forecasting', 'turbines', 'neural', 'forecast', 'scada', 'forecasts', 'monitoring']","['wind', 'fault', 'power', 'volatility', 'financial', 'load', 'industrial', 'forecasting', 'production', 'faults']","['  In the process industry, optimizing production lines for long-term efficiency\nrequires real-time monitoring and analysis of operation states to fine-tune\nproduction line parameters. However, the complexity in operational logic and\nthe intricate coupling of production process parameters make it difficult to\ndevelop an accurate mathematical model for the entire process, thus hindering\nthe deployment of efficient optimization mechanisms. In view of these\ndifficulties, we propose to deploy a digital twin of the production line by\ndigitally abstracting its physical layout and operational logic. By iteratively\nmapping the real-world data reflecting equipment operation status and product\nquality inspection in the digital twin, we adopt a quality prediction model for\nproduction process based on self-attention-enabled temporal convolutional\nneural networks. This model enables the data-driven state evolution of the\ndigital twin. The digital twin takes a role of aggregating the information of\nactual operating conditions and the results of quality-sensitive analysis,\nwhich facilitates the optimization of process production quality with\nvirtual-reality evolution under multi-dimensional constraints. Leveraging the\ndigital twin model as an information-flow carrier, we extract temporal features\nfrom key process indicators and establish a production process quality\nprediction model based on the proposed composite neural network. Our operation\nexperiments on a specific tobacco shredding line demonstrate that the proposed\ndigital twin-based production process optimization method fosters seamless\nintegration between virtual and real production lines. This integration\nachieves an average operating status prediction accuracy of over 98\\% and\nnear-optimal production process control.\n', '  In this study, we leverage SCADA data from diverse wind turbines to predict\npower output, employing advanced time series methods, specifically Functional\nNeural Networks (FNN) and Long Short-Term Memory (LSTM) networks. A key\ninnovation lies in the ensemble of FNN and LSTM models, capitalizing on their\ncollective learning. This ensemble approach outperforms individual models,\nensuring stable and accurate power output predictions. Additionally, machine\nlearning techniques are applied to detect wind turbine performance\ndeterioration, enabling proactive maintenance strategies and health assessment.\nCrucially, our analysis reveals the uniqueness of each wind turbine,\nnecessitating tailored models for optimal predictions. These insight\nunderscores the importance of providing automatized customization for different\nturbines to keep human modeling effort low. Importantly, the methodologies\ndeveloped in this analysis are not limited to wind turbines; they can be\nextended to predict and optimize performance in various machinery, highlighting\nthe versatility and applicability of our research across diverse industrial\ncontexts.\n', '  We provide a condition monitoring system for wind farms, based on normal\nbehaviour modelling using a probabilistic multi-layer perceptron with transfer\nlearning via fine-tuning. The model predicts the output power of the wind\nturbine under normal behaviour based on features retrieved from supervisory\ncontrol and data acquisition (SCADA) systems. Its advantages are that (i) it\ncan be trained with SCADA data of at least a few years, (ii) it can incorporate\nall SCADA data of all wind turbines in a wind farm as features, (iii) it\nassumes that the output power follows a normal density with heteroscedastic\nvariance and (iv) it can predict the output of one wind turbine by borrowing\nstrength from the data of all other wind turbines in a farm. Probabilistic\nguidelines for condition monitoring are given via a CUSUM control chart. We\nillustrate the performance of our model in a real SCADA data example which\nprovides evidence that it outperforms other probabilistic prediction models.\n']",Predictive Maintenance for Industrial Equipment,Predictive Maintenance and Fault Detection in Industrial Systems,Industrial Automation and Control Systems
157,157,60,157_classification_misclassification_losses_surrogate,"['classification', 'misclassification', 'losses', 'surrogate', 'loss', 'learning', 'generalization', 'prediction', 'learnability', 'multiclass']","['losses', 'surrogate', 'bounds', 'consistency', 'loss', 'deferral', 'subset', 'label', 'abstention', 'classification']","['  We present a detailed study of top-$k$ classification, the task of predicting\nthe $k$ most probable classes for an input, extending beyond single-class\nprediction. We demonstrate that several prevalent surrogate loss functions in\nmulti-class classification, such as comp-sum and constrained losses, are\nsupported by $H$-consistency bounds with respect to the top-$k$ loss. These\nbounds guarantee consistency in relation to the hypothesis set $H$, providing\nstronger guarantees than Bayes-consistency due to their non-asymptotic and\nhypothesis-set specific nature. To address the trade-off between accuracy and\ncardinality $k$, we further introduce cardinality-aware loss functions through\ninstance-dependent cost-sensitive learning. For these functions, we derive\ncost-sensitive comp-sum and constrained surrogate losses, establishing their\n$H$-consistency bounds and Bayes-consistency. Minimizing these losses leads to\nnew cardinality-aware algorithms for top-$k$ classification. We report the\nresults of extensive experiments on CIFAR-100, ImageNet, CIFAR-10, and SVHN\ndatasets demonstrating the effectiveness and benefit of these algorithms.\n', '  We present a detailed study of surrogate losses and algorithms for\nmulti-label learning, supported by $H$-consistency bounds. We first show that,\nfor the simplest form of multi-label loss (the popular Hamming loss), the\nwell-known consistent binary relevance surrogate suffers from a sub-optimal\ndependency on the number of labels in terms of $H$-consistency bounds, when\nusing smooth losses such as logistic losses. Furthermore, this loss function\nfails to account for label correlations. To address these drawbacks, we\nintroduce a novel surrogate loss, multi-label logistic loss, that accounts for\nlabel correlations and benefits from label-independent $H$-consistency bounds.\nWe then broaden our analysis to cover a more extensive family of multi-label\nlosses, including all common ones and a new extension defined based on\nlinear-fractional functions with respect to the confusion matrix. We also\nextend our multi-label logistic losses to more comprehensive multi-label\ncomp-sum losses, adapting comp-sum losses from standard classification to the\nmulti-label learning. We prove that this family of surrogate losses benefits\nfrom $H$-consistency bounds, and thus Bayes-consistency, across any general\nmulti-label loss. Our work thus proposes a unified surrogate loss framework\nbenefiting from strong consistency guarantees for any multi-label loss,\nsignificantly expanding upon previous work which only established\nBayes-consistency and for specific loss functions. Additionally, we adapt\nconstrained losses from standard classification to multi-label constrained\nlosses in a similar way, which also benefit from $H$-consistency bounds and\nthus Bayes-consistency for any multi-label loss. We further describe efficient\ngradient computation algorithms for minimizing the multi-label logistic loss.\n', '  We present a study of surrogate losses and algorithms for the general problem\nof learning to defer with multiple experts. We first introduce a new family of\nsurrogate losses specifically tailored for the multiple-expert setting, where\nthe prediction and deferral functions are learned simultaneously. We then prove\nthat these surrogate losses benefit from strong $H$-consistency bounds. We\nillustrate the application of our analysis through several examples of\npractical surrogate losses, for which we give explicit guarantees. These loss\nfunctions readily lead to the design of new learning to defer algorithms based\non their minimization. While the main focus of this work is a theoretical\nanalysis, we also report the results of several experiments on SVHN and\nCIFAR-10 datasets.\n']",Surrogate Losses for Multiclass Classification,Surrogate Modeling for Machine Learning and Prediction,Machine Learning and Optimization
158,158,60,158_federated_privacy_intrusion_distributed,"['federated', 'privacy', 'intrusion', 'distributed', 'decentralized', 'ddos', 'cloud', 'security', 'anomaly', 'malicious']","['privacy', 'federated', 'clients', 'anomaly', 'poisoning', 'fairness', 'devices', 'smart', 'centralized', 'decentralized']","[""  The emergence of federated learning (FL) presents a promising approach to\nleverage decentralized data while preserving privacy. Furthermore, the\ncombination of FL and anomaly detection is particularly compelling because it\nallows for detecting rare and critical anomalies (usually also rare in locally\ngathered data) in sensitive data from multiple sources, such as cybersecurity\nand healthcare. However, benchmarking the performance of anomaly detection\nmethods in FL environments remains an underexplored area. This paper introduces\nFedAD-Bench, a unified benchmark for evaluating unsupervised anomaly detection\nalgorithms within the context of FL. We systematically analyze and compare the\nperformance of recent deep learning anomaly detection models under federated\nsettings, which were typically assessed solely in centralized settings.\nFedAD-Bench encompasses diverse datasets and metrics to provide a holistic\nevaluation. Through extensive experiments, we identify key challenges such as\nmodel aggregation inefficiencies and metric unreliability. We present insights\ninto FL's regularization effects, revealing scenarios in which it outperforms\ncentralized approaches due to its inherent ability to mitigate overfitting. Our\nwork aims to establish a standardized benchmark to guide future research and\ndevelopment in federated anomaly detection, promoting reproducibility and fair\ncomparison across studies.\n"", ""  Federated learning (FL) is a decentralized learning technique that enables\nparticipating devices to collaboratively build a shared Machine Leaning (ML) or\nDeep Learning (DL) model without revealing their raw data to a third party. Due\nto its privacy-preserving nature, FL has sparked widespread attention for\nbuilding Intrusion Detection Systems (IDS) within the realm of cybersecurity.\nHowever, the data heterogeneity across participating domains and entities\npresents significant challenges for the reliable implementation of an FL-based\nIDS. In this paper, we propose an effective method called Statistical Averaging\n(StatAvg) to alleviate non-independently and identically (non-iid) distributed\nfeatures across local clients' data in FL. In particular, StatAvg allows the FL\nclients to share their individual data statistics with the server, which then\naggregates this information to produce global statistics. The latter are shared\nwith the clients and used for universal data normalisation. It is worth\nmentioning that StatAvg can seamlessly integrate with any FL aggregation\nstrategy, as it occurs before the actual FL training process. The proposed\nmethod is evaluated against baseline approaches using datasets for network and\nhost Artificial Intelligence (AI)-powered IDS. The experimental results\ndemonstrate the efficiency of StatAvg in mitigating non-iid feature\ndistributions across the FL clients compared to the baseline methods.\n"", '  The increasing security and privacy concerns in the Smart Grid sector have\nled to a significant demand for robust intrusion detection systems within\ncritical smart grid infrastructure. To address the challenges posed by privacy\npreservation and decentralized power system zones with distinct data ownership,\nFederated Learning (FL) has emerged as a promising privacy-preserving solution\nwhich facilitates collaborative training of attack detection models without\nnecessitating the sharing of raw data. However, FL presents several\nimplementation limitations in the power system domain due to its heavy reliance\non a centralized aggregator and the risks of privacy leakage during model\nupdate transmission. To overcome these technical bottlenecks, this paper\nintroduces a novel decentralized federated anomaly detection scheme based on\ntwo main gossip protocols namely Random Walk and Epidemic. Our findings\nindicate that the Random Walk protocol exhibits superior performance compared\nto the Epidemic protocol, highlighting its efficacy in decentralized federated\nlearning environments. Experimental validation of the proposed framework\nutilizing publicly available industrial control systems datasets demonstrates\nsuperior attack detection accuracy while safeguarding data confidentiality and\nmitigating the impact of communication latency and stragglers. Furthermore, our\napproach yields a notable 35% improvement in training time compared to\nconventional FL, underscoring the efficacy and robustness of our decentralized\nlearning method.\n']",Federated Learning for Anomaly Detection and Security,Federated Learning for Privacy-Preserving Machine Learning,Machine Learning and Data Privacy
159,159,60,159_videos_videoshop_gans_clips,"['videos', 'videoshop', 'gans', 'clips', 'scenes', 'video', 'frames', 'compression', 'editing', 'generative']","['video', 'editing', 'inpainting', 'image', 'videos', 'diffusion', 'generation', 'images', 'motion', 'frames']","['  Text-driven diffusion-based video editing presents a unique challenge not\nencountered in image editing literature: establishing real-world motion. Unlike\nexisting video editing approaches, here we focus on score distillation sampling\nto circumvent the standard reverse diffusion process and initiate optimization\nfrom videos that already exhibit natural motion. Our analysis reveals that\nwhile video score distillation can effectively introduce new content indicated\nby target text, it can also cause significant structure and motion deviation.\nTo counteract this, we propose to match space-time self-similarities of the\noriginal video and the edited video during the score distillation. Thanks to\nthe use of score distillation, our approach is model-agnostic, which can be\napplied for both cascaded and non-cascaded video diffusion frameworks. Through\nextensive comparisons with leading methods, our approach demonstrates its\nsuperiority in altering appearances while accurately preserving the original\nstructure and motion.\n', '  Recent advances in text-to-image (T2I) diffusion models have enabled\nimpressive image generation capabilities guided by text prompts. However,\nextending these techniques to video generation remains challenging, with\nexisting text-to-video (T2V) methods often struggling to produce high-quality\nand motion-consistent videos. In this work, we introduce Control-A-Video, a\ncontrollable T2V diffusion model that can generate videos conditioned on text\nprompts and reference control maps like edge and depth maps. To tackle video\nquality and motion consistency issues, we propose novel strategies to\nincorporate content prior and motion prior into the diffusion-based generation\nprocess. Specifically, we employ a first-frame condition scheme to transfer\nvideo generation from the image domain. Additionally, we introduce\nresidual-based and optical flow-based noise initialization to infuse motion\npriors from reference videos, promoting relevance among frame latents for\nreduced flickering. Furthermore, we present a Spatio-Temporal Reward Feedback\nLearning (ST-ReFL) algorithm that optimizes the video diffusion model using\nmultiple reward models for video quality and motion consistency, leading to\nsuperior outputs. Comprehensive experiments demonstrate that our framework\ngenerates higher-quality, more consistent videos compared to existing\nstate-of-the-art methods in controllable text-to-video generation\n', '  Diffusion models have made tremendous progress in text-driven image and video\ngeneration. Now text-to-image foundation models are widely applied to various\ndownstream image synthesis tasks, such as controllable image generation and\nimage editing, while downstream video synthesis tasks are less explored for\nseveral reasons. First, it requires huge memory and computation overhead to\ntrain a video generation foundation model. Even with video foundation models,\nadditional costly training is still required for downstream video synthesis\ntasks. Second, although some works extend image diffusion models into videos in\na training-free manner, temporal consistency cannot be well preserved. Finally,\nthese adaption methods are specifically designed for one task and fail to\ngeneralize to different tasks. To mitigate these issues, we propose a\ntraining-free general-purpose video synthesis framework, coined as {\\bf\nBIVDiff}, via bridging specific image diffusion models and general\ntext-to-video foundation diffusion models. Specifically, we first use a\nspecific image diffusion model (e.g., ControlNet and Instruct Pix2Pix) for\nframe-wise video generation, then perform Mixed Inversion on the generated\nvideo, and finally input the inverted latents into the video diffusion models\n(e.g., VidRD and ZeroScope) for temporal smoothing. This decoupled framework\nenables flexible image model selection for different purposes with strong task\ngeneralization and high efficiency. To validate the effectiveness and general\nuse of BIVDiff, we perform a wide range of video synthesis tasks, including\ncontrollable video generation, video editing, video inpainting, and\noutpainting.\n']",Text-Driven Video Editing and Generation,Text-Guided Visual Content Generation and Editing,Artificial Intelligence for Creative Content Generation
160,160,60,160_attention_captioning_recognition_captions,"['attention', 'captioning', 'recognition', 'captions', 'scene', 'visual', 'vision', 'images', 'segmentation', 'coco']","['semantic', 'segmentation', 'vocabulary', 'slot', 'image', 'object', 'mask', 'objects', 'vision', 'open']","['  In the field of visual scene understanding, deep neural networks have made\nimpressive advancements in various core tasks like segmentation, tracking, and\ndetection. However, most approaches operate on the close-set assumption,\nmeaning that the model can only identify pre-defined categories that are\npresent in the training set. Recently, open vocabulary settings were proposed\ndue to the rapid progress of vision language pre-training. These new approaches\nseek to locate and recognize categories beyond the annotated label space. The\nopen vocabulary approach is more general, practical, and effective compared to\nweakly supervised and zero-shot settings. This paper provides a thorough review\nof open vocabulary learning, summarizing and analyzing recent developments in\nthe field. In particular, we begin by comparing it to related concepts such as\nzero-shot learning, open-set recognition, and out-of-distribution detection.\nThen, we review several closely related tasks in the case of segmentation and\ndetection, including long-tail problems, few-shot, and zero-shot settings. For\nthe method survey, we first present the basic knowledge of detection and\nsegmentation in close-set as the preliminary knowledge. Next, we examine\nvarious scenarios in which open vocabulary learning is used, identifying common\ndesign elements and core ideas. Then, we compare the recent detection and\nsegmentation approaches in commonly used datasets and benchmarks. Finally, we\nconclude with insights, issues, and discussions regarding future research\ndirections. To our knowledge, this is the first comprehensive literature review\nof open vocabulary learning. We keep tracing related works at\nhttps://github.com/jianzongwu/Awesome-Open-Vocabulary.\n', ""  Existing open-vocabulary image segmentation methods require a fine-tuning\nstep on mask labels and/or image-text datasets. Mask labels are\nlabor-intensive, which limits the number of categories in segmentation\ndatasets. Consequently, the vocabulary capacity of pre-trained VLMs is severely\nreduced after fine-tuning. However, without fine-tuning, VLMs trained under\nweak image-text supervision tend to make suboptimal mask predictions. To\nalleviate these issues, we introduce a novel recurrent framework that\nprogressively filters out irrelevant texts and enhances mask quality without\ntraining efforts. The recurrent unit is a two-stage segmenter built upon a\nfrozen VLM. Thus, our model retains the VLM's broad vocabulary space and equips\nit with segmentation ability. Experiments show that our method outperforms not\nonly the training-free counterparts, but also those fine-tuned with millions of\ndata samples, and sets the new state-of-the-art records for both zero-shot\nsemantic and referring segmentation. Concretely, we improve the current record\nby 28.8, 16.0, and 6.9 mIoU on Pascal VOC, COCO Object, and Pascal Context.\n"", '  Slot attention aims to decompose an input image into a set of meaningful\nobject files (slots). These latent object representations enable various\ndownstream tasks. Yet, these slots often bind to object parts, not objects\nthemselves, especially for real-world datasets. To address this, we introduce\nGuided Latent Slot Diffusion - GLASS, an object-centric model that uses\ngenerated captions as a guiding signal to better align slots with objects. Our\nkey insight is to learn the slot-attention module in the space of generated\nimages. This allows us to repurpose the pre-trained diffusion decoder model,\nwhich reconstructs the images from the slots, as a semantic mask generator\nbased on the generated captions. GLASS learns an object-level representation\nsuitable for multiple tasks simultaneously, e.g., segmentation, image\ngeneration, and property prediction, outperforming previous methods. For object\ndiscovery, GLASS achieves approx. a +35% and +10% relative improvement for mIoU\nover the previous state-of-the-art (SOTA) method on the VOC and COCO datasets,\nrespectively, and establishes a new SOTA FID score for conditional image\ngeneration amongst slot-attention-based methods. For the segmentation task,\nGLASS surpasses SOTA weakly-supervised and language-based segmentation models,\nwhich were specifically designed for the task.\n']",Open-Vocabulary Visual Scene Understanding,Multimodal Video and Image Understanding,Multimodal Learning and Applications
161,161,60,161_nl2gql_semantic_retrieval_knowledge,"['nl2gql', 'semantic', 'retrieval', 'knowledge', 'textual', 'answering', 'knowledgenavigator', 'gql', 'entities', 'queries']","['knowledge', 'reasoning', 'graph', 'commonsense', 'graphs', 'retrieval', 'factual', 'question', 'entities', 'triples']","['  Knowledge graphs (KGs) are large datasets with specific structures\nrepresenting large knowledge bases (KB) where each node represents a key entity\nand relations amongst them are typed edges. Natural language queries formed to\nextract information from a KB entail starting from specific nodes and reasoning\nover multiple edges of the corresponding KG to arrive at the correct set of\nanswer nodes. Traditional approaches of question answering on KG are based on\n(a) semantic parsing (SP), where a logical form (e.g., S-expression, SPARQL\nquery, etc.) is generated using node and edge embeddings and then reasoning\nover these representations or tuning language models to generate the final\nanswer directly, or (b) information-retrieval based that works by extracting\nentities and relations sequentially. In this work, we evaluate the capability\nof (LLMs) to answer questions over KG that involve multiple hops. We show that\ndepending upon the size and nature of the KG we need different approaches to\nextract and feed the relevant information to an LLM since every LLM comes with\na fixed context window. We evaluate our approach on six KGs with and without\nthe availability of example-specific sub-graphs and show that both the IR and\nSP-based methods can be adopted by LLMs resulting in an extremely competitive\nperformance.\n', '  Ensuring factual accuracy while maintaining the creative capabilities of\nLarge Language Model Agents (LMAs) poses significant challenges in the\ndevelopment of intelligent agent systems. LMAs face prevalent issues such as\ninformation hallucinations, catastrophic forgetting, and limitations in\nprocessing long contexts when dealing with knowledge-intensive tasks. This\npaper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation)\npipeline, a novel framework designed to enhance the knowledge capabilities of\nLMAs by integrating structured Knowledge Graphs (KGs) with the functionalities\nof LLMs, thereby significantly reducing the reliance on the latent knowledge of\nLLMs. The KG-RAG pipeline constructs a KG from unstructured text and then\nperforms information retrieval over the newly created graph to perform KGQA\n(Knowledge Graph Question Answering). The retrieval methodology leverages a\nnovel algorithm called Chain of Explorations (CoE) which benefits from LLMs\nreasoning to explore nodes and relationships within the KG sequentially.\nPreliminary experiments on the ComplexWebQuestions dataset demonstrate notable\nimprovements in the reduction of hallucinated content and suggest a promising\npath toward developing intelligent systems adept at handling\nknowledge-intensive tasks.\n', ""  To address the issue of insufficient knowledge and the tendency to generate\nhallucination in Large Language Models (LLMs), numerous studies have endeavored\nto integrate LLMs with Knowledge Graphs (KGs). However, all these methods are\nevaluated on conventional Knowledge Graph Question Answering (KGQA) with\ncomplete KGs, where the factual triples involved in each question are entirely\ncovered by the given KG. In this situation, LLM mainly acts as an agent to find\nanswer entities by exploring the KG, rather than effectively integrating\ninternal and external knowledge sources. However, in real-world scenarios, KGs\nare often incomplete to cover all the knowledge required to answer questions.\nTo simulate real-world scenarios and evaluate the ability of LLMs to integrate\ninternal and external knowledge, in this paper, we propose leveraging LLMs for\nQA under Incomplete Knowledge Graph (IKGQA), where the given KG doesn't include\nall the factual triples involved in each question. To handle IKGQA, we propose\na training-free method called Generate-on-Graph (GoG) that can generate new\nfactual triples while exploring on KGs. Specifically, we propose a\nselecting-generating-answering framework, which not only treat the LLM as an\nagent to explore on KGs, but also treat it as a KG to generate new facts based\non the explored subgraph and its inherent knowledge. Experimental results on\ntwo datasets demonstrate that our GoG can solve IKGQA to a certain extent,\nwhile almost all previous methods cannot perform well on IKGQA.\n""]",Knowledge Graph Question Answering,Question Answering and Reasoning Systems,Artificial Intelligence and Reasoning Systems
161,161,60,161_nl2gql_semantic_retrieval_knowledge,"['nl2gql', 'semantic', 'retrieval', 'knowledge', 'textual', 'answering', 'knowledgenavigator', 'gql', 'entities', 'queries']","['knowledge', 'reasoning', 'graph', 'commonsense', 'graphs', 'retrieval', 'factual', 'question', 'entities', 'triples']","['  Knowledge graphs (KGs) are large datasets with specific structures\nrepresenting large knowledge bases (KB) where each node represents a key entity\nand relations amongst them are typed edges. Natural language queries formed to\nextract information from a KB entail starting from specific nodes and reasoning\nover multiple edges of the corresponding KG to arrive at the correct set of\nanswer nodes. Traditional approaches of question answering on KG are based on\n(a) semantic parsing (SP), where a logical form (e.g., S-expression, SPARQL\nquery, etc.) is generated using node and edge embeddings and then reasoning\nover these representations or tuning language models to generate the final\nanswer directly, or (b) information-retrieval based that works by extracting\nentities and relations sequentially. In this work, we evaluate the capability\nof (LLMs) to answer questions over KG that involve multiple hops. We show that\ndepending upon the size and nature of the KG we need different approaches to\nextract and feed the relevant information to an LLM since every LLM comes with\na fixed context window. We evaluate our approach on six KGs with and without\nthe availability of example-specific sub-graphs and show that both the IR and\nSP-based methods can be adopted by LLMs resulting in an extremely competitive\nperformance.\n', '  Ensuring factual accuracy while maintaining the creative capabilities of\nLarge Language Model Agents (LMAs) poses significant challenges in the\ndevelopment of intelligent agent systems. LMAs face prevalent issues such as\ninformation hallucinations, catastrophic forgetting, and limitations in\nprocessing long contexts when dealing with knowledge-intensive tasks. This\npaper introduces a KG-RAG (Knowledge Graph-Retrieval Augmented Generation)\npipeline, a novel framework designed to enhance the knowledge capabilities of\nLMAs by integrating structured Knowledge Graphs (KGs) with the functionalities\nof LLMs, thereby significantly reducing the reliance on the latent knowledge of\nLLMs. The KG-RAG pipeline constructs a KG from unstructured text and then\nperforms information retrieval over the newly created graph to perform KGQA\n(Knowledge Graph Question Answering). The retrieval methodology leverages a\nnovel algorithm called Chain of Explorations (CoE) which benefits from LLMs\nreasoning to explore nodes and relationships within the KG sequentially.\nPreliminary experiments on the ComplexWebQuestions dataset demonstrate notable\nimprovements in the reduction of hallucinated content and suggest a promising\npath toward developing intelligent systems adept at handling\nknowledge-intensive tasks.\n', ""  To address the issue of insufficient knowledge and the tendency to generate\nhallucination in Large Language Models (LLMs), numerous studies have endeavored\nto integrate LLMs with Knowledge Graphs (KGs). However, all these methods are\nevaluated on conventional Knowledge Graph Question Answering (KGQA) with\ncomplete KGs, where the factual triples involved in each question are entirely\ncovered by the given KG. In this situation, LLM mainly acts as an agent to find\nanswer entities by exploring the KG, rather than effectively integrating\ninternal and external knowledge sources. However, in real-world scenarios, KGs\nare often incomplete to cover all the knowledge required to answer questions.\nTo simulate real-world scenarios and evaluate the ability of LLMs to integrate\ninternal and external knowledge, in this paper, we propose leveraging LLMs for\nQA under Incomplete Knowledge Graph (IKGQA), where the given KG doesn't include\nall the factual triples involved in each question. To handle IKGQA, we propose\na training-free method called Generate-on-Graph (GoG) that can generate new\nfactual triples while exploring on KGs. Specifically, we propose a\nselecting-generating-answering framework, which not only treat the LLM as an\nagent to explore on KGs, but also treat it as a KG to generate new facts based\non the explored subgraph and its inherent knowledge. Experimental results on\ntwo datasets demonstrate that our GoG can solve IKGQA to a certain extent,\nwhile almost all previous methods cannot perform well on IKGQA.\n""]",Knowledge Graph Question Answering,Question Answering and Reasoning Systems,Artificial Intelligence and Reasoning Systems
162,162,59,162_benchmarking_tools_benchmarks_benchmark,"['benchmarking', 'tools', 'benchmarks', 'benchmark', 'tool', 'evaluations', 'language', 'answering', 'software', 'openai']","['evaluation', 'open', 'instruction', 'turn', 'source', 'benchmark', 'language', 'tool', 'prompts', 'feedback']","['  Log analysis is crucial for ensuring the orderly and stable operation of\ninformation systems, particularly in the field of Artificial Intelligence for\nIT Operations (AIOps). Large Language Models (LLMs) have demonstrated\nsignificant potential in natural language processing tasks. In the AIOps\ndomain, they excel in tasks such as anomaly detection, root cause analysis of\nfaults, operations and maintenance script generation, and alert information\nsummarization. However, the performance of current LLMs in log analysis tasks\nremains inadequately validated. To address this gap, we introduce LogEval, a\ncomprehensive benchmark suite designed to evaluate the capabilities of LLMs in\nvarious log analysis tasks for the first time. This benchmark covers tasks such\nas log parsing, log anomaly detection, log fault diagnosis, and log\nsummarization. LogEval evaluates each task using 4,000 publicly available log\ndata entries and employs 15 different prompts for each task to ensure a\nthorough and fair assessment. By rigorously evaluating leading LLMs, we\ndemonstrate the impact of various LLM technologies on log analysis performance,\nfocusing on aspects such as self-consistency and few-shot contextual learning.\nWe also discuss findings related to model quantification, Chinese-English\nquestion-answering evaluation, and prompt engineering. These findings provide\ninsights into the strengths and weaknesses of LLMs in multilingual environments\nand the effectiveness of different prompt strategies. Various evaluation\nmethods are employed for different tasks to accurately measure the performance\nof LLMs in log analysis, ensuring a comprehensive assessment. The insights\ngained from LogEvals evaluation reveal the strengths and limitations of LLMs in\nlog analysis tasks, providing valuable guidance for researchers and\npractitioners.\n', ""  With the rapid advancement of large language models (LLMs), there is a\npressing need for a comprehensive evaluation suite to assess their capabilities\nand limitations. Existing LLM leaderboards often reference scores reported in\nother papers without consistent settings and prompts, which may inadvertently\nencourage cherry-picking favored settings and prompts for better results. In\nthis work, we introduce GPT-Fathom, an open-source and reproducible LLM\nevaluation suite built on top of OpenAI Evals. We systematically evaluate 10+\nleading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across\n7 capability categories, all under aligned settings. Our retrospective study on\nOpenAI's earlier models offers valuable insights into the evolutionary path\nfrom GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3\nprogressively improves to GPT-4, including technical details like whether\nadding code data improves LLM's reasoning capability, which aspects of LLM\ncapability can be improved by SFT and RLHF, how much is the alignment tax, etc.\nOur analysis sheds light on many of these questions, aiming to improve the\ntransparency of advanced LLMs.\n"", ""  To solve complex tasks, large language models (LLMs) often require multiple\nrounds of interactions with the user, sometimes assisted by external tools.\nHowever, current evaluation protocols often emphasize benchmark performance\nwith single-turn exchanges, neglecting the nuanced interactions among the user,\nLLMs, and external tools, while also underestimating the importance of natural\nlanguage feedback from users. These oversights contribute to discrepancies\nbetween research benchmark evaluations and real-world use cases. We introduce\nMINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn\ninteractions by (1) using tools and (2) leveraging natural language feedback.\nTo ensure reproducibility, we provide an evaluation framework where LLMs can\naccess tools by executing Python code and receive users' natural language\nfeedback simulated by GPT-4. We repurpose a diverse set of established\nevaluation datasets focusing on reasoning, coding, and decision-making and\ncarefully curate them into a compact subset for efficient evaluation. Our\nanalysis of 20 open- and closed-source LLMs offers intriguing findings. (a)\nLLMs generally benefit from tools and language feedback, with performance gains\n(absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural\nlanguage feedback. (b) Better single-turn performance does not guarantee better\nmulti-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised\ninstruction-finetuning (SIFT) and reinforcement learning from human feedback\n(RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure\nprogress and incentivize research in improving LLMs' capabilities in multi-turn\ninteractions, especially for open-source communities where multi-turn human\nevaluation can be less accessible compared to commercial LLMs with a larger\nuser base.\n""]",Large Language Model Benchmarking and Evaluation,Evaluating Large Language Models,Large Language Models
163,163,59,163_backdoors_backdoor_adversarial_attacks,"['backdoors', 'backdoor', 'adversarial', 'attacks', 'backdoored', 'attacker', 'vulnerability', 'malicious', 'security', 'obfuscation']","['backdoor', 'attack', 'watermarking', 'attacks', 'poisoned', 'authorship', 'malicious', 'poisoning', 'obfuscation', 'security']","['  One key challenge in backdoor attacks against large foundation models is the\nresource limits. Backdoor attacks usually require retraining the target model,\nwhich is impractical for very large foundation models. Existing backdoor\nattacks are mainly designed for supervised classifiers or small foundation\nmodels (e.g., BERT). None of these attacks has successfully compromised a very\nlarge foundation model, such as Llama-3-70B, especially with limited\ncomputational resources. In this paper, we propose TrojFM, a novel backdoor\nattack tailored for very large foundation models. Our primary technical\ncontribution is the development of a novel backdoor injection method. This\nmethod forces a backdoored model to generate similar hidden representations for\npoisoned inputs regardless of their actual semantics. Our approach injects such\nbackdoors by fine-tuning only a very small proportion of model parameters. This\nenables TrojFM to efficiently launch downstream task-agnostic backdoor attacks\nagainst very large foundation models under limited computational resources.\nMoreover, we optimize the fine-tuning process with our customized QLoRA\ntechnique, enabling launching our attack via only~\\textit{one A100 GPU}.\nFurthermore, we design a new trigger injection method to ensure our attack\nstealthiness. Through extensive experiments, we first demonstrate that TrojFM\ncan launch effective backdoor attacks against widely used large GPT-style\nmodels without jeopardizing their normal functionalities (and outperforming\nexisting attacks on BERT-style models). Furthermore, we show that TrojFM is\nresilient to SOTA defenses and is insensitive to changes in key\nhyper-parameters. Finally, we conduct a resource analysis to quantify that our\nmethod can significantly save computational and memory costs compared to\nexisting backdoor attacks.\n', '  The field of few-shot learning (FSL) has shown promising results in scenarios\nwhere training data is limited, but its vulnerability to backdoor attacks\nremains largely unexplored. We first explore this topic by first evaluating the\nperformance of the existing backdoor attack methods on few-shot learning\nscenarios. Unlike in standard supervised learning, existing backdoor attack\nmethods failed to perform an effective attack in FSL due to two main issues.\nFirstly, the model tends to overfit to either benign features or trigger\nfeatures, causing a tough trade-off between attack success rate and benign\naccuracy. Secondly, due to the small number of training samples, the dirty\nlabel or visible trigger in the support set can be easily detected by victims,\nwhich reduces the stealthiness of attacks. It seemed that FSL could survive\nfrom backdoor attacks. However, in this paper, we propose the Few-shot Learning\nBackdoor Attack (FLBA) to show that FSL can still be vulnerable to backdoor\nattacks. Specifically, we first generate a trigger to maximize the gap between\npoisoned and benign features. It enables the model to learn both benign and\ntrigger features, which solves the problem of overfitting. To make it more\nstealthy, we hide the trigger by optimizing two types of imperceptible\nperturbation, namely attractive and repulsive perturbation, instead of\nattaching the trigger directly. Once we obtain the perturbations, we can poison\nall samples in the benign support set into a hidden poisoned support set and\nfine-tune the model on it. Our method demonstrates a high Attack Success Rate\n(ASR) in FSL tasks with different few-shot learning paradigms while preserving\nclean accuracy and maintaining stealthiness. This study reveals that few-shot\nlearning still suffers from backdoor attacks, and its security should be given\nattention.\n', '  Backdoor attacks involve the injection of a limited quantity of poisoned\nexamples containing triggers into the training dataset. During the inference\nstage, backdoor attacks can uphold a high level of accuracy for normal\nexamples, yet when presented with trigger-containing instances, the model may\nerroneously predict them as the targeted class designated by the attacker. This\npaper explores strategies for mitigating the risks associated with backdoor\nattacks by examining the filtration of poisoned samples.We primarily leverage\ntwo key characteristics of backdoor attacks: the ability for multiple backdoors\nto exist simultaneously within a single model, and the discovery through\nComposite Backdoor Attack (CBA) that altering two triggers in a sample to new\ntarget labels does not compromise the original functionality of the triggers,\nyet enables the prediction of the data as a new target class when both triggers\nare present simultaneously.Therefore, a novel three-stage poisoning data\nfiltering approach, known as Composite Backdoor Poison Filtering (CBPF), is\nproposed as an effective solution. Firstly, utilizing the identified\ndistinctions in output between poisoned and clean samples, a subset of data is\npartitioned to include both poisoned and clean instances. Subsequently, benign\ntriggers are incorporated and labels are adjusted to create new target and\nbenign target classes, thereby prompting the poisoned and clean data to be\nclassified as distinct entities during the inference stage. The experimental\nresults indicate that CBPF is successful in filtering out malicious data\nproduced by six advanced attacks on CIFAR10 and ImageNet-12. On average, CBPF\nattains a notable filtering success rate of 99.91% for the six attacks on\nCIFAR10. Additionally, the model trained on the uncontaminated samples exhibits\nsustained high accuracy levels.\n']",Backdoor Attacks on Large Foundation Models,Backdoor Attacks on AI Models,Adversarial Attacks and Vulnerabilities in AI and Machine Learning
164,164,58,164_hyperbolic_embeddings_embedding_manifolds,"['hyperbolic', 'embeddings', 'embedding', 'manifolds', 'manifold', 'dimensional', 'cnns', 'riemannian', 'classification', 'hessian']","['hyperbolic', 'manifold', 'manifolds', 'geometry', 'geometric', 'spaces', 'dimensional', 'space', 'curvature', 'networks']","['  Hyperbolic embeddings have demonstrated their effectiveness in capturing\nmeasures of uncertainty and hierarchical relationships across various\ndeep-learning tasks, including image segmentation and active learning. However,\ntheir application in modern vision-language models (VLMs) has been limited. A\nnotable exception is MERU, which leverages the hierarchical properties of\nhyperbolic space in the CLIP ViT-large model, consisting of hundreds of\nmillions parameters. In our work, we address the challenges of scaling\nmulti-modal hyperbolic models by orders of magnitude in terms of parameters\n(billions) and training complexity using the BLIP-2 architecture. Although\nhyperbolic embeddings offer potential insights into uncertainty not present in\nEuclidean embeddings, our analysis reveals that scaling these models is\nparticularly difficult. We propose a novel training strategy for a hyperbolic\nversion of BLIP-2, which allows to achieve comparable performance to its\nEuclidean counterpart, while maintaining stability throughout the training\nprocess and showing a meaningful indication of uncertainty with each embedding.\n', '  The need to understand the structure of hierarchical or high-dimensional data\nis present in a variety of fields. Hyperbolic spaces have proven to be an\nimportant tool for embedding computations and analysis tasks as their\nnon-linear nature lends itself well to tree or graph data. Subsequently, they\nhave also been used in the visualization of high-dimensional data, where they\nexhibit increased embedding performance. However, none of the existing\ndimensionality reduction methods for embedding into hyperbolic spaces scale\nwell with the size of the input data. That is because the embeddings are\ncomputed via iterative optimization schemes and the computation cost of every\niteration is quadratic in the size of the input. Furthermore, due to the\nnon-linear nature of hyperbolic spaces, Euclidean acceleration structures\ncannot directly be translated to the hyperbolic setting. This paper introduces\nthe first acceleration structure for hyperbolic embeddings, building upon a\npolar quadtree. We compare our approach with existing methods and demonstrate\nthat it computes embeddings of similar quality in significantly less time.\nImplementation and scripts for the experiments can be found at\nhttps://graphics.tudelft.nl/accelerating-hyperbolic-tsne.\n', '  Hyperbolic geometry is gaining traction in machine learning for its\neffectiveness at capturing hierarchical structures in real-world data.\nHyperbolic spaces, where neighborhoods grow exponentially, offer substantial\nadvantages and consistently deliver state-of-the-art results across diverse\napplications. However, hyperbolic classifiers often grapple with computational\nchallenges. Methods reliant on Riemannian optimization frequently exhibit\nsluggishness, stemming from the increased computational demands of operations\non Riemannian manifolds. In response to these challenges, we present hyperDT, a\nnovel extension of decision tree algorithms into hyperbolic space. Crucially,\nhyperDT eliminates the need for computationally intensive Riemannian\noptimization, numerically unstable exponential and logarithmic maps, or\npairwise comparisons between points by leveraging inner products to adapt\nEuclidean decision tree algorithms to hyperbolic space. Our approach is\nconceptually straightforward and maintains constant-time decision complexity\nwhile mitigating the scalability issues inherent in high-dimensional Euclidean\nspaces. Building upon hyperDT we introduce hyperRF, a hyperbolic random forest\nmodel. Extensive benchmarking across diverse datasets underscores the superior\nperformance of these models, providing a swift, precise, accurate, and\nuser-friendly toolkit for hyperbolic data analysis.\n']",Hyperbolic Embeddings and Manifolds,Geometric Deep Learning on Manifolds,Geometric and Equivariant Deep Learning
165,165,58,165_annotation_corpus_semantic_metadata,"['annotation', 'corpus', 'semantic', 'metadata', 'texts', 'scholarly', 'wikipedia', 'ontologies', 'ontology', 'retrieval']","['scientific', 'documents', 'literature', 'extraction', 'entities', 'tables', 'document', 'knowledge', 'ontology', 'entity']","[""  In scientific research and its application, scientific literature analysis is\ncrucial as it allows researchers to build on the work of others. However, the\nfast growth of scientific knowledge has led to a massive increase in scholarly\narticles, making in-depth literature analysis increasingly challenging and\ntime-consuming. The emergence of Large Language Models (LLMs) has offered a new\nway to address this challenge. Known for their strong abilities in summarizing\ntexts, LLMs are seen as a potential tool to improve the analysis of scientific\nliterature. However, existing LLMs have their own limits. Scientific literature\noften includes a wide range of multimodal elements, such as tables, charts, and\nmolecule, which are hard for text-focused LLMs to understand and analyze. This\nissue points to the urgent need for new solutions that can fully understand and\nanalyze multimodal content in scientific literature. To answer this demand, we\npresent \\textbf{Uni-SMART} (Universal Science Multimodal Analysis and Research\nTransformer), an innovative model designed for in-depth understanding of\nmultimodal scientific literature. Through rigorous quantitative evaluation\nacross several domains, Uni-SMART demonstrates superior performance over other\ntext-focused LLMs. Furthermore, our exploration extends to practical\napplications, including patent infringement detection and nuanced analysis of\ncharts. These applications not only highlight Uni-SMART's adaptability but also\nits potential to revolutionize how we interact with scientific literature.\n"", '  This project investigates the efficacy of Large Language Models (LLMs) in\nunderstanding and extracting scientific knowledge across specific domains and\nto create a deep learning framework: Knowledge AI. As a part of this framework,\nwe employ pre-trained models and fine-tune them on datasets in the scientific\ndomain. The models are adapted for four key Natural Language Processing (NLP)\ntasks: summarization, text generation, question answering, and named entity\nrecognition. Our results indicate that domain-specific fine-tuning\nsignificantly enhances model performance in each of these tasks, thereby\nimproving their applicability for scientific contexts. This adaptation enables\nnon-experts to efficiently query and extract information within targeted\nscientific fields, demonstrating the potential of fine-tuned LLMs as a tool for\nknowledge discovery in the sciences.\n', '  With the rapid development of the internet in the past decade, it has become\nincreasingly important to extract valuable information from vast resources\nefficiently, which is crucial for establishing a comprehensive digital\necosystem, particularly in the context of research surveys and comprehension.\nThe foundation of these tasks focuses on accurate extraction and deep mining of\ndata from scientific documents, which are essential for building a robust data\ninfrastructure. However, parsing raw data or extracting data from complex\nscientific documents have been ongoing challenges. Current data extraction\nmethods for scientific documents typically use rule-based (RB) or machine\nlearning (ML) approaches. However, using rule-based methods can incur high\ncoding costs for articles with intricate typesetting. Conversely, relying\nsolely on machine learning methods necessitates annotation work for complex\ncontent types within the scientific document, which can be costly.\nAdditionally, few studies have thoroughly defined and explored the hierarchical\nlayout within scientific documents. The lack of a comprehensive definition of\nthe internal structure and elements of the documents indirectly impacts the\naccuracy of text classification and object recognition tasks. From the\nperspective of analyzing the standard layout and typesetting used in the\nspecified publication, we propose a new document layout analysis framework\ncalled CTBR(Compartment & Text Blocks Refinement). Firstly, we define\nscientific documents into hierarchical divisions: base domain, compartment, and\ntext blocks. Next, we conduct an in-depth exploration and classification of the\nmeanings of text blocks. Finally, we utilize the results of text block\nclassification to implement object recognition within scientific documents\nbased on rule-based compartment segmentation.\n']",Scientific Literature Analysis with Large Language Models,Large Language Models for Text Analysis and Summarization,Large Language Models
166,166,57,166_chatbots_chatbot_socialbot_conversational,"['chatbots', 'chatbot', 'socialbot', 'conversational', 'conversation', 'conversations', 'dialogues', 'chat', 'dialogue', 'chatgpt']","['chatbots', 'conversational', 'chatbot', 'conversations', 'human', 'user', 'agents', 'research', 'dialogue', 'responses']","[""  Student commitment towards a learning recommendation is not separable from\ntheir understanding of the reasons it was recommended to them; and their\nability to modify it based on that understanding. Among explainability\napproaches, chatbots offer the potential to engage the student in a\nconversation, similar to a discussion with a peer or a mentor. The capabilities\nof chatbots, however, are still not sufficient to replace a human mentor,\ndespite the advancements of generative AI (GenAI) and large language models\n(LLM). Therefore, we propose an approach to utilize chatbots as mediators of\nthe conversation and sources of limited and controlled generation of\nexplanations, to harvest the potential of LLMs while reducing their potential\nrisks at the same time. The proposed LLM-based chatbot supports students in\nunderstanding learning-paths recommendations. We use a knowledge graph (KG) as\na human-curated source of information, to regulate the LLM's output through\ndefining its prompt's context. A group chat approach is developed to connect\nstudents with human mentors, either on demand or in cases that exceed the\nchatbot's pre-defined tasks. We evaluate the chatbot with a user study, to\nprovide a proof-of-concept and highlight the potential requirements and\nlimitations of utilizing chatbots in conversational explainability.\n"", '  AI-driven chatbots such as ChatGPT have caused a tremendous hype lately. For\nBPM applications, several applications for AI-driven chatbots have been\nidentified to be promising to generate business value, including explanation of\nprocess mining outcomes and preparation of input data. However, a systematic\nanalysis of chatbots for their support of conversational process modeling as a\nprocess-oriented capability is missing. This work aims at closing this gap by\nproviding a systematic analysis of existing chatbots. Application scenarios are\nidentified along the process life cycle. Then a systematic literature review on\nconversational process modeling is performed, resulting in a taxonomy of\napplication scenarios for conversational process modeling, including\nparaphrasing and improvement of process descriptions. In addition, this work\nsuggests and applies an evaluation method for the output of AI-driven chatbots\nwith respect to completeness and correctness of the process models. This method\nconsists of a set of KPIs on a test set, a set of prompts for task and control\nflow extraction, as well as a survey with users. Based on the literature and\nthe evaluation, recommendations for the usage (practical implications) and\nfurther development (research directions) of conversational process modeling\nare derived.\n', ""  Software developers use natural language to interact not only with other\nhumans, but increasingly also with chatbots. These interactions have different\nproperties and flow differently based on what goal the developer wants to\nachieve and who they interact with. In this paper, we aim to understand the\ndynamics of conversations that occur during modern software development after\nthe integration of AI and chatbots, enabling a deeper recognition of the\nadvantages and disadvantages of including chatbot interactions in addition to\nhuman conversations in collaborative work. We compile existing conversation\nattributes with humans and NLU-based chatbots and adapt them to the context of\nsoftware development. Then, we extend the comparison to include LLM-powered\nchatbots based on an observational study. We present similarities and\ndifferences between human-to-human and human-to-bot conversations, also\ndistinguishing between NLU- and LLM-based chatbots. Furthermore, we discuss how\nunderstanding the differences among the conversation styles guides the\ndeveloper on how to shape their expectations from a conversation and\nconsequently support the communication within a software team. We conclude that\nthe recent conversation styles that we observe with LLM-chatbots can not\nreplace conversations with humans due to certain attributes regarding social\naspects despite their ability to support productivity and decrease the\ndevelopers' mental load.\n""]",Conversational Chatbots in Learning and Software Development,Conversational AI and Chatbots,Conversational AI and Human-Computer Interaction
167,167,57,167_robotic_robotics_robot_robots,"['robotic', 'robotics', 'robot', 'robots', 'manipulators', 'grasping', 'softmac', 'manipulator', 'exoskeleton', 'mechanical']","['robot', 'soft', 'robots', 'force', 'surgical', 'robotic', 'surgery', 'motion', 'control', 'rigid']","['  Soft robots have many advantages over rigid robots thanks to their compliant\nand passive nature. However, it is generally challenging to model the dynamics\nof soft robots due to their high spatial dimensionality, making it difficult to\nuse model-based methods to accurately control soft robots. It often requires\ndirect numerical simulation of partial differential equations to simulate soft\nrobots. This not only requires an accurate numerical model, but also makes soft\nrobot modeling slow and expensive. Deep learning algorithms have shown promises\nin data-driven modeling of soft robots. However, these algorithms usually\nrequire a large amount of data, which are difficult to obtain in either\nsimulation or real-world experiments of soft robots. In this work, we propose\nKNODE-Cosserat, a framework that combines first-principle physics models and\nneural ordinary differential equations. We leverage the best from both worlds\n-- the generalization ability of physics-based models and the fast speed of\ndeep learning methods. We validate our framework in both simulation and\nreal-world experiments. In both cases, we show that the robot model\nsignificantly improves over the baseline models under different metrics.\n', ""  The safety and accuracy of robotic navigation hold paramount importance,\nespecially in the realm of soft continuum robotics, where the limitations of\ntraditional rigid sensors become evident. Encoders, piezoresistive, and\npotentiometer sensors often fail to integrate well with the flexible nature of\nthese robots, adding unwanted bulk and rigidity. To overcome these hurdles, our\nstudy presents a new approach to shape sensing in soft continuum robots through\nthe use of soft e-textile resistive sensors. This sensor, designed to\nflawlessly integrate with the robot's structure, utilizes a resistive material\nthat adjusts its resistance in response to the robot's movements and\ndeformations. This adjustment facilitates the capture of multidimensional force\nmeasurements across the soft sensor layers. A deep Convolutional Neural Network\n(CNN) is employed to decode the sensor signals, enabling precise estimation of\nthe robot's shape configuration based on the detailed data from the e-textile\nsensor. Our research investigates the efficacy of this e-textile sensor in\ndetermining the curvature parameters of soft continuum robots. The findings are\nencouraging, showing that the soft e-textile sensor not only matches but\npotentially exceeds the capabilities of traditional rigid sensors in terms of\nshape sensing and estimation. This advancement significantly boosts the safety\nand efficiency of robotic navigation systems.\n"", '  The dominant paradigm for end-to-end robot learning focuses on optimizing\ntask-specific objectives that solve a single robotic problem such as picking up\nan object or reaching a target position. However, recent work on high-capacity\nmodels in robotics has shown promise toward being trained on large collections\nof diverse and task-agnostic datasets of video demonstrations. These models\nhave shown impressive levels of generalization to unseen circumstances,\nespecially as the amount of data and the model complexity scale. Surgical robot\nsystems that learn from data have struggled to advance as quickly as other\nfields of robot learning for a few reasons: (1) there is a lack of existing\nlarge-scale open-source data to train models, (2) it is challenging to model\nthe soft-body deformations that these robots work with during surgery because\nsimulation cannot match the physical and visual complexity of biological\ntissue, and (3) surgical robots risk harming patients when tested in clinical\ntrials and require more extensive safety measures. This perspective article\naims to provide a path toward increasing robot autonomy in robot-assisted\nsurgery through the development of a multi-modal, multi-task,\nvision-language-action model for surgical robots. Ultimately, we argue that\nsurgical robots are uniquely positioned to benefit from general-purpose models\nand provide three guiding actions toward increased autonomy in robot-assisted\nsurgery.\n']",Soft Robotics and Robot Learning,Robotics and Autonomous Systems,Robotics and Artificial Intelligence
168,168,57,168_riemannian_manifold_manifolds_geodesics,"['riemannian', 'manifold', 'manifolds', 'geodesics', 'geodesic', 'generative', 'gps', 'divergence', 'spatiotemporal', 'gradient']","['manifold', 'density', 'manifolds', 'covariance', 'kernel', 'geodesics', 'flow', 'dimensional', 'kernels', 'estimation']","['  Learning the distribution of data on Riemannian manifolds is crucial for\nmodeling data from non-Euclidean space, which is required by many applications\nin diverse scientific fields. Yet, existing generative models on manifolds\nsuffer from expensive divergence computation or rely on approximations of heat\nkernel. These limitations restrict their applicability to simple geometries and\nhinder scalability to high dimensions. In this work, we introduce the\nRiemannian Diffusion Mixture, a principled framework for building a generative\ndiffusion process on manifolds. Instead of following the denoising approach of\nprevious diffusion models, we construct a diffusion process using a mixture of\nbridge processes derived on general manifolds without requiring heat kernel\nestimations. We develop a geometric understanding of the mixture process,\nderiving the drift as a weighted mean of tangent directions to the data points\nthat guides the process toward the data distribution. We further propose a\nscalable training objective for learning the mixture process that readily\napplies to general manifolds. Our method achieves superior performance on\ndiverse manifolds with dramatically reduced number of in-training simulation\nsteps for general manifolds.\n', '  The density ratio of two probability distributions is one of the fundamental\ntools in mathematical and computational statistics and machine learning, and it\nhas a variety of known applications. Therefore, density ratio estimation from\nfinite samples is a very important task, but it is known to be unstable when\nthe distributions are distant from each other. One approach to address this\nproblem is density ratio estimation using incremental mixtures of the two\ndistributions. We geometrically reinterpret existing methods for density ratio\nestimation based on incremental mixtures. We show that these methods can be\nregarded as iterating on the Riemannian manifold along a particular curve\nbetween the two probability distributions. Making use of the geometry of the\nmanifold, we propose to consider incremental density ratio estimation along\ngeneralized geodesics on this manifold. To achieve such a method requires Monte\nCarlo sampling along geodesics via transformations of the two distributions. We\nshow how to implement an iterative algorithm to sample along these geodesics\nand show how changing the distances along the geodesic affect the variance and\naccuracy of the estimation of the density ratio. Our experiments demonstrate\nthat the proposed approach outperforms the existing approaches using\nincremental mixtures that do not take the geometry of the\n', ""  We consider the fundamental task of optimising a real-valued function defined\nin a potentially high-dimensional Euclidean space, such as the loss function in\nmany machine-learning tasks or the logarithm of the probability distribution in\nstatistical inference. We use Riemannian geometry notions to redefine the\noptimisation problem of a function on the Euclidean space to a Riemannian\nmanifold with a warped metric, and then find the function's optimum along this\nmanifold. The warped metric chosen for the search domain induces a\ncomputational friendly metric-tensor for which optimal search directions\nassociated with geodesic curves on the manifold becomes easier to compute.\nPerforming optimization along geodesics is known to be generally infeasible,\nyet we show that in this specific manifold we can analytically derive Taylor\napproximations up to third-order. In general these approximations to the\ngeodesic curve will not lie on the manifold, however we construct suitable\nretraction maps to pull them back onto the manifold. Therefore, we can\nefficiently optimize along the approximate geodesic curves. We cover the\nrelated theory, describe a practical optimization algorithm and empirically\nevaluate it on a collection of challenging optimisation benchmarks. Our\nproposed algorithm, using 3rd-order approximation of geodesics, tends to\noutperform standard Euclidean gradient-based counterparts in term of number of\niterations until convergence.\n""]",Riemannian Manifolds in Generative Models and Optimization,Generative Modeling Techniques,Generative Modeling and Artificial Intelligence
169,169,56,169_corpus_syntactic_nlp_context,"['corpus', 'syntactic', 'nlp', 'context', 'linguistic', 'multilingual', 'parsing', 'lingual', 'sentences', 'language']","['syntactic', 'language', 'translation', 'context', 'sequence', 'languages', 'confidence', 'distribution', 'calibration', 'lingual']","[""  In the era of large language models (LLMs), in-context learning (ICL) stands\nout as an effective prompting strategy that explores LLMs' potency across\nvarious tasks. However, applying LLMs to grammatical error correction (GEC) is\nstill a challenging task. In this paper, we propose a novel\nungrammatical-syntax-based in-context example selection strategy for GEC.\nSpecifically, we measure similarity of sentences based on their syntactic\nstructures with diverse algorithms, and identify optimal ICL examples sharing\nthe most similar ill-formed syntax to the test input. Additionally, we carry\nout a two-stage process to further improve the quality of selection results. On\nbenchmark English GEC datasets, empirical results show that our proposed\nungrammatical-syntax-based strategies outperform commonly-used word-matching or\nsemantics-based methods with multiple LLMs. This indicates that for a\nsyntax-oriented task like GEC, paying more attention to syntactic information\ncan effectively boost LLMs' performance. Our code will be publicly available\nafter the publication of this paper.\n"", '  Recent interest has surged in employing Large Language Models (LLMs) for\nmachine translation (MT) via in-context learning (ICL) (Vilar et al., 2023).\nMost prior studies primarily focus on optimizing translation quality, with\nlimited attention to understanding the specific aspects of ICL that influence\nthe said quality. To this end, we perform the first of its kind, an exhaustive\nstudy of in-context learning for machine translation. We first establish that\nICL is primarily example-driven and not instruction-driven. Following this, we\nconduct an extensive exploration of various aspects of the examples to\nunderstand their influence on downstream performance. Our analysis includes\nfactors such as quality and quantity of demonstrations, spatial proximity, and\nsource versus target originality. Further, we also investigate challenging\nscenarios involving indirectness and misalignment of examples to understand the\nlimits of ICL. While we establish the significance of the quality of the target\ndistribution over the source distribution of demonstrations, we further observe\nthat perturbations sometimes act as regularizers, resulting in performance\nimprovements. Surprisingly, ICL does not necessitate examples from the same\ntask, and a related task with the same target distribution proves sufficient.\nWe hope that our study acts as a guiding resource for considerations in\nutilizing ICL for MT. Our code is available on\nhttps://github.com/PranjalChitale/in-context-mt-analysis.\n', ""  In-context learning (ICL) is the trending prompting strategy in the era of\nlarge language models (LLMs), where a few examples are demonstrated to evoke\nLLMs' power for a given task. How to select informative examples remains an\nopen issue. Previous works on in-context example selection for machine\ntranslation (MT) focus on superficial word-level features while ignoring deep\nsyntax-level knowledge. In this paper, we propose a syntax-based in-context\nexample selection method for MT, by computing the syntactic similarity between\ndependency trees using Polynomial Distance. In addition, we propose an ensemble\nstrategy combining examples selected by both word-level and syntax-level\ncriteria. Experimental results between English and 6 common languages indicate\nthat syntax can effectively enhancing ICL for MT, obtaining the highest COMET\nscores on 11 out of 12 translation directions.\n""]",In-Context Learning for NLP Tasks,Advances in Large Language Models,Large Language Models
169,169,56,169_corpus_syntactic_nlp_context,"['corpus', 'syntactic', 'nlp', 'context', 'linguistic', 'multilingual', 'parsing', 'lingual', 'sentences', 'language']","['syntactic', 'language', 'translation', 'context', 'sequence', 'languages', 'confidence', 'distribution', 'calibration', 'lingual']","[""  In the era of large language models (LLMs), in-context learning (ICL) stands\nout as an effective prompting strategy that explores LLMs' potency across\nvarious tasks. However, applying LLMs to grammatical error correction (GEC) is\nstill a challenging task. In this paper, we propose a novel\nungrammatical-syntax-based in-context example selection strategy for GEC.\nSpecifically, we measure similarity of sentences based on their syntactic\nstructures with diverse algorithms, and identify optimal ICL examples sharing\nthe most similar ill-formed syntax to the test input. Additionally, we carry\nout a two-stage process to further improve the quality of selection results. On\nbenchmark English GEC datasets, empirical results show that our proposed\nungrammatical-syntax-based strategies outperform commonly-used word-matching or\nsemantics-based methods with multiple LLMs. This indicates that for a\nsyntax-oriented task like GEC, paying more attention to syntactic information\ncan effectively boost LLMs' performance. Our code will be publicly available\nafter the publication of this paper.\n"", '  Recent interest has surged in employing Large Language Models (LLMs) for\nmachine translation (MT) via in-context learning (ICL) (Vilar et al., 2023).\nMost prior studies primarily focus on optimizing translation quality, with\nlimited attention to understanding the specific aspects of ICL that influence\nthe said quality. To this end, we perform the first of its kind, an exhaustive\nstudy of in-context learning for machine translation. We first establish that\nICL is primarily example-driven and not instruction-driven. Following this, we\nconduct an extensive exploration of various aspects of the examples to\nunderstand their influence on downstream performance. Our analysis includes\nfactors such as quality and quantity of demonstrations, spatial proximity, and\nsource versus target originality. Further, we also investigate challenging\nscenarios involving indirectness and misalignment of examples to understand the\nlimits of ICL. While we establish the significance of the quality of the target\ndistribution over the source distribution of demonstrations, we further observe\nthat perturbations sometimes act as regularizers, resulting in performance\nimprovements. Surprisingly, ICL does not necessitate examples from the same\ntask, and a related task with the same target distribution proves sufficient.\nWe hope that our study acts as a guiding resource for considerations in\nutilizing ICL for MT. Our code is available on\nhttps://github.com/PranjalChitale/in-context-mt-analysis.\n', ""  In-context learning (ICL) is the trending prompting strategy in the era of\nlarge language models (LLMs), where a few examples are demonstrated to evoke\nLLMs' power for a given task. How to select informative examples remains an\nopen issue. Previous works on in-context example selection for machine\ntranslation (MT) focus on superficial word-level features while ignoring deep\nsyntax-level knowledge. In this paper, we propose a syntax-based in-context\nexample selection method for MT, by computing the syntactic similarity between\ndependency trees using Polynomial Distance. In addition, we propose an ensemble\nstrategy combining examples selected by both word-level and syntax-level\ncriteria. Experimental results between English and 6 common languages indicate\nthat syntax can effectively enhancing ICL for MT, obtaining the highest COMET\nscores on 11 out of 12 translation directions.\n""]",In-Context Learning for NLP Tasks,Advances in Large Language Models,Large Language Models
169,169,56,169_corpus_syntactic_nlp_context,"['corpus', 'syntactic', 'nlp', 'context', 'linguistic', 'multilingual', 'parsing', 'lingual', 'sentences', 'language']","['syntactic', 'language', 'translation', 'context', 'sequence', 'languages', 'confidence', 'distribution', 'calibration', 'lingual']","[""  In the era of large language models (LLMs), in-context learning (ICL) stands\nout as an effective prompting strategy that explores LLMs' potency across\nvarious tasks. However, applying LLMs to grammatical error correction (GEC) is\nstill a challenging task. In this paper, we propose a novel\nungrammatical-syntax-based in-context example selection strategy for GEC.\nSpecifically, we measure similarity of sentences based on their syntactic\nstructures with diverse algorithms, and identify optimal ICL examples sharing\nthe most similar ill-formed syntax to the test input. Additionally, we carry\nout a two-stage process to further improve the quality of selection results. On\nbenchmark English GEC datasets, empirical results show that our proposed\nungrammatical-syntax-based strategies outperform commonly-used word-matching or\nsemantics-based methods with multiple LLMs. This indicates that for a\nsyntax-oriented task like GEC, paying more attention to syntactic information\ncan effectively boost LLMs' performance. Our code will be publicly available\nafter the publication of this paper.\n"", '  Recent interest has surged in employing Large Language Models (LLMs) for\nmachine translation (MT) via in-context learning (ICL) (Vilar et al., 2023).\nMost prior studies primarily focus on optimizing translation quality, with\nlimited attention to understanding the specific aspects of ICL that influence\nthe said quality. To this end, we perform the first of its kind, an exhaustive\nstudy of in-context learning for machine translation. We first establish that\nICL is primarily example-driven and not instruction-driven. Following this, we\nconduct an extensive exploration of various aspects of the examples to\nunderstand their influence on downstream performance. Our analysis includes\nfactors such as quality and quantity of demonstrations, spatial proximity, and\nsource versus target originality. Further, we also investigate challenging\nscenarios involving indirectness and misalignment of examples to understand the\nlimits of ICL. While we establish the significance of the quality of the target\ndistribution over the source distribution of demonstrations, we further observe\nthat perturbations sometimes act as regularizers, resulting in performance\nimprovements. Surprisingly, ICL does not necessitate examples from the same\ntask, and a related task with the same target distribution proves sufficient.\nWe hope that our study acts as a guiding resource for considerations in\nutilizing ICL for MT. Our code is available on\nhttps://github.com/PranjalChitale/in-context-mt-analysis.\n', ""  In-context learning (ICL) is the trending prompting strategy in the era of\nlarge language models (LLMs), where a few examples are demonstrated to evoke\nLLMs' power for a given task. How to select informative examples remains an\nopen issue. Previous works on in-context example selection for machine\ntranslation (MT) focus on superficial word-level features while ignoring deep\nsyntax-level knowledge. In this paper, we propose a syntax-based in-context\nexample selection method for MT, by computing the syntactic similarity between\ndependency trees using Polynomial Distance. In addition, we propose an ensemble\nstrategy combining examples selected by both word-level and syntax-level\ncriteria. Experimental results between English and 6 common languages indicate\nthat syntax can effectively enhancing ICL for MT, obtaining the highest COMET\nscores on 11 out of 12 translation directions.\n""]",In-Context Learning for NLP Tasks,Advances in Large Language Models,Large Language Models
170,170,56,170_lingual_multilingual_corpus_translations,"['lingual', 'multilingual', 'corpus', 'translations', 'linguistics', 'corpora', 'linguistic', 'languages', 'language', 'langue']","['languages', 'translation', 'lingual', 'hallucination', 'words', 'hallucinations', 'linguistic', 'diachronic', 'multilingual', 'texts']","['  As Uzbek language is agglutinative, has many morphological features which\nwords formed by combining root and affixes. Affixes play an important role in\nthe morphological analysis of words, by adding additional meanings and\ngrammatical functions to words. Inflectional endings are utilized to express\nvarious morphological features within the language. This feature introduces\nnumerous possibilities for word endings, thereby significantly expanding the\nword vocabulary and exacerbating issues related to data sparsity in statistical\nmodels. This paper present modeling of the morphological analysis of Uzbek\nwords, including stemming, lemmatizing, and the extraction of morphological\ninformation while considering morpho-phonetic exceptions. Main steps of the\nmodel involve developing a complete set of word-ending with assigned\nmorphological information, and additional datasets for morphological analysis.\nThe proposed model was evaluated using a curated test set comprising 5.3K\nwords. Through manual verification of stemming, lemmatizing, and morphological\nfeature corrections carried out by linguistic specialists, it obtained a\nword-level accuracy of over 91%. The developed tool based on the proposed model\nis available as a web-based application and an open-source Python library.\n', '  Massively multilingual machine translation models allow for the translation\nof a large number of languages with a single model, but have limited\nperformance on low- and very-low-resource translation directions. Pivoting via\nhigh-resource languages remains a strong strategy for low-resource directions,\nand in this paper we revisit ways of pivoting through multiple languages.\nPrevious work has used a simple averaging of probability distributions from\nmultiple paths, but we find that this performs worse than using a single pivot,\nand exacerbates the hallucination problem because the same hallucinations can\nbe probable across different paths. We also propose MaxEns, a novel combination\nstrategy that makes the output biased towards the most confident predictions,\nhypothesising that confident predictions are less prone to be hallucinations.\nWe evaluate different strategies on the FLORES benchmark for 20 low-resource\nlanguage directions, demonstrating that MaxEns improves translation quality for\nlow-resource languages while reducing hallucination in translations, compared\nto both direct translation and an averaging approach. On average, multi-pivot\nstrategies still lag behind using English as a single pivot language, raising\nthe question of how to identify the best pivoting strategy for a given\ntranslation direction.\n', '  Semitic morphologically-rich languages (MRLs) are characterized by extreme\nword ambiguity. Because most vowels are omitted in standard texts, many of the\nwords are homographs with multiple possible analyses, each with a different\npronunciation and different morphosyntactic properties. This ambiguity goes\nbeyond word-sense disambiguation (WSD), and may include token segmentation into\nmultiple word units. Previous research on MRLs claimed that standardly trained\npre-trained language models (PLMs) based on word-pieces may not sufficiently\ncapture the internal structure of such tokens in order to distinguish between\nthese analyses. Taking Hebrew as a case study, we investigate the extent to\nwhich Hebrew homographs can be disambiguated and analyzed using PLMs. We\nevaluate all existing models for contextualized Hebrew embeddings on a novel\nHebrew homograph challenge sets that we deliver. Our empirical results\ndemonstrate that contemporary Hebrew contextualized embeddings outperform\nnon-contextualized embeddings; and that they are most effective for\ndisambiguating segmentation and morphosyntactic features, less so regarding\npure word-sense disambiguation. We show that these embeddings are more\neffective when the number of word-piece splits is limited, and they are more\neffective for 2-way and 3-way ambiguities than for 4-way ambiguity. We show\nthat the embeddings are equally effective for homographs of both balanced and\nskewed distributions, whether calculated as masked or unmasked tokens. Finally,\nwe show that these embeddings are as effective for homograph disambiguation\nwith extensive supervised training as with a few-shot setup.\n']",Multilingual Language Processing and Linguistics,Multilingual Natural Language Processing,Natural Language Processing
171,171,56,171_knowledge_relational_reasoning_inference,"['knowledge', 'relational', 'reasoning', 'inference', 'entities', 'logic', 'language', 'entity', 'queries', 'relations']","['reasoning', 'knowledge', 'graphs', 'rule', 'logical', 'path', 'rules', 'graph', 'entities', 'factual']","[""  This survey investigates the synergistic relationship between Large Language\nModels (LLMs) and Knowledge Graphs (KGs), which is crucial for advancing AI's\ncapabilities in understanding, reasoning, and language processing. It aims to\naddress gaps in current research by exploring areas such as KG Question\nAnswering, ontology generation, KG validation, and the enhancement of KG\naccuracy and consistency through LLMs. The paper further examines the roles of\nLLMs in generating descriptive texts and natural language queries for KGs.\nThrough a structured analysis that includes categorizing LLM-KG interactions,\nexamining methodologies, and investigating collaborative uses and potential\nbiases, this study seeks to provide new insights into the combined potential of\nLLMs and KGs. It highlights the importance of their interaction for improving\nAI applications and outlines future research directions.\n"", '  Large language models (LLMs) have demonstrated impressive reasoning abilities\nin complex tasks. However, they lack up-to-date knowledge and experience\nhallucinations during reasoning, which can lead to incorrect reasoning\nprocesses and diminish their performance and trustworthiness. Knowledge graphs\n(KGs), which capture vast amounts of facts in a structured format, offer a\nreliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM\nreasoning methods only treat KGs as factual knowledge bases and overlook the\nimportance of their structural information for reasoning. In this paper, we\npropose a novel method called reasoning on graphs (RoG) that synergizes LLMs\nwith KGs to enable faithful and interpretable reasoning. Specifically, we\npresent a planning-retrieval-reasoning framework, where RoG first generates\nrelation paths grounded by KGs as faithful plans. These plans are then used to\nretrieve valid reasoning paths from the KGs for LLMs to conduct faithful\nreasoning. Furthermore, RoG not only distills knowledge from KGs to improve the\nreasoning ability of LLMs through training but also allows seamless integration\nwith any arbitrary LLMs during inference. Extensive experiments on two\nbenchmark KGQA datasets demonstrate that RoG achieves state-of-the-art\nperformance on KG reasoning tasks and generates faithful and interpretable\nreasoning results.\n', '  Large language models (LLMs), such as ChatGPT and GPT4, are making new waves\nin the field of natural language processing and artificial intelligence, due to\ntheir emergent ability and generalizability. However, LLMs are black-box\nmodels, which often fall short of capturing and accessing factual knowledge. In\ncontrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are\nstructured knowledge models that explicitly store rich factual knowledge. KGs\ncan enhance LLMs by providing external knowledge for inference and\ninterpretability. Meanwhile, KGs are difficult to construct and evolving by\nnature, which challenges the existing methods in KGs to generate new facts and\nrepresent unseen knowledge. Therefore, it is complementary to unify LLMs and\nKGs together and simultaneously leverage their advantages. In this article, we\npresent a forward-looking roadmap for the unification of LLMs and KGs. Our\nroadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,\nwhich incorporate KGs during the pre-training and inference phases of LLMs, or\nfor the purpose of enhancing understanding of the knowledge learned by LLMs; 2)\nLLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,\ncompletion, construction, graph-to-text generation, and question answering; and\n3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a\nmutually beneficial way to enhance both LLMs and KGs for bidirectional\nreasoning driven by both data and knowledge. We review and summarize existing\nefforts within these three frameworks in our roadmap and pinpoint their future\nresearch directions.\n']",Knowledge Graphs and Large Language Models for Reasoning,"Reasoning and Problem-Solving with Logic, Language, and Graphs",Artificial Intelligence and Reasoning Systems
172,172,55,172_safetybench_safety_unsafe_safeguards,"['safetybench', 'safety', 'unsafe', 'safeguards', 'risks', 'vulnerabilities', 'ai', 'risk', 'language', 'testing']","['safety', 'risks', 'defeaters', 'unsafe', 'risk', 'software', 'toxic', 'assurance', 'testing', 'evaluation']","['  Large language models (LLMs) have exhibited great potential in autonomously\ncompleting tasks across real-world applications. Despite this, these LLM agents\nintroduce unexpected safety risks when operating in interactive environments.\nInstead of centering on LLM-generated content safety in most prior studies,\nthis work addresses the imperative need for benchmarking the behavioral safety\nof LLM agents within diverse environments. We introduce R-Judge, a benchmark\ncrafted to evaluate the proficiency of LLMs in judging and identifying safety\nrisks given agent interaction records. R-Judge comprises 162 records of\nmulti-turn agent interaction, encompassing 27 key risk scenarios among 7\napplication categories and 10 risk types. It incorporates human consensus on\nsafety with annotated safety labels and high-quality risk descriptions.\nEvaluation of 9 LLMs on R-Judge shows considerable room for enhancing the risk\nawareness of LLMs: The best-performing model, GPT-4, achieves 72.52% in\ncontrast to the human score of 89.07%, while all other models score less than\nthe random. Moreover, further experiments demonstrate that leveraging risk\ndescriptions as environment feedback achieves substantial performance gains.\nWith case studies, we reveal that correlated to parameter amount, risk\nawareness in open agent scenarios is a multi-dimensional capability involving\nknowledge and reasoning, thus challenging for current LLMs. R-Judge is publicly\navailable at https://github.com/Lordog/R-Judge.\n', ""  With the profound development of large language models(LLMs), their safety\nconcerns have garnered increasing attention. However, there is a scarcity of\nChinese safety benchmarks for LLMs, and the existing safety taxonomies are\ninadequate, lacking comprehensive safety detection capabilities in authentic\nChinese scenarios. In this work, we introduce CHiSafetyBench, a dedicated\nsafety benchmark for evaluating LLMs' capabilities in identifying risky content\nand refusing answering risky questions in Chinese contexts. CHiSafetyBench\nincorporates a dataset that covers a hierarchical Chinese safety taxonomy\nconsisting of 5 risk areas and 31 categories. This dataset comprises two types\nof tasks: multiple-choice questions and question-answering, evaluating LLMs\nfrom the perspectives of risk content identification and the ability to refuse\nanswering risky questions respectively. Utilizing this benchmark, we validate\nthe feasibility of automatic evaluation as a substitute for human evaluation\nand conduct comprehensive automatic safety assessments on mainstream Chinese\nLLMs. Our experiments reveal the varying performance of different models across\nvarious safety domains, indicating that all models possess considerable\npotential for improvement in Chinese safety capabilities. Our dataset is\npublicly available at\nhttps://github.com/UnicomAI/DataSet/tree/main/TestData/Safety.\n"", ""  Large Language Models have gained considerable attention for their\nrevolutionary capabilities. However, there is also growing concern on their\nsafety implications, making a comprehensive safety evaluation for LLMs urgently\nneeded before model deployment. In this work, we propose S-Eval, a new\ncomprehensive, multi-dimensional and open-ended safety evaluation benchmark. At\nthe core of S-Eval is a novel LLM-based automatic test prompt generation and\nselection framework, which trains an expert testing LLM Mt combined with a\nrange of test selection strategies to automatically construct a high-quality\ntest suite for the safety evaluation. The key to the automation of this process\nis a novel expert safety-critique LLM Mc able to quantify the riskiness score\nof an LLM's response, and additionally produce risk tags and explanations.\nBesides, the generation process is also guided by a carefully designed risk\ntaxonomy with four different levels, covering comprehensive and\nmulti-dimensional safety risks of concern. Based on these, we systematically\nconstruct a new and large-scale safety evaluation benchmark for LLMs consisting\nof 220,000 evaluation prompts, including 20,000 base risk prompts (10,000 in\nChinese and 10,000 in English) and 200,000 corresponding attack prompts derived\nfrom 10 popular adversarial instruction attacks against LLMs. Moreover,\nconsidering the rapid evolution of LLMs and accompanied safety threats, S-Eval\ncan be flexibly configured and adapted to include new risks, attacks and\nmodels. S-Eval is extensively evaluated on 20 popular and representative LLMs.\nThe results confirm that S-Eval can better reflect and inform the safety risks\nof LLMs compared to existing benchmarks. We also explore the impacts of\nparameter scales, language environments, and decoding parameters on the\nevaluation, providing a systematic methodology for evaluating the safety of\nLLMs.\n""]",LLM Safety Evaluation Benchmarks,Autonomous Systems and Safety Assessment,Autonomous Systems and Safety Assessment
173,173,55,173_dynamics_cells_multicellular_networks,"['dynamics', 'cells', 'multicellular', 'networks', 'cell', 'neural', 'interactions', 'dynamical', 'cellular', 'motion']","['equivariant', 'cell', 'dynamics', 'collective', 'interacting', 'topological', 'interactions', 'cellular', 'systems', 'physical']","['  Learning to represent and simulate the dynamics of physical systems is a\ncrucial yet challenging task. Existing equivariant Graph Neural Network (GNN)\nbased methods have encapsulated the symmetry of physics, \\emph{e.g.},\ntranslations, rotations, etc, leading to better generalization ability.\nNevertheless, their frame-to-frame formulation of the task overlooks the\nnon-Markov property mainly incurred by unobserved dynamics in the environment.\nIn this paper, we reformulate dynamics simulation as a spatio-temporal\nprediction task, by employing the trajectory in the past period to recover the\nNon-Markovian interactions. We propose Equivariant Spatio-Temporal Attentive\nGraph Networks (ESTAG), an equivariant version of spatio-temporal GNNs, to\nfulfill our purpose. At its core, we design a novel Equivariant Discrete\nFourier Transform (EDFT) to extract periodic patterns from the history frames,\nand then construct an Equivariant Spatial Module (ESM) to accomplish spatial\nmessage passing, and an Equivariant Temporal Module (ETM) with the forward\nattention and equivariant pooling mechanisms to aggregate temporal message. We\nevaluate our model on three real datasets corresponding to the molecular-,\nprotein- and macro-level. Experimental results verify the effectiveness of\nESTAG compared to typical spatio-temporal GNNs and equivariant GNNs.\n', '  Regulation of cell proliferation is a crucial aspect of tissue development\nand homeostasis and plays a major role in morphogenesis, wound healing, and\ntumor invasion. A phenomenon of such regulation is contact inhibition, which\ndescribes the dramatic slowing of proliferation, cell migration and individual\ncell growth when multiple cells are in contact with each other. While many\nphysiological, molecular and genetic factors are known, the mechanism of\ncontact inhibition is still not fully understood. In particular, the relevance\nof cellular signaling due to interfacial contact for contact inhibition is\nstill debated. Cellular automata (CA) have been employed in the past as\nnumerically efficient mathematical models to study the dynamics of cell\nensembles, but they are not suitable to explore the origins of contact\ninhibition as such agent-based models assume fixed cell sizes. We develop a\nminimal, data-driven model to simulate the dynamics of planar cell cultures by\nextending a probabilistic CA to incorporate size changes of individual cells\nduring growth and cell division. We successfully apply this model to previous\nin-vitro experiments on contact inhibition in epithelial tissue: After a\nsystematic calibration of the model parameters to measurements of single-cell\ndynamics, our CA model quantitatively reproduces independent measurements of\nemergent, culture-wide features, like colony size, cell density and collective\ncell migration. In particular, the dynamics of the CA model also exhibit the\ntransition from a low-density confluent regime to a stationary postconfluent\nregime with a rapid decrease in cell size and motion. This implies that the\nvolume exclusion principle, a mechanical constraint which is the only\ninter-cellular interaction incorporated in the model, paired with a\nsize-dependent proliferation rate is sufficient to generate the observed\ncontact inhibition.\n', '  We present both a theoretical and a methodological framework that addresses a\ncritical challenge in applying deep learning to physical systems: the\nreconciliation of non-linear expressiveness with SO(3)-equivariance in\npredictions of SO(3)-equivariant quantities. Inspired by covariant theory in\nphysics, we address this problem by exploring the mathematical relationships\nbetween SO(3)-invariant and SO(3)-equivariant quantities and their\nrepresentations. We first construct theoretical SO(3)-invariant quantities\nderived from the SO(3)-equivariant regression targets, and use these invariant\nquantities as supervisory labels to guide the learning of high-quality\nSO(3)-invariant features. Given that SO(3)-invariance is preserved under\nnon-linear operations, the encoding process for invariant features can\nextensively utilize non-linear mappings, thereby fully capturing the non-linear\npatterns inherent in physical systems. Building on this foundation, we propose\na gradient-based mechanism to induce SO(3)-equivariant encodings of various\ndegrees from the learned SO(3)-invariant features. This mechanism can\nincorporate non-linear expressive capabilities into SO(3)-equivariant\nrepresentations, while theoretically preserving their equivariant properties as\nwe prove. We apply our theory and method to the electronic-structure\nHamiltonian prediction tasks, experimental results on eight benchmark databases\ncovering multiple types of elements and challenging scenarios show dramatic\nbreakthroughs on the state-of-the-art prediction accuracy, with improvements of\nup to 40% in predicting Hamiltonians and up to 76% in predicting downstream\nphysical quantities such as occupied orbital energy. Our approach goes beyond\nhandling physical systems and offers a promising general solution to the\ncritical dilemma between equivariance and non-linear expressiveness for the\ndeep learning paradigm.\n']",Cellular Dynamics and Interactions,Cellular Dynamics and Neural Cellular Automata for Morphogenesis and Pattern Formation,Computational Models of Complex Systems and Processes
174,174,55,174_prompts_prompting_prompt_conversational,"['prompts', 'prompting', 'prompt', 'conversational', 'ai', 'answering', 'tasks', 'questions', 'responses', 'task']","['prompt', 'reasoning', 'scientific', 'queries', 'answer', 'user', 'questions', 'language', 'abilities', 'instructions']","['  Prompt engineering is an essential technique for enhancing the abilities of\nlarge language models (LLMs) by providing explicit and specific instructions.\nIt enables LLMs to excel in various tasks, such as arithmetic reasoning,\nquestion answering, summarization, relation extraction, machine translation,\nand sentiment analysis. Researchers have been actively exploring different\nprompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and\nIn-context learning. However, an unresolved problem arises from the fact that\ncurrent approaches lack a solid mathematical solution for determining optimal\nprompts. To address this issue in prompt engineering, we propose a new and\neffective approach called Prompt Space. Our methodology utilizes text\nembeddings to obtain basis vectors by matrix decomposition, and then constructs\na space for representing all prompts. Prompt Space significantly outperforms\nstate-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably,\nwithout the help of the CoT method and the prompt ""Let\'s think step by step"",\nPrompt Space shows superior performance over the few-shot method. Overall, our\napproach provides a robust and effective mathematical framework for selecting\nsimple and effective prompts. This advancement marks a significant step towards\nimproving prompt engineering for a wide variety of applications in LLMs. Our\ncode is publicly available at\n\\textcolor{blue}{\\url{https://github.com/YouBLEI/Prompt-Space}}\n', '  We present and tackle the problem of Embodied Question Answering (EQA) with\nSituational Queries (S-EQA) in a household environment. Unlike prior EQA work\ntackling simple queries that directly reference target objects and quantifiable\nproperties pertaining them, EQA with situational queries (such as ""Is the\nbathroom clean and dry?"") is more challenging, as the agent needs to figure out\nnot just what the target objects pertaining to the query are, but also requires\na consensus on their states to be answerable. Towards this objective, we first\nintroduce a novel Prompt-Generate-Evaluate (PGE) scheme that wraps around an\nLLM\'s output to create a dataset of unique situational queries, corresponding\nconsensus object information, and predicted answers. PGE maintains uniqueness\namong the generated queries, using multiple forms of semantic similarity. We\nvalidate the generated dataset via a large scale user-study conducted on\nM-Turk, and introduce it as S-EQA, the first dataset tackling EQA with\nsituational queries. Our user study establishes the authenticity of S-EQA with\na high 97.26% of the generated queries being deemed answerable, given the\nconsensus object data. Conversely, we observe a low correlation of 46.2% on the\nLLM-predicted answers to human-evaluated ones; indicating the LLM\'s poor\ncapability in directly answering situational queries, while establishing\nS-EQA\'s usability in providing a human-validated consensus for an indirect\nsolution. We evaluate S-EQA via Visual Question Answering (VQA) on VirtualHome,\nwhich unlike other simulators, contains several objects with modifiable states\nthat also visually appear different upon modification -- enabling us to set a\nquantitative benchmark for S-EQA. To the best of our knowledge, this is the\nfirst work to introduce EQA with situational queries, and also the first to use\na generative approach for query creation.\n', ""  The evolution of Artificial Intelligence (AI) has been significantly\naccelerated by advancements in Large Language Models (LLMs) and Large\nMultimodal Models (LMMs), gradually showcasing potential cognitive reasoning\nabilities in problem-solving and scientific discovery (i.e., AI4Science) once\nexclusive to human intellect. To comprehensively evaluate current models'\nperformance in cognitive reasoning abilities, we introduce OlympicArena, which\nincludes 11,163 bilingual problems across both text-only and interleaved\ntext-image modalities. These challenges encompass a wide range of disciplines\nspanning seven fields and 62 international Olympic competitions, rigorously\nexamined for data leakage. We argue that the challenges in Olympic competition\nproblems are ideal for evaluating AI's cognitive reasoning due to their\ncomplexity and interdisciplinary nature, which are essential for tackling\ncomplex scientific challenges and facilitating discoveries. Beyond evaluating\nperformance across various disciplines using answer-only criteria, we conduct\ndetailed experiments and analyses from multiple perspectives. We delve into the\nmodels' cognitive reasoning abilities, their performance across different\nmodalities, and their outcomes in process-level evaluations, which are vital\nfor tasks requiring complex reasoning with lengthy solutions. Our extensive\nevaluations reveal that even advanced models like GPT-4o only achieve a 39.97%\noverall accuracy, illustrating current AI limitations in complex reasoning and\nmultimodal integration. Through the OlympicArena, we aim to advance AI towards\nsuperintelligence, equipping it to address more complex challenges in science\nand beyond. We also provide a comprehensive set of resources to support AI\nresearch, including a benchmark dataset, an open-source annotation platform, a\ndetailed evaluation tool, and a leaderboard with automatic submission features.\n""]",Prompt Engineering and Conversational AI Tasks,Conversational AI and Language Models,Conversational AI and Human-Computer Interaction
175,175,55,175_sram_memory_fpgas_fpga,"['sram', 'memory', 'fpgas', 'fpga', 'hardware', 'cpus', 'cim', 'gpu', 'gpus', 'cpu']","['memory', 'tensor', 'parallel', 'computing', 'accelerator', 'hardware', 'storage', 'scheduling', 'chip', 'schedules']","['  The demand for efficient machine learning (ML) accelerators is growing\nrapidly, driving the development of novel computing concepts such as resistive\nrandom access memory (RRAM)-based tiled computing-in-memory (CIM)\narchitectures. CIM allows to compute within the memory unit, resulting in\nfaster data processing and reduced power consumption. Efficient compiler\nalgorithms are essential to exploit the potential of tiled CIM architectures.\nWhile conventional ML compilers focus on code generation for CPUs, GPUs, and\nother von Neumann architectures, adaptations are needed to cover CIM\narchitectures. Cross-layer scheduling is a promising approach, as it enhances\nthe utilization of CIM cores, thereby accelerating computations. Although\nsimilar concepts are implicitly used in previous work, there is a lack of clear\nand quantifiable algorithmic definitions for cross-layer scheduling for tiled\nCIM architectures. To close this gap, we present CLSA-CIM, a cross-layer\nscheduling algorithm for tiled CIM architectures. We integrate CLSA-CIM with\nexisting weight-mapping strategies and compare performance against\nstate-of-the-art (SOTA) scheduling algorithms. CLSA-CIM improves the\nutilization by up to 17.9 x , resulting in an overall speedup increase of up to\n29.2 x compared to SOTA.\n', '  In recent years, various computing-in-memory (CIM) processors have been\npresented, showing superior performance over traditional architectures. To\nunleash the potential of various CIM architectures, such as device precision,\ncrossbar size, and crossbar number, it is necessary to develop compilation\ntools that are fully aware of the CIM architectural details and implementation\ndiversity. However, due to the lack of architectural support in current popular\nopen-source compiling stacks, existing CIM designs either manually deploy\nnetworks or build their own compilers, which is time-consuming and\nlabor-intensive. Although some works expose the specific CIM device programming\ninterfaces to compilers, they are often bound to a fixed CIM architecture,\nlacking the flexibility to support the CIM architectures with different\ncomputing granularity. On the other hand, existing compilation works usually\nconsider the scheduling of limited operation types (such as crossbar-bound\nmatrix-vector multiplication). Unlike conventional processors, CIM accelerators\nare featured by their diverse architecture, circuit, and device, which cannot\nbe simply abstracted by a single level if we seek to fully explore the\nadvantages brought by CIM. Therefore, we propose CIM-MLC, a universal\nmulti-level compilation framework for general CIM architectures. We first\nestablish a general hardware abstraction for CIM architectures and computing\nmodes to represent various CIM accelerators. Based on the proposed abstraction,\nCIM-MLC can compile tasks onto a wide range of CIM accelerators having\ndifferent devices, architectures, and programming interfaces. More importantly,\ncompared with existing compilation work, CIM-MLC can explore the mapping and\nscheduling strategies across multiple architectural tiers, which form a\ntractable yet effective design space, to achieve better scheduling and\ninstruction generation results.\n', '  Compute-in-memory (CiM) has emerged as a highly energy efficient solution for\nperforming matrix multiplication during Machine Learning (ML) inference.\nHowever, integrating compute in memory poses key questions, such as 1) What\ntype of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial. 3) Where to integrate CiM: Each memory level has different\nbandwidth and capacity, creating different data reuse opportunities for CiM\nintegration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture evaluation methodology where we\ntailor the dataflow mapping. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur experiments show that CiM integrated memory improves energy efficiency by\nup to 3.4x and throughput by up to 15.6x compared to tensor-core-like baseline\narchitecture, with INT-8 precision under iso-area constraints. We believe the\nproposed work provides insights into what type of CiM to use, and when and\nwhere to optimally integrate it in the cache hierarchy for efficient matrix\nmultiplication.\n']",Compute-in-Memory Architectures and Compilation Techniques,Hardware Design and Acceleration for AI and ML Applications,Artificial Intelligence and Machine Learning Hardware Acceleration and Optimization
176,176,54,176_semantic_parsing_nlp_annotated,"['semantic', 'parsing', 'nlp', 'annotated', 'entailment', 'lexical', 'structured', 'amr', 'corpora', 'entities']","['knowledge', 'language', 'question', 'abstract', 'domain', 'semantic', 'natural', 'entities', 'comprehension', 'adaptation']","['  Combining large language models with logical reasoning enhances their\ncapacity to address problems in a robust and reliable manner. Nevertheless, the\nintricate nature of logical reasoning poses challenges when gathering reliable\ndata from the web to build comprehensive training datasets, subsequently\naffecting performance on downstream tasks. To address this, we introduce a\nnovel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the\noriginal text into an Abstract Meaning Representation (AMR) graph, a structured\nsemantic representation that encapsulates the logical structure of the\nsentence, upon which operations are performed to generate logically modified\nAMR graphs. The modified AMR graphs are subsequently converted back into text\nto create augmented data. Notably, our methodology is architecture-agnostic and\nenhances both generative large language models, such as GPT-3.5 and GPT-4,\nthrough prompt augmentation, and discriminative large language models through\ncontrastive learning with logic-driven data augmentation. Empirical evidence\nunderscores the efficacy of our proposed method with improvement in performance\nacross seven downstream tasks, such as reading comprehension requiring logical\nreasoning, textual entailment, and natural language inference. Furthermore, our\nmethod leads on the ReClor leaderboard at\nhttps://eval.ai/web/challenges/challenge-page/503/leaderboard/1347. The source\ncode and data are publicly available at\nhttps://github.com/Strong-AI-Lab/Logical-Equivalence-driven-AMR-Data-Augmentation-for-Representation-Learning.\n', '  Traditionally, natural language processing (NLP) models often use a rich set\nof features created by linguistic expertise, such as semantic representations.\nHowever, in the era of large language models (LLMs), more and more tasks are\nturned into generic, end-to-end sequence generation problems. In this paper, we\ninvestigate the question: what is the role of semantic representations in the\nera of LLMs? Specifically, we investigate the effect of Abstract Meaning\nRepresentation (AMR) across five diverse NLP tasks. We propose an AMR-driven\nchain-of-thought prompting method, which we call AMRCoT, and find that it\ngenerally hurts performance more than it helps. To investigate what AMR may\nhave to offer on these tasks, we conduct a series of analysis experiments. We\nfind that it is difficult to predict which input examples AMR may help or hurt\non, but errors tend to arise with multi-word expressions, named entities, and\nin the final inference step where the LLM must connect its reasoning over the\nAMR to its prediction. We recommend focusing on these areas for future work in\nsemantic representations for LLMs. Our code:\nhttps://github.com/causalNLP/amr_llm.\n', '  AMR-to-text is one of the key techniques in the NLP community that aims at\ngenerating sentences from the Abstract Meaning Representation (AMR) graphs.\nSince AMR was proposed in 2013, the study on AMR-to-Text has become\nincreasingly prevalent as an essential branch of structured data to text\nbecause of the unique advantages of AMR as a high-level semantic description of\nnatural language. In this paper, we provide a brief survey of AMR-to-Text.\nFirstly, we introduce the current scenario of this technique and point out its\ndifficulties. Secondly, based on the methods used in previous studies, we\nroughly divided them into five categories according to their respective\nmechanisms, i.e., Rules-based, Seq-to-Seq-based, Graph-to-Seq-based,\nTransformer-based, and Pre-trained Language Model (PLM)-based. In particular,\nwe detail the neural network-based method and present the latest progress of\nAMR-to-Text, which refers to AMR reconstruction, Decoder optimization, etc.\nFurthermore, we present the benchmarks and evaluation methods of AMR-to-Text.\nEventually, we provide a summary of current techniques and the outlook for\nfuture research.\n']",Abstract Meaning Representation in NLP,Natural Language Processing (NLP) Techniques and Applications,Natural Language Processing
177,177,53,177_linguistic_grammars_syntactic_semantics,"['linguistic', 'grammars', 'syntactic', 'semantics', 'languages', 'grammar', 'language', 'monadic', 'contextual', 'parsing']","['grammars', 'child', 'categorial', 'language', 'context', 'verb', 'words', 'calculus', 'grammar', 'category']","[""  Word frequency is a strong predictor in most lexical processing tasks. Thus,\nany model of word recognition needs to account for how word frequency effects\narise. The Discriminative Lexicon Model (DLM; Baayen et al., 2018a, 2019)\nmodels lexical processing with linear mappings between words' forms and their\nmeanings. So far, the mappings can either be obtained incrementally via\nerror-driven learning, a computationally expensive process able to capture\nfrequency effects, or in an efficient, but frequency-agnostic solution\nmodelling the theoretical endstate of learning (EL) where all words are learned\noptimally. In this study we show how an efficient, yet frequency-informed\nmapping between form and meaning can be obtained (Frequency-informed learning;\nFIL). We find that FIL well approximates an incremental solution while being\ncomputationally much cheaper. FIL shows a relatively low type- and high\ntoken-accuracy, demonstrating that the model is able to process most word\ntokens encountered by speakers in daily life correctly. We use FIL to model\nreaction times in the Dutch Lexicon Project (Keuleers et al., 2010) and find\nthat FIL predicts well the S-shaped relationship between frequency and the mean\nof reaction times but underestimates the variance of reaction times for low\nfrequency words. FIL is also better able to account for priming effects in an\nauditory lexical decision task in Mandarin Chinese (Lee, 2007), compared to EL.\nFinally, we used ordered data from CHILDES (Brown, 1973; Demuth et al., 2006)\nto compare mappings obtained with FIL and incremental learning. The mappings\nare highly correlated, but with FIL some nuances based on word ordering effects\nare lost. Our results show how frequency effects in a learning model can be\nsimulated efficiently, and raise questions about how to best account for\nlow-frequency words in cognitive models.\n"", ""  TheBench is a tool to study monadic structures in natural language. It is for\nwriting monadic grammars to explore analyses, compare diverse languages through\ntheir categories, and to train models of grammar from form-meaning pairs where\nsyntax is latent variable.\n  Monadic structures are binary combinations of elements that employ semantics\nof composition only. TheBench is essentially old-school categorial grammar to\nsyntacticize the idea, with the implication that although syntax is autonomous\n(recall \\emph{colorless green ideas sleep furiously}), the treasure is in the\nbaggage it carries at every step, viz. semantics, more narrowly,\npredicate-argument structures indicating choice of categorial reference and its\nconsequent placeholders for decision in such structures.\n  There is some new thought in old school. Unlike traditional categorial\ngrammars, application is turned into composition in monadic analysis. Moreover,\nevery correspondence requires specifying two command relations, one on\nsyntactic command and the other on semantic command. A monadic grammar of\nTheBench contains only synthetic elements (called `objects' in category theory\nof mathematics) that are shaped by this analytic invariant, viz. composition.\nBoth ingredients (command relations) of any analytic step must therefore be\nfunctions (`arrows' in category theory). TheBench is one implementation of the\nidea for iterative development of such functions along with grammar of\nsynthetic elements.\n"", '  Neural network language models (LMs) have been shown to successfully capture\ncomplex linguistic knowledge. However, their utility for understanding language\nacquisition is still debated. We contribute to this debate by presenting a case\nstudy where we use LMs as simulated learners to derive novel experimental\nhypotheses to be tested with humans. We apply this paradigm to study\ncross-dative generalization (CDG): productive generalization of novel verbs\nacross dative constructions (she pilked me the ball/she pilked the ball to me)\n-- acquisition of which is known to involve a large space of contextual\nfeatures -- using LMs trained on child-directed speech. We specifically ask:\n""what properties of the training exposure facilitate a novel verb\'s\ngeneralization to the (unmodeled) alternate construction?"" To answer this, we\nsystematically vary the exposure context in which a novel dative verb occurs in\nterms of the properties of the theme and recipient, and then analyze the LMs\'\nusage of the novel verb in the unmodeled dative construction. We find LMs to\nreplicate known patterns of children\'s CDG, as a precondition to exploring\nnovel hypotheses. Subsequent simulations reveal a nuanced role of the features\nof the novel verbs\' exposure context on the LMs\' CDG. We find CDG to be\nfacilitated when the first postverbal argument of the exposure context is\npronominal, definite, short, and conforms to the prototypical animacy\nexpectations of the exposure dative. These patterns are characteristic of\nharmonic alignment in datives, where the argument with features ranking higher\non the discourse prominence scale tends to precede the other. This gives rise\nto a novel hypothesis that CDG is facilitated insofar as the features of the\nexposure context -- in particular, its first postverbal argument -- are\nharmonically aligned. We conclude by proposing future experiments that can test\nthis hypothesis in children.\n']",Linguistic Grammars and Semantics,Natural Language Processing and Linguistics,Natural Language Processing
178,178,53,178_ai_thinking_brainteaser_tasks,"['ai', 'thinking', 'brainteaser', 'tasks', 'subtasks', 'cognitive', 'interactive', 'reasoning', 'planning', 'cognition']","['reasoning', 'thinking', 'lateral', 'agents', 'agent', 'solving', 'knowledge', 'cognitive', 'complex', 'language']","['  To achieve faithful reasoning that aligns with human expectations, large\nlanguage models (LLMs) need to ground their reasoning to real-world knowledge\n(e.g., web facts, math and physical rules). Tools help LLMs access this\nexternal knowledge, but there remains challenges for fine-tuning LLM agents\n(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\ninter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in\nmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\nfirst decode reasoning chains with abstract placeholders, and then call domain\ntools to reify each reasoning chain by filling in specific knowledge. This\nplanning with abstract chains enables LLMs to learn more general reasoning\nstrategies, which are robust to shifts of domain knowledge (e.g., math results)\nrelevant to different reasoning questions. It also allows LLMs to perform\ndecoding and calling of external tools in parallel, which avoids the inference\ndelay caused by waiting for tool responses. In mathematical reasoning and Wiki\nQA domains, we show that our method consistently outperforms previous\nchain-of-thought and tool-augmented baselines on both in-distribution and\nout-of-distribution test sets, with an average ~6% absolute QA accuracy\nimprovement. LLM agents trained with our method also show more efficient tool\nuse, with inference speed being on average ~1.4x faster than baseline\ntool-augmented LLMs.\n', ""  While vertical thinking relies on logical and commonsense reasoning, lateral\nthinking requires systems to defy commonsense associations and overwrite them\nthrough unconventional thinking. Lateral thinking has been shown to be\nchallenging for current models but has received little attention. A recent\nbenchmark, BRAINTEASER, aims to evaluate current models' lateral thinking\nability in a zero-shot setting. In this paper, we split the original benchmark\nto also support fine-tuning setting and present SemEval Task 9:\nBRAIN-TEASER(S), the first task at this competition designed to test the\nsystem's reasoning and lateral thinking ability. As a popular task,\nBRAINTEASER(S)'s two subtasks receive 483 team submissions from 182\nparticipants during the competition. This paper provides a fine-grained system\nanalysis of the competition results, together with a reflection on what this\nmeans for the ability of the systems to reason laterally. We hope that the\nBRAINTEASER(S) subtasks and findings in this paper can stimulate future work on\nlateral thinking and robust reasoning by computational models.\n"", ""  With the continuous evolution and refinement of LLMs, they are endowed with\nimpressive logical reasoning or vertical thinking capabilities. But can they\nthink out of the box? Do they possess proficient lateral thinking abilities?\nFollowing the setup of Lateral Thinking Puzzles, we propose a novel evaluation\nbenchmark, LatEval, which assesses the model's lateral thinking within an\ninteractive framework. In our benchmark, we challenge LLMs with 2 aspects: the\nquality of questions posed by the model and the model's capability to integrate\ninformation for problem-solving. We find that nearly all LLMs struggle with\nemploying lateral thinking during interactions. For example, even the most\nadvanced model, GPT-4, exhibits the advantage to some extent, yet still\nmaintain a noticeable gap when compared to human. This evaluation benchmark\nprovides LLMs with a highly challenging and distinctive task that is crucial to\nan effective AI assistant.\n""]",Reasoning and Lateral Thinking in AI Models,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Cognitive Systems
179,179,53,179_recommender_embeddings_personalized_embedding,"['recommender', 'embeddings', 'personalized', 'embedding', 'recommendation', 'recommendations', 'ranking', 'items', 'modality', 'similarity']","['recommendation', 'item', 'recommender', 'items', 'user', 'recommendations', 'cold', 'personalized', 'preferences', 'collaborative']","['  With the rapid development of online multimedia services, especially in\ne-commerce platforms, there is a pressing need for personalised recommendation\nsystems that can effectively encode the diverse multi-modal content associated\nwith each item. However, we argue that existing multi-modal recommender systems\ntypically use isolated processes for both feature extraction and modality\nmodelling. Such isolated processes can harm the recommendation performance.\nFirstly, an isolated extraction process underestimates the importance of\neffective feature extraction in multi-modal recommendations, potentially\nincorporating non-relevant information, which is harmful to item\nrepresentations. Second, an isolated modality modelling process produces\ndisjointed embeddings for item modalities due to the individual processing of\neach modality, which leads to a suboptimal fusion of user/item representations\nfor effective user preferences prediction. We hypothesise that the use of a\nunified model for addressing both aforementioned isolated processes will enable\nthe consistent extraction and cohesive fusion of joint multi-modal features,\nthereby enhancing the effectiveness of multi-modal recommender systems. In this\npaper, we propose a novel model, called Unified Multi-modal Graph Transformer\n(UGT), which firstly leverages a multi-way transformer to extract aligned\nmulti-modal features from raw data for top-k recommendation. Subsequently, we\nbuild a unified graph neural network in our UGT model to jointly fuse the\nuser/item representations with their corresponding multi-modal features. Using\nthe graph transformer architecture of our UGT model, we show that the UGT model\ncan achieve significant effectiveness gains, especially when jointly optimised\nwith the commonly-used multi-modal recommendation losses.\n', '  Recommendation systems, as widely implemented nowadays on various platforms,\nrecommend relevant items to users based on their preferences. The classical\nmethods which rely on user-item interaction matrices has limitations,\nespecially in scenarios where there is a lack of interaction data for new\nitems. Knowledge graph (KG)-based recommendation systems have emerged as a\npromising solution. However, most KG-based methods adopt node embeddings, which\ndo not provide personalized recommendations for different users and cannot\ngeneralize well to the new items. To address these limitations, we propose\nKnowledge-enhanced User-Centric subgraph Network (KUCNet), a subgraph learning\napproach with graph neural network (GNN) for effective recommendation. KUCNet\nconstructs a U-I subgraph for each user-item pair that captures both the\nhistorical information of user-item interactions and the side information\nprovided in KG. An attention-based GNN is designed to encode the U-I subgraphs\nfor recommendation. Considering efficiency, the pruned user-centric computation\ngraph is further introduced such that multiple U-I subgraphs can be\nsimultaneously computed and that the size can be pruned by Personalized\nPageRank. Our proposed method achieves accurate, efficient, and interpretable\nrecommendations especially for new items. Experimental results demonstrate the\nsuperiority of KUCNet over state-of-the-art KG-based and collaborative\nfiltering (CF)-based methods.\n', '  The explainability of recommendation systems is crucial for enhancing user\ntrust and satisfaction. Leveraging large language models (LLMs) offers new\nopportunities for comprehensive recommendation logic generation. However, in\nexisting related studies, fine-tuning LLM models for recommendation tasks\nincurs high computational costs and alignment issues with existing systems,\nlimiting the application potential of proven proprietary/closed-source LLM\nmodels, such as GPT-4. In this work, our proposed effective strategy LANE\naligns LLMs with online recommendation systems without additional LLMs tuning,\nreducing costs and improving explainability. This innovative approach addresses\nkey challenges in integrating language models with recommendation systems while\nfully utilizing the capabilities of powerful proprietary models. Specifically,\nour strategy operates through several key components: semantic embedding, user\nmulti-preference extraction using zero-shot prompting, semantic alignment, and\nexplainable recommendation generation using Chain of Thought (CoT) prompting.\nBy embedding item titles instead of IDs and utilizing multi-head attention\nmechanisms, our approach aligns the semantic features of user preferences with\nthose of candidate items, ensuring coherent and user-aligned recommendations.\nSufficient experimental results including performance comparison, questionnaire\nvoting, and visualization cases prove that our method can not only ensure\nrecommendation performance, but also provide easy-to-understand and reasonable\nrecommendation logic.\n']",Personalized Recommendation Systems with Multi-modal Embeddings,Recommender Systems and Personalization Techniques,Recommender Systems and Personalization
180,180,53,180_gpu_gpus_fpga_memory,"['gpu', 'gpus', 'fpga', 'memory', 'compilers', 'implementations', 'sram', 'bottleneck', 'hardware', 'dataflow']","['memory', 'hardware', 'inference', 'softmax', 'accelerators', 'speedup', 'parallelism', 'throughput', 'computation', 'latency']","[""  Large language models (LLMs) are increasingly integrated into many online\nservices, yet they remain cost-prohibitive to deploy due to the requirement of\nexpensive GPU instances. Prior work has addressed the high cost of LLM serving\nby improving the inference engine, but less attention has been given to\nselecting the most cost-efficient GPU type(s) for a specific LLM service. There\nis a large and growing landscape of GPU types and, within these options, higher\ncost does not always lead to increased performance. Instead, through a\ncomprehensive investigation, we find that three key LLM service characteristics\n(request size, request rate, SLO) strongly influence GPU cost efficiency, and\ndiffering GPU types are most cost efficient for differing LLM service settings.\nAs a result, the most cost-efficient allocation for a given service is\ntypically a mix of heterogeneous GPU types. Based on this analysis, we\nintroduce M\\'elange, a GPU allocation framework that navigates these diverse\nLLM service characteristics and heterogeneous GPU option space to automatically\nand efficiently derive the minimal-cost GPU allocation for a given LLM service.\nWe formulate the GPU allocation task as a cost-aware bin packing problem where\nGPUs are bins and items are slices of the service workload. Our formulation's\nconstraints account for a service's unique characteristics, allowing M\\'elange\nto be flexible to support diverse service settings and heterogeneity-aware to\nadapt the GPU allocation to a specific service. Compared to using only a single\nGPU type, M\\'elange reduces deployment costs by up to 77% in conversational\nsettings, 33% in document-based settings, and 51% in a mixed setting.\n"", ""  Transformer-based Large Language Models (LLMs) have made a significant impact\non various domains. However, LLMs' efficiency suffers from both heavy\ncomputation and memory overheads. Compression techniques like sparsification\nand quantization are commonly used to mitigate the gap between LLM's\ncomputation/memory overheads and hardware capacity. However, existing GPU and\ntransformer-based accelerators cannot efficiently process compressed LLMs, due\nto the following unresolved challenges: low computational efficiency,\nunderutilized memory bandwidth, and large compilation overheads.\n  This paper proposes FlightLLM, enabling efficient LLMs inference with a\ncomplete mapping flow on FPGAs. In FlightLLM, we highlight an innovative\nsolution that the computation and memory overhead of LLMs can be solved by\nutilizing FPGA-specific resources (e.g., DSP48 and heterogeneous memory\nhierarchy). We propose a configurable sparse DSP chain to support different\nsparsity patterns with high computation efficiency. Second, we propose an\nalways-on-chip decode scheme to boost memory bandwidth with mixed-precision\nsupport. Finally, to make FlightLLM available for real-world LLMs, we propose a\nlength adaptive compilation method to reduce the compilation overhead.\nImplemented on the Xilinx Alveo U280 FPGA, FlightLLM achieves 6.0$\\times$\nhigher energy efficiency and 1.8$\\times$ better cost efficiency against\ncommercial GPUs (e.g., NVIDIA V100S) on modern LLMs (e.g., LLaMA2-7B) using\nvLLM and SmoothQuant under the batch size of one. FlightLLM beats NVIDIA A100\nGPU with 1.2$\\times$ higher throughput using the latest Versal VHK158 FPGA.\n"", '  Recent advancements in large language models (LLMs) boasting billions of\nparameters have generated a significant demand for efficient deployment in\ninference workloads. The majority of existing approaches rely on temporal\narchitectures that reuse hardware units for different network layers and\noperators. However, these methods often encounter challenges in achieving low\nlatency due to considerable memory access overhead. This paper investigates the\nfeasibility and potential of model-specific spatial acceleration for LLM\ninference on FPGAs. Our approach involves the specialization of distinct\nhardware units for specific operators or layers, facilitating direct\ncommunication between them through a dataflow architecture while minimizing\noff-chip memory accesses. We introduce a comprehensive analytical model for\nestimating the performance of a spatial LLM accelerator, taking into account\nthe on-chip compute and memory resources available on an FPGA. Through our\nanalysis, we can determine the scenarios in which FPGA-based spatial\nacceleration can outperform its GPU-based counterpart. To enable more\nproductive implementations of an LLM model on FPGAs, we further provide a\nlibrary of high-level synthesis (HLS) kernels that are composable and reusable.\nThis library will be made available as open-source. To validate the\neffectiveness of both our analytical model and HLS library, we have implemented\nBERT and GPT2 on an AMD Alveo U280 FPGA device. Experimental results\ndemonstrate our approach can achieve up to 13.4x speedup when compared to\nprevious FPGA-based accelerators for the BERT model. For GPT generative\ninference, we attain a 2.2x speedup compared to DFX, an FPGA overlay, in the\nprefill stage, while achieving a 1.9x speedup and a 5.7x improvement in energy\nefficiency compared to the NVIDIA A100 GPU in the decode stage.\n']",Optimizing Large Language Models on GPUs and FPGAs,Optimization and Efficiency of Large Language Models,Large Language Models
181,181,53,181_simplifications_simplification_readability_sentences,"['simplifications', 'simplification', 'readability', 'sentences', 'annotation', 'nlp', 'corpus', 'texts', 'lexical', 'text']","['simplification', 'readability', 'texts', 'text', 'lexical', 'sentence', 'comprehension', 'multilingual', 'languages', 'evaluation']","[""  Text simplification is a common task where the text is adapted to make it\neasier to understand. Similarly, text elaboration can make a passage more\nsophisticated, offering a method to control the complexity of reading\ncomprehension tests. However, text simplification and elaboration tasks are\nlimited to only relatively alter the readability of texts. It is useful to\ndirectly modify the readability of any text to an absolute target readability\nlevel to cater to a diverse audience. Ideally, the readability of\nreadability-controlled generated text should be independent of the source text.\nTherefore, we propose a novel readability-controlled text modification task.\nThe task requires the generation of 8 versions at various target readability\nlevels for each input text. We introduce novel readability-controlled text\nmodification metrics. The baselines for this task use ChatGPT and Llama-2, with\nan extension approach introducing a two-step process (generating paraphrases by\npassing through the language model twice). The zero-shot approaches are able to\npush the readability of the paraphrases in the desired direction but the final\nreadability remains correlated with the original text's readability. We also\nfind greater drops in semantic and lexical similarity between the source and\ntarget texts with greater shifts in the readability.\n"", '  Text simplification aims to make the text easier to understand by applying\nrewriting transformations. There has been very little research on Chinese text\nsimplification for a long time. The lack of generic evaluation data is an\nessential reason for this phenomenon. In this paper, we introduce MCTS, a\nmulti-reference Chinese text simplification dataset. We describe the annotation\nprocess of the dataset and provide a detailed analysis. Furthermore, we\nevaluate the performance of several unsupervised methods and advanced large\nlanguage models. We additionally provide Chinese text simplification parallel\ndata that can be used for training, acquired by utilizing machine translation\nand English text simplification. We hope to build a basic understanding of\nChinese text simplification through the foundational work and provide\nreferences for future research. All of the code and data are released at\nhttps://github.com/blcuicall/mcts/.\n', ""  Sentence simplification, which rewrites a sentence to be easier to read and\nunderstand, is a promising technique to help people with various reading\ndifficulties. With the rise of advanced large language models (LLMs),\nevaluating their performance in sentence simplification has become imperative.\nRecent studies have used both automatic metrics and human evaluations to assess\nthe simplification abilities of LLMs. However, the suitability of existing\nevaluation methodologies for LLMs remains in question. First, the suitability\nof current automatic metrics on LLMs' simplification evaluation is still\nuncertain. Second, current human evaluation approaches in sentence\nsimplification often fall into two extremes: they are either too superficial,\nfailing to offer a clear understanding of the models' performance, or overly\ndetailed, making the annotation process complex and prone to inconsistency,\nwhich in turn affects the evaluation's reliability. To address these problems,\nthis study provides in-depth insights into LLMs' performance while ensuring the\nreliability of the evaluation. We design an error-based human annotation\nframework to assess the GPT-4's simplification capabilities. Results show that\nGPT-4 generally generates fewer erroneous simplification outputs compared to\nthe current state-of-the-art. However, LLMs have their limitations, as seen in\nGPT-4's struggles with lexical paraphrasing. Furthermore, we conduct\nmeta-evaluations on widely used automatic metrics using our human annotations.\nWe find that while these metrics are effective for significant quality\ndifferences, they lack sufficient sensitivity to assess the overall\nhigh-quality simplification by GPT-4.\n""]",Text Simplification and Readability,Natural Language Processing for Text Correction and Simplification,Natural Language Processing
182,182,52,182_shapenet_3d_points_shapes,"['shapenet', '3d', 'points', 'shapes', 'maps', 'shape', 'mesh', 'surfaces', 'lidar', 'point2ssm']","['point', 'shape', 'cloud', 'clouds', 'registration', 'shapes', 'correspondences', 'deformations', 'geometric', 'deformation']","['  Diffusion models have been popular for point cloud generation tasks. Existing\nworks utilize the forward diffusion process to convert the original point\ndistribution into a noise distribution and then learn the reverse diffusion\nprocess to recover the point distribution from the noise distribution. However,\nthe reverse diffusion process can produce samples with non-smooth points on the\nsurface because of the ignorance of the point cloud geometric properties. We\npropose alleviating the problem by incorporating the local smoothness\nconstraint into the diffusion framework for point cloud generation. Experiments\ndemonstrate the proposed model can generate realistic shapes and smoother point\nclouds, outperforming multiple state-of-the-art methods.\n', '  Point cloud registration aligns 3D point clouds using spatial\ntransformations. It is an important task in computer vision, with applications\nin areas such as augmented reality (AR) and medical imaging. This work explores\nthe intersection of two research trends: the integration of AR into\nimage-guided surgery and the use of deep learning for point cloud registration.\nThe main objective is to evaluate the feasibility of applying deep\nlearning-based point cloud registration methods for image-to-patient\nregistration in augmented reality-guided surgery. We created a dataset of point\nclouds from medical imaging and corresponding point clouds captured with a\npopular AR device, the HoloLens 2. We evaluate three well-established deep\nlearning models in registering these data pairs. While we find that some deep\nlearning methods show promise, we show that a conventional registration\npipeline still outperforms them on our challenging dataset.\n', '  Point cloud registration is a crucial technique in 3D computer vision with a\nwide range of applications. However, this task can be challenging, particularly\nin large fields of view with dynamic objects, environmental noise, or other\nperturbations. To address this challenge, we propose a model called PosDiffNet.\nOur approach performs hierarchical registration based on window-level,\npatch-level, and point-level correspondence. We leverage a graph neural partial\ndifferential equation (PDE) based on Beltrami flow to obtain high-dimensional\nfeatures and position embeddings for point clouds. We incorporate position\nembeddings into a Transformer module based on a neural ordinary differential\nequation (ODE) to efficiently represent patches within points. We employ the\nmulti-level correspondence derived from the high feature similarity scores to\nfacilitate alignment between point clouds. Subsequently, we use registration\nmethods such as SVD-based algorithms to predict the transformation using\ncorresponding point pairs. We evaluate PosDiffNet on several 3D point cloud\ndatasets, verifying that it achieves state-of-the-art (SOTA) performance for\npoint cloud registration in large fields of view with perturbations. The\nimplementation code of experiments is available at\nhttps://github.com/AI-IT-AVs/PosDiffNet.\n']",Point Cloud Generation and Registration,Point Cloud Processing and Registration,Computer Vision and 3D Scene Understanding
183,183,52,183_underwater_submerged_cnn_sonar,"['underwater', 'submerged', 'cnn', 'sonar', 'recognition', 'camera', 'lidar', 'detection', 'detecting', 'detector']","['underwater', 'detection', 'object', 'radar', 'enhancement', 'dataset', 'driving', 'safety', 'vehicle', 'anomaly']","['  Synthetic Aperture Radar (SAR) object detection has gained significant\nattention recently due to its irreplaceable all-weather imaging capabilities.\nHowever, this research field suffers from both limited public datasets (mostly\ncomprising <2K images with only mono-category objects) and inaccessible source\ncode. To tackle these challenges, we establish a new benchmark dataset and an\nopen-source method for large-scale SAR object detection. Our dataset,\nSARDet-100K, is a result of intense surveying, collecting, and standardizing 10\nexisting SAR detection datasets, providing a large-scale and diverse dataset\nfor research purposes. To the best of our knowledge, SARDet-100K is the first\nCOCO-level large-scale multi-class SAR object detection dataset ever created.\nWith this high-quality dataset, we conducted comprehensive experiments and\nuncovered a crucial challenge in SAR object detection: the substantial\ndisparities between the pretraining on RGB datasets and finetuning on SAR\ndatasets in terms of both data domain and model structure. To bridge these\ngaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA)\npretraining framework that tackles the problems from the perspective of data\ninput, domain transition, and model migration. The proposed MSFA method\nsignificantly enhances the performance of SAR object detection models while\ndemonstrating exceptional generalizability and flexibility across diverse\nmodels. This work aims to pave the way for further advancements in SAR object\ndetection. The dataset and code is available at\nhttps://github.com/zcablii/SARDet_100K.\n', '  Underwater monocular depth estimation serves as the foundation for tasks such\nas 3D reconstruction of underwater scenes. However, due to the influence of\nlight and medium, the underwater environment undergoes a distinctive imaging\nprocess, which presents challenges in accurately estimating depth from a single\nimage. The existing methods fail to consider the unique characteristics of\nunderwater environments, leading to inadequate estimation results and limited\ngeneralization performance. Furthermore, underwater depth estimation requires\nextracting and fusing both local and global features, which is not fully\nexplored in existing methods. In this paper, an end-to-end learning framework\nfor underwater monocular depth estimation called UMono is presented, which\nincorporates underwater image formation model characteristics into network\narchitecture, and effectively utilize both local and global features of\nunderwater image. Experimental results demonstrate that the proposed method is\neffective for underwater monocular depth estimation and outperforms the\nexisting methods in both quantitative and qualitative analyses.\n', '  Degraded underwater images decrease the accuracy of underwater object\ndetection. However, existing methods for underwater image enhancement mainly\nfocus on improving the indicators in visual aspects, which may not benefit the\ntasks of underwater image detection, and may lead to serious degradation in\nperformance. To alleviate this problem, we proposed a bidirectional-guided\nmethod for underwater object detection, referred to as BG-YOLO. In the proposed\nmethod, network is organized by constructing an enhancement branch and a\ndetection branch in a parallel way. The enhancement branch consists of a\ncascade of an image enhancement subnet and an object detection subnet. And the\ndetection branch only consists of a detection subnet. A feature guided module\nconnects the shallow convolution layer of the two branches. When training the\nenhancement branch, the object detection subnet in the enhancement branch\nguides the image enhancement subnet to be optimized towards the direction that\nis most conducive to the detection task. The shallow feature map of the trained\nenhancement branch will be output to the feature guided module, constraining\nthe optimization of detection branch through consistency loss and prompting\ndetection branch to learn more detailed information of the objects. And hence\nthe detection performance will be refined. During the detection tasks, only\ndetection branch will be reserved so that no additional cost of computation\nwill be introduced. Extensive experiments demonstrate that the proposed method\nshows significant improvement in performance of the detector in severely\ndegraded underwater scenes while maintaining a remarkable detection speed.\n']",Object Detection in Underwater Environments,Object Detection and Signal Processing in Underwater Environments,Signal Processing and Analysis in Complex Environments
184,184,52,184_explanations_explainability_ai_interpretability,"['explanations', 'explainability', 'ai', 'interpretability', 'neural', 'cnn', 'explaining', 'classifiers', 'explainer', 'interpretable']","['concept', 'explanations', 'explainable', 'interpretability', 'explanation', 'interpretable', 'concepts', 'explainability', 'transparency', 'prototypes']","[""  Challenges persist in providing interpretable explanations for neural network\nreasoning in explainable AI (xAI). Existing methods like Integrated Gradients\nproduce noisy maps, and LIME, while intuitive, may deviate from the model's\nreasoning. We introduce a framework that uses hierarchical segmentation\ntechniques for faithful and interpretable explanations of Convolutional Neural\nNetworks (CNNs). Our method constructs model-based hierarchical segmentations\nthat maintain the model's reasoning fidelity and allows both human-centric and\nmodel-centric segmentation. This approach offers multiscale explanations,\naiding bias identification and enhancing understanding of neural network\ndecision-making. Experiments show that our framework, xAiTrees, delivers highly\ninterpretable and faithful model explanations, not only surpassing traditional\nxAI methods but shedding new light on a novel approach to enhancing xAI\ninterpretability. Code at: https://github.com/CarolMazini/reasoning_with_trees .\n"", ""  Concept-based explainable AI is promising as a tool to improve the\nunderstanding of complex models at the premises of a given user, viz.\\ as a\ntool for personalized explainability. An important class of concept-based\nexplainability methods is constructed with empirically defined concepts,\nindirectly defined through a set of positive and negative examples, as in the\nTCAV approach (Kim et al., 2018). While it is appealing to the user to avoid\nformal definitions of concepts and their operationalization, it can be\nchallenging to establish relevant concept datasets. Here, we address this\nchallenge using general knowledge graphs (such as, e.g., Wikidata or WordNet)\nfor comprehensive concept definition and present a workflow for user-driven\ndata collection in both text and image domains. The concepts derived from\nknowledge graphs are defined interactively, providing an opportunity for\npersonalization and ensuring that the concepts reflect the user's intentions.\nWe test the retrieved concept datasets on two concept-based explainability\nmethods, namely concept activation vectors (CAVs) and concept activation\nregions (CARs) (Crabbe and van der Schaar, 2022). We show that CAVs and CARs\nbased on these empirical concept datasets provide robust and accurate\nexplanations. Importantly, we also find good alignment between the models'\nrepresentations of concepts and the structure of knowledge graphs, i.e., human\nrepresentations. This supports our conclusion that knowledge graph-based\nconcepts are relevant for XAI.\n"", '  A major challenge in Explainable AI is in correctly interpreting activations\nof hidden neurons: accurate interpretations would help answer the question of\nwhat a deep learning system internally detects as relevant in the input,\ndemystifying the otherwise black-box nature of deep learning systems. The state\nof the art indicates that hidden node activations can, in some cases, be\ninterpretable in a way that makes sense to humans, but systematic automated\nmethods that would be able to hypothesize and verify interpretations of hidden\nneuron activations are underexplored. This is particularly the case for\napproaches that can both draw explanations from substantial background\nknowledge, and that are based on inherently explainable (symbolic) methods.\n  In this paper, we introduce a novel model-agnostic post-hoc Explainable AI\nmethod demonstrating that it provides meaningful interpretations. Our approach\nis based on using a Wikipedia-derived concept hierarchy with approximately 2\nmillion classes as background knowledge, and utilizes OWL-reasoning-based\nConcept Induction for explanation generation. Additionally, we explore and\ncompare the capabilities of off-the-shelf pre-trained multimodal-based\nexplainable methods.\n  Our results indicate that our approach can automatically attach meaningful\nclass expressions as explanations to individual neurons in the dense layer of a\nConvolutional Neural Network. Evaluation through statistical analysis and\ndegree of concept activation in the hidden layer show that our method provides\na competitive edge in both quantitative and qualitative aspects compared to\nprior work.\n']",Explainable AI for Neural Networks,Explainability and Interpretability in Neural Networks,Explainable Artificial Intelligence
185,185,52,185_epistemological_epistemic_probabilistic_belief,"['epistemological', 'epistemic', 'probabilistic', 'belief', 'uncertainty', 'beliefs', 'information', 'isopignistic', 'hypothesis', 'credibility']","['belief', 'theory', 'revision', 'uncertainty', 'probability', 'decision', 'sets', 'evidence', 'epistemic', 'beliefs']","['  In belief revision, agents typically modify their beliefs when they receive\nsome new piece of information that is in conflict with them. The guiding\nprinciple behind most belief revision frameworks is that of minimalism, which\nadvocates minimal changes to existing beliefs. However, minimalism may not\nnecessarily capture the nuanced ways in which human agents reevaluate and\nmodify their beliefs. In contrast, the explanatory hypothesis indicates that\npeople are inherently driven to seek explanations for inconsistencies, thereby\nstriving for explanatory coherence rather than minimal changes when revising\nbeliefs. Our contribution in this paper is two-fold. Motivated by the\nexplanatory hypothesis, we first present a novel, yet simple belief revision\noperator that, given a belief base and an explanation for an explanandum, it\nrevises the belief bases in a manner that preserves the explanandum and is not\nnecessarily minimal. We call this operator explanation-based belief revision.\nSecond, we conduct two human-subject studies to empirically validate our\napproach and investigate belief revision behavior in real-world scenarios. Our\nfindings support the explanatory hypothesis and provide insights into the\nstrategies people employ when resolving inconsistencies.\n', '  Knowledge Measures (KMs) aim at quantifying the amount of\nknowledge/information that a knowledge base carries. On the other hand, Belief\nChange (BC) is the process of changing beliefs (in our case, in terms of\ncontraction, expansion and revision) taking into account a new piece of\nknowledge, which possibly may be in contradiction with the current belief. We\npropose a new quantitative BC framework that is based on KMs by defining belief\nchange operators that try to minimise, from an information-theoretic point of\nview, the surprise that the changed belief carries. To this end, we introduce\nthe principle of minimal surprise. In particular, our contributions are (i) a\ngeneral information-theoretic approach to KMs for which [1] is a special case;\n(ii) KM-based BC operators that satisfy the so-called AGM postulates; and (iii)\na characterisation of any BC operator that satisfies the AGM postulates as a\nKM-based BC operator, i.e., any BC operator satisfying the AGM postulates can\nbe encoded within our quantitative BC framework. We also introduce quantitative\nmeasures that account for the information loss of contraction, information gain\nof expansion and information change of revision. We also give a succinct look\ninto the problem of iterated revision, which deals with the application of a\nsequence of revision operations in our framework, and also illustrate how one\nmay build from our KM-based contraction operator also one not satisfying the\n(in)famous recovery postulate, by focusing on the so-called severe withdrawal\nmodel as an illustrative example.\n', '  Developing a general information processing model in uncertain environments\nis fundamental for the advancement of explainable artificial intelligence.\nDempster-Shafer theory of evidence is a well-known and effective reasoning\nmethod for representing epistemic uncertainty, which is closely related to\nsubjective probability theory and possibility theory. Although they can be\ntransformed to each other under some particular belief structures, there\nremains a lack of a clear and interpretable transformation process, as well as\na unified approach for information processing. In this paper, we aim to address\nthese issues from the perspectives of isopignistic belief functions and the\nhyper-cautious transferable belief model. Firstly, we propose an isopignistic\ntransformation based on the belief evolution network. This transformation\nallows for the adjustment of the information granule while retaining the\npotential decision outcome. The isopignistic transformation is integrated with\na hyper-cautious transferable belief model to establish a new canonical\ndecomposition. This decomposition offers a reverse path between the possibility\ndistribution and its isopignistic mass functions. The result of the canonical\ndecomposition, called isopignistic function, is an identical information\ncontent distribution to reflect the propensity and relative commitment degree\nof the BPA. Furthermore, this paper introduces a method to reconstruct the\nbasic belief assignment by adjusting the isopignistic function. It explores the\nadvantages of this approach in modeling and handling uncertainty within the\nhyper-cautious transferable belief model. More general, this paper establishes\na theoretical basis for building general models of artificial intelligence\nbased on probability theory, Dempster-Shafer theory, and possibility theory.\n']",Belief Revision and Epistemic Uncertainty,Uncertainty Estimation and Quantification in Machine Learning,Machine Learning Reliability and Uncertainty
186,186,52,186_retrieval_queries_sparql_search,"['retrieval', 'queries', 'sparql', 'search', 'annotations', 'retrievers', 'litsearch', 'semantic', 'retriever', 'query']","['query', 'queries', 'retrieval', 'search', 'retriever', 'duplicate', 'question', 'documents', 'questions', 'retrievers']","['  Information retrieval models that aim to search for the documents relevant to\nthe given query have shown many successes, which have been applied to diverse\ntasks. However, the query provided by the user is oftentimes very short, which\nchallenges the retrievers to correctly fetch relevant documents. To tackle\nthis, existing studies have proposed expanding the query with a couple of\nadditional (user-related) features related to the query. Yet, they may be\nsuboptimal to effectively augment the query, though there is plenty of\ninformation available to augment it in a relational database. Motivated by\nthis, we present a novel retrieval framework called Database-Augmented Query\nrepresentation (DAQu), which augments the original query with various\n(query-related) metadata across multiple tables. In addition, as the number of\nfeatures in the metadata can be very large and there is no order among them, we\nencode them with our graph-based set encoding strategy, which considers\nhierarchies of features in the database without order. We validate DAQu in\ndiverse retrieval scenarios that can incorporate metadata from the relational\ndatabase, demonstrating that ours significantly enhances overall retrieval\nperformance, compared to existing query augmentation methods.\n', ""  CIS is a prominent area in IR which focuses on developing interactive\nknowledge assistants. These systems must adeptly comprehend the user's\ninformation requirements within the conversational context and retrieve the\nrelevant information. To this aim, the existing approaches model the user's\ninformation needs by generating a single query rewrite or a single\nrepresentation of the query in the query space embedding. However, to answer\ncomplex questions, a single query rewrite or representation is often\nineffective. To address this, a system needs to do reasoning over multiple\npassages. In this work, we propose using a generate-then-retrieve approach to\nimprove the passage retrieval performance for complex user queries. In this\napproach, we utilize large language models (LLMs) to (i) generate an initial\nanswer to the user's information need by doing reasoning over the context of\nthe conversation, and (ii) ground this answer to the collection. Based on the\nexperiments, our proposed approach significantly improves the retrieval\nperformance on TREC iKAT 23, TREC CAsT 20 and 22 datasets, under various\nsetups. Also, we show that grounding the LLM's answer requires more than one\nsearchable query, where an average of 3 queries outperforms human rewrites.\n"", ""  Query rewriting is a crucial technique for passage retrieval in open-domain\nconversational question answering (CQA). It decontexualizes conversational\nqueries into self-contained questions suitable for off-the-shelf retrievers.\nExisting methods attempt to incorporate retriever's preference during the\ntraining of rewriting models. However, these approaches typically rely on\nextensive annotations such as in-domain rewrites and/or relevant passage\nlabels, limiting the models' generalization and adaptation capabilities. In\nthis paper, we introduce AdaQR ($\\textbf{Ada}$ptive $\\textbf{Q}$uery\n$\\textbf{R}$ewriting), a framework for training query rewriting models with\nlimited rewrite annotations from seed datasets and completely no passage label.\nOur approach begins by fine-tuning compact large language models using only\n~$10\\%$ of rewrite annotations from the seed dataset training split. The models\nare then utilized to generate rewrite candidates for each query instance. A\nnovel approach is then proposed to assess retriever's preference for these\ncandidates by the probability of answers conditioned on the conversational\nquery by marginalizing the Top-$K$ passages. This serves as the reward for\noptimizing the rewriter further using Direct Preference Optimization (DPO), a\nprocess free of rewrite and retrieval annotations. Experimental results on four\nopen-domain CQA datasets demonstrate that AdaQR not only enhances the in-domain\ncapabilities of the rewriter with limited annotation requirement, but also\nadapts effectively to out-of-domain datasets.\n""]",Information Retrieval and Query Optimization,Optimization and Management of Computing Resources and Information Systems,Optimization and Management of Complex Systems
187,187,52,187_recognition_imagenet_neural_vision,"['recognition', 'imagenet', 'neural', 'vision', 'representations', 'supervised', 'objects', 'features', 'convolutional', 'visual']","['object', 'vision', 'visual', 'perception', 'child', 'images', 'features', 'detection', 'adult', 'detector']","['  High-level visual brain regions contain subareas in which neurons appear to\nrespond more strongly to examples of a particular semantic category, like faces\nor bodies, rather than objects. However, recent work has shown that while this\nfinding holds on average, some out-of-category stimuli also activate neurons in\nthese regions. This may be due to visual features common among the preferred\nclass also being present in other images. Here, we propose a\ndeep-learning-based approach for visualizing these features. For each neuron,\nwe identify relevant visual features driving its selectivity by modelling\nresponses to images based on latent activations of a deep neural network. Given\nan out-of-category image which strongly activates the neuron, our method first\nidentifies a reference image from the preferred category yielding a similar\nfeature activation pattern. We then backpropagate latent activations of both\nimages to the pixel level, while enhancing the identified shared dimensions and\nattenuating non-shared features. The procedure highlights image regions\ncontaining shared features driving responses of the model neuron. We apply the\nalgorithm to novel recordings from body-selective regions in macaque IT cortex\nin order to understand why some images of objects excite these neurons.\nVisualizations reveal object parts which resemble parts of a macaque body,\nshedding light on neural preference of these objects.\n', '  Recent work has shown that object-centric representations can greatly help\nimprove the accuracy of learning dynamics while also bringing interpretability.\nIn this work, we take this idea one step further, ask the following question:\n""can learning disentangled representation further improve the accuracy of\nvisual dynamics prediction in object-centric models?"" While there has been some\nattempt to learn such disentangled representations for the case of static\nimages \\citep{nsb}, to the best of our knowledge, ours is the first work which\ntries to do this in a general setting for video, without making any specific\nassumptions about the kind of attributes that an object might have. The key\nbuilding block of our architecture is the notion of a {\\em block}, where\nseveral blocks together constitute an object. Each block is represented as a\nlinear combination of a given number of learnable concept vectors, which is\niteratively refined during the learning process. The blocks in our model are\ndiscovered in an unsupervised manner, by attending over object masks, in a\nstyle similar to discovery of slots \\citep{slot_attention}, for learning a\ndense object-centric representation. We employ self-attention via transformers\nover the discovered blocks to predict the next state resulting in discovery of\nvisual dynamics. We perform a series of experiments on several benchmark 2-D,\nand 3-D datasets demonstrating that our architecture (1) can discover\nsemantically meaningful blocks (2) help improve accuracy of dynamics prediction\ncompared to SOTA object-centric models (3) perform significantly better in OOD\nsetting where the specific attribute combinations are not seen earlier during\ntraining. Our experiments highlight the importance discovery of disentangled\nrepresentation for visual dynamics prediction.\n', ""  We add one more invariance - state invariance - to the more commonly used\nother invariances for learning object representations for recognition and\nretrieval. By state invariance, we mean robust with respect to changes in the\nstructural form of the object, such as when an umbrella is folded, or when an\nitem of clothing is tossed on the floor. Since humans generally have no\ndifficulty in recognizing objects despite such state changes, we are naturally\nfaced with the question of whether it is possible to devise a neural\narchitecture with similar abilities. To that end, we present a novel dataset,\nObjectsWithStateChange, that captures state and pose variations in the object\nimages recorded from arbitrary viewpoints. We believe that this dataset will\nfacilitate research in fine-grained object recognition and retrieval of objects\nthat are capable of state changes. The goal of such research would be to train\nmodels capable of generating object embeddings that remain invariant to state\nchanges while also staying invariant to transformations induced by changes in\nviewpoint, pose, illumination, etc. To demonstrate the usefulness of the\nObjectsWithStateChange dataset, we also propose a curriculum learning strategy\nthat uses the similarity relationships in the learned embedding space after\neach epoch to guide the training process. The model learns discriminative\nfeatures by comparing visually similar objects within and across different\ncategories, encouraging it to differentiate between objects that may be\nchallenging to distinguish due to changes in their state. We believe that this\nstrategy enhances the model's ability to capture discriminative features for\nfine-grained tasks that may involve objects with state changes, leading to\nperformance improvements on object-level tasks not only on our new dataset, but\nalso on two other challenging multi-view datasets such as ModelNet40 and\nObjectPI.\n""]",Visual Object Representations and Recognition,Computer Vision and Object Recognition,Computer Vision
188,188,51,188_lidar_radar_pointnet_clouds,"['lidar', 'radar', 'pointnet', 'clouds', '3d', 'camera', 'terrain', 'slam', 'depth', 'mapping']","['lidar', 'radar', 'cloth', 'point', 'canopy', 'depth', 'clouds', 'simulation', 'cloud', 'mesh']","['  Depth estimation is critical in autonomous driving for interpreting 3D scenes\naccurately. Recently, radar-camera depth estimation has become of sufficient\ninterest due to the robustness and low-cost properties of radar. Thus, this\npaper introduces a two-stage, end-to-end trainable Confidence-aware Fusion Net\n(CaFNet) for dense depth estimation, combining RGB imagery with sparse and\nnoisy radar point cloud data. The first stage addresses radar-specific\nchallenges, such as ambiguous elevation and noisy measurements, by predicting a\nradar confidence map and a preliminary coarse depth map. A novel approach is\npresented for generating the ground truth for the confidence map, which\ninvolves associating each radar point with its corresponding object to identify\npotential projection surfaces. These maps, together with the initial radar\ninput, are processed by a second encoder. For the final depth estimation, we\ninnovate a confidence-aware gated fusion mechanism to integrate radar and image\nfeatures effectively, thereby enhancing the reliability of the depth map by\nfiltering out radar noise. Our methodology, evaluated on the nuScenes dataset,\ndemonstrates superior performance, improving upon the current leading model by\n3.2% in Mean Absolute Error (MAE) and 2.7% in Root Mean Square Error (RMSE).\nCode: https://github.com/harborsarah/CaFNet\n', ""  The 3D object detection capabilities in urban environments have been\nenormously improved by recent developments in Light Detection and Range (LiDAR)\ntechnology. This paper presents a novel framework that transforms the detection\nand analysis of 3D objects in traffic scenarios by utilizing the power of\nelevated LiDAR sensors. We are presenting our methodology's remarkable capacity\nto collect complex 3D point cloud data, which allows us to accurately and in\ndetail capture the dynamics of urban traffic. Due to the limitation in\nobtaining real-world traffic datasets, we utilize the simulator to generate 3D\npoint cloud for specific scenarios. To support our experimental analysis, we\nfirstly simulate various 3D point cloud traffic-related objects. Then, we use\nthis dataset as a basis for training and evaluating our 3D object detection\nmodels, in identifying and monitoring both vehicles and pedestrians in\nsimulated urban traffic environments. Next, we fine tune the Point\nVoxel-Region-based Convolutional Neural Network (PV-RCNN) architecture, making\nit more suited to handle and understand the massive volumes of point cloud data\ngenerated by our urban traffic simulations. Our results show the effectiveness\nof the proposed solution in accurately detecting objects in traffic scenes and\nhighlight the role of LiDAR in improving urban safety and advancing intelligent\ntransportation systems.\n"", '  The rapid evolution of deep learning and its integration with autonomous\ndriving systems have led to substantial advancements in 3D perception using\nmultimodal sensors. Notably, radar sensors show greater robustness compared to\ncameras and lidar under adverse weather and varying illumination conditions.\nThis study delves into the often-overlooked yet crucial issue of domain shift\nin 4D radar-based object detection, examining how varying environmental\nconditions, such as different weather patterns and road types, impact 3D object\ndetection performance. Our findings highlight distinct domain shifts across\nvarious weather scenarios, revealing unique dataset sensitivities that\nunderscore the critical role of radar point cloud generation. Additionally, we\ndemonstrate that transitioning between different road types, especially from\nhighways to urban settings, introduces notable domain shifts, emphasizing the\nnecessity for diverse data collection across varied road environments. To the\nbest of our knowledge, this is the first comprehensive analysis of domain shift\neffects on 4D radar-based object detection. We believe this empirical study\ncontributes to understanding the complex nature of domain shifts in radar data\nand suggests paths forward for data collection strategy in the face of\nenvironmental variability.\n']",3D Perception in Autonomous Driving with Radar and LiDAR,Sensor Fusion and Perception for Autonomous Driving,Autonomous Systems and Safety Assessment
189,189,51,189_classifier_classification_imagenet_supervised,"['classifier', 'classification', 'imagenet', 'supervised', 'learning', 'classes', 'datasets', 'imbalance', 'dataset', 'imbalanced']","['class', 'tail', 'imbalance', 'classes', 'balanced', 'label', 'contrastive', 'distributions', 'minority', 'feature']","['  Long-tailed (LT) classification is an unavoidable and challenging problem in\nthe real world. Most existing long-tailed classification methods focus only on\nsolving the class-wise imbalance while ignoring the attribute-wise imbalance.\nThe deviation of a classification model is caused by both class-wise and\nattribute-wise imbalance. Due to the fact that attributes are implicit in most\ndatasets and the combination of attributes is complex, attribute-wise imbalance\nis more difficult to handle. For this purpose, we proposed a novel long-tailed\nclassification framework, aiming to build a multi-granularity classification\nmodel by means of invariant feature learning. This method first unsupervisedly\nconstructs Coarse-Grained forest (CLF) to better characterize the distribution\nof attributes within a class. Depending on the distribution of attributes, one\ncan customize suitable sampling strategies to construct different imbalanced\ndatasets. We then introduce multi-center loss (MCL) that aims to gradually\neliminate confusing attributes during feature learning process. The proposed\nframework does not necessarily couple to a specific LT classification model\nstructure and can be integrated with any existing LT method as an independent\ncomponent. Extensive experiments show that our approach achieves\nstate-of-the-art performance on both existing benchmarks ImageNet-GLT and\nMSCOCO-GLT and can improve the performance of existing LT methods. Our codes\nare available on GitHub: \\url{https://github.com/jinyery/cognisance}\n', '  It is not uncommon that real-world data are distributed with a long tail. For\nsuch data, the learning of deep neural networks becomes challenging because it\nis hard to classify tail classes correctly. In the literature, several existing\nmethods have addressed this problem by reducing classifier bias, provided that\nthe features obtained with long-tailed data are representative enough. However,\nwe find that training directly on long-tailed data leads to uneven embedding\nspace. That is, the embedding space of head classes severely compresses that of\ntail classes, which is not conducive to subsequent classifier learning. This\npaper therefore studies the problem of long-tailed visual recognition from the\nperspective of feature level. We introduce feature augmentation to balance the\nembedding distribution. The features of different classes are perturbed with\nvarying amplitudes in Gaussian form. Based on these perturbed features, two\nnovel logit adjustment methods are proposed to improve model performance at a\nmodest computational overhead. Subsequently, the distorted embedding spaces of\nall classes can be calibrated. In such balanced-distributed embedding spaces,\nthe biased classifier can be eliminated by simply retraining the classifier\nwith class-balanced sampling data. Extensive experiments conducted on benchmark\ndatasets demonstrate the superior performance of the proposed method over the\nstate-of-the-art ones. Source code is available at\nhttps://github.com/Keke921/GCLLoss.\n', '  Long-tailed data is a special type of multi-class imbalanced data with a very\nlarge amount of minority/tail classes that have a very significant combined\ninfluence. Long-tailed learning aims to build high-performance models on\ndatasets with long-tailed distributions, which can identify all the classes\nwith high accuracy, in particular the minority/tail classes. It is a\ncutting-edge research direction that has attracted a remarkable amount of\nresearch effort in the past few years. In this paper, we present a\ncomprehensive survey of latest advances in long-tailed visual learning. We\nfirst propose a new taxonomy for long-tailed learning, which consists of eight\ndifferent dimensions, including data balancing, neural architecture, feature\nenrichment, logits adjustment, loss function, bells and whistles, network\noptimization, and post hoc processing techniques. Based on our proposed\ntaxonomy, we present a systematic review of long-tailed learning methods,\ndiscussing their commonalities and alignable differences. We also analyze the\ndifferences between imbalance learning and long-tailed learning approaches.\nFinally, we discuss prospects and future directions in this field.\n']",Long-tailed Classification Methods,Machine Learning Theory and Methods,Machine Learning and Artificial Intelligence
190,190,51,190_matting_mattes_matte_pixels,"['matting', 'mattes', 'matte', 'pixels', 'segmentation', 'pixel', 'videos', 'ximagenet', 'images', 'objectpi']","['matting', 'segmentation', 'video', 'semantic', 'object', 'image', 'instance', 'trimap', 'frames', 'background']","['  Matting with a static background, often referred to as ``Background Matting""\n(BGM), has garnered significant attention within the computer vision community\ndue to its pivotal role in various practical applications like webcasting and\nphoto editing. Nevertheless, achieving highly accurate background matting\nremains a formidable challenge, primarily owing to the limitations inherent in\nconventional RGB images. These limitations manifest in the form of\nsusceptibility to varying lighting conditions and unforeseen shadows.\n  In this paper, we leverage the rich depth information provided by the\nRGB-Depth (RGB-D) cameras to enhance background matting performance in\nreal-time, dubbed DART. Firstly, we adapt the original RGB-based BGM algorithm\nto incorporate depth information. The resulting model\'s output undergoes\nrefinement through Bayesian inference, incorporating a background depth prior.\nThe posterior prediction is then translated into a ""trimap,"" which is\nsubsequently fed into a state-of-the-art matting algorithm to generate more\nprecise alpha mattes. To ensure real-time matting capabilities, a critical\nrequirement for many real-world applications, we distill the backbone of our\nmodel from a larger and more versatile BGM network. Our experiments demonstrate\nthe superior performance of the proposed method. Moreover, thanks to the\ndistillation operation, our method achieves a remarkable processing speed of 33\nframes per second (fps) on a mid-range edge-computing device. This high\nefficiency underscores DART\'s immense potential for deployment in mobile\napplications}\n', '  Human matting is a foundation task in image and video processing, where human\nforeground pixels are extracted from the input. Prior works either improve the\naccuracy by additional guidance or improve the temporal consistency of a single\ninstance across frames. We propose a new framework MaGGIe, Masked Guided\nGradual Human Instance Matting, which predicts alpha mattes progressively for\neach human instances while maintaining the computational cost, precision, and\nconsistency. Our method leverages modern architectures, including transformer\nattention and sparse convolution, to output all instance mattes simultaneously\nwithout exploding memory and latency. Although keeping constant inference costs\nin the multiple-instance scenario, our framework achieves robust and versatile\nperformance on our proposed synthesized benchmarks. With the higher quality\nimage and video matting benchmarks, the novel multi-instance synthesis approach\nfrom publicly available sources is introduced to increase the generalization of\nmodels in real-world scenarios.\n', '  Human instance matting aims to estimate an alpha matte for each human\ninstance in an image, which is extremely challenging and has rarely been\nstudied so far. Despite some efforts to use instance segmentation to generate a\ntrimap for each instance and apply trimap-based matting methods, the resulting\nalpha mattes are often inaccurate due to inaccurate segmentation. In addition,\nthis approach is computationally inefficient due to multiple executions of the\nmatting method. To address these problems, this paper proposes a novel\nEnd-to-End Human Instance Matting (E2E-HIM) framework for simultaneous multiple\ninstance matting in a more efficient manner. Specifically, a general perception\nnetwork first extracts image features and decodes instance contexts into latent\ncodes. Then, a united guidance network exploits spatial attention and semantics\nembedding to generate united semantics guidance, which encodes the locations\nand semantic correspondences of all instances. Finally, an instance matting\nnetwork decodes the image features and united semantics guidance to predict all\ninstance-level alpha mattes. In addition, we construct a large-scale human\ninstance matting dataset (HIM-100K) comprising over 100,000 human images with\ninstance alpha matte labels. Experiments on HIM-100K demonstrate the proposed\nE2E-HIM outperforms the existing methods on human instance matting with 50%\nlower errors and 5X faster speed (6 instances in a 640X640 image). Experiments\non the PPM-100, RWP-636, and P3M datasets demonstrate that E2E-HIM also\nachieves competitive performance on traditional human matting.\n']",Image and Video Matting Techniques,Video and Image Processing Techniques,Image and Video Processing
191,191,51,191_pretraining_supervised_imagenet_trained,"['pretraining', 'supervised', 'imagenet', 'trained', 'learning', 'pretrained', 'learned', 'training', 'classification', 'datasets']","['pre', 'tuning', 'downstream', 'fine', 'generalization', 'transfer', 'collapse', 'magnitude', 'domain', 'training']","['  Foundation models are usually pre-trained on large-scale datasets and then\nadapted to downstream tasks through tuning. However, the large-scale\npre-training datasets, often inaccessible or too expensive to handle, can\ncontain label noise that may adversely affect the generalization of the model\nand pose unexpected risks. This paper stands out as the first work to\ncomprehensively understand and analyze the nature of noise in pre-training\ndatasets and then effectively mitigate its impacts on downstream tasks.\nSpecifically, through extensive experiments of fully-supervised and image-text\ncontrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M\ndatasets, we demonstrate that, while slight noise in pre-training can benefit\nin-domain (ID) performance, where the training and testing data share a similar\ndistribution, it always deteriorates out-of-domain (OOD) performance, where\ntraining and testing distributions are significantly different. These\nobservations are agnostic to scales of pre-training datasets, pre-training\nnoise types, model architectures, pre-training objectives, downstream tuning\nmethods, and downstream applications. We empirically ascertain that the reason\nbehind this is that the pre-training noise shapes the feature space\ndifferently. We then propose a tuning method (NMTune) to affine the feature\nspace to mitigate the malignant effect of noise and improve generalization,\nwhich is applicable in both parameter-efficient and black-box tuning manners.\nWe additionally conduct extensive experiments on popular vision and language\nmodels, including APIs, which are supervised and self-supervised pre-trained on\nrealistic noisy data for evaluation. Our analysis and results demonstrate the\nimportance of this novel and fundamental research direction, which we term as\nNoisy Model Learning.\n', '  Fine-tuning is becoming widely used for leveraging the power of pre-trained\nfoundation models in new downstream tasks. While there are many successes of\nfine-tuning on various tasks, recent studies have observed challenges in the\ngeneralization of fine-tuned models to unseen distributions (i.e.,\nout-of-distribution; OOD). To improve OOD generalization, some previous studies\nidentify the limitations of fine-tuning data and regulate fine-tuning to\npreserve the general representation learned from pre-training data. However,\npotential limitations in the pre-training data and models are often ignored. In\nthis paper, we contend that overly relying on the pre-trained representation\nmay hinder fine-tuning from learning essential representations for downstream\ntasks and thus hurt its OOD generalization. It can be especially catastrophic\nwhen new tasks are from different (sub)domains compared to pre-training data.\nTo address the issues in both pre-training and fine-tuning data, we propose a\nnovel generalizable fine-tuning method LEVI (Layer-wise Ensemble of different\nVIews), where the pre-trained model is adaptively ensembled layer-wise with a\nsmall task-specific model, while preserving its efficiencies. By combining two\ncomplementing models, LEVI effectively suppresses problematic features in both\nthe fine-tuning data and pre-trained model and preserves useful features for\nnew tasks. Broad experiments with large language and vision models show that\nLEVI greatly improves fine-tuning generalization via emphasizing different\nviews from fine-tuning data and pre-trained features.\n', '  Pre-training on large-scale datasets and then fine-tuning on downstream tasks\nhave become a standard practice in deep learning. However, pre-training data\noften contain label noise that may adversely affect the generalization of the\nmodel. This paper aims to understand the nature of noise in pre-training\ndatasets and to mitigate its impact on downstream tasks. More specifically,\nthrough extensive experiments of supervised pre-training models on synthetic\nnoisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise\nin pre-training can benefit in-domain (ID) transfer performance, where the\ntraining and testing data share the same distribution, it always deteriorates\nout-of-domain (OOD) performance, where training and testing data distribution\nare different. We empirically verify that the reason behind is noise in\npre-training shapes the feature space differently. We then propose a\nlight-weight black-box tuning method (NMTune) to affine the feature space to\nmitigate the malignant effect of noise and improve generalization on both ID\nand OOD tasks, considering one may not be able to fully fine-tune or even\naccess the pre-trained models. We conduct practical experiments on popular\nvision and language models that are pre-trained on noisy data for evaluation of\nour approach. Our analysis and results show the importance of this interesting\nand novel research direction, which we term Noisy Model Learning.\n']",Noisy Pre-training in Deep Learning,Deep Learning Optimization Techniques,Deep Learning Optimization and Training
192,192,51,192_supervised_classification_learning_recognition,"['supervised', 'classification', 'learning', 'recognition', 'imagenet', 'classes', 'trained', 'pretraining', 'learned', 'training']","['classes', 'pretraining', 'class', 'vision', 'backbone', 'foundation', 'video', 'incremental', 'pre', 'novel']","['  This paper revisits the standard pretrain-then-finetune paradigm used in\ncomputer vision for visual recognition tasks. Typically, state-of-the-art\nfoundation models are pretrained using large scale (weakly) supervised datasets\nwith billions of images. We introduce an additional pre-pretraining stage that\nis simple and uses the self-supervised MAE technique to initialize the model.\nWhile MAE has only been shown to scale with the size of models, we find that it\nscales with the size of the training dataset as well. Thus, our MAE-based\npre-pretraining scales with both model and data size making it applicable for\ntraining foundation models. Pre-pretraining consistently improves both the\nmodel convergence and the downstream transfer performance across a range of\nmodel scales (millions to billions of parameters), and dataset sizes (millions\nto billions of images). We measure the effectiveness of pre-pretraining on 10\ndifferent visual recognition tasks spanning image classification, video\nrecognition, object detection, low-shot classification and zero-shot\nrecognition. Our largest model achieves new state-of-the-art results on\niNaturalist-18 (91.7%), ImageNet-ReaL (91.1%), 1-shot ImageNet-1k (63.6%), and\nzero-shot transfer on Food-101 (96.2%). Our study reveals that model\ninitialization plays a significant role, even for web-scale pretraining with\nbillions of images, and our models are available publicly.\n', '  Parameter-efficient fine-tuning (PEFT) techniques such as low-rank adaptation\n(LoRA) can effectively adapt large pre-trained foundation models to downstream\ntasks using only a small fraction (0.1%-10%) of the original trainable weights.\nAn under-explored question of PEFT is in extending the pre-training phase\nwithout supervised labels; that is, can we adapt a pre-trained foundation model\nto a new domain via efficient self-supervised pre-training on this new domain?\nIn this work, we introduce ExPLoRA, a highly effective technique to improve\ntransfer learning of pre-trained vision transformers (ViTs) under domain\nshifts. Initializing a ViT with pre-trained weights on large, natural-image\ndatasets such as from DinoV2 or MAE, ExPLoRA continues the unsupervised\npre-training objective on a new domain. In this extended pre-training phase,\nExPLoRA only unfreezes 1-2 pre-trained ViT blocks and all normalization layers,\nand then tunes all other layers with LoRA. Finally, we fine-tune the resulting\nmodel only with LoRA on this new domain for supervised learning. Our\nexperiments demonstrate state-of-the-art results on satellite imagery, even\noutperforming fully pre-training and fine-tuning ViTs. Using the DinoV2\ntraining objective, we demonstrate up to 7% improvement in linear probing top-1\naccuracy on downstream tasks while using <10% of the number of parameters that\nare used in prior fully-tuned state-of-the art approaches. Our ablation studies\nconfirm the efficacy of our approach over other baselines, including PEFT and\nsimply unfreezing more transformer blocks.\n', '  The problem of Novel Class Discovery (NCD) consists in extracting knowledge\nfrom a labeled set of known classes to accurately partition an unlabeled set of\nnovel classes. While NCD has recently received a lot of attention from the\ncommunity, it is often solved on computer vision problems and under unrealistic\nconditions. In particular, the number of novel classes is usually assumed to be\nknown in advance, and their labels are sometimes used to tune hyperparameters.\nMethods that rely on these assumptions are not applicable in real-world\nscenarios. In this work, we focus on solving NCD in tabular data when no prior\nknowledge of the novel classes is available. To this end, we propose to tune\nthe hyperparameters of NCD methods by adapting the $k$-fold cross-validation\nprocess and hiding some of the known classes in each fold. Since we have found\nthat methods with too many hyperparameters are likely to overfit these hidden\nclasses, we define a simple deep NCD model. This method is composed of only the\nessential elements necessary for the NCD problem and performs impressively well\nunder realistic conditions. Furthermore, we find that the latent space of this\nmethod can be used to reliably estimate the number of novel classes.\nAdditionally, we adapt two unsupervised clustering algorithms ($k$-means and\nSpectral Clustering) to leverage the knowledge of the known classes. Extensive\nexperiments are conducted on 7 tabular datasets and demonstrate the\neffectiveness of the proposed method and hyperparameter tuning process, and\nshow that the NCD problem can be solved without relying on knowledge from the\nnovel classes.\n']",Visual Recognition and Classification via Pre-training,Computer Vision and Object Recognition,Computer Vision
193,193,51,193_fidelity_surrogate_modeling_prediction,"['fidelity', 'surrogate', 'modeling', 'prediction', 'modelling', 'surrogates', 'regression', 'neural', 'model', 'predictive']","['fidelity', 'surrogate', 'uncertainty', 'multifidelity', 'sources', 'active', 'fermentation', 'quantification', 'regression', 'engineering']","[""  Multifidelity surrogate modelling combines data of varying accuracy and cost\nfrom different sources. It strategically uses low-fidelity models for rapid\nevaluations, saving computational resources, and high-fidelity models for\ndetailed refinement. It improves decision-making by addressing uncertainties\nand surpassing the limits of single-fidelity models, which either oversimplify\nor are computationally intensive. Blending high-fidelity data for detailed\nresponses with frequent low-fidelity data for quick approximations facilitates\ndesign optimisation in various domains.\n  Despite progress in interpolation, regression, enhanced sampling, error\nestimation, variable fidelity, and data fusion techniques, challenges persist\nin selecting fidelity levels and developing efficient data fusion methods. This\nstudy proposes a new fusion approach to construct multi-fidelity surrogate\nmodels by constructing gradient-only surrogates that use only gradients to\nconstruct regression surfaces. Results are demonstrated on foundational example\nproblems that isolate and illustrate the fusion approach's efficacy, avoiding\nthe need for complex examples that obfuscate the main concept.\n"", '  Multi-fidelity machine learning methods address the accuracy-efficiency\ntrade-off by integrating scarce, resource-intensive high-fidelity data with\nabundant but less accurate low-fidelity data. We propose a practical\nmulti-fidelity strategy for problems spanning low- and high-dimensional\ndomains, integrating a non-probabilistic regression model for the low-fidelity\nwith a Bayesian model for the high-fidelity. The models are trained in a\nstaggered scheme, where the low-fidelity model is transfer-learned to the\nhigh-fidelity data and a Bayesian model is trained for the residual. This\nthree-model strategy -- deterministic low-fidelity, transfer learning, and\nBayesian residual -- leads to a prediction that includes uncertainty\nquantification both for noisy and noiseless multi-fidelity data. The strategy\nis general and unifies the topic, highlighting the expressivity trade-off\nbetween the transfer-learning and Bayesian models (a complex transfer-learning\nmodel leads to a simpler Bayesian model, and vice versa). We propose modeling\nchoices for two scenarios, and argue in favor of using a linear\ntransfer-learning model that fuses 1) kernel ridge regression for low-fidelity\nwith Gaussian processes for high-fidelity; or 2) deep neural network for\nlow-fidelity with a Bayesian neural network for high-fidelity. We demonstrate\nthe effectiveness and efficiency of the proposed strategies and contrast them\nwith the state-of-the-art based on various numerical examples. The simplicity\nof these formulations makes them practical for a broad scope of future\nengineering applications.\n', '  In this work, we consider the general problem of constructing a neural\nnetwork surrogate model using multi-fidelity information. Motivated by rigorous\nerror and complexity estimates for ReLU neural networks, given an inexpensive\nlow-fidelity and an expensive high-fidelity computational model, we present a\nresidual multi-fidelity computational framework that formulates the correlation\nbetween models as a residual function, a possibly non-linear mapping between 1)\nthe shared input space of the models together with the low-fidelity model\noutput and 2) the discrepancy between the two model outputs. To accomplish\nthis, we train two neural networks to work in concert. The first network learns\nthe residual function on a small set of high-fidelity and low-fidelity data.\nOnce trained, this network is used to generate additional synthetic\nhigh-fidelity data, which is used in the training of a second network. This\nsecond network, once trained, acts as our surrogate for the high-fidelity\nquantity of interest. We present three numerical examples to demonstrate the\npower of the proposed framework. In particular, we show that dramatic savings\nin computational cost may be achieved when the output predictions are desired\nto be accurate within small tolerances.\n']",Multi-Fidelity Surrogate Modeling,Surrogate Modeling for Machine Learning and Prediction,Machine Learning and Optimization
194,194,50,194_annotations_annotation_textual_retrieval,"['annotations', 'annotation', 'textual', 'retrieval', 'summarization', 'summaries', 'factuality', 'narratives', 'reading', 'narrative']","['document', 'documents', 'event', 'summarization', 'events', 'narrative', 'claim', 'summaries', 'narratives', 'retrieval']","['  Event Factuality Detection (EFD) task determines the factuality of textual\nevents, i.e., classifying whether an event is a fact, possibility, or\nimpossibility, which is essential for faithfully understanding and utilizing\nevent knowledge. However, due to the lack of high-quality large-scale data,\nevent factuality detection is under-explored in event understanding research,\nwhich limits the development of EFD community. To address these issues and\nprovide faithful event understanding, we introduce MAVEN-Fact, a large-scale\nand high-quality EFD dataset based on the MAVEN dataset. MAVEN-Fact includes\nfactuality annotations of 112,276 events, making it the largest EFD dataset.\nExtensive experiments demonstrate that MAVEN-Fact is challenging for both\nconventional fine-tuned models and large language models (LLMs). Thanks to the\ncomprehensive annotations of event arguments and relations in MAVEN, MAVEN-Fact\nalso supports some further analyses and we find that adopting event arguments\nand relations helps in event factuality detection for fine-tuned models but\ndoes not benefit LLMs. Furthermore, we preliminarily study an application case\nof event factuality detection and find it helps in mitigating event-related\nhallucination in LLMs. Our dataset and codes can be obtained from\n\\url{https://github.com/lcy2723/MAVEN-FACT}\n', '  While long-context large language models (LLMs) can technically summarize\nbook-length documents (>100K tokens), the length and complexity of the\ndocuments have so far prohibited evaluations of input-dependent aspects like\nfaithfulness. In this paper, we conduct the first large-scale human evaluation\nof faithfulness and content selection on LLM-generated summaries of fictional\nbooks. Our study mitigates the issue of data contamination by focusing on\nsummaries of books published in 2023 or 2024, and we hire annotators who have\nfully read each book prior to the annotation task to minimize cost and\ncognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims\nmade in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which\nallows us to rank LLM summarizers based on faithfulness: Claude-3-Opus\nsignificantly outperforms all closed-source LLMs, while the open-source Mixtral\nis on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most\nunfaithful claims relate to events and character states, and they generally\nrequire indirect reasoning over the narrative to invalidate. While LLM-based\nauto-raters have proven reliable for factuality and coherence in other\nsettings, we implement several LLM raters of faithfulness and find that none\ncorrelates strongly with human annotations, especially with regard to detecting\nunfaithful claims. Our experiments suggest that detecting unfaithful claims is\nan important future direction not only for summarization evaluation but also as\na testbed for long-context understanding. Finally, we move beyond faithfulness\nby exploring content selection errors in book-length summarization: we develop\na typology of omission errors related to crucial narrative elements and also\nidentify a systematic over-emphasis on events occurring towards the end of the\nbook.\n', ""  In this paper, we introduce the VerifAI project, a pioneering open-source\nscientific question-answering system, designed to provide answers that are not\nonly referenced but also automatically vetted and verifiable. The components of\nthe system are (1) an Information Retrieval system combining semantic and\nlexical search techniques over scientific papers (PubMed), (2) a\nRetrieval-Augmented Generation (RAG) module using fine-tuned generative model\n(Mistral 7B) and retrieved articles to generate claims with references to the\narticles from which it was derived, and (3) a Verification engine, based on a\nfine-tuned DeBERTa and XLM-RoBERTa models on Natural Language Inference task\nusing SciFACT dataset. The verification engine cross-checks the generated claim\nand the article from which the claim was derived, verifying whether there may\nhave been any hallucinations in generating the claim. By leveraging the\nInformation Retrieval and RAG modules, Verif.ai excels in generating factual\ninformation from a vast array of scientific sources. At the same time, the\nVerification engine rigorously double-checks this output, ensuring its accuracy\nand reliability. This dual-stage process plays a crucial role in acquiring and\nconfirming factual information, significantly enhancing the information\nlandscape. Our methodology could significantly enhance scientists'\nproductivity, concurrently fostering trust in applying generative language\nmodels within scientific domains, where hallucinations and misinformation are\nunacceptable.\n""]",Event Factuality Detection and Summarization,Automated Information Verification and Analysis,Information Verification and Validation
195,195,50,195_compression_decoding_compressed_encoder,"['compression', 'decoding', 'compressed', 'encoder', 'information', 'bottleneck', 'decoder', 'entropy', 'generalization', 'encoding']","['entropy', 'mutual', 'information', 'distortion', 'compression', 'channel', 'theoretic', 'divergence', 'theory', 'bottleneck']","['  The distortion-rate function of output-constrained lossy source coding with\nlimited common randomness is analyzed for the special case of squared error\ndistortion measure. An explicit expression is obtained when both source and\nreconstruction distributions are Gaussian. This further leads to a partial\ncharacterization of the information-theoretic limit of quadratic Gaussian\nrate-distortion-perception coding with the perception measure given by\nKullback-Leibler divergence or squared quadratic Wasserstein distance.\n', ""  In a feedforward network, Transfer Entropy (TE) can be used to measure the\ninfluence that one layer has on another by quantifying the information transfer\nbetween them during training. According to the Information Bottleneck\nprinciple, a neural model's internal representation should compress the input\ndata as much as possible while still retaining sufficient information about the\noutput. Information Plane analysis is a visualization technique used to\nunderstand the trade-off between compression and information preservation in\nthe context of the Information Bottleneck method by plotting the amount of\ninformation in the input data against the compressed representation. The claim\nthat there is a causal link between information-theoretic compression and\ngeneralization, measured by mutual information, is plausible, but results from\ndifferent studies are conflicting. In contrast to mutual information, TE can\ncapture temporal relationships between variables. To explore such links, in our\nnovel approach we use TE to quantify information transfer between neural layers\nand perform Information Plane analysis. We obtained encouraging experimental\nresults, opening the possibility for further investigations.\n"", '  The ability of machine learning (ML) algorithms to generalize well to unseen\ndata has been studied through the lens of information theory, by bounding the\ngeneralization error with the input-output mutual information (MI), i.e., the\nMI between the training data and the learned hypothesis. Yet, these bounds have\nlimited practicality for modern ML applications (e.g., deep learning), due to\nthe difficulty of evaluating MI in high dimensions. Motivated by recent\nfindings on the compressibility of neural networks, we consider algorithms that\noperate by slicing the parameter space, i.e., trained on random\nlower-dimensional subspaces. We introduce new, tighter information-theoretic\ngeneralization bounds tailored for such algorithms, demonstrating that slicing\nimproves generalization. Our bounds offer significant computational and\nstatistical advantages over standard MI bounds, as they rely on scalable\nalternative measures of dependence, i.e., disintegrated mutual information and\n$k$-sliced mutual information. Then, we extend our analysis to algorithms whose\nparameters do not need to exactly lie on random subspaces, by leveraging\nrate-distortion theory. This strategy yields generalization bounds that\nincorporate a distortion term measuring model compressibility under slicing,\nthereby tightening existing bounds without compromising performance or\nrequiring model compression. Building on this, we propose a regularization\nscheme enabling practitioners to control generalization through\ncompressibility. Finally, we empirically validate our results and achieve the\ncomputation of non-vacuous information-theoretic generalization bounds for\nneural networks, a task that was previously out of reach.\n']",Information-Theoretic Compression and Generalization,Image and Video Compression Techniques,Image and Video Processing
196,196,50,196_sgd_asynch_distributed_asynchronous,"['sgd', 'asynch', 'distributed', 'asynchronous', 'synchronization', 'stragglers', 'synchronous', 'sgbdt', 'parallelism', 'gradient']","['asynchronous', 'decentralized', 'workers', 'gradient', 'convergence', 'communication', 'stochastic', 'synchronization', 'momentum', 'synchronous']","[""  Distributed stochastic gradient descent (SGD) has attracted considerable\nrecent attention due to its potential for scaling computational resources,\nreducing training time, and helping protect user privacy in machine learning.\nHowever, the staggers and limited bandwidth may induce random\ncomputational/communication delays, thereby severely hindering the learning\nprocess. Therefore, how to accelerate asynchronous SGD by efficiently\nscheduling multiple workers is an important issue. In this paper, a unified\nframework is presented to analyze and optimize the convergence of asynchronous\nSGD based on stochastic delay differential equations (SDDEs) and the Poisson\napproximation of aggregated gradient arrivals. In particular, we present the\nrun time and staleness of distributed SGD without a memorylessness assumption\non the computation times. Given the learning rate, we reveal the relevant\nSDDE's damping coefficient and its delay statistics, as functions of the number\nof activated clients, staleness threshold, the eigenvalues of the Hessian\nmatrix of the objective function, and the overall computational/communication\ndelay. The formulated SDDE allows us to present both the distributed SGD's\nconvergence condition and speed by calculating its characteristic roots,\nthereby optimizing the scheduling policies for asynchronous/event-triggered\nSGD. It is interestingly shown that increasing the number of activated workers\ndoes not necessarily accelerate distributed SGD due to staleness. Moreover, a\nsmall degree of staleness does not necessarily slow down the convergence, while\na large degree of staleness will result in the divergence of distributed SGD.\nNumerical results demonstrate the potential of our SDDE framework, even in\ncomplex learning tasks with non-convex objective functions.\n"", ""  Local stochastic gradient descent (Local-SGD), also referred to as federated\naveraging, is an approach to distributed optimization where each device\nperforms more than one SGD update per communication. This work presents an\nempirical study of {\\it asynchronous} Local-SGD for training language models;\nthat is, each worker updates the global parameters as soon as it has finished\nits SGD steps. We conduct a comprehensive investigation by examining how worker\nhardware heterogeneity, model size, number of workers, and optimizer could\nimpact the learning performance. We find that with naive implementations,\nasynchronous Local-SGD takes more iterations to converge than its synchronous\ncounterpart despite updating the (global) model parameters more frequently. We\nidentify momentum acceleration on the global parameters when worker gradients\nare stale as a key challenge. We propose a novel method that utilizes a delayed\nNesterov momentum update and adjusts the workers' local training steps based on\ntheir computation speed. This approach, evaluated with models up to 150M\nparameters on the C4 dataset, matches the performance of synchronous Local-SGD\nin terms of perplexity per update step, and significantly surpasses it in terms\nof wall clock time.\n"", ""  We consider the distributed learning problem with data dispersed across\nmultiple workers under the orchestration of a central server. Asynchronous\nStochastic Gradient Descent (SGD) has been widely explored in such a setting to\nreduce the synchronization overhead associated with parallelization. However,\nthe performance of asynchronous SGD algorithms often depends on a bounded\ndissimilarity condition among the workers' local data, a condition that can\ndrastically affect their efficiency when the workers' data are highly\nheterogeneous. To overcome this limitation, we introduce the\n\\textit{dual-delayed asynchronous SGD (DuDe-ASGD)} algorithm designed to\nneutralize the adverse effects of data heterogeneity. DuDe-ASGD makes full use\nof stale stochastic gradients from all workers during asynchronous training,\nleading to two distinct time lags in the model parameters and data samples\nutilized in the server's iterations. Furthermore, by adopting an incremental\naggregation strategy, DuDe-ASGD maintains a per-iteration computational cost\nthat is on par with traditional asynchronous SGD algorithms. Our analysis\ndemonstrates that DuDe-ASGD achieves a near-minimax-optimal convergence rate\nfor smooth nonconvex problems, even when the data across workers are extremely\nheterogeneous. Numerical experiments indicate that DuDe-ASGD compares favorably\nwith existing asynchronous and synchronous SGD-based algorithms.\n""]",Asynchronous Distributed Stochastic Gradient Descent,Optimization Methods for Distributed Deep Learning,Deep Learning Optimization and Training
197,197,50,197_encodenet_autoencoders_representations_embeddings,"['encodenet', 'autoencoders', 'representations', 'embeddings', 'encoders', 'autoencoder', 'imagenet', 'neural', 'encoder', 'supervised']","['latent', 'equivariant', 'equivariance', 'representations', 'coding', 'transformations', 'invariant', 'predictive', 'spaces', 'space']","['  Recently, Neural Fields have emerged as a powerful modelling paradigm to\nrepresent continuous signals. In a conditional neural field, a field is\nrepresented by a latent variable that conditions the NeF, whose parametrisation\nis otherwise shared over an entire dataset. We propose Equivariant Neural\nFields based on cross attention transformers, in which NeFs are conditioned on\na geometric conditioning variable, a latent point cloud, that enables an\nequivariant decoding from latent to field. Our equivariant approach induces a\nsteerability property by which both field and latent are grounded in geometry\nand amenable to transformation laws if the field transforms, the latent\nrepresents transforms accordingly and vice versa. Crucially, the equivariance\nrelation ensures that the latent is capable of (1) representing geometric\npatterns faitfhully, allowing for geometric reasoning in latent space, (2)\nweightsharing over spatially similar patterns, allowing for efficient learning\nof datasets of fields. These main properties are validated using classification\nexperiments and a verification of the capability of fitting entire datasets, in\ncomparison to other non-equivariant NeF approaches. We further validate the\npotential of ENFs by demonstrate unique local field editing properties.\n', ""  At the core of self-supervised learning for vision is the idea of learning\ninvariant or equivariant representations with respect to a set of data\ntransformations. This approach, however, introduces strong inductive biases,\nwhich can render the representations fragile in downstream tasks that do not\nconform to these symmetries. In this work, drawing insights from world models,\nwe propose to instead learn a general representation that can adapt to be\ninvariant or equivariant to different transformations by paying attention to\ncontext -- a memory module that tracks task-specific states, actions, and\nfuture states. Here, the action is the transformation, while the current and\nfuture states respectively represent the input's representation before and\nafter the transformation. Our proposed algorithm, Contextual Self-Supervised\nLearning (ContextSSL), learns equivariance to all transformations (as opposed\nto invariance). In this way, the model can learn to encode all relevant\nfeatures as general representations while having the versatility to tail down\nto task-wise symmetries when given a few examples as the context. Empirically,\nwe demonstrate significant performance gains over existing methods on\nequivariance-related tasks, supported by both qualitative and quantitative\nevaluations.\n"", '  Latent representations are used extensively for downstream tasks, such as\nvisualization, interpolation or feature extraction of deep learning models.\nInvariant and equivariant neural networks are powerful and well-established\nmodels for enforcing inductive biases. In this paper, we demonstrate that the\ninductive bias imposed on the by an equivariant model must also be taken into\naccount when using latent representations. We show how not accounting for the\ninductive biases leads to decreased performance on downstream tasks, and vice\nversa, how accounting for inductive biases can be done effectively by using an\ninvariant projection of the latent representations. We propose principles for\nhow to choose such a projection, and show the impact of using these principles\nin two common examples: First, we study a permutation equivariant variational\nauto-encoder trained for molecule graph generation; here we show that invariant\nprojections can be designed that incur no loss of information in the resulting\ninvariant representation. Next, we study a rotation-equivariant representation\nused for image classification. Here, we illustrate how random invariant\nprojections can be used to obtain an invariant representation with a high\ndegree of retained information. In both cases, the analysis of invariant latent\nrepresentations proves superior to their equivariant counterparts. Finally, we\nillustrate that the phenomena documented here for equivariant neural networks\nhave counterparts in standard neural networks where invariance is encouraged\nvia augmentation. Thus, while these ambiguities may be known by experienced\ndevelopers of equivariant models, we make both the knowledge as well as\neffective tools to handle the ambiguities available to the broader community.\n']",Equivariant Neural Networks and Representations,Equivariant Neural Networks and Symmetry in Deep Learning,Geometric and Equivariant Deep Learning
198,198,50,198_imagenet_cnn_autoencoder_cnns,"['imagenet', 'cnn', 'autoencoder', 'cnns', 'neural', 'backpropagation', 'networks', 'supervised', 'deep', 'learning']","['deep', 'networks', 'network', 'neural', 'supervised', 'convolutional', 'architectures', 'coding', 'self', 'learning']","['  The current deep neural network algorithm still stays in the end-to-end\ntraining supervision method like Image-Label pairs, which makes traditional\nalgorithm is difficult to explain the reason for the results, and the\nprediction logic is difficult to understand and analyze. The current algorithm\ndoes not use the existing human knowledge information, which makes the model\nnot in line with the human cognition model and makes the model not suitable for\nhuman use. In order to solve the above problems, the present invention provides\na deep neural network training method based on the human knowledge, which uses\nthe human cognition model to construct the deep neural network training model,\nand uses the existing human knowledge information to construct the deep neural\nnetwork training model. This paper proposes a multi-level hierarchical deep\nlearning algorithm, which is composed of multi-level hierarchical deep neural\nnetwork architecture and multi-level hierarchical deep learning framework. The\nexperimental results show that the proposed algorithm can effectively explain\nthe hidden information of the neural network. The goal of our study is to\nimprove the interpretability of deep neural networks (DNNs) by providing an\nanalysis of the impact of knowledge injection on the classification task. We\nconstructed a knowledge injection dataset with matching knowledge data and\nimage classification data. The knowledge injection dataset is the benchmark\ndataset for the experiments in the paper. Our model expresses the improvement\nin interpretability and classification task performance of hidden layers at\ndifferent scales.\n', '  In this work, we explore the intersection of sparse coding theory and deep\nlearning to enhance our understanding of feature extraction capabilities in\nadvanced neural network architectures. We begin by introducing a novel class of\nDeep Sparse Coding (DSC) models and establish a thorough theoretical analysis\nof their uniqueness and stability properties. By applying iterative algorithms\nto these DSC models, we derive convergence rates for convolutional neural\nnetworks (CNNs) in their ability to extract sparse features. This provides a\nstrong theoretical foundation for the use of CNNs in sparse feature learning\ntasks. We additionally extend this convergence analysis to more general neural\nnetwork architectures, including those with diverse activation functions, as\nwell as self-attention and transformer-based models. This broadens the\napplicability of our findings to a wide range of deep learning methods for deep\nsparse feature extraction. Inspired by the strong connection between sparse\ncoding and CNNs, we also explore training strategies to encourage neural\nnetworks to learn more sparse features. Through numerical experiments, we\ndemonstrate the effectiveness of these approaches, providing valuable insights\nfor the design of efficient and interpretable deep learning models.\n', '  Distributed sparse block codes (SBCs) exhibit compact representations for\nencoding and manipulating symbolic data structures using fixed-width vectors.\nOne major challenge however is to disentangle, or factorize, the distributed\nrepresentation of data structures into their constituent elements without\nhaving to search through all possible combinations. This factorization becomes\nmore challenging when SBCs vectors are noisy due to perceptual uncertainty and\napproximations made by modern neural networks to generate the query SBCs\nvectors. To address these challenges, we first propose a fast and highly\naccurate method for factorizing a more flexible and hence generalized form of\nSBCs, dubbed GSBCs. Our iterative factorizer introduces a threshold-based\nnonlinear activation, conditional random sampling, and an $\\ell_\\infty$-based\nsimilarity metric. Secondly, the proposed factorizer maintains a high accuracy\nwhen queried by noisy product vectors generated using deep convolutional neural\nnetworks (CNNs). This facilitates its application in replacing the large fully\nconnected layer (FCL) in CNNs, whereby $C$ trainable class vectors, or\nattribute combinations, can be implicitly represented by our factorizer having\n$F$-factor codebooks, each with $\\sqrt[\\leftroot{-2}\\uproot{2}F]{C}$ fixed\ncodevectors. We provide a methodology to flexibly integrate our factorizer in\nthe classification layer of CNNs with a novel loss function. With this\nintegration, the convolutional layers can generate a noisy product vector that\nour factorizer can still decode, whereby the decoded factors can have different\ninterpretations based on downstream tasks. We demonstrate the feasibility of\nour method on four deep CNN architectures over CIFAR-100, ImageNet-1K, and\nRAVEN datasets. In all use cases, the number of parameters and operations are\nnotably reduced compared to the FCL.\n']",Deep Learning for Interpretable Neural Networks,Interpretable Deep Learning Models,Explainable Artificial Intelligence
199,199,50,199_reinforcement_optimal_games_bandits,"['reinforcement', 'optimal', 'games', 'bandits', 'strategy', 'allocation', 'strategies', 'game', 'risk', 'optimize']","['games', 'game', 'equilibrium', 'agents', 'cooperative', 'decision', 'regret', 'risk', 'allocation', 'agent']","['  Addressing the question of how to achieve optimal decision-making under risk\nand uncertainty is crucial for enhancing the capabilities of artificial agents\nthat collaborate with or support humans. In this work, we address this question\nin the context of Public Goods Games. We study learning in a novel\nmulti-objective version of the Public Goods Game where agents have different\nrisk preferences, by means of multi-objective reinforcement learning. We\nintroduce a parametric non-linear utility function to model risk preferences at\nthe level of individual agents, over the collective and individual reward\ncomponents of the game. We study the interplay between such preference\nmodelling and environmental uncertainty on the incentive alignment level in the\ngame. We demonstrate how different combinations of individual preferences and\nenvironmental uncertainties sustain the emergence of cooperative patterns in\nnon-cooperative environments (i.e., where competitive strategies are dominant),\nwhile others sustain competitive patterns in cooperative environments (i.e.,\nwhere cooperative strategies are dominant).\n', '  This paper proposes a new framework of Markov $\\alpha$-potential games to\nstudy Markov games. In this new framework, Markov games are shown to be Markov\n$\\alpha$-potential games, and the existence of an associated $\\alpha$-potential\nfunction is established. Any optimizer of an $\\alpha$-potential function is\nshown to be an $\\alpha$-stationary NE. Two important classes of practically\nsignificant Markov games, Markov congestion games and the perturbed Markov team\ngames, are studied via this framework of Markov $\\alpha$-potential games, with\nexplicit characterization of an upper bound for $\\alpha$ and its relation to\ngame parameters. Additionally, a semi-infinite linear programming based\nformulation is presented to obtain an upper bound for $\\alpha$ for any Markov\ngame. Furthermore, two equilibrium approximation algorithms, namely the\nprojected gradient-ascent algorithm and the sequential maximum improvement\nalgorithm, are presented along with their Nash regret analysis, and\ncorroborated by numerical experiments.\n', '  We study risk-sensitive multi-agent reinforcement learning under general-sum\nMarkov games, where agents optimize the entropic risk measure of rewards with\npossibly diverse risk preferences. We show that using the regret naively\nadapted from existing literature as a performance metric could induce policies\nwith equilibrium bias that favor the most risk-sensitive agents and overlook\nthe other agents. To address such deficiency of the naive regret, we propose a\nnovel notion of regret, which we call risk-balanced regret, and show through a\nlower bound that it overcomes the issue of equilibrium bias. Furthermore, we\ndevelop a self-play algorithm for learning Nash, correlated, and coarse\ncorrelated equilibria in risk-sensitive Markov games. We prove that the\nproposed algorithm attains near-optimal regret guarantees with respect to the\nrisk-balanced regret.\n']",Optimal Decision-Making in Multi-Agent Games,Multi-Agent Decision Making and Reinforcement Learning,Multi-Agent Systems and Artificial Intelligence
200,200,50,200_retrieval_search_personalization_personalized,"['retrieval', 'search', 'personalization', 'personalized', 'queries', 'personalize', 'profiles', 'mindsearch', 'seeking', 'cosearchagent']","['search', 'user', 'query', 'users', 'personalization', 'engines', 'personalized', 'decoy', 'profiles', 'web']","[""  Due to the advantages in the cost-efficiency and reproducibility, user\nsimulation has become a promising solution to the user-centric evaluation of\ninformation retrieval systems. Nonetheless, accurately simulating user search\nbehaviors has long been a challenge, because users' actions in search are\nhighly complex and driven by intricate cognitive processes such as learning,\nreasoning, and planning. Recently, Large Language Models (LLMs) have\ndemonstrated remarked potential in simulating human-level intelligence and have\nbeen used in building autonomous agents for various tasks. However, the\npotential of using LLMs in simulating search behaviors has not yet been fully\nexplored. In this paper, we introduce a LLM-based user search behavior\nsimulator, USimAgent. The proposed simulator can simulate users' querying,\nclicking, and stopping behaviors during search, and thus, is capable of\ngenerating complete search sessions for specific search tasks. Empirical\ninvestigation on a real user behavior dataset shows that the proposed simulator\noutperforms existing methods in query generation and is comparable to\ntraditional methods in predicting user clicks and stopping behaviors. These\nresults not only validate the effectiveness of using LLMs for user simulation\nbut also shed light on the development of a more robust and generic user\nsimulators.\n"", '  Due to the excellent capacities of large language models (LLMs), it becomes\nfeasible to develop LLM-based agents for reliable user simulation. Considering\nthe scarcity and limit (e.g., privacy issues) of real user data, in this paper,\nwe conduct large-scale user simulation for web search, to improve the analysis\nand modeling of user search behavior. Specially, we propose BASES, a novel user\nsimulation framework with LLM-based agents, designed to facilitate\ncomprehensive simulations of web search user behaviors. Our simulation\nframework can generate unique user profiles at scale, which subsequently leads\nto diverse search behaviors. To demonstrate the effectiveness of BASES, we\nconduct evaluation experiments based on two human benchmarks in both Chinese\nand English, demonstrating that BASES can effectively simulate large-scale\nhuman-like search behaviors. To further accommodate the research on web search,\nwe develop WARRIORS, a new large-scale dataset encompassing web search user\nbehaviors, including both Chinese and English versions, which can greatly\nbolster research in the field of information retrieval. Our code and data will\nbe publicly released soon.\n', ""  Large Language Models (LLMs) excel at tackling various natural language\ntasks. However, due to the significant costs involved in re-training or\nfine-tuning them, they remain largely static and difficult to personalize.\nNevertheless, a variety of applications could benefit from generations that are\ntailored to users' preferences, goals, and knowledge. Among them is web search,\nwhere knowing what a user is trying to accomplish, what they care about, and\nwhat they know can lead to improved search experiences. In this work, we\npropose a novel and general approach that augments an LLM with relevant context\nfrom users' interaction histories with a search engine in order to personalize\nits outputs. Specifically, we construct an entity-centric knowledge store for\neach user based on their search and browsing activities on the web, which is\nthen leveraged to provide contextually relevant LLM prompt augmentations. This\nknowledge store is light-weight, since it only produces user-specific aggregate\nprojections of interests and knowledge onto public knowledge graphs, and\nleverages existing search log infrastructure, thereby mitigating the privacy,\ncompliance, and scalability concerns associated with building deep user\nprofiles for personalization. We validate our approach on the task of\ncontextual query suggestion, which requires understanding not only the user's\ncurrent search context but also what they historically know and care about.\nThrough a number of experiments based on human evaluation, we show that our\napproach is significantly better than several other LLM-powered baselines,\ngenerating query suggestions that are contextually more relevant, personalized,\nand useful.\n""]",Large Language Models for Personalized Search,Optimization and Efficiency of Large Language Models,Large Language Models
201,201,50,201_adversarial_cybersecurity_security_ai,"['adversarial', 'cybersecurity', 'security', 'ai', 'attacks', 'threats', 'intrusion', 'attack', 'threat', 'defense']","['security', 'cyber', 'password', 'cybersecurity', 'attacks', 'threats', 'passwords', 'adversarial', 'threat', 'detection']","['  The last decades have been characterized by unprecedented technological\nadvances, many of them powered by modern technologies such as Artificial\nIntelligence (AI) and Machine Learning (ML). The world has become more\ndigitally connected than ever, but we face major challenges. One of the most\nsignificant is cybercrime, which has emerged as a global threat to governments,\nbusinesses, and civil societies. The pervasiveness of digital technologies\ncombined with a constantly shifting technological foundation has created a\ncomplex and powerful playground for cybercriminals, which triggered a surge in\ndemand for intelligent threat detection systems based on machine and deep\nlearning. This paper investigates AI-based cyber threat detection to protect\nour modern digital ecosystems. The primary focus is on evaluating ML-based\nclassifiers and ensembles for anomaly-based malware detection and network\nintrusion detection and how to integrate those models in the context of network\nsecurity, mobile security, and IoT security. The discussion highlights the\nchallenges when deploying and integrating AI-enabled cybersecurity solutions\ninto existing enterprise systems and IT infrastructures, including options to\novercome those challenges. Finally, the paper provides future research\ndirections to further increase the security and resilience of our modern\ndigital industries, infrastructures, and ecosystems.\n', '  The 2nd International Workshop on Adaptive Cyber Defense was held at the\nFlorida Institute of Technology, Florida. This workshop was organized to share\nresearch that explores unique applications of Artificial Intelligence (AI) and\nMachine Learning (ML) as foundational capabilities for the pursuit of adaptive\ncyber defense. The cyber domain cannot currently be reliably and effectively\ndefended without extensive reliance on human experts. Skilled cyber defenders\nare in short supply and often cannot respond fast enough to cyber threats.\n  Building on recent advances in AI and ML the Cyber defense research community\nhas been motivated to develop new dynamic and sustainable defenses through the\nadoption of AI and ML techniques to cyber settings. Bridging critical gaps\nbetween AI and Cyber researchers and practitioners can accelerate efforts to\ncreate semi-autonomous cyber defenses that can learn to recognize and respond\nto cyber attacks or discover and mitigate weaknesses in cooperation with other\ncyber operation systems and human experts. Furthermore, these defenses are\nexpected to be adaptive and able to evolve over time to thwart changes in\nattacker behavior, changes in the system health and readiness, and natural\nshifts in user behavior over time.\n  The workshop was comprised of invited keynote talks, technical presentations\nand a panel discussion about how AI/ML can enable autonomous mitigation of\ncurrent and future cyber attacks. Workshop submissions were peer reviewed by a\npanel of domain experts with a proceedings consisting of six technical articles\nexploring challenging problems of critical importance to national and global\nsecurity. Participation in this workshop offered new opportunities to stimulate\nresearch and innovation in the emerging domain of adaptive and autonomous cyber\ndefense.\n', '  We introduce the AI Security Pyramid of Pain, a framework that adapts the\ncybersecurity Pyramid of Pain to categorize and prioritize AI-specific threats.\nThis framework provides a structured approach to understanding and addressing\nvarious levels of AI threats. Starting at the base, the pyramid emphasizes Data\nIntegrity, which is essential for the accuracy and reliability of datasets and\nAI models, including their weights and parameters. Ensuring data integrity is\ncrucial, as it underpins the effectiveness of all AI-driven decisions and\noperations. The next level, AI System Performance, focuses on MLOps-driven\nmetrics such as model drift, accuracy, and false positive rates. These metrics\nare crucial for detecting potential security breaches, allowing for early\nintervention and maintenance of AI system integrity. Advancing further, the\npyramid addresses the threat posed by Adversarial Tools, identifying and\nneutralizing tools used by adversaries to target AI systems. This layer is key\nto staying ahead of evolving attack methodologies. At the Adversarial Input\nlayer, the framework addresses the detection and mitigation of inputs designed\nto deceive or exploit AI models. This includes techniques like adversarial\npatterns and prompt injection attacks, which are increasingly used in\nsophisticated attacks on AI systems. Data Provenance is the next critical\nlayer, ensuring the authenticity and lineage of data and models. This layer is\npivotal in preventing the use of compromised or biased data in AI systems. At\nthe apex is the tactics, techniques, and procedures (TTPs) layer, dealing with\nthe most complex and challenging aspects of AI security. This involves a deep\nunderstanding and strategic approach to counter advanced AI-targeted attacks,\nrequiring comprehensive knowledge and planning.\n']",AI-Powered Cybersecurity and Threat Detection,Cybersecurity and Artificial Intelligence in Emerging Technologies,Cybersecurity and Artificial Intelligence
202,202,50,202_operators_operator_approximating_approximation,"['operators', 'operator', 'approximating', 'approximation', 'neural', 'approximate', 'learning', 'banach', 'kernel', 'networks']","['operator', 'operators', 'approximation', 'preconditioners', 'spaces', 'neural', 'polynomial', 'functions', 'nonlinear', 'numerical']","[""  Neural operator architectures approximate operators between\ninfinite-dimensional Banach spaces of functions. They are gaining increased\nattention in computational science and engineering, due to their potential both\nto accelerate traditional numerical methods and to enable data-driven\ndiscovery. As the field is in its infancy basic questions about minimal\nrequirements for universal approximation remain open. It is clear that any\ngeneral approximation of operators between spaces of functions must be both\nnonlocal and nonlinear. In this paper we describe how these two attributes may\nbe combined in a simple way to deduce universal approximation. In so doing we\nunify the analysis of a wide range of neural operator architectures and open up\nconsideration of new ones.\n  A popular variant of neural operators is the Fourier neural operator (FNO).\nPrevious analysis proving universal operator approximation theorems for FNOs\nresorts to use of an unbounded number of Fourier modes, relying on intuition\nfrom traditional analysis of spectral methods. The present work challenges this\npoint of view: (i) the work reduces FNO to its core essence, resulting in a\nminimal architecture termed the ``averaging neural operator'' (ANO); and (ii)\nanalysis of the ANO shows that even this minimal ANO architecture benefits from\nuniversal approximation. This result is obtained based on only a spatial\naverage as its only nonlocal ingredient (corresponding to retaining only a\n\\emph{single} Fourier mode in the special case of the FNO). The analysis paves\nthe way for a more systematic exploration of nonlocality, both through the\ndevelopment of new operator learning architectures and the analysis of existing\nand new architectures. Numerical results are presented which give insight into\ncomplexity issues related to the roles of channel width (embedding dimension)\nand number of Fourier modes.\n"", '  Operator learning based on neural operators has emerged as a promising\nparadigm for the data-driven approximation of operators, mapping between\ninfinite-dimensional Banach spaces. Despite significant empirical progress, our\ntheoretical understanding regarding the efficiency of these approximations\nremains incomplete. This work addresses the parametric complexity of neural\noperator approximations for the general class of Lipschitz continuous\noperators. Motivated by recent findings on the limitations of specific\narchitectures, termed curse of parametric complexity, we here adopt an\ninformation-theoretic perspective. Our main contribution establishes lower\nbounds on the metric entropy of Lipschitz operators in two approximation\nsettings; uniform approximation over a compact set of input functions, and\napproximation in expectation, with input functions drawn from a probability\nmeasure. It is shown that these entropy bounds imply that, regardless of the\nactivation function used, neural operator architectures attaining an\napproximation accuracy $\\epsilon$ must have a size that is exponentially large\nin $\\epsilon^{-1}$. The size of architectures is here measured by counting the\nnumber of encoded bits necessary to store the given model in computational\nmemory. The results of this work elucidate fundamental trade-offs and\nlimitations in operator learning.\n', '  Operator learning refers to the application of ideas from machine learning to\napproximate (typically nonlinear) operators mapping between Banach spaces of\nfunctions. Such operators often arise from physical models expressed in terms\nof partial differential equations (PDEs). In this context, such approximate\noperators hold great potential as efficient surrogate models to complement\ntraditional numerical methods in many-query tasks. Being data-driven, they also\nenable model discovery when a mathematical description in terms of a PDE is not\navailable. This review focuses primarily on neural operators, built on the\nsuccess of deep neural networks in the approximation of functions defined on\nfinite dimensional Euclidean spaces. Empirically, neural operators have shown\nsuccess in a variety of applications, but our theoretical understanding remains\nincomplete. This review article summarizes recent progress and the current\nstate of our theoretical understanding of neural operators, focusing on an\napproximation theoretic point of view.\n']",Neural Operator Approximation,Deep Learning Theory and Foundations,Machine Learning and Artificial Intelligence
203,203,49,203_generative_autoencoder_adversarial_models,"['generative', 'autoencoder', 'adversarial', 'models', 'variational', 'energy', 'flow', 'modeling', 'rnn', 'generator']","['diffusion', 'distribution', 'distributions', 'denoising', 'score', 'energy', 'likelihood', 'mixture', 'sampling', 'latent']","['  Energy-Based Models (EBMs) have emerged as a powerful framework in the realm\nof generative modeling, offering a unique perspective that aligns closely with\nprinciples of statistical mechanics. This review aims to provide physicists\nwith a comprehensive understanding of EBMs, delineating their connection to\nother generative models such as Generative Adversarial Networks (GANs),\nVariational Autoencoders (VAEs), and Normalizing Flows. We explore the sampling\ntechniques crucial for EBMs, including Markov Chain Monte Carlo (MCMC) methods,\nand draw parallels between EBM concepts and statistical mechanics, highlighting\nthe significance of energy functions and partition functions. Furthermore, we\ndelve into state-of-the-art training methodologies for EBMs, covering recent\nadvancements and their implications for enhanced model performance and\nefficiency. This review is designed to clarify the often complex\ninterconnections between these models, which can be challenging due to the\ndiverse communities working on the topic.\n', '  Energy-Based Models (EBMs) are an important class of probabilistic models,\nalso known as random fields and undirected graphical models. EBMs are\nun-normalized and thus radically different from other popular self-normalized\nprobabilistic models such as hidden Markov models (HMMs), autoregressive\nmodels, generative adversarial nets (GANs) and variational auto-encoders\n(VAEs). Over the past years, EBMs have attracted increasing interest not only\nfrom the core machine learning community, but also from application domains\nsuch as speech, vision, natural language processing (NLP) and so on, due to\nsignificant theoretical and algorithmic progress. The sequential nature of\nspeech and language also presents special challenges and needs a different\ntreatment from processing fix-dimensional data (e.g., images). Therefore, the\npurpose of this monograph is to present a systematic introduction to\nenergy-based models, including both algorithmic progress and applications in\nspeech and language processing. First, the basics of EBMs are introduced,\nincluding classic models, recent models parameterized by neural networks,\nsampling methods, and various learning methods from the classic learning\nalgorithms to the most advanced ones. Then, the application of EBMs in three\ndifferent scenarios is presented, i.e., for modeling marginal, conditional and\njoint distributions, respectively. 1) EBMs for sequential data with\napplications in language modeling, where the main focus is on the marginal\ndistribution of a sequence itself; 2) EBMs for modeling conditional\ndistributions of target sequences given observation sequences, with\napplications in speech recognition, sequence labeling and text generation; 3)\nEBMs for modeling joint distributions of both sequences of observations and\ntargets, and their applications in semi-supervised learning and calibrated\nnatural language understanding.\n', ""  Generative models have shown strong generation ability while efficient\nlikelihood estimation is less explored. Energy-based models~(EBMs) define a\nflexible energy function to parameterize unnormalized densities efficiently but\nare notorious for being difficult to train. Adversarial EBMs introduce a\ngenerator to form a minimax training game to avoid expensive MCMC sampling used\nin traditional EBMs, but a noticeable gap between adversarial EBMs and other\nstrong generative models still exists. Inspired by diffusion-based models, we\nembedded EBMs into each denoising step to split a long-generated process into\nseveral smaller steps. Besides, we employ a symmetric Jeffrey divergence and\nintroduce a variational posterior distribution for the generator's training to\naddress the main challenges that exist in adversarial EBMs. Our experiments\nshow significant improvement in generation compared to existing adversarial\nEBMs, while also providing a useful energy function for efficient density\nestimation.\n""]",Energy-Based Generative Models,Generative Modeling Techniques,Generative Modeling and Artificial Intelligence
204,204,48,204_planning_planner_reinforcement_autonomous,"['planning', 'planner', 'reinforcement', 'autonomous', 'plan', 'policies', 'schedule', 'temporal', 'projects', 'actions']","['planning', 'drone', 'verification', 'safe', 'agent', 'actions', 'project', 'decision', 'unsafe', 'flight']","[""  Recently there has been a growing interest in industry and academia,\nregarding the use of wireless chargers to prolong the operational longevity of\nunmanned aerial vehicles (commonly knowns as drones). In this paper we consider\na charger-assisted drone application: a drone is deployed to observe a set\npoints of interest, while a charger can move to recharge the drone's battery.\nWe focus on the route and charging schedule of the drone and the mobile\ncharger, to obtain high observation utility with the shortest possible time,\nwhile ensuring the drone remains operational during task execution.\nEssentially, this proposed drone-charger scheduling problem is a multi-stage\ndecision-making process, in which the drone and the mobile charger act as two\nagents who cooperate to finish a task. The discrete-continuous hybrid action\nspace of the two agents poses a significant challenge in our problem. To\naddress this issue, we present a hybrid-action deep reinforcement learning\nframework, called HaDMC, which uses a standard policy learning algorithm to\ngenerate latent continuous actions. Motivated by representation learning, we\nspecifically design and train an action decoder. It involves two pipelines to\nconvert the latent continuous actions into original discrete and continuous\nactions, by which the drone and the charger can directly interact with\nenvironment. We embed a mutual learning scheme in model training, emphasizing\nthe collaborative rather than individual actions. We conduct extensive\nnumerical experiments to evaluate HaDMC and compare it with state-of-the-art\ndeep reinforcement learning approaches. The experimental results show the\neffectiveness and efficiency of our solution.\n"", ""  In recent years, Deep Reinforcement Learning (DRL) has emerged as an\neffective approach to solving real-world tasks. However, despite their\nsuccesses, DRL-based policies suffer from poor reliability, which limits their\ndeployment in safety-critical domains. Various methods have been put forth to\naddress this issue by providing formal safety guarantees. Two main approaches\ninclude shielding and verification. While shielding ensures the safe behavior\nof the policy by employing an external online component (i.e., a ``shield'')\nthat overrides potentially dangerous actions, this approach has a significant\ncomputational cost as the shield must be invoked at runtime to validate every\ndecision. On the other hand, verification is an offline process that can\nidentify policies that are unsafe, prior to their deployment, yet, without\nproviding alternative actions when such a policy is deemed unsafe. In this\nwork, we present verification-guided shielding -- a novel approach that bridges\nthe DRL reliability gap by integrating these two methods. Our approach combines\nboth formal and probabilistic verification tools to partition the input domain\ninto safe and unsafe regions. In addition, we employ clustering and symbolic\nrepresentation procedures that compress the unsafe regions into a compact\nrepresentation. This, in turn, allows to temporarily activate the shield solely\nin (potentially) unsafe regions, in an efficient manner. Our novel approach\nallows to significantly reduce runtime overhead while still preserving formal\nsafety guarantees. We extensively evaluate our approach on two benchmarks from\nthe robotic navigation domain, as well as provide an in-depth analysis of its\nscalability and completeness.\n"", '  Standard temporal planning assumes that planning takes place offline and then\nexecution starts at time 0. Recently, situated temporal planning was\nintroduced, where planning starts at time 0 and execution occurs after planning\nterminates. Situated temporal planning reflects a more realistic scenario where\ntime passes during planning. However, in situated temporal planning a complete\nplan must be generated before any action is executed. In some problems with\ntime pressure, timing is too tight to complete planning before the first action\nmust be executed. For example, an autonomous car that has a truck backing\ntowards it should probably move out of the way now and plan how to get to its\ndestination later. In this paper, we propose a new problem setting: concurrent\nplanning and execution, in which actions can be dispatched (executed) before\nplanning terminates. Unlike previous work on planning and execution, we must\nhandle wall clock deadlines that affect action applicability and goal\nachievement (as in situated planning) while also supporting dispatching actions\nbefore a complete plan has been found. We extend previous work on metareasoning\nfor situated temporal planning to develop an algorithm for this new setting.\nOur empirical evaluation shows that when there is strong time pressure, our\napproach outperforms situated temporal planning.\n']",Autonomous Planning and Scheduling with Reinforcement Learning,Reinforcement Learning Methods and Applications,Reinforcement Learning
204,204,48,204_planning_planner_reinforcement_autonomous,"['planning', 'planner', 'reinforcement', 'autonomous', 'plan', 'policies', 'schedule', 'temporal', 'projects', 'actions']","['planning', 'drone', 'verification', 'safe', 'agent', 'actions', 'project', 'decision', 'unsafe', 'flight']","[""  Recently there has been a growing interest in industry and academia,\nregarding the use of wireless chargers to prolong the operational longevity of\nunmanned aerial vehicles (commonly knowns as drones). In this paper we consider\na charger-assisted drone application: a drone is deployed to observe a set\npoints of interest, while a charger can move to recharge the drone's battery.\nWe focus on the route and charging schedule of the drone and the mobile\ncharger, to obtain high observation utility with the shortest possible time,\nwhile ensuring the drone remains operational during task execution.\nEssentially, this proposed drone-charger scheduling problem is a multi-stage\ndecision-making process, in which the drone and the mobile charger act as two\nagents who cooperate to finish a task. The discrete-continuous hybrid action\nspace of the two agents poses a significant challenge in our problem. To\naddress this issue, we present a hybrid-action deep reinforcement learning\nframework, called HaDMC, which uses a standard policy learning algorithm to\ngenerate latent continuous actions. Motivated by representation learning, we\nspecifically design and train an action decoder. It involves two pipelines to\nconvert the latent continuous actions into original discrete and continuous\nactions, by which the drone and the charger can directly interact with\nenvironment. We embed a mutual learning scheme in model training, emphasizing\nthe collaborative rather than individual actions. We conduct extensive\nnumerical experiments to evaluate HaDMC and compare it with state-of-the-art\ndeep reinforcement learning approaches. The experimental results show the\neffectiveness and efficiency of our solution.\n"", ""  In recent years, Deep Reinforcement Learning (DRL) has emerged as an\neffective approach to solving real-world tasks. However, despite their\nsuccesses, DRL-based policies suffer from poor reliability, which limits their\ndeployment in safety-critical domains. Various methods have been put forth to\naddress this issue by providing formal safety guarantees. Two main approaches\ninclude shielding and verification. While shielding ensures the safe behavior\nof the policy by employing an external online component (i.e., a ``shield'')\nthat overrides potentially dangerous actions, this approach has a significant\ncomputational cost as the shield must be invoked at runtime to validate every\ndecision. On the other hand, verification is an offline process that can\nidentify policies that are unsafe, prior to their deployment, yet, without\nproviding alternative actions when such a policy is deemed unsafe. In this\nwork, we present verification-guided shielding -- a novel approach that bridges\nthe DRL reliability gap by integrating these two methods. Our approach combines\nboth formal and probabilistic verification tools to partition the input domain\ninto safe and unsafe regions. In addition, we employ clustering and symbolic\nrepresentation procedures that compress the unsafe regions into a compact\nrepresentation. This, in turn, allows to temporarily activate the shield solely\nin (potentially) unsafe regions, in an efficient manner. Our novel approach\nallows to significantly reduce runtime overhead while still preserving formal\nsafety guarantees. We extensively evaluate our approach on two benchmarks from\nthe robotic navigation domain, as well as provide an in-depth analysis of its\nscalability and completeness.\n"", '  Standard temporal planning assumes that planning takes place offline and then\nexecution starts at time 0. Recently, situated temporal planning was\nintroduced, where planning starts at time 0 and execution occurs after planning\nterminates. Situated temporal planning reflects a more realistic scenario where\ntime passes during planning. However, in situated temporal planning a complete\nplan must be generated before any action is executed. In some problems with\ntime pressure, timing is too tight to complete planning before the first action\nmust be executed. For example, an autonomous car that has a truck backing\ntowards it should probably move out of the way now and plan how to get to its\ndestination later. In this paper, we propose a new problem setting: concurrent\nplanning and execution, in which actions can be dispatched (executed) before\nplanning terminates. Unlike previous work on planning and execution, we must\nhandle wall clock deadlines that affect action applicability and goal\nachievement (as in situated planning) while also supporting dispatching actions\nbefore a complete plan has been found. We extend previous work on metareasoning\nfor situated temporal planning to develop an algorithm for this new setting.\nOur empirical evaluation shows that when there is strong time pressure, our\napproach outperforms situated temporal planning.\n']",Autonomous Planning and Scheduling with Reinforcement Learning,Reinforcement Learning Methods and Applications,Reinforcement Learning
205,205,48,205_corpus_nlp_stemming_texts,"['corpus', 'nlp', 'stemming', 'texts', 'disambiguation', 'semantic', 'entities', 'keywords', 'entity', 'contextual']","['extraction', 'documents', 'document', 'entity', 'stemming', 'text', 'classification', 'sentence', 'disambiguation', 'burstiness']","[""  Documents that consist of diverse templates and exhibit complex spatial\nstructures pose a challenge for document entity classification. We propose\nKNN-former, which incorporates a new kind of spatial bias in attention\ncalculation based on the K-nearest-neighbor (KNN) graph of document entities.\nWe limit entities' attention only to their local radius defined by the KNN\ngraph. We also use combinatorial matching to address the one-to-one mapping\nproperty that exists in many documents, where one field has only one\ncorresponding entity. Moreover, our method is highly parameter-efficient\ncompared to existing approaches in terms of the number of trainable parameters.\nDespite this, experiments across various datasets show our method outperforms\nbaselines in most entity types. Many real-world documents exhibit combinatorial\nproperties which can be leveraged as inductive biases to improve extraction\naccuracy, but existing datasets do not cover these documents. To facilitate\nfuture research into these types of documents, we release a new ID document\ndataset that covers diverse templates and languages. We also release enhanced\nannotations for an existing dataset.\n"", '  Massive-scale historical document collections are crucial for social science\nresearch. Despite increasing digitization, these documents typically lack\nunique cross-document identifiers for individuals mentioned within the texts,\nas well as individual identifiers from external knowledgebases like\nWikipedia/Wikidata. Existing entity disambiguation methods often fall short in\naccuracy for historical documents, which are replete with individuals not\nremembered in contemporary knowledgebases. This study makes three key\ncontributions to improve cross-document coreference resolution and\ndisambiguation in historical texts: a massive-scale training dataset replete\nwith hard negatives - that sources over 190 million entity pairs from Wikipedia\ncontexts and disambiguation pages - high-quality evaluation data from\nhand-labeled historical newswire articles, and trained models evaluated on this\nhistorical benchmark. We contrastively train bi-encoder models for\ncoreferencing and disambiguating individuals in historical texts, achieving\naccurate, scalable performance that identifies out-of-knowledgebase\nindividuals. Our approach significantly surpasses other entity disambiguation\nmodels on our historical newswire benchmark. Our models also demonstrate\ncompetitive performance on modern entity disambiguation benchmarks,\nparticularly certain news disambiguation datasets.\n', '  The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural\nLanguage Processing (NLP) during the past decade. However, the demands of long\ndocument analysis are quite different from those of shorter texts, while the\never increasing size of documents uploaded online renders automated\nunderstanding of lengthy texts a critical issue. Relevant applications include\nautomated Web mining, legal document review, medical records analysis,\nfinancial reports analysis, contract management, environmental impact\nassessment, news aggregation, etc. Despite the relatively recent development of\nefficient algorithms for analyzing long documents, practical tools in this\nfield are currently flourishing. This article serves as an entry point into\nthis dynamic domain and aims to achieve two objectives. First of all, it\nprovides an introductory overview of the relevant neural building blocks,\nserving as a concise tutorial for the field. Secondly, it offers a brief\nexamination of the current state-of-the-art in two key long document analysis\ntasks: document classification and document summarization. Sentiment analysis\nfor long texts is also covered, since it is typically treated as a particular\ncase of document classification. Consequently, this article presents an\nintroductory exploration of document-level analysis, addressing the primary\nchallenges, concerns, and existing solutions. Finally, it offers a concise\ndefinition of ""long text/document"", presents an original overarching taxonomy\nof common deep neural methods for long document analysis and lists publicly\navailable annotated datasets that can facilitate further research in this area.\n']",Document Entity Classification and Disambiguation,Entity Understanding and Resolution,Entity Understanding and Semantic Knowledge Integration
206,206,48,206_dialogues_dialogue_conversational_utterances,"['dialogues', 'dialogue', 'conversational', 'utterances', 'conversation', 'conversations', 'utterance', 'responses', 'chatgpt', 'language']","['dialogue', 'conversation', 'hallucination', 'conversational', 'human', 'role', 'evaluation', 'evaluators', 'responses', 'dialogues']","['  Since large language models (LLMs) achieve significant success in recent\nyears, the hallucination issue remains a challenge, numerous benchmarks are\nproposed to detect the hallucination. Nevertheless, some of these benchmarks\nare not naturally generated by LLMs but are intentionally induced. Also, many\nmerely focus on the factuality hallucination while ignoring the faithfulness\nhallucination. Additionally, although dialogue pattern is more widely utilized\nin the era of LLMs, current benchmarks only concentrate on sentence-level and\npassage-level hallucination. In this study, we propose DiaHalu, the first\ndialogue-level hallucination evaluation benchmark to our knowledge. Initially,\nwe integrate the collected topics into system prompts and facilitate a dialogue\nbetween two ChatGPT3.5. Subsequently, we manually modify the contents that do\nnot adhere to human language conventions and then have LLMs re-generate,\nsimulating authentic human-machine interaction scenarios. Finally, professional\nscholars annotate all the samples in the dataset. DiaHalu covers four common\nmulti-turn dialogue domains and five hallucination subtypes, extended from\nfactuality and faithfulness hallucination. Experiments through some well-known\nLLMs and detection methods on the dataset show that DiaHalu is a challenging\nbenchmark, holding significant value for further research.\n', '  Empowered by the large-scale pretrained language models, existing dialogue\nsystems have demonstrated impressive performance conducting fluent and\nnatural-sounding conversations. However, they are still plagued by the\nhallucination problem, causing unpredictable factual errors in the generated\nresponses. Recently, knowledge-grounded dialogue generation models, that\nintentionally invoke external knowledge resources to more informative\nresponses, are also proven to be effective in reducing hallucination. Following\nthe idea of getting high-quality knowledge, a few efforts have achieved pretty\ngood performance on this issue. As some inevitable knowledge noises may also\nlead to hallucinations, it is emergent to investigate the reason and future\ndirections for building noise-tolerant methods in KGD tasks. In this paper, we\nanalyze the causal story behind this problem with counterfactual reasoning\nmethods. Based on the causal effect analysis, we propose a possible solution\nfor alleviating the hallucination in KGD by exploiting the dialogue-knowledge\ninteraction. Experimental results of our example implementation show that this\nmethod can reduce hallucination without disrupting other dialogue performance,\nwhile keeping adaptive to different generation models. We hope our efforts can\nsupport and call for more attention to developing lightweight techniques\ntowards robust and trusty dialogue systems.\n', '  Automatic evaluation is an integral aspect of dialogue system research. The\ntraditional reference-based NLG metrics are generally found to be unsuitable\nfor dialogue assessment. Consequently, recent studies have suggested various\nunique, reference-free neural metrics that better align with human evaluations.\nNotably among them, large language models (LLMs), particularly the\ninstruction-tuned variants like ChatGPT, are shown to be promising substitutes\nfor human judges. Yet, existing works on utilizing LLMs for automatic dialogue\nevaluation are limited in their scope in terms of the number of meta-evaluation\ndatasets, mode of evaluation, coverage of LLMs, etc. Hence, it remains\ninconclusive how effective these LLMs are. To this end, we conduct a\ncomprehensive study on the application of LLMs for automatic dialogue\nevaluation. Specifically, we analyze the multi-dimensional evaluation\ncapability of 30 recently emerged LLMs at both turn and dialogue levels, using\na comprehensive set of 12 meta-evaluation datasets. Additionally, we probe the\nrobustness of the LLMs in handling various adversarial perturbations at both\nturn and dialogue levels. Finally, we explore how model-level and\ndimension-level ensembles impact the evaluation performance. All resources are\navailable at https://github.com/e0397123/comp-analysis.\n']",Dialogue Hallucination in Conversational AI,Conversational AI and Language Models,Conversational AI and Human-Computer Interaction
207,207,48,207_softmax_gaussian_experts_overparameterization,"['softmax', 'gaussian', 'experts', 'overparameterization', 'generalization', 'minimax', 'expert', 'overparameterized', 'neural', 'regularization']","['gating', 'softmax', 'theoretical', 'generalization', 'covariate', 'regression', 'loss', 'estimation', 'transfer', 'bounds']","['  Top-K sparse softmax gating mixture of experts has been widely used for\nscaling up massive deep-learning architectures without increasing the\ncomputational cost. Despite its popularity in real-world applications, the\ntheoretical understanding of that gating function has remained an open problem.\nThe main challenge comes from the structure of the top-K sparse softmax gating\nfunction, which partitions the input space into multiple regions with distinct\nbehaviors. By focusing on a Gaussian mixture of experts, we establish\ntheoretical results on the effects of the top-K sparse softmax gating function\non both density and parameter estimations. Our results hinge upon defining\nnovel loss functions among parameters to capture different behaviors of the\ninput regions. When the true number of experts $k_{\\ast}$ is known, we\ndemonstrate that the convergence rates of density and parameter estimations are\nboth parametric on the sample size. However, when $k_{\\ast}$ becomes unknown\nand the true model is over-specified by a Gaussian mixture of $k$ experts where\n$k > k_{\\ast}$, our findings suggest that the number of experts selected from\nthe top-K sparse softmax gating function must exceed the total cardinality of a\ncertain number of Voronoi cells associated with the true parameters to\nguarantee the convergence of the density estimation. Moreover, while the\ndensity estimation rate remains parametric under this setting, the parameter\nestimation rates become substantially slow due to an intrinsic interaction\nbetween the softmax gating and expert functions.\n', '  The softmax gating function is arguably the most popular choice in mixture of\nexperts modeling. Despite its widespread use in practice, softmax gating may\nlead to unnecessary competition among experts, potentially causing the\nundesirable phenomenon of representation collapse due to its inherent\nstructure. In response, the sigmoid gating function has been recently proposed\nas an alternative and has been demonstrated empirically to achieve superior\nperformance. However, a rigorous examination of the sigmoid gating function is\nlacking in current literature. In this paper, we verify theoretically that\nsigmoid gating, in fact, enjoys a higher sample efficiency than softmax gating\nfor the statistical task of expert estimation. Towards that goal, we consider a\nregression framework in which the unknown regression function is modeled as a\nmixture of experts, and study the rates of convergence of the least squares\nestimator in the over-specified case in which the number of experts fitted is\nlarger than the true value. We show that two gating regimes naturally arise\nand, in each of them, we formulate identifiability conditions for the expert\nfunctions and derive the corresponding convergence rates. In both cases, we\nfind that experts formulated as feed-forward networks with commonly used\nactivation such as $\\mathrm{ReLU}$ and $\\mathrm{GELU}$ enjoy faster convergence\nrates under sigmoid gating than softmax gating. Furthermore, given the same\nchoice of experts, we demonstrate that the sigmoid gating function requires a\nsmaller sample size than its softmax counterpart to attain the same error of\nexpert estimation and, therefore, is more sample efficient.\n', '  Mixture-of-experts (MoE) model incorporates the power of multiple submodels\nvia gating functions to achieve greater performance in numerous regression and\nclassification applications. From a theoretical perspective, while there have\nbeen previous attempts to comprehend the behavior of that model under the\nregression settings through the convergence analysis of maximum likelihood\nestimation in the Gaussian MoE model, such analysis under the setting of a\nclassification problem has remained missing in the literature. We close this\ngap by establishing the convergence rates of density estimation and parameter\nestimation in the softmax gating multinomial logistic MoE model. Notably, when\npart of the expert parameters vanish, these rates are shown to be slower than\npolynomial rates owing to an inherent interaction between the softmax gating\nand expert functions via partial differential equations. To address this issue,\nwe propose using a novel class of modified softmax gating functions which\ntransform the input before delivering them to the gating functions. As a\nresult, the previous interaction disappears and the parameter estimation rates\nare significantly improved.\n']",Mixture of Experts Modeling and Gating Functions,Mixture of Experts (MoE) Models and Their Optimizations,Machine Learning and Optimization
208,208,48,208_quizzes_distractors_distractor_assessments,"['quizzes', 'distractors', 'distractor', 'assessments', 'students', 'comprehension', 'exam', 'quiz', 'pedagogical', 'assessment']","['questions', 'educational', 'distractors', 'distractor', 'students', 'choice', 'question', 'difficulty', 'math', 'clue']","['  High-quality distractors are crucial to both the assessment and pedagogical\nvalue of multiple-choice questions (MCQs), where manually crafting ones that\nanticipate knowledge deficiencies or misconceptions among real students is\ndifficult. Meanwhile, automated distractor generation, even with the help of\nlarge language models (LLMs), remains challenging for subjects like math. It is\ncrucial to not only identify plausible distractors but also understand the\nerror behind them. In this paper, we introduce DiVERT (Distractor Generation\nwith Variational Errors Represented as Text), a novel variational approach that\nlearns an interpretable representation of errors behind distractors in math\nMCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions\nused by hundreds of thousands of students, we show that DiVERT, despite using a\nbase open-source LLM with 7B parameters, outperforms state-of-the-art\napproaches using GPT-4o on downstream distractor generation. We also conduct a\nhuman evaluation with math educators and find that DiVERT leads to error labels\nthat are of comparable quality to human-authored ones.\n', '  Multiple-choice questions (MCQs) are ubiquitous in almost all levels of\neducation since they are easy to administer, grade, and are a reliable form of\nassessment. An important aspect of MCQs is the distractors, i.e., incorrect\noptions that are designed to target specific misconceptions or insufficient\nknowledge among students. To date, the task of crafting high-quality\ndistractors has largely remained a labor-intensive process for teachers and\nlearning content designers, which has limited scalability. In this work, we\nexplore the task of automated distractor and corresponding feedback message\ngeneration in math MCQs using large language models. We establish a formulation\nof these two tasks and propose a simple, in-context learning-based solution.\nMoreover, we propose generative AI-based metrics for evaluating the quality of\nthe feedback messages. We conduct extensive experiments on these tasks using a\nreal-world MCQ dataset. Our findings suggest that there is a lot of room for\nimprovement in automated distractor and feedback generation; based on these\nfindings, we outline several directions for future work.\n', '  Multiple-choice questions (MCQs) are ubiquitous in almost all levels of\neducation since they are easy to administer, grade, and are a reliable format\nin assessments and practices. One of the most important aspects of MCQs is the\ndistractors, i.e., incorrect options that are designed to target common errors\nor misconceptions among real students. To date, the task of crafting\nhigh-quality distractors largely remains a labor and time-intensive process for\nteachers and learning content designers, which has limited scalability. In this\nwork, we study the task of automated distractor generation in the domain of\nmath MCQs and explore a wide variety of large language model (LLM)-based\napproaches, from in-context learning to fine-tuning. We conduct extensive\nexperiments using a real-world math MCQ dataset and find that although LLMs can\ngenerate some mathematically valid distractors, they are less adept at\nanticipating common errors or misconceptions among real students.\n']",Automated Distractor Generation for Math MCQs,Explainable AI and Machine Learning,Artificial Intelligence and Machine Learning Interpretability and Explainability
209,209,48,209_pdes_pde_stabilization_lyapunov,"['pdes', 'pde', 'stabilization', 'lyapunov', 'control', 'stability', 'controller', 'controllers', 'nonlinear', 'stabilizing']","['control', 'dynamical', 'nonlinear', 'operator', 'backstepping', 'gain', 'dimensional', 'numerical', 'controller', 'stabilizing']","['  To stabilize PDE models, control laws require space-dependent functional\ngains mapped by nonlinear operators from the PDE functional coefficients. When\na PDE is nonlinear and its ""pseudo-coefficient"" functions are state-dependent,\na gain-scheduling (GS) nonlinear design is the simplest approach to the design\nof nonlinear feedback. The GS version of PDE backstepping employs gains\nobtained by solving a PDE at each value of the state. Performing such PDE\ncomputations in real time may be prohibitive. The recently introduced neural\noperators (NO) can be trained to produce the gain functions, rapidly in real\ntime, for each state value, without requiring a PDE solution. In this paper we\nintroduce NOs for GS-PDE backstepping. GS controllers act on the premise that\nthe state change is slow and, as a result, guarantee only local stability, even\nfor ODEs. We establish local stabilization of hyperbolic PDEs with nonlinear\nrecirculation using both a ""full-kernel"" approach and the ""gain-only"" approach\nto gain operator approximation. Numerical simulations illustrate stabilization\nand demonstrate speedup by three orders of magnitude over traditional PDE\ngain-scheduling. Code (Github) for the numerical implementation is published to\nenable exploration.\n', ""  The recently introduced DeepONet operator-learning framework for PDE control\nis extended from the results for basic hyperbolic and parabolic PDEs to an\nadvanced hyperbolic class that involves delays on both the state and the system\noutput or input. The PDE backstepping design produces gain functions that are\noutputs of a nonlinear operator, mapping functions on a spatial domain into\nfunctions on a spatial domain, and where this gain-generating operator's inputs\nare the PDE's coefficients. The operator is approximated with a DeepONet neural\nnetwork to a degree of accuracy that is provably arbitrarily tight. Once we\nproduce this approximation-theoretic result in infinite dimension, with it we\nestablish stability in closed loop under feedback that employs approximate\ngains. In addition to supplying such results under full-state feedback, we also\ndevelop DeepONet-approximated observers and output-feedback laws and prove\ntheir own stabilizing properties under neural operator approximations. With\nnumerical simulations we illustrate the theoretical results and quantify the\nnumerical effort savings, which are of two orders of magnitude, thanks to\nreplacing the numerical PDE solving with the DeepONet.\n"", ""  To stabilize PDEs, feedback controllers require gain kernel functions, which\nare themselves governed by PDEs. Furthermore, these gain-kernel PDEs depend on\nthe PDE plants' functional coefficients. The functional coefficients in PDE\nplants are often unknown. This requires an adaptive approach to PDE control,\ni.e., an estimation of the plant coefficients conducted concurrently with\ncontrol, where a separate PDE for the gain kernel must be solved at each\ntimestep upon the update in the plant coefficient function estimate. Solving a\nPDE at each timestep is computationally expensive and a barrier to the\nimplementation of real-time adaptive control of PDEs. Recently, results in\nneural operator (NO) approximations of functional mappings have been introduced\ninto PDE control, for replacing the computation of the gain kernel with a\nneural network that is trained, once offline, and reused in real-time for rapid\nsolution of the PDEs. In this paper, we present the first result on applying\nNOs in adaptive PDE control, presented for a benchmark 1-D hyperbolic PDE with\nrecirculation. We establish global stabilization via Lyapunov analysis, in the\nplant and parameter error states, and also present an alternative approach, via\npassive identifiers, which avoids the strong assumptions on kernel\ndifferentiability. We then present numerical simulations demonstrating\nstability and observe speedups up to three orders of magnitude, highlighting\nthe real-time efficacy of neural operators in adaptive control. Our code\n(Github) is made publicly available for future researchers.\n""]",Neural Operators for PDE Control and Stabilization,Neural Networks for Modeling and Control of Dynamical Systems,Machine Learning for Dynamical Systems and Differential Equations
210,210,48,210_ai_automation_generative_prototyping,"['ai', 'automation', 'generative', 'prototyping', 'tools', 'programming', 'intelligence', 'tooling', 'genai', 'technology']","['business', 'automation', 'generative', 'technologies', 'research', 'writing', 'intelligence', 'researchers', 'artificial', 'construction']","[""  This workshop paper presents a critical examination of the integration of\nGenerative AI (Gen AI) into the academic writing process, focusing on the use\nof AI as a collaborative tool. It contrasts the performance and interaction of\ntwo AI models, Gemini and ChatGPT, through a collaborative inquiry approach\nwhere researchers engage in facilitated sessions to design prompts that elicit\nspecific AI responses for crafting research outlines. This case study\nhighlights the importance of prompt design, output analysis, and recognizing\nthe AI's limitations to ensure responsible and effective AI integration in\nscholarly work. Preliminary findings suggest that prompt variation\nsignificantly affects output quality and reveals distinct capabilities and\nconstraints of each model. The paper contributes to the field of Human-Computer\nInteraction by exploring effective prompt strategies and providing a\ncomparative analysis of Gen AI models, ultimately aiming to enhance AI-assisted\nacademic writing and prompt a deeper dialogue within the HCI community.\n"", ""  In the last decade, despite rapid advancements in artificial intelligence\n(AI) transforming many industry practices, construction largely lags in\nadoption. Recently, the emergence and rapid adoption of advanced large language\nmodels (LLM) like OpenAI's GPT, Google's PaLM, and Meta's Llama have shown\ngreat potential and sparked considerable global interest. However, the current\nsurge lacks a study investigating the opportunities and challenges of\nimplementing Generative AI (GenAI) in the construction sector, creating a\ncritical knowledge gap for researchers and practitioners. This underlines the\nnecessity to explore the prospects and complexities of GenAI integration.\nBridging this gap is fundamental to optimizing GenAI's early-stage adoption\nwithin the construction sector. Given GenAI's unprecedented capabilities to\ngenerate human-like content based on learning from existing content, we reflect\non two guiding questions: What will the future bring for GenAI in the\nconstruction industry? What are the potential opportunities and challenges in\nimplementing GenAI in the construction industry? This study delves into\nreflected perception in literature, analyzes the industry perception using\nprogramming-based word cloud and frequency analysis, and integrates authors'\nopinions to answer these questions. This paper recommends a conceptual GenAI\nimplementation framework, provides practical recommendations, summarizes future\nresearch questions, and builds foundational literature to foster subsequent\nresearch expansion in GenAI within the construction and its allied architecture\n& engineering domains.\n"", '  The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community.\n']",Generative AI Integration and Applications,Generative AI Applications and Implications,Generative Modeling and Artificial Intelligence
211,211,48,211_keyphrases_keyphrase_keywords_keyword,"['keyphrases', 'keyphrase', 'keywords', 'keyword', 'corpus', 'annotations', 'annotators', 'nlp', 'annotated', 'texts']","['keyphrase', 'keyphrases', 'classification', 'text', 'shot', 'extraction', 'annotation', 'language', 'corpus', 'keywords']","['  Keyphrase extraction (KPE) is an important task in Natural Language\nProcessing for many scenarios, which aims to extract keyphrases that are\npresent in a given document. Many existing supervised methods treat KPE as\nsequential labeling, span-level classification, or generative tasks. However,\nthese methods lack the ability to utilize keyphrase information, which may\nresult in biased results. In this study, we propose Diff-KPE, which leverages\nthe supervised Variational Information Bottleneck (VIB) to guide the text\ndiffusion process for generating enhanced keyphrase representations. Diff-KPE\nfirst generates the desired keyphrase embeddings conditioned on the entire\ndocument and then injects the generated keyphrase embeddings into each phrase\nrepresentation. A ranking network and VIB are then optimized together with rank\nloss and classification loss, respectively. This design of Diff-KPE allows us\nto rank each candidate phrase by utilizing both the information of keyphrases\nand the document. Experiments show that Diff-KPE outperforms existing KPE\nmethods on a large open domain keyphrase extraction benchmark, OpenKP, and a\nscientific domain dataset, KP20K.\n', '  Zero-shot keyphrase extraction aims to build a keyphrase extractor without\ntraining by human-annotated data, which is challenging due to the limited human\nintervention involved. Challenging but worthwhile, zero-shot setting\nefficiently reduces the time and effort that data labeling takes. Recent\nefforts on pre-trained large language models (e.g., ChatGPT and ChatGLM) show\npromising performance on zero-shot settings, thus inspiring us to explore\nprompt-based methods. In this paper, we ask whether strong keyphrase extraction\nmodels can be constructed by directly prompting the large language model\nChatGPT. Through experimental results, it is found that ChatGPT still has a lot\nof room for improvement in the keyphrase extraction task compared to existing\nstate-of-the-art unsupervised and supervised models.\n', '  Neural models that do not rely on pre-training have excelled in the keyphrase\ngeneration task with large annotated datasets. Meanwhile, new approaches have\nincorporated pre-trained language models (PLMs) for their data efficiency.\nHowever, there lacks a systematic study of how the two types of approaches\ncompare and how different design choices can affect the performance of\nPLM-based models. To fill in this knowledge gap and facilitate a more informed\nuse of PLMs for keyphrase extraction and keyphrase generation, we present an\nin-depth empirical study. Formulating keyphrase extraction as sequence labeling\nand keyphrase generation as sequence-to-sequence generation, we perform\nextensive experiments in three domains. After showing that PLMs have\ncompetitive high-resource performance and state-of-the-art low-resource\nperformance, we investigate important design choices including in-domain PLMs,\nPLMs with different pre-training objectives, using PLMs with a parameter\nbudget, and different formulations for present keyphrases. Further results show\nthat (1) in-domain BERT-like PLMs can be used to build strong and\ndata-efficient keyphrase generation models; (2) with a fixed parameter budget,\nprioritizing model depth over width and allocating more layers in the encoder\nleads to better encoder-decoder models; and (3) introducing four in-domain\nPLMs, we achieve a competitive performance in the news domain and the\nstate-of-the-art performance in the scientific domain.\n']",Keyphrase Extraction Methods,Information Extraction from Text,Information Extraction
212,212,48,212_uav_drones_drone_uavs,"['uav', 'drones', 'drone', 'uavs', 'unmanned', 'lidar', 'sensing', 'flying', 'quadrotor', 'aerial']","['aerial', 'drones', 'vehicles', 'inertial', 'navigation', 'unmanned', 'autonomous', 'drone', 'perception', 'sensor']","['  UAV tracking and pose estimation plays an imperative role in various\nUAV-related missions, such as formation control and anti-UAV measures.\nAccurately detecting and tracking UAVs in a 3D space remains a particularly\nchallenging problem, as it requires extracting sparse features of micro UAVs\nfrom different flight environments and continuously matching correspondences,\nespecially during agile flight. Generally, cameras and LiDARs are the two main\ntypes of sensors used to capture UAV trajectories in flight. However, both\nsensors have limitations in UAV classification and pose estimation. This\ntechnical report briefly introduces the method proposed by our team ""NTU-ICG""\nfor the CVPR 2024 UG2+ Challenge Track 5. This work develops a clustering-based\nlearning detection approach, CL-Det, for UAV tracking and pose estimation using\ntwo types of LiDARs, namely Livox Avia and LiDAR 360. We combine the\ninformation from the two data sources to locate drones in 3D. We first align\nthe timestamps of Livox Avia data and LiDAR 360 data and then separate the\npoint cloud of objects of interest (OOIs) from the environment. The point cloud\nof OOIs is clustered using the DBSCAN method, with the midpoint of the largest\ncluster assumed to be the UAV position. Furthermore, we utilize historical\nestimations to fill in missing data. The proposed method shows competitive pose\nestimation performance and ranks 5th on the final leaderboard of the CVPR 2024\nUG2+ Challenge.\n', '  Unmanned Aerial Vehicles (UAVs) have emerged as a transformative technology\nacross diverse sectors, offering adaptable solutions to complex challenges in\nboth military and civilian domains. Their expanding capabilities present a\nplatform for further advancement by integrating cutting-edge computational\ntools like Artificial Intelligence (AI) and Machine Learning (ML) algorithms.\nThese advancements have significantly impacted various facets of human life,\nfostering an era of unparalleled efficiency and convenience. Large Language\nModels (LLMs), a key component of AI, exhibit remarkable learning and\nadaptation capabilities within deployed environments, demonstrating an evolving\nform of intelligence with the potential to approach human-level proficiency.\nThis work explores the significant potential of integrating UAVs and LLMs to\npropel the development of autonomous systems. We comprehensively review LLM\narchitectures, evaluating their suitability for UAV integration. Additionally,\nwe summarize the state-of-the-art LLM-based UAV architectures and identify\nnovel opportunities for LLM embedding within UAV frameworks. Notably, we focus\non leveraging LLMs to refine data analysis and decision-making processes,\nspecifically for enhanced spectral sensing and sharing in UAV applications.\nFurthermore, we investigate how LLM integration expands the scope of existing\nUAV applications, enabling autonomous data processing, improved\ndecision-making, and faster response times in emergency scenarios like disaster\nresponse and network restoration. Finally, we highlight crucial areas for\nfuture research that are critical for facilitating the effective integration of\nLLMs and UAVs.\n', '  In the last twenty years, unmanned aerial vehicles (UAVs) have garnered\ngrowing interest due to their expanding applications in both military and\ncivilian domains. Detecting non-cooperative aerial vehicles with efficiency and\nestimating collisions accurately are pivotal for achieving fully autonomous\naircraft and facilitating Advanced Air Mobility (AAM). This paper presents a\ndeep-learning framework that utilizes optical sensors for the detection,\ntracking, and distance estimation of non-cooperative aerial vehicles. In\nimplementing this comprehensive sensing framework, the availability of depth\ninformation is essential for enabling autonomous aerial vehicles to perceive\nand navigate around obstacles. In this work, we propose a method for estimating\nthe distance information of a detected aerial object in real time using only\nthe input of a monocular camera. In order to train our deep learning components\nfor the object detection, tracking and depth estimation tasks we utilize the\nAmazon Airborne Object Tracking (AOT) Dataset. In contrast to previous\napproaches that integrate the depth estimation module into the object detector,\nour method formulates the problem as image-to-image translation. We employ a\nseparate lightweight encoder-decoder network for efficient and robust depth\nestimation. In a nutshell, the object detection module identifies and localizes\nobstacles, conveying this information to both the tracking module for\nmonitoring obstacle movement and the depth estimation module for calculating\ndistances. Our approach is evaluated on the Airborne Object Tracking (AOT)\ndataset which is the largest (to the best of our knowledge) air-to-air airborne\nobject dataset.\n']",UAV Tracking and Sensing,Unmanned Aerial Vehicle (UAV) Systems and Technologies,Aerial Robotics and Agricultural Computer Vision
213,213,47,213_depression_depressive_health_wellbeing,"['depression', 'depressive', 'health', 'wellbeing', 'sentiment', 'data', 'mental', 'anxiety', 'psychological', 'monitoring']","['depression', 'health', 'mental', 'sleep', 'physiological', 'clinical', 'wearable', 'participants', 'digital', 'disorder']","[""  Depression is a common disease worldwide. It is difficult to diagnose and\ncontinues to be underdiagnosed. Because depressed patients constantly share\ntheir symptoms, major life events, and treatments on social media, researchers\nare turning to user-generated digital traces on social media for depression\ndetection. Such methods have distinct advantages in combating depression\nbecause they can facilitate innovative approaches to fight depression and\nalleviate its social and economic burden. However, most existing studies lack\neffective means to incorporate established medical domain knowledge in\ndepression detection or suffer from feature extraction difficulties that impede\ngreater performance. Following the design science research paradigm, we propose\na Deep Knowledge-aware Depression Detection (DKDD) framework to accurately\ndetect social media users at risk of depression and explain the critical\nfactors that contribute to such detection. Extensive empirical studies with\nreal-world data demonstrate that, by incorporating domain knowledge, our method\noutperforms existing state-of-the-art methods. Our work has significant\nimplications for IS research in knowledge-aware machine learning, digital\ntraces utilization, and NLP research in IS. Practically, by providing early\ndetection and explaining the critical factors, DKDD can supplement clinical\ndepression screening and enable large-scale evaluations of a population's\nmental health status.\n"", '  This work explores the utilization of Romanized Sinhala social media data to\nidentify individuals at risk of depression. A machine learning-based framework\nis presented for the automatic screening of depression symptoms by analyzing\nlanguage patterns, sentiment, and behavioural cues within a comprehensive\ndataset of social media posts. The research has been carried out to compare the\nsuitability of Neural Networks over the classical machine learning techniques.\nThe proposed Neural Network with an attention layer which is capable of\nhandling long sequence data, attains a remarkable accuracy of 93.25% in\ndetecting depression symptoms, surpassing current state-of-the-art methods.\nThese findings underscore the efficacy of this approach in pinpointing\nindividuals in need of proactive interventions and support. Mental health\nprofessionals, policymakers, and social media companies can gain valuable\ninsights through the proposed model. Leveraging natural language processing\ntechniques and machine learning algorithms, this work offers a promising\npathway for mental health screening in the digital era. By harnessing the\npotential of social media data, the framework introduces a proactive method for\nrecognizing and assisting individuals at risk of depression. In conclusion,\nthis research contributes to the advancement of proactive interventions and\nsupport systems for mental health, thereby influencing both research and\npractical applications in the field.\n', '  We introduce a multi-layer perceptron (MLP) called the COVID-19 Depression\nand Anxiety Predictor (CoDAP) to predict mental health trends, particularly\nanxiety and depression, during the COVID-19 pandemic. Our method utilizes a\ncomprehensive dataset, which tracked mental health symptoms weekly over ten\nweeks during the initial COVID-19 wave (April to June 2020) in a diverse cohort\nof U.S. adults. This period, characterized by a surge in mental health symptoms\nand conditions, offers a critical context for our analysis. Our focus was to\nextract and analyze patterns of anxiety and depression through a unique lens of\nqualitative individual attributes using CoDAP. This model not only predicts\npatterns of anxiety and depression during the pandemic but also unveils key\ninsights into the interplay of demographic factors, behavioral changes, and\nsocial determinants of mental health. These findings contribute to a more\nnuanced understanding of the complexity of mental health issues in times of\nglobal health crises, potentially guiding future early interventions.\n']",Depression Detection using Social Media Data,Depression Detection using AI and Multimodal Data,Human Behavior Analysis with AI and Multimodal Data
214,214,47,214_sketches_sketching_sketch_sketchime,"['sketches', 'sketching', 'sketch', 'sketchime', 'sketchinr', 'drawings', 'drawing', 'cadyface', 'abstraction', '3d']","['sketch', 'sketches', 'facial', 'editing', 'style', 'stylization', 'portrait', 'images', 'strokes', 'face']","['  This paper, for the first time, marries large foundation models with human\nsketch understanding. We demonstrate what this brings -- a paradigm shift in\nterms of generalised sketch representation learning (e.g., classification).\nThis generalisation happens on two fronts: (i) generalisation across unknown\ncategories (i.e., open-set), and (ii) generalisation traversing abstraction\nlevels (i.e., good and bad sketches), both being timely challenges that remain\nunsolved in the sketch literature. Our design is intuitive and centred around\ntransferring the already stellar generalisation ability of CLIP to benefit\ngeneralised learning for sketches. We first ""condition"" the vanilla CLIP model\nby learning sketch-specific prompts using a novel auxiliary head of raster to\nvector sketch conversion. This importantly makes CLIP ""sketch-aware"". We then\nmake CLIP acute to the inherently different sketch abstraction levels. This is\nachieved by learning a codebook of abstraction-specific prompt biases, a\nweighted combination of which facilitates the representation of sketches across\nabstraction levels -- low abstract edge-maps, medium abstract sketches in\nTU-Berlin, and highly abstract doodles in QuickDraw. Our framework surpasses\npopular sketch representation learning algorithms in both zero-shot and\nfew-shot setups and in novel settings across different abstraction boundaries.\n', ""  We present SENS, a novel method for generating and editing 3D models from\nhand-drawn sketches, including those of abstract nature. Our method allows\nusers to quickly and easily sketch a shape, and then maps the sketch into the\nlatent space of a part-aware neural implicit shape architecture. SENS analyzes\nthe sketch and encodes its parts into ViT patch encoding, subsequently feeding\nthem into a transformer decoder that converts them to shape embeddings suitable\nfor editing 3D neural implicit shapes. SENS provides intuitive sketch-based\ngeneration and editing, and also succeeds in capturing the intent of the user's\nsketch to generate a variety of novel and expressive 3D shapes, even from\nabstract and imprecise sketches. Additionally, SENS supports refinement via\npart reconstruction, allowing for nuanced adjustments and artifact removal. It\nalso offers part-based modeling capabilities, enabling the combination of\nfeatures from multiple sketches to create more complex and customized 3D\nshapes. We demonstrate the effectiveness of our model compared to the\nstate-of-the-art using objective metric evaluation criteria and a user study,\nboth indicating strong performance on sketches with a medium level of\nabstraction. Furthermore, we showcase our method's intuitive sketch-based shape\nediting capabilities, and validate it through a usability study.\n"", '  Recently, text-to-3D approaches have achieved high-fidelity 3D content\ngeneration using text description. However, the generated objects are\nstochastic and lack fine-grained control. Sketches provide a cheap approach to\nintroduce such fine-grained control. Nevertheless, it is challenging to achieve\nflexible control from these sketches due to their abstraction and ambiguity. In\nthis paper, we present a multi-view sketch-guided text-to-3D generation\nframework (namely, Sketch2NeRF) to add sketch control to 3D generation.\nSpecifically, our method leverages pretrained 2D diffusion models (e.g., Stable\nDiffusion and ControlNet) to supervise the optimization of a 3D scene\nrepresented by a neural radiance field (NeRF). We propose a novel synchronized\ngeneration and reconstruction method to effectively optimize the NeRF. In the\nexperiments, we collected two kinds of multi-view sketch datasets to evaluate\nthe proposed method. We demonstrate that our method can synthesize 3D\nconsistent contents with fine-grained sketch control while being high-fidelity\nto text prompts. Extensive results show that our method achieves\nstate-of-the-art performance in terms of sketch similarity and text alignment.\n']",Sketch-based 3D Modeling and Representation Learning,3D Geometry and Shape Representation Learning,Geometric and Equivariant Deep Learning
215,215,46,215_narratives_storytelling_narrative_writing,"['narratives', 'storytelling', 'narrative', 'writing', 'linguistic', 'conversations', 'stories', 'summaries', 'texts', 'discourse']","['stories', 'narrative', 'social', 'human', 'narratives', 'writing', 'texts', 'norms', 'summaries', 'humans']","['  Empathy serves as a cornerstone in enabling prosocial behaviors, and can be\nevoked through sharing of personal experiences in stories. While empathy is\ninfluenced by narrative content, intuitively, people respond to the way a story\nis told as well, through narrative style. Yet the relationship between empathy\nand narrative style is not fully understood. In this work, we empirically\nexamine and quantify this relationship between style and empathy using LLMs and\nlarge-scale crowdsourcing studies. We introduce a novel, theory-based taxonomy,\nHEART (Human Empathy and Narrative Taxonomy) that delineates elements of\nnarrative style that can lead to empathy with the narrator of a story. We\nestablish the performance of LLMs in extracting narrative elements from HEART,\nshowing that prompting with our taxonomy leads to reasonable, human-level\nannotations beyond what prior lexicon-based methods can do. To show empirical\nuse of our taxonomy, we collect a dataset of empathy judgments of stories via a\nlarge-scale crowdsourcing study with N=2,624 participants. We show that\nnarrative elements extracted via LLMs, in particular, vividness of emotions and\nplot volume, can elucidate the pathways by which narrative style cultivates\nempathy towards personal stories. Our work suggests that such models can be\nused for narrative analyses that lead to human-centered social and behavioral\ninsights.\n', ""  This research explores the nuanced differences in texts produced by AI and\nthose written by humans, aiming to elucidate how language is expressed\ndifferently by AI and humans. Through comprehensive statistical data analysis,\nthe study investigates various linguistic traits, patterns of creativity, and\npotential biases inherent in human-written and AI- generated texts. The\nsignificance of this research lies in its contribution to understanding AI's\ncreative capabilities and its impact on literature, communication, and societal\nframeworks. By examining a meticulously curated dataset comprising 500K essays\nspanning diverse topics and genres, generated by LLMs, or written by humans,\nthe study uncovers the deeper layers of linguistic expression and provides\ninsights into the cognitive processes underlying both AI and human-driven\ntextual compositions. The analysis revealed that human-authored essays tend to\nhave a higher total word count on average than AI-generated essays but have a\nshorter average word length compared to AI- generated essays, and while both\ngroups exhibit high levels of fluency, the vocabulary diversity of Human\nauthored content is higher than AI generated content. However, AI- generated\nessays show a slightly higher level of novelty, suggesting the potential for\ngenerating more original content through AI systems. The paper addresses\nchallenges in assessing the language generation capabilities of AI models and\nemphasizes the importance of datasets that reflect the complexities of human-AI\ncollaborative writing. Through systematic preprocessing and rigorous\nstatistical analysis, this study offers valuable insights into the evolving\nlandscape of AI-generated content and informs future developments in natural\nlanguage processing (NLP).\n"", ""  Evaluations of creative stories generated by large language models (LLMs)\noften focus on objective properties of the text, such as its style, coherence,\nand toxicity. While these metrics are indispensable, they do not speak to a\nstory's subjective, psychological impact from a reader's perspective. We\nintroduce the Psychological Depth Scale (PDS), a novel framework rooted in\nliterary theory that measures an LLM's ability to produce authentic and\nnarratively complex stories that provoke emotion, empathy, and engagement. We\nempirically validate our framework by showing that humans can consistently\nevaluate stories based on PDS (0.72 Krippendorff's alpha). We also explore\ntechniques for automating the PDS to easily scale future analyses. GPT-4o,\ncombined with a novel Mixture-of-Personas (MoP) prompting strategy, achieves an\naverage Spearman correlation of $0.51$ with human judgment while Llama-3-70B\nscores as high as 0.68 for empathy. Finally, we compared the depth of stories\nauthored by both humans and LLMs. Surprisingly, GPT-4 stories either surpassed\nor were statistically indistinguishable from highly-rated human-written stories\nsourced from Reddit. By shifting the focus from text to reader, the\nPsychological Depth Scale is a validated, automated, and systematic means of\nmeasuring the capacity of LLMs to connect with humans through the stories they\ntell.\n""]",Narrative Analysis and Storytelling,Narrative Generation and Analysis in Games and Interactive Media,Artificial Intelligence for Creative Content Generation
216,216,46,216_mimo_5g_wireless_cellular,"['mimo', '5g', 'wireless', 'cellular', 'antennas', 'mmwave', 'transmits', 'pilot', 'antenna', 'channel']","['beam', 'pilot', 'channel', 'wireless', 'coverage', 'aerial', 'vehicular', 'power', 'network', 'beams']","['  Massive MIMO is expected to play an important role in the development of 5G\nnetworks. This paper addresses the issue of pilot contamination and scalability\nin massive MIMO systems. The current practice of reusing orthogonal pilot\nsequences in adjacent cells leads to difficulty in differentiating incoming\ninter- and intra-cell pilot sequences. One possible solution is to increase the\nnumber of orthogonal pilot sequences, which results in dedicating more space of\ncoherence block to pilot transmission than data transmission. This, in turn,\nalso hinders the scalability of massive MIMO systems, particularly in\naccommodating a large number of IoT devices within a cell. To overcome these\nchallenges, this paper devises an innovative pilot allocation scheme based on\nthe data transfer patterns of IoT devices. The scheme assigns orthogonal pilot\nsequences to clusters of devices instead of individual devices, allowing\nmultiple devices to utilize the same pilot for periodically transmitting data.\nMoreover, we formulate the pilot assignment problem as a graph coloring problem\nand use the max k-cut graph partitioning approach to overcome the pilot\ncontamination in a multicell massive MIMO system. The proposed scheme\nsignificantly improves the spectral efficiency and enables the scalability of\nmassive MIMO systems; for instance, by using ten orthogonal pilot sequences, we\nare able to accommodate 200 devices with only a 12.5% omission rate.\n', '  Millimeter wave (mmWave) communications can potentially meet the high\ndata-rate requirements of unmanned aerial vehicle (UAV) networks. However, as\nthe prerequisite of mmWave communications, the narrow directional beam tracking\nis very challenging because of the three-dimensional (3D) mobility and attitude\nvariation of UAVs. Aiming to address the beam tracking difficulties, we propose\nto integrate the conformal array (CA) with the surface of each UAV, which\nenables the full spatial coverage and the agile beam tracking in highly dynamic\nUAV mmWave networks. More specifically, the key contributions of our work are\nthree-fold. 1) A new mmWave beam tracking framework is established for the\nCA-enabled UAV mmWave network. 2) A specialized hierarchical codebook is\nconstructed to drive the directional radiating element (DRE)-covered\ncylindrical conformal array (CCA), which contains both the angular beam pattern\nand the subarray pattern to fully utilize the potential of the CA. 3) A\ncodebook-based multiuser beam tracking scheme is proposed, where the Gaussian\nprocess machine learning enabled UAV position/attitude predication is developed\nto improve the beam tracking efficiency in conjunction with the tracking-error\naware adaptive beamwidth control. Simulation results validate the effectiveness\nof the proposed codebook-based beam tracking scheme in the CA-enabled UAV\nmmWave network, and demonstrate the advantages of CA over the conventional\nplanner array in terms of spectrum efficiency and outage probability in the\nhighly dynamic scenarios.\n', '  5G sets the foundation for an era of creativity with its faster speeds,\nincreased data throughput, reduced latency, and enhanced IoT connectivity, all\nenabled by Massive MIMO (M-MIMO) technology. M-MIMO boosts network efficiency\nand enhances user experience by employing intelligent user scheduling. This\npaper presents a user scheduling scheme and pilot assignment strategy designed\nfor IoT devices, emphasizing mitigating pilot contamination, a key obstacle to\nimproving spectral efficiency (SE) and system scalability in M-MIMO networks.\nWe utilize a user clustering-based pilot allocation scheme to boost IoT device\nscalability in M-MIMO systems. Additionally, our smart pilot allocation\nminimizes interference and enhances SE by treating pilot assignment as a graph\ncoloring problem, optimizing it through integer linear programming (ILP).\nRecognizing the computational complexity of ILP, we introduced a binary\nsearch-based heuristic predicated on interference threshold to expedite the\ncomputation, while maintaining a near-optimal solution. The simulation results\nshow a significant decrease in the required pilot overhead (about 17%), and\nsubstantial enhancement in SE (about 8-14%).\n']",Massive MIMO and mmWave for 5G Networks,Advanced Wireless Communication Technologies,Wireless Technologies and Sensing Systems
217,217,46,217_optimality_optimal_optimize_optimization,"['optimality', 'optimal', 'optimize', 'optimization', 'risk', 'regret', 'stochastic', 'predict', 'predictive', 'portfolios']","['portfolio', 'optimization', 'dominance', 'stochastic', 'decision', 'risk', 'optimal', 'regret', 'loss', 'uncertainty']","['  Optimization models used to make discrete decisions often contain uncertain\nparameters that are context-dependent and estimated through prediction. To\naccount for the quality of the decision made based on the prediction,\ndecision-focused learning (end-to-end predict-then-optimize) aims at training\nthe predictive model to minimize regret, i.e., the loss incurred by making a\nsuboptimal decision. Despite the challenge of the gradient of this loss w.r.t.\nthe predictive model parameters being zero almost everywhere for optimization\nproblems with a linear objective, effective gradient-based learning approaches\nhave been proposed to minimize the expected loss, using the empirical loss as a\nsurrogate. However, empirical regret can be an ineffective surrogate because\nempirical optimal decisions can vary substantially from expected optimal\ndecisions. To understand the impact of this deficiency, we evaluate the effect\nof aleatoric and epistemic uncertainty on the accuracy of empirical regret as a\nsurrogate. Next, we propose three novel loss functions that approximate\nexpected regret more robustly. Experimental results show that training two\nstate-of-the-art decision-focused learning approaches using robust regret\nlosses improves test-sample empirical regret in general while keeping\ncomputational time equivalent relative to the number of training epochs.\n', '  Stochastic dominance models risk-averse preferences for decision making with\nuncertain outcomes, which naturally captures the intrinsic structure of the\nunderlying uncertainty, in contrast to simply resorting to the expectations.\nDespite theoretically appealing, the application of stochastic dominance in\nmachine learning has been scarce, due to the following challenges:\n$\\textbf{i)}$, the original concept of stochastic dominance only provides a\n$\\textit{partial order}$, therefore, is not amenable to serve as an optimality\ncriterion; and $\\textbf{ii)}$, an efficient computational recipe remains\nlacking due to the continuum nature of evaluating stochastic dominance.%, which\nbarriers its application for machine learning.\n  In this work, we make the first attempt towards establishing a general\nframework of learning with stochastic dominance. We first generalize the\nstochastic dominance concept to enable feasible comparisons between any\narbitrary pair of random variables. We next develop a simple and\ncomputationally efficient approach for finding the optimal solution in terms of\nstochastic dominance, which can be seamlessly plugged into many learning tasks.\nNumerical experiments demonstrate that the proposed method achieves comparable\nperformance as standard risk-neutral strategies and obtains better trade-offs\nagainst risk across a variety of applications including supervised learning,\nreinforcement learning, and portfolio optimization.\n', '  Many real-world optimization problems contain parameters that are unknown\nbefore deployment time, either due to stochasticity or to lack of information\n(e.g., demand or travel times in delivery problems). A common strategy in such\ncases is to estimate said parameters via machine learning (ML) models trained\nto minimize the prediction error, which however is not necessarily aligned with\nthe downstream task-level error. The decision-focused learning (DFL) paradigm\novercomes this limitation by training to directly minimize a task loss, e.g.\nregret. Since the latter has non-informative gradients for combinatorial\nproblems, state-of-the-art DFL methods introduce surrogates and approximations\nthat enable training. But these methods exploit specific assumptions about the\nproblem structures (e.g., convex or linear problems, unknown parameters only in\nthe objective function). We propose an alternative method that makes no such\nassumptions, it combines stochastic smoothing with score function gradient\nestimation which works on any task loss. This opens up the use of DFL methods\nto nonlinear objectives, uncertain parameters in the problem constraints, and\neven two-stage stochastic optimization. Experiments show that it typically\nrequires more epochs, but that it is on par with specialized methods and\nperforms especially well for the difficult case of problems with uncertainty in\nthe constraints, in terms of solution quality, scalability, or both.\n']",Optimization under Uncertainty and Risk,Optimization Methods and Algorithms,Optimization and Design
218,218,46,218_compositionality_compositional_compositionally_compositions,"['compositionality', 'compositional', 'compositionally', 'compositions', 'composition', 'generalization', 'neural', 'representations', 'generalize', 'cognition']","['compositional', 'digit', 'arithmetic', 'generalization', 'numbers', 'composition', 'multiplication', 'compositionality', 'operations', 'input']","['  Compositional learning, mastering the ability to combine basic concepts and\nconstruct more intricate ones, is crucial for human cognition, especially in\nhuman language comprehension and visual perception. This notion is tightly\nconnected to generalization over unobserved situations. Despite its integral\nrole in intelligence, there is a lack of systematic theoretical and\nexperimental research methodologies, making it difficult to analyze the\ncompositional learning abilities of computational models. In this paper, we\nsurvey the literature on compositional learning of AI models and the\nconnections made to cognitive studies. We identify abstract concepts of\ncompositionality in cognitive and linguistic studies and connect these to the\ncomputational challenges faced by language and vision models in compositional\nreasoning. We overview the formal definitions, tasks, evaluation benchmarks,\nvariety of computational models, and theoretical findings. We cover modern\nstudies on large language models to provide a deeper understanding of the\ncutting-edge compositional capabilities exhibited by state-of-the-art AI models\nand pinpoint important directions for future research.\n', '  Compositional generalization (the ability to respond correctly to novel\ncombinations of familiar components) is thought to be a cornerstone of\nintelligent behavior. Compositionally structured (e.g. disentangled)\nrepresentations are essential for this; however, the conditions under which\nthey yield compositional generalization remain unclear. To address this gap, we\npresent a general theory of compositional generalization in kernel models with\nfixed, potentially nonlinear representations (which also applies to neural\nnetworks in the ""lazy regime""). We prove that these models are functionally\nlimited to adding up values assigned to conjunctions/combinations of components\nthat have been seen during training (""conjunction-wise additivity""), and\nidentify novel compositionality failure modes that arise from the data and\nmodel structure, even for disentangled inputs. For models in the representation\nlearning (or ""rich"") regime, we show that networks can generalize on an\nimportant non-additive task (associative inference), and give a mechanistic\nexplanation for why. Finally, we validate our theory empirically, showing that\nit captures the behavior of deep neural networks trained on a set of\ncompositional tasks. In sum, our theory characterizes the principles giving\nrise to compositional generalization in kernel models and shows how\nrepresentation learning can overcome their limitations. We further provide a\nformally grounded, novel generalization class for compositional tasks that\nhighlights fundamental differences in the required learning mechanisms\n(conjunction-wise additivity).\n', '  Compositionality is thought to be a key component of language, and various\ncompositional benchmarks have been developed to empirically probe the\ncompositional generalization of existing sequence processing models. These\nbenchmarks often highlight failures of existing models, but it is not clear why\nthese models fail in this way. In this paper, we seek to theoretically\nunderstand the role the compositional structure of the models plays in these\nfailures and how this structure relates to their expressivity and sample\ncomplexity. We propose a general neuro-symbolic definition of compositional\nfunctions and their compositional complexity. We then show how various existing\ngeneral and special purpose sequence processing models (such as recurrent,\nconvolution and attention-based ones) fit this definition and use it to analyze\ntheir compositional complexity. Finally, we provide theoretical guarantees for\nthe expressivity and systematic generalization of compositional models that\nexplicitly depend on our proposed definition and highlighting factors which\ndrive poor empirical performance.\n']",Compositional Learning and Generalization in AI Models,Machine Learning Foundations and Generalization,Machine Learning and Artificial Intelligence
219,219,45,219_bias_biases_learning_neural,"['bias', 'biases', 'learning', 'neural', 'deeppoly', 'benchmark', 'debiasing', 'classifier', 'generalization', 'networks']","['spurious', 'test', 'bias', 'deep', 'robustness', 'correlations', 'biases', 'testing', 'selection', 'training']","['  Despite the rapid development of machine learning algorithms for domain\ngeneralization (DG), there is no clear empirical evidence that the existing DG\nalgorithms outperform the classic empirical risk minimization (ERM) across\nstandard benchmarks. To better understand this phenomenon, we investigate\nwhether there are benefits of DG algorithms over ERM through the lens of label\nnoise. Specifically, our finite-sample analysis reveals that label noise\nexacerbates the effect of spurious correlations for ERM, undermining\ngeneralization. Conversely, we illustrate that DG algorithms exhibit implicit\nlabel-noise robustness during finite-sample training even when spurious\ncorrelation is present. Such desirable property helps mitigate spurious\ncorrelations and improve generalization in synthetic experiments. However,\nadditional comprehensive experiments on real-world benchmark datasets indicate\nthat label-noise robustness does not necessarily translate to better\nperformance compared to ERM. We conjecture that the failure mode of ERM arising\nfrom spurious correlations may be less pronounced in practice.\n', ""  Neural networks trained with (stochastic) gradient descent have an inductive\nbias towards learning simpler solutions. This makes them highly prone to\nlearning spurious correlations in the training data, that may not hold at test\ntime. In this work, we provide the first theoretical analysis of the effect of\nsimplicity bias on learning spurious correlations. Notably, we show that\nexamples with spurious features are provably separable based on the model's\noutput early in training. We further illustrate that if spurious features have\na small enough noise-to-signal ratio, the network's output on the majority of\nexamples is almost exclusively determined by the spurious features, leading to\npoor worst-group test accuracy. Finally, we propose SPARE, which identifies\nspurious correlations early in training and utilizes importance sampling to\nalleviate their effect. Empirically, we demonstrate that SPARE outperforms\nstate-of-the-art methods by up to 21.1% in worst-group accuracy, while being up\nto 12x faster. We also show that SPARE is a highly effective but lightweight\nmethod to discover spurious correlations.\n"", '  Existing research often posits spurious features as easier to learn than core\nfeatures in neural network optimization, but the impact of their relative\nsimplicity remains under-explored. Moreover, studies mainly focus on end\nperformance rather than the learning dynamics of feature learning. In this\npaper, we propose a theoretical framework and an associated synthetic dataset\ngrounded in boolean function analysis. This setup allows for fine-grained\ncontrol over the relative complexity (compared to core features) and\ncorrelation strength (with respect to the label) of spurious features to study\nthe dynamics of feature learning under spurious correlations. Our findings\nuncover several interesting phenomena: (1) stronger spurious correlations or\nsimpler spurious features slow down the learning rate of the core features, (2)\ntwo distinct subnetworks are formed to learn core and spurious features\nseparately, (3) learning phases of spurious and core features are not always\nseparable, (4) spurious features are not forgotten even after core features are\nfully learned. We demonstrate that our findings justify the success of\nretraining the last layer to remove spurious correlation and also identifies\nlimitations of popular debiasing algorithms that exploit early learning of\nspurious features. We support our empirical findings with theoretical analyses\nfor the case of learning XOR features with a one-hidden-layer ReLU network.\n']",Spurious Correlations in Neural Network Learning,Mitigating Spurious Correlations in Machine Learning,Machine Learning Reliability and Uncertainty
220,220,45,220_softmax_regularization_overfitting_underfitting,"['softmax', 'regularization', 'overfitting', 'underfitting', 'neural', 'classifiers', 'deep', 'memorization', 'networks', 'trained']","['softmax', 'loss', 'mislabeled', 'distribution', 'label', 'labels', 'noisy', 'classifiers', 'clean', 'deep']","['  We present a latent variable model for classification that provides a novel\nprobabilistic interpretation of neural network softmax classifiers. We derive a\nvariational objective to train the model, analogous to the evidence lower bound\n(ELBO) used to train variational auto-encoders, that generalises the softmax\ncross-entropy loss. Treating inputs to the softmax layer as samples of a latent\nvariable, our abstracted perspective reveals a potential inconsistency between\ntheir anticipated distribution, required for accurate label predictions, and\ntheir empirical distribution found in practice. We augment the variational\nobjective to mitigate such inconsistency and induce a chosen latent\ndistribution, instead of the implicit assumption found in a standard softmax\nlayer. Overall, we provide new theoretical insight into the inner workings of\nwidely-used softmax classifiers. Empirical evaluation on image and text\nclassification datasets demonstrates that our proposed approach, variational\nclassification, maintains classification accuracy while the reshaped latent\nspace improves other desirable properties of a classifier, such as calibration,\nadversarial robustness, robustness to distribution shift and sample efficiency\nuseful in low data settings.\n', '  Overfitting commonly occurs when applying deep neural networks (DNNs) on\nsmall-scale datasets, where DNNs do not generalize well from existing data to\nunseen data. The main reason resulting in overfitting is that small-scale\ndatasets cannot reflect the situations of the real world. Label smoothing (LS)\nis an effective regularization method to prevent overfitting, avoiding it by\nmixing one-hot labels with uniform label vectors. However, LS only focuses on\nlabels while ignoring the distribution of existing data. In this paper, we\nintroduce the distributionally robust optimization (DRO) to LS, achieving shift\nthe existing data distribution flexibly to unseen domains when training DNNs.\nSpecifically, we prove that the regularization of LS can be extended to a\nregularization term for the DNNs parameters when integrating DRO. The\nregularization term can be utilized to shift existing data to unseen domains\nand generate new data. Furthermore, we propose an approximate\ngradient-iteration label smoothing algorithm (GI-LS) to achieve the findings\nand train DNNs. We prove that the shift for the existing data does not\ninfluence the convergence of GI-LS. Since GI-LS incorporates a series of\nhyperparameters, we further consider using Bayesian optimization (BO) to find\nthe relatively optimal combinations of these hyperparameters. Taking\nsmall-scale anomaly classification tasks as a case, we evaluate GI-LS, and the\nresults clearly demonstrate its superior performance.\n', '  Given data with noisy labels, over-parameterized deep networks suffer\noverfitting mislabeled data, resulting in poor generalization. The memorization\neffect of deep networks shows that although the networks have the ability to\nmemorize all noisy data, they would first memorize clean training data, and\nthen gradually memorize mislabeled training data. A simple and effective method\nthat exploits the memorization effect to combat noisy labels is early stopping.\nHowever, early stopping cannot distinguish the memorization of clean data and\nmislabeled data, resulting in the network still inevitably overfitting\nmislabeled data in the early training stage.In this paper, to decouple the\nmemorization of clean data and mislabeled data, and further reduce the side\neffect of mislabeled data, we perform additive decomposition on network\nparameters. Namely, all parameters are additively decomposed into two groups,\ni.e., parameters $\\mathbf{w}$ are decomposed as\n$\\mathbf{w}=\\bm{\\sigma}+\\bm{\\gamma}$. Afterward, the parameters $\\bm{\\sigma}$\nare considered to memorize clean data, while the parameters $\\bm{\\gamma}$ are\nconsidered to memorize mislabeled data. Benefiting from the memorization\neffect, the updates of the parameters $\\bm{\\sigma}$ are encouraged to fully\nmemorize clean data in early training, and then discouraged with the increase\nof training epochs to reduce interference of mislabeled data. The updates of\nthe parameters $\\bm{\\gamma}$ are the opposite. In testing, only the parameters\n$\\bm{\\sigma}$ are employed to enhance generalization. Extensive experiments on\nboth simulated and real-world benchmarks confirm the superior performance of\nour method.\n']",Regularization Techniques for Deep Neural Networks,Deep Learning Optimization Techniques,Deep Learning Optimization and Training
221,221,45,221_programming_interpreter_solvers_automate,"['programming', 'interpreter', 'solvers', 'automate', 'solver', 'programs', 'code', 'tools', 'maxmind', 'optimization']","['programming', 'programs', 'problems', 'optimization', 'code', 'anthem', 'language', 'solutions', 'natural', 'logic']","['  Optimization problems are pervasive in sectors from manufacturing and\ndistribution to healthcare. However, most such problems are still solved\nheuristically by hand rather than optimally by state-of-the-art solvers because\nthe expertise required to formulate and solve these problems limits the\nwidespread adoption of optimization tools and techniques. This paper introduces\nOptiMUS, a Large Language Model (LLM)-based agent designed to formulate and\nsolve (mixed integer) linear programming problems from their natural language\ndescriptions. OptiMUS can develop mathematical models, write and debug solver\ncode, evaluate the generated solutions, and improve its model and code based on\nthese evaluations. OptiMUS utilizes a modular structure to process problems,\nallowing it to handle problems with long descriptions and complex data without\nlong prompts. Experiments demonstrate that OptiMUS outperforms existing\nstate-of-the-art methods on easy datasets by more than $20\\%$ and on hard\ndatasets (including a new dataset, NLP4LP, released with this paper that\nfeatures long and complex problems) by more than $30\\%$.\n', ""  Large language models (LLMs) have exhibited their problem-solving ability in\nmathematical reasoning. Solving realistic optimization (OPT) problems in\nindustrial application scenarios requires advanced and applied math ability.\nHowever, current OPT benchmarks that merely solve linear programming are far\nfrom complex realistic situations. In this work, we propose E-OPT, a benchmark\nfor end-to-end optimization problem-solving with human-readable inputs and\noutputs. E-OPT contains rich optimization problems, including linear/nonlinear\nprogramming with/without table data, which can comprehensively evaluate LLMs'\nsolving ability. In our benchmark, LLMs are required to correctly understand\nthe problem in E-OPT and call code solver to get precise numerical answers.\nFurthermore, to alleviate the data scarcity for optimization problems, and to\nbridge the gap between open-source LLMs on a small scale (e.g., Llama-2-7b and\nLlama-3-8b) and closed-source LLMs (e.g., GPT-4), we further propose a novel\ndata synthesis method namely ReSocratic. Unlike general data synthesis methods\nthat proceed from questions to answers, ReSocratic first incrementally\nsynthesizes optimization scenarios with mathematical formulations step by step\nand then back-translates the generated scenarios into questions. In such a way,\nwe construct the ReSocratic-29k dataset from a small seed sample pool with the\npowerful open-source large model DeepSeek-V2. To demonstrate the effectiveness\nof ReSocratic, we conduct supervised fine-tuning with ReSocratic-29k on\nmultiple open-source models. The results show that Llama3-8b is significantly\nimproved from 13.6% to 51.7% on E-OPT, while DeepSeek-V2 reaches 61.0%,\napproaching 65.5% of GPT-4.\n"", '  Optimization problems are pervasive in sectors from manufacturing and\ndistribution to healthcare. However, most such problems are still solved\nheuristically by hand rather than optimally by state-of-the art solvers because\nthe expertise required to formulate and solve these problems limits the\nwidespread adoption of optimization tools and techniques. We introduce a Large\nLanguage Model (LLM)-based system designed to formulate and solve (mixed\ninteger) linear programming problems from their natural language descriptions.\nOur system is capable of developing mathematical models, writing and debugging\nsolver code, evaluating the generated solutions, and improving efficiency and\ncorrectness of its model and code based on these evaluations. OptiMUS-0.3\nutilizes a modular structure to process problems, allowing it to handle\nproblems with long descriptions and complex data without long prompts.\nExperiments demonstrate that OptiMUS-0.3 outperforms existing state-of-the-art\nmethods on easy datasets by more than 12% and on hard datasets (including a new\ndataset, NLP4LP, released with this paper that features long and complex\nproblems) by more than 8%.\n']",Optimization Problem Solving with Large Language Models,Optimization and Efficiency of Large Language Models,Large Language Models
222,222,45,222_cultures_cultural_culturellm_culturebank,"['cultures', 'cultural', 'culturellm', 'culturebank', 'multilingual', 'culture', 'multicultural', 'culturally', 'language', 'culturepark']","['cultural', 'cultures', 'languages', 'culture', 'translation', 'linguistic', 'language', 'communities', 'commonsense', 'resource']","[""  Recent studies have highlighted the presence of cultural biases in Large\nLanguage Models (LLMs), yet often lack a robust methodology to dissect these\nphenomena comprehensively. Our work aims to bridge this gap by delving into the\nFood domain, a universally relevant yet culturally diverse aspect of human\nlife. We introduce FmLAMA, a multilingual dataset centered on food-related\ncultural facts and variations in food practices. We analyze LLMs across various\narchitectures and configurations, evaluating their performance in both\nmonolingual and multilingual settings. By leveraging templates in six different\nlanguages, we investigate how LLMs interact with language-specific and cultural\nknowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias\ntowards food knowledge prevalent in the United States; (2) Incorporating\nrelevant cultural context significantly improves LLMs' ability to access\ncultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is\nhighly dependent on the interplay between the probing language, the specific\nmodel architecture, and the cultural context in question. This research\nunderscores the complexity of integrating cultural understanding into LLMs and\nemphasizes the importance of culturally diverse datasets to mitigate biases and\nenhance model performance across different cultural domains.\n"", '  LLMs are increasingly being deployed for multilingual applications and have\ndemonstrated impressive translation capabilities between several low and high\nresource languages. An aspect of translation that often gets overlooked is that\nof cultural adaptation, or modifying source culture references to suit the\ntarget culture. Cultural adaptation has applications across several creative\nindustries and requires intimate knowledge of source and target cultures during\ntranslation. While specialized translation models still outperform LLMs on the\nmachine translation task when viewed from the lens of correctness, they are not\nsensitive to cultural differences often requiring manual correction. LLMs on\nthe other hand have a rich reservoir of cultural knowledge embedded within its\nparameters that can be potentially exploited for such applications. In this\npaper we define the task of cultural adaptation and create an evaluation\nframework to benchmark different models for this task. We evaluate the\nperformance of modern LLMs for cultural adaptation and analyze their cross\ncultural knowledge while connecting related concepts across different cultures.\nWe also analyze possible issues with automatic adaptation including cultural\nbiases and stereotypes. We hope that this task will offer more insight into the\ncultural understanding of LLMs and their creativity in cross-cultural\nscenarios.\n', ""  The deployment of large language models (LLMs) raises concerns regarding\ntheir cultural misalignment and potential ramifications on individuals and\nsocieties with diverse cultural backgrounds. While the discourse has focused\nmainly on political and social biases, our research proposes a Cultural\nAlignment Test (Hoftede's CAT) to quantify cultural alignment using Hofstede's\ncultural dimension framework, which offers an explanatory cross-cultural\ncomparison through the latent variable analysis. We apply our approach to\nquantitatively evaluate LLMs, namely Llama 2, GPT-3.5, and GPT-4, against the\ncultural dimensions of regions like the United States, China, and Arab\ncountries, using different prompting styles and exploring the effects of\nlanguage-specific fine-tuning on the models' behavioural tendencies and\ncultural values. Our results quantify the cultural alignment of LLMs and reveal\nthe difference between LLMs in explanatory cultural dimensions. Our study\ndemonstrates that while all LLMs struggle to grasp cultural values, GPT-4 shows\na unique capability to adapt to cultural nuances, particularly in Chinese\nsettings. However, it faces challenges with American and Arab cultures. The\nresearch also highlights that fine-tuning LLama 2 models with different\nlanguages changes their responses to cultural questions, emphasizing the need\nfor culturally diverse development in AI for worldwide acceptance and ethical\nuse. For more details or to contribute to this research, visit our GitHub page\nhttps://github.com/reemim/Hofstedes_CAT/\n""]",Cultural Understanding in Multilingual Language Models,Cultural Understanding in AI Models,Artificial Intelligence and Machine Learning Interpretability and Explainability
223,223,44,223_entities_entity_semantic_annotated,"['entities', 'entity', 'semantic', 'annotated', 'corpus', 'referential', 'corpora', 'hypernym', 'embeddings', 'retrieval']","['entity', 'entities', 'linking', 'ontology', 'taxonomies', 'biomedical', 'taxonomy', 'tabular', 'typing', 'seed']","['  Accurately typing entity mentions from text segments is a fundamental task\nfor various natural language processing applications. Many previous approaches\nrely on massive human-annotated data to perform entity typing. Nevertheless,\ncollecting such data in highly specialized science and engineering domains\n(e.g., software engineering and security) can be time-consuming and costly,\nwithout mentioning the domain gaps between training and inference data if the\nmodel needs to be applied to confidential datasets. In this paper, we study the\ntask of seed-guided fine-grained entity typing in science and engineering\ndomains, which takes the name and a few seed entities for each entity type as\nthe only supervision and aims to classify new entity mentions into both seen\nand unseen types (i.e., those without seed entities). To solve this problem, we\npropose SEType which first enriches the weak supervision by finding more\nentities for each seen type from an unlabeled corpus using the contextualized\nrepresentations of pre-trained language models. It then matches the enriched\nentities to unlabeled text to get pseudo-labeled samples and trains a textual\nentailment model that can make inferences for both seen and unseen types.\nExtensive experiments on two datasets covering four domains demonstrate the\neffectiveness of SEType in comparison with various baselines.\n', '  Entity Set Expansion (ESE) is a critical task aiming at expanding entities of\nthe target semantic class described by seed entities. Most existing ESE methods\nare retrieval-based frameworks that need to extract contextual features of\nentities and calculate the similarity between seed entities and candidate\nentities. To achieve the two purposes, they iteratively traverse the corpus and\nthe entity vocabulary, resulting in poor efficiency and scalability.\nExperimental results indicate that the time consumed by the retrieval-based ESE\nmethods increases linearly with entity vocabulary and corpus size. In this\npaper, we firstly propose Generative Entity Set Expansion (GenExpan) framework,\nwhich utilizes a generative pre-trained auto-regressive language model to\naccomplish ESE task. Specifically, a prefix tree is employed to guarantee the\nvalidity of entity generation, and automatically generated class names are\nadopted to guide the model to generate target entities. Moreover, we propose\nKnowledge Calibration and Generative Ranking to further bridge the gap between\ngeneric knowledge of the language model and the goal of ESE task. For\nefficiency, expansion time consumed by GenExpan is independent of entity\nvocabulary and corpus size, and GenExpan achieves an average 600% speedup\ncompared to strong baselines. For expansion effectiveness, our framework\noutperforms previous state-of-the-art ESE methods.\n', '  Entity disambiguation (ED), which links the mentions of ambiguous entities to\ntheir referent entities in a knowledge base, serves as a core component in\nentity linking (EL). Existing generative approaches demonstrate improved\naccuracy compared to classification approaches under the standardized ZELDA\nbenchmark. Nevertheless, generative approaches suffer from the need for\nlarge-scale pre-training and inefficient generation. Most importantly, entity\ndescriptions, which could contain crucial information to distinguish similar\nentities from each other, are often overlooked. We propose an encoder-decoder\nmodel to disambiguate entities with more detailed entity descriptions. Given\ntext and candidate entities, the encoder learns interactions between the text\nand each candidate entity, producing representations for each entity candidate.\nThe decoder then fuses the representations of entity candidates together and\nselects the correct entity. Our experiments, conducted on various entity\ndisambiguation benchmarks, demonstrate the strong and robust performance of\nthis model, particularly +1.5% in the ZELDA benchmark compared with GENRE.\nFurthermore, we integrate this approach into the retrieval/reader framework and\nobserve +1.5% improvements in end-to-end entity linking in the GERBIL benchmark\ncompared with EntQA.\n']",Entity Typing and Disambiguation,Entity Understanding and Resolution,Entity Understanding and Semantic Knowledge Integration
224,224,44,224_gans_gan_microscopy_dcgan,"['gans', 'gan', 'microscopy', 'dcgan', 'generative', 'cgan', 'autoencoders', 'imaging', 'adversarial', 'microscope']","['image', 'images', 'microscopy', 'generative', 'resolution', 'adversarial', 'imaging', 'material', 'autoencoder', 'confocal']","[""  Modelling the impact of a material's mesostructure on device level\nperformance typically requires access to 3D image data containing all the\nrelevant information to define the geometry of the simulation domain. This\nimage data must include sufficient contrast between phases to distinguish each\nmaterial, be of high enough resolution to capture the key details, but also\nhave a large enough field-of-view to be representative of the material in\ngeneral. It is rarely possible to obtain data with all of these properties from\na single imaging technique. In this paper, we present a method for combining\ninformation from pairs of distinct but complementary imaging techniques in\norder to accurately reconstruct the desired multi-phase, high resolution,\nrepresentative, 3D images. Specifically, we use deep convolutional generative\nadversarial networks to implement super-resolution, style transfer and\ndimensionality expansion. To demonstrate the widespread applicability of this\ntool, two pairs of datasets are used to validate the quality of the volumes\ngenerated by fusing the information from paired imaging techniques. Three key\nmesostructural metrics are calculated in each case to show the accuracy of this\nmethod. Having confidence in the accuracy of our method, we then demonstrate\nits power by applying to a real data pair from a lithium ion battery electrode,\nwhere the required 3D high resolution image data is not available anywhere in\nthe literature. We believe this approach is superior to previously reported\nstatistical material reconstruction methods both in terms of its fidelity and\nease of use. Furthermore, much of the data required to train this algorithm\nalready exists in the literature, waiting to be combined. As such, our\nopen-access code could precipitate a step change by generating the hard to\nobtain high quality image volumes necessary to simulate behaviour at the\nmesoscale.\n"", '  In this paper, an image recognition algorithm based on the combination of\ndeep learning and generative adversarial network (GAN) is studied, and compared\nwith traditional image recognition methods. The purpose of this study is to\nevaluate the advantages and application prospects of deep learning technology,\nespecially GAN, in the field of image recognition. Firstly, this paper reviews\nthe basic principles and techniques of traditional image recognition methods,\nincluding the classical algorithms based on feature extraction such as SIFT,\nHOG and their combination with support vector machine (SVM), random forest, and\nother classifiers. Then, the working principle, network structure, and unique\nadvantages of GAN in image generation and recognition are introduced. In order\nto verify the effectiveness of GAN in image recognition, a series of\nexperiments are designed and carried out using multiple public image data sets\nfor training and testing. The experimental results show that compared with\ntraditional methods, GAN has excellent performance in processing complex\nimages, recognition accuracy, and anti-noise ability. Specifically, Gans are\nbetter able to capture high-dimensional features and details of images,\nsignificantly improving recognition performance. In addition, Gans shows unique\nadvantages in dealing with image noise, partial missing information, and\ngenerating high-quality images.\n', '  Confocal fluorescence microscopy is one of the most accessible and widely\nused imaging techniques for the study of biological processes. Scanning\nconfocal microscopy allows the capture of high-quality images from 3D samples,\nyet suffers from well-known limitations such as photobleaching and\nphototoxicity of specimens caused by intense light exposure, which limits its\nuse in some applications, especially for living cells. Cellular damage can be\nalleviated by changing imaging parameters to reduce light exposure, often at\nthe expense of image quality. Machine/deep learning methods for single-image\nsuper-resolution (SISR) can be applied to restore image quality by upscaling\nlower-resolution (LR) images to produce high-resolution images (HR). These SISR\nmethods have been successfully applied to photo-realistic images due partly to\nthe abundance of publicly available data. In contrast, the lack of publicly\navailable data partly limits their application and success in scanning confocal\nmicroscopy. In this paper, we introduce a large scanning confocal microscopy\ndataset named SR-CACO-2 that is comprised of low- and high-resolution image\npairs marked for three different fluorescent markers. It allows the evaluation\nof performance of SISR methods on three different upscaling levels (X2, X4,\nX8). SR-CACO-2 contains the human epithelial cell line Caco-2 (ATCC HTB-37),\nand it is composed of 22 tiles that have been translated in the form of 9,937\nimage patches for experiments with SISR methods. Given the new SR-CACO-2\ndataset, we also provide benchmarking results for 15 state-of-the-art methods\nthat are representative of the main SISR families. Results show that these\nmethods have limited success in producing high-resolution textures, indicating\nthat SR-CACO-2 represents a challenging problem. Our dataset, code and\npretrained weights are available: https://github.com/sbelharbi/sr-caco-2.\n']",Generative Adversarial Networks for Imaging and Microscopy,Generative Adversarial Networks (GANs) and Their Applications,Generative Modeling and Artificial Intelligence
225,225,43,225_tutoring_tutors_grading_students,"['tutoring', 'tutors', 'grading', 'students', 'learnersourcing', 'pedagogical', 'automating', 'instructional', 'assessment', 'feedback']","['students', 'student', 'questions', 'feedback', 'educational', 'tutoring', 'grading', 'education', 'instructor', 'answers']","[""  Automatically generating feedback via large language models (LLMs) in\nintelligent tutoring systems and online learning platforms has the potential to\nimprove the learning outcomes of many students. However, both feedback\ngeneration and evaluation are challenging: feedback content has to be valid\nespecially in subjects like math, which requires models to understand the\nproblem, the solution, and where the student's error lies. Feedback also has to\nbe pedagogically valid to reflect effective tutoring strategies, such as\nexplaining possible misconceptions and encouraging the student, among other\ndesirable features. In this work, we address both problems of automatically\ngenerating and evaluating feedback while considering both correctness and\nalignment. First, we propose a rubric for evaluating math feedback and show\nthat GPT-4 is able to effectively use it to annotate human-written and\nLLM-generated feedback. Second, we propose a framework for feedback generation\nthat optimizes both correctness and alignment using reinforcement learning\n(RL). Specifically, we use GPT-4's annotations to create preferences over\nfeedback pairs in an augmented dataset for training via direct preference\noptimization (DPO). We show that our methods significantly increase the\ncorrectness and alignment of generated feedback with Llama 2, an open-source\nLLM, qualitatively analyze our generation and evaluation systems using case\nstudies, and outline several areas for future work.\n"", ""  Assessing student's answers and in particular natural language answers is a\ncrucial challenge in the field of education. Advances in machine learning,\nincluding transformer-based models such as Large Language Models(LLMs), have\nled to significant progress in various natural language tasks. Nevertheless,\namidst the growing trend of evaluating LLMs across diverse tasks, evaluating\nLLMs in the realm of automated answer assesment has not received much\nattention. To address this gap, we explore the potential of using LLMs for\nautomated assessment of student's short and open-ended answer. Particularly, we\nuse LLMs to compare students' explanations with expert explanations in the\ncontext of line-by-line explanations of computer programs.\n  For comparison purposes, we assess both Large Language Models (LLMs) and\nencoder-based Semantic Textual Similarity (STS) models in the context of\nassessing the correctness of students' explanation of computer code. Our\nfindings indicate that LLMs, when prompted in few-shot and chain-of-thought\nsetting perform comparable to fine-tuned encoder-based models in evaluating\nstudents' short answers in programming domain.\n"", '  Intelligent Tutoring Systems (ITSs) often contain an automated feedback\ncomponent, which provides a predefined feedback message to students when they\ndetect a predefined error. To such a feedback component, we often resort to\ntemplate-based approaches. These approaches require significant effort from\nhuman experts to detect a limited number of possible student errors and provide\ncorresponding feedback. This limitation is exemplified in open-ended math\nquestions, where there can be a large number of different incorrect errors. In\nour work, we examine the capabilities of large language models (LLMs) to\ngenerate feedback for open-ended math questions, similar to that of an\nestablished ITS that uses a template-based approach. We fine-tune both\nopen-source and proprietary LLMs on real student responses and corresponding\nITS-provided feedback. We measure the quality of the generated feedback using\ntext similarity metrics. We find that open-source and proprietary models both\nshow promise in replicating the feedback they see during training, but do not\ngeneralize well to previously unseen student errors. These results suggest that\ndespite being able to learn the formatting of feedback, LLMs are not able to\nfully understand mathematical errors made by students.\n']",Automated Feedback in Intelligent Tutoring Systems,Artificial Intelligence in Education,Artificial Intelligence in Education
226,226,43,226_ai_healthcare_bias_ethical,"['ai', 'healthcare', 'bias', 'ethical', 'ethics', 'fairness', 'outcomes', 'biases', 'intelligence', 'medical']","['healthcare', 'fairness', 'health', 'explanations', 'medical', 'clinical', 'patients', 'artificial', 'intelligence', 'bias']","['  Artificial intelligence (AI) is rapidly advancing in healthcare, enhancing\nthe efficiency and effectiveness of services across various specialties,\nincluding cardiology, ophthalmology, dermatology, emergency medicine, etc. AI\napplications have significantly improved diagnostic accuracy, treatment\npersonalization, and patient outcome predictions by leveraging technologies\nsuch as machine learning, neural networks, and natural language processing.\nHowever, these advancements also introduce substantial ethical and fairness\nchallenges, particularly related to biases in data and algorithms. These biases\ncan lead to disparities in healthcare delivery, affecting diagnostic accuracy\nand treatment outcomes across different demographic groups. This survey paper\nexamines the integration of AI in healthcare, highlighting critical challenges\nrelated to bias and exploring strategies for mitigation. We emphasize the\nnecessity of diverse datasets, fairness-aware algorithms, and regulatory\nframeworks to ensure equitable healthcare delivery. The paper concludes with\nrecommendations for future research, advocating for interdisciplinary\napproaches, transparency in AI decision-making, and the development of\ninnovative and inclusive AI applications.\n', '  Objectives: Leveraging artificial intelligence (AI) in conjunction with\nelectronic health records (EHRs) holds transformative potential to improve\nhealthcare. Yet, addressing bias in AI, which risks worsening healthcare\ndisparities, cannot be overlooked. This study reviews methods to detect and\nmitigate diverse forms of bias in AI models developed using EHR data. Methods:\nWe conducted a systematic review following the Preferred Reporting Items for\nSystematic Reviews and Meta-analyses (PRISMA) guidelines, analyzing articles\nfrom PubMed, Web of Science, and IEEE published between January 1, 2010, and\nDec 17, 2023. The review identified key biases, outlined strategies for\ndetecting and mitigating bias throughout the AI model development process, and\nanalyzed metrics for bias assessment. Results: Of the 450 articles retrieved,\n20 met our criteria, revealing six major bias types: algorithmic, confounding,\nimplicit, measurement, selection, and temporal. The AI models were primarily\ndeveloped for predictive tasks in healthcare settings. Four studies\nconcentrated on the detection of implicit and algorithmic biases employing\nfairness metrics like statistical parity, equal opportunity, and predictive\nequity. Sixty proposed various strategies for mitigating biases, especially\ntargeting implicit and selection biases. These strategies, evaluated through\nboth performance (e.g., accuracy, AUROC) and fairness metrics, predominantly\ninvolved data collection and preprocessing techniques like resampling,\nreweighting, and transformation. Discussion: This review highlights the varied\nand evolving nature of strategies to address bias in EHR-based AI models,\nemphasizing the urgent needs for the establishment of standardized,\ngeneralizable, and interpretable methodologies to foster the creation of\nethical AI systems that promote fairness and equity in healthcare.\n', '  The ethical integration of Artificial Intelligence (AI) in healthcare\nnecessitates addressing fairness-a concept that is highly context-specific\nacross medical fields. Extensive studies have been conducted to expand the\ntechnical components of AI fairness, while tremendous calls for AI fairness\nhave been raised from healthcare. Despite this, a significant disconnect\npersists between technical advancements and their practical clinical\napplications, resulting in a lack of contextualized discussion of AI fairness\nin clinical settings. Through a detailed evidence gap analysis, our review\nsystematically pinpoints several deficiencies concerning both healthcare data\nand the provided AI fairness solutions. We highlight the scarcity of research\non AI fairness in many medical domains where AI technology is increasingly\nutilized. Additionally, our analysis highlights a substantial reliance on group\nfairness, aiming to ensure equality among demographic groups from a macro\nhealthcare system perspective; in contrast, individual fairness, focusing on\nequity at a more granular level, is frequently overlooked. To bridge these\ngaps, our review advances actionable strategies for both the healthcare and AI\nresearch communities. Beyond applying existing AI fairness methods in\nhealthcare, we further emphasize the importance of involving healthcare\nprofessionals to refine AI fairness concepts and methods to ensure contextually\nrelevant and ethically sound AI applications in healthcare.\n']",AI Ethics in Healthcare: Bias and Fairness,Artificial Intelligence Ethics and Governance,Artificial Intelligence Applications and Implications
227,227,43,227_knowledge_wikidata_entities_provenance,"['knowledge', 'wikidata', 'entities', 'provenance', 'rdf', 'iconology', 'iconological', 'concepts', 'organizational', 'information']","['knowledge', 'graphs', 'manufacturing', 'graph', 'company', 'ecosystem', 'statements', 'service', 'management', 'entities']","['  Knowledge Representation (KR) and facet-analytical Knowledge Organization\n(KO) have been the two most prominent methodologies of data and knowledge\nmodelling in the Artificial Intelligence community and the Information Science\ncommunity, respectively. KR boasts of a robust and scalable ecosystem of\ntechnologies to support knowledge modelling while, often, underemphasizing the\nquality of its models (and model-based data). KO, on the other hand, is less\ntechnology-driven but has developed a robust framework of guiding principles\n(canons) for ensuring modelling (and model-based data) quality. This paper\nelucidates both the KR and facet-analytical KO methodologies in detail and\nprovides a functional mapping between them. Out of the mapping, the paper\nproposes an integrated KO-enriched KR methodology with all the standard\ncomponents of a KR methodology plus the guiding canons of modelling quality\nprovided by KO. The practical benefits of the methodological integration has\nbeen exemplified through a prominent case study of KR-based image annotation\nexercise.\n', ""  Sourcing and identification of new manufacturing partners is crucial for\nmanufacturing system integrators to enhance agility and reduce risk through\nsupply chain diversification in the global economy. The advent of advanced\nlarge language models has captured significant interest, due to their ability\nto generate comprehensive and articulate responses across a wide range of\nknowledge domains. However, the system often falls short in accuracy and\ncompleteness when responding to domain-specific inquiries, particularly in\nareas like manufacturing service discovery. This research explores the\npotential of leveraging Knowledge Graphs in conjunction with ChatGPT to\nstreamline the process for prospective clients in identifying small\nmanufacturing enterprises. In this study, we propose a method that integrates\nbottom-up ontology with advanced machine learning models to develop a\nManufacturing Service Knowledge Graph from an array of structured and\nunstructured data sources, including the digital footprints of small-scale\nmanufacturers throughout North America. The Knowledge Graph and the learned\ngraph embedding vectors are leveraged to tackle intricate queries within the\ndigital supply chain network, responding with enhanced reliability and greater\ninterpretability. The approach highlighted is scalable to millions of entities\nthat can be distributed to form a global Manufacturing Service Knowledge\nNetwork Graph that can potentially interconnect multiple types of Knowledge\nGraphs that span industry sectors, geopolitical boundaries, and business\ndomains. The dataset developed for this study, now publicly accessible,\nencompasses more than 13,000 manufacturers' weblinks, manufacturing services,\ncertifications, and location entity types.\n"", '  Knowledge Organization (KO) and Knowledge Representation (KR) have been the\ntwo mainstream methodologies of knowledge modelling in the Information Science\ncommunity and the Artificial Intelligence community, respectively. The\nfacet-analytical tradition of KO has developed an exhaustive set of guiding\ncanons for ensuring quality in organising and managing knowledge but has\nremained limited in terms of technology-driven activities to expand its scope\nand services beyond the bibliographic universe of knowledge. KR, on the other\nhand, boasts of a robust ecosystem of technologies and technology-driven\nservice design which can be tailored to model any entity or scale to any\nservice in the entire universe of knowledge. This paper elucidates both the\nfacet-analytical KO and KR methodologies in detail and provides a functional\nmapping between them. Out of the mapping, the paper proposes an integrated\nKR-enriched KO methodology with all the standard components of a KO methodology\nplus the advanced technologies provided by the KR approach. The practical\nbenefits of the methodological integration has been exemplified through the\nflagship application of the Digital University at the University of Trento,\nItaly.\n']",Knowledge Representation and Organization Methodologies,Knowledge Representation and Organization Methodologies,Artificial Intelligence and Cognitive Systems
228,228,43,228_ai_intelligence_chatbots_iq,"['ai', 'intelligence', 'chatbots', 'iq', 'language', 'turing', 'robot', 'agent', 'behavioral', 'assessment']","['intelligence', 'human', 'test', 'social', 'values', 'mental', 'humans', 'students', 'behavior', 'awareness']","['  Large Language Models have shown exceptional generative abilities in various\nnatural language and generation tasks. However, possible anthropomorphization\nand leniency towards failure cases have propelled discussions on emergent\nabilities of Large Language Models especially on Theory of Mind (ToM) abilities\nin Large Language Models. While several false-belief tests exists to verify the\nability to infer and maintain mental models of another entity, we study a\nspecial application of ToM abilities that has higher stakes and possibly\nirreversible consequences : Human Robot Interaction. In this work, we explore\nthe task of Perceived Behavior Recognition, where a robot employs a Large\nLanguage Model (LLM) to assess the robot\'s generated behavior in a manner\nsimilar to human observer. We focus on four behavior types, namely -\nexplicable, legible, predictable, and obfuscatory behavior which have been\nextensively used to synthesize interpretable robot behaviors. The LLMs goal is,\ntherefore to be a human proxy to the agent, and to answer how a certain agent\nbehavior would be perceived by the human in the loop, for example ""Given a\nrobot\'s behavior X, would the human observer find it explicable?"". We conduct a\nhuman subject study to verify that the users are able to correctly answer such\na question in the curated situations (robot setting and plan) across five\ndomains. A first analysis of the belief test yields extremely positive results\ninflating ones expectations of LLMs possessing ToM abilities. We then propose\nand perform a suite of perturbation tests which breaks this illusion, i.e.\nInconsistent Belief, Uninformative Context and Conviction Test. We conclude\nthat, the high score of LLMs on vanilla prompts showcases its potential use in\nHRI settings, however to possess ToM demands invariance to trivial or\nirrelevant perturbations in the context which LLMs lack.\n', '  With the release of ChatGPT and other large language models (LLMs) the\ndiscussion about the intelligence, possibilities, and risks, of current and\nfuture models have seen large attention. This discussion included much debated\nscenarios about the imminent rise of so-called ""super-human"" AI, i.e., AI\nsystems that are orders of magnitude smarter than humans. In the spirit of Alan\nTuring, there is no doubt that current state-of-the-art language models already\npass his famous test. Moreover, current models outperform humans in several\nbenchmark tests, so that publicly available LLMs have already become versatile\ncompanions that connect everyday life, industry and science. Despite their\nimpressive capabilities, LLMs sometimes fail completely at tasks that are\nthought to be trivial for humans. In other cases, the trustworthiness of LLMs\nbecomes much more elusive and difficult to evaluate. Taking the example of\nacademia, language models are capable of writing convincing research articles\non a given topic with only little input. Yet, the lack of trustworthiness in\nterms of factual consistency or the existence of persistent hallucinations in\nAI-generated text bodies has led to a range of restrictions for AI-based\ncontent in many scientific journals. In view of these observations, the\nquestion arises as to whether the same metrics that apply to human intelligence\ncan also be applied to computational methods and has been discussed\nextensively. In fact, the choice of metrics has already been shown to\ndramatically influence assessments on potential intelligence emergence. Here,\nwe argue that the intelligence of LLMs should not only be assessed by\ntask-specific statistical metrics, but separately in terms of qualitative and\nquantitative measures.\n', ""  Theory of Mind (ToM) refers to the ability to attribute mental states, such\nas beliefs, desires, intentions, and knowledge, to oneself and others, and to\nunderstand that these mental states can differ from one's own and from reality.\nWe investigate ToM in environments with multiple, distinct, independent AI\nagents, each possessing unique internal states, information, and objectives.\nInspired by human false-belief experiments, we present an AI ('focal AI') with\na scenario where its clone undergoes a human-centric ToM assessment. We prompt\nthe focal AI to assess whether its clone would benefit from additional\ninstructions. Concurrently, we give its clones the ToM assessment, both with\nand without the instructions, thereby engaging the focal AI in higher-order\ncounterfactual reasoning akin to human mentalizing--with respect to humans in\none test and to other AI in another. We uncover a discrepancy: Contemporary AI\ndemonstrates near-perfect accuracy on human-centric ToM assessments. Since\ninformation embedded in one AI is identically embedded in its clone, additional\ninstructions are redundant. Yet, we observe AI crafting elaborate instructions\nfor their clones, erroneously anticipating a need for assistance. An\nindependent referee AI agrees with these unsupported expectations. Neither the\nfocal AI nor the referee demonstrates ToM in our 'silico-centric' test.\n""]",Assessing Theory of Mind in Large Language Models,Large Language Models and Cognitive Abilities,Large Language Models
229,229,42,229_reviews_review_ratings_sentiment,"['reviews', 'review', 'ratings', 'sentiment', 'spam', 'classifier', 'language', 'texts', 'sentences', 'helpfulness']","['reviews', 'review', 'opinion', 'fake', 'commerce', 'customer', 'spam', 'summarization', 'product', 'summaries']","['  The proliferation of fake reviews on various online platforms has created a\nmajor concern for both consumers and businesses. Such reviews can deceive\ncustomers and cause damage to the reputation of products or services, making it\ncrucial to identify them. Although the detection of fake reviews has been\nextensively studied in English language, detecting fake reviews in non-English\nlanguages such as Bengali is still a relatively unexplored research area. This\npaper introduces the Bengali Fake Review Detection (BFRD) dataset, the first\npublicly available dataset for identifying fake reviews in Bengali. The dataset\nconsists of 7710 non-fake and 1339 fake food-related reviews collected from\nsocial media posts. To convert non-Bengali words in a review, a unique pipeline\nhas been proposed that translates English words to their corresponding Bengali\nmeaning and also back transliterates Romanized Bengali to Bengali. We have\nconducted rigorous experimentation using multiple deep learning and pre-trained\ntransformer language models to develop a reliable detection system. Finally, we\npropose a weighted ensemble model that combines four pre-trained transformers:\nBanglaBERT, BanglaBERT Base, BanglaBERT Large, and BanglaBERT Generator .\nAccording to the experiment results, the proposed ensemble model obtained a\nweighted F1-score of 0.9843 on 13390 reviews, including 1339 actual fake\nreviews and 5356 augmented fake reviews generated with the nlpaug library. The\nremaining 6695 reviews were randomly selected from the 7710 non-fake instances.\nThe model achieved a 0.9558 weighted F1-score when the fake reviews were\naugmented using the bnaug library.\n', ""  Product review generation is an important task in recommender systems, which\ncould provide explanation and persuasiveness for the recommendation. Recently,\nLarge Language Models (LLMs, e.g., ChatGPT) have shown superior text modeling\nand generating ability, which could be applied in review generation. However,\ndirectly applying the LLMs for generating reviews might be troubled by the\n``polite'' phenomenon of the LLMs and could not generate personalized reviews\n(e.g., negative reviews). In this paper, we propose Review-LLM that customizes\nLLMs for personalized review generation. Firstly, we construct the prompt input\nby aggregating user historical behaviors, which include corresponding item\ntitles and reviews. This enables the LLMs to capture user interest features and\nreview writing style. Secondly, we incorporate ratings as indicators of\nsatisfaction into the prompt, which could further improve the model's\nunderstanding of user preferences and the sentiment tendency control of\ngenerated reviews. Finally, we feed the prompt text into LLMs, and use\nSupervised Fine-Tuning (SFT) to make the model generate personalized reviews\nfor the given user and target item. Experimental results on the real-world\ndataset show that our fine-tuned model could achieve better review generation\nperformance than existing close-source LLMs.\n"", '  Online commerce relies heavily on user generated reviews to provide unbiased\ninformation about products that they have not physically seen. The importance\nof reviews has attracted multiple exploitative online behaviours and requires\nmethods for monitoring and detecting reviews. We present a machine learning\nmethodology for review detection and extraction, and demonstrate that it\ngeneralises for use across websites that were not contained in the training\ndata. This method promises to drive applications for automatic detection and\nevaluation of reviews, regardless of their source. Furthermore, we showcase the\nversatility of our method by implementing and discussing three key applications\nfor analysing reviews: Sentiment Inconsistency Analysis, which detects and\nfilters out unreliable reviews based on inconsistencies between ratings and\ncomments; Multi-language support, enabling the extraction and translation of\nreviews from various languages without relying on HTML scraping; and Fake\nreview detection, achieved by integrating a trained NLP model to identify and\ndistinguish between genuine and fake reviews.\n']",Fake Review Detection and Analysis,Misinformation and Disinformation Detection,Information Verification and Validation
230,230,42,230_3d_scenecraft_scenes_uni3d,"['3d', 'scenecraft', 'scenes', 'uni3d', 'interactive', 'embodied', 'visual', 'robotic', 'scene', 'robot']","['scene', 'scenes', 'objects', 'indoor', 'environments', 'environment', 'procedural', 'assets', 'layout', 'editing']","['  The integration of language and 3D perception is crucial for developing\nembodied agents and robots that comprehend and interact with the physical\nworld. While large language models (LLMs) have demonstrated impressive language\nunderstanding and generation capabilities, their adaptation to 3D environments\n(3D-LLMs) remains in its early stages. A primary challenge is the absence of\nlarge-scale datasets that provide dense grounding between language and 3D\nscenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset\ncomprising 40,087 household scenes paired with 6.2 million densely-grounded\nscene-language instructions. Our results show that instruction tuning with\n3D-GRAND significantly enhances grounding capabilities and reduces\nhallucinations in 3D-LLMs. As part of our contributions, we propose a\ncomprehensive benchmark 3D-POPE to systematically evaluate hallucination in\n3D-LLMs, enabling fair comparisons among future models. Our experiments\nhighlight a scaling effect between dataset size and 3D-LLM performance,\nemphasizing the critical role of large-scale 3D-text datasets in advancing\nembodied AI research. Notably, our results demonstrate early signals for\neffective sim-to-real transfer, indicating that models trained on large\nsynthetic data can perform well on real-world 3D scans. Through 3D-GRAND and\n3D-POPE, we aim to equip the embodied AI community with essential resources and\ninsights, setting the stage for more reliable and better-grounded 3D-LLMs.\nProject website: https://3d-grand.github.io\n', '  3D vision-language grounding, which focuses on aligning language with the 3D\nphysical environment, stands as a cornerstone in the development of embodied\nagents. In comparison to recent advancements in the 2D domain, grounding\nlanguage in 3D scenes faces several significant challenges: (i) the inherent\ncomplexity of 3D scenes due to the diverse object configurations, their rich\nattributes, and intricate relationships; (ii) the scarcity of paired 3D\nvision-language data to support grounded learning; and (iii) the absence of a\nunified learning framework to distill knowledge from grounded 3D data. In this\nwork, we aim to address these three major challenges in 3D vision-language by\nexamining the potential of systematically upscaling 3D vision-language learning\nin indoor environments. We introduce the first million-scale 3D vision-language\ndataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising\n2.5M vision-language pairs derived from both human annotations and our scalable\nscene-graph-based generation approach. We demonstrate that this scaling allows\nfor a unified pre-training framework, Grounded Pre-training for Scenes (GPS),\nfor 3D vision-language learning. Through extensive experiments, we showcase the\neffectiveness of GPS by achieving state-of-the-art performance on all existing\n3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is\nunveiled through zero-shot transfer experiments in the challenging 3D\nvision-language tasks. Project website: https://scene-verse.github.io.\n', ""  This paper introduces Scene-LLM, a 3D-visual-language model that enhances\nembodied agents' abilities in interactive 3D indoor environments by integrating\nthe reasoning strengths of Large Language Models (LLMs). Scene-LLM adopts a\nhybrid 3D visual feature representation, that incorporates dense spatial\ninformation and supports scene state updates. The model employs a projection\nlayer to efficiently project these features in the pre-trained textual\nembedding space, enabling effective interpretation of 3D visual information.\nUnique to our approach is the integration of both scene-level and ego-centric\n3D information. This combination is pivotal for interactive planning, where\nscene-level data supports global planning and ego-centric data is important for\nlocalization. Notably, we use ego-centric 3D frame features for feature\nalignment, an efficient technique that enhances the model's ability to align\nfeatures of small objects within the scene. Our experiments with Scene-LLM\ndemonstrate its strong capabilities in dense captioning, question answering,\nand interactive planning. We believe Scene-LLM advances the field of 3D visual\nunderstanding and reasoning, offering new possibilities for sophisticated agent\ninteractions in indoor settings.\n""]",3D Vision-Language Grounding for Embodied Agents,Vision-Language Models and Multimodal Learning,Multimodal Learning and Vision-Language Models
231,231,41,231_summarizing_summarization_summaries_abstracts,"['summarizing', 'summarization', 'summaries', 'abstracts', 'systematic', 'pubmed', 'metadata', 'summary', 'text', 'automate']","['summarization', 'summaries', 'screening', 'medical', 'claims', 'evidence', 'clinical', 'doctor', 'literature', 'systematic']","['  Systematic reviews are crucial for evidence-based medicine as they\ncomprehensively analyse published research findings on specific questions.\nConducting such reviews is often resource- and time-intensive, especially in\nthe screening phase, where abstracts of publications are assessed for inclusion\nin a review. This study investigates the effectiveness of using zero-shot large\nlanguage models~(LLMs) for automatic screening. We evaluate the effectiveness\nof eight different LLMs and investigate a calibration technique that uses a\npredefined recall threshold to determine whether a publication should be\nincluded in a systematic review. Our comprehensive evaluation using five\nstandard test collections shows that instruction fine-tuning plays an important\nrole in screening, that calibration renders LLMs practical for achieving a\ntargeted recall, and that combining both with an ensemble of zero-shot models\nsaves significant screening time compared to state-of-the-art approaches.\n', '  Community Question-Answering (CQA) forums have revolutionized how people seek\ninformation, especially those related to their healthcare needs, placing their\ntrust in the collective wisdom of the public. However, there can be several\nanswers in response to a single query, which makes it hard to grasp the key\ninformation related to the specific health concern. Typically, CQA forums\nfeature a single top-voted answer as a representative summary for each query.\nHowever, a single answer overlooks the alternative solutions and other\ninformation frequently offered in other responses. Our research focuses on\naspect-based summarization of health answers to address this limitation.\nSummarization of responses under different aspects such as suggestions,\ninformation, personal experiences, and questions can enhance the usability of\nthe platforms. We formalize a multi-stage annotation guideline and contribute a\nunique dataset comprising aspect-based human-written health answer summaries.\nWe build an automated multi-faceted answer summarization pipeline with this\ndataset based on task-specific fine-tuning of several state-of-the-art models.\nThe pipeline leverages question similarity to retrieve relevant answer\nsentences, subsequently classifying them into the appropriate aspect type.\nFollowing this, we employ several recent abstractive summarization models to\ngenerate aspect-based summaries. Finally, we present a comprehensive human\nanalysis and find that our summaries rank high in capturing relevant content\nand a wide range of solutions.\n', ""  Systematic review (SR) is a popular research method in software engineering\n(SE). However, conducting an SR takes an average of 67 weeks. Thus, automating\nany step of the SR process could reduce the effort associated with SRs. Our\nobjective is to investigate if Large Language Models (LLMs) can accelerate\ntitle-abstract screening by simplifying abstracts for human screeners, and\nautomating title-abstract screening. We performed an experiment where humans\nscreened titles and abstracts for 20 papers with both original and simplified\nabstracts from a prior SR. The experiment with human screeners was reproduced\nwith GPT-3.5 and GPT-4 LLMs to perform the same screening tasks. We also\nstudied if different prompting techniques (Zero-shot (ZS), One-shot (OS),\nFew-shot (FS), and Few-shot with Chain-of-Thought (FS-CoT)) improve the\nscreening performance of LLMs. Lastly, we studied if redesigning the prompt\nused in the LLM reproduction of screening leads to improved performance. Text\nsimplification did not increase the screeners' screening performance, but\nreduced the time used in screening. Screeners' scientific literacy skills and\nresearcher status predict screening performance. Some LLM and prompt\ncombinations perform as well as human screeners in the screening tasks. Our\nresults indicate that the GPT-4 LLM is better than its predecessor, GPT-3.5.\nAdditionally, Few-shot and One-shot prompting outperforms Zero-shot prompting.\nUsing LLMs for text simplification in the screening process does not\nsignificantly improve human performance. Using LLMs to automate title-abstract\nscreening seems promising, but current LLMs are not significantly more accurate\nthan human screeners. To recommend the use of LLMs in the screening process of\nSRs, more research is needed. We recommend future SR studies publish\nreplication packages with screening data to enable more conclusive\nexperimenting with LLM screening.\n""]",Automated Summarization in Systematic Reviews,Large Language Models for Text Analysis and Summarization,Large Language Models
232,232,41,232_feature_supervised_classifiers_features,"['feature', 'supervised', 'classifiers', 'features', 'classification', 'labeling', 'selection', 'prediction', 'learning', 'discriminative']","['selection', 'label', 'invariant', 'ordinal', 'feature', 'features', 'regression', 'subset', 'receiver', 'domains']","['  Under missing-not-at-random (MNAR) sample selection bias, the performance of\na prediction model is often degraded. This paper focuses on one classic\ninstance of MNAR sample selection bias where a subset of samples have\nnon-randomly missing outcomes. The Heckman selection model and its variants\nhave commonly been used to handle this type of sample selection bias. The\nHeckman model uses two separate equations to model the prediction and selection\nof samples, where the selection features include all prediction features. When\nusing the Heckman model, the prediction features must be properly chosen from\nthe set of selection features. However, choosing the proper prediction features\nis a challenging task for the Heckman model. This is especially the case when\nthe number of selection features is large. Existing approaches that use the\nHeckman model often provide a manually chosen set of prediction features. In\nthis paper, we propose Heckman-FA as a novel data-driven framework for\nobtaining prediction features for the Heckman model. Heckman-FA first trains an\nassignment function that determines whether or not a selection feature is\nassigned as a prediction feature. Using the parameters of the trained function,\nthe framework extracts a suitable set of prediction features based on the\ngoodness-of-fit of the prediction model given the chosen prediction features\nand the correlation between noise terms of the prediction and selection\nequations. Experimental results on real-world datasets show that Heckman-FA\nproduces a robust regression model under MNAR sample selection bias.\n', '  In the last decade, embedded multi-label feature selection methods,\nincorporating the search for feature subsets into model optimization, have\nattracted considerable attention in accurately evaluating the importance of\nfeatures in multi-label classification tasks. Nevertheless, the\nstate-of-the-art embedded multi-label feature selection algorithms based on\nleast square regression usually cannot preserve sufficient discriminative\ninformation in multi-label data. To tackle the aforementioned challenge, a\nnovel embedded multi-label feature selection method, termed global redundancy\nand relevance optimization in orthogonal regression (GRROOR), is proposed to\nfacilitate the multi-label feature selection. The method employs orthogonal\nregression with feature weighting to retain sufficient statistical and\nstructural information related to local label correlations of the multi-label\ndata in the feature learning process. Additionally, both global feature\nredundancy and global label relevancy information have been considered in the\northogonal regression model, which could contribute to the search for\ndiscriminative and non-redundant feature subsets in the multi-label data. The\ncost function of GRROOR is an unbalanced orthogonal Procrustes problem on the\nStiefel manifold. A simple yet effective scheme is utilized to obtain an\noptimal solution. Extensive experimental results on ten multi-label data sets\ndemonstrate the effectiveness of GRROOR.\n', '  Semi-supervised multi-label feature selection has recently been developed to\nsolve the curse of dimensionality problem in high-dimensional multi-label data\nwith certain samples missing labels. Although many efforts have been made, most\nexisting methods use a predefined graph approach to capture the sample\nsimilarity or the label correlation. In this manner, the presence of noise and\noutliers within the original feature space can undermine the reliability of the\nresulting sample similarity graph. It also fails to precisely depict the label\ncorrelation due to the existence of unknown labels. Besides, these methods only\nconsider the discriminative power of selected features, while neglecting their\nredundancy. In this paper, we propose an Adaptive Collaborative Correlation\nlEarning-based Semi-Supervised Multi-label Feature Selection (Access-MFS)\nmethod to address these issues. Specifically, a generalized regression model\nequipped with an extended uncorrelated constraint is introduced to select\ndiscriminative yet irrelevant features and maintain consistency between\npredicted and ground-truth labels in labeled data, simultaneously. Then, the\ninstance correlation and label correlation are integrated into the proposed\nregression model to adaptively learn both the sample similarity graph and the\nlabel similarity graph, which mutually enhance feature selection performance.\nExtensive experimental results demonstrate the superiority of the proposed\nAccess-MFS over other state-of-the-art methods.\n']",Feature Selection for Classification,Feature Selection and Optimization in Machine Learning,Machine Learning and Artificial Intelligence
233,233,40,233_molecular_molecule_molecules_networks,"['molecular', 'molecule', 'molecules', 'networks', 'neural', 'graphs', 'representations', 'ligand', 'graph', 'protein']","['molecular', 'protein', 'molecules', 'drug', 'equivariant', 'graph', 'chemical', 'atoms', 'atom', 'properties']","[""  The integration of molecule and language has garnered increasing attention in\nmolecular science. Recent advancements in Language Models (LMs) have\ndemonstrated potential for the comprehensive modeling of molecule and language.\nHowever, existing works exhibit notable limitations. Most existing works\noverlook the modeling of 3D information, which is crucial for understanding\nmolecular structures and also functions. While some attempts have been made to\nleverage external structure encoding modules to inject the 3D molecular\ninformation into LMs, there exist obvious difficulties that hinder the\nintegration of molecular structure and language text, such as modality\nalignment and separate tuning. To bridge this gap, we propose 3D-MolT5, a\nunified framework designed to model both 1D molecular sequence and 3D molecular\nstructure. The key innovation lies in our methodology for mapping fine-grained\n3D substructure representations (based on 3D molecular fingerprints) to a\nspecialized 3D token vocabulary for 3D-MolT5. This 3D structure token\nvocabulary enables the seamless combination of 1D sequence and 3D structure\nrepresentations in a tokenized format, allowing 3D-MolT5 to encode molecular\nsequence (SELFIES), molecular structure, and text sequences within a unified\narchitecture. Alongside, we further introduce 1D and 3D joint pre-training to\nenhance the model's comprehension of these diverse modalities in a joint\nrepresentation space and better generalize to various tasks for our foundation\nmodel. Through instruction tuning on multiple downstream datasets, our proposed\n3D-MolT5 shows superior performance than existing methods in molecular property\nprediction, molecule captioning, and text-based molecule generation tasks. Our\ncode will be available on GitHub soon.\n"", ""  Molecular property prediction is a key component of AI-driven drug discovery\nand molecular characterization learning. Despite recent advances, existing\nmethods still face challenges such as limited ability to generalize, and\ninadequate representation of learning from unlabeled data, especially for tasks\nspecific to molecular structures. To address these limitations, we introduce\nDIG-Mol, a novel self-supervised graph neural network framework for molecular\nproperty prediction. This architecture leverages the power of contrast learning\nwith dual interaction mechanisms and unique molecular graph enhancement\nstrategies. DIG-Mol integrates a momentum distillation network with two\ninterconnected networks to efficiently improve molecular characterization. The\nframework's ability to extract key information about molecular structure and\nhigher-order semantics is supported by minimizing loss of contrast. We have\nestablished DIG-Mol's state-of-the-art performance through extensive\nexperimental evaluation in a variety of molecular property prediction tasks. In\naddition to demonstrating superior transferability in a small number of\nlearning scenarios, our visualizations highlight DIG-Mol's enhanced\ninterpretability and representation capabilities. These findings confirm the\neffectiveness of our approach in overcoming challenges faced by traditional\nmethods and mark a significant advance in molecular property prediction.\n"", '  Diffusion generative models have emerged as a powerful framework for\naddressing problems in structural biology and structure-based drug design.\nThese models operate directly on 3D molecular structures. Due to the\nunfavorable scaling of graph neural networks (GNNs) with graph size as well as\nthe relatively slow inference speeds inherent to diffusion models, many\nexisting molecular diffusion models rely on coarse-grained representations of\nprotein structure to make training and inference feasible. However, such\ncoarse-grained representations discard essential information for modeling\nmolecular interactions and impair the quality of generated structures. In this\nwork, we present a novel GNN-based architecture for learning latent\nrepresentations of molecular structure. When trained end-to-end with a\ndiffusion model for de novo ligand design, our model achieves comparable\nperformance to one with an all-atom protein representation while exhibiting a\n3-fold reduction in inference time.\n']",Molecular Structure Modeling and Prediction,Computational Methods for Molecular and Materials Science,Computational Biology and Chemistry
234,234,40,234_spatial_visual_planning_abstractions,"['spatial', 'visual', 'planning', 'abstractions', 'reasoning', 'ai', 'language', 'tasks', 'intelligence', 'multimodal']","['reasoning', 'spatial', 'agents', 'planning', 'instructions', 'agent', 'instruction', 'capabilities', 'language', 'robot']","['  Artificial intelligence (AI) has made remarkable progress across various\ndomains, with large language models like ChatGPT gaining substantial attention\nfor their human-like text-generation capabilities. Despite these achievements,\nspatial reasoning remains a significant challenge for these models. Benchmarks\nlike StepGame evaluate AI spatial reasoning, where ChatGPT has shown\nunsatisfactory performance. However, the presence of template errors in the\nbenchmark has an impact on the evaluation results. Thus there is potential for\nChatGPT to perform better if these template errors are addressed, leading to\nmore accurate assessments of its spatial reasoning capabilities. In this study,\nwe refine the StepGame benchmark, providing a more accurate dataset for model\nevaluation. We analyze GPT\'s spatial reasoning performance on the rectified\nbenchmark, identifying proficiency in mapping natural language text to spatial\nrelations but limitations in multi-hop reasoning. We provide a flawless\nsolution to the benchmark by combining template-to-relation mapping with\nlogic-based reasoning. This combination demonstrates proficiency in performing\nqualitative reasoning on StepGame without encountering any errors. We then\naddress the limitations of GPT models in spatial reasoning. We deploy\nChain-of-thought and Tree-of-thoughts prompting strategies, offering insights\ninto GPT\'s ``cognitive process"", and achieving remarkable improvements in\naccuracy. Our investigation not only sheds light on model deficiencies but also\nproposes enhancements, contributing to the advancement of AI with more robust\nspatial reasoning capabilities.\n', ""  Vision language models (VLMs) are an exciting emerging class of language\nmodels (LMs) that have merged classic LM capabilities with those of image\nprocessing systems. However, the ways that these capabilities combine are not\nalways intuitive and warrant direct investigation. One understudied capability\nin VLMs is visual spatial planning -- the ability to comprehend the spatial\narrangements of objects and devise action plans to achieve desired outcomes in\nvisual scenes. In our study, we introduce VSP, a benchmark that 1) evaluates\nthe spatial planning capability in these models in general, and 2) breaks down\nthe visual planning task into finer-grained sub-tasks, including perception and\nreasoning, and measure the LMs capabilities in these sub-tasks. Our evaluation\nshows that both open-source and private VLMs fail to generate effective plans\nfor even simple spatial planning tasks. Evaluations on the fine-grained\nanalytical tasks further reveal fundamental deficiencies in the models' visual\nperception and bottlenecks in reasoning abilities, explaining their worse\nperformance in the general spatial planning tasks. Our work illuminates future\ndirections for improving VLMs' abilities in spatial planning. Our benchmark is\npublicly available at\nhttps://github.com/UCSB-NLP-Chang/Visual-Spatial-Planning.\n"", '  Large language models (LLMs) and vision-language models (VLMs) have\ndemonstrated remarkable performance across a wide range of tasks and domains.\nDespite this promise, spatial understanding and reasoning -- a fundamental\ncomponent of human cognition -- remains under-explored. We develop novel\nbenchmarks that cover diverse aspects of spatial reasoning such as relationship\nunderstanding, navigation, and counting. We conduct a comprehensive evaluation\nof competitive language and vision-language models. Our findings reveal several\ncounter-intuitive insights that have been overlooked in the literature: (1)\nSpatial reasoning poses significant challenges where competitive models can\nfall behind random guessing; (2) Despite additional visual input, VLMs often\nunder-perform compared to their LLM counterparts; (3) When both textual and\nvisual information is available, multi-modal language models become less\nreliant on visual information if sufficient textual clues are provided.\nAdditionally, we demonstrate that leveraging redundancy between vision and text\ncan significantly enhance model performance. We hope our study will inform the\ndevelopment of multimodal models to improve spatial intelligence and further\nclose the gap with human intelligence.\n']",Spatial Reasoning in AI and Multimodal Models,Visual and Spatial Reasoning in Artificial Intelligence,Artificial Intelligence and Cognitive Systems
235,235,40,235_embeddings_embedding_recommender_memory,"['embeddings', 'embedding', 'recommender', 'memory', 'recommendations', 'factorization', 'recommendation', 'storage', 'gpu', 'items']","['recommendation', 'indexes', 'memory', 'recommender', 'tables', 'index', 'embeddings', 'items', 'item', 'feature']","['  Recommender systems typically represent users and items by learning their\nembeddings, which are usually set to uniform dimensions and dominate the model\nparameters. However, real-world recommender systems often operate in streaming\nrecommendation scenarios, where the number of users and items continues to\ngrow, leading to substantial storage resource consumption for these embeddings.\nAlthough a few methods attempt to mitigate this by employing embedding size\nsearch strategies to assign different embedding dimensions in streaming\nrecommendations, they assume that the embedding size grows with the frequency\nof users/items, which eventually still exceeds the predefined memory budget\nover time. To address this issue, this paper proposes to learn Scalable\nLightweight Embeddings for streaming recommendation, called SCALL, which can\nadaptively adjust the embedding sizes of users/items within a given memory\nbudget over time. Specifically, we propose to sample embedding sizes from a\nprobabilistic distribution, with the guarantee to meet any predefined memory\nbudget. By fixing the memory budget, the proposed embedding size sampling\nstrategy can increase and decrease the embedding sizes in accordance to the\nfrequency of the corresponding users or items. Furthermore, we develop a\nreinforcement learning-based search paradigm that models each state with mean\npooling to keep the length of the state vectors fixed, invariant to the\nchanging number of users and items. As a result, the proposed method can\nprovide embedding sizes to unseen users and items. Comprehensive empirical\nevaluations on two public datasets affirm the advantageous effectiveness of our\nproposed method.\n', '  At the heart of contemporary recommender systems (RSs) are latent factor\nmodels that provide quality recommendation experience to users. These models\nuse embedding vectors, which are typically of a uniform and fixed size, to\nrepresent users and items. As the number of users and items continues to grow,\nthis design becomes inefficient and hard to scale. Recent lightweight embedding\nmethods have enabled different users and items to have diverse embedding sizes,\nbut are commonly subject to two major drawbacks. Firstly, they limit the\nembedding size search to optimizing a heuristic balancing the recommendation\nquality and the memory complexity, where the trade-off coefficient needs to be\nmanually tuned for every memory budget requested. The implicitly enforced\nmemory complexity term can even fail to cap the parameter usage, making the\nresultant embedding table fail to meet the memory budget strictly. Secondly,\nmost solutions, especially reinforcement learning based ones derive and\noptimize the embedding size for each each user/item on an instance-by-instance\nbasis, which impedes the search efficiency. In this paper, we propose Budgeted\nEmbedding Table (BET), a novel method that generates table-level actions (i.e.,\nembedding sizes for all users and items) that is guaranteed to meet\npre-specified memory budgets. Furthermore, by leveraging a set-based action\nformulation and engaging set representation learning, we present an innovative\naction search strategy powered by an action fitness predictor that efficiently\nevaluates each table-level action. Experiments have shown state-of-the-art\nperformance on two real-world datasets when BET is paired with three popular\nrecommender models under different memory budgets.\n', ""  Recommender models are commonly used to suggest relevant items to a user for\ne-commerce and online advertisement-based applications. These models use\nmassive embedding tables to store numerical representation of items' and users'\ncategorical variables (memory intensive) and employ neural networks (compute\nintensive) to generate final recommendations. Training these large-scale\nrecommendation models is evolving to require increasing data and compute\nresources. The highly parallel neural networks portion of these models can\nbenefit from GPU acceleration however, large embedding tables often cannot fit\nin the limited-capacity GPU device memory. Hence, this paper deep dives into\nthe semantics of training data and obtains insights about the feature access,\ntransfer, and usage patterns of these models. We observe that, due to the\npopularity of certain inputs, the accesses to the embeddings are highly skewed\nwith a few embedding entries being accessed up to 10000x more. This paper\nleverages this asymmetrical access pattern to offer a framework, called FAE,\nand proposes a hot-embedding aware data layout for training recommender models.\nThis layout utilizes the scarce GPU memory for storing the highly accessed\nembeddings, thus reduces the data transfers from CPU to GPU. At the same time,\nFAE engages the GPU to accelerate the executions of these hot embedding\nentries. Experiments on production-scale recommendation models with real\ndatasets show that FAE reduces the overall training time by 2.3x and 1.52x in\ncomparison to XDL CPU-only and XDL CPU-GPU execution while maintaining baseline\naccuracy\n""]",Scalable Embeddings for Recommender Systems,Recommender Systems and Personalization Techniques,Recommender Systems and Personalization
236,236,40,236_audiovisual_audio_audioldm_audiocaps,"['audiovisual', 'audio', 'audioldm', 'audiocaps', 'generative', 'recordings', 'sound', 'music', 'visual', 'videos']","['audio', 'sound', 'speech', 'acoustic', 'generation', 'audiovisual', 'waveform', 'music', 'visual', 'video']","['  This technical report details our work towards building an enhanced\naudio-visual sound event localization and detection (SELD) network. We build on\ntop of the audio-only SELDnet23 model and adapt it to be audio-visual by\nmerging both audio and video information prior to the gated recurrent unit\n(GRU) of the audio-only network. Our model leverages YOLO and DETIC object\ndetectors. We also build a framework that implements audio-visual data\naugmentation and audio-visual synthetic data generation. We deliver an\naudio-visual SELDnet system that outperforms the existing audio-visual SELD\nbaseline.\n', '  Generative Pre-trained Transformer (GPT) models have achieved remarkable\nperformance on various natural language processing tasks, and have shown great\npotential as backbones for audio-and-text large language models (LLMs).\nPrevious mainstream audio-and-text LLMs use discrete audio tokens to represent\nboth input and output audio; however, they suffer from performance degradation\non tasks such as automatic speech recognition, speech-to-text translation, and\nspeech enhancement over models using continuous speech features. In this paper,\nwe propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio\nrecognition, understanding, and generation. LauraGPT is a versatile LLM that\ncan process both audio and text inputs and generate outputs in either\nmodalities. We propose a novel data representation that combines continuous and\ndiscrete features for audio: LauraGPT encodes input audio into continuous\nrepresentations using an audio encoder and generates output audio from discrete\ncodec codes. We propose a one-step codec vocoder to overcome the prediction\nchallenge caused by the multimodal distribution of codec tokens. We fine-tune\nLauraGPT using supervised multi-task learning. Extensive experiments show that\nLauraGPT consistently achieves comparable to superior performance compared to\nstrong baselines on a wide range of audio tasks related to content, semantics,\nparalinguistics, and audio-signal analysis, such as automatic speech\nrecognition, speech-to-text translation, text-to-speech synthesis, speech\nenhancement, automated audio captioning, speech emotion recognition, and spoken\nlanguage understanding.\n', '  We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a\nnovel framework combining three tasks of video-to-audio, audio-to-text, and\ntext-to-audio together. C3LLM adapts the Large Language Model (LLM) structure\nas a bridge for aligning different modalities, synthesizing the given\nconditional information, and making multimodal generation in a discrete manner.\nOur contributions are as follows. First, we adapt a hierarchical structure for\naudio generation tasks with pre-trained audio codebooks. Specifically, we train\nthe LLM to generate audio semantic tokens from the given conditions, and\nfurther use a non-autoregressive transformer to generate different levels of\nacoustic tokens in layers to better enhance the fidelity of the generated\naudio. Second, based on the intuition that LLMs were originally designed for\ndiscrete tasks with the next-word prediction method, we use the discrete\nrepresentation for audio generation and compress their semantic meanings into\nacoustic tokens, similar to adding ""acoustic vocabulary"" to LLM. Third, our\nmethod combines the previous tasks of audio understanding, video-to-audio\ngeneration, and text-to-audio generation together into one unified model,\nproviding more versatility in an end-to-end fashion. Our C3LLM achieves\nimproved results through various automated evaluation metrics, providing better\nsemantic alignment compared to previous methods.\n']",Audio-Visual and Multimodal Learning Models,Multimodal Learning and Fusion,Multimodal Learning and Applications
237,237,40,237_repositories_documentation_retrieval_software,"['repositories', 'documentation', 'retrieval', 'software', 'discoverybench', 'language', 'models', 'tools', 'completions', 'benchmarks']","['software', 'engineering', 'tools', 'requirements', 'repositories', 'engineers', 'tool', 'open', 'language', 'documentation']","['  Large Language Models (LLMs) have the potential to revolutionize the Sixth\nGeneration (6G) communication networks. However, current mainstream LLMs\ngenerally lack the specialized knowledge in telecom domain. In this paper, for\nthe first time, we propose a pipeline to adapt any general purpose LLMs to a\ntelecom-specific LLMs. We collect and build telecom-specific pre-train dataset,\ninstruction dataset, preference dataset to perform continual pre-training,\ninstruct tuning and alignment tuning respectively. Besides, due to the lack of\nwidely accepted evaluation benchmarks in telecom domain, we extend existing\nevaluation benchmarks and proposed three new benchmarks, namely, Telecom Math\nModeling, Telecom Open QnA and Telecom Code Tasks. These new benchmarks provide\na holistic evaluation of the capabilities of LLMs including math modeling,\nOpen-Ended question answering, code generation, infilling, summarization and\nanalysis in telecom domain. Our fine-tuned LLM TelecomGPT outperforms state of\nthe art (SOTA) LLMs including GPT-4, Llama-3 and Mistral in Telecom Math\nModeling benchmark significantly and achieve comparable performance in various\nevaluation benchmarks such as TeleQnA, 3GPP technical documents classification,\ntelecom code summary and generation and infilling.\n', ""  The development and training of deep learning models have become increasingly\ncostly and complex. Consequently, software engineers are adopting pre-trained\nmodels (PTMs) for their downstream applications. The dynamics of the PTM supply\nchain remain largely unexplored, signaling a clear need for structured datasets\nthat document not only the metadata but also the subsequent applications of\nthese models. Without such data, the MSR community cannot comprehensively\nunderstand the impact of PTM adoption and reuse. This paper presents the\nPeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed\nsnapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with\n28,575 open-source software repositories from GitHub that utilize these models.\nAdditionally, the dataset includes 44,337 mappings from 15,129 downstream\nGitHub repositories to the 2,530 PTMs they use. To enhance the dataset's\ncomprehensiveness, we developed prompts for a large language model to\nautomatically extract model metadata, including the model's training datasets,\nparameters, and evaluation metrics. Our analysis of this dataset provides the\nfirst summary statistics for the PTM supply chain, showing the trend of PTM\ndevelopment and common shortcomings of PTM package documentation. Our example\napplication reveals inconsistencies in software licenses across PTMs and their\ndependent projects. PeaTMOSS lays the foundation for future research, offering\nrich opportunities to investigate the PTM supply chain. We outline mining\nopportunities on PTMs, their downstream usage, and cross-cutting questions.\n"", '  Open-source development has revolutionized the software industry by promoting\ncollaboration, transparency, and community-driven innovation. Today, a vast\namount of various kinds of open-source software, which form networks of\nrepositories, is often hosted on GitHub - a popular software development\nplatform. To enhance the discoverability of the repository networks, i.e.,\ngroups of similar repositories, GitHub introduced repository topics in 2017\nthat enable users to more easily explore relevant projects by type, technology,\nand more. It is thus crucial to accurately assign topics for each GitHub\nrepository. Current methods for automatic topic recommendation rely heavily on\nTF-IDF for encoding textual data, presenting challenges in understanding\nsemantic nuances. This paper addresses the limitations of existing techniques\nby proposing Legion, a novel approach that leverages Pre-trained Language\nModels (PTMs) for recommending topics for GitHub repositories. The key novelty\nof Legion is three-fold. First, Legion leverages the extensive capabilities of\nPTMs in language understanding to capture contextual information and semantic\nmeaning in GitHub repositories. Second, Legion overcomes the challenge of\nlong-tailed distribution, which results in a bias toward popular topics in\nPTMs, by proposing a Distribution-Balanced Loss (DB Loss) to better train the\nPTMs. Third, Legion employs a filter to eliminate vague recommendations,\nthereby improving the precision of PTMs. Our empirical evaluation on a\nbenchmark dataset of real-world GitHub repositories shows that Legion can\nimprove vanilla PTMs by up to 26% on recommending GitHubs topics. Legion also\ncan suggest GitHub topics more precisely and effectively than the\nstate-of-the-art baseline with an average improvement of 20% and 5% in terms of\nPrecision and F1-score, respectively.\n']",Large Language Models for Software Development and Documentation,Applications of Large Language Models,Large Language Models
238,238,39,238_anomaly_anomalies_detecting_detection,"['anomaly', 'anomalies', 'detecting', 'detection', 'supervised', 'datasets', 'boosting', 'hyperspectral', 'monitoring', 'unsupervised']","['anomaly', 'anomalies', 'detection', 'industrial', 'insurance', 'unsupervised', 'failure', 'hyperspectral', 'abnormal', 'isolation']","['  Fault detection is crucial in industrial systems to prevent failures and\noptimize performance by distinguishing abnormal from normal operating\nconditions. Data-driven methods have been gaining popularity for fault\ndetection tasks as the amount of condition monitoring data from complex\nindustrial systems increases. Despite these advances, early fault detection\nremains a challenge under real-world scenarios. The high variability of\noperating conditions and environments makes it difficult to collect\ncomprehensive training datasets that can represent all possible operating\nconditions, especially in the early stages of system operation. Furthermore,\nthese variations often evolve over time, potentially leading to entirely new\ndata distributions in the future that were previously unseen. These challenges\nprevent direct knowledge transfer across different units and over time, leading\nto the distribution gap between training and testing data and inducing\nperformance degradation of those methods in real-world scenarios. To overcome\nthis, our work introduces a novel approach for continuous test-time domain\nadaptation. This enables early-stage robust anomaly detection by addressing\ndomain shifts and limited data representativeness issues. We propose a\nTest-time domain Adaptation Anomaly Detection (TAAD) framework that separates\ninput variables into system parameters and measurements, employing two domain\nadaptation modules to independently adapt to each input category. This method\nallows for effective adaptation to evolving operating conditions and is\nparticularly beneficial in systems with scarce data. Our approach, tested on a\nreal-world pump monitoring dataset, shows significant improvements over\nexisting domain adaptation methods in fault detection, demonstrating enhanced\naccuracy and reliability.\n', '  Automating real-time anomaly detection is essential for identifying rare\ntransients in the era of large-scale astronomical surveys. Modern survey\ntelescopes are generating tens of thousands of alerts per night, and future\ntelescopes, such as the Vera C. Rubin Observatory, are projected to increase\nthis number dramatically. Currently, most anomaly detection algorithms for\nastronomical transients rely either on hand-crafted features extracted from\nlight curves or on features generated through unsupervised representation\nlearning, which are then coupled with standard machine learning anomaly\ndetection algorithms. In this work, we introduce an alternative approach to\ndetecting anomalies: using the penultimate layer of a neural network classifier\nas the latent space for anomaly detection. We then propose a novel method,\nnamed Multi-Class Isolation Forests (MCIF), which trains separate isolation\nforests for each class to derive an anomaly score for a light curve from the\nlatent space representation given by the classifier. This approach\nsignificantly outperforms a standard isolation forest. We also use a simpler\ninput method for real-time transient classifiers which circumvents the need for\ninterpolation in light curves and helps the neural network model inter-passband\nrelationships and handle irregular sampling. Our anomaly detection pipeline\nidentifies rare classes including kilonovae, pair-instability supernovae, and\nintermediate luminosity transients shortly after trigger on simulated Zwicky\nTransient Facility light curves. Using a sample of our simulations that matched\nthe population of anomalies expected in nature (54 anomalies and 12,040 common\ntransients), our method was able to discover $41\\pm3$ anomalies (~75% recall)\nafter following up the top 2000 (~15%) ranked transients. Our novel method\nshows that classifiers can be effectively repurposed for real-time anomaly\ndetection.\n', '  The majority of existing hyperspectral anomaly detection (HAD) methods use\nthe low-rank representation (LRR) model to separate the background and anomaly\ncomponents, where the anomaly component is optimized by handcrafted sparse\npriors (e.g., $\\ell_{2,1}$-norm). However, this may not be ideal since they\noverlook the spatial structure present in anomalies and make the detection\nresult largely dependent on manually set sparsity. To tackle these problems, we\nredefine the optimization criterion for the anomaly component in the LRR model\nwith a self-supervised network called self-supervised anomaly prior (SAP). This\nprior is obtained by the pretext task of self-supervised learning, which is\ncustomized to learn the characteristics of hyperspectral anomalies.\nSpecifically, this pretext task is a classification task to distinguish the\noriginal hyperspectral image (HSI) and the pseudo-anomaly HSI, where the\npseudo-anomaly is generated from the original HSI and designed as a prism with\narbitrary polygon bases and arbitrary spectral bands. In addition, a\ndual-purified strategy is proposed to provide a more refined background\nrepresentation with an enriched background dictionary, facilitating the\nseparation of anomalies from complex backgrounds. Extensive experiments on\nvarious hyperspectral datasets demonstrate that the proposed SAP offers a more\naccurate and interpretable solution than other advanced HAD methods.\n']",Anomaly Detection Methods,Anomaly and Outlier Detection Methods,Data Analysis and Pattern Discovery
239,239,39,239_scheduling_prediction_knapsack_algorithms,"['scheduling', 'prediction', 'knapsack', 'algorithms', 'optimal', 'predictive', 'optimally', 'predictions', 'schedules', 'optimization']","['predictions', 'scheduling', 'online', 'jobs', 'algorithms', 'worst', 'prediction', 'case', 'problem', 'algorithm']","['  The non-clairvoyant scheduling problem has gained new interest within\nlearning-augmented algorithms, where the decision-maker is equipped with\npredictions without any quality guarantees. In practical settings, access to\npredictions may be reduced to specific instances, due to cost or data\nlimitations. Our investigation focuses on scenarios where predictions for only\n$B$ job sizes out of $n$ are available to the algorithm. We first establish\nnear-optimal lower bounds and algorithms in the case of perfect predictions.\nSubsequently, we present a learning-augmented algorithm satisfying the\nrobustness, consistency, and smoothness criteria, and revealing a novel\ntradeoff between consistency and smoothness inherent in the scenario with a\nrestricted number of predictions.\n', '  This paper develops learning-augmented algorithms for energy trading in\nvolatile electricity markets. The basic problem is to sell (or buy) $k$ units\nof energy for the highest revenue (lowest cost) over uncertain time-varying\nprices, which can framed as a classic online search problem in the literature\nof competitive analysis. State-of-the-art algorithms assume no knowledge about\nfuture market prices when they make trading decisions in each time slot, and\naim for guaranteeing the performance for the worst-case price sequence. In\npractice, however, predictions about future prices become commonly available by\nleveraging machine learning. This paper aims to incorporate machine-learned\npredictions to design competitive algorithms for online search problems. An\nimportant property of our algorithms is that they achieve performances\ncompetitive with the offline algorithm in hindsight when the predictions are\naccurate (i.e., consistency) and also provide worst-case guarantees when the\npredictions are arbitrarily wrong (i.e., robustness). The proposed algorithms\nachieve the Pareto-optimal trade-off between consistency and robustness, where\nno other algorithms for online search can improve on the consistency for a\ngiven robustness. Further, we extend the basic online search problem to a more\ngeneral inventory management setting that can capture storage-assisted energy\ntrading in electricity markets. In empirical evaluations using traces from\nreal-world applications, our learning-augmented algorithms improve the average\nempirical performance compared to benchmark algorithms, while also providing\nimproved worst-case performance.\n', '  An important goal of modern scheduling systems is to efficiently manage power\nusage. In energy-efficient scheduling, the operating system controls the speed\nat which a machine is processing jobs with the dual objective of minimizing\nenergy consumption and optimizing the quality of service cost of the resulting\nschedule. Since machine-learned predictions about future requests can often be\nlearned from historical data, a recent line of work on learning-augmented\nalgorithms aims to achieve improved performance guarantees by leveraging\npredictions. In particular, for energy-efficient scheduling, Bamas et. al.\n[BamasMRS20] and Antoniadis et. al. [antoniadis2021novel] designed algorithms\nwith predictions for the energy minimization with deadlines problem and\nachieved an improved competitive ratio when the prediction error is small while\nalso maintaining worst-case bounds even when the prediction error is\narbitrarily large.\n  In this paper, we consider a general setting for energy-efficient scheduling\nand provide a flexible learning-augmented algorithmic framework that takes as\ninput an offline and an online algorithm for the desired energy-efficient\nscheduling problem. We show that, when the prediction error is small, this\nframework gives improved competitive ratios for many different energy-efficient\nscheduling problems, including energy minimization with deadlines, while also\nmaintaining a bounded competitive ratio regardless of the prediction error.\nFinally, we empirically demonstrate that this framework achieves an improved\nperformance on real and synthetic datasets.\n']",Learning-Augmented Scheduling Algorithms,Advanced Scheduling Techniques for Efficient Resource Utilization,Optimization and Management of Complex Systems
240,240,39,240_gans_gan_generative_adversarial,"['gans', 'gan', 'generative', 'adversarial', 'autoencoders', 'cgan', 'castgan', 'ctgan', 'datasets', 'gansemble']","['generative', 'tabular', 'synthetic', 'data', 'generation', 'conditional', 'valuation', 'samples', 'adversarial', 'variables']","[""  Synthetic tabular data generation becomes crucial when real data is limited,\nexpensive to collect, or simply cannot be used due to privacy concerns.\nHowever, producing good quality synthetic data is challenging. Several\nprobabilistic, statistical, generative adversarial networks (GANs), and\nvariational auto-encoder (VAEs) based approaches have been presented for\nsynthetic tabular data generation. Once generated, evaluating the quality of\nthe synthetic data is quite challenging. Some of the traditional metrics have\nbeen used in the literature but there is lack of a common, robust, and single\nmetric. This makes it difficult to properly compare the effectiveness of\ndifferent synthetic tabular data generation methods. In this paper we propose a\nnew universal metric, TabSynDex, for robust evaluation of synthetic data. The\nproposed metric assesses the similarity of synthetic data with real data\nthrough different component scores which evaluate the characteristics that are\ndesirable for ``high quality'' synthetic data. Being a single score metric and\nhaving an implicit bound, TabSynDex can also be used to observe and evaluate\nthe training of neural network based approaches. This would help in obtaining\ninsights that was not possible earlier. We present several baseline models for\ncomparative analysis of the proposed evaluation metric with existing generative\nmodels. We also give a comparative analysis between TabSynDex and existing\nsynthetic tabular data evaluation metrics. This shows the effectiveness and\nuniversality of our metric over the existing metrics. Source Code:\n\\url{https://github.com/vikram2000b/tabsyndex}\n"", '  Advancements in science rely on data sharing. In medicine, where personal\ndata are often involved, synthetic tabular data generated by generative\nadversarial networks (GANs) offer a promising avenue. However, existing GANs\nstruggle to capture the complexities of real-world tabular data, which often\ncontain a mix of continuous and categorical variables with potential imbalances\nand dependencies. We propose a novel correlation- and mean-aware loss function\ndesigned to address these challenges as a regularizer for GANs. To ensure a\nrigorous evaluation, we establish a comprehensive benchmarking framework using\nten real-world datasets and eight established tabular GAN baselines. The\nproposed loss function demonstrates statistically significant improvements over\nexisting methods in capturing the true data distribution, significantly\nenhancing the quality of synthetic data generated with GANs. The benchmarking\nframework shows that the enhanced synthetic data quality leads to improved\nperformance in downstream machine learning (ML) tasks, ultimately paving the\nway for easier data sharing.\n', '  Generative adversarial networks (GANs) have drawn considerable attention in\nrecent years for their proven capability in generating synthetic data which can\nbe utilised for multiple purposes. While GANs have demonstrated tremendous\nsuccesses in producing synthetic data samples that replicate the dynamics of\nthe original datasets, the validity of the synthetic data and the underlying\nprivacy concerns represent major challenges which are not sufficiently\naddressed. In this work, we design a cascaded tabular GAN framework (CasTGAN)\nfor generating realistic tabular data with a specific focus on the validity of\nthe output. In this context, validity refers to the the dependency between\nfeatures that can be found in the real data, but is typically misrepresented by\ntraditional generative models. Our key idea entails that employing a cascaded\narchitecture in which a dedicated generator samples each feature, the synthetic\noutput becomes more representative of the real data. Our experimental results\ndemonstrate that our model is capable of generating synthetic tabular data that\ncan be used for fitting machine learning models. In addition, our model\ncaptures well the constraints and the correlations between the features of the\nreal data, especially the high dimensional datasets. Furthermore, we evaluate\nthe risk of white-box privacy attacks on our model and subsequently show that\napplying some perturbations to the auxiliary learners in CasTGAN increases the\noverall robustness of our model against targeted attacks.\n']",Synthetic Tabular Data Generation with GANs,Generative Adversarial Networks (GANs) and Their Applications,Generative Modeling and Artificial Intelligence
241,241,39,241_forecast_forecasting_networks_spatiotemporal,"['forecast', 'forecasting', 'networks', 'spatiotemporal', 'graphrl', 'neural', 'prediction', 'graph', 'network', 'temporal']","['temporal', 'forecasting', 'spatial', 'series', 'sensors', 'spatiotemporal', 'graph', 'spatio', 'dependencies', 'time']","[""  Spatial-temporal forecasting systems play a crucial role in addressing\nnumerous real-world challenges. In this paper, we investigate the potential of\naddressing spatial-temporal forecasting problems using general time series\nforecasting models, i.e., models that do not leverage the spatial relationships\namong the nodes. We propose a all-Multi-Layer Perceptron (all-MLP) time series\nforecasting architecture called RPMixer. The all-MLP architecture was chosen\ndue to its recent success in time series forecasting benchmarks. Furthermore,\nour method capitalizes on the ensemble-like behavior of deep neural networks,\nwhere each individual block within the network behaves like a base learner in\nan ensemble model, particularly when identity mapping residual connections are\nincorporated. By integrating random projection layers into our model, we\nincrease the diversity among the blocks' outputs, thereby improving the overall\nperformance of the network. Extensive experiments conducted on the largest\nspatial-temporal forecasting benchmark datasets demonstrate that the proposed\nmethod outperforms alternative methods, including both spatial-temporal graph\nmodels and general forecasting models.\n"", '  Time series forecasting is essential for our daily activities and precise\nmodeling of the complex correlations and shared patterns among multiple time\nseries is essential for improving forecasting performance. Spatial-Temporal\nGraph Neural Networks (STGNNs) are widely used in multivariate time series\nforecasting tasks and have achieved promising performance on multiple\nreal-world datasets for their ability to model the underlying complex spatial\nand temporal dependencies. However, existing studies have mainly focused on\ndatasets comprising only a few hundred sensors due to the heavy computational\ncost and memory cost of spatial-temporal GNNs. When applied to larger datasets,\nthese methods fail to capture the underlying complex spatial dependencies and\nexhibit limited scalability and performance. To this end, we present a Scalable\nAdaptive Graph Diffusion Forecasting Network (SAGDFN) to capture complex\nspatial-temporal correlation for large-scale multivariate time series and\nthereby, leading to exceptional performance in multivariate time series\nforecasting tasks. The proposed SAGDFN is scalable to datasets of thousands of\nnodes without the need of prior knowledge of spatial correlation. Extensive\nexperiments demonstrate that SAGDFN achieves comparable performance with\nstate-of-the-art baselines on one real-world dataset of 207 nodes and\noutperforms all state-of-the-art baselines by a significant margin on three\nreal-world datasets of 2000 nodes.\n', ""  Predicting Remaining Useful Life (RUL) plays a crucial role in the\nprognostics and health management of industrial systems that involve a variety\nof interrelated sensors. Given a constant stream of time series sensory data\nfrom such systems, deep learning models have risen to prominence at identifying\ncomplex, nonlinear temporal dependencies in these data. In addition to the\ntemporal dependencies of individual sensors, spatial dependencies emerge as\nimportant correlations among these sensors, which can be naturally modelled by\na temporal graph that describes time-varying spatial relationships. However,\nthe majority of existing studies have relied on capturing discrete snapshots of\nthis temporal graph, a coarse-grained approach that leads to loss of temporal\ninformation. Moreover, given the variety of heterogeneous sensors, it becomes\nvital that such inherent heterogeneity is leveraged for RUL prediction in\ntemporal sensor graphs. To capture the nuances of the temporal and spatial\nrelationships and heterogeneous characteristics in an interconnected graph of\nsensors, we introduce a novel model named Temporal and Heterogeneous Graph\nNeural Networks (THGNN). Specifically, THGNN aggregates historical data from\nneighboring nodes to accurately capture the temporal dynamics and spatial\ncorrelations within the stream of sensor data in a fine-grained manner.\nMoreover, the model leverages Feature-wise Linear Modulation (FiLM) to address\nthe diversity of sensor types, significantly improving the model's capacity to\nlearn the heterogeneity in the data sources. Finally, we have validated the\neffectiveness of our approach through comprehensive experiments. Our empirical\nfindings demonstrate significant advancements on the N-CMAPSS dataset,\nachieving improvements of up to 19.2% and 31.6% in terms of two different\nevaluation metrics over state-of-the-art methods.\n""]",Spatiotemporal Forecasting with Graph Neural Networks,Spatiotemporal Forecasting and Prediction in Transportation Systems,Transportation Systems and Environmental Analytics
242,242,38,242_exploration_heuristic_algorithms_search,"['exploration', 'heuristic', 'algorithms', 'search', 'subtrees', 'monte', 'mcts', 'tree', 'pathfinding', 'randomized']","['search', 'tree', 'problem', 'shortest', 'path', 'combinatorial', 'routing', 'depth', 'algorithms', 'min']","['  Monte-Carlo Tree Search (MCTS) is a widely-used strategy for online planning\nthat combines Monte-Carlo sampling with forward tree search. Its success relies\non the Upper Confidence bound for Trees (UCT) algorithm, an extension of the\nUCB method for multi-arm bandits. However, the theoretical foundation of UCT is\nincomplete due to an error in the logarithmic bonus term for action selection,\nleading to the development of Fixed-Depth-MCTS with a polynomial exploration\nbonus to balance exploration and exploitation~\\citep{shah2022journal}. Both UCT\nand Fixed-Depth-MCTS suffer from biased value estimation: the weighted sum\nunderestimates the optimal value, while the maximum valuation overestimates\nit~\\citep{coulom2006efficient}. The power mean estimator offers a balanced\nsolution, lying between the average and maximum values.\nPower-UCT~\\citep{dam2019generalized} incorporates this estimator for more\naccurate value estimates but its theoretical analysis remains incomplete. This\npaper introduces Stochastic-Power-UCT, an MCTS algorithm using the power mean\nestimator and tailored for stochastic MDPs. We analyze its polynomial\nconvergence in estimating root node values and show that it shares the same\nconvergence rate of $\\mathcal{O}(n^{-1/2})$, with $n$ is the number of visited\ntrajectories, as Fixed-Depth-MCTS, with the latter being a special case of the\nformer. Our theoretical results are validated with empirical tests across\nvarious stochastic MDP environments.\n', '  Monte-Carlo tree search (MCTS) is an effective anytime algorithm with a vast\namount of applications. It strategically allocates computational resources to\nfocus on promising segments of the search tree, making it a very attractive\nsearch algorithm in large search spaces. However, it often expends its limited\nresources on reevaluating previously explored regions when they remain the most\npromising path. Our proposed methodology, denoted as AmEx-MCTS, solves this\nproblem by introducing a novel MCTS formulation. Central to AmEx-MCTS is the\ndecoupling of value updates, visit count updates, and the selected path during\nthe tree search, thereby enabling the exclusion of already explored subtrees or\nleaves. This segregation preserves the utility of visit counts for both\nexploration-exploitation balancing and quality metrics within MCTS. The\nresultant augmentation facilitates in a considerably broader search using\nidentical computational resources, preserving the essential characteristics of\nMCTS. The expanded coverage not only yields more precise estimations but also\nproves instrumental in larger and more complex problems. Our empirical\nevaluation demonstrates the superior performance of AmEx-MCTS, surpassing\nclassical MCTS and related approaches by a substantial margin.\n', '  Monte Carlo Tree Search (MCTS) is an immensely popular search-based framework\nused for decision making. It is traditionally applied to domains where a\nperfect simulation model of the environment is available. We study and improve\nMCTS in the context where the environment model is given but imperfect. We show\nthat the discrepancy between the model and the actual environment can lead to\nsignificant performance degradation with standard MCTS. We therefore develop\nUncertainty Adapted MCTS (UA-MCTS), a more robust algorithm within the MCTS\nframework. We estimate the transition uncertainty in the given model, and\ndirect the search towards more certain transitions in the state space. We\nmodify all four MCTS phases to improve the search behavior by considering these\nestimates. We prove, in the corrupted bandit case, that adding uncertainty\ninformation to adapt UCB leads to tighter regret bound than standard UCB.\nEmpirically, we evaluate UA-MCTS and its individual components on the\ndeterministic domains from the MinAtar test suite. Our results demonstrate that\nUA-MCTS strongly improves MCTS in the presence of model transition errors.\n']",Monte-Carlo Tree Search Algorithms,Probabilistic Methods for Sampling and Decision Making,Probabilistic Methods and Stochastic Processes
243,243,38,243_semantic_wordnet_lexical_semantics,"['semantic', 'wordnet', 'lexical', 'semantics', 'linguistic', 'disambiguation', 'semantically', 'parsing', 'nlp', 'parsers']","['semantic', 'parsing', 'idiomatic', 'meaning', 'lexical', 'conceptual', 'noun', 'words', 'meanings', 'word']","['  Word sense disambiguation primarily addresses the lexical ambiguity of common\nwords based on a predefined sense inventory. Conversely, proper names are\nusually considered to denote an ad-hoc real-world referent. Once the reference\nis decided, the ambiguity is purportedly resolved. However, proper names also\nexhibit ambiguities through appellativization, i.e., they act like common words\nand may denote different aspects of their referents. We proposed to address the\nambiguities of proper names through the light of regular polysemy, which we\nformalized as dot objects. This paper introduces a combined word sense\ndisambiguation (WSD) model for disambiguating common words against Chinese\nWordnet (CWN) and proper names as dot objects. The model leverages the\nflexibility of a gloss-based model architecture, which takes advantage of the\nglosses and example sentences of CWN. We show that the model achieves\ncompetitive results on both common and proper nouns, even on a relatively\nsparse sense dataset. Aside from being a performant WSD tool, the model further\nfacilitates the future development of the lexical resource.\n', '  This paper explores techniques that focus on understanding and resolving\nambiguity in language within the field of natural language processing (NLP),\nhighlighting the complexity of linguistic phenomena such as polysemy and\nhomonymy and their implications for computational models. Focusing extensively\non Word Sense Disambiguation (WSD), it outlines diverse approaches ranging from\ndeep learning techniques to leveraging lexical resources and knowledge graphs\nlike WordNet. The paper introduces cutting-edge methodologies like word sense\nextension (WSE) and neuromyotonic approaches, enhancing disambiguation accuracy\nby predicting new word senses. It examines specific applications in biomedical\ndisambiguation and language specific optimisation and discusses the\nsignificance of cognitive metaphors in discourse analysis. The research\nidentifies persistent challenges in the field, such as the scarcity of sense\nannotated corpora and the complexity of informal clinical texts. It concludes\nby suggesting future directions, including using large language models, visual\nWSD, and multilingual WSD systems, emphasising the ongoing evolution in\naddressing lexical complexities in NLP. This thinking perspective highlights\nthe advancement in this field to enable computers to understand language more\naccurately.\n', '  In this work, we propose a Distributional Semantic resource enriched with\nlinguistic and lexical information extracted from electronic dictionaries,\ndesigned to address the challenge of bridging the gap between the continuous\nsemantic values represented by distributional vectors and the discrete\ndescriptions offered by general semantics theory. Recently, many researchers\nhave concentrated on the nexus between embeddings and a comprehensive theory of\nsemantics and meaning. This often involves decoding the representation of word\nmeanings in Distributional Models into a set of discrete, manually constructed\nproperties such as semantic primitives or features, using neural decoding\ntechniques. Our approach introduces an alternative strategy grounded in\nlinguistic data. We have developed a collection of domain-specific\nco-occurrence matrices, derived from two sources: a classification of Italian\nnouns categorized into 4 semantic traits and 20 concrete noun sub-categories,\nand a list of Italian verbs classified according to their semantic classes. In\nthese matrices, the co-occurrence values for each word are calculated\nexclusively with a defined set of words pertinent to a particular lexical\ndomain. The resource comprises 21 domain-specific matrices, one comprehensive\nmatrix, and a Graphical User Interface. Our model facilitates the generation of\nreasoned semantic descriptions of concepts by selecting matrices directly\nassociated with concrete conceptual knowledge, such as a matrix based on\nlocation nouns and the concept of animal habitats. We assessed the utility of\nthe resource through two experiments, achieving promising outcomes in both: the\nautomatic classification of animal nouns and the extraction of animal features.\n']",Word Sense Disambiguation and Semantic Analysis,Natural Language Processing and Semantic Analysis,Natural Language Processing
244,244,38,244_choreography_dance_rhythm_motions,"['choreography', 'dance', 'rhythm', 'motions', 'animations', 'animation', 'motion', 'pose', 'poses', 'videos']","['motion', 'dance', 'motions', 'music', 'video', 'human', 'sequences', 'videos', 'movements', 'synthesis']","['  Recent years have seen a tremendous improvement in the quality of video\ngeneration and editing approaches. While several techniques focus on editing\nappearance, few address motion. Current approaches using text, trajectories, or\nbounding boxes are limited to simple motions, so we specify motions with a\nsingle motion reference video instead. We further propose to use a pre-trained\nimage-to-video model rather than a text-to-video model. This approach allows us\nto preserve the exact appearance and position of a target object or scene and\nhelps disentangle appearance from motion. Our method, called motion-textual\ninversion, leverages our observation that image-to-video models extract\nappearance mainly from the (latent) image input, while the text/image embedding\ninjected via cross-attention predominantly controls motion. We thus represent\nmotion using text/image embedding tokens. By operating on an inflated\nmotion-text embedding containing multiple text/image embedding tokens per\nframe, we achieve a high temporal motion granularity. Once optimized on the\nmotion reference video, this embedding can be applied to various target images\nto generate videos with semantically similar motions. Our approach does not\nrequire spatial alignment between the motion reference video and target image,\ngeneralizes across various domains, and can be applied to various tasks such as\nfull-body and face reenactment, as well as controlling the motion of inanimate\nobjects and the camera. We empirically demonstrate the effectiveness of our\nmethod in the semantic video motion transfer task, significantly outperforming\nexisting methods in this context.\n', '  Synthesizing human motion with a global structure, such as a choreography, is\na challenging task. Existing methods tend to concentrate on local smooth pose\ntransitions and neglect the global context or the theme of the motion. In this\nwork, we present a music-driven motion synthesis framework that generates\nlong-term sequences of human motions which are synchronized with the input\nbeats, and jointly form a global structure that respects a specific dance\ngenre. In addition, our framework enables generation of diverse motions that\nare controlled by the content of the music, and not only by the beat. Our\nmusic-driven dance synthesis framework is a hierarchical system that consists\nof three levels: pose, motif, and choreography. The pose level consists of an\nLSTM component that generates temporally coherent sequences of poses. The motif\nlevel guides sets of consecutive poses to form a movement that belongs to a\nspecific distribution using a novel motion perceptual-loss. And the\nchoreography level selects the order of the performed movements and drives the\nsystem to follow the global structure of a dance genre. Our results demonstrate\nthe effectiveness of our music-driven framework to generate natural and\nconsistent movements on various dance types, having control over the content of\nthe synthesized motions, and respecting the overall structure of the dance.\n', ""  Automated choreography advances by generating dance from music. Current\nmethods create skeleton keypoint sequences, not full dance videos, and cannot\nmake specific individuals dance, limiting their real-world use. These methods\nalso need precise keypoint annotations, making data collection difficult and\nrestricting the use of self-made video datasets. To overcome these challenges,\nwe introduce a novel task: generating dance videos directly from images of\nindividuals guided by music. This task enables the dance generation of specific\nindividuals without requiring keypoint annotations, making it more versatile\nand applicable to various situations. Our solution, the Dance Any Beat\nDiffusion model (DabFusion), utilizes a reference image and a music piece to\ngenerate dance videos featuring various dance types and choreographies. The\nmusic is analyzed by our specially designed music encoder, which identifies\nessential features including dance style, movement, and rhythm. DabFusion\nexcels in generating dance videos not only for individuals in the training\ndataset but also for any previously unseen person. This versatility stems from\nits approach of generating latent optical flow, which contains all necessary\nmotion information to animate any person in the image. We evaluate DabFusion's\nperformance using the AIST++ dataset, focusing on video quality, audio-video\nsynchronization, and motion-music alignment. We propose a 2D Motion-Music\nAlignment Score (2D-MM Align), which builds on the Beat Alignment Score to more\neffectively evaluate motion-music alignment for this new task. Experiments show\nthat our DabFusion establishes a solid baseline for this innovative task. Video\nresults can be found on our project page: https://DabFusion.github.io.\n""]",Video Generation and Animation of Dance and Motion,Video and Image Processing Techniques,Image and Video Processing
245,245,38,245_cad_metamaterial_metamaterials_generative,"['cad', 'metamaterial', 'metamaterials', 'generative', 'meshing', 'mesh', 'designs', 'neural', 'designing', 'shapes']","['design', 'metamaterials', 'optimization', 'inverse', 'designs', 'mechanical', 'metamaterial', 'shape', 'microstructure', 'topology']","['  Mechanical metamaterials represent an innovative class of artificial\nstructures, distinguished by their extraordinary mechanical characteristics,\nwhich are beyond the scope of traditional natural materials. The use of deep\ngenerative models has become increasingly popular in the design of metamaterial\nunits. The effectiveness of using deep generative models lies in their capacity\nto compress complex input data into a simplified, lower-dimensional latent\nspace, while also enabling the creation of novel optimal designs through\nsampling within this space. However, the design process does not take into\naccount the effect of model uncertainty due to data sparsity or the effect of\ninput data uncertainty due to inherent randomness in the data. This might lead\nto the generation of undesirable structures with high sensitivity to the\nuncertainties in the system. To address this issue, a novel uncertainty-aware\ndeep learning framework-based robust design approach is proposed for the design\nof metamaterial units with optimal target properties. The proposed approach\nutilizes the probabilistic nature of the deep learning framework and quantifies\nboth aleatoric and epistemic uncertainties associated with surrogate-based\ndesign optimization. We demonstrate that the proposed design approach is\ncapable of designing high-performance metamaterial units with high reliability.\nTo showcase the effectiveness of the proposed design approach, a\nsingle-objective design optimization problem and a multi-objective design\noptimization problem are presented. The optimal robust designs obtained are\nvalidated by comparing them to the designs obtained from the topology\noptimization method as well as the designs obtained from a deterministic deep\nlearning framework-based design optimization where none of the uncertainties in\nthe system are explicitly considered.\n', '  Geometric Deep Learning techniques have become a transformative force in the\nfield of Computer-Aided Design (CAD), and have the potential to revolutionize\nhow designers and engineers approach and enhance the design process. By\nharnessing the power of machine learning-based methods, CAD designers can\noptimize their workflows, save time and effort while making better informed\ndecisions, and create designs that are both innovative and practical. The\nability to process the CAD designs represented by geometric data and to analyze\ntheir encoded features enables the identification of similarities among diverse\nCAD models, the proposition of alternative designs and enhancements, and even\nthe generation of novel design alternatives. This survey offers a comprehensive\noverview of learning-based methods in computer-aided design across various\ncategories, including similarity analysis and retrieval, 2D and 3D CAD model\nsynthesis, and CAD generation from point clouds. Additionally, it provides a\ncomplete list of benchmark datasets and their characteristics, along with\nopen-source codes that have propelled research in this domain. The final\ndiscussion delves into the challenges prevalent in this field, followed by\npotential future research directions in this rapidly evolving field.\n', ""  CAD (Computer-Aided Design) plays a crucial role in mechanical industry,\nwhere large numbers of similar-shaped CAD parts are often created. Efficiently\nreusing these parts is key to reducing design and production costs for\nenterprises. Retrieval systems are vital for achieving CAD reuse, but the\ncomplex shapes of CAD models are difficult to accurately describe using text or\nkeywords, making traditional retrieval methods ineffective. While existing\nrepresentation learning approaches have been developed for CAD, manually\nlabeling similar samples in these methods is expensive. Additionally, CAD\nmodels' unique parameterized data structure presents challenges for applying\nexisting 3D shape representation learning techniques directly. In this work, we\npropose GC-CAD, a self-supervised contrastive graph neural network-based method\nfor mechanical CAD retrieval that directly models parameterized CAD raw files.\nGC-CAD consists of two key modules: structure-aware representation learning and\ncontrastive graph learning framework. The method leverages graph neural\nnetworks to extract both geometric and topological information from CAD models,\ngenerating feature representations. We then introduce a simple yet effective\ncontrastive graph learning framework approach, enabling the model to train\nwithout manual labels and generate retrieval-ready representations.\nExperimental results on four datasets including human evaluation demonstrate\nthat the proposed method achieves significant accuracy improvements and up to\n100 times efficiency improvement over the baseline methods.\n""]",Metamaterial Design and CAD Optimization,Materials Modeling and Design Optimization,Optimization and Design
246,246,38,246_ai_agent_agents_cooperation,"['ai', 'agent', 'agents', 'cooperation', 'intelligence', 'warfare', 'autonomous', 'robot', 'intelligent', 'combat']","['team', 'trust', 'agents', 'agent', 'human', 'teams', 'combat', 'behavior', 'humans', 'autonomous']","[""  We anticipate increased instances of humans and AI systems working together\nin what we refer to as a hybrid team. The increase in collaboration is expected\nas AI systems gain proficiency and their adoption becomes more widespread.\nHowever, their behavior is not error-free, making hybrid teams a very suitable\nsolution. As such, we consider methods for improving performance for these\nteams of humans and AI systems. For hybrid teams, we will refer to both the\nhumans and AI systems as agents. To improve team performance over that seen for\nagents operating individually, we propose a manager which learns, through a\nstandard Reinforcement Learning scheme, how to best delegate, over time, the\nresponsibility of taking a decision to any of the agents. We further guide the\nmanager's learning so they also minimize how many changes in delegation are\nmade resulting from undesirable team behavior. We demonstrate the optimality of\nour manager's performance in several grid environments which include failure\nstates which terminate an episode and should be avoided. We perform our\nexperiments with teams of agents with varying degrees of acceptable risk, in\nthe form of proximity to a failure state, and measure the manager's ability to\nmake effective delegation decisions with respect to its own risk-based\nconstraints, then compare these to the optimal decisions. Our results show our\nmanager can successfully learn desirable delegations which result in team paths\nnear/exactly optimal with respect to path length and number of delegations.\n"", ""  Defining and measuring trust in dynamic, multiagent teams is important in a\nrange of contexts, particularly in defense and security domains. Team members\nshould be trusted to work towards agreed goals and in accordance with shared\nvalues. In this paper, our concern is with the definition of goals and values\nsuch that it is possible to define 'trust' in a way that is interpretable, and\nhence usable, by both humans and robots. We argue that the outcome of team\nactivity can be considered in terms of 'goal', 'individual/team values', and\n'legal principles'. We question whether alignment is possible at the level of\n'individual/team values', or only at the 'goal' and 'legal principles' levels.\nWe argue for a set of metrics to define trust in human-robot teams that are\ninterpretable by human or robot team members, and consider an experiment that\ncould demonstrate the notion of 'satisficing trust' over the course of a\nsimulated mission.\n"", '  Future warfare will require Command and Control (C2) personnel to make\ndecisions at shrinking timescales in complex and potentially ill-defined\nsituations. Given the need for robust decision-making processes and\ndecision-support tools, integration of artificial and human intelligence holds\nthe potential to revolutionize the C2 operations process to ensure adaptability\nand efficiency in rapidly changing operational environments. We propose to\nleverage recent promising breakthroughs in interactive machine learning, in\nwhich humans can cooperate with machine learning algorithms to guide machine\nlearning algorithm behavior. This paper identifies several gaps in\nstate-of-the-art science and technology that future work should address to\nextend these approaches to function in complex C2 contexts. In particular, we\ndescribe three research focus areas that together, aim to enable scalable\ninteractive machine learning (SIML): 1) developing human-AI interaction\nalgorithms to enable planning in complex, dynamic situations; 2) fostering\nresilient human-AI teams through optimizing roles, configurations, and trust;\nand 3) scaling algorithms and human-AI teams for flexibility across a range of\npotential contexts and situations.\n']",Human-AI Collaboration and Teamwork,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Cognitive Systems
247,247,37,247_generative_synthetic_datasets_models,"['generative', 'synthetic', 'datasets', 'models', 'learning', 'classification', 'data', 'trained', 'unlearning', 'artificial']","['synthetic', 'data', 'augmentation', 'generative', 'fake', 'real', 'images', 'spurious', 'image', 'samples']","['  This paper provides a detailed survey of synthetic data techniques. We first\ndiscuss the expected goals of using synthetic data in data augmentation, which\ncan be divided into four parts: 1) Improving Diversity, 2) Data Balancing, 3)\nAddressing Domain Shift, and 4) Resolving Edge Cases. Synthesizing data are\nclosely related to the prevailing machine learning techniques at the time,\ntherefore, we summarize the domain of synthetic data techniques into four\ncategories: 1) Expert-knowledge, 2) Direct Training, 3) Pre-train then\nFine-tune, and 4) Foundation Models without Fine-tuning. Next, we categorize\nthe goals of synthetic data filtering into four types for discussion: 1) Basic\nQuality, 2) Label Consistency, and 3) Data Distribution. In section 5 of this\npaper, we also discuss the future directions of synthetic data and state three\ndirection that we believe is important: 1) focus more on quality, 2) the\nevaluation of synthetic data, and 3) multi-model data augmentation.\n', '  Synthetic data algorithms are widely employed in industries to generate\nartificial data for downstream learning tasks. While existing research\nprimarily focuses on empirically evaluating utility of synthetic data, its\ntheoretical understanding is largely lacking. This paper bridges the\npractice-theory gap by establishing relevant utility theory in a statistical\nlearning framework. It considers two utility metrics: generalization and\nranking of models trained on synthetic data. The former is defined as the\ngeneralization difference between models trained on synthetic and on real data.\nBy deriving analytical bounds for this utility metric, we demonstrate that the\nsynthetic feature distribution does not need to be similar as that of real data\nfor ensuring comparable generalization of synthetic models, provided proper\nmodel specifications in downstream learning tasks. The latter utility metric\nstudies the relative performance of models trained on synthetic data. In\nparticular, we discover that the distribution of synthetic data is not\nnecessarily similar as the real one to ensure consistent model comparison.\nInterestingly, consistent model comparison is still achievable even when\nsynthetic responses are not well generated, as long as downstream models are\nseparable by a generalization gap. Finally, extensive experiments on\nnon-parametric models and deep neural networks have been conducted to validate\nthese theoretical findings.\n', '  We study, from an empirical standpoint, the efficacy of synthetic data in\nreal-world scenarios. Leveraging synthetic data for training perception models\nhas become a key strategy embraced by the community due to its efficiency,\nscalability, perfect annotations, and low costs. Despite proven advantages, few\nstudies put their stress on how to efficiently generate synthetic datasets to\nsolve real-world problems and to what extent synthetic data can reduce the\neffort for real-world data collection. To answer the questions, we\nsystematically investigate several interesting properties of synthetic data --\nthe equivalency of synthetic data to real-world data, the substitutability of\nsynthetic data for real data, and the flexibility of synthetic data generators\nto close up domain gaps. Leveraging the M3Act synthetic data generator, we\nconduct experiments on DanceTrack and MOT17. Our results suggest that synthetic\ndata not only enhances model performance but also demonstrates substitutability\nfor real data, with 60% to 80% replacement without performance loss. In\naddition, our study of the impact of synthetic data distributions on downstream\nperformance reveals the importance of flexible data generators in narrowing\ndomain gaps for improved model adaptability.\n']",Synthetic Data Generation and Applications,Synthetic Data Generation and Applications,Artificial Intelligence in Data Generation and Chemical Synthesis
248,248,37,248_fuzzy_classifier_classification_intuitionistic,"['fuzzy', 'classifier', 'classification', 'intuitionistic', 'feature', 'features', 'ensemble', 'prediction', 'datasets', 'kernelshap']","['fuzzy', 'intuitionistic', 'black', 'box', 'rule', 'feature', 'decision', 'interpretability', 'interpretable', 'importance']","['  The ensemble deep random vector functional link (edRVFL) neural network has\ndemonstrated the ability to address the limitations of conventional artificial\nneural networks. However, since edRVFL generates features for its hidden layers\nthrough random projection, it can potentially lose intricate features or fail\nto capture certain non-linear features in its base models (hidden layers). To\nenhance the feature learning capabilities of edRVFL, we propose a novel edRVFL\nbased on fuzzy inference system (edRVFL-FIS). The proposed edRVFL-FIS leverages\nthe capabilities of two emerging domains, namely deep learning and ensemble\napproaches, with the intrinsic IF-THEN properties of fuzzy inference system\n(FIS) and produces rich feature representation to train the ensemble model.\nEach base model of the proposed edRVFL-FIS encompasses two key feature\naugmentation components: a) unsupervised fuzzy layer features and b) supervised\ndefuzzified features. The edRVFL-FIS model incorporates diverse clustering\nmethods (R-means, K-means, Fuzzy C-means) to establish fuzzy layer rules,\nresulting in three model variations (edRVFL-FIS-R, edRVFL-FIS-K, edRVFL-FIS-C)\nwith distinct fuzzified features and defuzzified features. Within the framework\nof edRVFL-FIS, each base model utilizes the original, hidden layer and\ndefuzzified features to make predictions. Experimental results, statistical\ntests, discussions and analyses conducted across UCI and NDC datasets\nconsistently demonstrate the superior performance of all variations of the\nproposed edRVFL-FIS model over baseline models. The source codes of the\nproposed models are available at https://github.com/mtanveer1/edRVFL-FIS.\n', '  Classification is essential to the applications in the field of data mining,\nartificial intelligence, and fault detection. There exists a strong need in\ndeveloping accurate, suitable, and efficient classification methods and\nalgorithms with broad applicability. Random forest is a general algorithm that\nis often used for classification under complex conditions. Although it has been\nwidely adopted, its combination with diverse fuzzy theory is still worth\nexploring. In this paper, we propose the intuitionistic fuzzy random forest\n(IFRF), a new random forest ensemble of intuitionistic fuzzy decision trees\n(IFDT). Such trees in forest use intuitionistic fuzzy information gain to\nselect features and consider hesitation in information transmission. The\nproposed method enjoys the power of the randomness from bootstrapped sampling\nand feature selection, the flexibility of fuzzy logic and fuzzy sets, and the\nrobustness of multiple classifier systems. Extensive experiments demonstrate\nthat the IFRF has competitative and superior performance compared to other\nstate-of-the-art fuzzy and ensemble algorithms. IFDT is more suitable for\nensemble learning with outstanding classification accuracy. This study is the\nfirst to propose a random forest ensemble based on the intuitionistic fuzzy\ntheory.\n', '  An important constraint of Fuzzy Inference Systems (FIS) is their structured\nrules defined based on evaluating all input variables. Indeed, the length of\nall fuzzy rules and the number of input variables are equal. However, in many\ndecision-making problems evaluating some conditions on a limited set of input\nvariables is sufficient to decide properly (unstructured rules). Therefore,\nthis constraint limits the performance, generalization, and interpretability of\nthe FIS. To address this issue, this paper presents a neuro-fuzzy inference\nsystem for classification applications that can select different sets of input\nvariables for constructing each fuzzy rule. To realize this capability, a new\nfuzzy selector neuron with an adaptive parameter is proposed that can select\ninput variables in the antecedent part of each fuzzy rule. Moreover, in this\npaper, the consequent part of the Takagi-Sugeno-Kang FIS is also changed\nproperly to consider only the selected set of input variables. To learn the\nparameters of the proposed architecture, a trust-region-based learning method\n(General quasi-Levenberg-Marquardt (GqLM)) is proposed to minimize\ncross-entropy in multiclass problems. The performance of the proposed method is\ncompared with some related previous approaches in some real-world\nclassification problems. Based on these comparisons the proposed method has\nbetter or very close performance with a parsimonious structure consisting of\nunstructured fuzzy.\n']",Fuzzy Ensemble Classifiers for Feature Learning,Ensemble Methods and Classification Techniques,Machine Learning Ensembles and Multi-View Methods
249,249,37,249_experimentation_experimenters_statistical_experiments,"['experimentation', 'experimenters', 'statistical', 'experiments', 'metrics', 'empirical', 'randomized', 'testing', 'estimators', 'metric']","['experiment', 'experimentation', 'treatment', 'statistical', 'experiments', 'tests', 'randomized', 'metrics', 'term', 'error']","['  In designing an online A/B experiment, it is crucial to select a sample size\nand duration that ensure the resulting confidence interval (CI) for the\ntreatment effect is the right width to detect an effect of meaningful magnitude\nwith sufficient statistical power without wasting resources. While the\nrelationship between sample size and CI width is well understood, the effect of\nexperiment duration on CI width remains less clear. This paper provides an\nanalytical formula for the width of a CI based on a ratio treatment effect\nestimator as a function of both sample size (N) and duration (T). The formula\nis derived from a mixed effects model with two variance components. One\ncomponent, referred to as the temporal variance, persists over time for\nexperiments where the same users are kept in the same experiment arm across\ndifferent days. The remaining error variance component, by contrast, decays to\nzero as T gets large. The formula we derive introduces a key parameter that we\ncall the user-specific temporal correlation (UTC), which quantifies the\nrelative sizes of the two variance components and can be estimated from\nhistorical experiments. Higher UTC indicates a slower decay in CI width over\ntime. On the other hand, when the UTC is 0 -- as for experiments where users\nshuffle in and out of the experiment across days -- the CI width decays at the\nstandard parametric 1/T rate. We also study how access to pre-period data for\nthe users in the experiment affects the CI width decay. We show our formula\nclosely explains CI widths on real A/B experiments at YouTube.\n', '  In many randomized experiments, the treatment effect of the long-term metric\n(i.e. the primary outcome of interest) is often difficult or infeasible to\nmeasure. Such long-term metrics are often slow to react to changes and\nsufficiently noisy they are challenging to faithfully estimate in short-horizon\nexperiments. A common alternative is to measure several short-term proxy\nmetrics in the hope they closely track the long-term metric -- so they can be\nused to effectively guide decision-making in the near-term. We introduce a new\nstatistical framework to both define and construct an optimal proxy metric for\nuse in a homogeneous population of randomized experiments. Our procedure first\nreduces the construction of an optimal proxy metric in a given experiment to a\nportfolio optimization problem which depends on the true latent treatment\neffects and noise level of experiment under consideration. We then denoise the\nobserved treatment effects of the long-term metric and a set of proxies in a\nhistorical corpus of randomized experiments to extract estimates of the latent\ntreatment effects for use in the optimization problem. One key insight derived\nfrom our approach is that the optimal proxy metric for a given experiment is\nnot apriori fixed; rather it should depend on the sample size (or effective\nnoise level) of the randomized experiment for which it is deployed. To\ninstantiate and evaluate our framework, we employ our methodology in a large\ncorpus of randomized experiments from an industrial recommendation system and\nconstruct proxy metrics that perform favorably relative to several baselines.\n', ""  Online controlled experiments, colloquially known as A/B-tests, are the bread\nand butter of real-world recommender system evaluation. Typically, end-users\nare randomly assigned some system variant, and a plethora of metrics are then\ntracked, collected, and aggregated throughout the experiment. A North Star\nmetric (e.g. long-term growth or revenue) is used to assess which system\nvariant should be deemed superior. As a result, most collected metrics are\nsupporting in nature, and serve to either (i) provide an understanding of how\nthe experiment impacts user experience, or (ii) allow for confident\ndecision-making when the North Star metric moves insignificantly (i.e. a false\nnegative or type-II error). The latter is not straightforward: suppose a\ntreatment variant leads to fewer but longer sessions, with more views but fewer\nengagements; should this be considered a positive or negative outcome?\n  The question then becomes: how do we assess a supporting metric's utility\nwhen it comes to decision-making using A/B-testing? Online platforms typically\nrun dozens of experiments at any given time. This provides a wealth of\ninformation about interventions and treatment effects that can be used to\nevaluate metrics' utility for online evaluation. We propose to collect this\ninformation and leverage it to quantify type-I, type-II, and type-III errors\nfor the metrics of interest, alongside a distribution of measurements of their\nstatistical power (e.g. $z$-scores and $p$-values). We present results and\ninsights from building this pipeline at scale for two large-scale short-video\nplatforms: ShareChat and Moj; leveraging hundreds of past experiments to find\nonline metrics with high statistical power.\n""]",Experiment Design and Analysis,Design Automation and Experimentation in Engineering,Design and Experimentation in Engineering
250,250,37,250_entropy_variational_generative_gaussian,"['entropy', 'variational', 'generative', 'gaussian', 'divergence', 'densities', 'estimation', 'approximations', 'likelihood', 'mixtures']","['density', 'distributions', 'entropy', 'kernel', 'mixture', 'conditional', 'entropies', 'divergence', 'distribution', 'mixtures']","['  Standard probabilistic sparse coding assumes a Laplace prior, a linear\nmapping from latents to observables, and Gaussian observable distributions. We\nhere derive a solely entropy-based learning objective for the parameters of\nstandard sparse coding. The novel variational objective has the following\nfeatures: (A) unlike MAP approximations, it uses non-trivial posterior\napproximations for probabilistic inference; (B) unlike for previous non-trivial\napproximations, the novel objective is fully analytical; and (C) the objective\nallows for a novel principled form of annealing. The objective is derived by\nfirst showing that the standard ELBO objective converges to a sum of entropies,\nwhich matches similar recent results for generative models with Gaussian\npriors. The conditions under which the ELBO becomes equal to entropies are then\nshown to have analytical solutions, which leads to the fully analytical\nobjective. Numerical experiments are used to demonstrate the feasibility of\nlearning with such entropy-based ELBOs. We investigate different posterior\napproximations including Gaussians with correlated latents and deep amortized\napproximations. Furthermore, we numerically investigate entropy-based annealing\nwhich results in improved learning. Our main contributions are theoretical,\nhowever, and they are twofold: (1) for non-trivial posterior approximations, we\nprovide the (to the knowledge of the authors) first analytical ELBO objective\nfor standard probabilistic sparse coding; and (2) we provide the first\ndemonstration on how a recently shown convergence of the ELBO to entropy sums\ncan be used for learning.\n', '  The central objective function of a variational autoencoder (VAE) is its\nvariational lower bound (the ELBO). Here we show that for standard (i.e.,\nGaussian) VAEs the ELBO converges to a value given by the sum of three\nentropies: the (negative) entropy of the prior distribution, the expected\n(negative) entropy of the observable distribution, and the average entropy of\nthe variational distributions (the latter is already part of the ELBO). Our\nderived analytical results are exact and apply for small as well as for\nintricate deep networks for encoder and decoder. Furthermore, they apply for\nfinitely and infinitely many data points and at any stationary point (including\nlocal maxima and saddle points). The result implies that the ELBO can for\nstandard VAEs often be computed in closed-form at stationary points while the\noriginal ELBO requires numerical approximations of integrals. As a main\ncontribution, we provide the proof that the ELBO for VAEs is at stationary\npoints equal to entropy sums. Numerical experiments then show that the obtained\nanalytical results are sufficiently precise also in those vicinities of\nstationary points that are reached in practice. Furthermore, we discuss how the\nnovel entropy form of the ELBO can be used to analyze and understand learning\nbehavior. More generally, we believe that our contributions can be useful for\nfuture theoretical and practical studies on VAE learning as they provide novel\ninformation on those points in parameters space that optimization of VAEs\nconverges to.\n', ""  The variational lower bound (a.k.a. ELBO or free energy) is the central\nobjective for many established as well as many novel algorithms for\nunsupervised learning. During learning such algorithms change model parameters\nto increase the variational lower bound. Learning usually proceeds until\nparameters have converged to values close to a stationary point of the learning\ndynamics. In this purely theoretical contribution, we show that (for a very\nlarge class of generative models) the variational lower bound is at all\nstationary points of learning equal to a sum of entropies. For standard machine\nlearning models with one set of latents and one set of observed variables, the\nsum consists of three entropies: (A) the (average) entropy of the variational\ndistributions, (B) the negative entropy of the model's prior distribution, and\n(C) the (expected) negative entropy of the observable distribution. The\nobtained result applies under realistic conditions including: finite numbers of\ndata points, at any stationary point (including saddle points) and for any\nfamily of (well behaved) variational distributions. The class of generative\nmodels for which we show the equality to entropy sums contains many well-known\ngenerative models. As concrete examples we discuss Sigmoid Belief Networks,\nprobabilistic PCA and (Gaussian and non-Gaussian) mixture models. The result\nalso applies for standard (Gaussian) variational autoencoders, a special case\nthat has been shown previously (Damm et al., 2023). The prerequisites we use to\nshow equality to entropy sums are relatively mild. Concretely, the\ndistributions of a given generative model have to be of the exponential family,\nand the model has to satisfy a parameterization criterion (which is usually\nfulfilled). Proving the equality of the ELBO to entropy sums at stationary\npoints (under the stated conditions) is the main contribution of this work.\n""]",Variational Inference and Entropy in Generative Models,Variational Methods for Bayesian Modeling and Generative Learning,Generative Modeling and Artificial Intelligence
251,251,37,251_backpropagation_rnns_neural_rnn,"['backpropagation', 'rnns', 'neural', 'rnn', 'networks', 'neurons', 'gradients', 'recurrent', 'gradient', 'dynamical']","['recurrent', 'networks', 'neural', 'approximation', 'network', 'gradient', 'flossing', 'residual', 'activation', 'nonlinear']","['  We propose a novel, brain-inspired deep neural network model known as the\nDeep Oscillatory Neural Network (DONN). Deep neural networks like the Recurrent\nNeural Networks indeed possess sequence processing capabilities but the\ninternal states of the network are not designed to exhibit brain-like\noscillatory activity. With this motivation, the DONN is designed to have\noscillatory internal dynamics. Neurons of the DONN are either nonlinear neural\noscillators or traditional neurons with sigmoidal or ReLU activation. The\nneural oscillator used in the model is the Hopf oscillator, with the dynamics\ndescribed in the complex domain. Input can be presented to the neural\noscillator in three possible modes. The sigmoid and ReLU neurons also use\ncomplex-valued extensions. All the weight stages are also complex-valued.\nTraining follows the general principle of weight change by minimizing the\noutput error and therefore has an overall resemblance to complex\nbackpropagation. A generalization of DONN to convolutional networks known as\nthe Oscillatory Convolutional Neural Network is also proposed. The two proposed\noscillatory networks are applied to a variety of benchmark problems in signal\nand image/video processing. The performance of the proposed models is either\ncomparable or superior to published results on the same data sets.\n', '  Residual connections have been proposed as an architecture-based inductive\nbias to mitigate the problem of exploding and vanishing gradients and increased\ntask performance in both feed-forward and recurrent networks (RNNs) when\ntrained with the backpropagation algorithm. Yet, little is known about how\nresidual connections in RNNs influence their dynamics and fading memory\nproperties. Here, we introduce weakly coupled residual recurrent networks\n(WCRNNs) in which residual connections result in well-defined Lyapunov\nexponents and allow for studying properties of fading memory. We investigate\nhow the residual connections of WCRNNs influence their performance, network\ndynamics, and memory properties on a set of benchmark tasks. We show that\nseveral distinct forms of residual connections yield effective inductive biases\nthat result in increased network expressivity. In particular, those are\nresidual connections that (i) result in network dynamics at the proximity of\nthe edge of chaos, (ii) allow networks to capitalize on characteristic spectral\nproperties of the data, and (iii) result in heterogeneous memory properties. In\naddition, we demonstrate how our results can be extended to non-linear\nresiduals and introduce a weakly coupled residual initialization scheme that\ncan be used for Elman RNNs.\n', '  We analyze recurrent neural networks trained with gradient descent in the\nsupervised learning setting for dynamical systems, and prove that gradient\ndescent can achieve optimality \\emph{without} massive overparameterization. Our\nin-depth nonasymptotic analysis (i) provides sharp bounds on the network size\n$m$ and iteration complexity $\\tau$ in terms of the sequence length $T$, sample\nsize $n$ and ambient dimension $d$, and (ii) identifies the significant impact\nof long-term dependencies in the dynamical system on the convergence and\nnetwork width bounds characterized by a cutoff point that depends on the\nLipschitz continuity of the activation function. Remarkably, this analysis\nreveals that an appropriately-initialized recurrent neural network trained with\n$n$ samples can achieve optimality with a network size $m$ that scales only\nlogarithmically with $n$. This sharply contrasts with the prior works that\nrequire high-order polynomial dependency of $m$ on $n$ to establish strong\nregularity conditions. Our results are based on an explicit characterization of\nthe class of dynamical systems that can be approximated and learned by\nrecurrent neural networks via norm-constrained transportation mappings, and\nestablishing local smoothness properties of the hidden state with respect to\nthe learnable parameters.\n']",Recurrent Neural Networks and Oscillatory Dynamics,Neural Networks for Modeling and Control of Dynamical Systems,Machine Learning for Dynamical Systems and Differential Equations
252,252,37,252_imputation_forecasting_datasets_data,"['imputation', 'forecasting', 'datasets', 'data', 'spatiotemporal', 'recurrent', 'monitoring', 'incomplete', 'temporal', 'ito']","['series', 'imputation', 'time', 'anomalies', 'multivariate', 'missing', 'industrial', 'sensor', 'forecasting', 'values']","['  The intricate nature of time series data analysis benefits greatly from the\ndistinct advantages offered by time and frequency domain representations. While\nthe time domain is superior in representing local dependencies, particularly in\nnon-periodic series, the frequency domain excels in capturing global\ndependencies, making it ideal for series with evident periodic patterns. To\ncapitalize on both of these strengths, we propose ATFNet, an innovative\nframework that combines a time domain module and a frequency domain module to\nconcurrently capture local and global dependencies in time series data.\nSpecifically, we introduce Dominant Harmonic Series Energy Weighting, a novel\nmechanism for dynamically adjusting the weights between the two modules based\non the periodicity of the input time series. In the frequency domain module, we\nenhance the traditional Discrete Fourier Transform (DFT) with our Extended DFT,\ndesigned to address the challenge of discrete frequency misalignment.\nAdditionally, our Complex-valued Spectrum Attention mechanism offers a novel\napproach to discern the intricate relationships between different frequency\ncombinations. Extensive experiments across multiple real-world datasets\ndemonstrate that our ATFNet framework outperforms current state-of-the-art\nmethods in long-term time series forecasting.\n', ""  We introduce a novel modeling approach for time series imputation and\nforecasting, tailored to address the challenges often encountered in real-world\ndata, such as irregular samples, missing data, or unaligned measurements from\nmultiple sensors. Our method relies on a continuous-time-dependent model of the\nseries' evolution dynamics. It leverages adaptations of conditional, implicit\nneural representations for sequential data. A modulation mechanism, driven by a\nmeta-learning algorithm, allows adaptation to unseen samples and extrapolation\nbeyond observed time-windows for long-term predictions. The model provides a\nhighly flexible and unified framework for imputation and forecasting tasks\nacross a wide range of challenging scenarios. It achieves state-of-the-art\nperformance on classical benchmarks and outperforms alternative time-continuous\nmodels.\n"", '  In real-world scenarios like traffic and energy, massive time-series data\nwith missing values and noises are widely observed, even sampled irregularly.\nWhile many imputation methods have been proposed, most of them work with a\nlocal horizon, which means models are trained by splitting the long sequence\ninto batches of fit-sized patches. This local horizon can make models ignore\nglobal trends or periodic patterns. More importantly, almost all methods assume\nthe observations are sampled at regular time stamps, and fail to handle complex\nirregular sampled time series arising from different applications. Thirdly,\nmost existing methods are learned in an offline manner. Thus, it is not\nsuitable for many applications with fast-arriving streaming data. To overcome\nthese limitations, we propose BayOTIDE: Bayesian Online Multivariate Time\nseries Imputation with functional decomposition. We treat the multivariate time\nseries as the weighted combination of groups of low-rank temporal factors with\ndifferent patterns. We apply a group of Gaussian Processes (GPs) with different\nkernels as functional priors to fit the factors. For computational efficiency,\nwe further convert the GPs into a state-space prior by constructing an\nequivalent stochastic differential equation (SDE), and developing a scalable\nalgorithm for online inference. The proposed method can not only handle\nimputation over arbitrary time stamps, but also offer uncertainty\nquantification and interpretability for the downstream application. We evaluate\nour method on both synthetic and real-world datasets.We release the code at\n{https://github.com/xuangu-fang/BayOTIDE}\n']",Time Series Analysis and Forecasting,Time Series Analysis and Prediction,Predictive Modeling and Forecasting
253,253,37,253_geoparsing_geospatial_geo_geolocalization,"['geoparsing', 'geospatial', 'geo', 'geolocalization', 'geographic', 'geolocation', 'geographical', 'geolinguistic', 'geollm', 'geolingit']","['geospatial', 'location', 'spatial', 'geographic', 'geographical', 'locations', 'geolocation', 'social', 'geoparsing', 'expressions']","['  Geoparsing is the task of estimating the latitude and longitude (coordinates)\nof location expressions in texts. Geoparsing must deal with the ambiguity of\nthe expressions that indicate multiple locations with the same notation. For\nevaluating geoparsing systems, several corpora have been proposed in previous\nwork. However, these corpora are small-scale and suffer from the coverage of\nlocation expressions on general domains. In this paper, we propose Wikipedia\nHyperlink-based Location Linking (WHLL), a novel method to construct a\nlarge-scale corpus for geoparsing from Wikipedia articles. WHLL leverages\nhyperlinks in Wikipedia to annotate multiple location expressions with\ncoordinates. With this method, we constructed the WHLL corpus, a new\nlarge-scale corpus for geoparsing. The WHLL corpus consists of 1.3M articles,\neach containing about 7.8 unique location expressions. 45.6% of location\nexpressions are ambiguous and refer to more than one location with the same\nnotation. In each article, location expressions of the article title and those\nhyperlinks to other articles are assigned with coordinates. By utilizing\nhyperlinks, we can accurately assign location expressions with coordinates even\nwith ambiguous location expressions in the texts. Experimental results show\nthat there remains room for improvement by disambiguating location expressions.\n', ""  Social geolocation is an important problem of predicting the originating\nlocations of social media posts. However, this task is challenging due to the\nneed for a substantial volume of training data, alongside well-annotated\nlabels. These issues are further exacerbated by new or less popular locations\nwith insufficient labels, further leading to an imbalanced dataset. In this\npaper, we propose \\textbf{ContrastGeo}, a \\textbf{Contrast}ive learning\nenhanced framework for few-shot social \\textbf{Geo}location. Specifically, a\nTweet-Location Contrastive learning objective is introduced to align\nrepresentations of tweets and locations within tweet-location pairs. To capture\nthe correlations between tweets and locations, a Tweet-Location Matching\nobjective is further adopted into the framework and refined via an online hard\nnegative mining approach. We also develop three fusion strategies with various\nfusion encoders to better generate joint representations of tweets and\nlocations. Comprehensive experiments on three social media datasets highlight\nContrastGeo's superior performance over several state-of-the-art baselines in\nfew-shot social geolocation.\n"", '  Geospatial Location Embedding (GLE) helps a Large Language Model (LLM)\nassimilate and analyze spatial data. GLE emergence in Geospatial Artificial\nIntelligence (GeoAI) is precipitated by the need for deeper geospatial\nawareness in our complex contemporary spaces and the success of LLMs in\nextracting deep meaning in Generative AI. We searched Google Scholar, Science\nDirect, and arXiv for papers on geospatial location embedding and LLM and\nreviewed articles focused on gaining deeper spatial ""knowing"" through LLMs. We\nscreened 304 titles, 30 abstracts, and 18 full-text papers that reveal four GLE\nthemes - Entity Location Embedding (ELE), Document Location Embedding (DLE),\nSequence Location Embedding (SLE), and Token Location Embedding (TLE).\nSynthesis is tabular and narrative, including a dialogic conversation between\n""Space"" and ""LLM."" Though GLEs aid spatial understanding by superimposing\nspatial data, they emphasize the need to advance in the intricacies of spatial\nmodalities and generalized reasoning. GLEs signal the need for a Spatial\nFoundation/Language Model (SLM) that embeds spatial knowing within the model\narchitecture. The SLM framework advances Spatial Artificial Intelligence\nSystems (SPAIS), establishing a Spatial Vector Space (SVS) that maps to\nphysical space. The resulting spatially imbued Language Model is unique. It\nsimultaneously represents actual space and an AI-capable space, paving the way\nfor AI native geo storage, analysis, and multi-modality as the basis for\nSpatial Artificial Intelligence Systems (SPAIS).\n']",Geospatial Location Analysis and Geoparsing,Geospatial Information Retrieval and Analysis,Information Retrieval and Knowledge Systems
254,254,36,254_embeddings_nlp_embedding_attention,"['embeddings', 'nlp', 'embedding', 'attention', 'textual', 'encoder', 'descriptors', 'detection', 'representations', 'semantic']","['embeddings', 'hyperbolic', 'interpretable', 'representation', 'semantic', 'detection', 'feature', 'attribution', 'task', 'interpretability']","['  For natural language understanding and generation, embedding concepts using\nan order-based representation is an essential task. Unlike traditional point\nvector based representation, an order-based representation imposes geometric\nconstraints on the representation vectors for explicitly capturing various\nsemantic relationships that may exist between a pair of concepts. In existing\nliterature, several approaches on order-based embedding have been proposed,\nmostly focusing on capturing hierarchical relationships; examples include\nvectors in Euclidean space, complex, Hyperbolic, order, and Box Embedding. Box\nembedding creates region-based rich representation of concepts, but along the\nprocess it sacrifices simplicity, requiring a custom-made optimization scheme\nfor learning the representation. Hyperbolic embedding improves embedding\nquality by exploiting the ever-expanding property of Hyperbolic space, but it\nalso suffers from the same fate as box embedding as gradient descent like\noptimization is not simple in the Hyperbolic space. In this work, we propose\nBinder, a novel approach for order-based representation. Binder uses binary\nvectors for embedding, so the embedding vectors are compact with an order of\nmagnitude smaller footprint than other methods. Binder uses a simple and\nefficient optimization scheme for learning representation vectors with a linear\ntime complexity. Our comprehensive experimental results show that Binder is\nvery accurate, yielding competitive results on the representation task. But\nBinder stands out from its competitors on the transitive closure link\nprediction task as it can learn concept embeddings just from the direct edges,\nwhereas all existing order-based approaches rely on the indirect edges.\n', '  Out-of-distribution (OOD) detection plays a crucial role in ensuring the\nsafety and reliability of deep neural networks in various applications. While\nthere has been a growing focus on OOD detection in visual data, the field of\ntextual OOD detection has received less attention. Only a few attempts have\nbeen made to directly apply general OOD detection methods to natural language\nprocessing (NLP) tasks, without adequately considering the characteristics of\ntextual data. In this paper, we delve into textual OOD detection with\nTransformers. We first identify a key problem prevalent in existing OOD\ndetection methods: the biased representation learned through the maximization\nof the conditional likelihood $p(y\\mid x)$ can potentially result in subpar\nperformance. We then propose a novel variational inference framework for OOD\ndetection (VI-OOD), which maximizes the likelihood of the joint distribution\n$p(x, y)$ instead of $p(y\\mid x)$. VI-OOD is tailored for textual OOD detection\nby efficiently exploiting the representations of pre-trained Transformers.\nThrough comprehensive experiments on various text classification tasks, VI-OOD\ndemonstrates its effectiveness and wide applicability. Our code has been\nreleased at \\url{https://github.com/liam0949/LLM-OOD}.\n', '  Out-of-distribution (OOD) detection plays a vital role in enhancing the\nreliability of machine learning (ML) models. The emergence of large language\nmodels (LLMs) has catalyzed a paradigm shift within the ML community,\nshowcasing their exceptional capabilities across diverse natural language\nprocessing tasks. While existing research has probed OOD detection with\nrelative small-scale Transformers like BERT, RoBERTa and GPT-2, the stark\ndifferences in scales, pre-training objectives, and inference paradigms call\ninto question the applicability of these findings to LLMs. This paper embarks\non a pioneering empirical investigation of OOD detection in the domain of LLMs,\nfocusing on LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate\ncommonly-used OOD detectors, scrutinizing their performance in both zero-grad\nand fine-tuning scenarios. Notably, we alter previous discriminative\nin-distribution fine-tuning into generative fine-tuning, aligning the\npre-training objective of LLMs with downstream tasks. Our findings unveil that\na simple cosine distance OOD detector demonstrates superior efficacy,\noutperforming other OOD detectors. We provide an intriguing explanation for\nthis phenomenon by highlighting the isotropic nature of the embedding spaces of\nLLMs, which distinctly contrasts with the anisotropic property observed in\nsmaller BERT family models. The new insight enhances our understanding of how\nLLMs detect OOD data, thereby enhancing their adaptability and reliability in\ndynamic environments. We have released the source code at\n\\url{https://github.com/Awenbocc/LLM-OOD} for other researchers to reproduce\nour results.\n']",Natural Language Embeddings and Detection,Graph and Text Embeddings for Representation Learning,Graph Representation Learning and Neural Networks
255,255,36,255_optimizing_optimize_generative_optimization,"['optimizing', 'optimize', 'generative', 'optimization', 'offline', 'bandit', 'surrogate', 'search', 'gradients', 'algorithms']","['offline', 'optimization', 'protein', 'surrogate', 'designs', 'actuarial', 'black', 'box', 'uncertainty', 'sequences']","['  This paper considers the problem of offline optimization, where the objective\nfunction is unknown except for a collection of ``offline"" data examples. While\nrecent years have seen a flurry of work on applying various machine learning\ntechniques to the offline optimization problem, the majority of these work\nfocused on learning a surrogate of the unknown objective function and then\napplying existing optimization algorithms. While the idea of modeling the\nunknown objective function is intuitive and appealing, from the learning point\nof view it also makes it very difficult to tune the objective of the learner\naccording to the objective of optimization. Instead of learning and then\noptimizing the unknown objective function, in this paper we take on a less\nintuitive but more direct view that optimization can be thought of as a process\nof sampling from a generative model. To learn an effective generative model\nfrom the offline data examples, we consider the standard technique of\n``re-weighting"", and our main technical contribution is a probably\napproximately correct (PAC) lower bound on the natural optimization objective,\nwhich allows us to jointly learn a weight function and a score-based generative\nmodel. The robustly competitive performance of the proposed approach is\ndemonstrated via empirical studies using the standard offline optimization\nbenchmarks.\n', '  Offline model-based optimization (MBO) aims to maximize a black-box objective\nfunction using only an offline dataset of designs and scores. A prevalent\napproach involves training a conditional generative model on existing designs\nand their associated scores, followed by the generation of new designs\nconditioned on higher target scores. However, these newly generated designs\noften underperform due to the lack of high-scoring training data. To address\nthis challenge, we introduce a novel method, Design Editing for Offline\nModel-based Optimization (DEMO), which consists of two phases. In the first\nphase, termed pseudo-target distribution generation, we apply gradient ascent\non the offline dataset using a trained surrogate model, producing a synthetic\ndataset where the predicted scores serve as new labels. A conditional diffusion\nmodel is subsequently trained on this synthetic dataset to capture a\npseudo-target distribution, which enhances the accuracy of the conditional\ndiffusion model in generating higher-scoring designs. Nevertheless, the\npseudo-target distribution is susceptible to noise stemming from inaccuracies\nin the surrogate model, consequently predisposing the conditional diffusion\nmodel to generate suboptimal designs. We hence propose the second phase,\nexisting design editing, to directly incorporate the high-scoring features from\nthe offline dataset into design generation. In this phase, top designs from the\noffline dataset are edited by introducing noise, which are subsequently refined\nusing the conditional diffusion model to produce high-scoring designs. Overall,\nhigh-scoring designs begin with inheriting high-scoring features from the\nsecond phase and are further refined with a more accurate conditional diffusion\nmodel in the first phase. Empirical evaluations on 7 offline MBO tasks show\nthat DEMO outperforms various baseline methods.\n', '  Offline optimization is an emerging problem in many experimental engineering\ndomains including protein, drug or aircraft design, where online\nexperimentation to collect evaluation data is too expensive or dangerous. To\navoid that, one has to optimize an unknown function given only its offline\nevaluation at a fixed set of inputs. A naive solution to this problem is to\nlearn a surrogate model of the unknown function and optimize this surrogate\ninstead. However, such a naive optimizer is prone to erroneous overestimation\nof the surrogate (possibly due to over-fitting on a biased sample of function\nevaluation) on inputs outside the offline dataset. Prior approaches addressing\nthis challenge have primarily focused on learning robust surrogate models.\nHowever, their search strategies are derived from the surrogate model rather\nthan the actual offline data. To fill this important gap, we introduce a new\nlearning-to-search perspective for offline optimization by reformulating it as\nan offline reinforcement learning problem. Our proposed policy-guided gradient\nsearch approach explicitly learns the best policy for a given surrogate model\ncreated from the offline data. Our empirical results on multiple benchmarks\ndemonstrate that the learned optimization policy can be combined with existing\noffline surrogates to significantly improve the optimization performance.\n']",Offline Optimization with Generative Models,Generative Modeling Techniques,Generative Modeling and Artificial Intelligence
256,256,36,256_learnware_software_tools_mlops,"['learnware', 'software', 'tools', 'mlops', 'projects', 'ai', 'development', 'engineering', 'tooling', 'learning']","['software', 'practices', 'learnware', 'projects', 'development', 'systems', 'machine', 'debt', 'engineering', 'tools']","['  Continuous Integration (CI) is a well-established practice in traditional\nsoftware development, but its nuances in the domain of Machine Learning (ML)\nprojects remain relatively unexplored. Given the distinctive nature of ML\ndevelopment, understanding how CI practices are adopted in this context is\ncrucial for tailoring effective approaches. In this study, we conduct a\ncomprehensive analysis of 185 open-source projects on GitHub (93 ML and 92\nnon-ML projects). Our investigation comprises both quantitative and qualitative\ndimensions, aiming to uncover differences in CI adoption between ML and non-ML\nprojects. Our findings indicate that ML projects often require longer build\ndurations, and medium-sized ML projects exhibit lower test coverage compared to\nnon-ML projects. Moreover, small and medium-sized ML projects show a higher\nprevalence of increasing build duration trends compared to their non-ML\ncounterparts. Additionally, our qualitative analysis illuminates the\ndiscussions around CI in both ML and non-ML projects, encompassing themes like\nCI Build Execution and Status, CI Testing, and CI Infrastructure. These\ninsights shed light on the unique challenges faced by ML projects in adopting\nCI practices effectively.\n', '  Software engineering (SE) is a dynamic field that involves multiple phases\nall of which are necessary to develop sustainable software systems. Machine\nlearning (ML), a branch of artificial intelligence (AI), has drawn a lot of\nattention in recent years thanks to its ability to analyze massive volumes of\ndata and extract useful patterns from data. Several studies have focused on\nexamining, categorising, and assessing the application of ML in SE processes.\nWe conducted a literature review on primary studies to address this gap. The\nstudy was carried out following the objective and the research questions to\nexplore the current state of the art in applying machine learning techniques in\nsoftware engineering processes. The review identifies the key areas within\nsoftware engineering where ML has been applied, including software quality\nassurance, software maintenance, software comprehension, and software\ndocumentation. It also highlights the specific ML techniques that have been\nleveraged in these domains, such as supervised learning, unsupervised learning,\nand deep learning.\n  Keywords: machine learning, deep learning, software engineering, natural\nlanguage processing, source code\n', '  The rise of machine learning (ML) and its embedding in systems has\ndrastically changed the engineering of software-intensive systems.\nTraditionally, software engineering focuses on manually created artifacts such\nas source code and the process of creating them, as well as best practices for\nintegrating them, i.e., software architectures. In contrast, the development of\nML artifacts, i.e. ML models, comes from data science and focuses on the ML\nmodels and their training data. However, to deliver value to end users, these\nML models must be embedded in traditional software, often forming complex\ntopologies. In fact, ML-enabled software can easily incorporate many different\nML models. While the challenges and practices of building ML-enabled systems\nhave been studied to some extent, beyond isolated examples, little is known\nabout the characteristics of real-world ML-enabled systems. Properly embedding\nML models in systems so that they can be easily maintained or reused is far\nfrom trivial. We need to improve our empirical understanding of such systems,\nwhich we address by presenting the first large-scale study of real ML-enabled\nsoftware systems, covering over 2,928 open source systems on GitHub. We\nclassified and analyzed them to determine their characteristics, as well as\ntheir practices for reusing ML models and related code, and the architecture of\nthese systems. Our findings provide practitioners and researchers with insight\ninto practices for embedding and integrating ML models, bringing data science\nand software engineering closer together.\n']",Machine Learning in Software Engineering,Machine Learning in Software Development and Engineering,Machine Learning and Data-Driven Applications
257,257,36,257_iot_ioe_generative_ai,"['iot', 'ioe', 'generative', 'ai', 'cloud', 'devices', 'smart', 'intelligent', 'sensing', 'gai']","['metaverse', 'internet', 'wireless', 'intelligent', 'things', 'smart', 'devices', 'computing', 'sensing', 'intelligence']","['  The success of Artificial Intelligence (AI) in multiple disciplines and\nvertical domains in recent years has promoted the evolution of mobile\nnetworking and the future Internet toward an AI-integrated Internet-of-Things\n(IoT) era. Nevertheless, most AI techniques rely on data generated by physical\ndevices (e.g., mobile devices and network nodes) or specific applications\n(e.g., fitness trackers and mobile gaming). To bypass this circumvent,\nGenerative AI (GAI), a.k.a. AI-generated content (AIGC), has emerged as a\npowerful AI paradigm; thanks to its ability to efficiently learn complex data\ndistributions and generate synthetic data to represent the original data in\nvarious forms. This impressive feature is projected to transform the management\nof mobile networking and diversify the current services and applications\nprovided. On this basis, this work presents a concise tutorial on the role of\nGAIs in mobile and wireless networking. In particular, this survey first\nprovides the fundamentals of GAI and representative GAI models, serving as an\nessential preliminary to the understanding of the applications of GAI in mobile\nand wireless networking. Then, this work provides a comprehensive review of\nstate-of-the-art studies and GAI applications in network management, wireless\nsecurity, semantic communication, and lessons learned from the open literature.\nFinally, this work summarizes the current research on GAI for mobile and\nwireless networking by outlining important challenges that need to be resolved\nto facilitate the development and applicability of GAI in this edge-cutting\narea.\n', '  This study confronts the growing challenges of energy consumption and the\ndepletion of energy resources, particularly in the context of smart buildings.\nAs the demand for energy increases alongside the necessity for efficient\nbuilding maintenance, it becomes imperative to explore innovative energy\nmanagement solutions. We present a comprehensive review of Internet of Things\n(IoT)-based frameworks aimed at smart city energy management, highlighting the\npivotal role of IoT devices in addressing these issues due to their\ncompactness, sensing, measurement, and computing capabilities. Our review\nmethodology encompasses a thorough analysis of existing literature on IoT\narchitectures and frameworks for intelligent energy management applications. We\nfocus on systems that not only collect and store data but also support\nintelligent analysis for monitoring, controlling, and enhancing system\nefficiency. Additionally, we examine the potential for these frameworks to\nserve as platforms for the development of third-party applications, thereby\nextending their utility and adaptability. The findings from our review indicate\nthat IoT-based frameworks offer significant potential to reduce energy\nconsumption and environmental impact in smart buildings. Through the adoption\nof intelligent mechanisms and solutions, these frameworks facilitate effective\nenergy management, leading to improved system efficiency and sustainability.\nConsidering these findings, we recommend further exploration and adoption of\nIoT-based wireless sensing systems in smart buildings as a strategic approach\nto energy management. Our review underscores the importance of incorporating\nintelligent analysis and enabling the development of third-party applications\nwithin the IoT framework to efficiently meet the evolving energy demands and\nmaintenance challenges\n', '  The Internet of things (IoT) can significantly enhance the quality of human\nlife, specifically in healthcare, attracting extensive attentions to\nIoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as\nan innovative paradigm that can comprehensively characterize the replication of\nthe individual human body in the digital world and reflect its physical status\nin real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the\napplication of healthcare monitoring by acting as a versatile and vivid human\ndigital testbed, simulating the outcomes and guiding the practical treatments.\nHowever, successfully establishing HDT requires high-fidelity virtual modeling\nand strong information interactions but possibly with scarce, biased and noisy\ndata. Fortunately, a recent popular technology called generative artificial\nintelligence (GAI) may be a promising solution because it can leverage advanced\nAI algorithms to automatically create, manipulate, and modify valuable while\ndiverse data. This survey particularly focuses on the implementation of\nGAI-driven HDT in IoT-healthcare. We start by introducing the background of\nIoT-healthcare and the potential of GAI-driven HDT. Then, we delve into the\nfundamental techniques and present the overall framework of GAI-driven HDT.\nAfter that, we explore the realization of GAI-driven HDT in detail, including\nGAI-enabled data acquisition, communication, data management, digital modeling,\nand data analysis. Besides, we discuss typical IoT-healthcare applications that\ncan be revolutionized by GAI-driven HDT, namely personalized health monitoring\nand diagnosis, personalized prescription, and personalized rehabilitation.\nFinally, we conclude this survey by highlighting some future research\ndirections.\n']",Generative AI in IoT and Smart Systems,Generative AI Applications and Implications,Generative Modeling and Artificial Intelligence
258,258,36,258_counterfactuals_counterfactual_causality_causal,"['counterfactuals', 'counterfactual', 'causality', 'causal', 'counterfactually', 'inference', 'reasoning', 'interventions', 'interventional', 'discrimination']","['causal', 'counterfactual', 'counterfactuals', 'complainant', 'discrimination', 'interventional', 'comparator', 'causality', 'decision', 'interventions']","[""  The framework of Pearl's Causal Hierarchy (PCH) formalizes three types of\nreasoning: observational, interventional, and counterfactual, that reflect the\nprogressive sophistication of human thought regarding causation. We investigate\nthe computational complexity aspects of reasoning in this framework focusing\nmainly on satisfiability problems expressed in probabilistic and causal\nlanguages across the PCH. That is, given a system of formulas in the standard\nprobabilistic and causal languages, does there exist a model satisfying the\nformulas? The resulting complexity changes depending on the level of the\nhierarchy as well as the operators allowed in the formulas (addition,\nmultiplication, or marginalization). We focus on formulas involving\nmarginalization that are widely used in probabilistic and causal inference, but\nwhose complexity issues are still little explored. Our main contribution are\nthe exact computational complexity results showing that linear languages\n(allowing addition and marginalization) yield NP^PP-, PSPACE-, and\nNEXP-complete satisfiability problems, depending on the level of the PCH.\nMoreover, we prove that the problem for the full language (allowing\nadditionally multiplication) is complete for the class succ$\\exists$R for\nlanguages on the highest, counterfactual level, which extends previous results\nfor the lower levels of the PCH. Finally, we consider constrained models that\nare restricted to a given Bayesian network, a Directed Acyclic Graph structure,\nor a small polynomial size. The complexity of languages on the interventional\nlevel is increased to the complexity of counterfactual languages without such a\nconstraint, that is, linear languages become NEXP-complete. On the other hand,\nthe complexity on the counterfactual level does not change. The constraint on\nthe size reduces the complexity of the interventional and counterfactual\nlanguages to NEXP-complete.\n"", '  We present counterfactual situation testing (CST), a causal data mining\nframework for detecting discrimination in classifiers. CST aims to answer in an\nactionable and meaningful way the intuitive question ""what would have been the\nmodel outcome had the individual, or complainant, been of a different protected\nstatus?"" It extends the legally-grounded situation testing of Thanh et al.\n(2011) by operationalizing the notion of fairness given the difference using\ncounterfactual reasoning. For any complainant, we find and compare similar\nprotected and non-protected instances in the dataset used by the classifier to\nconstruct a control and test group, where a difference between the decision\noutcomes of the two groups implies potential individual discrimination. Unlike\nsituation testing, which builds both groups around the complainant, we build\nthe test group on the complainant\'s counterfactual generated using causal\nknowledge. The counterfactual is intended to reflect how the protected\nattribute when changed affects the seemingly neutral attributes used by the\nclassifier, which is taken for granted in many frameworks for discrimination.\nUnder CST, we compare similar individuals within each group but dissimilar\nindividuals across both groups due to the possible difference between the\ncomplainant and its counterfactual. Evaluating our framework on two\nclassification scenarios, we show that it uncovers a greater number of cases\nthan situation testing, even when the classifier satisfies the counterfactual\nfairness condition of Kusner et al. (2017).\n', ""  Causal inference has recently garnered significant interest among recommender\nsystem (RS) researchers due to its ability to dissect cause-and-effect\nrelationships and its broad applicability across multiple fields. It offers a\nframework to model the causality in recommender systems like confounding\neffects and deal with counterfactual problems such as offline policy evaluation\nand data augmentation. Although there are already some valuable surveys on\ncausal recommendations, they typically classify approaches based on the\npractical issues faced in RS, a classification that may disperse and fragment\nthe unified causal theories. Considering RS researchers' unfamiliarity with\ncausality, it is necessary yet challenging to comprehensively review relevant\nstudies from a coherent causal theoretical perspective, thereby facilitating a\ndeeper integration of causal inference in RS. This survey provides a systematic\nreview of up-to-date papers in this area from a causal theory standpoint and\ntraces the evolutionary development of RS methods within the same causal\nstrategy. Firstly, we introduce the fundamental concepts of causal inference as\nthe basis of the following review. Subsequently, we propose a novel\ntheory-driven taxonomy, categorizing existing methods based on the causal\ntheory employed - namely, those based on the potential outcome framework, the\nstructural causal model, and general counterfactuals. The review then delves\ninto the technical details of how existing methods apply causal inference to\naddress particular recommender issues. Finally, we highlight some promising\ndirections for future research in this field. Representative papers and\nopen-source resources will be progressively available at\nhttps://github.com/Chrissie-Law/Causal-Inference-for-Recommendation.\n""]",Causal Inference and Counterfactual Reasoning,Causal Analysis and Counterfactual Reasoning,Causal Analysis and Reasoning
259,259,35,259_embeddings_biodiversity_embedding_supervised,"['embeddings', 'biodiversity', 'embedding', 'supervised', 'genomes', 'genome', 'taxonomic', 'classification', 'datasets', 'species']","['species', 'fossil', 'clustering', 'labels', 'supervised', 'pseudo', 'label', 'image', 'segmentation', 'semi']","['  This study proposes CGRclust, a novel combination of unsupervised twin\ncontrastive clustering of Chaos Game Representations (CGR) of DNA sequences,\nwith convolutional neural networks (CNNs). To the best of our knowledge,\nCGRclust is the first method to use unsupervised learning for image\nclassification (herein applied to two-dimensional CGR images) for clustering\ndatasets of DNA sequences. CGRclust overcomes the limitations of traditional\nsequence classification methods by leveraging unsupervised twin contrastive\nlearning to detect distinctive sequence patterns, without requiring DNA\nsequence alignment or biological/taxonomic labels. CGRclust accurately\nclustered twenty-five diverse datasets, with sequence lengths ranging from 664\nbp to 100 kbp, including mitochondrial genomes of fish, fungi, and protists, as\nwell as viral whole genome assemblies and synthetic DNA sequences. Compared\nwith three recent clustering methods for DNA sequences (DeLUCS, iDeLUCS, and\nMeShClust v3.0.), CGRclust is the only method that surpasses 81.70% accuracy\nacross all four taxonomic levels tested for mitochondrial DNA genomes of fish.\nMoreover, CGRclust also consistently demonstrates superior performance across\nall the viral genomic datasets. The high clustering accuracy of CGRclust on\nthese twenty-five datasets, which vary significantly in terms of sequence\nlength, number of genomes, number of clusters, and level of taxonomy,\ndemonstrates its robustness, scalability, and versatility.\n', ""  Effective DNA embedding remains crucial in genomic analysis, particularly in\nscenarios lacking labeled data for model fine-tuning, despite the significant\nadvancements in genome foundation models. A prime example is metagenomics\nbinning, a critical process in microbiome research that aims to group DNA\nsequences by their species from a complex mixture of DNA sequences derived from\npotentially thousands of distinct, often uncharacterized species. To fill the\nlack of effective DNA embedding models, we introduce DNABERT-S, a genome\nfoundation model that specializes in creating species-aware DNA embeddings. To\nencourage effective embeddings to error-prone long-read DNA sequences, we\nintroduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes\nthe hidden representations of DNA sequences at randomly selected layers and\ntrains the model to recognize and differentiate these mixed proportions at the\noutput layer. We further enhance it with the proposed Curriculum Contrastive\nLearning (C$^2$LR) strategy. Empirical results on 18 diverse datasets showed\nDNABERT-S's remarkable performance. It outperforms the top baseline's\nperformance in 10-shot species classification with just a 2-shot training while\ndoubling the Adjusted Rand Index (ARI) in species clustering and substantially\nincreasing the number of correctly identified species in metagenomics binning.\nThe code, data, and pre-trained model are publicly available at\nhttps://github.com/Zhihan1996/DNABERT_S.\n"", '  Measuring biodiversity is crucial for understanding ecosystem health. While\nprior works have developed machine learning models for the taxonomic\nclassification of photographic images and DNA separately, in this work, we\nintroduce a multimodal approach combining both, using CLIP-style contrastive\nlearning to align images, DNA barcodes, and textual data in a unified embedding\nspace. This allows for accurate classification of both known and unknown insect\nspecies without task-specific fine-tuning, leveraging contrastive learning for\nthe first time to fuse DNA and image data. Our method surpasses previous\nsingle-modality approaches in accuracy by over 11% on zero-shot learning tasks,\nshowcasing its effectiveness in biodiversity studies.\n']",Genomic Analysis and Biodiversity Classification,Multimodal Data Analysis for Environmental and Biological Applications,Multimodal Learning and Applications
260,260,35,260_ai_explainability_xai_explanations,"['ai', 'explainability', 'xai', 'explanations', 'xaiport', 'intelligence', 'understandable', 'explainable', 'insights', 'understanding']","['explainable', 'explanation', 'explanations', 'explainability', 'transparency', 'accountability', 'black', 'trust', 'box', 'specification']","['  In this study, we propose the early adoption of Explainable AI (XAI) with a\nfocus on three properties: Quality of explanation, the explanation summaries\nshould be consistent across multiple XAI methods; Architectural Compatibility,\nfor effective integration in XAI, the architecture styles of both the XAI\nmethods and the models to be explained must be compatible with the framework;\nConfigurable operations, XAI explanations are operable, akin to machine\nlearning operations. Thus, an explanation for AI models should be reproducible\nand tractable to be trustworthy. We present XAIport, a framework of XAI\nmicroservices encapsulated into Open APIs to deliver early explanations as\nobservation for learning model quality assurance. XAIport enables configurable\nXAI operations along with machine learning development. We quantify the\noperational costs of incorporating XAI with three cloud computer vision\nservices on Microsoft Azure Cognitive Services, Google Cloud Vertex AI, and\nAmazon Rekognition. Our findings show comparable operational costs between XAI\nand traditional machine learning, with XAIport significantly improving both\ncloud AI model performance and explanation stability.\n', '  The increasing complexity of AI systems has led to the growth of the field of\nExplainable Artificial Intelligence (XAI), which aims to provide explanations\nand justifications for the outputs of AI algorithms. While there is\nconsiderable demand for XAI, there remains a scarcity of studies aimed at\ncomprehensively understanding the practical distinctions among different\nmethods and effectively aligning each method with users individual needs, and\nideally, offer a mapping function which can map each user with its specific\nneeds to a method of explainability. This study endeavors to bridge this gap by\nconducting a thorough review of extant research in XAI, with a specific focus\non Explainable Machine Learning (XML), and a keen eye on user needs. Our main\nobjective is to offer a classification of XAI methods within the realm of XML,\ncategorizing current works into three distinct domains: philosophy, theory, and\npractice, and providing a critical review for each category. Moreover, our\nstudy seeks to facilitate the connection between XAI users and the most\nsuitable methods for them and tailor explanations to meet their specific needs\nby proposing a mapping function that take to account users and their desired\nproperties and suggest an XAI method to them. This entails an examination of\nprevalent XAI approaches and an evaluation of their properties. The primary\noutcome of this study is the formulation of a clear and concise strategy for\nselecting the optimal XAI method to achieve a given goal, all while delivering\npersonalized explanations tailored to individual users.\n', '  Explainable AI (XAI) refers to techniques that provide human-understandable\ninsights into the workings of AI models. Recently, the focus of XAI is being\nextended towards Large Language Models (LLMs) which are often criticized for\ntheir lack of transparency. This extension calls for a significant\ntransformation in XAI methodologies because of two reasons. First, many\nexisting XAI methods cannot be directly applied to LLMs due to their complexity\nadvanced capabilities. Second, as LLMs are increasingly deployed across diverse\nindustry applications, the role of XAI shifts from merely opening the ""black\nbox"" to actively enhancing the productivity and applicability of LLMs in\nreal-world settings. Meanwhile, unlike traditional machine learning models that\nare passive recipients of XAI insights, the distinct abilities of LLMs can\nreciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in\nthe context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems,\nand (2) how LLMs can contribute to the advancement of XAI. We introduce 10\nstrategies, introducing the key techniques for each and discussing their\nassociated challenges. We also provide case studies to demonstrate how to\nobtain and leverage explanations. The code used in this paper can be found at:\nhttps://github.com/JacksonWuxs/UsableXAI_LLM.\n']",Explainable AI (XAI) Techniques and Applications,Explainable Artificial Intelligence (XAI),Artificial Intelligence and Machine Learning Interpretability and Explainability
261,261,35,261_markovian_stochastic_martingale_markov,"['markovian', 'stochastic', 'martingale', 'markov', 'approximation', 'ergodic', 'asynchronous', 'algorithms', 'asymptotic', 'optimal']","['asymptotic', 'stochastic', 'iterates', 'approximation', 'filtration', 'convergence', 'chain', 'limit', 'ergodic', 'mixing']","['  In this work, we investigate stochastic approximation (SA) with Markovian\ndata and nonlinear updates under constant stepsize $\\alpha>0$. Existing work\nhas primarily focused on either i.i.d. data or linear update rules. We take a\nnew perspective and carefully examine the simultaneous presence of Markovian\ndependency of data and nonlinear update rules, delineating how the interplay\nbetween these two structures leads to complications that are not captured by\nprior techniques. By leveraging the smoothness and recurrence properties of the\nSA updates, we develop a fine-grained analysis of the correlation between the\nSA iterates $\\theta_k$ and Markovian data $x_k$. This enables us to overcome\nthe obstacles in existing analysis and establish for the first time the weak\nconvergence of the joint process $(x_k, \\theta_k)_{k\\geq0}$. Furthermore, we\npresent a precise characterization of the asymptotic bias of the SA iterates,\ngiven by\n$\\mathbb{E}[\\theta_\\infty]-\\theta^\\ast=\\alpha(b_\\text{m}+b_\\text{n}+b_\\text{c})+O(\\alpha^{3/2})$.\nHere, $b_\\text{m}$ is associated with the Markovian noise, $b_\\text{n}$ is tied\nto the nonlinearity, and notably, $b_\\text{c}$ represents a multiplicative\ninteraction between the Markovian noise and nonlinearity, which is absent in\nprevious works. As a by-product of our analysis, we derive finite-time bounds\non higher moment $\\mathbb{E}[\\|\\theta_k-\\theta^\\ast\\|^{2p}]$ and present\nnon-asymptotic geometric convergence rates for the iterates, along with a\nCentral Limit Theorem.\n', '  We study stochastic approximation procedures for approximately solving a\n$d$-dimensional linear fixed point equation based on observing a trajectory of\nlength $n$ from an ergodic Markov chain. We first exhibit a non-asymptotic\nbound of the order $t_{\\mathrm{mix}} \\tfrac{d}{n}$ on the squared error of the\nlast iterate of a standard scheme, where $t_{\\mathrm{mix}}$ is a mixing time.\nWe then prove a non-asymptotic instance-dependent bound on a suitably averaged\nsequence of iterates, with a leading term that matches the local asymptotic\nminimax limit, including sharp dependence on the parameters $(d,\nt_{\\mathrm{mix}})$ in the higher order terms. We complement these upper bounds\nwith a non-asymptotic minimax lower bound that establishes the\ninstance-optimality of the averaged SA estimator. We derive corollaries of\nthese results for policy evaluation with Markov noise -- covering the\nTD($\\lambda$) family of algorithms for all $\\lambda \\in [0, 1)$ -- and linear\nautoregressive models. Our instance-dependent characterizations open the door\nto the design of fine-grained model selection procedures for hyperparameter\ntuning (e.g., choosing the value of $\\lambda$ when running the TD($\\lambda$)\nalgorithm).\n', ""  We study a family of distributed stochastic optimization algorithms where\ngradients are sampled by a token traversing a network of agents in random-walk\nfashion. Typically, these random-walks are chosen to be Markov chains that\nasymptotically sample from a desired target distribution, and play a critical\nrole in the convergence of the optimization iterates. In this paper, we take a\nnovel approach by replacing the standard linear Markovian token by one which\nfollows a nonlinear Markov chain - namely the Self-Repellent Radom Walk (SRRW).\nDefined for any given 'base' Markov chain, the SRRW, parameterized by a\npositive scalar {\\alpha}, is less likely to transition to states that were\nhighly visited in the past, thus the name. In the context of MCMC sampling on a\ngraph, a recent breakthrough in Doshi et al. (2023) shows that the SRRW\nachieves O(1/{\\alpha}) decrease in the asymptotic variance for sampling. We\npropose the use of a 'generalized' version of the SRRW to drive token\nalgorithms for distributed stochastic optimization in the form of stochastic\napproximation, termed SA-SRRW. We prove that the optimization iterate errors of\nthe resulting SA-SRRW converge to zero almost surely and prove a central limit\ntheorem, deriving the explicit form of the resulting asymptotic covariance\nmatrix corresponding to iterate errors. This asymptotic covariance is always\nsmaller than that of an algorithm driven by the base Markov chain and decreases\nat rate O(1/{\\alpha}^2) - the performance benefit of using SRRW thereby\namplified in the stochastic optimization context. Empirical results support our\ntheoretical findings.\n""]",Stochastic Approximation with Markovian Data,Stochastic Methods for Sampling and Dynamics,Probabilistic Methods and Stochastic Processes
262,262,35,262_feature_features_classification_biomarker,"['feature', 'features', 'classification', 'biomarker', 'ensemble', 'selection', 'datasets', 'prediction', 'predictive', 'optimization']","['selection', 'feature', 'microbiome', 'genetic', 'gene', 'features', 'biological', 'cancer', 'functional', 'diseases']","['  Understanding how multiple features are associated and contribute to a\nspecific objective is as important as understanding how each feature\ncontributes to a particular outcome. Interpretability of a single feature in a\nprediction may be handled in multiple ways; however, in a multi-objective\nprediction, it is difficult to obtain interpretability of a combination of\nfeature values. To address this issue, we propose an objective specific feature\ninteraction design using multi-labels to find the optimal combination of\nfeatures in agricultural settings. One of the novel aspects of this design is\nthe identification of a method that integrates feature explanations with global\nsensitivity analysis in order to ensure combinatorial optimization in\nmulti-objective settings. We have demonstrated in our preliminary experiments\nthat an approximate combination of feature values can be found to achieve the\ndesired outcome using two agricultural datasets: one with pre-harvest poultry\nfarm practices for multi-drug resistance presence, and one with post-harvest\npoultry farm practices for food-borne pathogens. In our combinatorial\noptimization approach, all three pathogens are taken into consideration\nsimultaneously to account for the interaction between conditions that favor\ndifferent types of pathogen growth. These results indicate that\nexplanation-based approaches are capable of identifying combinations of\nfeatures that reduce pathogen presence in fewer iterations than a baseline.\n', ""  The challenge in biomarker discovery using machine learning from omics data\nlies in the abundance of molecular features but scarcity of samples. Most\nfeature selection methods in machine learning require evaluating various sets\nof features (models) to determine the most effective combination. This process,\ntypically conducted using a validation dataset, involves testing different\nfeature sets to optimize the model's performance. Evaluations have performance\nestimation error and when the selection involves many models the best ones are\nalmost certainly overestimated. Biomarker identification with feature selection\nmethods can be addressed as a multi-objective problem with trade-offs between\npredictive ability and parsimony in the number of features. Genetic algorithms\nare a popular tool for multi-objective optimization but they evolve numerous\nsolutions thus are prone to overestimation. Methods have been proposed to\nreduce the overestimation after a model has already been selected in\nsingle-objective problems, but no algorithm existed capable of reducing the\noverestimation during the optimization, improving model selection, or applied\nin the more general multi-objective domain. We propose DOSA-MO, a novel\nmulti-objective optimization wrapper algorithm that learns how the original\nestimation, its variance, and the feature set size of the solutions predict the\noverestimation. DOSA-MO adjusts the expectation of the performance during the\noptimization, improving the composition of the solution set. We verify that\nDOSA-MO improves the performance of a state-of-the-art genetic algorithm on\nleft-out or external sample sets, when predicting cancer subtypes and/or\npatient overall survival, using three transcriptomics datasets for kidney and\nbreast cancer.\n"", '  In this study, we investigated the application of bio-inspired optimization\nalgorithms, including Genetic Algorithm, Particle Swarm Optimization, and Whale\nOptimization Algorithm, for feature selection in chronic disease prediction.\nThe primary goal was to enhance the predictive accuracy of models streamline\ndata dimensionality, and make predictions more interpretable and actionable.\n  The research encompassed a comparative analysis of the three bio-inspired\nfeature selection approaches across diverse chronic diseases, including\ndiabetes, cancer, kidney, and cardiovascular diseases. Performance metrics such\nas accuracy, precision, recall, and f1 score are used to assess the\neffectiveness of the algorithms in reducing the number of features needed for\naccurate classification.\n  The results in general demonstrate that the bio-inspired optimization\nalgorithms are effective in reducing the number of features required for\naccurate classification. However, there have been variations in the performance\nof the algorithms on different datasets.\n  The study highlights the importance of data pre-processing and cleaning in\nensuring the reliability and effectiveness of the analysis.\n  This study contributes to the advancement of predictive analytics in the\nrealm of chronic diseases. The potential impact of this work extends to early\nintervention, precision medicine, and improved patient outcomes, providing new\navenues for the delivery of healthcare services tailored to individual needs.\nThe findings underscore the potential benefits of using bio-inspired\noptimization algorithms for feature selection in chronic disease prediction,\noffering valuable insights for improving healthcare outcomes.\n']",Feature Selection and Optimization in Predictive Analytics,Feature Selection and Optimization in Machine Learning,Machine Learning and Artificial Intelligence
263,263,35,263_reading_gaze_dyslexia_corpus,"['reading', 'gaze', 'dyslexia', 'corpus', 'attention', 'linguistic', 'texts', 'comprehension', 'eye', 'text']","['reading', 'eye', 'fixation', 'multimodal', 'tracking', 'movements', 'linguistic', 'webcam', 'form', 'rich']","[""  The Eye Movements on Machine-Generated Texts Corpus (EMTeC) is a naturalistic\neye-movements-while-reading corpus of 107 native English speakers reading\nmachine-generated texts. The texts are generated by three large language models\nusing five different decoding strategies, and they fall into six different text\ntype categories. EMTeC entails the eye movement data at all stages of\npre-processing, i.e., the raw coordinate data sampled at 2000 Hz, the fixation\nsequences, and the reading measures. It further provides both the original and\na corrected version of the fixation sequences, accounting for vertical\ncalibration drift. Moreover, the corpus includes the language models' internals\nthat underlie the generation of the stimulus texts: the transition scores, the\nattention scores, and the hidden states. The stimuli are annotated for a range\nof linguistic features both at text and at word level. We anticipate EMTeC to\nbe utilized for a variety of use cases such as, but not restricted to, the\ninvestigation of reading behavior on machine-generated text and the impact of\ndifferent decoding strategies; reading behavior on different text types; the\ndevelopment of new pre-processing, data filtering, and drift correction\nalgorithms; the cognitive interpretability and enhancement of language models;\nand the assessment of the predictive power of surprisal and entropy for human\nreading times. The data at all stages of pre-processing, the model internals,\nand the code to reproduce the stimulus generation, data pre-processing and\nanalyses can be accessed via https://github.com/DiLi-Lab/EMTeC/.\n"", ""  We present a novel computational model employing hierarchical active\ninference to simulate reading and eye movements. The model characterizes\nlinguistic processing as inference over a hierarchical generative model,\nfacilitating predictions and inferences at various levels of granularity, from\nsyllables to sentences.\n  Our approach combines the strengths of large language models for realistic\ntextual predictions and active inference for guiding eye movements to\ninformative textual information, enabling the testing of predictions. The model\nexhibits proficiency in reading both known and unknown words and sentences,\nadhering to the distinction between lexical and nonlexical routes in dual-route\ntheories of reading. Notably, our model permits the exploration of maladaptive\ninference effects on eye movements during reading, such as in dyslexia. To\nsimulate this condition, we attenuate the contribution of priors during the\nreading process, leading to incorrect inferences and a more fragmented reading\nstyle, characterized by a greater number of shorter saccades. This alignment\nwith empirical findings regarding eye movements in dyslexic individuals\nhighlights the model's potential to aid in understanding the cognitive\nprocesses underlying reading and eye movements, as well as how reading deficits\nassociated with dyslexia may emerge from maladaptive predictive processing.\n  In summary, our model represents a significant advancement in comprehending\nthe intricate cognitive processes involved in reading and eye movements, with\npotential implications for understanding and addressing dyslexia through the\nsimulation of maladaptive inference. It may offer valuable insights into this\ncondition and contribute to the development of more effective interventions for\ntreatment.\n"", '  We present WebQAmGaze, a multilingual low-cost eye-tracking-while-reading\ndataset, designed as the first webcam-based eye-tracking corpus of reading to\nsupport the development of explainable computational language processing\nmodels. WebQAmGaze includes webcam eye-tracking data from 600 participants of a\nwide age range naturally reading English, German, Spanish, and Turkish texts.\nEach participant performs two reading tasks composed of five texts each, a\nnormal reading and an information-seeking task, followed by a comprehension\nquestion. We compare the collected webcam data to high-quality eye-tracking\nrecordings. The results show a moderate to strong correlation between the eye\nmovement measures obtained with the webcam compared to those obtained with a\ncommercial eye-tracking device. When validating the data, we find that higher\nfixation duration on relevant text spans accurately indicates correctness when\nanswering the corresponding questions. This dataset advances webcam-based\nreading studies and opens avenues to low-cost and diverse data collection.\nWebQAmGaze is beneficial to learn about the cognitive processes behind\nquestion-answering and to apply these insights to computational models of\nlanguage understanding.\n']",Eye Movements and Reading Behavior Corpus,Eye Movement and Gaze in Human Behavior and Cognition,Eye and Vision Research
264,264,35,264_sentences_linguistic_semantics_syntactic,"['sentences', 'linguistic', 'semantics', 'syntactic', 'ambiguities', 'semantic', 'lexical', 'negation', 'nlg', 'ambiguity']","['negation', 'sentences', 'ambiguity', 'entailment', 'sentence', 'syntactic', 'underspecified', 'lexical', 'scope', 'context']","['  When reading temporarily ambiguous garden-path sentences, misinterpretations\nsometimes linger past the point of disambiguation. This phenomenon has\ntraditionally been studied in psycholinguistic experiments using online\nmeasures such as reading times and offline measures such as comprehension\nquestions. Here, we investigate the processing of garden-path sentences and the\nfate of lingering misinterpretations using four large language models (LLMs):\nGPT-2, LLaMA-2, Flan-T5, and RoBERTa. The overall goal is to evaluate whether\nhumans and LLMs are aligned in their processing of garden-path sentences and in\nthe lingering misinterpretations past the point of disambiguation, especially\nwhen extra-syntactic information (e.g., a comma delimiting a clause boundary)\nis present to guide processing. We address this goal using 24 garden-path\nsentences that have optional transitive and reflexive verbs leading to\ntemporary ambiguities. For each sentence, there are a pair of comprehension\nquestions corresponding to the misinterpretation and the correct\ninterpretation. In three experiments, we (1) measure the dynamic semantic\ninterpretations of LLMs using the question-answering task; (2) track whether\nthese models shift their implicit parse tree at the point of disambiguation (or\nby the end of the sentence); and (3) visualize the model components that attend\nto disambiguating information when processing the question probes. These\nexperiments show promising alignment between humans and LLMs in the processing\nof garden-path sentences, especially when extra-syntactic information is\navailable to guide processing.\n', '  Contradictory results about the encoding of the semantic impact of negation\nin pretrained language models (PLMs). have been drawn recently (e.g. Kassner\nand Sch{\\""u}tze (2020); Gubelmann and Handschuh (2022)). In this paper we focus\nrather on the way PLMs encode negation and its formal impact, through the\nphenomenon of the Negative Polarity Item (NPI) licensing in English. More\nprecisely, we use probes to identify which contextual representations best\nencode 1) the presence of negation in a sentence, and 2) the polarity of a\nneighboring masked polarity item. We find that contextual representations of\ntokens inside the negation scope do allow for (i) a better prediction of the\npresence of not compared to those outside the scope and (ii) a better\nprediction of the right polarity of a masked polarity item licensed by not,\nalthough the magnitude of the difference varies from PLM to PLM. Importantly,\nin both cases the trend holds even when controlling for distance to not. This\ntends to indicate that the embeddings of these models do reflect the notion of\nnegation scope, and do encode the impact of negation on NPI licensing. Yet,\nfurther control experiments reveal that the presence of other lexical items is\nalso better captured when using the contextual representation of a token within\nthe same syntactic clause than outside from it, suggesting that PLMs simply\ncapture the more general notion of syntactic clause.\n', ""  Previous works of negation understanding mainly focus on negation cue\ndetection and scope resolution, without identifying negation subject which is\nalso significant to the downstream tasks. In this paper, we propose a new\nnegation triplet extraction (NTE) task which aims to extract negation subject\nalong with negation cue and scope. To achieve NTE, we devise a novel\nSyntax&Semantic-Enhanced Negation Extraction model, namely SSENE, which is\nbuilt based on a generative pretrained language model (PLM) {of Encoder-Decoder\narchitecture} with a multi-task learning framework. Specifically, the given\nsentence's syntactic dependency tree is incorporated into the PLM's encoder to\ndiscover the correlations between the negation subject, cue and scope.\nMoreover, the semantic consistency between the sentence and the extracted\ntriplet is ensured by an auxiliary task learning. Furthermore, we have\nconstructed a high-quality Chinese dataset NegComment based on the users'\nreviews from the real-world platform of Meituan, upon which our evaluations\nshow that SSENE achieves the best NTE performance compared to the baselines.\nOur ablation and case studies also demonstrate that incorporating the syntactic\ninformation helps the PLM's recognize the distant dependency between the\nsubject and cue, and the auxiliary task learning is helpful to extract the\nnegation triplets with more semantic consistency.\n""]",Linguistic Semantics and Negation in Sentences,Natural Language Processing and Linguistics,Natural Language Processing
265,265,34,265_attributions_attribution_classifiers_classification,"['attributions', 'attribution', 'classifiers', 'classification', 'neural', 'learning', 'deep', 'datasets', 'ensemble', 'deeprepviz']","['attribution', 'class', 'explanations', 'trees', 'interpretable', 'methods', 'machine', 'points', 'black', 'attributions']","['  Local data attribution (or influence estimation) techniques aim at estimating\nthe impact that individual data points seen during training have on particular\npredictions of an already trained Machine Learning model during test time.\nPrevious methods either do not perform well consistently across different\nevaluation criteria from literature, are characterized by a high computational\ndemand, or suffer from both. In this work we present DualView, a novel method\nfor post-hoc data attribution based on surrogate modelling, demonstrating both\nhigh computational efficiency, as well as good evaluation results. With a focus\non neural networks, we evaluate our proposed technique using suitable\nquantitative evaluation strategies from the literature against related\nprincipal local data attribution methods. We find that DualView requires\nconsiderably lower computational resources than other methods, while\ndemonstrating comparable performance to competing approaches across evaluation\nmetrics. Futhermore, our proposed method produces sparse explanations, where\nsparseness can be tuned via a hyperparameter. Finally, we showcase that with\nDualView, we can now render explanations from local data attributions\ncompatible with established local feature attribution methods: For each\nprediction on (test) data points explained in terms of impactful samples from\nthe training set, we are able to compute and visualize how the prediction on\n(test) sample relates to each influential training sample in terms of features\nrecognized and by the model. We provide an Open Source implementation of\nDualView online, together with implementations for all other local data\nattribution methods we compare against, as well as the metrics reported here,\nfor full reproducibility.\n', ""  Deep neural networks are very successful on many vision tasks, but hard to\ninterpret due to their black box nature. To overcome this, various post-hoc\nattribution methods have been proposed to identify image regions most\ninfluential to the models' decisions. Evaluating such methods is challenging\nsince no ground truth attributions exist. We thus propose three novel\nevaluation schemes to more reliably measure the faithfulness of those methods,\nto make comparisons between them more fair, and to make visual inspection more\nsystematic. To address faithfulness, we propose a novel evaluation setting\n(DiFull) in which we carefully control which parts of the input can influence\nthe output in order to distinguish possible from impossible attributions. To\naddress fairness, we note that different methods are applied at different\nlayers, which skews any comparison, and so evaluate all methods on the same\nlayers (ML-Att) and discuss how this impacts their performance on quantitative\nmetrics. For more systematic visualizations, we propose a scheme (AggAtt) to\nqualitatively evaluate the methods on complete datasets. We use these\nevaluation schemes to study strengths and shortcomings of some widely used\nattribution methods. Finally, we propose a post-processing smoothing step that\nsignificantly improves the performance of some attribution methods, and discuss\nits applicability.\n"", ""  Deep neural networks are very successful on many vision tasks, but hard to\ninterpret due to their black box nature. To overcome this, various post-hoc\nattribution methods have been proposed to identify image regions most\ninfluential to the models' decisions. Evaluating such methods is challenging\nsince no ground truth attributions exist. We thus propose three novel\nevaluation schemes to more reliably measure the faithfulness of those methods,\nto make comparisons between them more fair, and to make visual inspection more\nsystematic. To address faithfulness, we propose a novel evaluation setting\n(DiFull) in which we carefully control which parts of the input can influence\nthe output in order to distinguish possible from impossible attributions. To\naddress fairness, we note that different methods are applied at different\nlayers, which skews any comparison, and so evaluate all methods on the same\nlayers (ML-Att) and discuss how this impacts their performance on quantitative\nmetrics. For more systematic visualizations, we propose a scheme (AggAtt) to\nqualitatively evaluate the methods on complete datasets. We use these\nevaluation schemes to study strengths and shortcomings of some widely used\nattribution methods over a wide range of models. Finally, we propose a\npost-processing smoothing step that significantly improves the performance of\nsome attribution methods, and discuss its applicability.\n""]",Deep Learning Attribution Methods,Interpretable Machine Learning and Data Valuation,Explainable Artificial Intelligence
266,266,33,266_deception_deceiving_deceive_deceptive,"['deception', 'deceiving', 'deceive', 'deceptive', 'deceptively', 'disinformation', 'linguistic', 'language', 'ai', 'detecting']","['deception', 'deceptive', 'trustworthiness', 'misinformation', 'disinformation', 'ethical', 'cues', 'reviews', 'detection', 'trust']","['  Recent developments in large language models (LLMs), while offering a\npowerful foundation for developing natural language agents, raise safety\nconcerns about them and the autonomous agents built upon them. Deception is one\npotential capability of AI agents of particular concern, which we refer to as\nan act or statement that misleads, hides the truth, or promotes a belief that\nis not true in its entirety or in part. We move away from the conventional\nunderstanding of deception through straight-out lying, making objective selfish\ndecisions, or giving false information, as seen in previous AI safety research.\nWe target a specific category of deception achieved through obfuscation and\nequivocation. We broadly explain the two types of deception by analogizing them\nwith the rabbit-out-of-hat magic trick, where (i) the rabbit either comes out\nof a hidden trap door or (ii) (our focus) the audience is completely distracted\nto see the magician bring out the rabbit right in front of them using sleight\nof hand or misdirection. Our novel testbed framework displays intrinsic\ndeception capabilities of LLM agents in a goal-driven environment when directed\nto be deceptive in their natural language generations in a two-agent\nadversarial dialogue system built upon the legislative task of ""lobbying"" for a\nbill. Along the lines of a goal-driven environment, we show developing\ndeceptive capacity through a reinforcement learning setup, building it around\nthe theories of language philosophy and cognitive psychology. We find that the\nlobbyist agent increases its deceptive capabilities by ~ 40% (relative) through\nsubsequent reinforcement trials of adversarial interactions, and our deception\ndetection mechanism shows a detection capability of up to 92%. Our results\nhighlight potential issues in agent-human interaction, with agents potentially\nmanipulating humans towards its programmed end-goal.\n', ""  Internet-based economies and societies are drowning in deceptive attacks.\nThese attacks take many forms, such as fake news, phishing, and job scams,\nwhich we call ``domains of deception.'' Machine-learning and\nnatural-language-processing researchers have been attempting to ameliorate this\nprecarious situation by designing domain-specific detectors. Only a few recent\nworks have considered domain-independent deception. We collect these disparate\nthreads of research and investigate domain-independent deception. First, we\nprovide a new computational definition of deception and break down deception\ninto a new taxonomy. Then, we analyze the debate on linguistic cues for\ndeception and supply guidelines for systematic reviews. Finally, we investigate\ncommon linguistic features and give evidence for knowledge transfer across\ndifferent forms of deception.\n"", '  Deception, a prevalent aspect of human communication, has undergone a\nsignificant transformation in the digital age. With the globalization of online\ninteractions, individuals are communicating in multiple languages and mixing\nlanguages on social media, with varied data becoming available in each language\nand dialect. At the same time, the techniques for detecting deception are\nsimilar across the board. Recent studies have shown the possibility of the\nexistence of universal linguistic cues to deception across domains within the\nEnglish language; however, the existence of such cues in other languages\nremains unknown. Furthermore, the practical task of deception detection in\nlow-resource languages is not a well-studied problem due to the lack of labeled\ndata. Another dimension of deception is multimodality. For example, a picture\nwith an altered caption in fake news or disinformation may exist. This paper\ncalls for a comprehensive investigation into the complexities of deceptive\nlanguage across linguistic boundaries and modalities within the realm of\ncomputer security and natural language processing and the possibility of using\nmultilingual transformer models and labeled data in various languages to\nuniversally address the task of deception detection.\n']",Deception Detection in AI and Language,Language Analysis for Human Behavior and Intent,Human Behavior and Emotion Analysis through Language and Interaction
267,267,33,267_autoencoders_autoencoder_encoder_quantization,"['autoencoders', 'autoencoder', 'encoder', 'quantization', 'decoder', 'softmax', 'coding', 'quantized', 'codebook', 'representations']","['codebook', 'semantic', 'compression', 'variational', 'vector', 'quantization', 'generative', 'latent', 'discrete', 'hyperbolic']","['  Vector quantization (VQ) is a technique to deterministically learn features\nwith discrete codebook representations. It is commonly performed with a\nvariational autoencoding model, VQ-VAE, which can be further extended to\nhierarchical structures for making high-fidelity reconstructions. However, such\nhierarchical extensions of VQ-VAE often suffer from the codebook/layer collapse\nissue, where the codebook is not efficiently used to express the data, and\nhence degrades reconstruction accuracy. To mitigate this problem, we propose a\nnovel unified framework to stochastically learn hierarchical discrete\nrepresentation on the basis of the variational Bayes framework, called\nhierarchically quantized variational autoencoder (HQ-VAE). HQ-VAE naturally\ngeneralizes the hierarchical variants of VQ-VAE, such as VQ-VAE-2 and\nresidual-quantized VAE (RQ-VAE), and provides them with a Bayesian training\nscheme. Our comprehensive experiments on image datasets show that HQ-VAE\nenhances codebook usage and improves reconstruction performance. We also\nvalidated HQ-VAE in terms of its applicability to a different modality with an\naudio dataset.\n', '  Vector Quantized Variational AutoEncoder (VQ-VAE) is an established technique\nin machine learning for learning discrete representations across various\nmodalities. However, its scalability and applicability are limited by the need\nto retrain the model to adjust the codebook for different data or model scales.\nWe introduce the Rate-Adaptive VQ-VAE (RAQ-VAE) framework, which addresses this\nchallenge with two novel codebook representation methods: a model-based\napproach using a clustering-based technique on an existing well-trained VQ-VAE\nmodel, and a data-driven approach utilizing a sequence-to-sequence (Seq2Seq)\nmodel for variable-rate codebook generation. Our experiments demonstrate that\nRAQ-VAE achieves effective reconstruction performance across multiple rates,\noften outperforming conventional fixed-rate VQ-VAE models. This work enhances\nthe adaptability and performance of VQ-VAEs, with broad applications in data\nreconstruction, generation, and computer vision tasks.\n', '  The dimensionality of the embedding and the number of available embeddings (\nalso called codebook size) are critical factors influencing the performance of\nVector Quantization(VQ), a discretization process used in many models such as\nthe Vector Quantized Variational Autoencoder (VQ-VAE) architecture. This study\nexamines the balance between the codebook sizes and dimensions of embeddings in\nVQ, while maintaining their product constant. Traditionally, these hyper\nparameters are static during training; however, our findings indicate that\naugmenting the codebook size while simultaneously reducing the embedding\ndimension can significantly boost the effectiveness of the VQ-VAE. As a result,\nthe strategic selection of codebook size and embedding dimensions, while\npreserving the capacity of the discrete codebook space, is critically\nimportant. To address this, we propose a novel adaptive dynamic quantization\napproach, underpinned by the Gumbel-Softmax mechanism, which allows the model\nto autonomously determine the optimal codebook configuration for each data\ninstance. This dynamic discretizer gives the VQ-VAE remarkable flexibility.\nThorough empirical evaluations across multiple benchmark datasets validate the\nnotable performance enhancements achieved by our approach, highlighting the\nsignificant potential of adaptive dynamic quantization to improve model\nperformance.\n']",Vector Quantization Autoencoders,Quantization and Compression Techniques for Deep Learning Models,Deep Learning Optimization and Security
268,268,33,268_imaging_deep_gans_telescope,"['imaging', 'deep', 'gans', 'telescope', 'iterative', 'regularization', 'astronomy', 'dnns', 'dnn', 'generative']","['series', 'reconstruction', 'imaging', 'time', 'interferometric', 'diffusion', 'residual', 'radio', 'visibility', 'generation']","['  We propose a new approach for non-Cartesian magnetic resonance image\nreconstruction. While unrolled architectures provide robustness via\ndata-consistency layers, embedding measurement operators in Deep Neural Network\n(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)\napproaches, where the denoising DNNs are blind to the measurement setting, are\nnot affected by this limitation and have also proven effective, but their\nhighly iterative nature also affects scalability. To address this scalability\nchallenge, we leverage the ""Residual-to-Residual DNN series for high-Dynamic\nrange imaging (R2D2)"" approach recently introduced in astronomical imaging.\nR2D2\'s reconstruction is formed as a series of residual images, iteratively\nestimated as outputs of DNNs taking the previous iteration\'s image estimate and\nassociated data residual as inputs. The method can be interpreted as a learned\nversion of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,\nconsidering radial k-space sampling acquisition sequences. Our preliminary\nresults suggest that R2D2 achieves: (i) suboptimal performance compared to its\nunrolled incarnation R2D2-Net, which is however non-scalable due to the\nnecessary embedding of NUFFT-based data-consistency layers; (ii) superior\nreconstruction quality to a scalable version of R2D2-Net embedding an FFT-based\napproximation for data consistency; (iii) superior reconstruction quality to\nPnP, while only requiring few iterations.\n', '  Radio-interferometric (RI) imaging entails solving high-resolution\nhigh-dynamic range inverse problems from large data volumes. Recent image\nreconstruction techniques grounded in optimization theory have demonstrated\nremarkable capability for imaging precision, well beyond CLEAN\'s capability.\nThese range from advanced proximal algorithms propelled by handcrafted\nregularization operators, such as the SARA family, to hybrid plug-and-play\n(PnP) algorithms propelled by learned regularization denoisers, such as AIRI.\nOptimization and PnP structures are however highly iterative, which hinders\ntheir ability to handle the extreme data sizes expected from future\ninstruments. To address this scalability challenge, we introduce a novel deep\nlearning approach, dubbed ""Residual-to-Residual DNN series for high-Dynamic\nrange imaging"". R2D2\'s reconstruction is formed as a series of residual images,\niteratively estimated as outputs of Deep Neural Networks (DNNs) taking the\nprevious iteration\'s image estimate and associated data residual as inputs. It\nthus takes a hybrid structure between a PnP algorithm and a learned version of\nthe matching pursuit algorithm that underpins CLEAN. We present a comprehensive\nstudy of our approach, featuring its multiple incarnations distinguished by\ntheir DNN architectures. We provide a detailed description of its training\nprocess, targeting a telescope-specific approach. R2D2\'s capability to deliver\nhigh precision is demonstrated in simulation, across a variety of image and\nobservation settings using the Very Large Array (VLA). Its reconstruction speed\nis also demonstrated: with only few iterations required to clean data residuals\nat dynamic ranges up to 100000, R2D2 opens the door to fast precision imaging.\nR2D2 codes are available in the BASPLib library on GitHub.\n', ""  The ``Residual-to-Residual DNN series for high-Dynamic range imaging'' (R2D2)\napproach was recently introduced for Radio-Interferometric (RI) imaging in\nastronomy. R2D2's reconstruction is formed as a series of residual images,\niteratively estimated as outputs of Deep Neural Networks (DNNs) taking the\nprevious iteration's image estimate and associated data residual as inputs. In\nthis work, we investigate the robustness of the R2D2 image estimation process,\nby studying the uncertainty associated with its series of learned models.\nAdopting an ensemble averaging approach, multiple series can be trained,\narising from different random DNN initializations of the training process at\neach iteration. The resulting multiple R2D2 instances can also be leveraged to\ngenerate ``R2D2 samples'', from which empirical mean and standard deviation\nendow the algorithm with a joint estimation and uncertainty quantification\nfunctionality. Focusing on RI imaging, and adopting a telescope-specific\napproach, multiple R2D2 instances were trained to encompass the most general\nobservation setting of the Very Large Array (VLA). Simulations and real-data\nexperiments confirm that: (i) R2D2's image estimation capability is superior to\nthat of the state-of-the-art algorithms; (ii) its ultra-fast reconstruction\ncapability (arising from series with only few DNNs) makes the computation of\nmultiple reconstruction samples and of uncertainty maps practical even at large\nimage dimension; (iii) it is characterized by a very low model uncertainty.\n""]",Deep Learning for Radio-Interferometric Imaging,Deep Learning for Medical Imaging and Analysis,Deep Learning for Medical Imaging and Analysis
269,269,32,269_intents_intent_automation_automate,"['intents', 'intent', 'automation', 'automate', 'automated', 'semantic', 'ai', 'orchestration', 'networks', 'network']","['intent', 'networking', 'intents', 'network', 'engineering', 'science', 'management', 'software', 'orchestration', 'telemetry']","['  Automated management requires decomposing high-level user requests, such as\nintents, to an abstraction that the system can understand and execute. This is\nchallenging because even a simple intent requires performing a number of\nordered steps. And the task of identifying and adapting these steps (as\nconditions change) requires a decomposition approach that cannot be exactly\npre-defined beforehand. To tackle these challenges and support automated intent\ndecomposition and execution, we explore the few-shot capability of Large\nLanguage Models (LLMs). We propose a pipeline that progressively decomposes\nintents by generating the required actions using a policy-based abstraction.\nThis allows us to automate the policy execution by creating a closed control\nloop for the intent deployment. To do so, we generate and map the policies to\nAPIs and form application management loops that perform the necessary\nmonitoring, analysis, planning and execution. We evaluate our proposal with a\nuse-case to fulfill and assure an application service chain of virtual network\nfunctions. Using our approach, we can generalize and generate the necessary\nsteps to realize intents, thereby enabling intent automation for application\nmanagement.\n', ""  Large language models (LLMs) are rapidly emerging in Artificial Intelligence\n(AI) applications, especially in the fields of natural language processing and\ngenerative AI. Not limited to text generation applications, these models\ninherently possess the opportunity to leverage prompt engineering, where the\ninputs of such models can be appropriately structured to articulate a model's\npurpose explicitly. A prominent example of this is intent-based networking, an\nemerging approach for automating and maintaining network operations and\nmanagement. This paper presents semantic routing to achieve enhanced\nperformance in LLM-assisted intent-based management and orchestration of 5G\ncore networks. This work establishes an end-to-end intent extraction framework\nand presents a diverse dataset of sample user intents accompanied by a thorough\nanalysis of the effects of encoders and quantization on overall system\nperformance. The results show that using a semantic router improves the\naccuracy and efficiency of the LLM deployment compared to stand-alone LLMs with\nprompting architectures.\n"", ""  To effectively express and satisfy network application requirements,\nintent-based network management has emerged as a promising solution. In\nintent-based methods, users and applications express their intent in a\nhigh-level abstract language to the network. Although this abstraction\nsimplifies network operation, it induces many challenges to efficiently express\napplications' intents and map them to different network capabilities.\nTherefore, in this work, we propose an AI-based framework for intent profiling\nand translation. We consider a scenario where applications interacting with the\nnetwork express their needs for network services in their domain language. The\nmachine-to-machine communication (i.e., between applications and the network)\nis complex since it requires networks to learn how to understand the domain\nlanguages of each application, which is neither practical nor scalable.\nInstead, a framework based on emergent communication is proposed for intent\nprofiling, in which applications express their abstract quality-of-experience\n(QoE) intents to the network through emergent communication messages.\nSubsequently, the network learns how to interpret these communication messages\nand map them to network capabilities (i.e., slices) to guarantee the requested\nQuality-of-Service (QoS). Simulation results show that the proposed method\noutperforms self-learning slicing and other baselines, and achieves a\nperformance close to the perfect knowledge baseline.\n""]",Intent-Based Network Automation with AI,Artificial Intelligence in Technology and Engineering,Artificial Intelligence and Cognitive Systems
270,270,31,270_interpretability_neural_ai_abstraction,"['interpretability', 'neural', 'ai', 'abstraction', 'neuron', 'intelligence', 'explanations', 'brain', 'cognition', 'neurons']","['interpretability', 'mechanistic', 'granular', 'activations', 'causal', 'balls', 'networks', 'intelligence', 'neural', 'philosophical']","['  Mechanistic Interpretability (MI) promises a path toward fully understanding\nhow neural networks make their predictions. Prior work demonstrates that even\nwhen trained to perform simple arithmetic, models can implement a variety of\nalgorithms (sometimes concurrently) depending on initialization and\nhyperparameters. Does this mean neuron-level interpretability techniques have\nlimited applicability? We argue that high-dimensional neural networks can learn\nlow-dimensional representations of their training data that are useful beyond\nsimply making good predictions. Such representations can be understood through\nthe mechanistic interpretability lens and provide insights that are\nsurprisingly faithful to human-derived domain knowledge. This indicates that\nsuch approaches to interpretability can be useful for deriving a new\nunderstanding of a problem from models trained to solve it. As a case study, we\nextract nuclear physics concepts by studying models trained to reproduce\nnuclear data.\n', '  Human cognition operates on a ""Global-first"" cognitive mechanism,\nprioritizing information processing based on coarse-grained details. This\nmechanism inherently possesses an adaptive multi-granularity description\ncapacity, resulting in computational traits such as efficiency, robustness, and\ninterpretability. The analysis pattern reliance on the finest granularity and\nsingle-granularity makes most existing computational methods less efficient,\nrobust, and interpretable, which is an important reason for the current lack of\ninterpretability in neural networks. Multi-granularity granular-ball computing\nemploys granular-balls of varying sizes to daptively represent and envelop the\nsample space, facilitating learning based on these granular-balls. Given that\nthe number of coarse-grained ""granular-balls"" is fewer than sample points,\ngranular-ball computing proves more efficient. Moreover, the inherent\ncoarse-grained nature of granular-balls reduces susceptibility to fine-grained\nsample disturbances, enhancing robustness. The multi-granularity construct of\ngranular-balls generates topological structures and coarse-grained\ndescriptions, naturally augmenting interpretability. Granular-ball computing\nhas successfully ventured into diverse AI domains, fostering the development of\ninnovative theoretical methods, including granular-ball classifiers, clustering\ntechniques, neural networks, rough sets, and evolutionary computing. This has\nnotably ameliorated the efficiency, noise robustness, and interpretability of\ntraditional methods. Overall, granular-ball computing is a rare and innovative\ntheoretical approach in AI that can adaptively and simultaneously enhance\nefficiency, robustness, and interpretability. This article delves into the main\napplication landscapes for granular-ball computing, aiming to equip future\nresearchers with references and insights to refine and expand this promising\ntheory.\n', ""  Understanding AI systems' inner workings is critical for ensuring value\nalignment and safety. This review explores mechanistic interpretability:\nreverse-engineering the computational mechanisms and representations learned by\nneural networks into human-understandable algorithms and concepts to provide a\ngranular, causal understanding. We establish foundational concepts such as\nfeatures encoding knowledge within neural activations and hypotheses about\ntheir representation and computation. We survey methodologies for causally\ndissecting model behaviors and assess the relevance of mechanistic\ninterpretability to AI safety. We investigate challenges surrounding\nscalability, automation, and comprehensive interpretation. We advocate for\nclarifying concepts, setting standards, and scaling techniques to handle\ncomplex models and behaviors and expand to domains such as vision and\nreinforcement learning. Mechanistic interpretability could help prevent\ncatastrophic outcomes as AI systems become more powerful and inscrutable.\n""]",Mechanistic Interpretability in AI Systems,Explainable Artificial Intelligence (XAI),Artificial Intelligence and Machine Learning Interpretability and Explainability
271,271,30,271_networks_symmetries_neural_symmetry,"['networks', 'symmetries', 'neural', 'symmetry', 'neurons', 'metanetworks', 'scalegmn', 'representations', 'layers', 'permuted']","['symmetries', 'permutation', 'permutations', 'networks', 'neural', 'weight', 'merging', 'network', 'barriers', 'specialization']","['  Permutation symmetries of deep networks make basic operations like model\nmerging and similarity estimation challenging. In many cases, aligning the\nweights of the networks, i.e., finding optimal permutations between their\nweights, is necessary. Unfortunately, weight alignment is an NP-hard problem.\nPrior research has mainly focused on solving relaxed versions of the alignment\nproblem, leading to either time-consuming methods or sub-optimal solutions. To\naccelerate the alignment process and improve its quality, we propose a novel\nframework aimed at learning to solve the weight alignment problem, which we\nname Deep-Align. To that end, we first prove that weight alignment adheres to\ntwo fundamental symmetries and then, propose a deep architecture that respects\nthese symmetries. Notably, our framework does not require any labeled data. We\nprovide a theoretical analysis of our approach and evaluate Deep-Align on\nseveral types of network architectures and learning setups. Our experimental\nresults indicate that a feed-forward pass with Deep-Align produces better or\nequivalent alignments compared to those produced by current optimization\nalgorithms. Additionally, our alignments can be used as an effective\ninitialization for other methods, leading to improved solutions with a\nsignificant speedup in convergence.\n', '  Many algorithms and observed phenomena in deep learning appear to be affected\nby parameter symmetries -- transformations of neural network parameters that do\nnot change the underlying neural network function. These include linear mode\nconnectivity, model merging, Bayesian neural network inference, metanetworks,\nand several other characteristics of optimization or loss-landscapes. However,\ntheoretical analysis of the relationship between parameter space symmetries and\nthese phenomena is difficult. In this work, we empirically investigate the\nimpact of neural parameter symmetries by introducing new neural network\narchitectures that have reduced parameter space symmetries. We develop two\nmethods, with some provable guarantees, of modifying standard neural networks\nto reduce parameter space symmetries. With these new methods, we conduct a\ncomprehensive experimental study consisting of multiple tasks aimed at\nassessing the effect of removing parameter symmetries. Our experiments reveal\nseveral interesting observations on the empirical impact of parameter\nsymmetries; for instance, we observe linear mode connectivity between our\nnetworks without alignment of weight spaces, and we find that our networks\nallow for faster and more effective Bayesian neural network training.\n', '  Neural networks typically exhibit permutation symmetries which contribute to\nthe non-convexity of the networks\' loss landscapes, since linearly\ninterpolating between two permuted versions of a trained network tends to\nencounter a high loss barrier. Recent work has argued that permutation\nsymmetries are the only sources of non-convexity, meaning there are essentially\nno such barriers between trained networks if they are permuted appropriately.\nIn this work, we refine these arguments into three distinct claims of\nincreasing strength. We show that existing evidence only supports ""weak linear\nconnectivity""-that for each pair of networks belonging to a set of SGD\nsolutions, there exist (multiple) permutations that linearly connect it with\nthe other networks. In contrast, the claim ""strong linear connectivity""-that\nfor each network, there exists one permutation that simultaneously connects it\nwith the other networks-is both intuitively and practically more desirable.\nThis stronger claim would imply that the loss landscape is convex after\naccounting for permutation, and enable linear interpolation between three or\nmore independently trained models without increased loss. In this work, we\nintroduce an intermediate claim-that for certain sequences of networks, there\nexists one permutation that simultaneously aligns matching pairs of networks\nfrom these sequences. Specifically, we discover that a single permutation\naligns sequences of iteratively trained as well as iteratively pruned networks,\nmeaning that two networks exhibit low loss barriers at each step of their\noptimization and sparsification trajectories respectively. Finally, we provide\nthe first evidence that strong linear connectivity may be possible under\ncertain conditions, by showing that barriers decrease with increasing network\nwidth when interpolating among three networks.\n']",Neural Network Symmetries and Alignment,Equivariant Neural Networks and Symmetry in Deep Learning,Geometric and Equivariant Deep Learning
272,272,30,272_retrieval_intents_search_intent,"['retrieval', 'intents', 'search', 'intent', 'queryner', 'relevance', 'queries', 'pagerank', 'ranking', 'matching']","['intent', 'product', 'relevance', 'intents', 'commerce', 'query', 'matching', 'search', 'sellers', 'ranking']","[""  Semantic relevance calculation is crucial for e-commerce search engines, as\nit ensures that the items selected closely align with customer intent.\nInadequate attention to this aspect can detrimentally affect user experience\nand engagement. Traditional text-matching techniques are prevalent but often\nfail to capture the nuances of search intent accurately, so neural networks now\nhave become a preferred solution to processing such complex text matching.\nExisting methods predominantly employ representation-based architectures, which\nstrike a balance between high traffic capacity and low latency. However, they\nexhibit significant shortcomings in generalization and robustness when compared\nto interaction-based architectures. In this work, we introduce a robust\ninteraction-based modeling paradigm to address these shortcomings. It\nencompasses 1) a dynamic length representation scheme for expedited inference,\n2) a professional terms recognition method to identify subjects and core\nattributes from complex sentence structures, and 3) a contrastive adversarial\ntraining protocol to bolster the model's robustness and matching capabilities.\nExtensive offline evaluations demonstrate the superior robustness and\neffectiveness of our approach, and online A/B testing confirms its ability to\nimprove relevance in the same exposure position, resulting in more clicks and\nconversions. To the best of our knowledge, this method is the first\ninteraction-based approach for large e-commerce search relevance calculation.\nNotably, we have deployed it for the entire search traffic on alibaba.com, the\nlargest B2B e-commerce platform in the world.\n"", ""  Text matching systems have become a fundamental service in most searching\nplatforms. For instance, they are responsible for matching user queries to\nrelevant candidate items, or rewriting the user-input query to a pre-selected\nhigh-performing one for a better search experience. In practice, both the\nqueries and items often contain multiple attributes, such as the category of\nthe item and the location mentioned in the query, which represent condensed key\ninformation that is helpful for matching. However, most of the existing works\ndownplay the effectiveness of attributes by integrating them into text\nrepresentations as supplementary information. Hence, in this work, we focus on\nexploring the relationship between the attributes from two sides. Since\nattributes from two ends are often not aligned in terms of number and type, we\npropose to exploit the benefit of attributes by multiple-intent modeling. The\nintents extracted from attributes summarize the diverse needs of queries and\nprovide rich content of items, which are more refined and abstract, and can be\naligned for paired inputs. Concretely, we propose a multi-intent\nattribute-aware matching model (MIM), which consists of three main components:\nattribute-aware encoder, multi-intent modeling, and intent-aware matching. In\nthe attribute-aware encoder, the text and attributes are weighted and processed\nthrough a scaled attention mechanism with regard to the attributes' importance.\nAfterward, the multi-intent modeling extracts intents from two ends and aligns\nthem. Herein, we come up with a distribution loss to ensure the learned intents\nare diverse but concentrated, and a kullback-leibler divergence loss that\naligns the learned intents. Finally, in the intent-aware matching, the intents\nare evaluated by a self-supervised masking task, and then incorporated to\noutput the final matching result.\n"", ""  Text relevance or text matching of query and product is an essential\ntechnique for the e-commerce search system to ensure that the displayed\nproducts can match the intent of the query. Many studies focus on improving the\nperformance of the relevance model in search system. Recently, pre-trained\nlanguage models like BERT have achieved promising performance on the text\nrelevance task. While these models perform well on the offline test dataset,\nthere are still obstacles to deploy the pre-trained language model to the\nonline system as their high latency. The two-tower model is extensively\nemployed in industrial scenarios, owing to its ability to harmonize performance\nwith computational efficiency. Regrettably, such models present an opaque\n``black box'' nature, which prevents developers from making special\noptimizations. In this paper, we raise deep Bag-of-Words (DeepBoW) model, an\nefficient and interpretable relevance architecture for Chinese e-commerce. Our\napproach proposes to encode the query and the product into the sparse BoW\nrepresentation, which is a set of word-weight pairs. The weight means the\nimportant or the relevant score between the corresponding word and the raw\ntext. The relevance score is measured by the accumulation of the matched word\nbetween the sparse BoW representation of the query and the product. Compared to\npopular dense distributed representation that usually suffers from the drawback\nof black-box, the most advantage of the proposed representation model is highly\nexplainable and interventionable, which is a superior advantage to the\ndeployment and operation of online search engines. Moreover, the online\nefficiency of the proposed model is even better than the most efficient inner\nproduct form of dense representation ...\n""]",E-commerce Search Relevance Modeling,Online Advertising and E-commerce Search Optimization,Optimization and Decision Making in Complex Systems
273,273,30,273_interpretability_explainability_interpretable_nlp,"['interpretability', 'explainability', 'interpretable', 'nlp', 'explanations', 'contextual', 'ai', 'explaining', 'text', 'syntaxshap']","['explanations', 'interpretability', 'explainability', 'explanation', 'interpretable', 'interpretation', 'methods', 'black', 'box', 'explainable']","[""  Explainable AI (XAI) aids in deciphering 'black-box' models. While several\nmethods have been proposed and evaluated primarily in the image domain, the\nexploration of explainability in the text domain remains a growing research\narea. In this paper, we delve into the applicability of XAI methods for the\ntext domain. In this context, the 'Similarity Difference and Uniqueness' (SIDU)\nXAI method, recognized for its superior capability in localizing entire salient\nregions in image-based classification is extended to textual data. The extended\nmethod, SIDU-TXT, utilizes feature activation maps from 'black-box' models to\ngenerate heatmaps at a granular, word-based level, thereby providing\nexplanations that highlight contextually significant textual elements crucial\nfor model predictions. Given the absence of a unified standard for assessing\nXAI methods, this study applies a holistic three-tiered comprehensive\nevaluation framework: Functionally-Grounded, Human-Grounded and\nApplication-Grounded, to assess the effectiveness of the proposed SIDU-TXT\nacross various experiments. We find that, in sentiment analysis task of a movie\nreview dataset, SIDU-TXT excels in both functionally and human-grounded\nevaluations, demonstrating superior performance through quantitative and\nqualitative analyses compared to benchmarks like Grad-CAM and LIME. In the\napplication-grounded evaluation within the sensitive and complex legal domain\nof asylum decision-making, SIDU-TXT and Grad-CAM demonstrate comparable\nperformances, each with its own set of strengths and weaknesses. However, both\nmethods fall short of entirely fulfilling the sophisticated criteria of expert\nexpectations, highlighting the imperative need for additional research in XAI\nmethods suitable for such domains.\n"", '  The necessity for interpretability in natural language processing (NLP) has\nrisen alongside the growing prominence of large language models. Among the\nmyriad tasks within NLP, text generation stands out as a primary objective of\nautoregressive models. The NLP community has begun to take a keen interest in\ngaining a deeper understanding of text generation, leading to the development\nof model-agnostic explainable artificial intelligence (xAI) methods tailored to\nthis task. The design and evaluation of explainability methods are non-trivial\nsince they depend on many factors involved in the text generation process,\ne.g., the autoregressive model and its stochastic nature. This paper outlines\n17 challenges categorized into three groups that arise during the development\nand assessment of attribution-based explainability methods. These challenges\nencompass issues concerning tokenization, defining explanation similarity,\ndetermining token importance and prediction change metrics, the level of human\nintervention required, and the creation of suitable test datasets. The paper\nillustrates how these challenges can be intertwined, showcasing new\nopportunities for the community. These include developing probabilistic\nword-level explainability methods and engaging humans in the explainability\npipeline, from the data design to the final evaluation, to draw robust\nconclusions on xAI methods.\n', ""  Saliency post-hoc explainability methods are important tools for\nunderstanding increasingly complex NLP models. While these methods can reflect\nthe model's reasoning, they may not align with human intuition, making the\nexplanations not plausible. In this work, we present a methodology for\nincorporating rationales, which are text annotations explaining human\ndecisions, into text classification models. This incorporation enhances the\nplausibility of post-hoc explanations while preserving their faithfulness. Our\napproach is agnostic to model architectures and explainability methods. We\nintroduce the rationales during model training by augmenting the standard\ncross-entropy loss with a novel loss function inspired by contrastive learning.\nBy leveraging a multi-objective optimization algorithm, we explore the\ntrade-off between the two loss functions and generate a Pareto-optimal frontier\nof models that balance performance and plausibility. Through extensive\nexperiments involving diverse models, datasets, and explainability methods, we\ndemonstrate that our approach significantly enhances the quality of model\nexplanations without causing substantial (sometimes negligible) degradation in\nthe original model's performance.\n""]",Explainability in NLP Models,Transparency and Explainability in AI Systems,Artificial Intelligence and Machine Learning Interpretability and Explainability
274,274,29,274_markets_agents_market_traders,"['markets', 'agents', 'market', 'traders', 'agent', 'trading', 'trader', 'investors', 'liquidity', 'finance']","['market', 'collusion', 'pricing', 'agents', 'trading', 'price', 'agent', 'markets', 'rational', 'stock']","[""  We consider the dynamics and the interactions of multiple reinforcement\nlearning optimal execution trading agents interacting with a reactive\nAgent-Based Model (ABM) of a financial market in event time. The model\nrepresents a market ecology with 3-trophic levels represented by: optimal\nexecution learning agents, minimally intelligent liquidity takers, and fast\nelectronic liquidity providers. The optimal execution agent classes include\nbuying and selling agents that can either use a combination of limit orders and\nmarket orders, or only trade using market orders. The reward function\nexplicitly balances trade execution slippage against the penalty of not\nexecuting the order timeously. This work demonstrates how multiple competing\nlearning agents impact a minimally intelligent market simulation as functions\nof the number of agents, the size of agents' initial orders, and the state\nspaces used for learning. We use phase space plots to examine the dynamics of\nthe ABM, when various specifications of learning agents are included. Further,\nwe examine whether the inclusion of optimal execution agents that can learn is\nable to produce dynamics with the same complexity as empirical data. We find\nthat the inclusion of optimal execution agents changes the stylised facts\nproduced by ABM to conform more with empirical data, and are a necessary\ninclusion for ABMs investigating market micro-structure. However, including\nexecution agents to chartist-fundamentalist-noise ABMs is insufficient to\nrecover the complexity observed in empirical data.\n"", '  Investors and regulators can greatly benefit from a realistic market\nsimulator that enables them to anticipate the consequences of their decisions\nin real markets. However, traditional rule-based market simulators often fall\nshort in accurately capturing the dynamic behavior of market participants,\nparticularly in response to external market impact events or changes in the\nbehavior of other participants. In this study, we explore an agent-based\nsimulation framework employing reinforcement learning (RL) agents. We present\nthe implementation details of these RL agents and demonstrate that the\nsimulated market exhibits realistic stylized facts observed in real-world\nmarkets. Furthermore, we investigate the behavior of RL agents when confronted\nwith external market impacts, such as a flash crash. Our findings shed light on\nthe effectiveness and adaptability of RL-based agents within the simulation,\noffering insights into their response to significant market events.\n', '  We present a novel agent-based approach to simulating an over-the-counter\n(OTC) financial market in which trades are intermediated solely by market\nmakers and agent visibility is constrained to a network topology. Dynamics,\nsuch as changes in price, result from agent-level interactions that\nubiquitously occur via market maker agents acting as liquidity providers. Two\nadditional agents are considered: trend investors use a deep convolutional\nneural network paired with a deep Q-learning framework to inform trading\ndecisions by analysing price history; and value investors use a static\nprice-target to determine their trade directions and sizes. We demonstrate that\nour novel inclusion of a network topology with market makers facilitates\nexplorations into various market structures. First, we present the model and an\noverview of its mechanics. Second, we validate our findings via comparison to\nthe real-world: we demonstrate a fat-tailed distribution of price changes,\nauto-correlated volatility, a skew negatively correlated to market maker\npositioning, predictable price-history patterns and more. Finally, we\ndemonstrate that our network-based model can lend insights into the effect of\nmarket-structure on price-action. For example, we show that markets with\nsparsely connected intermediaries can have a critical point of fragmentation,\nbeyond which the market forms distinct clusters and arbitrage becomes rapidly\npossible between the prices of different market makers. A discussion is\nprovided on future work that would be beneficial.\n']",Agent-Based Modeling of Financial Markets,Agent-Based Modeling and Ontological Representations in Finance and Complex Systems,Artificial Intelligence and Cognitive Systems
275,275,29,275_networks_nodes_influence_graphs,"['networks', 'nodes', 'influence', 'graphs', 'influential', 'maximization', 'spreading', 'node', 'cascades', 'cascade']","['influence', 'maximization', 'seed', 'causal', 'nodes', 'node', 'multiplex', 'network', 'spread', 'propagation']","[""  The identification of a seed set to maximize information spread in a network\nis crucial, a concept known as Influence Maximization (IM). Elegant IM\nalgorithms could naturally extend to cases where each node is equipped with\nspecific weight, referred to as individual effect, to measure the node's\nimportance. Prevailing literature has typically assumed that the individual\neffect remains constant during the cascade process. However, this assumption is\nnot always feasible, as the individual effect of each node is primarily\nevaluated by the difference between the outputs in the activated and\nnon-activated states, with one of these states always being unobservable after\npropagation. Moreover, the individual effect is sensitive to the environmental\ninformation provided by surrounding nodes. To address these challenges, we\nextend the consideration of IM to a broader scenario involving general networks\nwith dynamic node individual effects, leveraging causality techniques. In our\npaper, we address this through the development of a Causal Influence\nMaximization (CauIM) algorithm. Theoretically, for CauIM, we present the\ngeneralized lower bound of influence spread and provide robustness analysis.\nEmpirically, in synthetic and real-world experiments, we demonstrate the\neffectiveness and robustness of CauIM, along with a novel acceleration\ntechnique.\n"", '  Since the structure of complex networks is often unknown, we may identify the\nmost influential seed nodes by exploring only a part of the underlying network,\ngiven a small budget for node queries. We propose IM-META, a solution to\ninfluence maximization (IM) in networks with unknown topology by retrieving\ninformation from queries and node metadata. Since using such metadata is not\nwithout risk due to the noisy nature of metadata and uncertainties in\nconnectivity inference, we formulate a new IM problem that aims to find both\nseed nodes and queried nodes. In IM-META, we develop an effective method that\niteratively performs three steps: 1) we learn the relationship between\ncollected metadata and edges via a Siamese neural network, 2) we select a\nnumber of inferred confident edges to construct a reinforced graph, and 3) we\nidentify the next node to query by maximizing the inferred influence spread\nusing our topology-aware ranking strategy. Through experimental evaluation of\nIM-META on four real-world datasets, we demonstrate a) the speed of network\nexploration via node queries, b) the effectiveness of each module, c) the\nsuperiority over benchmark methods, d) the robustness to more difficult\nsettings, e) the hyperparameter sensitivity, and f) the scalability.\n', '  Influence maximization (IM) is the problem of identifying a limited number of\ninitial influential users within a social network to maximize the number of\ninfluenced users. However, previous research has mostly focused on individual\ninformation propagation, neglecting the simultaneous and interactive\ndissemination of multiple information items. In reality, when users encounter a\npiece of information, such as a smartphone product, they often associate it\nwith related products in their minds, such as earphones or computers from the\nsame brand. Additionally, information platforms frequently recommend related\ncontent to users, amplifying this cascading effect and leading to multiplex\ninfluence diffusion.\n  This paper first formulates the Multiplex Influence Maximization (Multi-IM)\nproblem using multiplex diffusion models with an information association\nmechanism. In this problem, the seed set is a combination of influential users\nand information. To effectively manage the combinatorial complexity, we propose\nGraph Bayesian Optimization for Multi-IM (GBIM). The multiplex diffusion\nprocess is thoroughly investigated using a highly effective global kernelized\nattention message-passing module. This module, in conjunction with Bayesian\nlinear regression (BLR), produces a scalable surrogate model. A data\nacquisition module incorporating the exploration-exploitation trade-off is\ndeveloped to optimize the seed set further. Extensive experiments on synthetic\nand real-world datasets have proven our proposed framework effective. The code\nis available at https://github.com/zirui-yuan/GBIM.\n']",Influence Maximization in Complex Networks,Influence and Information Diffusion in Complex Networks,Information Dynamics and Network Influence
276,276,28,276_neural_neuron_neurons_networks,"['neural', 'neuron', 'neurons', 'networks', 'representations', 'deep', 'dnn', 'dnns', 'learning', 'backpropagation']","['diagrams', 'neurons', 'neuron', 'cito', 'concepts', 'torch', 'package', 'neural', 'circuit', 'networks']","['  Polysemantic neurons -- neurons that activate for a set of unrelated features\n-- have been seen as a significant obstacle towards interpretability of\ntask-optimized deep networks, with implications for AI safety. The classic\norigin story of polysemanticity is that the data contains more ``features"" than\nneurons, such that learning to perform a task forces the network to co-allocate\nmultiple unrelated features to the same neuron, endangering our ability to\nunderstand networks\' internal processing. In this work, we present a second and\nnon-mutually exclusive origin story of polysemanticity. We show that\npolysemanticity can arise incidentally, even when there are ample neurons to\nrepresent all features in the data, a phenomenon we term \\textit{incidental\npolysemanticity}. Using a combination of theory and experiments, we show that\nincidental polysemanticity can arise due to multiple reasons including\nregularization and neural noise; this incidental polysemanticity occurs because\nrandom initialization can, by chance alone, initially assign multiple features\nto the same neuron, and the training dynamics then strengthen such overlap. Our\npaper concludes by calling for further research quantifying the\nperformance-polysemanticity tradeoff in task-optimized deep neural networks to\nbetter understand to what extent polysemanticity is avoidable.\n', '  The field of mechanistic interpretability aims to study the role of\nindividual neurons in Deep Neural Networks. Single neurons, however, have the\ncapability to act polysemantically and encode for multiple (unrelated)\nfeatures, which renders their interpretation difficult. We present a method for\ndisentangling polysemanticity of any Deep Neural Network by decomposing a\npolysemantic neuron into multiple monosemantic ""virtual"" neurons. This is\nachieved by identifying the relevant sub-graph (""circuit"") for each ""pure""\nfeature. We demonstrate how our approach allows us to find and disentangle\nvarious polysemantic units of ResNet models trained on ImageNet. While\nevaluating feature visualizations using CLIP, our method effectively\ndisentangles representations, improving upon methods based on neuron\nactivations. Our code is available at https://github.com/maxdreyer/PURE.\n', ""  Despite substantial efforts, neural network interpretability remains an\nelusive goal, with previous research failing to provide succinct explanations\nof most single neurons' impact on the network output. This limitation is due to\nthe polysemantic nature of most neurons, whereby a given neuron is involved in\nmultiple unrelated network states, complicating the interpretation of that\nneuron. In this paper, we apply tools developed in neuroscience and information\ntheory to propose both a novel practical approach to network interpretability\nand theoretical insights into polysemanticity and the density of codes. We\ninfer levels of redundancy in the network's code by inspecting the\neigenspectrum of the activation's covariance matrix. Furthermore, we show how\nrandom projections can reveal whether a network exhibits a smooth or\nnon-differentiable code and hence how interpretable the code is. This same\nframework explains the advantages of polysemantic neurons to learning\nperformance and explains trends found in recent results by Elhage et\nal.~(2022). Our approach advances the pursuit of interpretability in neural\nnetworks, providing insights into their underlying structure and suggesting new\navenues for circuit-level interpretability.\n""]",Polysemantic Neurons in Deep Neural Networks,Deep Learning Theory and Foundations,Machine Learning and Artificial Intelligence
277,277,28,277_ai_shapley_feature_shapg,"['ai', 'shapley', 'feature', 'shapg', 'shap', 'features', 'predictive', 'interpretability', 'explanations', 'generalizability']","['causal', 'importance', 'features', 'feature', 'explanations', 'machine', 'values', 'research', 'counterfactual', 'quantities']","['  With wide application of Artificial Intelligence (AI), it has become\nparticularly important to make decisions of AI systems explainable and\ntransparent. In this paper, we proposed a new Explainable Artificial\nIntelligence (XAI) method called ShapG (Explanations based on Shapley value for\nGraphs) for measuring feature importance. ShapG is a model-agnostic global\nexplanation method. At the first stage, it defines an undirected graph based on\nthe dataset, where nodes represent features and edges are added based on\ncalculation of correlation coefficients between features. At the second stage,\nit calculates an approximated Shapley value by sampling the data taking into\naccount this graph structure. The sampling approach of ShapG allows to\ncalculate the importance of features efficiently, i.e. to reduce computational\ncomplexity. Comparison of ShapG with other existing XAI methods shows that it\nprovides more accurate explanations for two examined datasets. We also compared\nother XAI methods developed based on cooperative game theory with ShapG in\nrunning time, and the results show that ShapG exhibits obvious advantages in\nits running time, which further proves efficiency of ShapG. In addition,\nextensive experiments demonstrate a wide range of applicability of the ShapG\nmethod for explaining complex models. We find ShapG an important tool in\nimproving explainability and transparency of AI systems and believe it can be\nwidely used in various fields.\n', '  Shapley values originated in cooperative game theory but are extensively used\ntoday as a model-agnostic explanation framework to explain predictions made by\ncomplex machine learning models in the industry and academia. There are several\nalgorithmic approaches for computing different versions of Shapley value\nexplanations. Here, we focus on conditional Shapley values for predictive\nmodels fitted to tabular data. Estimating precise conditional Shapley values is\ndifficult as they require the estimation of non-trivial conditional\nexpectations. In this article, we develop new methods, extend earlier proposed\napproaches, and systematize the new refined and existing methods into different\nmethod classes for comparison and evaluation. The method classes use either\nMonte Carlo integration or regression to model the conditional expectations. We\nconduct extensive simulation studies to evaluate how precisely the different\nmethod classes estimate the conditional expectations, and thereby the\nconditional Shapley values, for different setups. We also apply the methods to\nseveral real-world data experiments and provide recommendations for when to use\nthe different method classes and approaches. Roughly speaking, we recommend\nusing parametric methods when we can specify the data distribution almost\ncorrectly, as they generally produce the most accurate Shapley value\nexplanations. When the distribution is unknown, both generative methods and\nregression models with a similar form as the underlying predictive model are\ngood and stable options. Regression-based methods are often slow to train but\nproduce the Shapley value explanations quickly once trained. The vice versa is\ntrue for Monte Carlo-based methods, making the different methods appropriate in\ndifferent practical situations.\n', '  As Artificial Intelligence (AI) is having more influence on our everyday\nlives, it becomes important that AI-based decisions are transparent and\nexplainable. As a consequence, the field of eXplainable AI (or XAI) has become\npopular in recent years. One way to explain AI models is to elucidate the\npredictive importance of the input features for the AI model in general, also\nreferred to as global explanations. Inspired by cooperative game theory,\nShapley values offer a convenient way for quantifying the feature importance as\nexplanations. However many methods based on Shapley values are built on the\nassumption of feature independence and often overlook causal relations of the\nfeatures which could impact their importance for the ML model. Inspired by\nstudies of explanations at the local level, we propose CAGE (Causally-Aware\nShapley Values for Global Explanations). In particular, we introduce a novel\nsampling procedure for out-coalition features that respects the causal\nrelations of the input features. We derive a practical approach that\nincorporates causal knowledge into global explanation and offers the\npossibility to interpret the predictive feature importance considering their\ncausal relation. We evaluate our method on synthetic data and real-world data.\nThe explanations from our approach suggest that they are not only more\nintuitive but also more faithful compared to previous global explanation\nmethods.\n']",Explainable AI with Shapley Values,Explainable Artificial Intelligence (XAI),Artificial Intelligence and Machine Learning Interpretability and Explainability
278,278,27,278_ai_autonomous_autonomy_safety,"['ai', 'autonomous', 'autonomy', 'safety', 'automated', 'driving', 'prediction', 'vehicles', 'vehicle', 'assessment']","['safety', 'autonomous', 'autonomy', 'systems', 'driving', 'critical', 'prediction', 'hazardous', 'operational', 'vehicles']","['  Contemporary artificial intelligence systems are pivotal in enhancing human\nefficiency and safety across various domains. One such domain is autonomous\nsystems, especially in automotive and defense use cases. Artificial\nintelligence brings learning and enhanced decision-making to autonomy system\ngoal-oriented behaviors and human independence. However, the lack of clear\nunderstanding of autonomy system capabilities hampers human-machine or\nmachine-machine interaction and interdiction. This necessitates varying degrees\nof human involvement for safety, accountability, and explainability purposes.\nYet, measuring the level autonomous capability in an autonomous system presents\na challenge. Two scales of measurement exist, yet measuring autonomy\npresupposes a variety of elements not available in the wild. This is why\nexisting measures for level of autonomy are operationalized only during design\nor test and evaluation phases. No measure for level of autonomy based on\nobserved system behavior exists at this time. To address this, we outline a\npotential measure for predicting level of autonomy using observable actions. We\nalso present an algorithm incorporating the proposed measure. The measure and\nalgorithm have significance to researchers and practitioners interested in a\nmethod to blind compare autonomous systems at runtime. Defense-based\nimplementations are likewise possible because counter-autonomy depends on\nrobust identification of autonomous systems.\n', '  This paper explores the role and challenges of Artificial Intelligence (AI)\nalgorithms, specifically AI-based software elements, in autonomous driving\nsystems. These AI systems are fundamental in executing real-time critical\nfunctions in complex and high-dimensional environments. They handle vital tasks\nlike multi-modal perception, cognition, and decision-making tasks such as\nmotion planning, lane keeping, and emergency braking. A primary concern relates\nto the ability (and necessity) of AI models to generalize beyond their initial\ntraining data. This generalization issue becomes evident in real-time\nscenarios, where models frequently encounter inputs not represented in their\ntraining or validation data. In such cases, AI systems must still function\neffectively despite facing distributional or domain shifts. This paper\ninvestigates the risk associated with overconfident AI models in\nsafety-critical applications like autonomous driving. To mitigate these risks,\nmethods for training AI models that help maintain performance without\noverconfidence are proposed. This involves implementing certainty reporting\narchitectures and ensuring diverse training data. While various\ndistribution-based methods exist to provide safety mechanisms for AI models,\nthere is a noted lack of systematic assessment of these methods, especially in\nthe context of safety-critical automotive applications. Many methods in the\nliterature do not adapt well to the quick response times required in\nsafety-critical edge applications. This paper reviews these methods, discusses\ntheir suitability for safety-critical applications, and highlights their\nstrengths and limitations. The paper also proposes potential improvements to\nenhance the safety and reliability of AI algorithms in autonomous vehicles in\nthe context of rapid and accurate decision-making processes.\n', '  In learning-enabled autonomous systems, safety monitoring of learned\ncomponents is crucial to ensure their outputs do not lead to system safety\nviolations, given the operational context of the system. However, developing a\nsafety monitor for practical deployment in real-world applications is\nchallenging. This is due to limited access to internal workings and training\ndata of the learned component. Furthermore, safety monitors should predict\nsafety violations with low latency, while consuming a reasonable amount of\ncomputation.\n  To address the challenges, we propose a safety monitoring method based on\nprobabilistic time series forecasting. Given the learned component outputs and\nan operational context, we empirically investigate different Deep Learning\n(DL)-based probabilistic forecasting to predict the objective measure capturing\nthe satisfaction or violation of a safety requirement (safety metric). We\nempirically evaluate safety metric and violation prediction accuracy, and\ninference latency and resource usage of four state-of-the-art models, with\nvarying horizons, using an autonomous aviation case study. Our results suggest\nthat probabilistic forecasting of safety metrics, given learned component\noutputs and scenarios, is effective for safety monitoring. Furthermore, for the\nautonomous aviation case study, Temporal Fusion Transformer (TFT) was the most\naccurate model for predicting imminent safety violations, with acceptable\nlatency and resource consumption.\n']",Autonomous Systems and AI Safety Assessment,Autonomous Systems and Safety Assessment,Autonomous Systems and Safety Assessment
279,279,27,279_multimodal_attention_taskbot_modality,"['multimodal', 'attention', 'taskbot', 'modality', 'multidialog', 'depression', 'modal', 'features', 'conversational', 'visual']","['multimodal', 'depression', 'engagement', 'mental', 'modal', 'audio', 'emotion', 'modality', 'modalities', 'media']","['  Multimodal depression detection is an important research topic that aims to\npredict human mental states using multimodal data. Previous methods treat\ndifferent modalities equally and fuse each modality by na\\""ive mathematical\noperations without measuring the relative importance between them, which cannot\nobtain well-performed multimodal representations for downstream depression\ntasks. In order to tackle the aforementioned concern, we present a Cross-modal\nAttention Network with Adaptive Multi-modal Recurrent Fusion (CANAMRF) for\nmultimodal depression detection. CANAMRF is constructed by a multimodal feature\nextractor, an Adaptive Multimodal Recurrent Fusion module, and a Hybrid\nAttention Module. Through experimentation on two benchmark datasets, CANAMRF\ndemonstrates state-of-the-art performance, underscoring the effectiveness of\nour proposed approach.\n', '  Early detection plays a crucial role in the treatment of depression.\nTherefore, numerous studies have focused on social media platforms, where\nindividuals express their emotions, aiming to achieve early detection of\ndepression. However, the majority of existing approaches often rely on specific\nfeatures, leading to limited scalability across different types of social media\ndatasets, such as text, images, or videos. To overcome this limitation, we\nintroduce a Multimodal Object-Oriented Graph Attention Model (MOGAM), which can\nbe applied to diverse types of data, offering a more scalable and versatile\nsolution. Furthermore, to ensure that our model can capture authentic symptoms\nof depression, we only include vlogs from users with a clinical diagnosis. To\nleverage the diverse features of vlogs, we adopt a multimodal approach and\ncollect additional metadata such as the title, description, and duration of the\nvlogs. To effectively aggregate these multimodal features, we employed a\ncross-attention mechanism. MOGAM achieved an accuracy of 0.871 and an F1-score\nof 0.888. Moreover, to validate the scalability of MOGAM, we evaluated its\nperformance with a benchmark dataset and achieved comparable results with prior\nstudies (0.61 F1-score). In conclusion, we believe that the proposed model,\nMOGAM, is an effective solution for detecting depression in social media,\noffering potential benefits in the early detection and treatment of this mental\nhealth condition.\n', ""  Depression, a prevalent and serious mental health issue, affects\napproximately 3.8\\% of the global population. Despite the existence of\neffective treatments, over 75\\% of individuals in low- and middle-income\ncountries remain untreated, partly due to the challenge in accurately\ndiagnosing depression in its early stages. This paper introduces a novel method\nfor detecting depression based on multi-modal feature fusion utilizing\ncross-attention. By employing MacBERT as a pre-training model to extract\nlexical features from text and incorporating an additional Transformer module\nto refine task-specific contextual understanding, the model's adaptability to\nthe targeted task is enhanced. Diverging from previous practices of simply\nconcatenating multimodal features, this approach leverages cross-attention for\nfeature integration, significantly improving the accuracy in depression\ndetection and enabling a more comprehensive and precise analysis of user\nemotions and behaviors. Furthermore, a Multi-Modal Feature Fusion Network based\non Cross-Attention (MFFNC) is constructed, demonstrating exceptional\nperformance in the task of depression identification. The experimental results\nindicate that our method achieves an accuracy of 0.9495 on the test dataset,\nmarking a substantial improvement over existing approaches. Moreover, it\noutlines a promising methodology for other social media platforms and tasks\ninvolving multi-modal processing. Timely identification and intervention for\nindividuals with depression are crucial for saving lives, highlighting the\nimmense potential of technology in facilitating early intervention for mental\nhealth issues.\n""]",Multimodal Depression Detection,Depression Detection using AI and Multimodal Data,Human Behavior Analysis with AI and Multimodal Data
280,280,27,280_adversarially_adversarial_explainers_explainability,"['adversarially', 'adversarial', 'explainers', 'explainability', 'explanations', 'interpretability', 'privacy', 'ai', 'evasion', 'security']","['explanations', 'attacks', 'adversarial', 'box', 'privacy', 'eclectic', 'explainable', 'black', 'extraction', 'functional']","['  Explainable Artificial Intelligence (XAI) aims to uncover the decision-making\nprocesses of AI models. However, the data used for such explanations can pose\nsecurity and privacy risks. Existing literature identifies attacks on machine\nlearning models, including membership inference, model inversion, and model\nextraction attacks. These attacks target either the model or the training data,\ndepending on the settings and parties involved.\n  XAI tools can increase the vulnerability of model extraction attacks, which\nis a concern when model owners prefer black-box access, thereby keeping model\nparameters and architecture private. To exploit this risk, we propose\nAUTOLYCUS, a novel retraining (learning) based model extraction attack\nframework against interpretable models under black-box settings. As XAI tools,\nwe exploit Local Interpretable Model-Agnostic Explanations (LIME) and Shapley\nvalues (SHAP) to infer decision boundaries and create surrogate models that\nreplicate the functionality of the target model. LIME and SHAP are mainly\nchosen for their realistic yet information-rich explanations, coupled with\ntheir extensive adoption, simplicity, and usability.\n  We evaluate AUTOLYCUS on six machine learning datasets, measuring the\naccuracy and similarity of the surrogate model to the target model. The results\nshow that AUTOLYCUS is highly effective, requiring significantly fewer queries\ncompared to state-of-the-art attacks, while maintaining comparable accuracy and\nsimilarity. We validate its performance and transferability on multiple\ninterpretable ML models, including decision trees, logistic regression, naive\nbayes, and k-nearest neighbor. Additionally, we show the resilience of\nAUTOLYCUS against proposed countermeasures.\n', ""  The field of Explainable Artificial Intelligence (XAI) focuses on techniques\nfor providing explanations to end-users about the decision-making processes\nthat underlie modern-day machine learning (ML) models. Within the vast universe\nof XAI techniques, counterfactual (CF) explanations are often preferred by\nend-users as they help explain the predictions of ML models by providing an\neasy-to-understand & actionable recourse (or contrastive) case to individual\nend-users who are adversely impacted by predicted outcomes. However, recent\nstudies have shown significant security concerns with using CF explanations in\nreal-world applications; in particular, malicious adversaries can exploit CF\nexplanations to perform query-efficient model extraction attacks on proprietary\nML models. In this paper, we propose a model-agnostic watermarking framework\n(for adding watermarks to CF explanations) that can be leveraged to detect\nunauthorized model extraction attacks (which rely on the watermarked CF\nexplanations). Our novel framework solves a bi-level optimization problem to\nembed an indistinguishable watermark into the generated CF explanation such\nthat any future model extraction attacks that rely on these watermarked CF\nexplanations can be detected using a null hypothesis significance testing\n(NHST) scheme, while ensuring that these embedded watermarks do not compromise\nthe quality of the generated CF explanations. We evaluate this framework's\nperformance across a diverse set of real-world datasets, CF explanation\nmethods, and model extraction techniques, and show that our watermarking\ndetection system can be used to accurately identify extracted ML models that\nare trained using the watermarked CF explanations. Our work paves the way for\nthe secure adoption of CF explanations in real-world applications.\n"", '  Machine learning (ML) models, demonstrably powerful, suffer from a lack of\ninterpretability. The absence of transparency, often referred to as the black\nbox nature of ML models, undermines trust and urges the need for efforts to\nenhance their explainability. Explainable AI (XAI) techniques address this\nchallenge by providing frameworks and methods to explain the internal\ndecision-making processes of these complex models. Techniques like\nCounterfactual Explanations (CF) and Feature Importance play a crucial role in\nachieving this goal. Furthermore, high-quality and diverse data remains the\nfoundational element for robust and trustworthy ML applications. In many\napplications, the data used to train ML and XAI explainers contain sensitive\ninformation. In this context, numerous privacy-preserving techniques can be\nemployed to safeguard sensitive information in the data, such as differential\nprivacy. Subsequently, a conflict between XAI and privacy solutions emerges due\nto their opposing goals. Since XAI techniques provide reasoning for the model\nbehavior, they reveal information relative to ML models, such as their decision\nboundaries, the values of features, or the gradients of deep learning models\nwhen explanations are exposed to a third entity. Attackers can initiate privacy\nbreaching attacks using these explanations, to perform model extraction,\ninference, and membership attacks. This dilemma underscores the challenge of\nfinding the right equilibrium between understanding ML decision-making and\nsafeguarding privacy.\n']",Explainable AI Security and Privacy Risks,Security and Privacy in Artificial Intelligence Systems,Artificial Intelligence Applications and Implications
281,281,27,281_forecasting_forecast_forecasts_prediction,"['forecasting', 'forecast', 'forecasts', 'prediction', 'bidding', 'schedulers', 'predictions', 'trading', 'markets', 'bids']","['market', 'ahead', 'bidding', 'price', 'forecast', 'markets', 'day', 'trading', 'reserve', 'demand']","['  This paper presents an integrated model for bidding energy storage in\nday-ahead and real-time markets to maximize profits. We show that in integrated\ntwo-stage bidding, the real-time bids are independent of day-ahead settlements,\nwhile the day-ahead bids should be based on predicted real-time prices. We\nutilize a transformer-based model for real-time price prediction, which\ncaptures complex dynamical patterns of real-time prices, and use the result for\nday-ahead bidding design. For real-time bidding, we utilize a long short-term\nmemory-dynamic programming hybrid real-time bidding model. We train and test\nour model with historical data from New York State, and our results showed that\nthe integrated system achieved promising results of almost a 20\\% increase in\nprofit compared to only bidding in real-time markets, and at the same time\nreducing the risk in terms of the number of days with negative profits.\n', '  Efficiently integrating renewable resources into electricity markets is vital\nfor addressing the challenges of matching real-time supply and demand while\nreducing the significant energy wastage resulting from curtailments. To address\nthis challenge effectively, the incorporation of storage devices can enhance\nthe reliability and efficiency of the grid, improving market liquidity and\nreducing price volatility. In short-term electricity markets, participants\nnavigate numerous options, each presenting unique challenges and opportunities,\nunderscoring the critical role of the trading strategy in maximizing profits.\nThis study delves into the optimization of day-ahead and balancing market\ntrading, leveraging quantile-based forecasts. Employing three trading\napproaches with practical constraints, our research enhances forecast\nassessment, increases trading frequency, and employs flexible timestamp orders.\nOur findings underscore the profit potential of simultaneous participation in\nboth day-ahead and balancing markets, especially with larger battery storage\nsystems; despite increased costs and narrower profit margins associated with\nhigher-volume trading, the implementation of high-frequency strategies plays a\nsignificant role in maximizing profits and addressing market challenges.\nFinally, we modelled four commercial battery storage systems and evaluated\ntheir economic viability through a scenario analysis, with larger batteries\nshowing a shorter return on investment.\n', '  Large penetration of renewable energy sources (RESs) brings huge uncertainty\ninto the electricity markets. While existing deterministic market clearing\nfails to accommodate the uncertainty, the recently proposed stochastic market\nclearing struggles to achieve desirable market properties. In this work, we\npropose a value-oriented forecasting approach, which tactically determines the\nRESs generation that enters the day-ahead market. With such a forecast, the\nexisting deterministic market clearing framework can be maintained, and the\nday-ahead and real-time overall operation cost is reduced. At the training\nphase, the forecast model parameters are estimated to minimize expected\nday-ahead and real-time overall operation costs, instead of minimizing forecast\nerrors in a statistical sense. Theoretically, we derive the exact form of the\nloss function for training the forecast model that aligns with such a goal. For\nmarket clearing modeled by linear programs, this loss function is a piecewise\nlinear function. Additionally, we derive the analytical gradient of the loss\nfunction with respect to the forecast, which inspires an efficient training\nstrategy. A numerical study shows our forecasts can bring significant benefits\nof the overall cost reduction to deterministic market clearing, compared to\nquality-oriented forecasting approach.\n']",Energy Market Forecasting and Bidding Strategies,Energy Forecasting and Management,Predictive Modeling and Forecasting
282,282,26,282_generative_counterfactuals_counterfactual_images,"['generative', 'counterfactuals', 'counterfactual', 'images', 'causal', 'leveraging', 'causaldiffae', 'explanations', 'classifiers', 'inference']","['counterfactual', 'causal', 'explanations', 'image', 'classifiers', 'generative', 'latent', 'counterfactuals', 'explanation', 'spurious']","[""  In the realm of Artificial Intelligence (AI), the importance of Explainable\nArtificial Intelligence (XAI) is increasingly recognized, particularly as AI\nmodels become more integral to our lives. One notable single-instance XAI\napproach is counterfactual explanation, which aids users in comprehending a\nmodel's decisions and offers guidance on altering these decisions. Specifically\nin the context of image classification models, effective image counterfactual\nexplanations can significantly enhance user understanding. This paper\nintroduces a novel method for computing feature importance within the feature\nspace of a black-box model. By employing information fusion techniques, our\nmethod maximizes the use of data to address feature counterfactual explanations\nin the feature space. Subsequently, we utilize an image generation model to\ntransform these feature counterfactual explanations into image counterfactual\nexplanations. Our experiments demonstrate that the counterfactual explanations\ngenerated by our method closely resemble the original images in both pixel and\nfeature spaces. Additionally, our method outperforms established baselines,\nachieving impressive experimental results.\n"", '  Counterfactual image editing is an important task in generative AI, which\nasks how an image would look if certain features were different. The current\nliterature on the topic focuses primarily on changing individual features while\nremaining silent about the causal relationships between these features, as\npresent in the real world. In this paper, we formalize the counterfactual image\nediting task using formal language, modeling the causal relationships between\nlatent generative factors and images through a special type of model called\naugmented structural causal models (ASCMs). Second, we show two fundamental\nimpossibility results: (1) counterfactual editing is impossible from i.i.d.\nimage samples and their corresponding labels alone; (2) even when the causal\nrelationships between the latent generative factors and images are available,\nno guarantees regarding the output of the model can be provided. Third, we\npropose a relaxation for this challenging problem by approximating\nnon-identifiable counterfactual distributions with a new family of\ncounterfactual-consistent estimators. This family exhibits the desirable\nproperty of preserving features that the user cares about across both factual\nand counterfactual worlds. Finally, we develop an efficient algorithm to\ngenerate counterfactual images by leveraging neural causal models.\n', ""  In this paper, we propose leveraging causal generative learning as an\ninterpretable tool for explaining image classifiers. Specifically, we present a\ngenerative counterfactual inference approach to study the influence of visual\nfeatures (i.e., pixels) as well as causal factors through generative learning.\nTo this end, we first uncover the most influential pixels on a classifier's\ndecision by varying the value of a causal attribute via counterfactual\ninference and computing both Shapely and contrastive explanations for\ncounterfactual images with these different attribute values. We then establish\na Monte-Carlo mechanism using the generator of a causal generative model in\norder to adapt Shapley explainers to produce feature importances for the\nhuman-interpretable attributes of a causal dataset in the case where a\nclassifier has been trained exclusively on the images of the dataset. Finally,\nwe present optimization methods for creating counterfactual explanations of\nclassifiers by means of counterfactual inference, proposing straightforward\napproaches for both differentiable and arbitrary classifiers. We exploit the\nMorpho-MNIST causal dataset as a case study for exploring our proposed methods\nfor generating counterfacutl explantions. We employ visual explanation methods\nfrom OmnixAI open source toolkit to compare them with our proposed methods. By\nemploying quantitative metrics to measure the interpretability of\ncounterfactual explanations, we find that our proposed methods of\ncounterfactual explanation offer more interpretable explanations compared to\nthose generated from OmnixAI. This finding suggests that our methods are\nwell-suited for generating highly interpretable counterfactual explanations on\ncausal datasets.\n""]",Counterfactual Image Explanations,Causal Analysis and Counterfactual Reasoning,Causal Analysis and Reasoning
283,283,26,283_hypergraph_laplacian_graphs_manifolds,"['hypergraph', 'laplacian', 'graphs', 'manifolds', 'regularization', 'manifold', 'dimensionality', 'clustering', 'graphons', 'supervised']","['matrices', 'manifold', 'graph', 'geometry', 'topological', 'brain', 'eigenvectors', 'topology', 'bundle', 'continuum']","[""  This paper extends the possibility to examine the underlying curvature of\ndata through the lens of topology by using the Betti curves, tools of\nPersistent Homology, as key topological descriptors, building on the clique\ntopology approach. It was previously shown that Betti curves distinguish random\nfrom Euclidean geometric matrices - i.e. distance matrices of points randomly\ndistributed in a cube with Euclidean distance. In line with previous\nexperiments, we consider their low-dimensional approximations named integral\nBetti values, or signatures that effectively distinguish not only Euclidean,\nbut also spherical and hyperbolic geometric matrices, both from purely random\nmatrices as well as among themselves. To prove this, we analyse the behaviour\nof Betti curves for various geometric matrices -- i.e. distance matrices of\npoints randomly distributed on manifolds of constant sectional curvature,\nconsidering the classical models of curvature 0, 1, -1, given by the Euclidean\nspace, the sphere, and the hyperbolic space. We further investigate the\ndependence of integral Betti signatures on factors including the sample size\nand dimension. This is important for assessment of real-world connectivity\nmatrices, as we show that the standard approach to network construction gives\nrise to (spurious) spherical geometry, with topology dependent on sample\ndimensions. Finally, we use the manifolds of constant curvature as comparison\nmodels to infer curvature underlying real-world datasets coming from\nneuroscience, finance and climate. Their associated topological features\nexhibit a hyperbolic character: the integral Betti signatures associated to\nthese datasets sit in between Euclidean and hyperbolic (of small curvature).\nThe potential confounding ``hyperbologenic effect'' of intrinsic low-rank\nmodular structures is also evaluated through simulations.\n"", '  Bi-stochastic normalization provides an alternative normalization of graph\nLaplacians in graph-based data analysis and can be computed efficiently by\nSinkhorn-Knopp (SK) iterations. This paper proves the convergence of\nbi-stochastically normalized graph Laplacian to manifold (weighted-)Laplacian\nwith rates, when $n$ data points are i.i.d. sampled from a general\n$d$-dimensional manifold embedded in a possibly high-dimensional space. Under\ncertain joint limit of $n \\to \\infty$ and kernel bandwidth $\\epsilon \\to 0$,\nthe point-wise convergence rate of the graph Laplacian operator (under 2-norm)\nis proved to be $ O( n^{-1/(d/2+3)})$ at finite large $n$ up to log factors,\nachieved at the scaling of $\\epsilon \\sim n^{-1/(d/2+3)} $. When the manifold\ndata are corrupted by outlier noise, we theoretically prove the graph Laplacian\npoint-wise consistency which matches the rate for clean manifold data plus an\nadditional term proportional to the boundedness of the inner-products of the\nnoise vectors among themselves and with data vectors. Motivated by our\nanalysis, which suggests that not exact bi-stochastic normalization but an\napproximate one will achieve the same consistency rate, we propose an\napproximate and constrained matrix scaling problem that can be solved by SK\niterations with early termination. Numerical experiments support our\ntheoretical results and show the robustness of bi-stochastically normalized\ngraph Laplacian to high-dimensional outlier noise.\n', ""  Graph Laplacian based algorithms for data lying on a manifold have been\nproven effective for tasks such as dimensionality reduction, clustering, and\ndenoising. In this work, we consider data sets whose data points lie on a\nmanifold that is closed under the action of a known unitary matrix Lie group G.\nWe propose to construct the graph Laplacian by incorporating the distances\nbetween all the pairs of points generated by the action of G on the data set.\nWe deem the latter construction the ``G-invariant Graph Laplacian'' (G-GL). We\nshow that the G-GL converges to the Laplace-Beltrami operator on the data\nmanifold, while enjoying a significantly improved convergence rate compared to\nthe standard graph Laplacian which only utilizes the distances between the\npoints in the given data set. Furthermore, we show that the G-GL admits a set\nof eigenfunctions that have the form of certain products between the group\nelements and eigenvectors of certain matrices, which can be estimated from the\ndata efficiently using FFT-type algorithms. We demonstrate our construction and\nits advantages on the problem of filtering data on a noisy manifold closed\nunder the action of the special unitary group SU(2).\n""]",Manifold Learning and Graph Laplacians,Geometric Deep Learning on Manifolds,Geometric and Equivariant Deep Learning
284,284,25,284_imputation_missingness_data_datasets,"['imputation', 'missingness', 'data', 'datasets', 'imputing', 'impute', 'missing', 'records', 'comprehensive', 'rows']","['imputation', 'series', 'missing', 'aeon', 'metabolomics', 'time', 'clustering', 'distance', 'data', 'tabular']","['  The ubiquity of missing data has sparked considerable attention and focus on\ntabular data imputation methods. Diffusion models, recognized as the\ncutting-edge technique for data generation, demonstrate significant potential\nin tabular data imputation tasks. However, in pursuit of diversity, vanilla\ndiffusion models often exhibit sensitivity to initialized noises, which hinders\nthe models from generating stable and accurate imputation results.\nAdditionally, the sparsity inherent in tabular data poses challenges for\ndiffusion models in accurately modeling the data manifold, impacting the\nrobustness of these models for data imputation. To tackle these challenges,\nthis paper introduces an advanced diffusion model named Self-supervised\nimputation Diffusion Model (SimpDM for brevity), specifically tailored for\ntabular data imputation tasks. To mitigate sensitivity to noise, we introduce a\nself-supervised alignment mechanism that aims to regularize the model, ensuring\nconsistent and stable imputation predictions. Furthermore, we introduce a\ncarefully devised state-dependent data augmentation strategy within SimpDM,\nenhancing the robustness of the diffusion model when dealing with limited data.\nExtensive experiments demonstrate that SimpDM matches or outperforms\nstate-of-the-art imputation methods across various scenarios.\n', '  We introduce a novel classification framework for time-series imputation\nusing deep learning, with a particular focus on clinical data. By identifying\nconceptual gaps in the literature and existing reviews, we devise a taxonomy\ngrounded on the inductive bias of neural imputation frameworks, resulting in a\nclassification of existing deep imputation strategies based on their\nsuitability for specific imputation scenarios and data-specific properties. Our\nreview further examines the existing methodologies employed to benchmark deep\nimputation models, evaluating their effectiveness in capturing the missingness\nscenarios found in clinical data and emphasising the importance of reconciling\nmathematical abstraction with clinical insights. Our classification aims to\nserve as a guide for researchers to facilitate the selection of appropriate\ndeep learning imputation techniques tailored to their specific clinical data.\nOur novel perspective also highlights the significance of bridging the gap\nbetween computational methodologies and medical insights to achieve clinically\nsound imputation models.\n', '  Objective: The proper handling of missing values is critical to delivering\nreliable estimates and decisions, especially in high-stakes fields such as\nclinical research. The increasing diversity and complexity of data have led\nmany researchers to develop deep learning (DL)-based imputation techniques. We\nconducted a systematic review to evaluate the use of these techniques, with a\nparticular focus on data types, aiming to assist healthcare researchers from\nvarious disciplines in dealing with missing values.\n  Methods: We searched five databases (MEDLINE, Web of Science, Embase, CINAHL,\nand Scopus) for articles published prior to August 2021 that applied DL-based\nmodels to imputation. We assessed selected publications from four perspectives:\nhealth data types, model backbone (i.e., main architecture), imputation\nstrategies, and comparison with non-DL-based methods. Based on data types, we\ncreated an evidence map to illustrate the adoption of DL models.\n  Results: We included 64 articles, of which tabular static (26.6%, 17/64) and\ntemporal data (37.5%, 24/64) were the most frequently investigated. We found\nthat model backbone(s) differed among data types as well as the imputation\nstrategy. The ""integrated"" strategy, that is, the imputation task being solved\nconcurrently with downstream tasks, was popular for tabular temporal (50%,\n12/24) and multi-modal data (71.4%, 5/7), but limited for other data types.\nMoreover, DL-based imputation methods yielded better imputation accuracy in\nmost studies, compared with non-DL-based methods.\n  Conclusion: DL-based imputation models can be customized based on data type,\naddressing the corresponding missing patterns, and its associated ""integrated""\nstrategy can enhance the efficacy of imputation, especially in scenarios where\ndata is complex. Future research may focus on the portability and fairness of\nDL-based models for healthcare data imputation.\n']",Data Imputation Methods,Data Imputation and Missing Value Analysis,Handling Missing or Inconsistent Data
285,285,25,285_bioinformatics_proteins_knowledgebase_molecular,"['bioinformatics', 'proteins', 'knowledgebase', 'molecular', 'protein', 'bio', 'molecule', 'biomedical', 'gene', 'enzyme']","['bioinformatics', 'scientific', 'chemical', 'gene', 'biological', 'protein', 'biomedical', 'plant', 'molecular', 'nach0']","[""  In this study, we generate and maintain a database of 10 million virtual\nlipids through METiS's in-house de novo lipid generation algorithms and lipid\nvirtual screening techniques. These virtual lipids serve as a corpus for\npre-training, lipid representation learning, and downstream task knowledge\ntransfer, culminating in state-of-the-art LNP property prediction performance.\nWe propose LipidBERT, a BERT-like model pre-trained with the Masked Language\nModel (MLM) and various secondary tasks. Additionally, we compare the\nperformance of embeddings generated by LipidBERT and PhatGPT, our GPT-like\nlipid generation model, on downstream tasks. The proposed bilingual LipidBERT\nmodel operates in two languages: the language of ionizable lipid pre-training,\nusing in-house dry-lab lipid structures, and the language of LNP fine-tuning,\nutilizing in-house LNP wet-lab data. This dual capability positions LipidBERT\nas a key AI-based filter for future screening tasks, including new versions of\nMETiS de novo lipid libraries and, more importantly, candidates for in vivo\ntesting for orgran-targeting LNPs. To the best of our knowledge, this is the\nfirst successful demonstration of the capability of a pre-trained language\nmodel on virtual lipids and its effectiveness in downstream tasks using web-lab\ndata. This work showcases the clever utilization of METiS's in-house de novo\nlipid library as well as the power of dry-wet lab integration.\n"", '  Recent research trends in computational biology have increasingly focused on\nintegrating text and bio-entity modeling, especially in the context of\nmolecules and proteins. However, previous efforts like BioT5 faced challenges\nin generalizing across diverse tasks and lacked a nuanced understanding of\nmolecular structures, particularly in their textual representations (e.g.,\nIUPAC). This paper introduces BioT5+, an extension of the BioT5 framework,\ntailored to enhance biological research and drug discovery. BioT5+ incorporates\nseveral novel features: integration of IUPAC names for molecular understanding,\ninclusion of extensive bio-text and molecule data from sources like bioRxiv and\nPubChem, the multi-task instruction tuning for generality across tasks, and a\nnumerical tokenization technique for improved processing of numerical data.\nThese enhancements allow BioT5+ to bridge the gap between molecular\nrepresentations and their textual descriptions, providing a more holistic\nunderstanding of biological entities, and largely improving the grounded\nreasoning of bio-text and bio-sequences. The model is pre-trained and\nfine-tuned with a large number of experiments, including \\emph{3 types of\nproblems (classification, regression, generation), 15 kinds of tasks, and 21\ntotal benchmark datasets}, demonstrating the remarkable performance and\nstate-of-the-art results in most cases. BioT5+ stands out for its ability to\ncapture intricate relationships in biological data, thereby contributing\nsignificantly to bioinformatics and computational biology. Our code is\navailable at \\url{https://github.com/QizhiPei/BioT5}.\n', '  Expert curation is essential to capture knowledge of enzyme functions from\nthe scientific literature in FAIR open knowledgebases but cannot keep pace with\nthe rate of new discoveries and new publications. In this work we present\nEnzChemRED, for Enzyme Chemistry Relation Extraction Dataset, a new training\nand benchmarking dataset to support the development of Natural Language\nProcessing (NLP) methods such as (large) language models that can assist enzyme\ncuration. EnzChemRED consists of 1,210 expert curated PubMed abstracts in which\nenzymes and the chemical reactions they catalyze are annotated using\nidentifiers from the UniProt Knowledgebase (UniProtKB) and the ontology of\nChemical Entities of Biological Interest (ChEBI). We show that fine-tuning\npre-trained language models with EnzChemRED can significantly boost their\nability to identify mentions of proteins and chemicals in text (Named Entity\nRecognition, or NER) and to extract the chemical conversions in which they\nparticipate (Relation Extraction, or RE), with average F1 score of 86.30% for\nNER, 86.66% for RE for chemical conversion pairs, and 83.79% for RE for\nchemical conversion pairs and linked enzymes. We combine the best performing\nmethods after fine-tuning using EnzChemRED to create an end-to-end pipeline for\nknowledge extraction from text and apply this to abstracts at PubMed scale to\ncreate a draft map of enzyme functions in literature to guide curation efforts\nin UniProtKB and the reaction knowledgebase Rhea. The EnzChemRED corpus is\nfreely available at https://ftp.expasy.org/databases/rhea/nlp/.\n']",Bioinformatics and Molecular Biology,Computational Methods for Molecular and Materials Science,Computational Biology and Chemistry
286,286,25,286_multilingual_multilinguality_monolingual_lingual,"['multilingual', 'multilinguality', 'monolingual', 'lingual', 'languages', 'language', 'linguistic', 'corpora', 'code', 'switching']","['switching', 'script', 'code', 'languages', 'transliteration', 'propaganda', 'language', 'lingual', 'monolingual', 'multilingual']","['  Code-switching is a prevalent linguistic phenomenon in which multilingual\nindividuals seamlessly alternate between languages. Despite its widespread use\nonline and recent research trends in this area, research in code-switching\npresents unique challenges, primarily stemming from the scarcity of labelled\ndata and available resources. In this study we investigate how pre-trained\nLanguage Models handle code-switched text in three dimensions: a) the ability\nof PLMs to detect code-switched text, b) variations in the structural\ninformation that PLMs utilise to capture code-switched text, and c) the\nconsistency of semantic information representation in code-switched text. To\nconduct a systematic and controlled evaluation of the language models in\nquestion, we create a novel dataset of well-formed naturalistic code-switched\ntext along with parallel translations into the source languages. Our findings\nreveal that pre-trained language models are effective in generalising to\ncode-switched text, shedding light on the abilities of these models to\ngeneralise representations to CS corpora. We release all our code and data\nincluding the novel corpus at https://github.com/francesita/code-mixed-probes.\n', '  Code-switching entails mixing multiple languages. It is an increasingly\noccurring phenomenon in social media texts. Usually, code-mixed texts are\nwritten in a single script, even though the languages involved have different\nscripts. Pre-trained multilingual models primarily utilize the data in the\nnative script of the language. In existing studies, the code-switched texts are\nutilized as they are. However, using the native script for each language can\ngenerate better representations of the text owing to the pre-trained knowledge.\nTherefore, a cross-language-script knowledge sharing architecture utilizing the\ncross attention and alignment of the representations of text in individual\nlanguage scripts was proposed in this study. Experimental results on two\ndifferent datasets containing Nepali-English and Hindi-English code-switched\ntexts, demonstrate the effectiveness of the proposed method. The interpretation\nof the model using model explainability technique illustrates the sharing of\nlanguage-specific knowledge between language-specific representations.\n', ""  Multilingual code-switching research is often hindered by the lack and\nlinguistically biased status of available datasets. To expand language\nrepresentation, we synthesize code-switching data by replacing intonation units\ndetected through PSST, a speech segmentation model fine-tuned from OpenAI's\nWhisper, using a speech-to-text translation dataset, CoVoST 2. With our\ndataset, CoVoSwitch, spanning 13 languages, we evaluate the code-switching\ntranslation performance of two multilingual translation models, M2M-100 418M\nand NLLB-200 600M. We reveal that the inclusion of code-switching units results\nin higher translation performance than monolingual settings and that models are\nbetter at code-switching translation into English than non-English. Further,\nlow-resource languages gain most from integration of code-switched units when\ntranslating into English but much less when translating into non-English.\nTranslations into low-resource languages also perform worse than even raw\ncode-switched inputs. We find that systems excel at copying English tokens but\nstruggle with non-English tokens, that the off-target problem in monolingual\nsettings is also relevant in code-switching settings, and that models\nhallucinate in code-switching translation by introducing words absent in both\nof the original source sentences. CoVoSwitch and code are available at\nhttps://github.com/sophiayk20/covoswitch.\n""]",Code-Switching in Multilingual Texts,Multilingual Natural Language Processing,Natural Language Processing
287,287,24,287_classifiers_prediction_probabilistic_bayes,"['classifiers', 'prediction', 'probabilistic', 'bayes', 'predictive', 'uncertainties', 'uncertainty', 'predictions', 'boosting', 'probabilities']","['calibration', 'uncertainty', 'risk', 'measures', 'probabilities', 'quantification', 'estimates', 'metrics', 'machine', 'probability']","['  We present a novel approach to uncertainty quantification in classification\ntasks based on label-wise decomposition of uncertainty measures. This\nlabel-wise perspective allows uncertainty to be quantified at the individual\nclass level, thereby improving cost-sensitive decision-making and helping\nunderstand the sources of uncertainty. Furthermore, it allows to define total,\naleatoric, and epistemic uncertainty on the basis of non-categorical measures\nsuch as variance, going beyond common entropy-based measures. In particular,\nvariance-based measures address some of the limitations associated with\nestablished methods that have recently been discussed in the literature. We\nshow that our proposed measures adhere to a number of desirable properties.\nThrough empirical evaluation on a variety of benchmark data sets -- including\napplications in the medical domain where accurate uncertainty quantification is\ncrucial -- we establish the effectiveness of label-wise uncertainty\nquantification.\n', '  In binary classification tasks, accurate representation of probabilistic\npredictions is essential for various real-world applications such as predicting\npayment defaults or assessing medical risks. The model must then be\nwell-calibrated to ensure alignment between predicted probabilities and actual\noutcomes. However, when score heterogeneity deviates from the underlying data\nprobability distribution, traditional calibration metrics lose reliability,\nfailing to align score distribution with actual probabilities. In this study,\nwe highlight approaches that prioritize optimizing the alignment between\npredicted scores and true probability distributions over minimizing traditional\nperformance or calibration metrics. When employing tree-based models such as\nRandom Forest and XGBoost, our analysis emphasizes the flexibility these models\noffer in tuning hyperparameters to minimize the Kullback-Leibler (KL)\ndivergence between predicted and true distributions. Through extensive\nempirical analysis across 10 UCI datasets and simulations, we demonstrate that\noptimizing tree-based models based on KL divergence yields superior alignment\nbetween predicted scores and actual probabilities without significant\nperformance loss. In real-world scenarios, the reference probability is\ndetermined a priori as a Beta distribution estimated through maximum\nlikelihood. Conversely, minimizing traditional calibration metrics may lead to\nsuboptimal results, characterized by notable performance declines and inferior\nKL values. Our findings reveal limitations in traditional calibration metrics,\nwhich could undermine the reliability of predictive models for critical\ndecision-making.\n', ""  Most machine learning classifiers are designed to output posterior\nprobabilities for the classes given the input sample. These probabilities may\nbe used to make the categorical decision on the class of the sample; provided\nas input to a downstream system; or provided to a human for interpretation.\nEvaluating the quality of the posteriors generated by these system is an\nessential problem which was addressed decades ago with the invention of proper\nscoring rules (PSRs). Unfortunately, much of the recent machine learning\nliterature uses calibration metrics -- most commonly, the expected calibration\nerror (ECE) -- as a proxy to assess posterior performance. The problem with\nthis approach is that calibration metrics reflect only one aspect of the\nquality of the posteriors, ignoring the discrimination performance. For this\nreason, we argue that calibration metrics should play no role in the assessment\nof posterior quality. Expected PSRs should instead be used for this job,\npreferably normalized for ease of interpretation. In this work, we first give a\nbrief review of PSRs from a practical perspective, motivating their definition\nusing Bayes decision theory. We discuss why expected PSRs provide a principled\nmeasure of the quality of a system's posteriors and why calibration metrics are\nnot the right tool for this job. We argue that calibration metrics, while not\nuseful for performance assessment, may be used as diagnostic tools during\nsystem development. With this purpose in mind, we discuss a simple and\npractical calibration metric, called calibration loss, derived from a\ndecomposition of expected PSRs. We compare this metric with the ECE and with\nthe expected score divergence calibration metric from the PSR literature and\nargue, using theoretical and empirical evidence, that calibration loss is\nsuperior to these two metrics.\n""]",Uncertainty Quantification in Classification,Uncertainty Estimation and Quantification in Machine Learning,Machine Learning Reliability and Uncertainty
288,288,24,288_generative_populations_census_generating,"['generative', 'populations', 'census', 'generating', 'population', 'autoencoders', 'data', 'generation', 'samples', 'sample']","['population', 'synthetic', 'generative', 'conditional', 'distribution', 'density', 'tabular', 'distributions', 'data', 'statistics']","['  A common objective in the analysis of tabular data is estimating the\nconditional distribution (in contrast to only producing predictions) of a set\nof ""outcome"" variables given a set of ""covariates"", which is sometimes referred\nto as the ""density regression"" problem. Beyond estimation on the conditional\ndistribution, the generative ability of drawing synthetic samples from the\nlearned conditional distribution is also desired as it further widens the range\nof applications. We propose a flow-based generative model tailored for the\ndensity regression task on tabular data. Our flow applies a sequence of\ntree-based piecewise-linear transforms on initial uniform noise to eventually\ngenerate samples from complex conditional densities of (univariate or\nmultivariate) outcomes given the covariates and allows efficient analytical\nevaluation of the fitted conditional density on any point in the sample space.\nWe introduce a training algorithm for fitting the tree-based transforms using a\ndivide-and-conquer strategy that transforms maximum likelihood training of the\ntree-flow into training a collection of binary classifiers--one at each tree\nsplit--under cross-entropy loss. We assess the performance of our method under\nout-of-sample likelihood evaluation and compare it with a variety of\nstate-of-the-art conditional density learners on a range of simulated and real\nbenchmark tabular datasets. Our method consistently achieves comparable or\nsuperior performance at a fraction of the training and sampling budget.\nFinally, we demonstrate the utility of our method\'s generative ability through\nan application to generating synthetic longitudinal microbiome compositional\ndata based on training our flow on a publicly available microbiome study.\n', '  Household and individual-level sociodemographic data are essential for\nunderstanding human-infrastructure interaction and policymaking. However, the\nPublic Use Microdata Sample (PUMS) offers only a sample at the state level,\nwhile census tract data only provides the marginal distributions of variables\nwithout correlations. Therefore, we need an accurate synthetic population\ndataset that maintains consistent variable correlations observed in microdata,\npreserves household-individual and individual-individual relationships, adheres\nto state-level statistics, and accurately represents the geographic\ndistribution of the population. We propose a deep generative framework\nleveraging the variational autoencoder (VAE) to generate a synthetic population\nwith the aforementioned features. The methodological contributions include (1)\na new data structure for capturing household-individual and\nindividual-individual relationships, (2) a transfer learning process with\npre-training and fine-tuning steps to generate households and individuals whose\naggregated distributions align with the census tract marginal distribution, and\n(3) decoupled binary cross-entropy (D-BCE) loss function enabling distribution\nshift and out-of-sample records generation. Model results for an application in\nDelaware, USA demonstrate the ability to ensure the realism of generated\nhousehold-individual records and accurately describe population statistics at\nthe census tract level compared to existing methods. Furthermore, testing in\nNorth Carolina, USA yielded promising results, supporting the transferability\nof our method.\n', ""  Population synthesis involves generating synthetic yet realistic\nrepresentations of a target population of micro-agents for behavioral modeling\nand simulation. Traditional methods, often reliant on target population\nsamples, such as census data or travel surveys, face limitations due to high\ncosts and small sample sizes, particularly at smaller geographical scales. We\npropose a novel framework based on copulas to generate synthetic data for\ntarget populations where only empirical marginal distributions are known. This\nmethod utilizes samples from different populations with similar marginal\ndependencies, introduces a spatial component into population synthesis, and\nconsiders various information sources for more realistic generators.\nConcretely, the process involves normalizing the data and treat it as\nrealizations of a given copula, and then training a generative model before\nincorporating the information on the marginals of the target population.\nUtilizing American Community Survey data, we assess our framework's performance\nthrough standardized root mean squared error (SRMSE) and so-called sampled\nzeros. We focus on its capacity to transfer a model learned from one population\nto another. Our experiments include transfer tests between regions at the same\ngeographical level as well as to lower geographical levels, hence evaluating\nthe framework's adaptability in varied spatial contexts. We compare Bayesian\nNetworks, Variational Autoencoders, and Generative Adversarial Networks, both\nindividually and combined with our copula framework. Results show that the\ncopula enhances machine learning methods in matching the marginals of the\nreference data. Furthermore, it consistently surpasses Iterative Proportional\nFitting in terms of SRMSE in the transferability experiments, while introducing\nunique observations not found in the original training sample.\n""]",Generative Models for Population Synthesis,Generative Modeling Techniques,Generative Modeling and Artificial Intelligence
289,289,23,289_deepinfomax_supervised_dimensionality_embedding,"['deepinfomax', 'supervised', 'dimensionality', 'embedding', 'embeddings', 'regularization', 'dimensional', 'learning', 'representations', 'dimension']","['manifold', 'collapse', 'representation', 'supervised', 'dimensional', 'embeddings', 'self', 'augmentation', 'representations', 'separability']","['  Maximum Manifold Capacity Representations (MMCR) is a recent multi-view\nself-supervised learning (MVSSL) method that matches or surpasses other leading\nMVSSL methods. MMCR is intriguing because it does not fit neatly into any of\nthe commonplace MVSSL lineages, instead originating from a statistical\nmechanical perspective on the linear separability of data manifolds. In this\npaper, we seek to improve our understanding and our utilization of MMCR. To\nbetter understand MMCR, we leverage tools from high dimensional probability to\ndemonstrate that MMCR incentivizes alignment and uniformity of learned\nembeddings. We then leverage tools from information theory to show that such\nembeddings maximize a well-known lower bound on mutual information between\nviews, thereby connecting the geometric perspective of MMCR to the\ninformation-theoretic perspective commonly discussed in MVSSL. To better\nutilize MMCR, we mathematically predict and experimentally confirm\nnon-monotonic changes in the pretraining loss akin to double descent but with\nrespect to atypical hyperparameters. We also discover compute scaling laws that\nenable predicting the pretraining loss as a function of gradients steps, batch\nsize, embedding dimension and number of views. We then show that MMCR,\noriginally applied to image data, is performant on multimodal image-text data.\nBy more deeply understanding the theoretical and empirical behavior of MMCR,\nour work reveals insights on improving MVSSL methods.\n', '  The manifold hypothesis posits that high-dimensional data often lies on a\nlower-dimensional manifold and that utilizing this manifold as the target space\nyields more efficient representations. While numerous traditional\nmanifold-based techniques exist for dimensionality reduction, their application\nin self-supervised learning has witnessed slow progress. The recent MSimCLR\nmethod combines manifold encoding with SimCLR but requires extremely low target\nencoding dimensions to outperform SimCLR, limiting its applicability. This\npaper introduces a novel learning paradigm using an unbalanced atlas (UA),\ncapable of surpassing state-of-the-art self-supervised learning approaches. We\ninvestigated and engineered the DeepInfomax with an unbalanced atlas (DIM-UA)\nmethod by adapting the Spatiotemporal DeepInfomax (ST-DIM) framework to align\nwith our proposed UA paradigm. The efficacy of DIM-UA is demonstrated through\ntraining and evaluation on the Atari Annotated RAM Interface (AtariARI)\nbenchmark, a modified version of the Atari 2600 framework that produces\nannotated image samples for representation learning. The UA paradigm improves\nexisting algorithms significantly as the number of target encoding dimensions\ngrows. For instance, the mean F1 score averaged over categories of DIM-UA is\n~75% compared to ~70% of ST-DIM when using 16384 hidden units.\n', '  The expanding research on manifold-based self-supervised learning (SSL)\nbuilds on the manifold hypothesis, which suggests that the inherent complexity\nof high-dimensional data can be unraveled through lower-dimensional manifold\nembeddings. Capitalizing on this, DeepInfomax with an unbalanced atlas (DIM-UA)\nhas emerged as a powerful tool and yielded impressive results for state\nrepresentations in reinforcement learning. Meanwhile, Maximum Manifold Capacity\nRepresentation (MMCR) presents a new frontier for SSL by optimizing class\nseparability via manifold compression. However, MMCR demands extensive input\nviews, resulting in significant computational costs and protracted pre-training\ndurations. Bridging this gap, we present an innovative integration of MMCR into\nexisting SSL methods, incorporating a discerning regularization strategy that\nenhances the lower bound of mutual information. We also propose a novel state\nrepresentation learning method extending DIM-UA, embedding a nuclear norm loss\nto enforce manifold consistency robustly. On experimentation with the Atari\nAnnotated RAM Interface, our method improves DIM-UA significantly with the same\nnumber of target encoding dimensions. The mean F1 score averaged over\ncategories is 78% compared to 75% of DIM-UA. There are also compelling gains\nwhen implementing SimCLR and Barlow Twins. This supports our SSL innovation as\na paradigm shift, enabling more nuanced high-dimensional data representations.\n']",Manifold-based Self-Supervised Learning Methods,Self-Supervised Learning and Representation Learning,Self-Supervised Representation Learning
290,290,23,290_tensorization_tensors_tensorflow_tensor,"['tensorization', 'tensors', 'tensorflow', 'tensor', 'tensorial', 'tensorkrowch', 'sparse', 'pytorch', 'decomposition', 'decompositions']","['tensor', 'tensors', 'sparse', 'decompositions', 'decomposition', 'operations', 'dense', 'tensorization', 'convolution', 'sparsity']","['  Tensor decompositions are invaluable tools in analyzing multimodal datasets.\nIn many real-world scenarios, such datasets are far from being static, to the\ncontrary they tend to grow over time. For instance, in an online social network\nsetting, as we observe new interactions over time, our dataset gets updated in\nits ""time"" mode. How can we maintain a valid and accurate tensor decomposition\nof such a dynamically evolving multimodal dataset, without having to re-compute\nthe entire decomposition after every single update? In this paper we introduce\nSaMbaTen, a Sampling-based Batch Incremental Tensor Decomposition algorithm,\nwhich incrementally maintains the decomposition given new updates to the tensor\ndataset. SaMbaTen is able to scale to datasets that the state-of-the-art in\nincremental tensor decomposition is unable to operate on, due to its ability to\neffectively summarize the existing tensor and the incoming updates, and perform\nall computations in the reduced summary space. We extensively evaluate SaMbaTen\nusing synthetic and real datasets. Indicatively, SaMbaTen achieves comparable\naccuracy to state-of-the-art incremental and non-incremental techniques, while\nbeing 25-30 times faster. Furthermore, SaMbaTen scales to very large sparse and\ndense dynamically evolving tensors of dimensions up to 100K x 100K x 100K where\nstate-of-the-art incremental approaches were not able to operate.\n', ""  Graphical tensor notation is a simple way of denoting linear operations on\ntensors, originating from physics. Modern deep learning consists almost\nentirely of operations on or between tensors, so easily understanding tensor\noperations is quite important for understanding these systems. This is\nespecially true when attempting to reverse-engineer the algorithms learned by a\nneural network in order to understand its behavior: a field known as\nmechanistic interpretability. It's often easy to get confused about which\noperations are happening between tensors and lose sight of the overall\nstructure, but graphical tensor notation makes it easier to parse things at a\nglance and see interesting equivalences. The first half of this document\nintroduces the notation and applies it to some decompositions (SVD, CP, Tucker,\nand tensor network decompositions), while the second half applies it to some\nexisting some foundational approaches for mechanistically understanding\nlanguage models, loosely following ``A Mathematical Framework for Transformer\nCircuits'', then constructing an example ``induction head'' circuit in\ngraphical tensor notation.\n"", '  Sparse tensor operations are gaining attention in emerging applications such\nas social networks, deep learning, diagnosis, crime, and review analysis.\nHowever, a major obstacle for research in sparse tensor operations is the\ndeficiency of a broad-scale sparse tensor dataset. Another challenge in sparse\ntensor operations is examining the sparse tensor features, which are not only\nimportant for revealing its nonzero pattern but also have a significant impact\non determining the best-suited storage format, the decomposition algorithm, and\nthe reordering methods. However, due to the large sizes of real tensors, even\nextracting these features becomes costly without caution. To address these gaps\nin the literature, we have developed a smart sparse tensor generator that\nmimics the substantial features of real sparse tensors. Moreover, we propose\nvarious methods for efficiently extracting an extensive set of features for\nsparse tensors. The effectiveness of our generator is validated through the\nquality of features and the performance of decomposition in the generated\ntensors. Both the sparse tensor feature extractor and the tensor generator are\nopen source with all the artifacts available at\nhttps://github.com/sparcityeu/feaTen and https://github.com/sparcityeu/genTen,\nrespectively.\n']",Tensor Decomposition and Operations,Tensor Methods and Applications,Tensor and Matrix Methods for Data Representation and Completion
291,291,21,291_autoencoders_autoencoder_embeddings_encoder,"['autoencoders', 'autoencoder', 'embeddings', 'encoder', 'dimensionality', 'dimensional', 'manifolds', 'manifold', 'representations', 'descriptors']","['latent', 'manifold', 'dimensional', 'spaces', 'manifolds', 'autoencoder', 'autoencoders', 'space', 'dimensionality', 'reduction']","['  Representing a manifold of very high-dimensional data with generative models\nhas been shown to be computationally efficient in practice. However, this\nrequires that the data manifold admits a global parameterization. In order to\nrepresent manifolds of arbitrary topology, we propose to learn a mixture model\nof variational autoencoders. Here, every encoder-decoder pair represents one\nchart of a manifold. We propose a loss function for maximum likelihood\nestimation of the model weights and choose an architecture that provides us the\nanalytical expression of the charts and of their inverses. Once the manifold is\nlearned, we use it for solving inverse problems by minimizing a data fidelity\nterm restricted to the learned manifold. To solve the arising minimization\nproblem we propose a Riemannian gradient descent algorithm on the learned\nmanifold. We demonstrate the performance of our method for low-dimensional toy\nexamples as well as for deblurring and electrical impedance tomography on\ncertain image manifolds.\n', '  The Manifold Hypothesis is a widely accepted tenet of Machine Learning which\nasserts that nominally high-dimensional data are in fact concentrated near a\nlow-dimensional manifold, embedded in high-dimensional space. This phenomenon\nis observed empirically in many real world situations, has led to development\nof a wide range of statistical methods in the last few decades, and has been\nsuggested as a key factor in the success of modern AI technologies. We show\nthat rich and sometimes intricate manifold structure in data can emerge from a\ngeneric and remarkably simple statistical model -- the Latent Metric Model --\nvia elementary concepts such as latent variables, correlation and stationarity.\nThis establishes a general statistical explanation for why the Manifold\nHypothesis seems to hold in so many situations. Informed by the Latent Metric\nModel we derive procedures to discover and interpret the geometry of\nhigh-dimensional data, and explore hypotheses about the data generating\nmechanism. These procedures operate under minimal assumptions and make use of\nwell known, scaleable graph-analytic algorithms.\n', '  Autoencoders, which consist of an encoder and a decoder, are widely used in\nmachine learning for dimension reduction of high-dimensional data. The encoder\nembeds the input data manifold into a lower-dimensional latent space, while the\ndecoder represents the inverse map, providing a parametrization of the data\nmanifold by the manifold in latent space. A good regularity and structure of\nthe embedded manifold may substantially simplify further data processing tasks\nsuch as cluster analysis or data interpolation. We propose and analyze a novel\nregularization for learning the encoder component of an autoencoder: a loss\nfunctional that prefers isometric, extrinsically flat embeddings and allows to\ntrain the encoder on its own. To perform the training it is assumed that for\npairs of nearby points on the input manifold their local Riemannian distance\nand their local Riemannian average can be evaluated. The loss functional is\ncomputed via Monte Carlo integration with different sampling strategies for\npairs of points on the input manifold. Our main theorem identifies a geometric\nloss functional of the embedding map as the $\\Gamma$-limit of the\nsampling-dependent loss functionals. Numerical tests, using image data that\nencodes different explicitly given data manifolds, show that smooth manifold\nembeddings into latent space are obtained. Due to the promotion of extrinsic\nflatness, these embeddings are regular enough such that interpolation between\nnot too distant points on the manifold is well approximated by linear\ninterpolation in latent space as one possible postprocessing.\n']",Manifold Learning with Autoencoders,Geometric Deep Learning on Manifolds,Geometric and Equivariant Deep Learning
292,292,19,292_search_retrieval_crowdsourced_google,"['search', 'retrieval', 'crowdsourced', 'google', 'bing', 'snippets', 'bingchat', 'responses', 'relevance', 'generative']","['engines', 'search', 'explanations', 'snippets', 'queries', 'responses', 'engine', 'conversational', 'explanation', 'explainability']","[""  There is a growing demand for transparency in search engines to understand\nhow search results are curated and to enhance users' trust. Prior research has\nintroduced search result explanations with a focus on how to explain, assuming\nexplanations are beneficial. Our study takes a step back to examine if search\nexplanations are needed and when they are likely to provide benefits.\nAdditionally, we summarize key characteristics of helpful explanations and\nshare users' perspectives on explanation features provided by Google and Bing.\nInterviews with non-technical individuals reveal that users do not always seek\nor understand search explanations and mostly desire them for complex and\ncritical tasks. They find Google's search explanations too obvious but\nappreciate the ability to contest search results. Based on our findings, we\noffer design recommendations for search engines and explanations to help users\nbetter evaluate search results and enhance their search experience.\n"", '  The advent of large language models (LLMs) has ushered in a new paradigm of\nsearch engines that use generative models to gather and summarize information\nto answer user queries. This emerging technology, which we formalize under the\nunified framework of generative engines (GEs), can generate accurate and\npersonalized responses, rapidly replacing traditional search engines like\nGoogle and Bing. Generative Engines typically satisfy queries by synthesizing\ninformation from multiple sources and summarizing them using LLMs. While this\nshift significantly improves $\\textit{user}$ utility and $\\textit{generative\nsearch engine}$ traffic, it poses a huge challenge for the third stakeholder --\nwebsite and content creators. Given the black-box and fast-moving nature of\ngenerative engines, content creators have little to no control over\n$\\textit{when}$ and $\\textit{how}$ their content is displayed. With generative\nengines here to stay, we must ensure the creator economy is not disadvantaged.\nTo address this, we introduce Generative Engine Optimization (GEO), the first\nnovel paradigm to aid content creators in improving their content visibility in\ngenerative engine responses through a flexible black-box optimization framework\nfor optimizing and defining visibility metrics. We facilitate systematic\nevaluation by introducing GEO-bench, a large-scale benchmark of diverse user\nqueries across multiple domains, along with relevant web sources to answer\nthese queries. Through rigorous evaluation, we demonstrate that GEO can boost\nvisibility by up to $40\\%$ in generative engine responses. Moreover, we show\nthe efficacy of these strategies varies across domains, underscoring the need\nfor domain-specific optimization methods. Our work opens a new frontier in\ninformation discovery systems, with profound implications for both developers\nof generative engines and content creators.\n', ""  Large Language Models (LLMs) are increasingly used for accessing information\non the web. Their truthfulness and factuality are thus of great interest. To\nhelp users make the right decisions about the information they get, LLMs should\nnot only provide information but also help users fact-check it. Our experiments\nwith 80 crowdworkers compare language models with search engines (information\nretrieval systems) at facilitating fact-checking. We prompt LLMs to validate a\ngiven claim and provide corresponding explanations. Users reading LLM\nexplanations are significantly more efficient than those using search engines\nwhile achieving similar accuracy. However, they over-rely on the LLMs when the\nexplanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide\ncontrastive information - explain both why the claim is true and false, and\nthen we present both sides of the explanation to users. This contrastive\nexplanation mitigates users' over-reliance on LLMs, but cannot significantly\noutperform search engines. Further, showing both search engine results and LLM\nexplanations offers no complementary benefits compared to search engines alone.\nTaken together, our study highlights that natural language explanations by LLMs\nmay not be a reliable replacement for reading the retrieved passages,\nespecially in high-stakes settings where over-relying on wrong AI explanations\ncould lead to critical consequences.\n""]",Search Engine Transparency and Explanation Systems,Transparency and Explainability in AI Systems,Artificial Intelligence and Machine Learning Interpretability and Explainability
293,293,19,293_automata_neural_evolution_neuronal,"['automata', 'neural', 'evolution', 'neuronal', 'evolved', 'developmental', 'morphogenesis', 'dynamics', 'automaton', 'cells']","['nca', 'developmental', 'empowerment', 'automata', 'cellular', 'morphogenesis', 'self', 'sensorimotor', 'evolution', 'plasticity']","[""  Information-theoretic fitness functions are becoming increasingly popular to\nproduce generally useful, task-independent behaviors. One such universal\nfunction, dubbed empowerment, measures the amount of control an agent exerts on\nits environment via its sensorimotor system. Specifically, empowerment attempts\nto maximize the mutual information between an agent's actions and its received\nsensor states at a later point in time. Traditionally, empowerment has been\napplied to a conventional sensorimotor apparatus, such as a robot. Here, we\nexpand the approach to a distributed, multi-agent sensorimotor system embodied\nby a neural cellular automaton (NCA). We show that the addition of empowerment\nas a secondary objective in the evolution of NCA to perform the task of\nmorphogenesis, growing and maintaining a pre-specified shape, results in higher\nfitness compared to evolving for morphogenesis alone. Results suggest there may\nbe a synergistic relationship between morphogenesis and empowerment. That is,\nindirectly selecting for coordination between neighboring cells over the\nduration of development is beneficial to the developmental process itself. Such\na finding may have applications in developmental biology by providing potential\nmechanisms of communication between cells during growth from a single cell to a\nmulticellular, target morphology. Source code for the experiments in this paper\ncan be found at: \\url{https://github.com/caitlingrasso/empowered-nca}.\n"", '  Neural Cellular Automata (NCA) are a powerful combination of machine learning\nand mechanistic modelling. We train NCA to learn complex dynamics from time\nseries of images and PDE trajectories. Our method is designed to identify\nunderlying local rules that govern large scale dynamic emergent behaviours.\nPrevious work on NCA focuses on learning rules that give stationary emergent\nstructures. We extend NCA to capture both transient and stable structures\nwithin the same system, as well as learning rules that capture the dynamics of\nTuring pattern formation in nonlinear Partial Differential Equations (PDEs). We\ndemonstrate that NCA can generalise very well beyond their PDE training data,\nwe show how to constrain NCA to respect given symmetries, and we explore the\neffects of associated hyperparameters on model performance and stability. Being\nable to learn arbitrary dynamics gives NCA great potential as a data driven\nmodelling framework, especially for modelling biological pattern formation.\n', '  Neural Cellular Automata (NCA) is a class of Cellular Automata where the\nupdate rule is parameterized by a neural network that can be trained using\ngradient descent. In this paper, we focus on NCA models used for texture\nsynthesis, where the update rule is inspired by partial differential equations\n(PDEs) describing reaction-diffusion systems. To train the NCA model, the\nspatio-temporal domain is discretized, and Euler integration is used to\nnumerically simulate the PDE. However, whether a trained NCA truly learns the\ncontinuous dynamic described by the corresponding PDE or merely overfits the\ndiscretization used in training remains an open question. We study NCA models\nat the limit where space-time discretization approaches continuity. We find\nthat existing NCA models tend to overfit the training discretization,\nespecially in the proximity of the initial condition, also called ""seed"". To\naddress this, we propose a solution that utilizes uniform noise as the initial\ncondition. We demonstrate the effectiveness of our approach in preserving the\nconsistency of NCA dynamics across a wide range of spatio-temporal\ngranularities. Our improved NCA model enables two new test-time interactions by\nallowing continuous control over the speed of pattern formation and the scale\nof the synthesized patterns. We demonstrate this new NCA feature in our\ninteractive online demo. Our work reveals that NCA models can learn continuous\ndynamics and opens new venues for NCA research from a dynamical system\'s\nperspective.\n']",Neural Cellular Automata for Morphogenesis and Dynamics,Cellular Dynamics and Neural Cellular Automata for Morphogenesis and Pattern Formation,Computational Models of Complex Systems and Processes
294,294,19,294_multimodal_modality_audio_videos,"['multimodal', 'modality', 'audio', 'videos', 'nonverbal', 'modal', 'recognition', 'fusionnets', 'supervised', 'decoder']","['modality', 'audio', 'video', 'modalities', 'modal', 'multimodal', 'fusion', 'speech', 'intent', 'scope']","[""  Multimodal intent recognition aims to leverage diverse modalities such as\nexpressions, body movements and tone of speech to comprehend user's intent,\nconstituting a critical task for understanding human language and behavior in\nreal-world multimodal scenarios. Nevertheless, the majority of existing methods\nignore potential correlations among different modalities and own limitations in\neffectively learning semantic features from nonverbal modalities. In this\npaper, we introduce a token-level contrastive learning method with\nmodality-aware prompting (TCL-MAP) to address the above challenges. To\nestablish an optimal multimodal semantic environment for text modality, we\ndevelop a modality-aware prompting module (MAP), which effectively aligns and\nfuses features from text, video and audio modalities with similarity-based\nmodality alignment and cross-modality attention mechanism. Based on the\nmodality-aware prompt and ground truth labels, the proposed token-level\ncontrastive learning framework (TCL) constructs augmented samples and employs\nNT-Xent loss on the label token. Specifically, TCL capitalizes on the optimal\ntextual semantic insights derived from intent labels to guide the learning\nprocesses of other modalities in return. Extensive experiments show that our\nmethod achieves remarkable improvements compared to state-of-the-art methods.\nAdditionally, ablation analyses demonstrate the superiority of the\nmodality-aware prompt over the handcrafted prompt, which holds substantial\nsignificance for multimodal prompt learning. The codes are released at\nhttps://github.com/thuiar/TCL-MAP.\n"", '  Audio and video are two most common modalities in the mainstream media\nplatforms, e.g., YouTube. To learn from multimodal videos effectively, in this\nwork, we propose a novel audio-video recognition approach termed audio video\nTransformer, AVT, leveraging the effective spatio-temporal representation by\nthe video Transformer to improve action recognition accuracy. For multimodal\nfusion, simply concatenating multimodal tokens in a cross-modal Transformer\nrequires large computational and memory resources, instead we reduce the\ncross-modality complexity through an audio-video bottleneck Transformer. To\nimprove the learning efficiency of multimodal Transformer, we integrate\nself-supervised objectives, i.e., audio-video contrastive learning, audio-video\nmatching, and masked audio and video learning, into AVT training, which maps\ndiverse audio and video representations into a common multimodal representation\nspace. We further propose a masked audio segment loss to learn semantic audio\nactivities in AVT. Extensive experiments and ablation studies on three public\ndatasets and two in-house datasets consistently demonstrate the effectiveness\nof the proposed AVT. Specifically, AVT outperforms its previous\nstate-of-the-art counterparts on Kinetics-Sounds by 8%. AVT also surpasses one\nof the previous state-of-the-art video Transformers [25] by 10% on VGGSound by\nleveraging the audio signal. Compared to one of the previous state-of-the-art\nmultimodal methods, MBT [32], AVT is 1.3% more efficient in terms of FLOPs and\nimproves the accuracy by 3.8% on Epic-Kitchens-100.\n', '  Action quality assessment (AQA) is to assess how well an action is performed.\nPrevious works perform modelling by only the use of visual information,\nignoring audio information. We argue that although AQA is highly dependent on\nvisual information, the audio is useful complementary information for improving\nthe score regression accuracy, especially for sports with background music,\nsuch as figure skating and rhythmic gymnastics. To leverage multimodal\ninformation for AQA, i.e., RGB, optical flow and audio information, we propose\na Progressive Adaptive Multimodal Fusion Network (PAMFN) that separately models\nmodality-specific information and mixed-modality information. Our model\nconsists of with three modality-specific branches that independently explore\nmodality-specific information and a mixed-modality branch that progressively\naggregates the modality-specific information from the modality-specific\nbranches. To build the bridge between modality-specific branches and the\nmixed-modality branch, three novel modules are proposed. First, a\nModality-specific Feature Decoder module is designed to selectively transfer\nmodality-specific information to the mixed-modality branch. Second, when\nexploring the interaction between modality-specific information, we argue that\nusing an invariant multimodal fusion policy may lead to suboptimal results, so\nas to take the potential diversity in different parts of an action into\nconsideration. Therefore, an Adaptive Fusion Module is proposed to learn\nadaptive multimodal fusion policies in different parts of an action. This\nmodule consists of several FusionNets for exploring different multimodal fusion\nstrategies and a PolicyNet for deciding which FusionNets are enabled. Third, a\nmodule called Cross-modal Feature Decoder is designed to transfer cross-modal\nfeatures generated by Adaptive Fusion Module to the mixed-modality branch.\n']",Multimodal Learning and Fusion,Multimodal Learning and Fusion,Multimodal Learning and Applications
294,294,19,294_multimodal_modality_audio_videos,"['multimodal', 'modality', 'audio', 'videos', 'nonverbal', 'modal', 'recognition', 'fusionnets', 'supervised', 'decoder']","['modality', 'audio', 'video', 'modalities', 'modal', 'multimodal', 'fusion', 'speech', 'intent', 'scope']","[""  Multimodal intent recognition aims to leverage diverse modalities such as\nexpressions, body movements and tone of speech to comprehend user's intent,\nconstituting a critical task for understanding human language and behavior in\nreal-world multimodal scenarios. Nevertheless, the majority of existing methods\nignore potential correlations among different modalities and own limitations in\neffectively learning semantic features from nonverbal modalities. In this\npaper, we introduce a token-level contrastive learning method with\nmodality-aware prompting (TCL-MAP) to address the above challenges. To\nestablish an optimal multimodal semantic environment for text modality, we\ndevelop a modality-aware prompting module (MAP), which effectively aligns and\nfuses features from text, video and audio modalities with similarity-based\nmodality alignment and cross-modality attention mechanism. Based on the\nmodality-aware prompt and ground truth labels, the proposed token-level\ncontrastive learning framework (TCL) constructs augmented samples and employs\nNT-Xent loss on the label token. Specifically, TCL capitalizes on the optimal\ntextual semantic insights derived from intent labels to guide the learning\nprocesses of other modalities in return. Extensive experiments show that our\nmethod achieves remarkable improvements compared to state-of-the-art methods.\nAdditionally, ablation analyses demonstrate the superiority of the\nmodality-aware prompt over the handcrafted prompt, which holds substantial\nsignificance for multimodal prompt learning. The codes are released at\nhttps://github.com/thuiar/TCL-MAP.\n"", '  Audio and video are two most common modalities in the mainstream media\nplatforms, e.g., YouTube. To learn from multimodal videos effectively, in this\nwork, we propose a novel audio-video recognition approach termed audio video\nTransformer, AVT, leveraging the effective spatio-temporal representation by\nthe video Transformer to improve action recognition accuracy. For multimodal\nfusion, simply concatenating multimodal tokens in a cross-modal Transformer\nrequires large computational and memory resources, instead we reduce the\ncross-modality complexity through an audio-video bottleneck Transformer. To\nimprove the learning efficiency of multimodal Transformer, we integrate\nself-supervised objectives, i.e., audio-video contrastive learning, audio-video\nmatching, and masked audio and video learning, into AVT training, which maps\ndiverse audio and video representations into a common multimodal representation\nspace. We further propose a masked audio segment loss to learn semantic audio\nactivities in AVT. Extensive experiments and ablation studies on three public\ndatasets and two in-house datasets consistently demonstrate the effectiveness\nof the proposed AVT. Specifically, AVT outperforms its previous\nstate-of-the-art counterparts on Kinetics-Sounds by 8%. AVT also surpasses one\nof the previous state-of-the-art video Transformers [25] by 10% on VGGSound by\nleveraging the audio signal. Compared to one of the previous state-of-the-art\nmultimodal methods, MBT [32], AVT is 1.3% more efficient in terms of FLOPs and\nimproves the accuracy by 3.8% on Epic-Kitchens-100.\n', '  Action quality assessment (AQA) is to assess how well an action is performed.\nPrevious works perform modelling by only the use of visual information,\nignoring audio information. We argue that although AQA is highly dependent on\nvisual information, the audio is useful complementary information for improving\nthe score regression accuracy, especially for sports with background music,\nsuch as figure skating and rhythmic gymnastics. To leverage multimodal\ninformation for AQA, i.e., RGB, optical flow and audio information, we propose\na Progressive Adaptive Multimodal Fusion Network (PAMFN) that separately models\nmodality-specific information and mixed-modality information. Our model\nconsists of with three modality-specific branches that independently explore\nmodality-specific information and a mixed-modality branch that progressively\naggregates the modality-specific information from the modality-specific\nbranches. To build the bridge between modality-specific branches and the\nmixed-modality branch, three novel modules are proposed. First, a\nModality-specific Feature Decoder module is designed to selectively transfer\nmodality-specific information to the mixed-modality branch. Second, when\nexploring the interaction between modality-specific information, we argue that\nusing an invariant multimodal fusion policy may lead to suboptimal results, so\nas to take the potential diversity in different parts of an action into\nconsideration. Therefore, an Adaptive Fusion Module is proposed to learn\nadaptive multimodal fusion policies in different parts of an action. This\nmodule consists of several FusionNets for exploring different multimodal fusion\nstrategies and a PolicyNet for deciding which FusionNets are enabled. Third, a\nmodule called Cross-modal Feature Decoder is designed to transfer cross-modal\nfeatures generated by Adaptive Fusion Module to the mixed-modality branch.\n']",Multimodal Learning and Fusion,Multimodal Learning and Fusion,Multimodal Learning and Applications
295,295,18,295_storytelling_narratives_narrative_gameplay,"['storytelling', 'narratives', 'narrative', 'gameplay', 'procedural', 'storyline', 'rpgs', 'drama', 'characters', 'dialogue']","['narrative', 'plot', 'game', 'drama', 'stories', 'players', 'story', 'characters', 'character', 'playing']","[""  World-building, the process of developing both the narrative and physical\nworld of a game, plays a vital role in the game's experience.\nCritically-acclaimed independent and AAA video games are praised for strong\nworld-building, with game maps that masterfully intertwine with and elevate the\nnarrative, captivating players and leaving a lasting impression. However,\ndesigning game maps that support a desired narrative is challenging, as it\nrequires satisfying complex constraints from various considerations. Most\nexisting map generation methods focus on considerations about gameplay\nmechanics or map topography, while the need to support the story is typically\nneglected. As a result, extensive manual adjustment is still required to design\na game world that facilitates particular stories. In this work, we approach\nthis problem by introducing an extra layer of plot facility layout design that\nis independent of the underlying map generation method in a world-building\npipeline.\n  Concretely, we define (plot) facility layout tasks as the tasks of assigning\nconcrete locations on a game map to abstract locations mentioned in a given\nstory (plot facilities), following spatial constraints derived from the story.\nWe present two methods for solving these tasks automatically: an evolutionary\ncomputation based approach through Covariance Matrix Adaptation Evolution\nStrategy (CMA-ES), and a Reinforcement Learning (RL) based approach. We develop\na method of generating datasets of facility layout tasks, create a gym-like\nenvironment for experimenting with and evaluating different methods, and\nfurther analyze the two methods with comprehensive experiments, aiming to\nprovide insights for solving facility layout tasks. We will release the code\nand a dataset containing 10, 000 tasks of different scales.\n"", ""  This research introduces Procedural Artificial Narrative using Generative AI\n(PANGeA), a structured approach for leveraging large language models (LLMs),\nguided by a game designer's high-level criteria, to generate narrative content\nfor turn-based role-playing video games (RPGs). Distinct from prior\napplications of LLMs used for video game design, PANGeA innovates by not only\ngenerating game level data (which includes, but is not limited to, setting, key\nitems, and non-playable characters (NPCs)), but by also fostering dynamic,\nfree-form interactions between the player and the environment that align with\nthe procedural game narrative. The NPCs generated by PANGeA are\npersonality-biased and express traits from the Big 5 Personality Model in their\ngenerated responses. PANGeA addresses challenges behind ingesting free-form\ntext input, which can prompt LLM responses beyond the scope of the game\nnarrative. A novel validation system that uses the LLM's intelligence evaluates\ntext input and aligns generated responses with the unfolding narrative. Making\nthese interactions possible, PANGeA is supported by a server that hosts a\ncustom memory system that supplies context for augmenting generated responses\nthus aligning them with the procedural narrative. For its broad application,\nthe server has a REST interface enabling any game engine to integrate directly\nwith PANGeA, as well as an LLM interface adaptable with local or private LLMs.\nPANGeA's ability to foster dynamic narrative generation by aligning responses\nwith the procedural narrative is demonstrated through an empirical study and\nablation test of two versions of a demo game. These are, a custom,\nbrowser-based GPT and a Unity demo. As the results show, PANGeA holds potential\nto assist game designers in using LLMs to generate narrative-consistent content\neven when provided varied and unpredictable, free-form text input.\n"", '  Automated plot generation for games enhances the player\'s experience by\nproviding rich and immersive narrative experience that adapts to the player\'s\nactions. Traditional approaches adopt a symbolic narrative planning method\nwhich limits the scale and complexity of the generated plot by requiring\nextensive knowledge engineering work. Recent advancements use Large Language\nModels (LLMs) to drive the behavior of virtual characters, allowing plots to\nemerge from interactions between characters and their environments. However,\nthe emergent nature of such decentralized plot generation makes it difficult\nfor authors to direct plot progression. We propose a novel plot creation\nworkflow that mediates between a writer\'s authorial intent and the emergent\nbehaviors from LLM-driven character simulation, through a novel authorial\nstructure called ""abstract acts"". The writers define high-level plot outlines\nthat are later transformed into concrete character action sequences via an\nLLM-based narrative planning process, based on the game world state. The\nprocess creates ""living stories"" that dynamically adapt to various game world\nstates, resulting in narratives co-created by the author, character simulation,\nand player. We present StoryVerse as a proof-of-concept system to demonstrate\nthis plot creation workflow. We showcase the versatility of our approach with\nexamples in different stories and game environments.\n']",Procedural Narrative Generation for Games,Narrative Generation and Analysis in Games and Interactive Media,Artificial Intelligence for Creative Content Generation
296,296,17,296_relational_neural_relations_inference,"['relational', 'neural', 'relations', 'inference', 'embeddings', 'database', 'reasoning', 'gmpnn', 'algorithmic', 'logic']","['relational', 'algorithmic', 'reasoning', 'acceptable', 'transformers', 'symbolic', 'lattices', 'transformer', 'vertex', 'memory']","['  Developing models that can learn to reason is a notoriously challenging\nproblem. We focus on reasoning in relational domains, where the use of Graph\nNeural Networks (GNNs) seems like a natural choice. However, previous work on\nreasoning with GNNs has shown that such models tend to fail when presented with\ntest examples that require longer inference chains than those seen during\ntraining. This suggests that GNNs lack the ability to generalize from training\nexamples in a systematic way, which would fundamentally limit their reasoning\nabilities. A common solution is to instead rely on neuro-symbolic methods,\nwhich are capable of reasoning in a systematic way by design. Unfortunately,\nthe scalability of such methods is often limited and they tend to rely on\noverly strong assumptions, e.g.\\ that queries can be answered by inspecting a\nsingle relational path. In this paper, we revisit the idea of reasoning with\nGNNs, showing that systematic generalization is possible as long as the right\ninductive bias is provided. In particular, we argue that node embeddings should\nbe treated as epistemic states and that GNN should be parameterised\naccordingly. We propose a simple GNN architecture which is based on this view\nand show that it is capable of achieving state-of-the-art results. We\nfurthermore introduce a benchmark which requires models to aggregate evidence\nfrom multiple relational paths. We show that existing neuro-symbolic approaches\nfail on this benchmark, whereas our considered GNN model learns to reason\naccurately.\n', '  Although database systems perform well in data access and manipulation, their\nrelational model hinders data scientists from formulating machine learning\nalgorithms in SQL. Nevertheless, we argue that modern database systems perform\nwell for machine learning algorithms expressed in relational algebra. To\novercome the barrier of the relational model, this paper shows how to transform\ndata into a relational representation for training neural networks in SQL: We\nfirst describe building blocks for data transformation, model training and\ninference in SQL-92 and their counterparts using an extended array data type.\nThen, we compare the implementation for model training and inference using\narray data types to the one using a relational representation in SQL-92 only.\nThe evaluation in terms of runtime and memory consumption proves the\nsuitability of modern database systems for matrix algebra, although specialised\narray data types perform better than matrices in relational representation.\n', '  An extension of Transformers is proposed that enables explicit relational\nreasoning through a novel module called the Abstractor. At the core of the\nAbstractor is a variant of attention called relational cross-attention. The\napproach is motivated by an architectural inductive bias for relational\nlearning that disentangles relational information from object-level features.\nThis enables explicit relational reasoning, supporting abstraction and\ngeneralization from limited data. The Abstractor is first evaluated on simple\ndiscriminative relational tasks and compared to existing relational\narchitectures. Next, the Abstractor is evaluated on purely relational\nsequence-to-sequence tasks, where dramatic improvements are seen in sample\nefficiency compared to standard Transformers. Finally, Abstractors are\nevaluated on a collection of tasks based on mathematical problem solving, where\nconsistent improvements in performance and sample efficiency are observed.\n']",Relational Neural Networks for Reasoning,"Reasoning and Problem-Solving with Logic, Language, and Graphs",Artificial Intelligence and Reasoning Systems
297,297,17,297_neural_cnns_tensorflow_networks,"['neural', 'cnns', 'tensorflow', 'networks', 'backpropagation', 'cvnn', 'cvnns', 'hypercomplex', 'deepshap', 'multidimensional']","['hypercomplex', 'neural', 'networks', 'complex', 'algebras', 'multidimensional', 'algebra', 'intercorrelation', 'noncommutative', 'numbers']","['  Deep Neural Networks are widely used in academy as well as corporate and\npublic applications, including safety critical applications such as health care\nand autonomous driving. The ability to explain their output is critical for\nsafety reasons as well as acceptance among applicants. A multitude of methods\nhave been proposed to explain real-valued neural networks. Recently,\ncomplex-valued neural networks have emerged as a new class of neural networks\ndealing with complex-valued input data without the necessity of projecting them\nonto $\\mathbb{R}^2$. This brings up the need to develop explanation algorithms\nfor this kind of neural networks. In this paper we provide these developments.\nWhile we focus on adapting the widely used DeepSHAP algorithm to the complex\ndomain, we also present versions of four gradient based explanation methods\nsuitable for use in complex-valued neural networks. We evaluate the explanation\nquality of all presented algorithms and provide all of them as an open source\nlibrary adaptable to most recent complex-valued neural network architectures.\n', '  The universal approximation theorem states that a neural network with one\nhidden layer can approximate continuous functions on compact sets with any\ndesired precision. This theorem supports using neural networks for various\napplications, including regression and classification tasks. Furthermore, it is\nvalid for real-valued neural networks and some hypercomplex-valued neural\nnetworks such as complex-, quaternion-, tessarine-, and Clifford-valued neural\nnetworks. However, hypercomplex-valued neural networks are a type of\nvector-valued neural network defined on an algebra with additional algebraic or\ngeometric properties. This paper extends the universal approximation theorem\nfor a wide range of vector-valued neural networks, including\nhypercomplex-valued models as particular instances. Precisely, we introduce the\nconcept of non-degenerate algebra and state the universal approximation theorem\nfor neural networks defined on such algebras.\n', '  Despite the many successful applications of deep learning models for\nmultidimensional signal and image processing, most traditional neural networks\nprocess data represented by (multidimensional) arrays of real numbers. The\nintercorrelation between feature channels is usually expected to be learned\nfrom the training data, requiring numerous parameters and careful training. In\ncontrast, vector-valued neural networks are conceived to process arrays of\nvectors and naturally consider the intercorrelation between feature channels.\nConsequently, they usually have fewer parameters and often undergo more robust\ntraining than traditional neural networks. This paper aims to present a broad\nframework for vector-valued neural networks, referred to as V-nets. In this\ncontext, hypercomplex-valued neural networks are regarded as vector-valued\nmodels with additional algebraic properties. Furthermore, this paper explains\nthe relationship between vector-valued and traditional neural networks.\nPrecisely, a vector-valued neural network can be obtained by placing\nrestrictions on a real-valued model to consider the intercorrelation between\nfeature channels. Finally, we show how V-nets, including hypercomplex-valued\nneural networks, can be implemented in current deep-learning libraries as\nreal-valued networks.\n']",Explainability and Applications of Complex-Valued Neural Networks,Explainability and Interpretability in Neural Networks,Explainable Artificial Intelligence
298,298,16,298_recommender_recommenders_explanations_ai,"['recommender', 'recommenders', 'explanations', 'ai', 'predictability', 'explainability', 'visualizations', 'personalised', 'suggesting', 'predictable']","['recommender', 'app', 'users', 'explanations', 'nudge', 'explanation', 'explainable', 'systems', 'habits', 'user']","[""  Recent years have witnessed a rapid growth of recommender systems, providing\nsuggestions in numerous applications with potentially high social impact, such\nas health or justice. Meanwhile, in Europe, the upcoming AI Act mentions\n\\emph{transparency} as a requirement for critical AI systems in order to\n``mitigate the risks to fundamental rights''. Post-hoc explanations seamlessly\nalign with this goal and extensive literature on the subject produced several\nforms of such objects, graphs being one of them. Early studies in visualization\ndemonstrated the graphs' ability to improve user understanding, positioning\nthem as potentially ideal explanations. However, it remains unclear how\ngraph-based explanations compare to other explanation designs. In this work, we\naim to determine the effectiveness of graph-based explanations in improving\nusers' perception of AI-based recommendations using a mixed-methods approach.\nWe first conduct a qualitative study to collect users' requirements for graph\nexplanations. We then run a larger quantitative study in which we evaluate the\ninfluence of various explanation designs, including enhanced graph-based ones,\non aspects such as understanding, usability and curiosity toward the AI system.\nWe find that users perceive graph-based explanations as more usable than\ndesigns involving feature importance. However, we also reveal that textual\nexplanations lead to higher objective understanding than graph-based designs.\nMost importantly, we highlight the strong contrast between participants'\nexpressed preferences for graph design and their actual ratings using it, which\nare lower compared to textual design. These findings imply that meeting\nstakeholders' expressed preferences might not alone guarantee ``good''\nexplanations. Therefore, crafting hybrid designs successfully balancing social\nexpectations with downstream performance emerges as a significant challenge.\n"", '  Providing system-generated explanations for recommendations represents an\nimportant step towards transparent and trustworthy recommender systems.\nExplainable recommender systems provide a human-understandable rationale for\ntheir outputs. Over the last two decades, explainable recommendation has\nattracted much attention in the recommender systems research community. This\npaper aims to provide a comprehensive review of research efforts on visual\nexplanation in recommender systems. More concretely, we systematically review\nthe literature on explanations in recommender systems based on four dimensions,\nnamely explanation goal, explanation scope, explanation style, and explanation\nformat. Recognizing the importance of visualization, we approach the\nrecommender system literature from the angle of explanatory visualizations,\nthat is using visualizations as a display style of explanation. As a result, we\nderive a set of guidelines that might be constructive for designing explanatory\nvisualizations in recommender systems and identify perspectives for future work\nin this field. The aim of this review is to help recommendation researchers and\npractitioners better understand the potential of visually explainable\nrecommendation research and to support them in the systematic design of visual\nexplanations in current and future recommender systems.\n', ""  In recent years, predicting mobile app usage has become increasingly\nimportant for areas like app recommendation, user behaviour analysis, and\nmobile resource management. Existing models, however, struggle with the\nheterogeneous nature of contextual data and the user cold start problem. This\nstudy introduces a novel prediction model, Mobile App Prediction Leveraging\nLarge Language Model Embeddings (MAPLE), which employs Large Language Models\n(LLMs) and installed app similarity to overcome these challenges. MAPLE\nutilises the power of LLMs to process contextual data and discern intricate\nrelationships within it effectively. Additionally, we explore the use of\ninstalled app similarity to address the cold start problem, facilitating the\nmodelling of user preferences and habits, even for new users with limited\nhistorical data. In essence, our research presents MAPLE as a novel, potent,\nand practical approach to app usage prediction, making significant strides in\nresolving issues faced by existing models. MAPLE stands out as a comprehensive\nand effective solution, setting a new benchmark for more precise and\npersonalised app usage predictions. In tests on two real-world datasets, MAPLE\nsurpasses contemporary models in both standard and cold start scenarios. These\noutcomes validate MAPLE's capacity for precise app usage predictions and its\nresilience against the cold start problem. This enhanced performance stems from\nthe model's proficiency in capturing complex temporal patterns and leveraging\ncontextual information. As a result, MAPLE can potentially improve personalised\nmobile app usage predictions and user experiences markedly.\n""]",Explainable Recommender Systems and Predictions,Advances in Recommender Systems,Recommender Systems and Personalization
299,299,16,299_visualizations_visualization_charts_visual,"['visualizations', 'visualization', 'charts', 'visual', 'interactive', 'colormaps', 'analytics', 'colormap', 'tools', 'bioinformatics']","['visualization', 'visualizations', 'colormaps', 'analysts', 'tools', 'analytics', 'interactive', 'interviews', 'visual', 'builders']","[""  Computational notebooks, such as Jupyter Notebook, have become data\nscientists' de facto programming environments. Many visualization researchers\nand practitioners have developed interactive visualization tools that support\nnotebooks, yet little is known about the appropriate design of these tools. To\naddress this critical research gap, we investigate the design strategies in\nthis space by analyzing 163 notebook visualization tools. Our analysis\nencompasses 64 systems from academic papers and 105 systems sourced from a pool\nof 55k notebooks containing interactive visualizations that we obtain via\nscraping 8.6 million notebooks on GitHub. Through this study, we identify key\ndesign implications and trade-offs, such as leveraging multimodal data in\nnotebooks as well as balancing the degree of visualization-notebook\nintegration. Furthermore, we provide empirical evidence that tools compatible\nwith more notebook platforms have a greater impact. Finally, we develop\nSuperNOVA, an open-source interactive browser to help researchers explore\nexisting notebook visualization tools. SuperNOVA is publicly accessible at:\nhttps://poloclub.github.io/supernova/.\n"", '  Machine learning (ML) models are nowadays used in complex applications in\nvarious domains, such as medicine, bioinformatics, and other sciences. Due to\ntheir black box nature, however, it may sometimes be hard to understand and\ntrust the results they provide. This has increased the demand for reliable\nvisualization tools related to enhancing trust in ML models, which has become a\nprominent topic of research in the visualization community over the past\ndecades. To provide an overview and present the frontiers of current research\non the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in\nML models with the use of interactive visualization. We define and describe the\nbackground of the topic, introduce a categorization for visualization\ntechniques that aim to accomplish this goal, and discuss insights and\nopportunities for future research directions. Among our contributions is a\ncategorization of trust against different facets of interactive ML, expanded\nand improved from previous research. Our results are investigated from\ndifferent analytical perspectives: (a) providing a statistical overview, (b)\nsummarizing key findings, (c) performing topic analyses, and (d) exploring the\ndata sets used in the individual papers, all with the support of an interactive\nweb-based survey browser. We intend this survey to be beneficial for\nvisualization researchers whose interests involve making ML models more\ntrustworthy, as well as researchers and practitioners from other disciplines in\ntheir search for effective visualization techniques suitable for solving their\ntasks with confidence and conveying meaning to their data.\n', '  The growing importance of data visualization in business intelligence and\ndata science emphasizes the need for tools that can efficiently generate\nmeaningful visualizations from large datasets. Existing tools fall into two\nmain categories: human-powered tools (e.g., Tableau and PowerBI), which require\nintensive expert involvement, and AI-powered automated tools (e.g., Draco and\nTable2Charts), which often fall short of guessing specific user needs. In this\npaper, we aim to achieve the best of both worlds. Our key idea is to initially\nauto-generate a set of high-quality visualizations to minimize manual effort,\nthen refine this process iteratively with user feedback to more closely align\nwith their needs. To this end, we present HAIChart, a reinforcement\nlearning-based framework designed to iteratively recommend good visualizations\nfor a given dataset by incorporating user feedback. Specifically, we propose a\nMonte Carlo Graph Search-based visualization generation algorithm paired with a\ncomposite reward function to efficiently explore the visualization space and\nautomatically generate good visualizations. We devise a visualization hints\nmechanism to actively incorporate user feedback, thus progressively refining\nthe visualization generation module. We further prove that the top-k\nvisualization hints selection problem is NP-hard and design an efficient\nalgorithm. We conduct both quantitative evaluations and user studies, showing\nthat HAIChart significantly outperforms state-of-the-art human-powered tools\n(21% better at Recall and 1.8 times faster) and AI-powered automatic tools\n(25.1% and 14.9% better in terms of Hit@3 and R10@30, respectively).\n']",Data Visualization Tools and Techniques,Data Visualization and Chart Understanding,Data Analysis and Visualization
300,300,15,300_agricultural_agriculture_ai_farming,"['agricultural', 'agriculture', 'ai', 'farming', 'farmers', 'automating', 'gardening', 'crop', 'machinery', 'expertise']","['agriculture', 'agricultural', 'farmers', 'care', 'plant', 'crop', 'farming', 'pest', 'engineering', 'industrial']","['  Agriculture, vital for global sustenance, necessitates innovative solutions\ndue to a lack of organized domain experts, particularly in developing countries\nwhere many farmers are impoverished and cannot afford expert consulting.\nInitiatives like Farmers Helpline play a crucial role in such countries, yet\nchallenges such as high operational costs persist. Automating query resolution\ncan alleviate the burden on traditional call centers, providing farmers with\nimmediate and contextually relevant information. The integration of Agriculture\nand Artificial Intelligence (AI) offers a transformative opportunity to empower\nfarmers and bridge information gaps. Language models like transformers, the\nrising stars of AI, possess remarkable language understanding capabilities,\nmaking them ideal for addressing information gaps in agriculture. This work\nexplores and demonstrates the transformative potential of Large Language Models\n(LLMs) in automating query resolution for agricultural farmers, leveraging\ntheir expertise in deciphering natural language and understanding context.\nUsing a subset of a vast dataset of real-world farmer queries collected in\nIndia, our study focuses on approximately 4 million queries from the state of\nTamil Nadu, spanning various sectors, seasonal crops, and query types.\n', '  The past decade has witnessed the rapid development and adoption of ML & DL\nmethodologies in agricultural systems, showcased by great successes in\nagricultural applications. However, these conventional ML/DL models have\ncertain limitations: they heavily rely on large, costly-to-acquire labeled\ndatasets for training, require specialized expertise for development and\nmaintenance, and are mostly tailored for specific tasks, thus lacking\ngeneralizability. Recently, large pre-trained models, also known as FMs, have\ndemonstrated remarkable successes in language, vision, and decision-making\ntasks across various domains. These models are trained on a large amount of\ndata from multiple domains and modalities. Once trained, they can accomplish\nversatile tasks with just minor fine-tuning and minimal task-specific labeled\ndata. Despite their proven effectiveness and huge potential, there has been\nlittle exploration of applying FMs to agriculture AI. Thus, this study aims to\nexplore the potential of FMs in the field of smart agriculture. In particular,\nconceptual tools and technical background are presented to help the\nunderstanding of the problem space and uncover new research directions. To this\nend, recent FMs in the general CS domain are reviewed, and the models are\ncategorized into four categories: language FMs, vision FMs, multimodal FMs, and\nreinforcement learning FMs. Then, the steps of developing agriculture FMs\n(AFMs) are outlined and potential applications in smart agriculture are\ndiscussed. Moreover, challenges and risks associated with developing AFMs are\ndiscussed, including model training, validation, and deployment. In summary,\nthe advancement of AI in agriculture is explored by introducing AFMs as a\npromising paradigm that can significantly mitigate the reliance on extensive\nlabeled datasets and enhance the efficiency, effectiveness, and generalization\nof agricultural AI systems.\n', '  Food production is a vital global concern and the potential for an agritech\nrevolution through artificial intelligence (AI) remains largely unexplored.\nThis paper presents a comprehensive review focused on the application of\nmachine learning (ML) in agriculture, aiming to explore its transformative\npotential in farming practices and efficiency enhancement. To understand the\nextent of research activity in this field, statistical data have been gathered,\nrevealing a substantial growth trend in recent years. This indicates that it\nstands out as one of the most dynamic and vibrant research domains. By\nintroducing the concept of ML and delving into the realm of smart agriculture,\nincluding Precision Agriculture, Smart Farming, Digital Agriculture, and\nAgriculture 4.0, we investigate how AI can optimize crop output and minimize\nenvironmental impact. We highlight the capacity of ML to analyze and classify\nagricultural data, providing examples of improved productivity and\nprofitability on farms. Furthermore, we discuss prominent ML models and their\nunique features that have shown promising results in agricultural applications.\nThrough a systematic review of the literature, this paper addresses the\nexisting literature gap on AI in agriculture and offers valuable information to\nnewcomers and researchers. By shedding light on unexplored areas within this\nemerging field, our objective is to facilitate a deeper understanding of the\nsignificant contributions and potential of AI in agriculture, ultimately\nbenefiting the research community.\n']",Agricultural AI for Farming Efficiency,Sustainable AI and Digital Agriculture Innovations,Innovations in Sustainable Technology and Food Systems
301,301,15,301_ontology_ontological_agents_owl,"['ontology', 'ontological', 'agents', 'owl', 'agent', 'semantics', 'services', 'netlogo', 'infrastructures', 'modelling']","['ontology', 'simulation', 'smart', 'blockchain', 'coordination', 'agents', 'systems', 'services', 'agent', 'contracts']","['  In this paper an agent-based simulation is developed in order to evaluate an\nAmI scenario based on agents. Many AmI applications are implemented through\nagents but they are not compared to any other existing alternative in order to\nevaluate the relative benefits of using them. The proposal simulation\nenvironment developed in Netlogo analyse such benefits using two evaluation\ncriteria: First, measuring agent satisfaction of different types of desires\nalong the execution. Second, measuring time savings obtained through a correct\nuse of context information.\n  So, here, a previously suggested agent architecture, an ontology and a\n12-steps protocol to provide AmI services in airports, is evaluated using a\nNetLogo simulation environment. The present work uses a NetLogo model\nconsidering scalability problems of this application domain but using FIPA and\nBDI extensions to be coherent with our previous works and our previous JADE\nimplementation of them.\n  The NetLogo model presented simulates an airport with agent users passing\nthrough several zones located in a specific order in a map: passport controls,\ncheck-in counters of airline companies, boarding gates, different types of\nshopping. Although initial data in simulations are generated randomly, and the\nmodel is just an approximation of real-world airports, the definition of this\ncase of use of Ambient Intelligence through NetLogo agents opens an interesting\nway to evaluate the benefits of using Ambient Intelligence, which is a\nsignificant contribution to the final development of them.\n', ""  A critical function of an organization is to foster the level of integration\n(coordination and cooperation) necessary to achieve its objectives. The need to\ncoordinate and motivation to cooperate emerges from the myriad dependencies\nbetween an organization's members and their work. Therefore, to reason about\nsolutions to coordination and cooperation problems requires a robust\nrepresentation that includes the underlying dependencies. We find that such a\nrepresentation remains missing from formal organizational models, and we\nleverage semantics to bridge this gap. Drawing on well-established\norganizational research and our extensive fieldwork with one of North America's\nlargest municipalities, (1) we introduce an ontology, formalized in first-order\nlogic, that operationalizes concepts like outcome, reward, and epistemic\ndependence, and their links to potential integration risks; and (2) present\nreal-world applications of this ontology to analyze and support integration in\ncomplex government infrastructure projects. Our ontology is implemented and\nvalidated in both Z3 and OWL. Key features of our model include inferable\ndependencies, explainable coordination and cooperation risks, and actionable\ninsights on how dependency structures within an organization can be altered to\nmitigate the risks. Conceptualizing real-world challenges like incentive\nmisalignment, free-riding, and subgoal optimization in terms of dependency\nstructures, our semantics-based approach represents a novel method for\nmodelling and enhancing coordination and cooperation. Integrated within a\ndecision-support system, our model may serve as an impactful aid for\norganizational design and effectiveness. More broadly, our approach underscores\nthe transformative potential of semantics in deriving tangible, real-world\nvalue from existing organization theory.\n"", '  In this contribution we extend an ontology for modelling agents and their\ninteractions, called Ontology for Agents, Systems, and Integration of Services\n(in short, OASIS), with conditionals and ontological smart contracts (in short,\nOSCs). OSCs are ontological representations of smart contracts that allow to\nestablish responsibilities and authorizations among agents and set agreements,\nwhereas conditionals allow one to restrict and limit agent interactions, define\nactivation mechanisms that trigger agent actions, and define constraints and\ncontract terms on OSCs. Conditionals and OSCs, as defined in OASIS, are applied\nto extend with ontological capabilities digital public ledgers such as the\nblockchain and smart contracts implemented on it. We will also sketch the\narchitecture of a framework based on the OASIS definition of OSCs that exploits\nthe Ethereum platform and the Interplanetary File System.\n']",Ontology-based Modelling of Agents and Services,Agent-Based Modeling and Ontological Representations in Finance and Complex Systems,Artificial Intelligence and Cognitive Systems
302,302,13,302_intelligencetest_intelligence_visual_abstraction,"['intelligencetest', 'intelligence', 'visual', 'abstraction', 'iq', 'attributes', 'learner', 'tasks', 'deductive', 'solvers']","['abstract', 'reasoning', 'progressive', 'rule', 'rules', 'matrices', 'visual', 'tests', 'symbolic', 'arrangements']","[""  The field of Abstract Visual Reasoning (AVR) encompasses a wide range of\nproblems, many of which are inspired by human IQ tests. The variety of AVR\ntasks has resulted in state-of-the-art AVR methods being task-specific\napproaches. Furthermore, contemporary methods consider each AVR problem\ninstance not as a whole, but in the form of a set of individual panels with\nparticular locations and roles (context vs. answer panels) pre-assigned\naccording to the task-specific arrangements. While these highly specialized\napproaches have recently led to significant progress in solving particular AVR\ntasks, considering each task in isolation hinders the development of universal\nlearning systems in this domain. In this paper, we introduce a unified view of\nAVR tasks, where each problem instance is rendered as a single image, with no a\npriori assumptions about the number of panels, their location, or role. The\nmain advantage of the proposed unified view is the ability to develop universal\nlearning models applicable to various AVR tasks. What is more, the proposed\napproach inherently facilitates transfer learning in the AVR domain, as various\ntypes of problems share a common representation. The experiments conducted on\nfour AVR datasets with Raven's Progressive Matrices and Visual Analogy\nProblems, and one real-world visual analogy dataset show that the proposed\nunified representation of AVR tasks poses a challenge to state-of-the-art Deep\nLearning (DL) AVR models and, more broadly, contemporary DL image recognition\nmethods. In order to address this challenge, we introduce the Unified Model for\nAbstract Visual Reasoning (UMAVR) capable of dealing with various types of AVR\nproblems in a unified manner. UMAVR outperforms existing AVR methods in\nselected single-task learning experiments, and demonstrates effective knowledge\nreuse in transfer learning and curriculum learning setups.\n"", ""  A hallmark of human intelligence is the ability to infer abstract rules from\nlimited experience and apply these rules to unfamiliar situations. This\ncapacity is widely studied in the visual domain using the Raven's Progressive\nMatrices. Recent advances in deep learning have led to multiple artificial\nneural network models matching or even surpassing human performance. However,\nwhile humans can identify and express the rule underlying these tasks with\nlittle to no exposure, contemporary neural networks often rely on massive\npattern-based training and cannot express or extrapolate the rule inferred from\nthe task. Furthermore, most Raven's Progressive Matrices or Raven-like tasks\nused for neural network training used symbolic representations, whereas humans\ncan flexibly switch between symbolic and continuous perceptual representations.\nIn this work, we present an algorithmic approach to rule detection and\napplication using feature detection, affine transformation estimation and\nsearch. We applied our model to a simplified Raven's Progressive Matrices task,\npreviously designed for behavioral testing and neuroimaging in humans. The\nmodel exhibited one-shot learning and achieved near human-level performance in\nthe symbolic reasoning condition of the simplified task. Furthermore, the model\ncan express the relationships discovered and generate multi-step predictions in\naccordance with the underlying rule. Finally, the model can reason using\ncontinuous patterns. We discuss our results and their relevance to studying\nabstract reasoning in humans, as well as their implications for improving\nintelligent machines.\n"", ""  We study generalization and knowledge reuse capabilities of deep neural\nnetworks in the domain of abstract visual reasoning (AVR), employing Raven's\nProgressive Matrices (RPMs), a recognized benchmark task for assessing AVR\nabilities. Two knowledge transfer scenarios referring to the I-RAVEN dataset\nare investigated. Firstly, inspired by generalization assessment capabilities\nof the PGM dataset and popularity of I-RAVEN, we introduce\nAttributeless-I-RAVEN, a benchmark with four generalization regimes that allow\nto test generalization of abstract rules applied to held-out attributes.\nSecondly, we construct I-RAVEN-Mesh, a dataset that enriches RPMs with a novel\ncomponent structure comprising line-based patterns, facilitating assessment of\nprogressive knowledge acquisition in transfer learning setting. The developed\nbenchmarks reveal shortcomings of the contemporary deep learning models, which\nwe partly address with Pathways of Normalized Group Convolution (PoNG) model, a\nnovel neural architecture for solving AVR tasks. PoNG excels in both presented\nchallenges, as well as the standard I-RAVEN and PGM setups.\n""]",Abstract Visual Reasoning and Intelligence Tests,Visual and Spatial Reasoning in Artificial Intelligence,Artificial Intelligence and Cognitive Systems
303,303,13,303_orbit_orbiting_satellites_spacecraft,"['orbit', 'orbiting', 'satellites', 'spacecraft', 'satellite', 'orbits', 'orbital', 'jupiter', 'prediction', 'asteroids']","['orbit', 'orbital', 'atmospheric', 'spacecraft', 'thrust', 'satellite', 'satellites', 'debris', 'propagation', 'transfers']","['  The Space Domain Awareness (SDA) community routinely tracks satellites in\norbit by fitting an orbital state to observations made by the Space\nSurveillance Network (SSN). In order to fit such orbits, an accurate model of\nthe forces that are acting on the satellite is required. Over the past several\ndecades, high-quality, physics-based models have been developed for satellite\nstate estimation and propagation. These models are exceedingly good at\nestimating and propagating orbital states for non-maneuvering satellites;\nhowever, there are several classes of anomalous accelerations that a satellite\nmight experience which are not well-modeled, such as satellites that use\nlow-thrust electric propulsion to modify their orbit. Physics-Informed Neural\nNetworks (PINNs) are a valuable tool for these classes of satellites as they\ncombine physics models with Deep Neural Networks (DNNs), which are highly\nexpressive and versatile function approximators. By combining a physics model\nwith a DNN, the machine learning model need not learn astrodynamics, which\nresults in more efficient and effective utilization of machine learning\nresources. This paper details the application of PINNs to estimate the orbital\nstate and a continuous, low-amplitude anomalous acceleration profile for\nsatellites. The PINN is trained to learn the unknown acceleration by minimizing\nthe mean square error of observations. We evaluate the performance of pure\nphysics models with PINNs in terms of their observation residuals and their\npropagation accuracy beyond the fit span of the observations. For a two-day\nsimulation of a GEO satellite using an unmodeled acceleration profile on the\norder of $10^{-8} \\text{ km/s}^2$, the PINN outperformed the best-fit physics\nmodel by orders of magnitude for both observation residuals (123 arcsec vs 1.00\narcsec) as well as propagation accuracy (3860 km vs 164 km after five days).\n', ""  The increasing volume of space objects in Earth's orbit presents a\nsignificant challenge for Space Situational Awareness (SSA). And in particular,\naccurate orbit prediction is crucial to anticipate the position and velocity of\nspace objects, for collision avoidance and space debris mitigation. When\nperforming Orbit Prediction (OP), it is necessary to consider the impact of\nnon-conservative forces, such as atmospheric drag and gravitational\nperturbations, that contribute to uncertainty around the future position of\nspacecraft and space debris alike. Conventional propagator methods like the\nSGP4 inadequately account for these forces, while numerical propagators are\nable to model the forces at a high computational cost. To address these\nlimitations, we propose an orbit prediction algorithm utilizing machine\nlearning. This algorithm forecasts state vectors on a spacecraft using past\npositions and environmental variables like atmospheric density from external\nsources. The orbital data used in the paper is gathered from precision\nephemeris data from the International Laser Ranging Service (ILRS), for the\nperiod of almost a year. We show how the use of machine learning and\ntime-series techniques can produce low positioning errors at a very low\ncomputational cost, thus significantly improving SSA capabilities by providing\nfaster and reliable orbit determination for an ever increasing number of space\nobjects.\n"", ""  The Simplified General Perturbations 4 (SGP4) orbital propagation method is\nwidely used for predicting the positions and velocities of Earth-orbiting\nobjects rapidly and reliably. Despite continuous refinement, SGP models still\nlack the precision of numerical propagators, which offer significantly smaller\nerrors. This study presents dSGP4, a novel differentiable version of SGP4\nimplemented using PyTorch. By making SGP4 differentiable, dSGP4 facilitates\nvarious space-related applications, including spacecraft orbit determination,\nstate conversion, covariance transformation, state transition matrix\ncomputation, and covariance propagation. Additionally, dSGP4's PyTorch\nimplementation allows for embarrassingly parallel orbital propagation across\nbatches of Two-Line Element Sets (TLEs), leveraging the computational power of\nCPUs, GPUs, and advanced hardware for distributed prediction of satellite\npositions at future times. Furthermore, dSGP4's differentiability enables\nintegration with modern machine learning techniques. Thus, we propose a novel\norbital propagation paradigm, ML-dSGP4, where neural networks are integrated\ninto the orbital propagator. Through stochastic gradient descent, this combined\nmodel's inputs, outputs, and parameters can be iteratively refined, surpassing\nSGP4's precision. Neural networks act as identity operators by default,\nadhering to SGP4's behavior. However, dSGP4's differentiability allows\nfine-tuning with ephemeris data, enhancing precision while maintaining\ncomputational speed. This empowers satellite operators and researchers to train\nthe model using specific ephemeris or high-precision numerical propagation\ndata, significantly advancing orbital prediction capabilities.\n""]",Orbit Prediction and Satellite Tracking,Satellite Data Analysis and Space Object Tracking,Remote Sensing and Space Data Analysis
304,304,12,304_automated_willingness_technology_tools,"['automated', 'willingness', 'technology', 'tools', 'collusion', 'detection', 'auditing', 'awareness', 'service', 'reviewers']","['acceptance', 'rings', 'collusion', 'leakage', 'colluders', 'reviewers', 'technology', 'expectancy', 'bidding', 'conference']","['  Public acceptance of conditionally automated vehicles is a crucial step in\nthe realization of smart cities. Prior research in Europe has shown that the\nfactors of hedonic motivation, social influence, and performance expectancy, in\ndecreasing order of importance, influence acceptance. Moreover, a generally\npositive acceptance of the technology was reported. However, there is a lack of\ninformation regarding the public acceptance of conditionally automated vehicles\nin the United States. In this study, we carried out a web-based experiment\nwhere participants were provided information regarding the technology and then\ncompleted a questionnaire on their perceptions. The collected data was analyzed\nusing PLS-SEM to examine the factors that may lead to public acceptance of the\ntechnology in the United States. Our findings showed that social influence,\nperformance expectancy, effort expectancy, hedonic motivation, and facilitating\nconditions determine conditionally automated vehicle acceptance. Additionally,\ncertain factors were found to influence the perception of how useful the\ntechnology is, the effort required to use it, and the facilitating conditions\nfor its use. By integrating the insights gained from this study, stakeholders\ncan better facilitate the adoption of autonomous vehicle technology,\ncontributing to safer, more efficient, and user-friendly transportation systems\nin the future that help realize the vision of the smart city.\n', '  A major threat to the peer-review systems of computer science conferences is\nthe existence of ""collusion rings"" between reviewers. In such collusion rings,\nreviewers who have also submitted their own papers to the conference work\ntogether to manipulate the conference\'s paper assignment, with the aim of being\nassigned to review each other\'s papers. The most straightforward way that\ncolluding reviewers can manipulate the paper assignment is by indicating their\ninterest in each other\'s papers through strategic paper bidding. One potential\napproach to solve this important problem would be to detect the colluding\nreviewers from their manipulated bids, after which the conference can take\nappropriate action. While prior work has developed effective techniques to\ndetect other kinds of fraud, no research has yet established that detecting\ncollusion rings is even possible. In this work, we tackle the question of\nwhether it is feasible to detect collusion rings from the paper bidding. To\nanswer this question, we conduct empirical analysis of two realistic conference\nbidding datasets, including evaluations of existing algorithms for fraud\ndetection in other applications. We find that collusion rings can achieve\nconsiderable success at manipulating the paper assignment while remaining\nhidden from detection: for example, in one dataset, undetected colluders are\nable to achieve assignment to up to 30% of the papers authored by other\ncolluders. In addition, when 10 colluders bid on all of each other\'s papers, no\ndetection algorithm outputs a group of reviewers with more than 31% overlap\nwith the true colluders. These results suggest that collusion cannot be\neffectively detected from the bidding using popular existing tools,\ndemonstrating the need to develop more complex detection algorithms as well as\nthose that leverage additional metadata (e.g., reviewer-paper text-similarity\nscores).\n', ""  The detrimental effects of air pollutants on human health have prompted\nincreasing concerns regarding indoor air quality (IAQ). The emergence of\ndigital health interventions and citizen science initiatives has provided new\navenues for raising awareness, improving IAQ, and promoting behavioural\nchanges. The Technology Acceptance Model (TAM) offers a theoretical framework\nto understand user acceptance and adoption of IAQ technology. This paper\npresents a case study using the COM-B model and Internet of Things (IoT)\ntechnology to design a human-centred digital visualisation platform, leading to\nbehavioural changes and improved IAQ. The study also investigates users'\nacceptance and adoption of the technology, focusing on their experiences,\nexpectations, and the impact on IAQ. Integrating IAQ sensing, digital\nhealth-related interventions, citizen science, and the TAM model offers\nopportunities to address IAQ challenges, enhance public health, and foster\nsustainable indoor environments. The analytical results show that factors such\nas human behaviour, indoor activities, and awareness play crucial roles in\nshaping IAQ.\n""]",Technology Acceptance and Adoption in Various Domains,Technology Adoption and Acceptance in Various Domains,Human Interaction with Emerging Technologies
305,305,11,305_analogical_analogies_analogy_abstraction,"['analogical', 'analogies', 'analogy', 'abstraction', 'algebras', 'proportions', 'similarity', 'proportion', 'logic', 'reasoning']","['analogical', 'proportions', 'unification', 'analogy', 'algebra', 'algebraic', 'mathematical', 'notion', 'proportion', 'universal']","['  The author has recently introduced abstract algebraic frameworks of\nanalogical proportions and similarity within the general setting of universal\nalgebra. The purpose of this paper is to build a bridge from similarity to\nanalogical proportions by formulating the latter in terms of the former. The\nbenefit of this similarity-based approach is that the connection between\nproportions and similarity is built into the framework and therefore evident\nwhich is appealing since proportions and similarity are both at the center of\nanalogy; moreover, future results on similarity can directly be applied to\nanalogical proportions.\n', ""  Analogical reasoning is the ability to detect parallels between two seemingly\ndistant objects or situations, a fundamental human capacity used for example in\ncommonsense reasoning, learning, and creativity which is believed by many\nresearchers to be at the core of human and artificial general intelligence.\nAnalogical proportions are expressions of the form ``$a$ is to $b$ what $c$ is\nto $d$'' at the core of analogical reasoning. The author has recently\nintroduced an abstract algebraic framework of analogical proportions within the\ngeneral setting of universal algebra. It is the purpose of this paper to\nfurther develop the mathematical theory of analogical proportions within that\nframework as motivated by the fact that it has already been successfully\napplied to logic program synthesis in artificial intelligence.\n"", ""  Analogy-making is at the core of human and artificial intelligence and\ncreativity with applications to such diverse tasks as proving mathematical\ntheorems and building mathematical theories, common sense reasoning, learning,\nlanguage acquisition, and story telling. This paper introduces from first\nprinciples an abstract algebraic framework of analogical proportions of the\nform `$a$ is to $b$ what $c$ is to $d$' in the general setting of universal\nalgebra. This enables us to compare mathematical objects possibly across\ndifferent domains in a uniform way which is crucial for AI-systems. It turns\nout that our notion of analogical proportions has appealing mathematical\nproperties. As we construct our model from first principles using only\nelementary concepts of universal algebra, and since our model questions some\nbasic properties of analogical proportions presupposed in the literature, to\nconvince the reader of the plausibility of our model we show that it can be\nnaturally embedded into first-order logic via model-theoretic types and prove\nfrom that perspective that analogical proportions are compatible with\nstructure-preserving mappings. This provides conceptual evidence for its\napplicability. In a broader sense, this paper is a first step towards a theory\nof analogical reasoning and learning systems with potential applications to\nfundamental AI-problems like common sense reasoning and computational learning\nand creativity.\n""]",Analogical Reasoning and Proportions in Algebra,Analogical Reasoning and Knowledge Representation,Artificial Intelligence and Cognitive Systems
306,306,11,306_clustering_cluster_clusters_organizing,"['clustering', 'cluster', 'clusters', 'organizing', 'dataset', 'map', 'maps', 'visualization', 'soms', 'unsupervised']","['clusters', 'cluster', 'flight', 'phenotypic', 'cell', 'imbalanced', 'biological', 'streaming', 'clustering', 'maps']","['  Streaming data clustering is a popular research topic in the fields of data\nmining and machine learning. Compared to static data, streaming data, which is\nusually analyzed in data chunks, is more susceptible to encountering the\ndynamic cluster imbalanced issue. That is, the imbalanced degree of clusters\nvaries in different streaming data chunks, leading to corruption in either the\naccuracy or the efficiency of streaming data analysis based on existing\nclustering methods. Therefore, we propose an efficient approach called Learning\nSelf-Refined Organizing Map (LSROM) to handle the imbalanced streaming data\nclustering problem, where we propose an advanced SOM for representing the\nglobal data distribution. The constructed SOM is first refined for guiding the\npartition of the dataset to form many micro-clusters to avoid the missing small\nclusters in imbalanced data. Then an efficient merging of the micro-clusters is\nconducted through quick retrieval based on the SOM, which can automatically\nyield a true number of imbalanced clusters. In comparison to existing\nimbalanced data clustering approaches, LSROM is with a lower time complexity\n$O(n\\log n)$, while achieving very competitive clustering accuracy. Moreover,\nLSROM is interpretable and insensitive to hyper-parameters. Extensive\nexperiments have verified its efficacy.\n', '  High-dimensional single-cell data poses significant challenges in identifying\nunderlying biological patterns due to the complexity and heterogeneity of\ncellular states. We propose a comprehensive gene-cell dependency visualization\nvia unsupervised clustering, Growing Hierarchical Self-Organizing Map (GHSOM),\nspecifically designed for analyzing high-dimensional single-cell data like\nsingle-cell sequencing and CRISPR screens. GHSOM is applied to cluster samples\nin a hierarchical structure such that the self-growth structure of clusters\nsatisfies the required variations between and within. We propose a novel\nSignificant Attributes Identification Algorithm to identify features that\ndistinguish clusters. This algorithm pinpoints attributes with minimal\nvariation within a cluster but substantial variation between clusters. These\nkey attributes can then be used for targeted data retrieval and downstream\nanalysis. Furthermore, we present two innovative visualization tools: Cluster\nFeature Map and Cluster Distribution Map. The Cluster Feature Map highlights\nthe distribution of specific features across the hierarchical structure of\nGHSOM clusters. This allows for rapid visual assessment of cluster uniqueness\nbased on chosen features. The Cluster Distribution Map depicts leaf clusters as\ncircles on the GHSOM grid, with circle size reflecting cluster data size and\ncolor customizable to visualize features like cell type or other attributes. We\napply our analysis to three single-cell datasets and one CRISPR dataset\n(cell-gene database) and evaluate clustering methods with internal and external\nCH and ARI scores. GHSOM performs well, being the best performer in internal\nevaluation (CH=4.2). In external evaluation, GHSOM has the third-best\nperformance of all methods.\n', '  Motivation: Unraveling the connection between genes and traits is crucial for\nsolving many biological puzzles. Genes provide instructions for building\ncellular machinery, directing the processes that sustain life. RNA molecules\nand proteins, derived from these genetic instructions, play crucial roles in\nshaping cell structures, influencing reactions, and guiding behavior. This\nfundamental biological principle links genetic makeup to observable traits, but\nintegrating and extracting meaningful relationships from this complex,\nmultimodal data presents a significant challenge. Results: We introduce\nevolSOM, a novel R package that utilizes Self-Organizing Maps (SOMs) to explore\nand visualize the conservation of biological variables, easing the integration\nof phenotypic and genotypic attributes. By constructing species-specific or\ncondition-specific SOMs that capture non-redundant patterns, evolSOM allows the\nanalysis of displacement of biological variables between species or conditions.\nVariables displaced together suggest membership in the same regulatory network,\nand the nature of the displacement may hold biological significance. The\npackage automatically calculates and graphically presents these displacements,\nenabling efficient comparison and revealing conserved and displaced variables.\nThe package facilitates the integration of diverse phenotypic data types,\nenabling the exploration of potential gene drivers underlying observed\nphenotypic changes. Its user-friendly interface and visualization capabilities\nenhance the accessibility of complex network analyses. Illustratively, we\nemployed evolSOM to study the displacement of genes and phenotypic traits,\nsuccessfully identifying potential drivers of phenotypic differentiation in\ngrass leaves. Availability: The package is open-source and is available at\nhttps://github.com/sanprochetto/evolSOM.\n']",Clustering and Visualization with Self-Organizing Maps,Clustering Techniques and Algorithms,Data Analysis and Pattern Discovery
307,307,11,307_multimodal_mllm_mllms_textual,"['multimodal', 'mllm', 'mllms', 'textual', 'detection', 'toolkit', 'benchmarking', 'modal', 'empirically', 'lvlm']","['misinformation', 'multimodal', 'oversensitivity', 'failures', 'harmless', 'stimuli', 'fact', 'checking', 'trilemma', 'queries']","[""  Multimodal large language models (MLLMs) (e.g., GPT-4V, LLaVA, and Claude-3)\nhave broadened the scope of AI applications. Yet, evaluating their performance\npresents a significant challenge owing to the inherently subjective nature of\ntasks that do not yield clear-cut solutions especially for those open-ended\nqueries. Existing automatic evaluation methodologies are mainly limited in\nevaluating objective queries without considering real-world user experiences,\ninadequately addressing the nuances of creative and associative multimodal\ntasks. In our paper, we propose a new evaluation paradigm for MLLMs, which is\nevaluating MLLMs with \\textit{per-sample criteria} using potent MLLM as the\njudge. To validate the feasibility and effectiveness of this paradigm, we\ndesign a benchmark, dubbed \\textit{MLLM-Bench}, with the evaluation samples\nacross six critical levels following the revised Bloom's Taxonomy with the\nethical consideration. We benchmark 21 popular MLLMs in a pairwise-comparison\nfashion, showing diverse performance across models. Moreover, the validity of\nour benchmark manifests itself in reaching 88.02\\% agreement with human\nevaluation. We contend that the proposed paradigm explores the potential of\nMLLMs as effective evaluation tools with the help of per-sample criteria, and\nthat MLLM-Bench will serve as a catalyst for encouraging the development of\nuser-centric MLLMs tailored to real-world applications. Our benchmark data,\nonline leaderboard and submission entry are at https://mllm-bench.llmzoo.com.\n"", '  Nowadays, misinformation is widely spreading over various social media\nplatforms and causes extremely negative impacts on society. To combat this\nissue, automatically identifying misinformation, especially those containing\nmultimodal content, has attracted growing attention from the academic and\nindustrial communities, and induced an active research topic named Multimodal\nMisinformation Detection (MMD). Typically, existing MMD methods capture the\nsemantic correlation and inconsistency between multiple modalities, but neglect\nsome potential clues in multimodal content. Recent studies suggest that\nmanipulated traces of the images in articles are non-trivial clues for\ndetecting misinformation. Meanwhile, we find that the underlying intentions\nbehind the manipulation, e.g., harmful and harmless, also matter in MMD.\nAccordingly, in this work, we propose to detect misinformation by learning\nmanipulation features that indicate whether the image has been manipulated, as\nwell as intention features regarding the harmful and harmless intentions of the\nmanipulation. Unfortunately, the manipulation and intention labels that make\nthese features discriminative are unknown. To overcome the problem, we propose\ntwo weakly supervised signals as alternatives by introducing additional\ndatasets on image manipulation detection and formulating two classification\ntasks as positive and unlabeled learning problems. Based on these ideas, we\npropose a novel MMD method, namely Harmfully Manipulated Images Matter in MMD\n(HAMI-M3D). Extensive experiments across three benchmark datasets can\ndemonstrate that HAMI-M3D can consistently improve the performance of any MMD\nbaselines.\n', ""  Humans are prone to cognitive distortions -- biased thinking patterns that\nlead to exaggerated responses to specific stimuli, albeit in very different\ncontexts. This paper demonstrates that advanced Multimodal Large Language\nModels (MLLMs) exhibit similar tendencies. While these models are designed to\nrespond queries under safety mechanism, they sometimes reject harmless queries\nin the presence of certain visual stimuli, disregarding the benign nature of\ntheir contexts. As the initial step in investigating this behavior, we identify\nthree types of stimuli that trigger the oversensitivity of existing MLLMs:\nExaggerated Risk, Negated Harm, and Counterintuitive Interpretation. To\nsystematically evaluate MLLMs' oversensitivity to these stimuli, we propose the\nMultimodal OverSenSitivity Benchmark (MOSSBench). This toolkit consists of 300\nmanually collected benign multimodal queries, cross-verified by third-party\nreviewers (AMT). Empirical studies using MOSSBench on 20 MLLMs reveal several\ninsights: (1). Oversensitivity is prevalent among SOTA MLLMs, with refusal\nrates reaching up to 76% for harmless queries. (2). Safer models are more\noversensitive: increasing safety may inadvertently raise caution and\nconservatism in the model's responses. (3). Different types of stimuli tend to\ncause errors at specific stages -- perception, intent reasoning, and safety\njudgement -- in the response process of MLLMs. These findings highlight the\nneed for refined safety mechanisms that balance caution with contextually\nappropriate responses, improving the reliability of MLLMs in real-world\napplications. We make our project available at\nhttps://turningpoint-ai.github.io/MOSSBench/.\n""]",Multimodal Large Language Models (MLLMs) Evaluation,Multimodal Large Language Models (MLLMs),Multimodal Learning and Vision-Language Models
308,308,10,308_neural_neuron_hippocampal_hippocampus,"['neural', 'neuron', 'hippocampal', 'hippocampus', 'autoencoders', 'neuroscience', 'neocortical', 'memory', 'synaptic', 'brain']","['sleep', 'wake', 'disentangling', 'neuroscientific', 'grid', 'representations', 'hippocampal', 'brain', 'neuroscience', 'infomorphic']","['  Recent years have witnessed a growing call for renewed emphasis on\nneuroscience-inspired approaches in artificial intelligence research, under the\nbanner of NeuroAI. A prime example of this is predictive coding networks\n(PCNs), based on the neuroscientific framework of predictive coding. This\nframework views the brain as a hierarchical Bayesian inference model that\nminimizes prediction errors through feedback connections. Unlike traditional\nneural networks trained with backpropagation (BP), PCNs utilize inference\nlearning (IL), a more biologically plausible algorithm that explains patterns\nof neural activity that BP cannot. Historically, IL has been more\ncomputationally intensive, but recent advancements have demonstrated that it\ncan achieve higher efficiency than BP with sufficient parallelization.\nFurthermore, PCNs can be mathematically considered a superset of traditional\nfeedforward neural networks (FNNs), significantly extending the range of\ntrainable architectures. As inherently probabilistic (graphical) latent\nvariable models, PCNs provide a versatile framework for both supervised\nlearning and unsupervised (generative) modeling that goes beyond traditional\nartificial neural networks. This work provides a comprehensive review and\ndetailed formal specification of PCNs, particularly situating them within the\ncontext of modern ML methods. Additionally, we introduce a Python library\n(PRECO) for practical implementation. This positions PC as a promising\nframework for future ML innovations.\n', ""  We propose Wake-Sleep Consolidated Learning (WSCL), a learning strategy\nleveraging Complementary Learning System theory and the wake-sleep phases of\nthe human brain to improve the performance of deep neural networks for visual\nclassification tasks in continual learning settings. Our method learns\ncontinually via the synchronization between distinct wake and sleep phases.\nDuring the wake phase, the model is exposed to sensory input and adapts its\nrepresentations, ensuring stability through a dynamic parameter freezing\nmechanism and storing episodic memories in a short-term temporary memory\n(similarly to what happens in the hippocampus). During the sleep phase, the\ntraining process is split into NREM and REM stages. In the NREM stage, the\nmodel's synaptic weights are consolidated using replayed samples from the\nshort-term and long-term memory and the synaptic plasticity mechanism is\nactivated, strengthening important connections and weakening unimportant ones.\nIn the REM stage, the model is exposed to previously-unseen realistic visual\nsensory experience, and the dreaming process is activated, which enables the\nmodel to explore the potential feature space, thus preparing synapses to future\nknowledge. We evaluate the effectiveness of our approach on three benchmark\ndatasets: CIFAR-10, Tiny-ImageNet and FG-ImageNet. In all cases, our method\noutperforms the baselines and prior work, yielding a significant performance\ngain on continual visual classification tasks. Furthermore, we demonstrate the\nusefulness of all processing stages and the importance of dreaming to enable\npositive forward transfer.\n"", '  Why do mammals need to sleep? Neuroscience treats sleep and wake as default\nand perturbation modes of the brain. It is hypothesized that the brain\nself-organizes neural activities without environmental inputs. This paper\npresents a new computational model of the sleep-wake cycle (SWC) for learning\nand memory. During the sleep mode, the memory consolidation by the\nthalamocortical system is abstracted by a disentangling operator that maps\ncontext-dependent representations (CDR) to context-independent representations\n(CIR) for generalization. Such a disentangling operator can be mathematically\nformalized by an integral transform that integrates the context variable from\nCDR. During the wake mode, the memory formation by the hippocampal-neocortical\nsystem is abstracted by an entangling operator from CIR to CDR where the\ncontext is introduced by physical motion. When designed as inductive bias,\nentangled CDR linearizes the problem of unsupervised learning for sensory\nmemory by direct-fit. The concatenation of disentangling and entangling\noperators forms a disentangling-entangling cycle (DEC) as the building block\nfor sensorimotor learning. We also discuss the relationship of DEC and SWC to\nthe perception-action cycle (PAC) for internal model learning and perceptual\ncontrol theory for the ecological origin of natural languages.\n']",Neural Networks and Neuroscience-Inspired Learning,Deep Learning Theory and Foundations,Machine Learning and Artificial Intelligence
309,309,8,309_uplift_coupon_coupons_promotion,"['uplift', 'coupon', 'coupons', 'promotion', 'roi', 'modeling', 'discounts', 'sales', 'revenue', 'marketing']","['uplift', 'treatment', 'marketing', 'sales', 'providers', 'coupons', 'discounts', 'modeling', 'revenue', 'platform']","[""  Uplift modeling is a technique used to predict the effect of a treatment\n(e.g., discounts) on an individual's response. Although several methods have\nbeen proposed for multi-valued treatment, they are extended from binary\ntreatment methods. There are still some limitations. Firstly, existing methods\ncalculate uplift based on predicted responses, which may not guarantee a\nconsistent uplift distribution between treatment and control groups. Moreover,\nthis may cause cumulative errors for multi-valued treatment. Secondly, the\nmodel parameters become numerous with many prediction heads, leading to reduced\nefficiency. To address these issues, we propose a novel \\underline{M}ulti-gate\n\\underline{M}ixture-of-Experts based \\underline{M}ulti-valued\n\\underline{T}reatment \\underline{N}etwork (M$^3$TN). M$^3$TN consists of two\ncomponents: 1) a feature representation module with Multi-gate\nMixture-of-Experts to improve the efficiency; 2) a reparameterization module by\nmodeling uplift explicitly to improve the effectiveness. We also conduct\nextensive experiments to demonstrate the effectiveness and efficiency of our\nM$^3$TN.\n"", '  Uplift modeling, vital in online marketing, seeks to accurately measure the\nimpact of various strategies, such as coupons or discounts, on different users\nby predicting the Individual Treatment Effect (ITE). In an e-commerce setting,\nuser behavior follows a defined sequential chain, including impression, click,\nand conversion. Marketing strategies exert varied uplift effects at each stage\nwithin this chain, impacting metrics like click-through and conversion rate.\nDespite its utility, existing research has neglected to consider the inter-task\nacross all stages impacts within a specific treatment and has insufficiently\nutilized the treatment information, potentially introducing substantial bias\ninto subsequent marketing decisions. We identify these two issues as the\nchain-bias problem and the treatment-unadaptive problem. This paper introduces\nthe Entire Chain UPlift method with context-enhanced learning (ECUP), devised\nto tackle these issues. ECUP consists of two primary components: 1) the Entire\nChain-Enhanced Network, which utilizes user behavior patterns to estimate ITE\nthroughout the entire chain space, models the various impacts of treatments on\neach task, and integrates task prior information to enhance context awareness\nacross all stages, capturing the impact of treatment on different tasks, and 2)\nthe Treatment-Enhanced Network, which facilitates fine-grained treatment\nmodeling through bit-level feature interactions, thereby enabling adaptive\nfeature adjustment. Extensive experiments on public and industrial datasets\nvalidate ECUPs effectiveness. Moreover, ECUP has been deployed on the Meituan\nfood delivery platform, serving millions of daily active users, with the\nrelated dataset released for future research.\n', '  Uplift modeling has been widely employed in online marketing by predicting\nthe response difference between the treatment and control groups, so as to\nidentify the sensitive individuals toward interventions like coupons or\ndiscounts. Compared with traditional \\textit{conversion uplift modeling},\n\\textit{revenue uplift modeling} exhibits higher potential due to its direct\nconnection with the corporate income. However, previous works can hardly handle\nthe continuous long-tail response distribution in revenue uplift modeling.\nMoreover, they have neglected to optimize the uplift ranking among different\nindividuals, which is actually the core of uplift modeling. To address such\nissues, in this paper, we first utilize the zero-inflated lognormal (ZILN) loss\nto regress the responses and customize the corresponding modeling network,\nwhich can be adapted to different existing uplift models. Then, we study the\nranking-related uplift modeling error from the theoretical perspective and\npropose two tighter error bounds as the additional loss terms to the\nconventional response regression loss. Finally, we directly model the uplift\nranking error for the entire population with a listwise uplift ranking loss.\nThe experiment results on offline public and industrial datasets validate the\neffectiveness of our method for revenue uplift modeling. Furthermore, we\nconduct large-scale experiments on a prominent online fintech marketing\nplatform, Tencent FiT, which further demonstrates the superiority of our method\nin real-world applications.\n']",Uplift Modeling for Marketing Strategies,Business and Marketing Analytics,Business and Marketing Analytics
310,310,7,310_neural_shape_3d_shapes,"['neural', 'shape', '3d', 'shapes', 'deep', 'sensing', 'mechanics', 'rigid', 'dynamics', 'linearization']","['body', 'origami', 'capacitive', 'fiber', 'capacitors', 'eccentric', 'rigid', 'conductive', 'shape', 'sensors']","['  In many real-world settings, image observations of freely rotating 3D rigid\nbodies may be available when low-dimensional measurements are not. However, the\nhigh-dimensionality of image data precludes the use of classical estimation\ntechniques to learn the dynamics. The usefulness of standard deep learning\nmethods is also limited, because an image of a rigid body reveals nothing about\nthe distribution of mass inside the body, which, together with initial angular\nvelocity, is what determines how the body will rotate. We present a\nphysics-based neural network model to estimate and predict 3D rotational\ndynamics from image sequences. We achieve this using a multi-stage prediction\npipeline that maps individual images to a latent representation homeomorphic to\n$\\mathbf{SO}(3)$, computes angular velocities from latent pairs, and predicts\nfuture latent states using the Hamiltonian equations of motion. We demonstrate\nthe efficacy of our approach on new rotating rigid-body datasets of sequences\nof synthetic images of rotating objects, including cubes, prisms and\nsatellites, with unknown uniform and non-uniform mass distributions. Our model\noutperforms competing baselines on our datasets, producing better qualitative\npredictions and reducing the error observed for the state-of-the-art\nHamiltonian Generative Network by a factor of 2.\n', ""  Fiber optic shape sensors have enabled unique advances in various navigation\ntasks, from medical tool tracking to industrial applications. Eccentric fiber\nBragg gratings (FBG) are cheap and easy-to-fabricate shape sensors that are\noften interrogated with simple setups. However, using low-cost interrogation\nsystems for such intensity-based quasi-distributed sensors introduces further\ncomplications to the sensor's signal. Therefore, eccentric FBGs have not been\nable to accurately estimate complex multi-bend shapes. Here, we present a novel\ntechnique to overcome these limitations and provide accurate and precise shape\nestimation in eccentric FBG sensors. We investigate the most important\nbending-induced effects in curved optical fibers that are usually eliminated in\nintensity-based fiber sensors. These effects contain shape deformation\ninformation with a higher spatial resolution that we are now able to extract\nusing deep learning techniques. We design a deep learning model based on a\nconvolutional neural network that is trained to predict shapes given the\nsensor's spectra. We also provide a visual explanation, highlighting wavelength\nelements whose intensities are more relevant in making shape predictions. These\nfindings imply that deep learning techniques benefit from the bending-induced\neffects that impact the desired signal in a complex manner. This is the first\nstep toward cheap yet accurate fiber shape sensing solutions.\n"", '  In this work, we propose a novel single-end morphing capacitive sensing\nmethod for shape tracking, FxC, by combining Folding origami structures and\nCapacitive sensing to detect the morphing structural motions using\nstate-of-the-art sensing circuits and deep learning. It was observed through\nembedding areas of origami structures with conductive materials as single-end\ncapacitive sensing patches, that the sensor signals change coherently with the\nmotion of the structure. Different from other origami capacitors where the\norigami structures are used in adjusting the thickness of the dielectric layer\nof double-plate capacitors, FxC uses only a single conductive plate per\nchannel, and the origami structure directly changes the geometry of the\nconductive plate. We examined the operation principle of morphing single-end\ncapacitors through 3D geometry simulation combined with physics theoretical\ndeduction, which deduced similar behaviour as observed in experimentation. Then\na software pipeline was developed to use the sensor signals to reconstruct the\ndynamic structural geometry through data-driven deep neural network regression\nof geometric primitives extracted from vision tracking. We created multiple\nfolding patterns to validate our approach, based on folding patterns including\nAccordion, Chevron, Sunray and V-Fold patterns with different layouts of\ncapacitive sensors using paper-based and textile-based materials.\nExperimentation results show that the geometry primitives predicted from the\ncapacitive signals have a strong correlation with the visual ground truth with\nR-squared value of up to 95% and tracking error of 6.5 mm for patches. The\nsimulation and machine learning constitute two-way information exchange between\nthe sensing signals and structural geometry.\n']",Physics-based Deep Learning for 3D Shape Sensing and Dynamics,Deep Learning for Geophysical and Structural Analysis,Deep Learning Applications in Engineering and Computer Vision
311,311,6,311_expertise_explanations_automated_experts,"['expertise', 'explanations', 'automated', 'experts', 'understandability', 'expert', 'prediction', 'information', 'interactive', 'predictions']","['explanations', 'centric', 'experts', 'understandability', 'affected', 'healthcare', 'forecasts', 'trust', 'forecasting', 'configuration']","['  Explanations of AI systems rarely address the information needs of people\naffected by algorithmic decision-making (ADM). This gap between conveyed\ninformation and information that matters to affected stakeholders can impede\nunderstanding and adherence to regulatory frameworks such as the AI Act. To\naddress this gap, we present the ""XAI Novice Question Bank"": A catalog of\naffected stakeholders\' information needs in two ADM use cases (employment\nprediction and health monitoring), covering the categories data, system\ncontext, system usage, and system specifications. Information needs were\ngathered in an interview study where participants received explanations in\nresponse to their inquiries. Participants further reported their understanding\nand decision confidence, showing that while confidence tended to increase after\nreceiving explanations, participants also met understanding challenges, such as\nbeing unable to tell why their understanding felt incomplete. Explanations\nfurther influenced participants\' perceptions of the systems\' risks and\nbenefits, which they confirmed or changed depending on the use case. When risks\nwere perceived as high, participants expressed particular interest in\nexplanations about intention, such as why and to what end a system was put in\nplace. With this work, we aim to support the inclusion of affected stakeholders\ninto explainability by contributing an overview of information and challenges\nrelevant to them when deciding on the adoption of ADM systems. We close by\nsummarizing our findings in a list of six key implications that inform the\ndesign of future explanations for affected stakeholder audiences.\n', '  In the realm of interactive machine-learning systems, the provision of\nexplanations serves as a vital aid in the processes of debugging and enhancing\nprediction models. However, the extent to which various global model-centric\nand data-centric explanations can effectively assist domain experts in\ndetecting and resolving potential data-related issues for the purpose of model\nimprovement has remained largely unexplored. In this technical report, we\nsummarise the key findings of our two user studies. Our research involved a\ncomprehensive examination of the impact of global explanations rooted in both\ndata-centric and model-centric perspectives within systems designed to support\nhealthcare experts in optimising machine learning models through both automated\nand manual data configurations. To empirically investigate these dynamics, we\nconducted two user studies, comprising quantitative analysis involving a sample\nsize of 70 healthcare experts and qualitative assessments involving 30\nhealthcare experts. These studies were aimed at illuminating the influence of\ndifferent explanation types on three key dimensions: trust, understandability,\nand model improvement. Results show that global model-centric explanations\nalone are insufficient for effectively guiding users during the intricate\nprocess of data configuration. In contrast, data-centric explanations exhibited\ntheir potential by enhancing the understanding of system changes that occur\npost-configuration. However, a combination of both showed the highest level of\nefficacy for fostering trust, improving understandability, and facilitating\nmodel enhancement among healthcare experts. We also present essential\nimplications for developing interactive machine-learning systems driven by\nexplanations. These insights can guide the creation of more effective systems\nthat empower domain experts to harness the full potential of machine learning\n', '  Explanations in interactive machine-learning systems facilitate debugging and\nimproving prediction models. However, the effectiveness of various global\nmodel-centric and data-centric explanations in aiding domain experts to detect\nand resolve potential data issues for model improvement remains unexplored.\nThis research investigates the influence of data-centric and model-centric\nglobal explanations in systems that support healthcare experts in optimising\nmodels through automated and manual data configurations. We conducted\nquantitative (n=70) and qualitative (n=30) studies with healthcare experts to\nexplore the impact of different explanations on trust, understandability and\nmodel improvement. Our results reveal the insufficiency of global model-centric\nexplanations for guiding users during data configuration. Although data-centric\nexplanations enhanced understanding of post-configuration system changes, a\nhybrid fusion of both explanation types demonstrated the highest effectiveness.\nBased on our study results, we also present design implications for effective\nexplanation-driven interactive machine-learning systems.\n']",Explainability in AI Decision-Making Systems,Explainable Artificial Intelligence (XAI),Artificial Intelligence and Machine Learning Interpretability and Explainability
312,312,6,312_bridge_autoencoder_generative_keras,"['bridge', 'autoencoder', 'generative', 'keras', 'tensorflow', 'pixelcnn', 'span', 'dataset', 'variational', 'arch']","['bridge', 'suspension', 'arch', 'cable', 'types', 'span', 'beam', 'type', 'transformation', 'technology']","['  Try to generate new bridge types using generative artificial intelligence\ntechnology. The grayscale images of the bridge facade with the change of\ncomponent width was rendered by 3dsMax animation software, and then the OpenCV\nmodule performed an appropriate amount of geometric transformation (rotation,\nhorizontal scale, vertical scale) to obtain the image dataset of three-span\nbeam bridge, arch bridge, cable-stayed bridge and suspension bridge. Based on\nPython programming language, TensorFlow and Keras deep learning platform\nframework, variational autoencoder was constructed and trained, and\nlow-dimensional bridge-type latent space that is convenient for vector\noperations was obtained. Variational autoencoder can combine two bridge types\non the basis of the original of human into one that is a new bridge type.\nGenerative artificial intelligence technology can assist bridge designers in\nbridge-type innovation, and can be used as copilot.\n', '  Try to generate new bridge types using generative artificial intelligence\ntechnology. Symmetric structured image dataset of three-span beam bridge, arch\nbridge, cable-stayed bridge and suspension bridge are used . Based on Python\nprogramming language, TensorFlow and Keras deep learning platform framework ,\nas well as Wasserstein loss function and Lipschitz constraints, generative\nadversarial network is constructed and trained. From the obtained low\ndimensional bridge-type latent space sampling, new bridge types with asymmetric\nstructures can be generated. Generative adversarial network can create new\nbridge types by organically combining different structural components on the\nbasis of human original bridge types. It has a certain degree of human original\nability. Generative artificial intelligence technology can open up imagination\nspace and inspire humanity.\n', '  Try to generate new bridge types using generative artificial intelligence\ntechnology. Using symmetric structured image dataset of three-span beam bridge,\narch bridge, cable-stayed bridge and suspension bridge , based on Python\nprogramming language, TensorFlow and Keras deep learning platform framework ,\nPixelCNN is constructed and trained. The model can capture the statistical\nstructure of the images and calculate the probability distribution of the next\npixel when the previous pixels are given. From the obtained latent space\nsampling, new bridge types different from the training dataset can be\ngenerated. PixelCNN can organically combine different structural components on\nthe basis of human original bridge types, creating new bridge types that have a\ncertain degree of human original ability. Autoregressive models cannot\nunderstand the meaning of the sequence, while multimodal models combine\nregression and autoregressive models to understand the sequence. Multimodal\nmodels should be the way to achieve artificial general intelligence in the\nfuture.\n']",Generative Bridge Design with AI,Generative AI Applications and Implications,Generative Modeling and Artificial Intelligence
