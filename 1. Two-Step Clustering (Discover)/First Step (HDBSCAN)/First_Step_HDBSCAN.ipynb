{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8718d882-855b-40e6-bbfe-a65b0816f685",
   "metadata": {},
   "source": [
    "### Python Code Overview\n",
    "\n",
    "This script performs topic modeling on a dataset of text using the `BERTopic` library with several steps:\n",
    "\n",
    "1. **Import Required Libraries**:\n",
    "    - Imports various libraries for data handling (`pandas`, `pickle`), text embedding (`SentenceTransformer`), dimensionality reduction (`UMAP`), clustering (`HDBSCAN`), and topic modeling (`BERTopic`).\n",
    "\n",
    "2. **Load Data**:\n",
    "    - `load_data(file_path)`: Loads a tab-separated text file into a pandas DataFrame with columns `[\"text\", \"dataset-part\", \"ground truth\"]`.\n",
    "\n",
    "3. **Configure Topic Model**:\n",
    "    - `configure_topic_model()`: Sets up a BERTopic model with specific configurations:\n",
    "        - Uses `SentenceTransformer` for embeddings.\n",
    "        - Reduces dimensionality with `UMAP`.\n",
    "        - Clusters data with `HDBSCAN`.\n",
    "        - Tokenizes text with `CountVectorizer`.\n",
    "        - Extracts topics with `ClassTfidfTransformer`.\n",
    "        - Adds optional aspect-based representations.\n",
    "\n",
    "4. **Fit, Transform, and Save**:\n",
    "    - `fit_transform_and_save_all(df, topic_model, model_filename, dataset_file_name, topic_info_file_name)`:\n",
    "        - Fits the BERTopic model to the text data, saves the model and topic information.\n",
    "        - Updates the DataFrame with topic labels and names.\n",
    "        - Saves the transformed DataFrame to a CSV file.\n",
    "\n",
    "5. **Save Outliers**:\n",
    "    - `save_outliers(df, file_name)`: Identifies and saves rows labeled as outliers (i.e., with topic label `-1`) to a separate CSV file.\n",
    "\n",
    "6. **Usage Example**:\n",
    "    - The code at the end demonstrates how to load data, configure the topic model, fit and save the results, and finally, save any outliers.\n",
    "\n",
    "### Summary\n",
    "The script is designed for topic modeling on a text corpus, providing a complete pipeline from data loading to model fitting, topic extraction, and saving results, including handling of outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c173354-7cd1-4bb1-bb45-fb5e8d53f0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully after fitting at: BERTopic_first_step.pkl\n",
      "Topic info saved successfully at: BERTopic_topic_info.csv\n",
      "Dataset with topics saved successfully at: BERTopic_result.csv\n",
      "Outliers saved successfully at: First_step_outliers.csv\n"
     ]
    }
   ],
   "source": [
    "# For TSV files\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired, OpenAI, PartOfSpeech, MaximalMarginalRelevance\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "    df.columns = [\"text\", \"dataset-part\", \"ground truth\"]\n",
    "    return df.copy()\n",
    "\n",
    "def configure_topic_model():\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "    ctfidf_model = ClassTfidfTransformer()\n",
    "#     Add different aspects\n",
    "    main_representation = KeyBERTInspired()\n",
    "    aspect_model1 = PartOfSpeech(\"en_core_web_sm\")\n",
    "    representation_model = {\n",
    "    \"Main\": main_representation,\n",
    "    \"Aspect1\": aspect_model1,\n",
    "    }\n",
    "    return BERTopic(\n",
    "      embedding_model=embedding_model,          # Step 1 - Extract embeddings\n",
    "      umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n",
    "      hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n",
    "      vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n",
    "      ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n",
    "      representation_model=representation_model # Step 6 - (Optional) Fine-tune topic represenations\n",
    "    )\n",
    "\n",
    "def fit_transform_and_save_all(df, topic_model, model_filename, dataset_file_name, topic_info_file_name):\n",
    "    # Fit the model with your text data\n",
    "    try:\n",
    "        topics, _ = topic_model.fit_transform(df['text'])\n",
    "    except Exception as e:\n",
    "        print(\"Error during model fitting:\", str(e))\n",
    "        return\n",
    "    \n",
    "    # Save the model after fitting\n",
    "    with open(model_filename, \"wb\") as file:\n",
    "        pickle.dump(topic_model, file)\n",
    "    print(\"Model saved successfully after fitting at:\", model_filename)\n",
    "    \n",
    "    # Save topic info\n",
    "    try:\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        topic_info.to_csv(topic_info_file_name)  # Save topic info to CSV\n",
    "        print(\"Topic info saved successfully at:\", topic_info_file_name)\n",
    "    except Exception as e:\n",
    "        print(\"Error during saving topic info:\", str(e))\n",
    "        return\n",
    "\n",
    "    # Ensure the transformation length matches the dataframe length\n",
    "    if len(topics) != len(df):\n",
    "        print(f\"Length mismatch: Expected {len(df)}, got {len(topics)} topics.\")\n",
    "        return\n",
    "\n",
    "    # Update dataset with topics\n",
    "    topic_names = {row['Topic']: row['Name'] for index, row in topic_info.iterrows()}\n",
    "    df['Topic Label'] = topics\n",
    "    df['First_Step_Topic_Name'] = df['Topic Label'].apply(lambda topic_num: topic_names.get(topic_num, 'Unknown'))\n",
    "    df['First_Step_Topic_Keywords'] = df['Topic Label'].apply(lambda topic_num: ', '.join(term for term, _ in topic_model.get_topic(topic_num)))\n",
    "    df['First_Step_Topic_Representation'] = df['Topic Label'].apply(lambda topic_num: ', '.join(f\"{term} ({score:.2f})\" for term, score in topic_model.get_topic(topic_num)))\n",
    "    df['First_Step_Representative_Docs'] = df['Topic Label'].apply(lambda topic_num: ', '.join(topic_model.get_representative_docs(topic_num)))\n",
    "\n",
    "    # Save the updated DataFrame\n",
    "    df.to_csv(dataset_file_name)\n",
    "    print(\"Dataset with topics saved successfully at:\", dataset_file_name)\n",
    "\n",
    "def save_outliers(df, file_name):\n",
    "    outliers_df = df[df['Topic Label'] == -1]\n",
    "    outliers_df.to_csv(file_name, index=False)\n",
    "    print(\"Outliers saved successfully at:\", file_name)\n",
    "\n",
    "# Usage\n",
    "file_path = 'corpus.tsv'\n",
    "filtered_df = load_data(file_path)\n",
    "topic_model = configure_topic_model()\n",
    "fit_transform_and_save_all(filtered_df, topic_model, \"BERTopic_first_step.pkl\", 'BERTopic_result.csv', 'BERTopic_topic_info.csv')\n",
    "save_outliers(filtered_df, 'First_step_outliers.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3f7d5a9-b917-4409-a622-1fc049e14dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully after fitting at: BERTopic_first_step.pkl\n",
      "Topic info saved successfully at: BERTopic_topic_info.csv\n",
      "Dataset with topics saved successfully at: BERTopic_result.csv\n",
      "Outliers saved successfully at: First_step_outliers.csv\n"
     ]
    }
   ],
   "source": [
    "# For TSV files\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired, OpenAI, PartOfSpeech, MaximalMarginalRelevance\n",
    "\n",
    "def load_data(file_path, text_column_name='text', has_header=True, nrows=1000):\n",
    "    # Load the first `nrows` rows of the CSV file with or without a header based on the `has_header` flag\n",
    "    df = pd.read_csv(file_path, nrows=nrows)\n",
    "    # Ensure the correct column is being used for text\n",
    "    if text_column_name not in df.columns:\n",
    "        raise ValueError(f\"Specified text column '{text_column_name}' not found in the CSV.\")\n",
    "    \n",
    "    df = df.rename(columns={text_column_name: 'text'})\n",
    "    \n",
    "    return df.copy()\n",
    "\n",
    "def configure_topic_model():\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "    ctfidf_model = ClassTfidfTransformer()\n",
    "    main_representation = KeyBERTInspired()\n",
    "    aspect_model1 = PartOfSpeech(\"en_core_web_sm\")\n",
    "    representation_model = {\n",
    "        \"Main\": main_representation,\n",
    "        \"Aspect1\": aspect_model1,\n",
    "    }\n",
    "    return BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        ctfidf_model=ctfidf_model,\n",
    "        representation_model=representation_model\n",
    "    )\n",
    "\n",
    "def fit_transform_and_save_all(df, topic_model, model_filename, dataset_file_name, topic_info_file_name):\n",
    "    try:\n",
    "        topics, _ = topic_model.fit_transform(df['text'])\n",
    "    except Exception as e:\n",
    "        print(\"Error during model fitting:\", str(e))\n",
    "        return\n",
    "    \n",
    "    with open(model_filename, \"wb\") as file:\n",
    "        pickle.dump(topic_model, file)\n",
    "    print(\"Model saved successfully after fitting at:\", model_filename)\n",
    "    \n",
    "    try:\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        topic_info.to_csv(topic_info_file_name, index=False)\n",
    "        print(\"Topic info saved successfully at:\", topic_info_file_name)\n",
    "    except Exception as e:\n",
    "        print(\"Error during saving topic info:\", str(e))\n",
    "        return\n",
    "\n",
    "    if len(topics) != len(df):\n",
    "        print(f\"Length mismatch: Expected {len(df)}, got {len(topics)} topics.\")\n",
    "        return\n",
    "\n",
    "    topic_names = {row['Topic']: row['Name'] for index, row in topic_info.iterrows()}\n",
    "    df['Topic Label'] = topics\n",
    "    df['First_Step_Topic_Name'] = df['Topic Label'].apply(lambda topic_num: topic_names.get(topic_num, 'Unknown'))\n",
    "    df['First_Step_Topic_Keywords'] = df['Topic Label'].apply(lambda topic_num: ', '.join(term for term, _ in topic_model.get_topic(topic_num)))\n",
    "    df['First_Step_Topic_Representation'] = df['Topic Label'].apply(lambda topic_num: ', '.join(f\"{term} ({score:.2f})\" for term, score in topic_model.get_topic(topic_num)))\n",
    "    df['First_Step_Representative_Docs'] = df['Topic Label'].apply(lambda topic_num: ', '.join(topic_model.get_representative_docs(topic_num)))\n",
    "\n",
    "    df.to_csv(dataset_file_name, index=False)\n",
    "    print(\"Dataset with topics saved successfully at:\", dataset_file_name)\n",
    "\n",
    "def save_outliers(df, file_name):\n",
    "    outliers_df = df[df['Topic Label'] == -1]\n",
    "    outliers_df.to_csv(file_name, index=False)\n",
    "    print(\"Outliers saved successfully at:\", file_name)\n",
    "\n",
    "# Usage\n",
    "file_path = 'filtered_arxiv_metadata_2024.csv'\n",
    "text_column_name = 'abstract'  # Adjust this to the actual name of your text column\n",
    "has_header = True  # Set to False if the CSV file does not have a header\n",
    "filtered_df = load_data(file_path, text_column_name, has_header)\n",
    "topic_model = configure_topic_model()\n",
    "fit_transform_and_save_all(filtered_df, topic_model, \"BERTopic_first_step.pkl\", 'BERTopic_result.csv', 'BERTopic_topic_info.csv')\n",
    "save_outliers(filtered_df, 'First_step_outliers.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df0e2582-22e6-4a20-93ae-12d94e59e7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model and transforming text data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text data: 100%|██████████| 54947/54947 [00:00<00:00, 2623385.57it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully after fitting at: First_Step_BERTopic_first_step.pkl\n",
      "Saving topic information...\n",
      "Topic info saved successfully at: First_Step_BERTopic_topic_info.csv\n",
      "Updating DataFrame with topic labels and keywords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating DataFrame: 100%|██████████| 54947/54947 [00:06<00:00, 8771.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with topics saved successfully at: First_Step_BERTopic_result.csv\n",
      "Outliers saved successfully at: First_step_outliers.csv\n"
     ]
    }
   ],
   "source": [
    "# For CSV files\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired, OpenAI, PartOfSpeech, MaximalMarginalRelevance\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_data(file_path, text_column_name='text', has_header=True):\n",
    "    # Load the first `nrows` rows of the CSV file with or without a header based on the `has_header` flag\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure the correct column is being used for text\n",
    "    if text_column_name not in df.columns:\n",
    "        raise ValueError(f\"Specified text column '{text_column_name}' not found in the CSV.\")\n",
    "    \n",
    "    df = df.rename(columns={text_column_name: 'text'})\n",
    "    \n",
    "    return df.copy()\n",
    "\n",
    "def configure_topic_model():\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "    ctfidf_model = ClassTfidfTransformer()\n",
    "    main_representation = KeyBERTInspired()\n",
    "    aspect_model1 = PartOfSpeech(\"en_core_web_sm\")\n",
    "    representation_model = {\n",
    "        \"Main\": main_representation,\n",
    "        \"Aspect1\": aspect_model1,\n",
    "    }\n",
    "    return BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        ctfidf_model=ctfidf_model,\n",
    "        representation_model=representation_model\n",
    "    )\n",
    "\n",
    "def fit_transform_and_save_all(df, topic_model, model_filename, dataset_file_name, topic_info_file_name):\n",
    "    # Display progress bar during model fitting\n",
    "    print(\"Fitting the model and transforming text data...\")\n",
    "    try:\n",
    "        topics, _ = topic_model.fit_transform(tqdm(df['text'], desc=\"Processing text data\"))\n",
    "    except Exception as e:\n",
    "        print(\"Error during model fitting:\", str(e))\n",
    "        return\n",
    "    \n",
    "    with open(model_filename, \"wb\") as file:\n",
    "        pickle.dump(topic_model, file)\n",
    "    print(\"Model saved successfully after fitting at:\", model_filename)\n",
    "    \n",
    "    try:\n",
    "        print(\"Saving topic information...\")\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        topic_info.to_csv(topic_info_file_name, index=False)\n",
    "        print(\"Topic info saved successfully at:\", topic_info_file_name)\n",
    "    except Exception as e:\n",
    "        print(\"Error during saving topic info:\", str(e))\n",
    "        return\n",
    "\n",
    "    if len(topics) != len(df):\n",
    "        print(f\"Length mismatch: Expected {len(df)}, got {len(topics)} topics.\")\n",
    "        return\n",
    "\n",
    "    topic_names = {row['Topic']: row['Name'] for index, row in topic_info.iterrows()}\n",
    "    \n",
    "    # Adding progress bar for DataFrame update\n",
    "    print(\"Updating DataFrame with topic labels and keywords...\")\n",
    "    for index in tqdm(range(len(df)), desc=\"Updating DataFrame\"):\n",
    "        topic_num = topics[index]\n",
    "        df.at[index, 'Topic Label'] = topic_num\n",
    "        df.at[index, 'First_Step_Topic_Name'] = topic_names.get(topic_num, 'Unknown')\n",
    "        df.at[index, 'First_Step_Topic_Keywords'] = ', '.join(term for term, _ in topic_model.get_topic(topic_num))\n",
    "        df.at[index, 'First_Step_Topic_Representation'] = ', '.join(f\"{term} ({score:.2f})\" for term, score in topic_model.get_topic(topic_num))\n",
    "        df.at[index, 'First_Step_Representative_Docs'] = ', '.join(topic_model.get_representative_docs(topic_num))\n",
    "\n",
    "    df.to_csv(dataset_file_name, index=False)\n",
    "    print(\"Dataset with topics saved successfully at:\", dataset_file_name)\n",
    "\n",
    "def save_outliers(df, file_name):\n",
    "    outliers_df = df[df['Topic Label'] == -1]\n",
    "    outliers_df.to_csv(file_name, index=False)\n",
    "    print(\"Outliers saved successfully at:\", file_name)\n",
    "\n",
    "# Usage\n",
    "file_path = 'filtered_arxiv_metadata_2024.csv'\n",
    "text_column_name = 'abstract'  # Adjust this to the actual name of your text column\n",
    "has_header = True  # Set to False if the CSV file does not have a header\n",
    "filtered_df = load_data(file_path, text_column_name, has_header)\n",
    "topic_model = configure_topic_model()\n",
    "fit_transform_and_save_all(filtered_df, topic_model, \"First_Step_BERTopic_first_step.pkl\", 'First_Step_BERTopic_result.csv', 'First_Step_BERTopic_topic_info.csv')\n",
    "save_outliers(filtered_df, 'First_step_outliers.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01573515-0bc4-4344-93c5-f33ffad91fd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### K-estimation based on the first step\n",
    "\n",
    "This algorithm is designed to determine the optimal number of topics for a second round of topic modeling, specifically focusing on documents labeled as outliers (`-1`) in the initial run.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. **Load the Dataset**:\n",
    "   - Import a CSV file containing topic modeling results, where each row represents a document and the `Topic Label` column indicates the assigned topic.\n",
    "\n",
    "2. **Calculate Total Rows**:\n",
    "   - Count the total number of rows (documents) in the dataset.\n",
    "\n",
    "3. **Identify Valid Topic Rows**:\n",
    "   - Filter and count rows with valid topic labels (i.e., not labeled as `-1`).\n",
    "\n",
    "4. **Identify Outlier Rows**:\n",
    "   - Filter and count rows labeled as `-1`, representing outliers.\n",
    "\n",
    "5. **Count Unique Valid Topics**:\n",
    "   - Calculate the number of unique valid topics identified in the dataset, excluding `-1`.\n",
    "\n",
    "6. **Calculate Topic Proportion for Outliers**:\n",
    "   - Determine the proportion of outlier rows relative to valid topic rows.\n",
    "   - Multiply this proportion by the number of unique valid topics to estimate the ideal number of topics for outliers.\n",
    "   - Round the result to the nearest whole number.\n",
    "\n",
    "7. **Output**:\n",
    "   - Provide the total number of unique valid topics, the percentage of data labeled as outliers, and the recommended number of topics for the outliers in the second modeling run.\n",
    "\n",
    "### Purpose\n",
    "This algorithm ensures that the number of topics generated for outliers is proportional to the distribution observed in the valid topics, facilitating consistent and focused topic modeling on previously unclassified documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5374814-13fb-480e-83ec-725cf6a8d52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique valid topics: 523\n",
      "Percentage of data labeled as outliers (-1): 37.41%\n",
      "Ideal number of topics for the outliers (-1): 313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/w0/tmp/slurm_yc656703.48711137/ipykernel_108768/2644776816.py:16: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calculate_proportional_topics_for_outliers(file_path, topic_column='Topic Label'):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of data assigned to valid topics and propose an ideal number of topics \n",
    "    for the rows labeled as `-1` (outliers).\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): Path to the CSV file containing the topic model results.\n",
    "    - topic_column (str): Name of the column in the CSV file that contains the topic assignments.\n",
    "\n",
    "    Returns:\n",
    "    - int: The ideal number of topics for the rows labeled as `-1`.\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure the topic column exists\n",
    "    if topic_column not in df.columns:\n",
    "        raise ValueError(f\"Column {topic_column} not found in the dataset.\")\n",
    "\n",
    "    # Calculate total number of rows\n",
    "    total_rows = len(df)\n",
    "\n",
    "    # Calculate the number of rows assigned a valid topic (excluding -1)\n",
    "    valid_topic_rows = df[df[topic_column] != -1].shape[0]\n",
    "\n",
    "    # Calculate the number of rows labeled as -1\n",
    "    outlier_rows = df[df[topic_column] == -1].shape[0]\n",
    "\n",
    "    # Calculate the number of unique topics (excluding -1)\n",
    "    unique_valid_topics = df[df[topic_column] != -1][topic_column].nunique()\n",
    "\n",
    "    # Calculate the proportion of the dataset labeled as -1\n",
    "    percentage_outliers = (outlier_rows / total_rows) * 100\n",
    "\n",
    "    # Estimate the ideal number of topics for the outliers\n",
    "    ideal_topics_for_outliers = round((outlier_rows / valid_topic_rows) * unique_valid_topics)\n",
    "    \n",
    "    print(f\"Total unique valid topics: {unique_valid_topics}\")\n",
    "    print(f\"Percentage of data labeled as outliers (-1): {percentage_outliers:.2f}%\")\n",
    "    print(f\"Ideal number of topics for the outliers (-1): {ideal_topics_for_outliers}\")\n",
    "    \n",
    "    return ideal_topics_for_outliers\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'First_Step_BERTopic_result.csv'\n",
    "ideal_topics_for_outliers = calculate_proportional_topics_for_outliers(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
