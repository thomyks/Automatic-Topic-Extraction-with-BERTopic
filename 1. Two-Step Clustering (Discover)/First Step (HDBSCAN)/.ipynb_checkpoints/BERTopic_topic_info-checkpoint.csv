Topic,Count,Name,Representation,Aspect1,Representative_Docs
-1,278,-1_learning_classification_neural_prediction,"['learning', 'classification', 'neural', 'prediction', 'datasets', 'dataset', 'models', 'networks', 'deep', 'training']","['data', 'learning', 'model', 'problem', 'methods', 'models', 'results', 'approach', 'method', 'new']","['  This paper is concerned with the statistical analysis of matrix-valued time\nseries. These are data collected over a network of sensors (typically a set of\nspatial locations) along time, where a vector of features is observed per time\ninstant per sensor. Thus each sensor is characterized by a vectorial time\nseries. We would like to identify the dependency structure among these sensors\nand represent it by a graph. When there is only one feature per sensor, the\nvector auto-regressive models have been widely adapted to infer the structure\nof Granger causality. The resulting graph is referred to as causal graph. Our\nfirst contribution is then extending VAR models to matrix-variate models to\nserve the purpose of graph learning. Secondly, we propose two online procedures\nrespectively in low and high dimensions, which can update quickly the estimates\nof coefficients when new samples arrive. In particular in high dimensional\nregime, a novel Lasso-type is introduced and we develop its homotopy algorithms\nfor the online learning. We also provide an adaptive tuning procedure for the\nregularization parameter. Lastly, we consider that, the application of AR\nmodels onto data usually requires detrending the raw data, however, this step\nis forbidden in online context. Therefore, we augment the proposed AR models by\nincorporating trend as extra parameter, and then adapt the online algorithms to\nthe augmented data models, which allow us to simultaneously learn the graph and\ntrend from streaming samples. In this work, we consider primarily the periodic\ntrend. Numerical experiments using both synthetic and real data are performed,\nwhose results support the effectiveness of the proposed methods.\n', '  Contemporary Artificial Intelligence (AI) stands on two legs: large training\ndata corpora and many-parameter artificial neural networks (ANNs). The data\ncorpora are needed to represent the complexity and heterogeneity of the world.\nThe role of the networks is less transparent due to the obscure dependence of\nthe network parameters and outputs on the training data and inputs. This raises\nproblems, ranging from technical-scientific to legal-ethical. We hypothesize\nthat a transparent approach to machine learning is possible without using\nnetworks at all. By generalizing a parameter-free, statistically consistent\ndata interpolation method, which we analyze theoretically in detail, we develop\na network-free framework for AI incorporating generative modeling. We\ndemonstrate this framework with examples from three different disciplines -\nethology, control theory, and mathematics. Our generative Hilbert framework\napplied to the trajectories of small groups of swimming fish outperformed\nstate-of-the-art traditional mathematical behavioral models and current\nANN-based models. We demonstrate pure data interpolation based control by\nstabilizing an inverted pendulum and a driven logistic map around unstable\nfixed points. Finally, we present a mathematical application by predicting\nzeros of the Riemann Zeta function, achieving comparable performance as a\ntransformer network. We do not suggest that the proposed framework will always\noutperform networks as over-parameterized networks can interpolate. However,\nour framework is theoretically sound, transparent, deterministic, and parameter\nfree: remarkably, it does not require any compute-expensive training, does not\ninvolve optimization, has no model selection, and is easily reproduced and\nported. We also propose an easily computed method of credit assignment based on\nthis framework, to help address ethical-legal challenges raised by generative\nAI.\n', '  Anomaly detection is a challenging task for machine learning algorithms due\nto the inherent class imbalance. It is costly and time-demanding to manually\nanalyse the observed data, thus usually only few known anomalies if any are\navailable. Inspired by generative models and the analysis of the hidden\nactivations of neural networks, we introduce a novel unsupervised anomaly\ndetection method called DA3D. Here, we use adversarial autoencoders to generate\nanomalous counterexamples based on the normal data only. These artificial\nanomalies used during training allow the detection of real, yet unseen\nanomalies. With our novel generative approach, we transform the unsupervised\ntask of anomaly detection to a supervised one, which is more tractable by\nmachine learning and especially deep learning methods. DA3D surpasses the\nperformance of state-of-the-art anomaly detection methods in a purely\ndata-driven way, where no domain knowledge is required.\n']"
0,113,0_nlp_corpus_semantic_textual,"['nlp', 'corpus', 'semantic', 'textual', 'linguistic', 'corpora', 'embeddings', 'text', 'bert', 'attention']","['language', 'text', 'model', 'languages', 'knowledge', 'models', 'word', 'task', 'performance', 'attention']","['  Recently, with the help of deep learning models, significant advances have\nbeen made in different Natural Language Processing (NLP) tasks. Unfortunately,\nstate-of-the-art models are vulnerable to noisy texts. We propose a new\ncontextual text denoising algorithm based on the ready-to-use masked language\nmodel. The proposed algorithm does not require retraining of the model and can\nbe integrated into any NLP system without additional training on paired\ncleaning training data. We evaluate our method under synthetic noise and\nnatural noise and show that the proposed algorithm can use context information\nto correct noise text and improve the performance of noisy inputs in several\ndownstream tasks.\n', '  Existing technologies expand BERT from different perspectives, e.g. designing\ndifferent pre-training tasks, different semantic granularities, and different\nmodel architectures. Few models consider expanding BERT from different text\nformats. In this paper, we propose a heterogeneous knowledge language model\n(\\textbf{HKLM}), a unified pre-trained language model (PLM) for all forms of\ntext, including unstructured text, semi-structured text, and well-structured\ntext. To capture the corresponding relations among these multi-format\nknowledge, our approach uses masked language model objective to learn word\nknowledge, uses triple classification objective and title matching objective to\nlearn entity knowledge and topic knowledge respectively. To obtain the\naforementioned multi-format text, we construct a corpus in the tourism domain\nand conduct experiments on 5 tourism NLP datasets. The results show that our\napproach outperforms the pre-training of plain text using only 1/4 of the data.\nWe further pre-train the domain-agnostic HKLM and achieve performance gains on\nthe XNLI dataset.\n', '  Pre-training large-scale neural language models on raw texts has made a\nsignificant contribution to improving transfer learning in natural language\nprocessing (NLP). With the introduction of transformer-based language models,\nsuch as bidirectional encoder representations from transformers (BERT), the\nperformance of information extraction from a free text by NLP has significantly\nimproved for both the general domain and medical domain; however, it is\ndifficult to train specific BERT models that perform well for domains in which\nthere are few publicly available databases of high quality and large size. We\nhypothesized that this problem can be addressed by up-sampling a\ndomain-specific corpus and using it for pre-training with a larger corpus in a\nbalanced manner. Our proposed method consists of a single intervention with one\noption: simultaneous pre-training after up-sampling and amplified vocabulary.\nWe conducted three experiments and evaluated the resulting products. We\nconfirmed that our Japanese medical BERT outperformed conventional baselines\nand the other BERT models in terms of the medical document classification task\nand that our English BERT pre-trained using both the general and medical-domain\ncorpora performed sufficiently well for practical use in terms of the\nbiomedical language understanding evaluation (BLUE) benchmark. Moreover, our\nenhanced biomedical BERT model, in which clinical notes were not used during\npre-training, showed that both the clinical and biomedical scores of the BLUE\nbenchmark were 0.3 points above that of the ablation model trained without our\nproposed method. Well-balanced pre-training by up-sampling instances derived\nfrom a corpus appropriate for the target task allows us to construct a\nhigh-performance BERT model.\n']"
1,57,1_reinforcement_learning_reward_regularization,"['reinforcement', 'learning', 'reward', 'regularization', 'planning', 'rewards', 'exploration', 'optimal', 'policy', 'training']","['reinforcement', 'policy', 'learning', 'action', 'reward', 'agent', 'state', 'algorithm', 'gradient', 'control']","['  Agents trained with deep reinforcement learning algorithms are capable of\nperforming highly complex tasks including locomotion in continuous\nenvironments. We investigate transferring the learning acquired in one task to\na set of previously unseen tasks. Generalization and overfitting in deep\nreinforcement learning are not commonly addressed in current transfer learning\nresearch. Conducting a comparative analysis without an intermediate\nregularization step results in underperforming benchmarks and inaccurate\nalgorithm comparisons due to rudimentary assessments. In this study, we propose\nregularization techniques in deep reinforcement learning for continuous control\nthrough the application of sample elimination, early stopping and maximum\nentropy regularized adversarial learning. First, the importance of the\ninclusion of training iteration number to the hyperparameters in deep transfer\nreinforcement learning will be discussed. Because source task performance is\nnot indicative of the generalization capacity of the algorithm, we start by\nacknowledging the training iteration number as a hyperparameter. In line with\nthis, we introduce an additional step of resorting to earlier snapshots of\npolicy parameters to prevent overfitting to the source task. Then, to generate\nrobust policies, we discard the samples that lead to overfitting via a method\nwe call strict clipping. Furthermore, we increase the generalization capacity\nin widely used transfer learning benchmarks by using maximum entropy\nregularization, different critic methods, and curriculum learning in an\nadversarial setup. Subsequently, we propose maximum entropy adversarial\nreinforcement learning to increase the domain randomization. Finally, we\nevaluate the robustness of these methods on simulated robots in target\nenvironments where the morphology of the robot, gravity, and tangential\nfriction coefficient of the environment are altered.\n', '  We propose two policy gradient algorithms for solving the problem of control\nin an off-policy reinforcement learning (RL) context. Both algorithms\nincorporate a smoothed functional (SF) based gradient estimation scheme. The\nfirst algorithm is a straightforward combination of importance sampling-based\noff-policy evaluation with SF-based gradient estimation. The second algorithm,\ninspired by the stochastic variance-reduced gradient (SVRG) algorithm,\nincorporates variance reduction in the update iteration. For both algorithms,\nwe derive non-asymptotic bounds that establish convergence to an approximate\nstationary point. From these results, we infer that the first algorithm\nconverges at a rate that is comparable to the well-known REINFORCE algorithm in\nan off-policy RL context, while the second algorithm exhibits an improved rate\nof convergence.\n', ""  Traffic signal control aims to coordinate traffic signals across\nintersections to improve the traffic efficiency of a district or a city. Deep\nreinforcement learning (RL) has been applied to traffic signal control recently\nand demonstrated promising performance where each traffic signal is regarded as\nan agent. However, there are still several challenges that may limit its\nlarge-scale application in the real world. To make the policy learned from a\ntraining scenario generalizable to new unseen scenarios, a novel Meta\nVariationally Intrinsic Motivated (MetaVIM) RL method is proposed to learn the\ndecentralized policy for each intersection that considers neighbor information\nin a latent way. Specifically, we formulate the policy learning as a\nmeta-learning problem over a set of related tasks, where each task corresponds\nto traffic signal control at an intersection whose neighbors are regarded as\nthe unobserved part of the state. Then, a learned latent variable is introduced\nto represent the task's specific information and is further brought into the\npolicy for learning. In addition, to make the policy learning stable, a novel\nintrinsic reward is designed to encourage each agent's received rewards and\nobservation transition to be predictable only conditioned on its own history.\nExtensive experiments conducted on CityFlow demonstrate that the proposed\nmethod substantially outperforms existing approaches and shows superior\ngeneralizability.\n""]"
2,54,2_bandits_bandit_optimal_greedy,"['bandits', 'bandit', 'optimal', 'greedy', 'reward', 'regret', 'rewards', 'strategy', 'stochastic', 'exploration']","['regret', 'algorithm', 'bandits', 'bandit', 'problem', 'optimal', 'arm', 'algorithms', 'feedback', 'reward']","[""  We consider a decentralized multi-agent Multi Armed Bandit (MAB) setup\nconsisting of $N$ agents, solving the same MAB instance to minimize individual\ncumulative regret. In our model, agents collaborate by exchanging messages\nthrough pairwise gossip style communications on an arbitrary connected graph.\nWe develop two novel algorithms, where each agent only plays from a subset of\nall the arms. Agents use the communication medium to recommend only arm-IDs\n(not samples), and thus update the set of arms from which they play. We\nestablish that, if agents communicate $\\Omega(\\log(T))$ times through any\nconnected pairwise gossip mechanism, then every agent's regret is a factor of\norder $N$ smaller compared to the case of no collaborations. Furthermore, we\nshow that the communication constraints only have a second order effect on the\nregret of our algorithm. We then analyze this second order term of the regret\nto derive bounds on the regret-communication tradeoffs. Finally, we empirically\nevaluate our algorithm and conclude that the insights are fundamental and not\nartifacts of our bounds. We also show a lower bound which gives that the regret\nscaling obtained by our algorithm cannot be improved even in the absence of any\ncommunication constraints. Our results thus demonstrate that even a minimal\nlevel of collaboration among agents greatly reduces regret for all agents.\n"", ""  We investigate a Bayesian $k$-armed bandit problem in the \\emph{many-armed}\nregime, where $k \\geq \\sqrt{T}$ and $T$ represents the time horizon. Initially,\nand aligned with recent literature on many-armed bandit problems, we observe\nthat subsampling plays a key role in designing optimal algorithms; the\nconventional UCB algorithm is sub-optimal, whereas a subsampled UCB (SS-UCB),\nwhich selects $\\Theta(\\sqrt{T})$ arms for execution under the UCB framework,\nachieves rate-optimality. However, despite SS-UCB's theoretical promise of\noptimal regret, it empirically underperforms compared to a greedy algorithm\nthat consistently chooses the empirically best arm. This observation extends to\ncontextual settings through simulations with real-world data. Our findings\nsuggest a new form of \\emph{free exploration} beneficial to greedy algorithms\nin the many-armed context, fundamentally linked to a tail event concerning the\nprior distribution of arm rewards. This finding diverges from the notion of\nfree exploration, which relates to covariate variation, as recently discussed\nin contextual bandit literature. Expanding upon these insights, we establish\nthat the subsampled greedy approach not only achieves rate-optimality for\nBernoulli bandits within the many-armed regime but also attains sublinear\nregret across broader distributions. Collectively, our research indicates that\nin the many-armed regime, practitioners might find greater value in adopting\ngreedy algorithms.\n"", ""  We consider a novel stochastic multi-armed bandit setting, where playing an\narm makes it unavailable for a fixed number of time slots thereafter. This\nmodels situations where reusing an arm too often is undesirable (e.g. making\nthe same product recommendation repeatedly) or infeasible (e.g. compute job\nscheduling on machines). We show that with prior knowledge of the rewards and\ndelays of all the arms, the problem of optimizing cumulative reward does not\nadmit any pseudo-polynomial time algorithm (in the number of arms) unless\nrandomized exponential time hypothesis is false, by mapping to the PINWHEEL\nscheduling problem. Subsequently, we show that a simple greedy algorithm that\nplays the available arm with the highest reward is asymptotically $(1-1/e)$\noptimal. When the rewards are unknown, we design a UCB based algorithm which is\nshown to have $c \\log T + o(\\log T)$ cumulative regret against the greedy\nalgorithm, leveraging the free exploration of arms due to the unavailability.\nFinally, when all the delays are equal the problem reduces to Combinatorial\nSemi-bandits providing us with a lower bound of $c' \\log T+ \\omega(\\log T)$.\n""]"
3,50,3_graphs_networks_graph_nodes,"['graphs', 'networks', 'graph', 'nodes', 'embeddings', 'neural', 'embedding', 'vertices', 'edges', 'deep']","['graph', 'node', 'graphs', 'networks', 'clustering', 'embedding', 'network', 'learning', 'nodes', 'neural']","['  Deep learning models for graphs have advanced the state of the art on many\ntasks. Despite their recent success, little is known about their robustness. We\ninvestigate training time attacks on graph neural networks for node\nclassification that perturb the discrete graph structure. Our core principle is\nto use meta-gradients to solve the bilevel problem underlying training-time\nattacks, essentially treating the graph as a hyperparameter to optimize. Our\nexperiments show that small graph perturbations consistently lead to a strong\ndecrease in performance for graph convolutional networks, and even transfer to\nunsupervised embeddings. Remarkably, the perturbations created by our algorithm\ncan misguide the graph neural networks such that they perform worse than a\nsimple baseline that ignores all relational information. Our attacks do not\nassume any knowledge about or access to the target classifiers.\n', '  In graph neural networks (GNNs), pooling operators compute local summaries of\ninput graphs to capture their global properties, and they are fundamental for\nbuilding deep GNNs that learn hierarchical representations. In this work, we\npropose the Node Decimation Pooling (NDP), a pooling operator for GNNs that\ngenerates coarser graphs while preserving the overall graph topology. During\ntraining, the GNN learns new node representations and fits them to a pyramid of\ncoarsened graphs, which is computed offline in a pre-processing stage. NDP\nconsists of three steps. First, a node decimation procedure selects the nodes\nbelonging to one side of the partition identified by a spectral algorithm that\napproximates the \\maxcut{} solution. Afterwards, the selected nodes are\nconnected with Kron reduction to form the coarsened graph. Finally, since the\nresulting graph is very dense, we apply a sparsification procedure that prunes\nthe adjacency matrix of the coarsened graph to reduce the computational cost in\nthe GNN. Notably, we show that it is possible to remove many edges without\nsignificantly altering the graph structure. Experimental results show that NDP\nis more efficient compared to state-of-the-art graph pooling operators while\nreaching, at the same time, competitive performance on a significant variety of\ngraph classification tasks.\n', '  Graph-based clustering plays an important role in the clustering area. Recent\nstudies about graph convolution neural networks have achieved impressive\nsuccess on graph type data. However, in general clustering tasks, the graph\nstructure of data does not exist such that the strategy to construct a graph is\ncrucial for performance. Therefore, how to extend graph convolution networks\ninto general clustering tasks is an attractive problem. In this paper, we\npropose a graph auto-encoder for general data clustering, which constructs the\ngraph adaptively according to the generative perspective of graphs. The\nadaptive process is designed to induce the model to exploit the high-level\ninformation behind data and utilize the non-Euclidean structure sufficiently.\nWe further design a novel mechanism with rigorous analysis to avoid the\ncollapse caused by the adaptive construction. Via combining the generative\nmodel for network embedding and graph-based clustering, a graph auto-encoder\nwith a novel decoder is developed such that it performs well in weighted graph\nused scenarios. Extensive experiments prove the superiority of our model.\n']"
4,48,4_cnn_neural_recognition_convolutional,"['cnn', 'neural', 'recognition', 'convolutional', 'deep', 'learning', 'networks', 'trained', 'convolution', 'layers']","['image', 'deep', 'neural', 'convolutional', 'network', 'networks', 'learning', 'layer', 'accuracy', 'methods']","['  Convolutional Neural Networks (CNNs) are successfully used for the important\nautomotive visual perception tasks including object recognition, motion and\ndepth estimation, visual SLAM, etc. However, these tasks are typically\nindependently explored and modeled. In this paper, we propose a joint\nmulti-task network design for learning several tasks simultaneously. Our main\nmotivation is the computational efficiency achieved by sharing the expensive\ninitial convolutional layers between all tasks. Indeed, the main bottleneck in\nautomated driving systems is the limited processing power available on\ndeployment hardware. There is also some evidence for other benefits in\nimproving accuracy for some tasks and easing development effort. It also offers\nscalability to add more tasks leveraging existing features and achieving better\ngeneralization. We survey various CNN based solutions for visual perception\ntasks in automated driving. Then we propose a unified CNN model for the\nimportant tasks and discuss several advanced optimization and architecture\ndesign techniques to improve the baseline model. The paper is partly review and\npartly positional with demonstration of several preliminary results promising\nfor future research. We first demonstrate results of multi-stream learning and\nauxiliary learning which are important ingredients to scale to a large\nmulti-task model. Finally, we implement a two-stream three-task network which\nperforms better in many cases compared to their corresponding single-task\nmodels, while maintaining network size.\n', ""  Whereas conventional state-of-the-art image processing systems of recording\nand output devices almost exclusively utilize square arranged methods,\nbiological models, however, suggest an alternative, evolutionarily-based\nstructure. Inspired by the human visual perception system, hexagonal image\nprocessing in the context of machine learning offers a number of key advantages\nthat can benefit both researchers and users alike. The hexagonal deep learning\nframework Hexnet leveraged in this contribution serves therefore the generation\nof hexagonal images by utilizing hexagonal deep neural networks (H-DNN). As the\nresults of our created test environment show, the proposed models can surpass\ncurrent approaches of conventional image generation. While resulting in a\nreduction of the models' complexity in the form of trainable parameters, they\nfurthermore allow an increase of test rates in comparison to their square\ncounterparts.\n"", ""  One of the main challenges since the advancement of convolutional neural\nnetworks is how to connect the extracted feature map to the final\nclassification layer. VGG models used two sets of fully connected layers for\nthe classification part of their architectures, which significantly increased\nthe number of models' weights. ResNet and the next deep convolutional models\nused the Global Average Pooling (GAP) layer to compress the feature map and\nfeed it to the classification layer. Although using the GAP layer reduces the\ncomputational cost, but also causes losing spatial resolution of the feature\nmap, which results in decreasing learning efficiency. In this paper, we aim to\ntackle this problem by replacing the GAP layer with a new architecture called\nWise-SrNet. It is inspired by the depthwise convolutional idea and is designed\nfor processing spatial resolution while not increasing computational cost. We\nhave evaluated our method using three different datasets: Intel Image\nClassification Challenge, MIT Indoors Scenes, and a part of the ImageNet\ndataset. We investigated the implementation of our architecture on several\nmodels of the Inception, ResNet, and DenseNet families. Applying our\narchitecture has revealed a significant effect on increasing convergence speed\nand accuracy. Our Experiments on images with 224*224 resolution increased the\nTop-1 accuracy between 2% to 8% on different datasets and models. Running our\nmodels on 512*512 resolution images of the MIT Indoors Scenes dataset showed a\nnotable result of improving the Top-1 accuracy within 3% to 26%. We will also\ndemonstrate the GAP layer's disadvantage when the input images are large and\nthe number of classes is not few. In this circumstance, our proposed\narchitecture can do a great help in enhancing classification results. The code\nis shared at https://github.com/mr7495/image-classification-spatial.\n""]"
5,40,5_neural_networks_tensor_gradient,"['neural', 'networks', 'tensor', 'gradient', 'learning', 'nonlinear', 'generalization', 'approximating', 'deep', 'layer']","['neural', 'networks', 'approximation', 'functions', 'solution', 'equations', 'differential', 'network', 'spaces', 'deep']","['  The classical development of neural networks has primarily focused on\nlearning mappings between finite dimensional Euclidean spaces or finite sets.\nWe propose a generalization of neural networks to learn operators, termed\nneural operators, that map between infinite dimensional function spaces. We\nformulate the neural operator as a composition of linear integral operators and\nnonlinear activation functions. We prove a universal approximation theorem for\nour proposed neural operator, showing that it can approximate any given\nnonlinear continuous operator. The proposed neural operators are also\ndiscretization-invariant, i.e., they share the same model parameters among\ndifferent discretization of the underlying function spaces. Furthermore, we\nintroduce four classes of efficient parameterization, viz., graph neural\noperators, multi-pole graph neural operators, low-rank neural operators, and\nFourier neural operators. An important application for neural operators is\nlearning surrogate maps for the solution operators of partial differential\nequations (PDEs). We consider standard PDEs such as the Burgers, Darcy\nsubsurface flow, and the Navier-Stokes equations, and show that the proposed\nneural operators have superior performance compared to existing machine\nlearning based methodologies, while being several orders of magnitude faster\nthan conventional PDE solvers.\n', '  Recently, physics-informed neural networks (PINNs) have offered a powerful\nnew paradigm for solving problems relating to differential equations. Compared\nto classical numerical methods PINNs have several advantages, for example their\nability to provide mesh-free solutions of differential equations and their\nability to carry out forward and inverse modelling within the same optimisation\nproblem. Whilst promising, a key limitation to date is that PINNs have\nstruggled to accurately and efficiently solve problems with large domains\nand/or multi-scale solutions, which is crucial for their real-world\napplication. Multiple significant and related factors contribute to this issue,\nincluding the increasing complexity of the underlying PINN optimisation problem\nas the problem size grows and the spectral bias of neural networks. In this\nwork we propose a new, scalable approach for solving large problems relating to\ndifferential equations called Finite Basis PINNs (FBPINNs). FBPINNs are\ninspired by classical finite element methods, where the solution of the\ndifferential equation is expressed as the sum of a finite set of basis\nfunctions with compact support. In FBPINNs neural networks are used to learn\nthese basis functions, which are defined over small, overlapping subdomains.\nFBINNs are designed to address the spectral bias of neural networks by using\nseparate input normalisation over each subdomain, and reduce the complexity of\nthe underlying optimisation problem by using many smaller neural networks in a\nparallel divide-and-conquer approach. Our numerical experiments show that\nFBPINNs are effective in solving both small and larger, multi-scale problems,\noutperforming standard PINNs in both accuracy and computational resources\nrequired, potentially paving the way to the application of PINNs on large,\nreal-world problems.\n', '  We study the approximation of functions by tensor networks (TNs). We show\nthat Lebesgue $L^p$-spaces in one dimension can be identified with tensor\nproduct spaces of arbitrary order through tensorization. We use this tensor\nproduct structure to define subsets of $L^p$ of rank-structured functions of\nfinite representation complexity. These subsets are then used to define\ndifferent approximation classes of tensor networks, associated with different\nmeasures of complexity. These approximation classes are shown to be\nquasi-normed linear spaces. We study some elementary properties and\nrelationships of said spaces. In part II of this work, we will show that\nclassical smoothness (Besov) spaces are continuously embedded into these\napproximation classes. We will also show that functions in these approximation\nclasses do not possess any Besov smoothness, unless one restricts the depth of\nthe tensor networks. The results of this work are both an analysis of the\napproximation spaces of TNs and a study of the expressivity of a particular\ntype of neural networks (NN) -- namely feed-forward sum-product networks with\nsparse architecture. The input variables of this network result from the\ntensorization step, interpreted as a particular featuring step which can also\nbe implemented with a neural network with a specific architecture. We point out\ninteresting parallels to recent results on the expressivity of rectified linear\nunit (ReLU) networks -- currently one of the most popular type of NNs.\n']"
6,36,6_logics_semantics_ai_logic,"['logics', 'semantics', 'ai', 'logic', 'relational', 'reasoning', 'expressiveness', 'analogical', 'ontology', 'intelligence']","['logic', 'intelligence', 'relational', 'systems', 'semantics', 'artificial', 'logics', 'framework', 'expressiveness', 'complexity']","[""  In order to construct an ethical artificial intelligence (AI) two complex\nproblems must be overcome. Firstly, humans do not consistently agree on what is\nor is not ethical. Second, contemporary AI and machine learning methods tend to\nbe blunt instruments which either search for solutions within the bounds of\npredefined rules, or mimic behaviour. An ethical AI must be capable of\ninferring unspoken rules, interpreting nuance and context, possess and be able\nto infer intent, and explain not just its actions but its intent. Using\nenactivism, semiotics, perceptual symbol systems and symbol emergence, we\nspecify an agent that learns not just arbitrary relations between signs but\ntheir meaning in terms of the perceptual states of its sensorimotor system.\nSubsequently it can learn what is meant by a sentence and infer the intent of\nothers in terms of its own experiences. It has malleable intent because the\nmeaning of symbols changes as it learns, and its intent is represented\nsymbolically as a goal. As such it may learn a concept of what is most likely\nto be considered ethical by the majority within a population of humans, which\nmay then be used as a goal. The meaning of abstract symbols is expressed using\nperceptual symbols of raw sensorimotor stimuli as the weakest (consistent with\nOckham's Razor) necessary and sufficient concept, an intensional definition\nlearned from an ostensive definition, from which the extensional definition or\ncategory of all ethical decisions may be obtained. Because these abstract\nsymbols are the same for both situation and response, the same symbol is used\nwhen either performing or observing an action. This is akin to mirror neurons\nin the human brain. Mirror symbols may allow the agent to empathise, because\nits own experiences are associated with the symbol, which is also associated\nwith the observation of another agent experiencing something that symbol\nrepresents.\n"", ""  Analogy-making is at the core of human and artificial intelligence and\ncreativity with applications to such diverse tasks as proving mathematical\ntheorems and building mathematical theories, common sense reasoning, learning,\nlanguage acquisition, and story telling. This paper introduces from first\nprinciples an abstract algebraic framework of analogical proportions of the\nform `$a$ is to $b$ what $c$ is to $d$' in the general setting of universal\nalgebra. This enables us to compare mathematical objects possibly across\ndifferent domains in a uniform way which is crucial for AI-systems. It turns\nout that our notion of analogical proportions has appealing mathematical\nproperties. As we construct our model from first principles using only\nelementary concepts of universal algebra, and since our model questions some\nbasic properties of analogical proportions presupposed in the literature, to\nconvince the reader of the plausibility of our model we show that it can be\nnaturally embedded into first-order logic via model-theoretic types and prove\nfrom that perspective that analogical proportions are compatible with\nstructure-preserving mappings. This provides conceptual evidence for its\napplicability. In a broader sense, this paper is a first step towards a theory\nof analogical reasoning and learning systems with potential applications to\nfundamental AI-problems like common sense reasoning and computational learning\nand creativity.\n"", '  This survey explores the integration of learning and reasoning in two\ndifferent fields of artificial intelligence: neurosymbolic and statistical\nrelational artificial intelligence. Neurosymbolic artificial intelligence\n(NeSy) studies the integration of symbolic reasoning and neural networks, while\nstatistical relational artificial intelligence (StarAI) focuses on integrating\nlogic with probabilistic graphical models. This survey identifies seven shared\ndimensions between these two subfields of AI. These dimensions can be used to\ncharacterize different NeSy and StarAI systems. They are concerned with (1) the\napproach to logical inference, whether model or proof-based; (2) the syntax of\nthe used logical theories; (3) the logical semantics of the systems and their\nextensions to facilitate learning; (4) the scope of learning, encompassing\neither parameter or structure learning; (5) the presence of symbolic and\nsubsymbolic representations; (6) the degree to which systems capture the\noriginal logic, probabilistic, and neural paradigms; and (7) the classes of\nlearning tasks the systems are applied to. By positioning various NeSy and\nStarAI systems along these dimensions and pointing out similarities and\ndifferences between them, this survey contributes fundamental concepts for\nunderstanding the integration of learning and reasoning.\n']"
7,36,7_kernels_distributions_kernel_gaussian,"['kernels', 'distributions', 'kernel', 'gaussian', 'sampling', 'statistics', 'dimensional', 'estimation', 'distances', 'metric']","['distance', 'kernel', 'gaussian', 'data', 'likelihood', 'distributions', 'test', 'sampling', 'dimensional', 'sample']","['  We present a study of a kernel-based two-sample test statistic related to the\nMaximum Mean Discrepancy (MMD) in the manifold data setting, assuming that\nhigh-dimensional observations are close to a low-dimensional manifold. We\ncharacterize the test level and power in relation to the kernel bandwidth, the\nnumber of samples, and the intrinsic dimensionality of the manifold.\nSpecifically, when data densities $p$ and $q$ are supported on a\n$d$-dimensional sub-manifold ${M}$ embedded in an $m$-dimensional space and are\nH\\""older with order $\\beta$ (up to 2) on ${M}$, we prove a guarantee of the\ntest power for finite sample size $n$ that exceeds a threshold depending on\n$d$, $\\beta$, and $\\Delta_2$ the squared $L^2$-divergence between $p$ and $q$\non the manifold, and with a properly chosen kernel bandwidth $\\gamma$. For\nsmall density departures, we show that with large $n$ they can be detected by\nthe kernel test when $\\Delta_2$ is greater than $n^{- { 2 \\beta/( d + 4 \\beta )\n}}$ up to a certain constant and $\\gamma$ scales as $n^{-1/(d+4\\beta)}$. The\nanalysis extends to cases where the manifold has a boundary and the data\nsamples contain high-dimensional additive noise. Our results indicate that the\nkernel two-sample test has no curse-of-dimensionality when the data lie on or\nnear a low-dimensional manifold. We validate our theory and the properties of\nthe kernel test for manifold data through a series of numerical experiments.\n', '  Distance correlation has gained much recent attention in the data science\ncommunity: the sample statistic is straightforward to compute and\nasymptotically equals zero if and only if independence, making it an ideal\nchoice to discover any type of dependency structure given sufficient sample\nsize. One major bottleneck is the testing process: because the null\ndistribution of distance correlation depends on the underlying random variables\nand metric choice, it typically requires a permutation test to estimate the\nnull and compute the p-value, which is very costly for large amount of data. To\novercome the difficulty, in this paper we propose a chi-square test for\ndistance correlation. Method-wise, the chi-square test is non-parametric,\nextremely fast, and applicable to bias-corrected distance correlation using any\nstrong negative type metric or characteristic kernel. The test exhibits a\nsimilar testing power as the standard permutation test, and can be utilized for\nK-sample and partial testing. Theory-wise, we show that the underlying\nchi-square distribution well approximates and dominates the limiting null\ndistribution in upper tail, prove the chi-square test can be valid and\nuniversally consistent for testing independence, and establish a testing power\ninequality with respect to the permutation test.\n', '  This paper introduces and investigates the utilization of maximum and average\ndistance correlations for multivariate independence testing. We characterize\ntheir consistency properties in high-dimensional settings with respect to the\nnumber of marginally dependent dimensions, assess the advantages of each test\nstatistic, examine their respective null distributions, and present a fast\nchi-square-based testing procedure. The resulting tests are non-parametric and\napplicable to both Euclidean distance and the Gaussian kernel as the underlying\nmetric. To better understand the practical use cases of the proposed tests, we\nevaluate the empirical performance of the maximum distance correlation, average\ndistance correlation, and the original distance correlation across various\nmultivariate dependence scenarios, as well as conduct a real data experiment to\ntest the presence of various cancer types and peptide levels in human plasma.\n']"
8,35,8_federated_learning_fedcluster_fedcg,"['federated', 'learning', 'fedcluster', 'fedcg', 'distributed', 'privacy', 'training', 'sharing', 'collaboratively', 'centralized']","['privacy', 'learning', 'local', 'communication', 'training', 'data', 'convergence', 'model', 'clients', 'framework']","[""  Federated learning (FL) aims to protect data privacy by enabling clients to\nbuild machine learning models collaboratively without sharing their private\ndata. Recent works demonstrate that information exchanged during FL is subject\nto gradient-based privacy attacks, and consequently, a variety of\nprivacy-preserving methods have been adopted to thwart such attacks. However,\nthese defensive methods either introduce orders of magnitude more computational\nand communication overheads (e.g., with homomorphic encryption) or incur\nsubstantial model performance losses in terms of prediction accuracy (e.g.,\nwith differential privacy). In this work, we propose $\\textsc{FedCG}$, a novel\nfederated learning method that leverages conditional generative adversarial\nnetworks to achieve high-level privacy protection while still maintaining\ncompetitive model performance. $\\textsc{FedCG}$ decomposes each client's local\nnetwork into a private extractor and a public classifier and keeps the\nextractor local to protect privacy. Instead of exposing extractors,\n$\\textsc{FedCG}$ shares clients' generators with the server for aggregating\nclients' shared knowledge, aiming to enhance the performance of each client's\nlocal networks. Extensive experiments demonstrate that $\\textsc{FedCG}$ can\nachieve competitive model performance compared with FL baselines, and privacy\nanalysis shows that $\\textsc{FedCG}$ has a high-level privacy-preserving\ncapability. Code is available at https://github.com/yankang18/FedCG\n"", ""  Federated Learning (FL) enables the multiple participating devices to\ncollaboratively contribute to a global neural network model while keeping the\ntraining data locally. Unlike the centralized training setting, the non-IID,\nimbalanced (statistical heterogeneity) and distribution shifted training data\nof FL is distributed in the federated network, which will increase the\ndivergences between the local models and the global model, further degrading\nperformance. In this paper, we propose a flexible clustered federated learning\n(CFL) framework named FlexCFL, in which we 1) group the training of clients\nbased on the similarities between the clients' optimization directions for\nlower training divergence; 2) implement an efficient newcomer device cold start\nmechanism for framework scalability and practicality; 3) flexibly migrate\nclients to meet the challenge of client-level data distribution shift. FlexCFL\ncan achieve improvements by dividing joint optimization into groups of\nsub-optimization and can strike a balance between accuracy and communication\nefficiency in the distribution shift environment. The convergence and\ncomplexity are analyzed to demonstrate the efficiency of FlexCFL. We also\nevaluate FlexCFL on several open datasets and made comparisons with related CFL\nframeworks. The results show that FlexCFL can significantly improve absolute\ntest accuracy by +10.6% on FEMNIST compared to FedAvg, +3.5% on FashionMNIST\ncompared to FedProx, +8.4% on MNIST compared to FeSEM. The experiment results\nshow that FlexCFL is also communication efficient in the distribution shift\nenvironment.\n"", ""  Federated Learning (FL) enables the multiple participating devices to\ncollaboratively contribute to a global neural network model while keeping the\ntraining data locally. Unlike the centralized training setting, the non-IID and\nimbalanced (statistical heterogeneity) training data of FL is distributed in\nthe federated network, which will increase the divergences between the local\nmodels and global model, further degrading performance. In this paper, we\npropose a novel clustered federated learning (CFL) framework FedGroup, in which\nwe 1) group the training of clients based on the similarities between the\nclients' optimization directions for high training performance; 2) construct a\nnew data-driven distance measure to improve the efficiency of the client\nclustering procedure. 3) implement a newcomer device cold start mechanism based\non the auxiliary global model for framework scalability and practicality.\n  FedGroup can achieve improvements by dividing joint optimization into groups\nof sub-optimization and can be combined with FL optimizer FedProx. The\nconvergence and complexity are analyzed to demonstrate the efficiency of our\nproposed framework. We also evaluate FedGroup and FedGrouProx (combined with\nFedProx) on several open datasets and made comparisons with related CFL\nframeworks. The results show that FedGroup can significantly improve absolute\ntest accuracy by +14.1% on FEMNIST compared to FedAvg. +3.4% on Sentiment140\ncompared to FedProx, +6.9% on MNIST compared to FeSEM.\n""]"
9,26,9_optimizers_gradient_optimization_adaptive,"['optimizers', 'gradient', 'optimization', 'adaptive', 'minimax', 'learning', 'algorithms', 'sgd', 'iteration', 'neural']","['convergence', 'gradient', 'stochastic', 'nonconvex', 'convex', 'optimization', 'adaptive', 'problems', 'iteration', 'descent']","['  In this paper, we study zeroth-order algorithms for nonconvex-concave minimax\nproblems, which have attracted widely attention in machine learning, signal\nprocessing and many other fields in recent years. We propose a zeroth-order\nalternating randomized gradient projection (ZO-AGP) algorithm for smooth\nnonconvex-concave minimax problems, and its iteration complexity to obtain an\n$\\varepsilon$-stationary point is bounded by $\\mathcal{O}(\\varepsilon^{-4})$,\nand the number of function value estimation is bounded by\n$\\mathcal{O}(d_{x}+d_{y})$ per iteration. Moreover, we propose a zeroth-order\nblock alternating randomized proximal gradient algorithm (ZO-BAPG) for solving\nblock-wise nonsmooth nonconvex-concave minimax optimization problems, and the\niteration complexity to obtain an $\\varepsilon$-stationary point is bounded by\n$\\mathcal{O}(\\varepsilon^{-4})$ and the number of function value estimation per\niteration is bounded by $\\mathcal{O}(K d_{x}+d_{y})$. To the best of our\nknowledge, this is the first time that zeroth-order algorithms with iteration\ncomplexity gurantee are developed for solving both general smooth and\nblock-wise nonsmooth nonconvex-concave minimax problems. Numerical results on\ndata poisoning attack problem and distributed nonconvex sparse principal\ncomponent analysis problem validate the efficiency of the proposed algorithms.\n', '  It is known that adaptive optimization algorithms represent the key pillar\nbehind the rise of the Machine Learning field. In the Optimization literature\nnumerous studies have been devoted to accelerated gradient methods but only\nrecently adaptive iterative techniques were analyzed from a theoretical point\nof view. In the present paper we introduce new adaptive algorithms endowed with\nmomentum terms for stochastic non-convex optimization problems. Our purpose is\nto show a deep connection between accelerated methods endowed with different\ninertial steps and AMSGrad-type momentum methods. Our methodology is based on\nthe framework of stochastic and possibly non-convex objective mappings, along\nwith some assumptions that are often used in the investigation of adaptive\nalgorithms. In addition to discussing the finite-time horizon analysis in\nrelation to a certain final iteration and the almost sure convergence to\nstationary points, we shall also look at the worst-case iteration complexity.\nThis will be followed by an estimate for the expectation of the squared\nEuclidean norm of the gradient. Various computational simulations for the\ntraining of neural networks are being used to support the theoretical analysis.\nFor future research we emphasize that there are multiple possible extensions to\nour work, from which we mention the investigation regarding non-smooth\nobjective functions and the theoretical analysis of a more general formulation\nthat encompass our adaptive optimizers in a stochastic framework.\n', '  Adaptive gradient methods are workhorses in deep learning. However, the\nconvergence guarantees of adaptive gradient methods for nonconvex optimization\nhave not been thoroughly studied. In this paper, we provide a fine-grained\nconvergence analysis for a general class of adaptive gradient methods including\nAMSGrad, RMSProp and AdaGrad. For smooth nonconvex functions, we prove that\nadaptive gradient methods in expectation converge to a first-order stationary\npoint. Our convergence rate is better than existing results for adaptive\ngradient methods in terms of dimension. In addition, we also prove high\nprobability bounds on the convergence rates of AMSGrad, RMSProp as well as\nAdaGrad, which have not been established before. Our analyses shed light on\nbetter understanding the mechanism behind adaptive gradient methods in\noptimizing nonconvex objectives.\n']"
10,24,10_adversarial_adversary_robustness_gans,"['adversarial', 'adversary', 'robustness', 'gans', 'robust', 'attacks', 'evasion', 'security', 'malicious', 'defending']","['adversarial', 'robustness', 'attacks', 'attack', 'examples', 'defense', 'malware', 'neural', 'security', 'perturbations']","['  Deep Neural Networks (DNNs) are vulnerable to invisible perturbations on the\nimages generated by adversarial attacks, which raises researches on the\nadversarial robustness of DNNs. A series of methods represented by the\nadversarial training and its variants have proven as one of the most effective\ntechniques in enhancing the DNN robustness. Generally, adversarial training\nfocuses on enriching the training data by involving perturbed data. Such data\naugmentation effect of the involved perturbed data in adversarial training does\nnot contribute to the robustness of DNN itself and usually suffers from clean\naccuracy drop. Towards the robustness of DNN itself, we in this paper propose a\nnovel defense that aims at augmenting the model in order to learn features that\nare adaptive to diverse inputs, including adversarial examples. More\nspecifically, to augment the model, multiple paths are embedded into the\nnetwork, and an orthogonality constraint is imposed on these paths to guarantee\nthe diversity among them. A margin-maximization loss is then designed to\nfurther boost such DIversity via Orthogonality (DIO). In this way, the proposed\nDIO augments the model and enhances the robustness of DNN itself as the learned\nfeatures can be corrected by these mutually-orthogonal paths. Extensive\nempirical results on various data sets, structures and attacks verify the\nstronger adversarial robustness of the proposed DIO utilizing model\naugmentation. Besides, DIO can also be flexibly combined with different data\naugmentation techniques (e.g., TRADES and DDPM), further promoting robustness\ngains.\n', '  Deep neural networks have been successfully applied in various machine\nlearning tasks. However, studies show that neural networks are susceptible to\nadversarial attacks. This exposes a potential threat to neural network-based\nintelligent systems. We observe that the probability of the correct result\noutputted by the neural network increases by applying small first-order\nperturbations generated for non-predicted class labels to adversarial examples.\nBased on this observation, we propose a method for counteracting adversarial\nperturbations to improve adversarial robustness. In the proposed method, we\nrandomly select a number of class labels and generate small first-order\nperturbations for these selected labels. The generated perturbations are added\ntogether and then clamped onto a specified space. The obtained perturbation is\nfinally added to the adversarial example to counteract the adversarial\nperturbation contained in the example. The proposed method is applied at\ninference time and does not require retraining or finetuning the model. We\nexperimentally validate the proposed method on CIFAR-10 and CIFAR-100. The\nresults demonstrate that our method effectively improves the defense\nperformance of several transformation-based defense methods, especially against\nstrong adversarial examples generated using more iterations.\n', '  Previous works have shown that automatic speaker verification (ASV) is\nseriously vulnerable to malicious spoofing attacks, such as replay, synthetic\nspeech, and recently emerged adversarial attacks. Great efforts have been\ndedicated to defending ASV against replay and synthetic speech; however, only a\nfew approaches have been explored to deal with adversarial attacks. All the\nexisting approaches to tackle adversarial attacks for ASV require the knowledge\nfor adversarial samples generation, but it is impractical for defenders to know\nthe exact attack algorithms that are applied by the in-the-wild attackers. This\nwork is among the first to perform adversarial defense for ASV without knowing\nthe specific attack algorithms. Inspired by self-supervised learning models\n(SSLMs) that possess the merits of alleviating the superficial noise in the\ninputs and reconstructing clean samples from the interrupted ones, this work\nregards adversarial perturbations as one kind of noise and conducts adversarial\ndefense for ASV by SSLMs. Specifically, we propose to perform adversarial\ndefense from two perspectives: 1) adversarial perturbation purification and 2)\nadversarial perturbation detection. Experimental results show that our\ndetection module effectively shields the ASV by detecting adversarial samples\nwith an accuracy of around 80%. Moreover, since there is no common metric for\nevaluating the adversarial defense performance for ASV, this work also\nformalizes evaluation metrics for adversarial defense considering both\npurification and detection based approaches into account. We sincerely\nencourage future works to benchmark their approaches based on the proposed\nevaluation framework.\n']"
11,24,11_regularization_lasso_regularized_minimization,"['regularization', 'lasso', 'regularized', 'minimization', 'matrix', 'predictors', 'matrices', 'pca', 'penalized', 'completion']","['matrix', 'rank', 'lasso', 'regression', 'linear', 'screening', 'squares', 'low', 'rule', 'completion']","['  Predictor screening rules, which discard predictors before fitting a model,\nhave had considerable impact on the speed with which sparse regression\nproblems, such as the lasso, can be solved. In this paper we present a new\nscreening rule for solving the lasso path: the Hessian Screening Rule. The rule\nuses second-order information from the model to provide both effective\nscreening, particularly in the case of high correlation, as well as accurate\nwarm starts. The proposed rule outperforms all alternatives we study on\nsimulated data sets with both low and high correlation for $\\ell_1$-regularized\nleast-squares (the lasso) and logistic regression. It also performs best in\ngeneral on the real data sets that we examine.\n', '  In this paper, we develop a relative error bound for nuclear norm regularized\nmatrix completion, with the focus on the completion of full-rank matrices.\nUnder the assumption that the top eigenspaces of the target matrix are\nincoherent, we derive a relative upper bound for recovering the best low-rank\napproximation of the unknown matrix. Although multiple works have been devoted\nto analyzing the recovery error of full-rank matrix completion, their error\nbounds are usually additive, making it impossible to obtain the perfect\nrecovery case and more generally difficult to leverage the skewed distribution\nof eigenvalues. Our analysis is built upon the optimality condition of the\nregularized formulation and existing guarantees for low-rank matrix completion.\nTo the best of our knowledge, this is the first relative bound that has been\nproved for the regularized formulation of matrix completion.\n', '  This paper considers the problem of estimating a low-rank matrix from the\nobservation of all or a subset of its entries in the presence of Poisson noise.\nWhen we observe all entries, this is a problem of matrix denoising; when we\nobserve only a subset of the entries, this is a problem of matrix completion.\nIn both cases, we exploit an assumption that the underlying matrix is low-rank.\nSpecifically, we analyze several estimators, including a constrained\nnuclear-norm minimization program, nuclear-norm regularized least squares, and\na nonconvex constrained low-rank optimization problem. We show that for all\nthree estimators, with high probability, we have an upper error bound (in the\nFrobenius norm error metric) that depends on the matrix rank, the fraction of\nthe elements observed, and maximal row and column sums of the true matrix. We\nfurthermore show that the above results are minimax optimal (within a universal\nconstant) in classes of matrices with low rank and bounded row and column sums.\nWe also extend these results to handle the case of matrix multinomial denoising\nand completion.\n']"
12,22,12_gans_generative_gan_cnn,"['gans', 'generative', 'gan', 'cnn', 'adversarial', 'codegan', 'cgan', 'asymmetricgan', 'deep', 'neural']","['image', 'generative', 'quality', 'images', 'training', 'generation', 'latent', 'disentanglement', 'adversarial', 'learning']","['  Generative adversarial networks (GANs) can be trained to generate 3D image\ndata, which is useful for design optimisation. However, this conventionally\nrequires 3D training data, which is challenging to obtain. 2D imaging\ntechniques tend to be faster, higher resolution, better at phase identification\nand more widely available. Here, we introduce a generative adversarial network\narchitecture, SliceGAN, which is able to synthesise high fidelity 3D datasets\nusing a single representative 2D image. This is especially relevant for the\ntask of material microstructure generation, as a cross-sectional micrograph can\ncontain sufficient information to statistically reconstruct 3D samples. Our\narchitecture implements the concept of uniform information density, which both\nensures that generated volumes are equally high quality at all points in space,\nand that arbitrarily large volumes can be generated. SliceGAN has been\nsuccessfully trained on a diverse set of materials, demonstrating the\nwidespread applicability of this tool. The quality of generated micrographs is\nshown through a statistical comparison of synthetic and real datasets of a\nbattery electrode in terms of key microstructural metrics. Finally, we find\nthat the generation time for a $10^8$ voxel volume is on the order of a few\nseconds, yielding a path for future studies into high-throughput\nmicrostructural optimisation.\n', ""  Modelling the impact of a material's mesostructure on device level\nperformance typically requires access to 3D image data containing all the\nrelevant information to define the geometry of the simulation domain. This\nimage data must include sufficient contrast between phases to distinguish each\nmaterial, be of high enough resolution to capture the key details, but also\nhave a large enough field-of-view to be representative of the material in\ngeneral. It is rarely possible to obtain data with all of these properties from\na single imaging technique. In this paper, we present a method for combining\ninformation from pairs of distinct but complementary imaging techniques in\norder to accurately reconstruct the desired multi-phase, high resolution,\nrepresentative, 3D images. Specifically, we use deep convolutional generative\nadversarial networks to implement super-resolution, style transfer and\ndimensionality expansion. To demonstrate the widespread applicability of this\ntool, two pairs of datasets are used to validate the quality of the volumes\ngenerated by fusing the information from paired imaging techniques. Three key\nmesostructural metrics are calculated in each case to show the accuracy of this\nmethod. Having confidence in the accuracy of our method, we then demonstrate\nits power by applying to a real data pair from a lithium ion battery electrode,\nwhere the required 3D high resolution image data is not available anywhere in\nthe literature. We believe this approach is superior to previously reported\nstatistical material reconstruction methods both in terms of its fidelity and\nease of use. Furthermore, much of the data required to train this algorithm\nalready exists in the literature, waiting to be combined. As such, our\nopen-access code could precipitate a step change by generating the hard to\nobtain high quality image volumes necessary to simulate behaviour at the\nmesoscale.\n"", '  Current autoencoder-based disentangled representation learning methods\nachieve disentanglement by penalizing the (aggregate) posterior to encourage\nstatistical independence of the latent factors. This approach introduces a\ntrade-off between disentangled representation learning and reconstruction\nquality since the model does not have enough capacity to learn correlated\nlatent variables that capture detail information present in most image data. To\novercome this trade-off, we present a novel multi-stage modeling approach where\nthe disentangled factors are first learned using a penalty-based disentangled\nrepresentation learning method; then, the low-quality reconstruction is\nimproved with another deep generative model that is trained to model the\nmissing correlated latent variables, adding detail information while\nmaintaining conditioning on the previously learned disentangled factors. Taken\ntogether, our multi-stage modelling approach results in a single, coherent\nprobabilistic model that is theoretically justified by the principal of\nD-separation and can be realized with a variety of model classes including\nlikelihood-based models such as variational autoencoders, implicit models such\nas generative adversarial networks, and tractable models like normalizing flows\nor mixtures of Gaussians. We demonstrate that our multi-stage model has higher\nreconstruction quality than current state-of-the-art methods with equivalent\ndisentanglement performance across multiple standard benchmarks. In addition,\nwe apply the multi-stage model to generate synthetic tabular datasets,\nshowcasing an enhanced performance over benchmark models across a variety of\nmetrics. The interpretability analysis further indicates that the multi-stage\nmodel can effectively uncover distinct and meaningful features of variations\nfrom which the original distribution can be recovered.\n']"
13,21,13_boosting_classification_ensemble_features,"['boosting', 'classification', 'ensemble', 'features', 'feature', 'prediction', 'predictive', 'predict', 'forests', 'models']","['decision', 'machine', 'models', 'forests', 'learning', 'performance', 'trees', 'data', 'predictive', 'random']","['  Decision forests, including Random Forests and Gradient Boosting Trees, have\nrecently demonstrated state-of-the-art performance in a variety of machine\nlearning settings. Decision forests are typically ensembles of axis-aligned\ndecision trees; that is, trees that split only along feature dimensions. In\ncontrast, many recent extensions to decision forests are based on axis-oblique\nsplits. Unfortunately, these extensions forfeit one or more of the favorable\nproperties of decision forests based on axis-aligned splits, such as robustness\nto many noise dimensions, interpretability, or computational efficiency. We\nintroduce yet another decision forest, called ""Sparse Projection Oblique\nRandomer Forests"" (SPORF). SPORF uses very sparse random projections, i.e.,\nlinear combinations of a small subset of features. SPORF significantly improves\naccuracy over existing state-of-the-art algorithms on a standard benchmark\nsuite for classification with >100 problems of varying dimension, sample size,\nand number of classes. To illustrate how SPORF addresses the limitations of\nboth axis-aligned and existing oblique decision forest methods, we conduct\nextensive simulated experiments. SPORF typically yields improved performance\nover existing decision forests, while mitigating computational efficiency and\nscalability and maintaining interpretability. SPORF can easily be incorporated\ninto other ensemble methods such as boosting to obtain potentially similar\ngains.\n', ""  Problem definition. In retailing, discrete choice models (DCMs) are commonly\nused to capture the choice behavior of customers when offered an assortment of\nproducts. When estimating DCMs using transaction data, flexible models (such as\nmachine learning models or nonparametric models) are typically not\ninterpretable and hard to estimate, while tractable models (such as the\nmultinomial logit model) tend to misspecify the complex behavior represeted in\nthe data. Methodology/results. In this study, we use a forest of binary\ndecision trees to represent DCMs. This approach is based on random forests, a\npopular machine learning algorithm. The resulting model is interpretable: the\ndecision trees can explain the decision-making process of customers during the\npurchase. We show that our approach can predict the choice probability of any\nDCM consistently and thus never suffers from misspecification. Moreover, our\nalgorithm predicts assortments unseen in the training data. The mechanism and\nerrors can be theoretically analyzed. We also prove that the random forest can\nrecover preference rankings of customers thanks to the splitting criterion such\nas the Gini index and information gain ratio. Managerial implications. The\nframework has unique practical advantages. It can capture customers' behavioral\npatterns such as irrationality or sequential searches when purchasing a\nproduct. It handles nonstandard formats of training data that result from\naggregation. It can measure product importance based on how frequently a random\ncustomer would make decisions depending on the presence of the product. It can\nalso incorporate price information and customer features. Our numerical\nexperiments using synthetic and real data show that using random forests to\nestimate customer choices can outperform existing methods.\n"", '  In machine learning (ML), ensemble methods such as bagging, boosting, and\nstacking are widely-established approaches that regularly achieve top-notch\npredictive performance. Stacking (also called ""stacked generalization"") is an\nensemble method that combines heterogeneous base models, arranged in at least\none layer, and then employs another metamodel to summarize the predictions of\nthose models. Although it may be a highly-effective approach for increasing the\npredictive performance of ML, generating a stack of models from scratch can be\na cumbersome trial-and-error process. This challenge stems from the enormous\nspace of available solutions, with different sets of data instances and\nfeatures that could be used for training, several algorithms to choose from,\nand instantiations of these algorithms using diverse parameters (i.e., models)\nthat perform differently according to various metrics. In this work, we present\na knowledge generation model, which supports ensemble learning with the use of\nvisualization, and a visual analytics system for stacked generalization. Our\nsystem, StackGenVis, assists users in dynamically adapting performance metrics,\nmanaging data instances, selecting the most important features for a given data\nset, choosing a set of top-performant and diverse algorithms, and measuring the\npredictive performance. In consequence, our proposed tool helps users to decide\nbetween distinct models and to reduce the complexity of the resulting stack by\nremoving overpromising and underperforming models. The applicability and\neffectiveness of StackGenVis are demonstrated with two use cases: a real-world\nhealthcare data set and a collection of data related to sentiment/stance\ndetection in texts. Finally, the tool has been evaluated through interviews\nwith three ML experts.\n']"
14,21,14_causal_predictive_interventions_inference,"['causal', 'predictive', 'interventions', 'inference', 'counterfactual', 'observational', 'generalization', 'regression', 'predictions', 'estimation']","['causal', 'effects', 'effect', 'inference', 'estimation', 'data', 'outcome', 'treatment', 'covariates', 'kernel']","[""  We merge computational mechanics' definition of causal states\n(predictively-equivalent histories) with reproducing-kernel Hilbert space\n(RKHS) representation inference. The result is a widely-applicable method that\ninfers causal structure directly from observations of a system's behaviors\nwhether they are over discrete or continuous events or time. A structural\nrepresentation -- a finite- or infinite-state kernel $\\epsilon$-machine -- is\nextracted by a reduced-dimension transform that gives an efficient\nrepresentation of causal states and their topology. In this way, the system\ndynamics are represented by a stochastic (ordinary or partial) differential\nequation that acts on causal states. We introduce an algorithm to estimate the\nassociated evolution operator. Paralleling the Fokker-Plank equation, it\nefficiently evolves causal-state distributions and makes predictions in the\noriginal data space via an RKHS functional mapping. We demonstrate these\ntechniques, together with their predictive abilities, on discrete-time,\ndiscrete-value infinite Markov-order processes generated by finite-state hidden\nMarkov models with (i) finite or (ii) uncountably-infinite causal states and\n(iii) continuous-time, continuous-value processes generated by thermally-driven\nchaotic flows. The method robustly estimates causal structure in the presence\nof varying external and measurement noise levels and for very high dimensional\ndata.\n"", '  Causal discovery, i.e., inferring underlying causal relationships from\nobservational data, has been shown to be highly challenging for AI systems. In\ntime series modeling context, traditional causal discovery methods mainly\nconsider constrained scenarios with fully observed variables and/or data from\nstationary time-series. We develop a causal discovery approach to handle a wide\nclass of non-stationary time-series that are conditionally stationary, where\nthe non-stationary behaviour is modeled as stationarity conditioned on a set of\n(possibly hidden) state variables. Named State-Dependent Causal Inference\n(SDCI), our approach is able to recover the underlying causal dependencies,\nprovably with fully-observed states and empirically with hidden states. The\nlatter is confirmed by experiments on synthetic linear system and nonlinear\nparticle interaction data, where SDCI achieves superior performance over\nbaseline causal discovery methods. Improved results over non-causal RNNs on\nmodeling NBA player movements demonstrate the potential of our method and\nmotivate the use of causality-driven methods for forecasting.\n', '  What is the difference of a prediction that is made with a causal model and a\nnon-causal model? Suppose we intervene on the predictor variables or change the\nwhole environment. The predictions from a causal model will in general work as\nwell under interventions as for observational data. In contrast, predictions\nfrom a non-causal model can potentially be very wrong if we actively intervene\non variables. Here, we propose to exploit this invariance of a prediction under\na causal model for causal inference: given different experimental settings (for\nexample various interventions) we collect all models that do show invariance in\ntheir predictive accuracy across settings and interventions. The causal model\nwill be a member of this set of models with high probability. This approach\nyields valid confidence intervals for the causal relationships in quite general\nscenarios. We examine the example of structural equation models in more detail\nand provide sufficient assumptions under which the set of causal predictors\nbecomes identifiable. We further investigate robustness properties of our\napproach under model misspecification and discuss possible extensions. The\nempirical properties are studied for various data sets, including large-scale\ngene perturbation experiments.\n']"
15,20,15_supervised_ai_classification_imaging,"['supervised', 'ai', 'classification', 'imaging', 'learning', 'segmentation', 'deep', 'images', 'trained', 'features']","['medical', 'segmentation', 'learning', 'deep', 'imaging', 'image', 'scans', 'images', 'classification', 'models']","['  Coronavirus, or COVID-19, is a hazardous disease that has endangered the\nhealth of many people around the world by directly affecting the lungs.\nCOVID-19 is a medium-sized, coated virus with a single-stranded RNA, and also\nhas one of the largest RNA genomes and is approximately 120 nm. The X-Ray and\ncomputed tomography (CT) imaging modalities are widely used to obtain a fast\nand accurate medical diagnosis. Identifying COVID-19 from these medical images\nis extremely challenging as it is time-consuming and prone to human errors.\nHence, artificial intelligence (AI) methodologies can be used to obtain\nconsistent high performance. Among the AI methods, deep learning (DL) networks\nhave gained popularity recently compared to conventional machine learning (ML).\nUnlike ML, all stages of feature extraction, feature selection, and\nclassification are accomplished automatically in DL models. In this paper, a\ncomplete survey of studies on the application of DL techniques for COVID-19\ndiagnostic and segmentation of lungs is discussed, concentrating on works that\nused X-Ray and CT images. Additionally, a review of papers on the forecasting\nof coronavirus prevalence in different parts of the world with DL is presented.\nLastly, the challenges faced in the detection of COVID-19 using DL techniques\nand directions for future research are discussed.\n', '  Learning from noisy labels is an important concern in plenty of real-world\nscenarios. Various approaches for this concern first make corrections\ncorresponding to potentially noisy-labeled instances, and then update\npredictive model with information of the made corrections. However, in specific\nareas, such as medical histopathology whole slide image analysis (MHWSIA), it\nis often difficult or impossible for experts to manually achieve the noisy-free\nground-truth labels which leads to labels with complex noise. This situation\nraises two more difficult problems: 1) the methodology of approaches making\ncorrections corresponding to potentially noisy-labeled instances has\nlimitations due to the complex noise existing in labels; and 2) the appropriate\nevaluation strategy for validation/testing is unclear because of the great\ndifficulty in collecting the noisy-free ground-truth labels. For the problem\n1), we present one-step abductive multi-target learning (OSAMTL) that imposes a\none-step logical reasoning upon machine learning via a multi-target learning\nprocedure to constrain the predictions of the learning model to be subject to\nour prior knowledge about the true target. For the problem 2), we propose a\nlogical assessment formula (LAF) that evaluates the logical rationality of the\noutputs of an approach by estimating the consistencies between the predictions\nof the learning model and the logical facts narrated from the results of the\none-step logical reasoning of OSAMTL. Based on the Helicobacter pylori (H.\npylori) segmentation task in MHWSIA, we show that OSAMTL enables the machine\nlearning model achieving logically more rational predictions, which is beyond\nvarious state-of-the-art approaches in handling complex noisy labels.\n', '  Recent studies have demonstrated the effectiveness of the combination of\nmachine learning and logical reasoning, including data-driven logical\nreasoning, knowledge driven machine learning and abductive learning, in\ninventing advanced technologies for different artificial intelligence\napplications. One-step abductive multi-target learning (OSAMTL), an approach\ninspired by abductive learning, via simply combining machine learning and\nlogical reasoning in a one-step balanced multi-target learning way, has as well\nshown its effectiveness in handling complex noisy labels of a single noisy\nsample in medical histopathology whole slide image analysis (MHWSIA). However,\nOSAMTL is not suitable for the situation where diverse noisy samples (DiNS) are\nprovided for a learning task. In this paper, giving definition of DiNS, we\npropose one-step abductive multi-target learning with DiNS (OSAMTL-DiNS) to\nexpand the original OSAMTL to handle complex noisy labels of DiNS. Applying\nOSAMTL-DiNS to tumour segmentation for breast cancer in MHWSIA, we show that\nOSAMTL-DiNS is able to enable various state-of-the-art approaches for learning\nfrom noisy labels to achieve more rational predictions. We released a model\npre-trained with OSAMTL-DiNS for tumour segmentation in HE-stained\npre-treatment biopsy images in breast cancer, which has been successfully\napplied as a pre-processing tool to extract tumour-associated stroma\ncompartment for predicting the pathological complete response to neoadjuvant\nchemotherapy in breast cancer.\n']"
16,18,16_cluster_clusters_clustering_dbscan,"['cluster', 'clusters', 'clustering', 'dbscan', 'dendrogram', 'groupings', 'algorithms', 'grouping', 'similarity', 'algorithm']","['clustering', 'clusters', 'means', 'kernel', 'density', 'cluster', 'data', 'algorithm', 'distance', 'method']","['  A recent proposal of data dependent similarity called Isolation\nKernel/Similarity has enabled SVM to produce better classification accuracy. We\nidentify shortcomings of using a tree method to implement Isolation Similarity;\nand propose a nearest neighbour method instead. We formally prove the\ncharacteristic of Isolation Similarity with the use of the proposed method. The\nimpact of Isolation Similarity on density-based clustering is studied here. We\nshow for the first time that the clustering performance of the classic\ndensity-based clustering algorithm DBSCAN can be significantly uplifted to\nsurpass that of the recent density-peak clustering algorithm DP. This is\nachieved by simply replacing the distance measure with the proposed\nnearest-neighbour-induced Isolation Similarity in DBSCAN, leaving the rest of\nthe procedure unchanged. A new type of clusters called mass-connected clusters\nis formally defined. We show that DBSCAN, which detects density-connected\nclusters, becomes one which detects mass-connected clusters, when the distance\nmeasure is replaced with the proposed similarity. We also provide the condition\nunder which mass-connected clusters can be detected, while density-connected\nclusters cannot.\n', '  Clustering non-Euclidean data is difficult, and one of the most used\nalgorithms besides hierarchical clustering is the popular algorithm\nPartitioning Around Medoids (PAM), also simply referred to as k-medoids. In\nEuclidean geometry the mean-as used in k-means-is a good estimator for the\ncluster center, but this does not hold for arbitrary dissimilarities. PAM uses\nthe medoid instead, the object with the smallest dissimilarity to all others in\nthe cluster. This notion of centrality can be used with any (dis-)similarity,\nand thus is of high relevance to many domains such as biology that require the\nuse of Jaccard, Gower, or more complex distances.\n  A key issue with PAM is its high run time cost. We propose modifications to\nthe PAM algorithm to achieve an O(k)-fold speedup in the second SWAP phase of\nthe algorithm, but will still find the same results as the original PAM\nalgorithm. If we slightly relax the choice of swaps performed (at comparable\nquality), we can further accelerate the algorithm by performing up to k swaps\nin each iteration. With the substantially faster SWAP, we can now also explore\nalternative strategies for choosing the initial medoids. We also show how the\nCLARA and CLARANS algorithms benefit from these modifications. It can easily be\ncombined with earlier approaches to use PAM and CLARA on big data (some of\nwhich use PAM as a subroutine, hence can immediately benefit from these\nimprovements), where the performance with high k becomes increasingly\nimportant.\n  In experiments on real data with k=100, we observed a 200-fold speedup\ncompared to the original PAM SWAP algorithm, making PAM applicable to larger\ndata sets as long as we can afford to compute a distance matrix, and in\nparticular to higher k (at k=2, the new SWAP was only 1.5 times faster, as the\nspeedup is expected to increase with k).\n', '  Clustering non-Euclidean data is difficult, and one of the most used\nalgorithms besides hierarchical clustering is the popular algorithm\nPartitioning Around Medoids (PAM), also simply referred to as k-medoids\nclustering. In Euclidean geometry the mean-as used in k-means-is a good\nestimator for the cluster center, but this does not exist for arbitrary\ndissimilarities. PAM uses the medoid instead, the object with the smallest\ndissimilarity to all others in the cluster. This notion of centrality can be\nused with any (dis-)similarity, and thus is of high relevance to many domains\nand applications. A key issue with PAM is its high run time cost. We propose\nmodifications to the PAM algorithm that achieve an O(k)-fold speedup in the\nsecond (""SWAP"") phase of the algorithm, but will still find the same results as\nthe original PAM algorithm. If we relax the choice of swaps performed (while\nretaining comparable quality), we can further accelerate the algorithm by\neagerly performing additional swaps in each iteration. With the substantially\nfaster SWAP, we can now explore faster initialization strategies, because (i)\nthe classic (""BUILD"") initialization now becomes the bottleneck, and (ii) our\nswap is fast enough to compensate for worse starting conditions. We also show\nhow the CLARA and CLARANS algorithms benefit from the proposed modifications.\nWhile we do not study the parallelization of our approach in this work, it can\neasily be combined with earlier approaches to use PAM and CLARA on big data\n(some of which use PAM as a subroutine, hence can immediately benefit from\nthese improvements), where the performance with high k becomes increasingly\nimportant. In experiments on real data with k=100,200, we observed a 458x\nrespectively 1191x speedup compared to the original PAM SWAP algorithm, making\nPAM applicable to larger data sets, and in particular to higher k.\n']"
17,18,17_quantum_quantumnas_qubit_qnns,"['quantum', 'quantumnas', 'qubit', 'qnns', 'pqc', 'qml', 'unitary', 'nisq', 'hamiltonians', 'learning']","['quantum', 'noise', 'classical', 'gates', 'gate', 'ansatz', 'computers', 'variational', 'networks', 'complexity']","['  Quantum noise is the key challenge in Noisy Intermediate-Scale Quantum (NISQ)\ncomputers. Previous work for mitigating noise has primarily focused on\ngate-level or pulse-level noise-adaptive compilation. However, limited research\nefforts have explored a higher level of optimization by making the quantum\ncircuits themselves resilient to noise.\n  We propose QuantumNAS, a comprehensive framework for noise-adaptive co-search\nof the variational circuit and qubit mapping. Variational quantum circuits are\na promising approach for constructing QML and quantum simulation. However,\nfinding the best variational circuit and its optimal parameters is challenging\ndue to the large design space and parameter training cost. We propose to\ndecouple the circuit search and parameter training by introducing a novel\nSuperCircuit. The SuperCircuit is constructed with multiple layers of\npre-defined parameterized gates and trained by iteratively sampling and\nupdating the parameter subsets (SubCircuits) of it. It provides an accurate\nestimation of SubCircuits performance trained from scratch. Then we perform an\nevolutionary co-search of SubCircuit and its qubit mapping. The SubCircuit\nperformance is estimated with parameters inherited from SuperCircuit and\nsimulated with real device noise models. Finally, we perform iterative gate\npruning and finetuning to remove redundant gates.\n  Extensively evaluated with 12 QML and VQE benchmarks on 14 quantum computers,\nQuantumNAS significantly outperforms baselines. For QML, QuantumNAS is the\nfirst to demonstrate over 95% 2-class, 85% 4-class, and 32% 10-class\nclassification accuracy on real QC. It also achieves the lowest eigenvalue for\nVQE tasks on H2, H2O, LiH, CH4, BeH2 compared with UCCSD. We also open-source\nTorchQuantum (https://github.com/mit-han-lab/torchquantum) for fast training of\nparameterized quantum circuits to facilitate future research.\n', '  Quantum machine learning -- and specifically Variational Quantum Algorithms\n(VQAs) -- offers a powerful, flexible paradigm for programming near-term\nquantum computers, with applications in chemistry, metrology, materials\nscience, data science, and mathematics. Here, one trains an ansatz, in the form\nof a parameterized quantum circuit, to accomplish a task of interest. However,\nchallenges have recently emerged suggesting that deep ansatzes are difficult to\ntrain, due to flat training landscapes caused by randomness or by hardware\nnoise. This motivates our work, where we present a variable structure approach\nto build ansatzes for VQAs. Our approach, called VAns (Variable Ansatz),\napplies a set of rules to both grow and (crucially) remove quantum gates in an\ninformed manner during the optimization. Consequently, VAns is ideally suited\nto mitigate trainability and noise-related issues by keeping the ansatz\nshallow. We employ VAns in the variational quantum eigensolver for condensed\nmatter and quantum chemistry applications, in the quantum autoencoder for data\ncompression and in unitary compilation problems showing successful results in\nall cases.\n', '  Parameterized Quantum Circuits (PQC) are promising towards quantum advantage\non near-term quantum hardware. However, due to the large quantum noises\n(errors), the performance of PQC models has a severe degradation on real\nquantum devices. Take Quantum Neural Network (QNN) as an example, the accuracy\ngap between noise-free simulation and noisy results on IBMQ-Yorktown for\nMNIST-4 classification is over 60%. Existing noise mitigation methods are\ngeneral ones without leveraging unique characteristics of PQC; on the other\nhand, existing PQC work does not consider noise effect. To this end, we present\nQuantumNAT, a PQC-specific framework to perform noise-aware optimizations in\nboth training and inference stages to improve robustness. We experimentally\nobserve that the effect of quantum noise to PQC measurement outcome is a linear\nmap from noise-free outcome with a scaling and a shift factor. Motivated by\nthat, we propose post-measurement normalization to mitigate the feature\ndistribution differences between noise-free and noisy scenarios. Furthermore,\nto improve the robustness against noise, we propose noise injection to the\ntraining process by inserting quantum error gates to PQC according to realistic\nnoise models of quantum hardware. Finally, post-measurement quantization is\nintroduced to quantize the measurement outcomes to discrete values, achieving\nthe denoising effect. Extensive experiments on 8 classification tasks using 6\nquantum devices demonstrate that QuantumNAT improves accuracy by up to 43%, and\nachieves over 94% 2-class, 80% 4-class, and 34% 10-class classification\naccuracy measured on real quantum computers. The code for construction and\nnoise-aware training of PQC is available in the TorchQuantum library.\n']"
18,15,18_supervised_classification_classifier_labels,"['supervised', 'classification', 'classifier', 'labels', 'label', 'unlabeled', 'learning', 'datasets', 'classified', 'mislabeled']","['labels', 'label', 'supervised', 'hierarchical', 'data', 'semi', 'learning', 'loss', 'information', 'patterns']","['  Classification is a major tool of statistics and machine learning. A\nclassification method first processes a training set of objects with given\nclasses (labels), with the goal of afterward assigning new objects to one of\nthese classes. When running the resulting prediction method on the training\ndata or on test data, it can happen that an object is predicted to lie in a\nclass that differs from its given label. This is sometimes called label bias,\nand raises the question whether the object was mislabeled. The proposed class\nmap reflects the probability that an object belongs to an alternative class,\nhow far it is from the other objects in its given class, and whether some\nobjects lie far from all classes. The goal is to visualize aspects of the\nclassification results to obtain insight in the data. The display is\nconstructed for discriminant analysis, the k-nearest neighbor classifier,\nsupport vector machines, logistic regression, and coupling pairwise\nclassifications. It is illustrated on several benchmark datasets, including\nsome about images and texts.\n', '  Semi-supervised learning is a model training method that uses both labeled\nand unlabeled data. This paper proposes a fully Bayes semi-supervised learning\nalgorithm that can be applied to any multi-category classification problem. We\nassume the labels are missing at random when using unlabeled data in a\nsemi-supervised setting. Suppose we have $K$ classes in the data. We assume\nthat the observations follow $K$ multivariate normal distributions depending on\ntheir true class labels after some common unknown transformation is applied to\neach component of the observation vector. The function is expanded in a\nB-splines series, and a prior is added to the coefficients. We consider a\nnormal prior on the coefficients and constrain the values to meet the normality\nand identifiability constraints requirement. The precision matrices of the\nGaussian distributions are given a conjugate Wishart prior, while the means are\ngiven the improper uniform prior. The resulting posterior is still\nconditionally conjugate, and the Gibbs sampler aided by a data-augmentation\ntechnique can thus be adopted. An extensive simulation study compares the\nproposed method with several other available methods. The proposed method is\nalso applied to real datasets on diagnosing breast cancer and classification of\nsignals. We conclude that the proposed method has a better prediction accuracy\nin various cases.\n', ""  Learning with supervision has achieved remarkable success in numerous\nartificial intelligence (AI) applications. In the current literature, by\nreferring to the properties of the labels prepared for the training dataset,\nlearning with supervision is categorized as supervised learning (SL) and weakly\nsupervised learning (WSL). SL concerns the situation where the training data\nset is assigned with ideal (complete, exact and accurate) labels, while WSL\nconcerns the situation where the training data set is assigned with non-ideal\n(incomplete, inexact or inaccurate) labels. However, various solutions for SL\ntasks have shown that the given labels are not always easy to learn, and the\ntransformation from the given labels to easy-to-learn targets can significantly\naffect the performance of the final SL solutions. Without considering the\nproperties of the transformation from the given labels to easy-to-learn\ntargets, the definition of SL conceals some details that can be critical to\nbuilding the appropriate solutions for specific SL tasks. Thus, for engineers\nin the AI application field, it is desirable to reveal these details\nsystematically. This article attempts to achieve this goal by expanding the\ncategorization of SL and investigating the sub-type moderately supervised\nlearning (MSL) that concerns the situation where the given labels are ideal,\nbut due to the simplicity in annotation, careful designs are required to\ntransform the given labels into easy-to-learn targets. From the perspectives of\nthe definition, framework and generality, we conceptualize MSL to present a\ncomplete fundamental basis to systematically analyse MSL tasks. At meantime,\nrevealing the relation between the conceptualization of MSL and the\nmathematicians' vision, this paper as well establishes a tutorial for AI\napplication engineers to refer to viewing a problem to be solved from the\nmathematicians' vision.\n""]"
19,15,19_predicting_prediction_forecasting_datasets,"['predicting', 'prediction', 'forecasting', 'datasets', 'models', 'dataset', 'forecast', 'learning', 'autoencoders', 'imbalance']","['strain', 'data', 'model', 'models', 'prediction', 'clinical', 'demand', 'forecasting', 'time', 'variables']","['  Brain strain and strain rate are effective in predicting traumatic brain\ninjury (TBI) caused by head impacts. However, state-of-the-art finite element\nmodeling (FEM) demands considerable computational time in the computation,\nlimiting its application in real-time TBI risk monitoring. To accelerate,\nmachine learning head models (MLHMs) were developed, and the model accuracy was\nfound to decrease when the training/test datasets were from different head\nimpacts types. However, the size of dataset for specific impact types may not\nbe enough for model training. To address the computational cost of FEM, the\nlimited strain rate prediction, and the generalizability of MLHMs to on-field\ndatasets, we propose data fusion and transfer learning to develop a series of\nMLHMs to predict the maximum principal strain (MPS) and maximum principal\nstrain rate (MPSR). We trained and tested the MLHMs on 13,623 head impacts from\nsimulations, American football, mixed martial arts, car crash, and compared\nagainst the models trained on only simulations or only on-field impacts. The\nMLHMs developed with transfer learning are significantly more accurate in\nestimating MPS and MPSR than other models, with a mean absolute error (MAE)\nsmaller than 0.03 in predicting MPS and smaller than 7 (1/s) in predicting MPSR\non all impact datasets. The MLHMs can be applied to various head impact types\nfor rapidly and accurately calculating brain strain and strain rate. Besides\nthe clinical applications in real-time brain strain and strain rate monitoring,\nthis model helps researchers estimate the brain strain and strain rate caused\nby head impacts more efficiently than FEM.\n', '  Background: Medical decision-making impacts both individual and public\nhealth. Clinical scores are commonly used among a wide variety of\ndecision-making models for determining the degree of disease deterioration at\nthe bedside. AutoScore was proposed as a useful clinical score generator based\non machine learning and a generalized linear model. Its current framework,\nhowever, still leaves room for improvement when addressing unbalanced data of\nrare events. Methods: Using machine intelligence approaches, we developed\nAutoScore-Imbalance, which comprises three components: training dataset\noptimization, sample weight optimization, and adjusted AutoScore. All scoring\nmodels were evaluated on the basis of their area under the curve (AUC) in the\nreceiver operating characteristic analysis and balanced accuracy (i.e., mean\nvalue of sensitivity and specificity). By utilizing a publicly accessible\ndataset from Beth Israel Deaconess Medical Center, we assessed the proposed\nmodel and baseline approaches in the prediction of inpatient mortality.\nResults: AutoScore-Imbalance outperformed baselines in terms of AUC and\nbalanced accuracy. The nine-variable AutoScore-Imbalance sub-model achieved the\nhighest AUC of 0.786 (0.732-0.839) while the eleven-variable original AutoScore\nobtained an AUC of 0.723 (0.663-0.783), and the logistic regression with 21\nvariables obtained an AUC of 0.743 (0.685-0.800). The AutoScore-Imbalance\nsub-model (using down-sampling algorithm) yielded an AUC of 0. 0.771\n(0.718-0.823) with only five variables, demonstrating a good balance between\nperformance and variable sparsity. Conclusions: The AutoScore-Imbalance tool\nhas the potential to be applied to highly unbalanced datasets to gain further\ninsight into rare medical events and to facilitate real-world clinical\ndecision-making.\n', ""  Scoring systems are highly interpretable and widely used to evaluate\ntime-to-event outcomes in healthcare research. However, existing time-to-event\nscores are predominantly created ad-hoc using a few manually selected variables\nbased on clinician's knowledge, suggesting an unmet need for a robust and\nefficient generic score-generating method.\n  AutoScore was previously developed as an interpretable machine learning score\ngenerator, integrated both machine learning and point-based scores in the\nstrong discriminability and accessibility. We have further extended it to\ntime-to-event data and developed AutoScore-Survival, for automatically\ngenerating time-to-event scores with right-censored survival data. Random\nsurvival forest provides an efficient solution for selecting variables, and Cox\nregression was used for score weighting. We illustrated our method in a\nreal-life study of 90-day mortality of patients in intensive care units and\ncompared its performance with survival models (i.e., Cox) and the random\nsurvival forest.\n  The AutoScore-Survival-derived scoring model was more parsimonious than\nsurvival models built using traditional variable selection methods (e.g.,\npenalized likelihood approach and stepwise variable selection), and its\nperformance was comparable to survival models using the same set of variables.\nAlthough AutoScore-Survival achieved a comparable integrated area under the\ncurve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores\ngenerated are favorable in clinical applications because they are easier to\ncompute and interpret.\n  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use\nmachine learning-based clinical score generator to studies of time-to-event\noutcomes. It provides a systematic guideline to facilitate the future\ndevelopment of time-to-event scores for clinical applications.\n""]"
20,15,20_robotic_robotics_articulated_grasp,"['robotic', 'robotics', 'articulated', 'grasp', 'robot', 'manipulators', 'robots', 'objects', 'motions', 'movements']","['object', 'manipulation', 'robot', 'objects', 'grasp', 'robots', 'motion', 'intent', 'dynamic', 'control']","['  Pushing is an essential non-prehensile manipulation skill used for tasks\nranging from pre-grasp manipulation to scene rearrangement, reasoning about\nobject relations in the scene, and thus pushing actions have been widely\nstudied in robotics. The effective use of pushing actions often requires an\nunderstanding of the dynamics of the manipulated objects and adaptation to the\ndiscrepancies between prediction and reality. For this reason, effect\nprediction and parameter estimation with pushing actions have been heavily\ninvestigated in the literature. However, current approaches are limited because\nthey either model systems with a fixed number of objects or use image-based\nrepresentations whose outputs are not very interpretable and quickly accumulate\nerrors. In this paper, we propose a graph neural network based framework for\neffect prediction and parameter estimation of pushing actions by modeling\nobject relations based on contacts or articulations. Our framework is validated\nboth in real and simulated environments containing different shaped multi-part\nobjects connected via different types of joints and objects with different\nmasses, and it outperforms image-based representations on physics prediction.\nOur approach enables the robot to predict and adapt the effect of a pushing\naction as it observes the scene. It can also be used for tool manipulation with\nnever-seen tools. Further, we demonstrate 6D effect prediction in the lever-up\naction in the context of robot-based hard-disk disassembly.\n', ""  This paper presents INVIGORATE, a robot system that interacts with human\nthrough natural language and grasps a specified object in clutter. The objects\nmay occlude, obstruct, or even stack on top of one another. INVIGORATE embodies\nseveral challenges: (i) infer the target object among other occluding objects,\nfrom input language expressions and RGB images, (ii) infer object blocking\nrelationships (OBRs) from the images, and (iii) synthesize a multi-step plan to\nask questions that disambiguate the target object and to grasp it successfully.\nWe train separate neural networks for object detection, for visual grounding,\nfor question generation, and for OBR detection and grasping. They allow for\nunrestricted object categories and language expressions, subject to the\ntraining datasets. However, errors in visual perception and ambiguity in human\nlanguages are inevitable and negatively impact the robot's performance. To\novercome these uncertainties, we build a partially observable Markov decision\nprocess (POMDP) that integrates the learned neural network modules. Through\napproximate POMDP planning, the robot tracks the history of observations and\nasks disambiguation questions in order to achieve a near-optimal sequence of\nactions that identify and grasp the target object. INVIGORATE combines the\nbenefits of model-based POMDP planning and data-driven deep learning.\nPreliminary experiments with INVIGORATE on a Fetch robot show significant\nbenefits of this integrated approach to object grasping in clutter with natural\nlanguage interactions. A demonstration video is available at\nhttps://youtu.be/zYakh80SGcU.\n"", '  A kitchen assistant needs to operate human-scale objects, such as cabinets\nand ovens, in unmapped environments with dynamic obstacles. Autonomous\ninteractions in such environments require integrating dexterous manipulation\nand fluid mobility. While mobile manipulators in different form factors provide\nan extended workspace, their real-world adoption has been limited. Executing a\nhigh-level task for general objects requires a perceptual understanding of the\nobject as well as adaptive whole-body control among dynamic obstacles. In this\npaper, we propose a two-stage architecture for autonomous interaction with\nlarge articulated objects in unknown environments. The first stage,\nobject-centric planner, only focuses on the object to provide an\naction-conditional sequence of states for manipulation using RGB-D data. The\nsecond stage, agent-centric planner, formulates the whole-body motion control\nas an optimal control problem that ensures safe tracking of the generated plan,\neven in scenes with moving obstacles. We show that the proposed pipeline can\nhandle complex static and dynamic kitchen settings for both wheel-based and\nlegged mobile manipulators. Compared to other agent-centric planners, our\nproposed planner achieves a higher success rate and a lower execution time. We\nalso perform hardware tests on a legged mobile manipulator to interact with\nvarious articulated objects in a kitchen. For additional material, please\ncheck: www.pair.toronto.edu/articulated-mm/.\n']"
21,14,21_activity_monitoring_activities_sensing,"['activity', 'monitoring', 'activities', 'sensing', 'wearable', 'sensor', 'sensors', 'iot', 'ecg', 'signals']","['activity', 'sensor', 'sleep', 'sensors', 'quality', 'recognition', 'signal', 'leads', 'signals', 'smart']","['  Human Activity Recognition from body-worn sensor data poses an inherent\nchallenge in capturing spatial and temporal dependencies of time-series\nsignals. In this regard, the existing recurrent or convolutional or their\nhybrid models for activity recognition struggle to capture spatio-temporal\ncontext from the feature space of sensor reading sequence. To address this\ncomplex problem, we propose a self-attention based neural network model that\nforegoes recurrent architectures and utilizes different types of attention\nmechanisms to generate higher dimensional feature representation used for\nclassification. We performed extensive experiments on four popular publicly\navailable HAR datasets: PAMAP2, Opportunity, Skoda and USC-HAD. Our model\nachieve significant performance improvement over recent state-of-the-art models\nin both benchmark test subjects and Leave-one-subject-out evaluation. We also\nobserve that the sensor attention maps produced by our model is able capture\nthe importance of the modality and placement of the sensors in predicting the\ndifferent activity classes.\n', ""  The quality of sleep has a deep impact on people's physical and mental\nhealth. People with insufficient sleep are more likely to report physical and\nmental distress, activity limitation, anxiety, and pain. Moreover, in the past\nfew years, there has been an explosion of applications and devices for activity\nmonitoring and health tracking. Signals collected from these wearable devices\ncan be used to study and improve sleep quality. In this paper, we utilize the\nrelationship between physical activity and sleep quality to find ways of\nassisting people improve their sleep using machine learning techniques. People\nusually have several behavior modes that their bio-functions can be divided\ninto. Performing time series clustering on activity data, we find cluster\ncenters that would correlate to the most evident behavior modes for a specific\nsubject. Activity recipes are then generated for good sleep quality for each\nbehavior mode within each cluster. These activity recipes are supplied to an\nactivity recommendation engine for suggesting a mix of relaxed to intense\nactivities to subjects during their daily routines. The recommendations are\nfurther personalized based on the subjects' lifestyle constraints, i.e. their\nage, gender, body mass index (BMI), resting heart rate, etc, with the objective\nof the recommendation being the improvement of that night's quality of sleep.\nThis would in turn serve a longer-term health objective, like lowering heart\nrate, improving the overall quality of sleep, etc.\n"", '  Wearable sensor based human activity recognition is a challenging problem due\nto difficulty in modeling spatial and temporal dependencies of sensor signals.\nRecognition models in closed-set assumption are forced to yield members of\nknown activity classes as prediction. However, activity recognition models can\nencounter an unseen activity due to body-worn sensor malfunction or disability\nof the subject performing the activities. This problem can be addressed through\nmodeling solution according to the assumption of open-set recognition. Hence,\nthe proposed self attention based approach combines data hierarchically from\ndifferent sensor placements across time to classify closed-set activities and\nit obtains notable performance improvement over state-of-the-art models on five\npublicly available datasets. The decoder in this autoencoder architecture\nincorporates self-attention based feature representations from encoder to\ndetect unseen activity classes in open-set recognition setting. Furthermore,\nattention maps generated by the hierarchical model demonstrate explainable\nselection of features in activity recognition. We conduct extensive leave one\nsubject out validation experiments that indicate significantly improved\nrobustness to noise and subject specific variability in body-worn sensor\nsignals. The source code is available at:\ngithub.com/saif-mahmud/hierarchical-attention-HAR\n']"
