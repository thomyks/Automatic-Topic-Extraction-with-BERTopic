,Topic,Count,Name,Representation,Aspect1,Representative_Docs
0,0,46,0_anomaly_anomalies_datasets_dataset,"['anomaly', 'anomalies', 'datasets', 'dataset', 'features', 'learning', 'neural', 'deep', 'detection', 'dnn']","['detection', 'data', 'model', 'learning', 'models', 'datasets', 'large', 'algorithms', 'results', 'use']","[""  Semi-supervised anomaly detection aims to detect anomalies from normal\nsamples using a model that is trained on normal data. With recent advancements\nin deep learning, researchers have designed efficient deep anomaly detection\nmethods. Existing works commonly use neural networks to map the data into a\nmore informative representation and then apply an anomaly detection algorithm.\nIn this paper, we propose a method, DASVDD, that jointly learns the parameters\nof an autoencoder while minimizing the volume of an enclosing hyper-sphere on\nits latent representation. We propose an anomaly score which is a combination\nof autoencoder's reconstruction error and the distance from the center of the\nenclosing hypersphere in the latent representation. Minimizing this anomaly\nscore aids us in learning the underlying distribution of the normal class\nduring training. Including the reconstruction error in the anomaly score\nensures that DASVDD does not suffer from the common hypersphere collapse issue\nsince the DASVDD model does not converge to the trivial solution of mapping all\ninputs to a constant point in the latent representation. Experimental\nevaluations on several benchmark datasets show that the proposed method\noutperforms the commonly used state-of-the-art anomaly detection algorithms\nwhile maintaining robust performance across different anomaly classes.\n"", '  In many real-world AD applications including computer security and fraud\nprevention, the anomaly detector must be configurable by the human analyst to\nminimize the effort on false positives. One important way to configure the\ndetector is by providing true labels (nominal or anomaly) for a few instances.\nRecent work on active anomaly discovery has shown that greedily querying the\ntop-scoring instance and tuning the weights of ensemble detectors based on\nlabel feedback allows us to quickly discover true anomalies.\n  This paper makes four main contributions to improve the state-of-the-art in\nanomaly discovery using tree-based ensembles. First, we provide an important\ninsight that explains the practical successes of unsupervised tree-based\nensembles and active learning based on greedy query selection strategy. We also\npresent empirical results on real-world data to support our insights and\ntheoretical analysis to support active learning. Second, we develop a novel\nbatch active learning algorithm to improve the diversity of discovered\nanomalies based on a formalism called compact description to describe the\ndiscovered anomalies. Third, we develop a novel active learning algorithm to\nhandle streaming data setting. We present a data drift detection algorithm that\nnot only detects the drift robustly, but also allows us to take corrective\nactions to adapt the anomaly detector in a principled manner. Fourth, we\npresent extensive experiments to evaluate our insights and our tree-based\nactive anomaly discovery algorithms in both batch and streaming data settings.\nOur results show that active learning allows us to discover significantly more\nanomalies than state-of-the-art unsupervised baselines, our batch active\nlearning algorithm discovers diverse anomalies, and our algorithms under the\nstreaming-data setup are competitive with the batch setup.\n', '  Anomaly detection is a challenging task for machine learning algorithms due\nto the inherent class imbalance. It is costly and time-demanding to manually\nanalyse the observed data, thus usually only few known anomalies if any are\navailable. Inspired by generative models and the analysis of the hidden\nactivations of neural networks, we introduce a novel unsupervised anomaly\ndetection method called DA3D. Here, we use adversarial autoencoders to generate\nanomalous counterexamples based on the normal data only. These artificial\nanomalies used during training allow the detection of real, yet unseen\nanomalies. With our novel generative approach, we transform the unsupervised\ntask of anomaly detection to a supervised one, which is more tractable by\nmachine learning and especially deep learning methods. DA3D surpasses the\nperformance of state-of-the-art anomaly detection methods in a purely\ndata-driven way, where no domain knowledge is required.\n']"
1,1,42,1_learning_regularization_classifiers_forgetting,"['learning', 'regularization', 'classifiers', 'forgetting', 'classification', 'learn', 'training', 'memory', 'trained', 'recognition']","['learning', 'tasks', 'task', 'training', 'methods', 'kernel', 'model', 'transfer', 'models', 'network']","['  In lifelong learning, data are used to improve performance not only on the\npresent task, but also on past and future (unencountered) tasks. While typical\ntransfer learning algorithms can improve performance on future tasks, their\nperformance on prior tasks degrades upon learning new tasks (called\nforgetting). Many recent approaches for continual or lifelong learning have\nattempted to maintain performance on old tasks given new tasks. But striving to\navoid forgetting sets the goal unnecessarily low. The goal of lifelong learning\nshould be to use data to improve performance on both future tasks (forward\ntransfer) and past tasks (backward transfer). In this paper, we show that a\nsimple approach -- representation ensembling -- demonstrates both forward and\nbackward transfer in a variety of simulated and benchmark data scenarios,\nincluding tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and\nFood1k), and speech (spoken digit), in contrast to various reference\nalgorithms, which typically failed to transfer either forward or backward, or\nboth. Moreover, our proposed approach can flexibly operate with or without a\ncomputational budget.\n', '  We study a fundamental transfer learning process from source to target linear\nregression tasks, including overparameterized settings where there are more\nlearned parameters than data samples. The target task learning is addressed by\nusing its training data together with the parameters previously computed for\nthe source task. We define a transfer learning approach to the target task as a\nlinear regression optimization with a regularization on the distance between\nthe to-be-learned target parameters and the already-learned source parameters.\nWe analytically characterize the generalization performance of our transfer\nlearning approach and demonstrate its ability to resolve the peak in\ngeneralization errors in double descent phenomena of the minimum L2-norm\nsolution to linear regression. Moreover, we show that for sufficiently related\ntasks, the optimally tuned transfer learning approach can outperform the\noptimally tuned ridge regression method, even when the true parameter vector\nconforms to an isotropic Gaussian prior distribution. Namely, we demonstrate\nthat transfer learning can beat the minimum mean square error (MMSE) solution\nof the independent target task. Our results emphasize the ability of transfer\nlearning to extend the solution space to the target task and, by that, to have\nan improved MMSE solution. We formulate the linear MMSE solution to our\ntransfer learning setting and point out its key differences from the common\ndesign philosophy to transfer learning.\n', '  There is a growing interest in the learning-to-learn paradigm, also known as\nmeta-learning, where models infer on new tasks using a few training examples.\nRecently, meta-learning based methods have been widely used in few-shot\nclassification, regression, reinforcement learning, and domain adaptation. The\nmodel-agnostic meta-learning (MAML) algorithm is a well-known algorithm that\nobtains model parameter initialization at meta-training phase. In the meta-test\nphase, this initialization is rapidly adapted to new tasks by using gradient\ndescent. However, meta-learning models are prone to overfitting since there are\ninsufficient training tasks resulting in over-parameterized models with poor\ngeneralization performance for unseen tasks. In this paper, we propose a\nBayesian neural network based MAML algorithm, which we refer to as the B-SMALL\nalgorithm. The proposed framework incorporates a sparse variational loss term\nalongside the loss function of MAML, which uses a sparsifying approximated KL\ndivergence as a regularizer. We demonstrate the performance of B-MAML using\nclassification and regression tasks, and highlight that training a sparsifying\nBNN using MAML indeed improves the parameter footprint of the model while\nperforming at par or even outperforming the MAML approach. We also illustrate\napplicability of our approach in distributed sensor networks, where sparsity\nand meta-learning can be beneficial.\n']"
2,2,41,2_classification_learning_boosting_prediction,"['classification', 'learning', 'boosting', 'prediction', 'complexity', 'optimal', 'algorithms', 'datasets', 'classes', 'accuracy']","['sample', 'learning', 'data', 'class', 'prediction', 'performance', 'methods', 'multi', 'problem', 'method']","[""  The goal of multi-task learning is to enable more efficient learning than\nsingle task learning by sharing model structures for a diverse set of tasks. A\nstandard multi-task learning objective is to minimize the average loss across\nall tasks. While straightforward, using this objective often results in much\nworse final performance for each task than learning them independently. A major\nchallenge in optimizing a multi-task model is the conflicting gradients, where\ngradients of different task objectives are not well aligned so that following\nthe average gradient direction can be detrimental to specific tasks'\nperformance. Previous work has proposed several heuristics to manipulate the\ntask gradients for mitigating this problem. But most of them lack convergence\nguarantee and/or could converge to any Pareto-stationary point. In this paper,\nwe introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the\naverage loss function, while leveraging the worst local improvement of\nindividual tasks to regularize the algorithm trajectory. CAGrad balances the\nobjectives automatically and still provably converges to a minimum over the\naverage loss. It includes the regular gradient descent (GD) and the multiple\ngradient descent algorithm (MGDA) in the multi-objective optimization (MOO)\nliterature as special cases. On a series of challenging multi-task supervised\nlearning and reinforcement learning tasks, CAGrad achieves improved performance\nover prior state-of-the-art multi-objective gradient manipulation methods.\n"", '  One of the central problems studied in the theory of machine learning is the\nquestion of whether, for a given class of hypotheses, it is possible to\nefficiently find a {consistent} hypothesis, i.e., which has zero training\nerror. While problems involving {\\em convex} hypotheses have been extensively\nstudied, the question of whether efficient learning is possible for non-convex\nhypotheses composed of possibly several disconnected regions is still less\nunderstood. Although it has been shown quite a while ago that efficient\nlearning of weakly convex hypotheses, a parameterized relaxation of convex\nhypotheses, is possible for the special case of Boolean functions, the question\nof whether this idea can be developed into a generic paradigm has not been\nstudied yet. In this paper, we provide a positive answer and show that the\nconsistent hypothesis finding problem can indeed be solved in polynomial time\nfor a broad class of weakly convex hypotheses over metric spaces. To this end,\nwe propose a general domain-independent algorithm for finding consistent weakly\nconvex hypotheses and prove sufficient conditions for its efficiency that\ncharacterize the corresponding hypothesis classes. To illustrate our general\nalgorithm and its properties, we discuss several non-trivial learning examples\nto demonstrate how it can be used to efficiently solve the corresponding\nconsistent hypothesis finding problem. Without the weak convexity constraint,\nthese problems are known to be computationally intractable. We then proceed to\nshow that the general idea of our algorithm can even be extended to the case of\nextensional weakly convex hypotheses, as it naturally arise, e.g., when\nperforming vertex classification in graphs. We prove that using our extended\nalgorithm, the problem can be solved in polynomial time provided the distances\nin the domain can be computed efficiently.\n', '  Objective: Provide guidance on sample size considerations for developing\npredictive models by empirically establishing the adequate sample size, which\nbalances the competing objectives of improving model performance and reducing\nmodel complexity as well as computational requirements.\n  Materials and Methods: We empirically assess the effect of sample size on\nprediction performance and model complexity by generating learning curves for\n81 prediction problems (23 outcomes predicted in a depression cohort, 58\noutcomes predicted in a hypertension cohort) in three large observational\nhealth databases, requiring training of 17,248 prediction models. The adequate\nsample size was defined as the sample size for which the performance of a model\nequalled the maximum model performance minus a small threshold value.\n  Results: The adequate sample size achieves a median reduction of the number\nof observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,\n0.005, 0.01, and 0.02, respectively. The median reduction of the number of\npredictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds\nof 0.001, 0.005, 0.01, and 0.02, respectively.\n  Discussion: Based on our results a conservative, yet significant, reduction\nin sample size and model complexity can be estimated for future prediction\nwork. Though, if a researcher is willing to generate a learning curve a much\nlarger reduction of the model complexity may be possible as suggested by a\nlarge outcome-dependent variability.\n  Conclusion: Our results suggest that in most cases only a fraction of the\navailable data was sufficient to produce a model close to the performance of\none developed on the full data set, but with a substantially reduced model\ncomplexity.\n']"
3,3,39,3_manifold_models_gaussian_dimensional,"['manifold', 'models', 'gaussian', 'dimensional', 'sparse', 'data', 'modeling', 'multivariate', 'learning', 'estimation']","['data', 'manifold', 'model', 'operators', 'models', 'space', 'points', 'problem', 'structure', 'likelihood']","['  This paper is concerned with the statistical analysis of matrix-valued time\nseries. These are data collected over a network of sensors (typically a set of\nspatial locations) along time, where a vector of features is observed per time\ninstant per sensor. Thus each sensor is characterized by a vectorial time\nseries. We would like to identify the dependency structure among these sensors\nand represent it by a graph. When there is only one feature per sensor, the\nvector auto-regressive models have been widely adapted to infer the structure\nof Granger causality. The resulting graph is referred to as causal graph. Our\nfirst contribution is then extending VAR models to matrix-variate models to\nserve the purpose of graph learning. Secondly, we propose two online procedures\nrespectively in low and high dimensions, which can update quickly the estimates\nof coefficients when new samples arrive. In particular in high dimensional\nregime, a novel Lasso-type is introduced and we develop its homotopy algorithms\nfor the online learning. We also provide an adaptive tuning procedure for the\nregularization parameter. Lastly, we consider that, the application of AR\nmodels onto data usually requires detrending the raw data, however, this step\nis forbidden in online context. Therefore, we augment the proposed AR models by\nincorporating trend as extra parameter, and then adapt the online algorithms to\nthe augmented data models, which allow us to simultaneously learn the graph and\ntrend from streaming samples. In this work, we consider primarily the periodic\ntrend. Numerical experiments using both synthetic and real data are performed,\nwhose results support the effectiveness of the proposed methods.\n', '  In analyzing complex datasets, it is often of interest to infer lower\ndimensional structure underlying the higher dimensional observations. As a\nflexible class of nonlinear structures, it is common to focus on Riemannian\nmanifolds. Most existing manifold learning algorithms replace the original data\nwith lower dimensional coordinates without providing an estimate of the\nmanifold in the observation space or using the manifold to denoise the original\ndata. This article proposes a new methodology for addressing these problems,\nallowing interpolation of the estimated manifold between fitted data points.\nThe proposed approach is motivated by novel theoretical properties of local\ncovariance matrices constructed from noisy samples on a manifold. Our results\nenable us to turn a global manifold reconstruction problem into a local\nregression problem, allowing application of Gaussian processes for\nprobabilistic manifold reconstruction. In addition to theory justifying the\nalgorithm, we provide simulated and real data examples to illustrate the\nperformance.\n', ""  We present a novel approach for data-driven modeling of the time-domain\ninduced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs\nare Bayesian neural networks that aim to learn a latent statistical\ndistribution to encode extensive data sets as lower dimension representations.\nWe collected 1 600 319 IP decay curves in various regions of Canada, the United\nStates and Kazakhstan, and compiled them to train a deep VAE. The proposed deep\nlearning approach is strictly unsupervised and data-driven: it does not require\nmanual processing or ground truth labeling of IP data. Moreover, our VAE\napproach avoids the pitfalls of IP parametrization with the empirical Cole-Cole\nand Debye decomposition models, simple power-law models, or other sophisticated\nmechanistic models. We demonstrate four applications of VAEs to model and\nprocess IP data: (1) representative synthetic data generation, (2) unsupervised\nBayesian denoising and data uncertainty estimation, (3) quantitative evaluation\nof the signal-to-noise ratio, and (4) automated outlier detection. We also\ninterpret the IP compilation's latent representation and reveal a strong\ncorrelation between its first dimension and the average chargeability of IP\ndecays. Finally, we experiment with varying VAE latent space dimensions and\ndemonstrate that a single real-valued scalar parameter contains sufficient\ninformation to encode our extensive IP data compilation. This new finding\nsuggests that modeling time-domain IP data using mathematical models governed\nby more than one free parameter is ambiguous, whereas modeling only the average\nchargeability is justified. A pre-trained implementation of our model --\nreadily applicable to new IP data from any geolocation -- is available as\nopen-source Python code for the applied geophysics community.\n""]"
4,4,29,4_predicting_datasets_supervised_dataset,"['predicting', 'datasets', 'supervised', 'dataset', 'prediction', 'data', 'predict', 'temporal', 'models', 'forecasting']","['time', 'data', 'learning', 'clinical', 'social', 'information', 'temporal', 'series', 'machine', 'neural']","['  We present a neural network framework for learning a survival model to\npredict a time-to-event outcome while simultaneously learning a topic model\nthat reveals feature relationships. In particular, we model each subject as a\ndistribution over ""topics"", where a topic could, for instance, correspond to an\nage group, a disorder, or a disease. The presence of a topic in a subject means\nthat specific clinical features are more likely to appear for the subject.\nTopics encode information about related features and are learned in a\nsupervised manner to predict a time-to-event outcome. Our framework supports\ncombining many different topic and survival models; training the resulting\njoint survival-topic model readily scales to large datasets using standard\nneural net optimizers with minibatch gradient descent. For example, a special\ncase is to combine LDA with a Cox model, in which case a subject\'s distribution\nover topics serves as the input feature vector to the Cox model. We explain how\nto address practical implementation issues that arise when applying these\nneural survival-supervised topic models to clinical data, including how to\nvisualize results to assist clinical interpretation. We study the effectiveness\nof our proposed framework on seven clinical datasets on predicting time until\ndeath as well as hospital ICU length of stay, where we find that neural\nsurvival-supervised topic models achieve competitive accuracy with existing\napproaches while yielding interpretable clinical topics that explain feature\nrelationships. Our code is available at:\nhttps://github.com/georgehc/survival-topics\n', '  The individual data collected throughout patient follow-up constitute crucial\ninformation for assessing the risk of a clinical event, and eventually for\nadapting a therapeutic strategy. Joint models and landmark models have been\nproposed to compute individual dynamic predictions from repeated measures to\none or two markers. However, they hardly extend to the case where the complete\npatient history includes much more repeated markers possibly. Our objective was\nthus to propose a solution for the dynamic prediction of a health event that\nmay exploit repeated measures of a possibly large number of markers. We\ncombined a landmark approach extended to endogenous markers history with\nmachine learning methods adapted to survival data. Each marker trajectory is\nmodeled using the information collected up to landmark time, and summary\nvariables that best capture the individual trajectories are derived. These\nsummaries and additional covariates are then included in different prediction\nmethods. To handle a possibly large dimensional history, we rely on machine\nlearning methods adapted to survival data, namely regularized regressions and\nrandom survival forests, to predict the event from the landmark time, and we\nshow how they can be combined into a superlearner. Then, the performances are\nevaluated by cross-validation using estimators of Brier Score and the area\nunder the Receiver Operating Characteristic curve adapted to censored data. We\ndemonstrate in a simulation study the benefits of machine learning survival\nmethods over standard survival models, especially in the case of numerous\nand/or nonlinear relationships between the predictors and the event. We then\napplied the methodology in two prediction contexts: a clinical context with the\nprediction of death for patients with primary biliary cholangitis, and a public\nhealth context with the prediction of death in the general elderly population\nat different ages. Our methodology, implemented in R, enables the prediction of\nan event using the entire longitudinal patient history, even when the number of\nrepeated markers is large. Although introduced with mixed models for the\nrepeated markers and methods for a single right censored time-to-event, our\nmethod can be used with any other appropriate modeling technique for the\nmarkers and can be easily extended to competing risks setting.\n', ""  Scoring systems are highly interpretable and widely used to evaluate\ntime-to-event outcomes in healthcare research. However, existing time-to-event\nscores are predominantly created ad-hoc using a few manually selected variables\nbased on clinician's knowledge, suggesting an unmet need for a robust and\nefficient generic score-generating method.\n  AutoScore was previously developed as an interpretable machine learning score\ngenerator, integrated both machine learning and point-based scores in the\nstrong discriminability and accessibility. We have further extended it to\ntime-to-event data and developed AutoScore-Survival, for automatically\ngenerating time-to-event scores with right-censored survival data. Random\nsurvival forest provides an efficient solution for selecting variables, and Cox\nregression was used for score weighting. We illustrated our method in a\nreal-life study of 90-day mortality of patients in intensive care units and\ncompared its performance with survival models (i.e., Cox) and the random\nsurvival forest.\n  The AutoScore-Survival-derived scoring model was more parsimonious than\nsurvival models built using traditional variable selection methods (e.g.,\npenalized likelihood approach and stepwise variable selection), and its\nperformance was comparable to survival models using the same set of variables.\nAlthough AutoScore-Survival achieved a comparable integrated area under the\ncurve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores\ngenerated are favorable in clinical applications because they are easier to\ncompute and interpret.\n  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use\nmachine learning-based clinical score generator to studies of time-to-event\noutcomes. It provides a systematic guideline to facilitate the future\ndevelopment of time-to-event scores for clinical applications.\n""]"
5,5,27,5_neural_dynamics_networks_dynamical,"['neural', 'dynamics', 'networks', 'dynamical', 'neuronal', 'trajectories', 'robots', 'robotic', 'models', 'learning']","['networks', 'dynamics', 'systems', 'learning', 'neural', 'control', 'dynamical', 'network', 'developmental', 'behaviors']","['  There is much to learn through synthesis of Developmental Biology, Cognitive\nScience and Computational Modeling. Our path forward involves a design for\ndevelopmentally-inspired learning agents based on Braitenberg Vehicles.\nContinual developmental neurosimulation allows us to consider the role of\ndevelopmental trajectories in bridging the related phenomena of nervous system\nmorphogenesis, developmental learning, and plasticity. Being closely tied to\ncontinual learning, our approach is tightly integrated with developmental\nembodiment, and can be implemented using a type of agent called developmental\nBraitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined\nstructures that transform into agent-based systems including a body, sensors,\neffectors, and nervous system. This phenotype is characterized in terms of\ndevelopmental timing: with distinct morphogenetic, critical, and acquisition\n(developmental learning) periods. We further propose that network morphogenesis\ncan be accomplished using a genetic algorithmic approach, while developmental\nlearning can be implemented using a number of computational methodologies. This\napproach provides a framework for adaptive agent behavior that might result\nfrom a developmental approach: namely by exploiting critical periods or growth\nand acquisition, an explicitly embodied network architecture, and a distinction\nbetween the assembly of neuronal networks and active learning on these\nnetworks. In conclusion, we will consider agent learning and development at\ndifferent timescales, from very short (<100ms) intervals to long-term\nevolution. The development, evolution, and learning in an embodied agent-based\napproach is key to an integrative view of biologically-inspired intelligence.\n', '  Modeling of non-rigid object launching and manipulation is complex\nconsidering the wide range of dynamics affecting trajectory, many of which may\nbe unknown. Using physics models can be inaccurate because they cannot account\nfor unknown factors and the effects of the deformation of the object as it is\nlaunched; moreover, deriving force coefficients for these models is not\npossible without extensive experimental testing. Recently, advancements in\ndata-powered artificial intelligence methods have allowed learnable models and\nsystems to emerge. It is desirable to train a model for launch prediction on a\nrobot, as deep neural networks can account for immeasurable dynamics. However,\nthe inability to collect large amounts of experimental data decreases\nperformance of deep neural networks. Through estimating force coefficients, the\naccepted physics models can be leveraged to produce adequate supplemental data\nto artificially increase the size of the training set, yielding improved neural\nnetworks. In this paper, we introduce a new framework for algorithmic\nestimation of force coefficients for non-rigid object launching, which can be\ngeneralized to other domains, in order to generate large datasets. We implement\na novel training algorithm and objective for our deep neural network to\naccurately model launch trajectory of non-rigid objects and predict whether\nthey will hit a series of targets. Our experimental results demonstrate the\neffectiveness of using simulated data from force coefficient estimation and\nshows the importance of simulated data for training an effective neural\nnetwork.\n', ""  It has long been believed that the brain is highly modular both in terms of\nstructure and function, although recent evidence has led some to question the\nextent of both types of modularity. We used artificial neural networks to test\nthe hypothesis that structural modularity is sufficient to guarantee functional\nspecialization, and find that in general, this doesn't necessarily hold. We\nthen systematically tested which features of the environment and network do\nlead to the emergence of specialization. We used a simple toy environment, task\nand network, allowing us precise control, and show that in this setup, several\ndistinct measures of specialization give qualitatively similar results. We\nfurther find that in this setup (1) specialization can only emerge in\nenvironments where features of that environment are meaningfully separable, (2)\nspecialization preferentially emerges when the network is strongly\nresource-constrained, and (3) these findings are qualitatively similar across\nthe different variations of network architectures that we tested, but that the\nquantitative relationships depend on the precise architecture. Finally, we show\nthat functional specialization varies dynamically across time, and demonstrate\nthat these dynamics depend on both the timing and bandwidth of information flow\nin the network. We conclude that a static notion of specialization, based on\nstructural modularity, is likely too simple a framework for understanding\nintelligence in situations of real-world complexity, from biology to\nbrain-inspired neuromorphic systems. We propose that thoroughly stress testing\ncandidate definitions of functional modularity in simplified scenarios before\nextending to more complex data, network models and electrophysiological\nrecordings is likely to be a fruitful approach.\n""]"
6,6,25,6_optimization_optimisation_constraints_constraint,"['optimization', 'optimisation', 'constraints', 'constraint', 'scheduling', 'algorithms', 'planning', 'algorithm', 'search', 'benchmark']","['problem', 'optimization', 'problems', 'algorithms', 'approach', 'constraints', 'learning', 'solutions', 'algorithm', 'search']","[""  Molecule optimization is a fundamental task for accelerating drug discovery,\nwith the goal of generating new valid molecules that maximize multiple drug\nproperties while maintaining similarity to the input molecule. Existing\ngenerative models and reinforcement learning approaches made initial success,\nbut still face difficulties in simultaneously optimizing multiple drug\nproperties. To address such challenges, we propose the MultI-constraint\nMOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule\nas an initial guess and sample molecules from the target distribution. MIMOSA\nfirst pretrains two property agnostic graph neural networks (GNNs) for molecule\ntopology and substructure-type prediction, where a substructure can be either\natom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and\nemploys three basic substructure operations (add, replace, delete) to generate\nnew molecules and associated weights. The weights can encode multiple\nconstraints including similarity and drug property constraints, upon which we\nselect promising molecules for next iteration. MIMOSA enables flexible encoding\nof multiple property- and similarity-constraints and can efficiently generate\nnew molecules that satisfy various property constraints and achieved up to\n49.6% relative improvement over the best baseline in terms of success rate. The\ncode repository (including readme file, data preprocessing and model\nconstruction, evaluation) is available https://github.com/futianfan/MIMOSA.\n"", '  Single-objective bilevel optimization is a specialized form of constraint\noptimization problems where one of the constraints is an optimization problem\nitself. These problems are typically non-convex and strongly NP-Hard. Recently,\nthere has been an increased interest from the evolutionary computation\ncommunity to model bilevel problems due to its applicability in the real-world\napplications for decision-making problems. In this work, a partial nested\nevolutionary approach with a local heuristic search has been proposed to solve\nthe benchmark problems and have outstanding results. This approach relies on\nthe concept of intermarriage-crossover in search of feasible regions by\nexploiting information from the constraints. A new variant has also been\nproposed to the commonly used convergence approaches, i.e., optimistic and\npessimistic. It is called extreme optimistic approach. The experimental results\ndemonstrate the algorithm converges differently to known optimum solutions with\nthe optimistic variants. Optimistic approach also outperforms pessimistic\napproach. Comparative statistical analysis of our approach with other recently\npublished partial to complete evolutionary approaches demonstrates very\ncompetitive results.\n', ""  Many science and engineering applications require finding solutions to\nplanning and optimization problems by satisfying a set of constraints. These\nconstraint problems (CPs) are typically NP-complete and can be formalized as\nconstraint satisfaction problems (CSPs) or constraint optimization problems\n(COPs). Evolutionary algorithms (EAs) are good solvers for optimization\nproblems ubiquitous in various problem domains, however traditional operators\nfor EAs are 'blind' to constraints or generally use problem dependent objective\nfunctions; as they do not exploit information from the constraints in search\nfor solutions. A variation of EA, Intelligent constraint handling evolutionary\nalgorithm (ICHEA), has been demonstrated to be a versatile constraints-guided\nEA for continuous constrained problems in our earlier works in (Sharma and\nSharma, 2012) where it extracts information from constraints and exploits it in\nthe evolutionary search to make the search more efficient. In this paper ICHEA\nhas been demonstrated to solve benchmark exam timetabling problems, a classic\nCOP. The presented approach demonstrates competitive results with other\nstate-of-the-art approaches in EAs in terms of quality of solutions. ICHEA\nfirst uses its inter-marriage crossover operator to satisfy all the given\nconstraints incrementally and then uses combination of traditional and enhanced\noperators to optimize the solution. Generally CPs solved by EAs are problem\ndependent penalty based fitness functions. We also proposed a generic\npreference based solution model that does not require a problem dependent\nfitness function, however currently it only works for mutually exclusive\nconstraints.\n""]"
7,7,24,7_imaging_cnn_convolutional_neural,"['imaging', 'cnn', 'convolutional', 'neural', 'deep', 'learning', 'scatter', '3d', 'scattering', 'images']","['data', 'mass', 'neural', 'network', 'imaging', 'wave', 'material', 'materials', 'physics', 'methods']","['  Scatter due to interaction of photons with the imaged object is a fundamental\nproblem in X-ray Computed Tomography (CT). It manifests as various artifacts in\nthe reconstruction, making its abatement or correction critical for image\nquality. Despite success in specific settings, hardware-based methods require\nmodification in the hardware, or increase in the scan time or dose. This\naccounts for the great interest in software-based methods, including\nMonte-Carlo based scatter estimation, analytical-numerical, and kernel-based\nmethods, with data-driven learning-based approaches demonstrated recently. In\nthis work, two novel physics-inspired deep-learning-based methods, PhILSCAT and\nOV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in\nthe acquired projection measurements. Different from previous works, they\nincorporate both an initial reconstruction of the object of interest and the\nscatter-corrupted measurements related to it, and use a deep neural network\narchitecture and cost function, both specifically tailored to the problem.\nNumerical experiments with data generated by Monte-Carlo simulations of the\nimaging of phantoms reveal consistent improvement over a recent purely\nprojection-domain deep neural network scatter correction method.\n', ""  Modelling the impact of a material's mesostructure on device level\nperformance typically requires access to 3D image data containing all the\nrelevant information to define the geometry of the simulation domain. This\nimage data must include sufficient contrast between phases to distinguish each\nmaterial, be of high enough resolution to capture the key details, but also\nhave a large enough field-of-view to be representative of the material in\ngeneral. It is rarely possible to obtain data with all of these properties from\na single imaging technique. In this paper, we present a method for combining\ninformation from pairs of distinct but complementary imaging techniques in\norder to accurately reconstruct the desired multi-phase, high resolution,\nrepresentative, 3D images. Specifically, we use deep convolutional generative\nadversarial networks to implement super-resolution, style transfer and\ndimensionality expansion. To demonstrate the widespread applicability of this\ntool, two pairs of datasets are used to validate the quality of the volumes\ngenerated by fusing the information from paired imaging techniques. Three key\nmesostructural metrics are calculated in each case to show the accuracy of this\nmethod. Having confidence in the accuracy of our method, we then demonstrate\nits power by applying to a real data pair from a lithium ion battery electrode,\nwhere the required 3D high resolution image data is not available anywhere in\nthe literature. We believe this approach is superior to previously reported\nstatistical material reconstruction methods both in terms of its fidelity and\nease of use. Furthermore, much of the data required to train this algorithm\nalready exists in the literature, waiting to be combined. As such, our\nopen-access code could precipitate a step change by generating the hard to\nobtain high quality image volumes necessary to simulate behaviour at the\nmesoscale.\n"", '  Phase retrieval, the problem of recovering lost phase information from\nmeasured intensity alone, is an inverse problem that is widely faced in various\nimaging modalities ranging from astronomy to nanoscale imaging. The current\nprocess of phase recovery is iterative in nature. As a result, the image\nformation is time-consuming and computationally expensive, precluding real-time\nimaging. Here, we use 3D nanoscale X-ray imaging as a representative example to\ndevelop a deep learning model to address this phase retrieval problem. We\nintroduce 3D-CDI-NN, a deep convolutional neural network and differential\nprogramming framework trained to predict 3D structure and strain solely from\ninput 3D X-ray coherent scattering data. Our networks are designed to be\n""physics-aware"" in multiple aspects; in that the physics of x-ray scattering\nprocess is explicitly enforced in the training of the network, and the training\ndata are drawn from atomistic simulations that are representative of the\nphysics of the material. We further refine the neural network prediction\nthrough a physics-based optimization procedure to enable maximum accuracy at\nlowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction\npattern to real-space structure and strain hundreds of times faster than\ntraditional iterative phase retrieval methods, with negligible loss in\naccuracy. Our integrated machine learning and differential programming solution\nto the phase retrieval problem is broadly applicable across inverse problems in\nother application areas.\n']"
8,8,19,8_ai_robots_intelligence_autonomous,"['ai', 'robots', 'intelligence', 'autonomous', 'robot', 'agent', 'intelligent', 'cognitive', 'communication', 'agents']","['agent', 'human', 'agents', 'systems', 'intelligence', 'language', 'goals', 'cognitive', 'research', 'behaviour']","['  The following briefly discusses possible difficulties in communication with\nand control of an AGI (artificial general intelligence), building upon an\nexplanation of The Fermi Paradox and preceding work on symbol emergence and\nartificial general intelligence. The latter suggests that to infer what someone\nmeans, an agent constructs a rationale for the observed behaviour of others.\nCommunication then requires two agents labour under similar compulsions and\nhave similar experiences (construct similar solutions to similar tasks). Any\nnon-human intelligence may construct solutions such that any rationale for\ntheir behaviour (and thus the meaning of their signals) is outside the scope of\nwhat a human is inclined to notice or comprehend. Further, the more compressed\na signal, the closer it will appear to random noise. Another intelligence may\npossess the ability to compress information to the extent that, to us, their\nsignals would appear indistinguishable from noise (an explanation for The Fermi\nParadox). To facilitate predictive accuracy an AGI would tend to more\ncompressed representations of the world, making any rationale for their\nbehaviour more difficult to comprehend for the same reason. Communication with\nand control of an AGI may subsequently necessitate not only human-like\ncompulsions and experiences, but imposed cognitive impairment.\n', ""  As computational power has continued to increase, and sensors have become\nmore accurate, the corresponding advent of systems that are at once cognitive\nand immersive has arrived. These \\textit{cognitive and immersive systems}\n(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems\ninteract with and assist the human agents that enter them, in no small part\nbecause such systems are infused with AI able to understand and reason about\nthese humans and their knowledge, beliefs, goals, communications, plans, etc.\nWe herein explain our approach to engineering CAISs. We emphasize the capacity\nof a CAIS to develop and reason over a `theory of the mind' of its human\npartners. This capacity entails that the AI in question has a sophisticated\nmodel of the beliefs, knowledge, goals, desires, emotions, etc.\\ of these\nhumans. To accomplish this engineering, a formal framework of very high\nexpressivity is needed. In our case, this framework is a \\textit{cognitive\nevent calculus}, a particular kind of quantified multi-operator modal logic,\nand a matching high-expressivity automated reasoner and planner. To explain,\nadvance, and to a degree validate our approach, we show that a calculus of this\ntype satisfies a set of formal requirements, and can enable a CAIS to\nunderstand a psychologically tricky scenario couched in what we call the\n\\textit{cognitive polysolid framework} (CPF). We also formally show that a room\nthat satisfies these requirements can have a useful property we term\n\\emph{expectation of usefulness}. CPF, a sub-class of \\textit{cognitive\nmicroworlds}, includes machinery able to represent and plan over not merely\nblocks and actions (such as seen in the primitive `blocks worlds' of old), but\nalso over agents and their mental attitudes about both other agents and\ninanimate objects.\n"", ""  In order to construct an ethical artificial intelligence (AI) two complex\nproblems must be overcome. Firstly, humans do not consistently agree on what is\nor is not ethical. Second, contemporary AI and machine learning methods tend to\nbe blunt instruments which either search for solutions within the bounds of\npredefined rules, or mimic behaviour. An ethical AI must be capable of\ninferring unspoken rules, interpreting nuance and context, possess and be able\nto infer intent, and explain not just its actions but its intent. Using\nenactivism, semiotics, perceptual symbol systems and symbol emergence, we\nspecify an agent that learns not just arbitrary relations between signs but\ntheir meaning in terms of the perceptual states of its sensorimotor system.\nSubsequently it can learn what is meant by a sentence and infer the intent of\nothers in terms of its own experiences. It has malleable intent because the\nmeaning of symbols changes as it learns, and its intent is represented\nsymbolically as a goal. As such it may learn a concept of what is most likely\nto be considered ethical by the majority within a population of humans, which\nmay then be used as a goal. The meaning of abstract symbols is expressed using\nperceptual symbols of raw sensorimotor stimuli as the weakest (consistent with\nOckham's Razor) necessary and sufficient concept, an intensional definition\nlearned from an ostensive definition, from which the extensional definition or\ncategory of all ethical decisions may be obtained. Because these abstract\nsymbols are the same for both situation and response, the same symbol is used\nwhen either performing or observing an action. This is akin to mirror neurons\nin the human brain. Mirror symbols may allow the agent to empathise, because\nits own experiences are associated with the symbol, which is also associated\nwith the observation of another agent experiencing something that symbol\nrepresents.\n""]"
