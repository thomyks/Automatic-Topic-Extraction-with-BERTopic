,id,submitter,authors,title,comments,journal-ref,doi,report-no,categories,license,text,versions,update_date,authors_parsed,Topic Label,First_Step_Topic_Name,First_Step_Topic_Keywords,First_Step_Topic_Representation,First_Step_Representative_Docs
0,1203.055,Afshin Rostamizadeh,"Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh",Algorithms for Learning Kernels Based on Centered Alignment,,Journal of Machine Learning Research 13 (2012) 795-828,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents new and effective algorithms for learning kernels. In
particular, as shown by our empirical results, these algorithms consistently
outperform the so-called uniform combination solution that has proven to be
difficult to improve upon in the past, as well as other algorithms for learning
kernels based on convex combinations of base kernels in both classification and
regression. Our algorithms are based on the notion of centered alignment which
is used as a similarity measure between kernels or kernel matrices. We present
a number of novel algorithmic, theoretical, and empirical results for learning
kernels based on our notion of centered alignment. In particular, we describe
efficient algorithms for learning a maximum alignment kernel by showing that
the problem can be reduced to a simple QP and discuss a one-stage algorithm for
learning both a kernel and a hypothesis based on that kernel using an
alignment-based regularization. Our theoretical results include a novel
concentration bound for centered alignment between kernel matrices, the proof
of the existence of effective predictors for kernels with high alignment, both
for classification and for regression, and the proof of stability-based
generalization bounds for a broad family of algorithms for learning kernels
based on centered alignment. We also report the results of experiments with our
centered alignment-based algorithms in both classification and regression.
","[{'version': 'v1', 'created': 'Fri, 2 Mar 2012 19:20:42 GMT'}, {'version': 'v2', 'created': 'Tue, 8 Apr 2014 18:30:21 GMT'}, {'version': 'v3', 'created': 'Mon, 29 Apr 2024 18:15:29 GMT'}]",2024-05-01,"[['Cortes', 'Corinna', ''], ['Mohri', 'Mehryar', ''], ['Rostamizadeh', 'Afshin', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
1,1301.6714,Pierfrancesco La Mura,"Pierfrancesco La Mura, Yoav Shoham",Expected Utility Networks,"Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)",,,UAI-P-1999-PG-366-373,cs.GT cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce a new class of graphical representations, expected utility
networks (EUNs), and discuss some of its properties and potential applications
to artificial intelligence and economic theory. In EUNs not only probabilities,
but also utilities enjoy a modular representation. EUNs are undirected graphs
with two types of arc, representing probability and utility dependencies
respectively. The representation of utilities is based on a novel notion of
conditional utility independence, which we introduce and discuss in the context
of other existing proposals. Just as probabilistic inference involves the
computation of conditional probabilities, strategic inference involves the
computation of conditional expected utilities for alternative plans of action.
We define a new notion of conditional expected utility (EU) independence, and
show that in EUNs node separation with respect to the probability and utility
subgraphs implies conditional EU independence.
","[{'version': 'v1', 'created': 'Wed, 23 Jan 2013 15:59:18 GMT'}]",2024-01-18,"[['La Mura', 'Pierfrancesco', ''], ['Shoham', 'Yoav', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
2,1403.1076,Kieran Greer Dr,Kieran Greer,Is Intelligence Artificial?,Newly edited version,"Euroasia Summit, Congress on Scientific Researches and Recent
  Trends-8, August 2-4, 2021, The Philippine Merchant Marine Academy,
  Philippines, pp. 307 - 324",,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Our understanding of intelligence is directed primarily at the human level.
This paper attempts to give a more unifying definition that can be applied to
the natural world in general and then Artificial Intelligence. The definition
would be used more to qualify than quantify it and might help when making
judgements on the matter. While correct behaviour is the preferred definition,
a metric that is grounded in Kolmogorov's Complexity Theory is suggested, which
leads to a measurement about entropy. A version of an accepted AI test is then
put forward as the 'acid test' and might be what a free-thinking program would
try to achieve. Recent work by the author has been more from a direction of
mechanical processes, or ones that might operate automatically. This paper
agrees that intelligence is a pro-active event, but also notes a second aspect
to it that is in the background and mechanical. The paper suggests looking at
intelligence and the conscious as being slightly different, where the conscious
is this more mechanical aspect. In fact, a surprising conclusion can be a
passive but intelligent brain being invoked by active and less intelligent
senses.
","[{'version': 'v1', 'created': 'Wed, 5 Mar 2014 11:09:55 GMT'}, {'version': 'v2', 'created': 'Sat, 8 Nov 2014 13:20:49 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Nov 2014 16:19:47 GMT'}, {'version': 'v4', 'created': 'Wed, 28 Jan 2015 17:10:05 GMT'}, {'version': 'v5', 'created': 'Mon, 29 Jun 2015 11:49:38 GMT'}, {'version': 'v6', 'created': 'Mon, 18 Jan 2021 10:59:41 GMT'}, {'version': 'v7', 'created': 'Mon, 14 Jun 2021 11:29:11 GMT'}, {'version': 'v8', 'created': 'Thu, 29 Jul 2021 11:44:43 GMT'}, {'version': 'v9', 'created': 'Wed, 17 Jul 2024 08:52:08 GMT'}]",2024-07-18,"[['Greer', 'Kieran', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
3,1406.6046,Changwang Zhang,"Changwang Zhang, Shi Zhou, Benjamin M. Chain",Hybrid Epidemics - A Case Study on Computer Worm Conficker,,PLoS ONE. 2015 May 15;10(5):e0127478,10.1371/journal.pone.0127478,,cs.CR cs.AI cs.NI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Conficker is a computer worm that erupted on the Internet in 2008. It is
unique in combining three different spreading strategies: local probing,
neighbourhood probing, and global probing. We propose a mathematical model that
combines three modes of spreading, local, neighbourhood and global to capture
the worm's spreading behaviour. The parameters of the model are inferred
directly from network data obtained during the first day of the Conifcker
epidemic. The model is then used to explore the trade-off between spreading
modes in determining the worm's effectiveness. Our results show that the
Conficker epidemic is an example of a critically hybrid epidemic, in which the
different modes of spreading in isolation do not lead to successful epidemics.
Such hybrid spreading strategies may be used beneficially to provide the most
effective strategies for promulgating information across a large population.
When used maliciously, however, they can present a dangerous challenge to
current internet security protocols.
","[{'version': 'v1', 'created': 'Sun, 22 Jun 2014 16:52:39 GMT'}, {'version': 'v2', 'created': 'Mon, 9 Feb 2015 17:24:45 GMT'}, {'version': 'v3', 'created': 'Tue, 31 Mar 2015 09:30:05 GMT'}]",2024-01-02,"[['Zhang', 'Changwang', ''], ['Zhou', 'Shi', ''], ['Chain', 'Benjamin M.', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
4,1409.7291,Changwang Zhang,"Changwang Zhang, Shi Zhou, Joel C. Miller, Ingemar J. Cox, and
  Benjamin M. Chain",Optimizing Hybrid Spreading in Metapopulations,,Scientific Reports. 2015 Apr 29;5:9924,10.1038/srep09924,,physics.soc-ph cs.AI cs.SI q-bio.PE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Epidemic spreading phenomena are ubiquitous in nature and society. Examples
include the spreading of diseases, information, and computer viruses. Epidemics
can spread by local spreading, where infected nodes can only infect a limited
set of direct target nodes and global spreading, where an infected node can
infect every other node. In reality, many epidemics spread using a hybrid
mixture of both types of spreading. In this study we develop a theoretical
framework for studying hybrid epidemics, and examine the optimum balance
between spreading mechanisms in terms of achieving the maximum outbreak size.
We show the existence of critically hybrid epidemics where neither spreading
mechanism alone can cause a noticeable spread but a combination of the two
spreading mechanisms would produce an enormous outbreak. Our results provide
new strategies for maximising beneficial epidemics and estimating the worst
outcome of damaging hybrid epidemics.
","[{'version': 'v1', 'created': 'Thu, 25 Sep 2014 15:21:34 GMT'}, {'version': 'v2', 'created': 'Thu, 11 Dec 2014 17:39:52 GMT'}, {'version': 'v3', 'created': 'Tue, 31 Mar 2015 08:54:01 GMT'}]",2024-01-02,"[['Zhang', 'Changwang', ''], ['Zhou', 'Shi', ''], ['Miller', 'Joel C.', ''], ['Cox', 'Ingemar J.', ''], ['Chain', 'Benjamin M.', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
5,1501.01678,Changwang Zhang,"Changwang Zhang, Shi Zhou, and Benjamin M. Chain","LeoTask: a fast, flexible and reliable framework for computational
  research",,,,,cs.SE cs.AI cs.DC cs.PL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  LeoTask is a Java library for computation-intensive and time-consuming
research tasks. It automatically executes tasks in parallel on multiple CPU
cores on a computing facility. It uses a configuration file to enable automatic
exploration of parameter space and flexible aggregation of results, and
therefore allows researchers to focus on programming the key logic of a
computing task. It also supports reliable recovery from interruptions, dynamic
and cloneable networks, and integration with the plotting software Gnuplot.
","[{'version': 'v1', 'created': 'Wed, 7 Jan 2015 22:33:40 GMT'}]",2024-01-02,"[['Zhang', 'Changwang', ''], ['Zhou', 'Shi', ''], ['Chain', 'Benjamin M.', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
6,1503.08992,Changwang Zhang,"Changwang Zhang, Shi Zhou, Elisabetta Groppelli, Pierre Pellegrino,
  Ian Williams, Persephone Borrow, Benjamin M. Chain, Clare Jolly","Hybrid spreading mechanisms and T cell activation shape the dynamics of
  HIV-1 infection",,PLOS Computational Biology. 2015 Apr 2;11(4):e1004179,10.1371/journal.pcbi.1004179,,q-bio.PE cs.AI cs.CE physics.bio-ph q-bio.CB,http://creativecommons.org/licenses/by-nc-sa/3.0/,"  HIV-1 can disseminate between susceptible cells by two mechanisms: cell-free
infection following fluid-phase diffusion of virions and by highly-efficient
direct cell-to-cell transmission at immune cell contacts. The contribution of
this hybrid spreading mechanism, which is also a characteristic of some
important computer worm outbreaks, to HIV-1 progression in vivo remains
unknown. Here we present a new mathematical model that explicitly incorporates
the ability of HIV-1 to use hybrid spreading mechanisms and evaluate the
consequences for HIV-1 pathogenenesis. The model captures the major phases of
the HIV-1 infection course of a cohort of treatment naive patients and also
accurately predicts the results of the Short Pulse Anti-Retroviral Therapy at
Seroconversion (SPARTAC) trial. Using this model we find that hybrid spreading
is critical to seed and establish infection, and that cell-to-cell spread and
increased CD4+ T cell activation are important for HIV-1 progression. Notably,
the model predicts that cell-to-cell spread becomes increasingly effective as
infection progresses and thus may present a considerable treatment barrier.
Deriving predictions of various treatments' influence on HIV-1 progression
highlights the importance of earlier intervention and suggests that treatments
effectively targeting cell-to-cell HIV-1 spread can delay progression to AIDS.
This study suggests that hybrid spreading is a fundamental feature of HIV
infection, and provides the mathematical framework incorporating this feature
with which to evaluate future therapeutic strategies.
","[{'version': 'v1', 'created': 'Tue, 31 Mar 2015 10:14:54 GMT'}]",2024-01-02,"[['Zhang', 'Changwang', ''], ['Zhou', 'Shi', ''], ['Groppelli', 'Elisabetta', ''], ['Pellegrino', 'Pierre', ''], ['Williams', 'Ian', ''], ['Borrow', 'Persephone', ''], ['Chain', 'Benjamin M.', ''], ['Jolly', 'Clare', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
7,1603.00751,Nikola Milo\v{s}evi\'c MSc,Nikola Milosevic,"Equity forecast: Predicting long term stock price movement using machine
  learning","9 pages, 3 tables, computational finance, algorithmic finance","Journal of Economics Library, 3(2), 2016, 288-294",10.1453/jel.v3i2.750,,cs.LG q-fin.GN,http://creativecommons.org/licenses/by/4.0/,"  Long term investment is one of the major investment strategies. However,
calculating intrinsic value of some company and evaluating shares for long term
investment is not easy, since analyst have to care about a large number of
financial indicators and evaluate them in a right manner. So far, little help
in predicting the direction of the company value over the longer period of time
has been provided from the machines. In this paper we present a machine
learning aided approach to evaluate the equity's future price over the long
time. Our method is able to correctly predict whether some company's value will
be 10% higher or not over the period of one year in 76.5% of cases.
","[{'version': 'v1', 'created': 'Wed, 2 Mar 2016 15:33:30 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Nov 2018 12:55:27 GMT'}]",2024-04-11,"[['Milosevic', 'Nikola', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
8,1610.02496,Kaiwei Li,"Kaiwei Li, Jianfei Chen, Wenguang Chen, Jun Zhu",SaberLDA: Sparsity-Aware Learning of Topic Models on GPUs,"13 pages, 12 figures",,10.1109/TPDS.2020.2979702,,cs.DC cs.IR cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Latent Dirichlet Allocation (LDA) is a popular tool for analyzing discrete
count data such as text and images. Applications require LDA to handle both
large datasets and a large number of topics. Though distributed CPU systems
have been used, GPU-based systems have emerged as a promising alternative
because of the high computational power and memory bandwidth of GPUs. However,
existing GPU-based LDA systems cannot support a large number of topics because
they use algorithms on dense data structures whose time and space complexity is
linear to the number of topics. In this paper, we propose SaberLDA, a GPU-based
LDA system that implements a sparsity-aware algorithm to achieve sublinear time
complexity and scales well to learn a large number of topics. To address the
challenges introduced by sparsity, we propose a novel data layout, a new
warp-based sampling kernel, and an efficient sparse count matrix updating
algorithm that improves locality, makes efficient utilization of GPU warps, and
reduces memory consumption. Experiments show that SaberLDA can learn from
billions-token-scale data with up to 10,000 topics, which is almost two orders
of magnitude larger than that of the previous GPU-based systems. With a single
GPU card, SaberLDA is able to learn 10,000 topics from a dataset of billions of
tokens in a few hours, which is only achievable with clusters with tens of
machines before.
","[{'version': 'v1', 'created': 'Sat, 8 Oct 2016 07:57:00 GMT'}, {'version': 'v2', 'created': 'Wed, 12 Oct 2016 12:39:07 GMT'}]",2024-06-21,"[['Li', 'Kaiwei', ''], ['Chen', 'Jianfei', ''], ['Chen', 'Wenguang', ''], ['Zhu', 'Jun', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
9,1611.10189,Martino Trassinelli,Martino Trassinelli (E10),Bayesian data analysis tools for atomic physics,,"Nucl. Instrum. Methods B 408, 301-312 (2017)",10.1016/j.nimb.2017.05.030,,physics.data-an hep-ex physics.atom-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present an introduction to some concepts of Bayesian data analysis in the
context of atomic physics. Starting from basic rules of probability, we present
the Bayes' theorem and its applications. In particular we discuss about how to
calculate simple and joint probability distributions and the Bayesian evidence,
a model dependent quantity that allows to assign probabilities to different
hypotheses from the analysis of a same data set. To give some practical
examples, these methods are applied to two concrete cases. In the first
example, the presence or not of a satellite line in an atomic spectrum is
investigated. In the second example, we determine the most probable model among
a set of possible profiles from the analysis of a statistically poor spectrum.
We show also how to calculate the probability distribution of the main spectral
component without having to determine uniquely the spectrum modeling. For these
two studies, we implement the program Nested fit to calculate the different
probability distributions and other related quantities. Nested fit is a
Fortran90/Python code developed during the last years for analysis of atomic
spectra. As indicated by the name, it is based on the nested algorithm, which
is presented in details together with the program itself.
","[{'version': 'v1', 'created': 'Wed, 30 Nov 2016 14:53:36 GMT'}, {'version': 'v2', 'created': 'Fri, 19 May 2017 11:51:09 GMT'}]",2024-01-30,"[['Trassinelli', 'Martino', '', 'E10']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
10,1709.00668,Evangelos Papalexakis,"Ekta Gujral, Ravdeep Pasricha, Evangelos E. Papalexakis",SamBaTen: Sampling-based Batch Incremental Tensor Decomposition,,,,,stat.ML cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Tensor decompositions are invaluable tools in analyzing multimodal datasets.
In many real-world scenarios, such datasets are far from being static, to the
contrary they tend to grow over time. For instance, in an online social network
setting, as we observe new interactions over time, our dataset gets updated in
its ""time"" mode. How can we maintain a valid and accurate tensor decomposition
of such a dynamically evolving multimodal dataset, without having to re-compute
the entire decomposition after every single update? In this paper we introduce
SaMbaTen, a Sampling-based Batch Incremental Tensor Decomposition algorithm,
which incrementally maintains the decomposition given new updates to the tensor
dataset. SaMbaTen is able to scale to datasets that the state-of-the-art in
incremental tensor decomposition is unable to operate on, due to its ability to
effectively summarize the existing tensor and the incoming updates, and perform
all computations in the reduced summary space. We extensively evaluate SaMbaTen
using synthetic and real datasets. Indicatively, SaMbaTen achieves comparable
accuracy to state-of-the-art incremental and non-incremental techniques, while
being 25-30 times faster. Furthermore, SaMbaTen scales to very large sparse and
dense dynamically evolving tensors of dimensions up to 100K x 100K x 100K where
state-of-the-art incremental approaches were not able to operate.
","[{'version': 'v1', 'created': 'Sun, 3 Sep 2017 06:05:07 GMT'}, {'version': 'v2', 'created': 'Mon, 18 Sep 2017 20:42:33 GMT'}]",2024-06-03,"[['Gujral', 'Ekta', ''], ['Pasricha', 'Ravdeep', ''], ['Papalexakis', 'Evangelos E.', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
11,1709.05958,Matthew Peveler,"Matthew Peveler, Naveen Sundar Govindarajulu, Selmer Bringsjord,
  Atriya Sen, Biplav Srivastava, Kartik Talamadupula, Hui Su","Toward Cognitive and Immersive Systems: Experiments in a Cognitive
  Microworld",Submitted to Advances of Cognitive Systems 2018,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
","[{'version': 'v1', 'created': 'Thu, 14 Sep 2017 21:52:54 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Dec 2018 17:26:47 GMT'}]",2024-06-10,"[['Peveler', 'Matthew', ''], ['Govindarajulu', 'Naveen Sundar', ''], ['Bringsjord', 'Selmer', ''], ['Sen', 'Atriya', ''], ['Srivastava', 'Biplav', ''], ['Talamadupula', 'Kartik', ''], ['Su', 'Hui', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
12,1709.0748,Michael Albert,"Mathijs de Weerdt, Michael Albert, Vincent Conitzer",Complexity of Scheduling Charging in the Smart Grid,,,10.24963/ijcai.2018/658,,cs.CC cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the smart grid, the intent is to use flexibility in demand, both to
balance demand and supply as well as to resolve potential congestion. A first
prominent example of such flexible demand is the charging of electric vehicles,
which do not necessarily need to be charged as soon as they are plugged in. The
problem of optimally scheduling the charging demand of electric vehicles within
the constraints of the electricity infrastructure is called the charge
scheduling problem. The models of the charging speed, horizon, and charging
demand determine the computational complexity of the charge scheduling problem.
For about 20 variants, we show, using a dynamic programming approach, that the
problem is either in P or weakly NP-hard. We also show that about 10 variants
of the problem are strongly NP-hard, presenting a potentially significant
obstacle to their use in practical situations of scale.
","[{'version': 'v1', 'created': 'Thu, 21 Sep 2017 18:33:35 GMT'}]",2024-03-14,"[['de Weerdt', 'Mathijs', ''], ['Albert', 'Michael', ''], ['Conitzer', 'Vincent', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
13,1710.04872,Abhishake Rastogi,Abhishake Rastogi and Sivananthan Sampath,"Manifold regularization based on Nystr{\""o}m type subsampling",,Applied and Computational Harmonic Analysis 49 (1) (2020) 152-179,10.1016/j.acha.2018.12.002,,stat.ML cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we study the Nystr{\""o}m type subsampling for large scale
kernel methods to reduce the computational complexities of big data. We discuss
the multi-penalty regularization scheme based on Nystr{\""o}m type subsampling
which is motivated from well-studied manifold regularization schemes. We
develop a theoretical analysis of multi-penalty least-square regularization
scheme under the general source condition in vector-valued function setting,
therefore the results can also be applied to multi-task learning problems. We
achieve the optimal minimax convergence rates of multi-penalty regularization
using the concept of effective dimension for the appropriate subsampling size.
We discuss an aggregation approach based on linear function strategy to combine
various Nystr{\""o}m approximants. Finally, we demonstrate the performance of
multi-penalty regularization based on Nystr{\""o}m type subsampling on
Caltech-101 data set for multi-class image classification and NSL-KDD benchmark
data set for intrusion detection problem.
","[{'version': 'v1', 'created': 'Fri, 13 Oct 2017 11:13:38 GMT'}]",2024-04-09,"[['Rastogi', 'Abhishake', ''], ['Sampath', 'Sivananthan', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
14,1802.03308,Frieder Stolzenburg,"Frieder Stolzenburg, Sandra Litz, Olivia Michael, Oliver Obst",The Power of Linear Recurrent Neural Networks,"50 pages, 12 figures, 4 tables",,,,cs.LG cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recurrent neural networks are a powerful means to cope with time series. We
show how autoregressive linear, i.e., linearly activated recurrent neural
networks (LRNNs) can approximate any time-dependent function f(t). The
approximation can effectively be learned by simply solving a linear equation
system; no backpropagation or similar methods are needed. Furthermore, and this
is the main contribution of this article, the size of an LRNN can be reduced
significantly in one step after inspecting the spectrum of the network
transition matrix, i.e., its eigenvalues, by taking only the most relevant
components. Therefore, in contrast to other approaches, we do not only learn
network weights but also the network architecture. LRNNs have interesting
properties: They end up in ellipse trajectories in the long run and allow the
prediction of further values and compact representations of functions. We
demonstrate this by several experiments, among them multiple superimposed
oscillators (MSO), robotic soccer (RoboCup), and stock price prediction. LRNNs
outperform the previous state-of-the-art for the MSO task with a minimal number
of units.
","[{'version': 'v1', 'created': 'Fri, 9 Feb 2018 15:35:41 GMT'}, {'version': 'v2', 'created': 'Wed, 2 May 2018 06:28:35 GMT'}, {'version': 'v3', 'created': 'Tue, 22 Jan 2019 11:51:16 GMT'}, {'version': 'v4', 'created': 'Tue, 10 Mar 2020 14:17:57 GMT'}, {'version': 'v5', 'created': 'Tue, 4 May 2021 11:34:37 GMT'}, {'version': 'v6', 'created': 'Thu, 14 Apr 2022 05:22:25 GMT'}, {'version': 'v7', 'created': 'Fri, 12 May 2023 10:52:45 GMT'}, {'version': 'v8', 'created': 'Thu, 25 May 2023 13:38:08 GMT'}, {'version': 'v9', 'created': 'Wed, 24 Jan 2024 16:22:36 GMT'}]",2024-01-25,"[['Stolzenburg', 'Frieder', ''], ['Litz', 'Sandra', ''], ['Michael', 'Olivia', ''], ['Obst', 'Oliver', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
15,1802.05319,Suvodeep Majumder,"Suvodeep Majumder, Nikhila Balaji, Katie Brey, Wei Fu, Tim Menzies","500+ Times Faster Than Deep Learning (A Case Study Exploring Faster
  Methods for Text Mining StackOverflow)",,"MSR '18, Proceedings of the 15th International Conference on
  Mining Software Repositories, May 2018, Pages 554 to 563",10.1145/3196398.3196424,,cs.SE cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep learning methods are useful for high-dimensional data and are becoming
widely used in many areas of software engineering. Deep learners utilizes
extensive computational power and can take a long time to train-- making it
difficult to widely validate and repeat and improve their results. Further,
they are not the best solution in all domains. For example, recent results show
that for finding related Stack Overflow posts, a tuned SVM performs similarly
to a deep learner, but is significantly faster to train. This paper extends
that recent result by clustering the dataset, then tuning very learners within
each cluster. This approach is over 500 times faster than deep learning (and
over 900 times faster if we use all the cores on a standard laptop computer).
Significantly, this faster approach generates classifiers nearly as good
(within 2\% F1 Score) as the much slower deep learning method. Hence we
recommend this faster methods since it is much easier to reproduce and utilizes
far fewer CPU resources. More generally, we recommend that before researchers
release research results, that they compare their supposedly sophisticated
methods against simpler alternatives (e.g applying simpler learners to build
local models).
","[{'version': 'v1', 'created': 'Wed, 14 Feb 2018 20:57:48 GMT'}]",2024-02-19,"[['Majumder', 'Suvodeep', ''], ['Balaji', 'Nikhila', ''], ['Brey', 'Katie', ''], ['Fu', 'Wei', ''], ['Menzies', 'Tim', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
16,1804.04659,Daning Cheng,"Cheng Daning, Xia Fen, Li Shigang, Zhang Yunquan","Asynch-SGBDT: Asynchronous Parallel Stochastic Gradient Boosting
  Decision Tree based on Parameters Server",,,10.1109/IPDPS54959.2023.00034,,cs.LG cs.DC stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In AI research and industry, machine learning is the most widely used tool.
One of the most important machine learning algorithms is Gradient Boosting
Decision Tree, i.e. GBDT whose training process needs considerable
computational resources and time. To shorten GBDT training time, many works
tried to apply GBDT on Parameter Server. However, those GBDT algorithms are
synchronous parallel algorithms which fail to make full use of Parameter
Server. In this paper, we examine the possibility of using asynchronous
parallel methods to train GBDT model and name this algorithm as asynch-SGBDT
(asynchronous parallel stochastic gradient boosting decision tree). Our
theoretical and experimental results indicate that the scalability of
asynch-SGBDT is influenced by the sample diversity of datasets, sampling rate,
step length and the setting of GBDT tree. Experimental results also show
asynch-SGBDT training process reaches a linear speedup in asynchronous parallel
manner when datasets and GBDT trees meet high scalability requirements.
","[{'version': 'v1', 'created': 'Thu, 12 Apr 2018 14:06:05 GMT'}, {'version': 'v2', 'created': 'Fri, 18 May 2018 04:26:26 GMT'}, {'version': 'v3', 'created': 'Fri, 17 Aug 2018 01:57:44 GMT'}, {'version': 'v4', 'created': 'Thu, 18 Jul 2019 06:50:05 GMT'}]",2024-04-30,"[['Daning', 'Cheng', ''], ['Fen', 'Xia', ''], ['Shigang', 'Li', ''], ['Yunquan', 'Zhang', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
17,1805.0477,Tommaso Furlanello,"Tommaso Furlanello, Zachary C. Lipton, Michael Tschannen, Laurent Itti
  and Anima Anandkumar",Born Again Neural Networks,Published @ICML 2018,,,,stat.ML cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge Distillation (KD) consists of transferring âknowledgeâ from one
machine learning model (the teacher) to another (the student). Commonly, the
teacher is a high-capacity model with formidable performance, while the student
is more compact. By transferring knowledge, one hopes to benefit from the
studentâs compactness, without sacrificing too much performance. We study KD
from a new perspective: rather than compressing models, we train students
parameterized identically to their teachers. Surprisingly, these Born-Again
Networks (BANs), outperform their teachers significantly, both on computer
vision and language modeling tasks. Our experiments with BANs based on
DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and
CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore
two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and
(ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate
the essential components of KD, demonstrating the effect of the teacher outputs
on both predicted and non-predicted classes.
","[{'version': 'v1', 'created': 'Sat, 12 May 2018 19:48:50 GMT'}, {'version': 'v2', 'created': 'Fri, 29 Jun 2018 10:46:28 GMT'}]",2024-03-05,"[['Furlanello', 'Tommaso', ''], ['Lipton', 'Zachary C.', ''], ['Tschannen', 'Michael', ''], ['Itti', 'Laurent', ''], ['Anandkumar', 'Anima', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
18,1806.05514,Cencheng Shen,Cencheng Shen and Joshua T. Vogelstein,"The Exact Equivalence of Distance and Kernel Methods for Hypothesis
  Testing","24 pages main + 7 pages appendix, 3 figures","AStA Advances in Statistical Analysis 105(3), 385-403, 2021",10.1007/s10182-020-00378-1,,stat.ML cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Distance-based tests, also called ""energy statistics"", are leading methods
for two-sample and independence tests from the statistics community.
Kernel-based tests, developed from ""kernel mean embeddings"", are leading
methods for two-sample and independence tests from the machine learning
community. A fixed-point transformation was previously proposed to connect the
distance methods and kernel methods for the population statistics. In this
paper, we propose a new bijective transformation between metrics and kernels.
It simplifies the fixed-point transformation, inherits similar theoretical
properties, allows distance methods to be exactly the same as kernel methods
for sample statistics and p-value, and better preserves the data structure upon
transformation. Our results further advance the understanding in distance and
kernel-based tests, streamline the code base for implementing these tests, and
enable a rich literature of distance-based and kernel-based methodologies to
directly communicate with each other.
","[{'version': 'v1', 'created': 'Thu, 14 Jun 2018 12:57:57 GMT'}, {'version': 'v2', 'created': 'Mon, 9 Jul 2018 12:32:43 GMT'}, {'version': 'v3', 'created': 'Sun, 25 Nov 2018 19:51:46 GMT'}, {'version': 'v4', 'created': 'Sun, 20 Oct 2019 22:33:14 GMT'}, {'version': 'v5', 'created': 'Tue, 15 Sep 2020 00:33:43 GMT'}]",2024-06-27,"[['Shen', 'Cencheng', ''], ['Vogelstein', 'Joshua T.', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
19,1807.05996,Robert Cousins,Robert D. Cousins,Lectures on Statistics in Theory: Prelude to Statistics in Practice,"97 pages, updates for African School of Fundamental Physics and
  Applications in 2024",,,,physics.data-an,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This is a writeup of lectures on ""statistics"" that have evolved from the
initial version for the 2009 Hadron Collider Physics Summer School at CERN to
versions for other venues and, most recently, for the African School of
Fundamental Physics and Applications in 2024. The emphasis is on foundations,
using simple examples to illustrate the points that are still debated in the
professional statistics literature. The three main approaches to interval
estimation (Neyman confidence, Bayesian, likelihood ratio) are discussed and
compared in detail, with and without nuisance parameters. Hypothesis testing is
discussed mainly from the frequentist point of view, with pointers to the
Bayesian literature. Various foundational issues are emphasized, including the
conditionality principle and the likelihood principle.
","[{'version': 'v1', 'created': 'Mon, 16 Jul 2018 17:54:13 GMT'}, {'version': 'v2', 'created': 'Sat, 4 Aug 2018 07:02:49 GMT'}, {'version': 'v3', 'created': 'Tue, 29 Aug 2023 10:02:46 GMT'}, {'version': 'v4', 'created': 'Wed, 26 Jun 2024 13:31:03 GMT'}]",2024-06-27,"[['Cousins', 'Robert D.', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
20,1810.01864,Idan Attias,"Idan Attias, Steve Hanneke, Aryeh Kontorovich, Menachem Sadigurschi",Agnostic Sample Compression Schemes for Regression,"New results in this version: (1) Approximate agnostic sample
  compression scheme for function classes with finite fat-shattering dimension
  and the $\ell_p$ loss (section 3), (2) Near-optimal approximate compression
  for linear functions and the $\ell_p$ loss (section 4.1) The results in
  sections 4.2 and 4.3 appear in the previous version",,,,cs.LG cs.IT math.IT math.ST stat.ML stat.TH,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We obtain the first positive results for bounded sample compression in the
agnostic regression setting with the $\ell_p$ loss, where $p\in [1,\infty]$. We
construct a generic approximate sample compression scheme for real-valued
function classes exhibiting exponential size in the fat-shattering dimension
but independent of the sample size. Notably, for linear regression, an
approximate compression of size linear in the dimension is constructed.
Moreover, for $\ell_1$ and $\ell_\infty$ losses, we can even exhibit an
efficient exact sample compression scheme of size linear in the dimension. We
further show that for every other $\ell_p$ loss, $p\in (1,\infty)$, there does
not exist an exact agnostic compression scheme of bounded size. This refines
and generalizes a negative result of David, Moran, and Yehudayoff for the
$\ell_2$ loss. We close by posing general open questions: for agnostic
regression with $\ell_1$ loss, does every function class admits an exact
compression scheme of size equal to its pseudo-dimension? For the $\ell_2$
loss, does every function class admit an approximate compression scheme of
polynomial size in the fat-shattering dimension? These questions generalize
Warmuth's classic sample compression conjecture for realizable-case
classification.
","[{'version': 'v1', 'created': 'Wed, 3 Oct 2018 11:46:59 GMT'}, {'version': 'v2', 'created': 'Sat, 3 Feb 2024 21:49:19 GMT'}]",2024-02-06,"[['Attias', 'Idan', ''], ['Hanneke', 'Steve', ''], ['Kontorovich', 'Aryeh', ''], ['Sadigurschi', 'Menachem', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
21,1810.06119,Vasileios Basios,"Vasileios Basios, Robin De Gernier, Thomas Oikonomou",Symbolic Dynamics of Music from Europe and Japan,,,10.1063/5.0048396,,nlin.CD physics.data-an,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  After a brief introduction to the theory underlying block-entropy, and its
relation to the dynamics of complex systems as well as certain information
theory aspects, we study musical texts coming from two distinct musical
traditions (Japanese and Western European) encoded via symbolic dynamics. We
quantify their information content or also known as the degree of
""non-randomness"" which essentially defines the complexity of the text. We
analyse the departure of ""total randomness"" to the constrains underlying the
dynamics of the symbol generating process. Following Shannon on his attribution
to these constraints as the emergence of complexity, we observe that it can be
accurately assessed by the texts' block-entropy versus block-length scaling
laws.
","[{'version': 'v1', 'created': 'Sun, 14 Oct 2018 22:39:49 GMT'}]",2024-06-19,"[['Basios', 'Vasileios', ''], ['De Gernier', 'Robin', ''], ['Oikonomou', 'Thomas', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
22,1811.09539,Josef Lorenz Rumberger,Elias Baumann and Josef Lorenz Rumberger,"State of the Art in Fair ML: From Moral Philosophy and Legislation to
  Fair Classifiers",,,,,cs.CY cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning is becoming an ever present part in our lives as many
decisions, e.g. to lend a credit, are no longer made by humans but by machine
learning algorithms. However those decisions are often unfair and
discriminating individuals belonging to protected groups based on race or
gender. With the recent General Data Protection Regulation (GDPR) coming into
effect, new awareness has been raised for such issues and with computer
scientists having such a large impact on peoples lives it is necessary that
actions are taken to discover and prevent discrimination. This work aims to
give an introduction into discrimination, legislative foundations to counter it
and strategies to detect and prevent machine learning algorithms from showing
such behavior.
","[{'version': 'v1', 'created': 'Tue, 20 Nov 2018 12:03:55 GMT'}, {'version': 'v2', 'created': 'Sun, 26 May 2024 17:32:06 GMT'}]",2024-05-28,"[['Baumann', 'Elias', ''], ['Rumberger', 'Josef Lorenz', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
23,1901.0893,Shubhomoy Das,"Shubhomoy Das, Md Rakibul Islam, Nitthilan Kannappan Jayakodi,
  Janardhan Rao Doppa","Effectiveness of Tree-based Ensembles for Anomaly Discovery: Insights,
  Batch and Streaming Active Learning","Accepted for Publication in Journal of Artificial Intelligence
  Research. 46 pages; code is available at
  https://github.com/shubhomoydas/ad_examples. arXiv admin note: substantial
  text overlap with arXiv:1809.06477",Journal of Artificial Intelligence Research 80 (2024) 127-172,,,cs.LG stat.ML,http://creativecommons.org/licenses/by/4.0/,"  In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
","[{'version': 'v1', 'created': 'Wed, 23 Jan 2019 23:41:11 GMT'}, {'version': 'v2', 'created': 'Mon, 8 Apr 2024 19:23:50 GMT'}, {'version': 'v3', 'created': 'Tue, 14 May 2024 05:29:51 GMT'}]",2024-05-15,"[['Das', 'Shubhomoy', ''], ['Islam', 'Md Rakibul', ''], ['Jayakodi', 'Nitthilan Kannappan', ''], ['Doppa', 'Janardhan Rao', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
24,1902.06931,Erwan Scornet,"Julie Josse (XPOP, CMAP), Jacob M. Chen, Nicolas Prost (CMAP, XPOP,
  PARIETAL), Erwan Scornet (X, CMAP, SU), Ga\""el Varoquaux (PARIETAL)",On the consistency of supervised learning with missing values,,,,,stat.ML cs.LG math.ST stat.TH,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In many application settings, the data have missing entries which make
analysis challenging. An abundant literature addresses missing values in an
inferential framework: estimating parameters and their variance from incomplete
tables. Here, we consider supervised-learning settings: predicting a target
when missing values appear in both training and testing data. We show the
consistency of two approaches in prediction. A striking result is that the
widely-used method of imputing with a constant, such as the mean prior to
learning is consistent when missing values are not informative. This contrasts
with inferential settings where mean imputation is pointed at for distorting
the distribution of the data. That such a simple approach can be consistent is
important in practice. We also show that a predictor suited for complete
observations can predict optimally on incomplete data, through multiple
imputation. Finally, to compare imputation with learning directly with a model
that accounts for missing values, we analyze further decision trees. These can
naturally tackle empirical risk minimization with missing values, due to their
ability to handle the half-discrete nature of incomplete variables. After
comparing theoretically and empirically different missing values strategies in
trees, we recommend using the ""missing incorporated in attribute"" method as it
can handle both non-informative and informative missing values.
","[{'version': 'v1', 'created': 'Tue, 19 Feb 2019 07:27:19 GMT'}, {'version': 'v2', 'created': 'Mon, 25 Mar 2019 15:26:55 GMT'}, {'version': 'v3', 'created': 'Fri, 3 Jul 2020 15:12:20 GMT'}, {'version': 'v4', 'created': 'Thu, 7 Mar 2024 09:27:39 GMT'}, {'version': 'v5', 'created': 'Thu, 21 Mar 2024 09:01:19 GMT'}]",2024-03-22,"[['Josse', 'Julie', '', 'XPOP, CMAP'], ['Chen', 'Jacob M.', '', 'CMAP, XPOP,\n  PARIETAL'], ['Prost', 'Nicolas', '', 'CMAP, XPOP,\n  PARIETAL'], ['Scornet', 'Erwan', '', 'X, CMAP, SU'], ['Varoquaux', 'Gaël', '', 'PARIETAL']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
25,1903.08519,Eduardo Paluzo-Hidalgo,"Rocio Gonzalez-Diaz, Miguel A. Guti\'errez-Naranjo, Eduardo
  Paluzo-Hidalgo","Topology-based Representative Datasets to Reduce Neural Network Training
  Resources",,,10.1007/s00521-022-07252-y,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  One of the main drawbacks of the practical use of neural networks is the long
time required in the training process. Such a training process consists of an
iterative change of parameters trying to minimize a loss function. These
changes are driven by a dataset, which can be seen as a set of labelled points
in an n-dimensional space. In this paper, we explore the concept of are
representative dataset which is a dataset smaller than the original one,
satisfying a nearness condition independent of isometric transformations.
Representativeness is measured using persistence diagrams (a computational
topology tool) due to its computational efficiency. We prove that the accuracy
of the learning process of a neural network on a representative dataset is
""similar"" to the accuracy on the original dataset when the neural network
architecture is a perceptron and the loss function is the mean squared error.
These theoretical results accompanied by experimentation open a door to
reducing the size of the dataset to gain time in the training process of any
neural network.
","[{'version': 'v1', 'created': 'Wed, 20 Mar 2019 14:33:20 GMT'}, {'version': 'v2', 'created': 'Mon, 27 Apr 2020 17:07:55 GMT'}, {'version': 'v3', 'created': 'Mon, 4 Oct 2021 13:45:22 GMT'}]",2024-03-14,"[['Gonzalez-Diaz', 'Rocio', ''], ['Gutiérrez-Naranjo', 'Miguel A.', ''], ['Paluzo-Hidalgo', 'Eduardo', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
26,1904.04579,Kieran Greer Dr,Kieran Greer,A Concept-Value Network as a Brain Model,,,,,cs.NE cs.AI q-bio.NC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper suggests a statistical framework for describing the relations
between the physical and conceptual entities of a brain-like model. Features
and concept instances are put into context, where the paper suggests that
features may be the electrical wiring, although chemical connections are also
possible. With this idea, the actual length of the connection is important,
because it is related to firing rates and neuron synchronization, but the
signal type is less important. The paper then suggests that concepts are neuron
groups that link feature sets and concept instances are determined by chemical
signals from those groups. Therefore, features become the static horizontal
framework of the neural system and concepts are vertically interconnected
combinations of these. With regards to functionality, the neuron is then
considered to be functional and the more horizontal memory structures can be
glial. This would also suggest that features can be distributed entities and
not concentrated to a single area.
","[{'version': 'v1', 'created': 'Tue, 9 Apr 2019 10:30:23 GMT'}, {'version': 'v2', 'created': 'Tue, 23 Jul 2019 10:15:52 GMT'}, {'version': 'v3', 'created': 'Mon, 27 May 2024 12:50:58 GMT'}, {'version': 'v4', 'created': 'Tue, 9 Jul 2024 14:26:52 GMT'}]",2024-07-10,"[['Greer', 'Kieran', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
27,1905.0961,Lu\'is Cruz-Filipe,"Lu\'is Cruz-Filipe, Gra\c{c}a Gaspar, Isabel Nunes",Hypothetical answers to continuous queries over data streams,,,,,cs.PL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Continuous queries over data streams may suffer from blocking operations
and/or unbound wait, which may delay answers until some relevant input arrives
through the data stream. These delays may turn answers, when they arrive,
obsolete to users who sometimes have to make decisions with no help whatsoever.
Therefore, it can be useful to provide hypothetical answers - ""given the
current information, it is possible that X will become true at time t"" -
instead of no information at all.
  In this paper we present a semantics for queries and corresponding answers
that covers such hypothetical answers, together with an online algorithm for
updating the set of facts that are consistent with the currently available
information.
","[{'version': 'v1', 'created': 'Thu, 23 May 2019 12:11:51 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Nov 2019 11:26:13 GMT'}, {'version': 'v3', 'created': 'Mon, 15 Jan 2024 17:16:12 GMT'}]",2024-01-17,"[['Cruz-Filipe', 'Luís', ''], ['Gaspar', 'Graça', ''], ['Nunes', 'Isabel', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
28,1906.01868,Alexander Fabisch,"Alexander Fabisch, Christoph Petzoldt, Marc Otto, Frank Kirchner","A Survey of Behavior Learning Applications in Robotics -- State of the
  Art and Perspectives","Research Report of DFKI GmbH, Robotics Innovation Center",,,RR-24-01,cs.RO cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent success of machine learning in many domains has been overwhelming,
which often leads to false expectations regarding the capabilities of behavior
learning in robotics. In this survey, we analyze the current state of machine
learning for robotic behaviors. We will give a broad overview of behaviors that
have been learned and used on real robots. Our focus is on kinematically or
sensorially complex robots. That includes humanoid robots or parts of humanoid
robots, for example, legged robots or robotic arms. We will classify presented
behaviors according to various categories and we will draw conclusions about
what can be learned and what should be learned. Furthermore, we will give an
outlook on problems that are challenging today but might be solved by machine
learning in the future and argue that classical robotics and other approaches
from artificial intelligence should be integrated more with machine learning to
form complete, autonomous systems.
","[{'version': 'v1', 'created': 'Wed, 5 Jun 2019 07:54:33 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Aug 2024 16:19:48 GMT'}]",2024-08-14,"[['Fabisch', 'Alexander', ''], ['Petzoldt', 'Christoph', ''], ['Otto', 'Marc', ''], ['Kirchner', 'Frank', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
29,1906.06439,Remi Denton,"Remi Denton, Ben Hutchinson, Margaret Mitchell, Timnit Gebru, Andrew
  Zaldivar",Image Counterfactual Sensitivity Analysis for Detecting Unintended Bias,"Presented at CVPR 2019 Workshop on Fairness Accountability
  Transparency and Ethics in Computer Vision",,,,cs.CV cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Facial analysis models are increasingly used in applications that have
serious impacts on people's lives, ranging from authentication to surveillance
tracking. It is therefore critical to develop techniques that can reveal
unintended biases in facial classifiers to help guide the ethical use of facial
analysis technology. This work proposes a framework called \textit{image
counterfactual sensitivity analysis}, which we explore as a proof-of-concept in
analyzing a smiling attribute classifier trained on faces of celebrities. The
framework utilizes counterfactuals to examine how a classifier's prediction
changes if a face characteristic slightly changes. We leverage recent advances
in generative adversarial networks to build a realistic generative model of
face images that affords controlled manipulation of specific image
characteristics. We then introduce a set of metrics that measure the effect of
manipulating a specific property on the output of the trained classifier.
Empirically, we find several different factors of variation that affect the
predictions of the smiling classifier. This proof-of-concept demonstrates
potential ways generative models can be leveraged for fine-grained analysis of
bias and fairness.
","[{'version': 'v1', 'created': 'Fri, 14 Jun 2019 23:50:04 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Jun 2019 18:45:47 GMT'}, {'version': 'v3', 'created': 'Sat, 3 Oct 2020 21:33:55 GMT'}]",2024-03-14,"[['Denton', 'Remi', ''], ['Hutchinson', 'Ben', ''], ['Mitchell', 'Margaret', ''], ['Gebru', 'Timnit', ''], ['Zaldivar', 'Andrew', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
30,1906.08619,David Ruhe,"David Ruhe, Giovanni Cin\`a, Michele Tonutti, Daan de Bruin, Paul
  Elbers","Bayesian Modelling in Practice: Using Uncertainty to Improve
  Trustworthiness in Medical Applications","Presented at AISG @ ICML2019:
  https://aiforsocialgood.github.io/icml2019/index.htm",,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Intensive Care Unit (ICU) is a hospital department where machine learning
has the potential to provide valuable assistance in clinical decision making.
Classical machine learning models usually only provide point-estimates and no
uncertainty of predictions. In practice, uncertain predictions should be
presented to doctors with extra care in order to prevent potentially
catastrophic treatment decisions. In this work we show how Bayesian modelling
and the predictive uncertainty that it provides can be used to mitigate risk of
misguided prediction and to detect out-of-domain examples in a medical setting.
We derive analytically a bound on the prediction loss with respect to
predictive uncertainty. The bound shows that uncertainty can mitigate loss.
Furthermore, we apply a Bayesian Neural Network to the MIMIC-III dataset,
predicting risk of mortality of ICU patients. Our empirical results show that
uncertainty can indeed prevent potential errors and reliably identifies
out-of-domain patients. These results suggest that Bayesian predictive
uncertainty can greatly improve trustworthiness of machine learning models in
high-risk settings such as the ICU.
","[{'version': 'v1', 'created': 'Thu, 20 Jun 2019 13:51:07 GMT'}, {'version': 'v2', 'created': 'Thu, 25 Jul 2024 08:29:47 GMT'}]",2024-07-26,"[['Ruhe', 'David', ''], ['Cinà', 'Giovanni', ''], ['Tonutti', 'Michele', ''], ['de Bruin', 'Daan', ''], ['Elbers', 'Paul', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
31,1906.11374,Jennifer Rieser,"Jennifer M Rieser, Baxi Chong, Chaohui Gong, Henry C Astley, Perrin E
  Schiebel, Kelimar Diaz, Christopher Pierce, Hang Lu, Ross L Hatton, Howie
  Choset, Daniel I Goldman","Geometric phase predicts locomotion performance in undulating living
  systems across scales",,,,,physics.bio-ph physics.data-an,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Self-propelling organisms locomote via generation of patterns of
self-deformation. Despite the diversity of body plans, internal actuation
schemes and environments in limbless vertebrates and invertebrates, such
organisms often use similar travelling waves of axial body bending for
movement. Delineating how parameters (wave amplitudes, frequencies) lead to
locomotor performance (e.g. speed, energy, turning capabilities) remains
challenging. Here we show that a geometric framework that replaces laborious
calculation with a diagrammatic scheme is well suited to discovery and
comparison of optimal patterns of wave dynamics in diverse living systems. We
focus on a regime of undulatory locomotion, that of highly damped environments,
which is applicable not only to small organism movement in viscous fluids, but
also larger animals moving in frictional fluids (sand) and on frictional
ground. We find that the travelling wave dynamics used by mm-scale nematode
worms and cm-scale desert dwelling snakes and lizards can be described by time
series of the weights associated with two principal modes. The approximately
circular closed path trajectories of mode weights in a self-deformation space
enclose near-maximal surface integral (geometric phase) for organisms spanning
two decades in body length. We hypothesize that such trajectories are targets
of control (which we refer to as ``serpenoid templates""). Further, the
geometric approach reveals how seemingly complex behaviors such as turning in
worms and sidewinding snakes can be described as modulations of templates.
Thus, the use of differential geometry in living systems can assist in the
growth of comparative neuromechanics, allowing a common description of
locomotion across taxa and providing hypotheses for function at lower levels of
organization.
","[{'version': 'v1', 'created': 'Wed, 26 Jun 2019 22:50:41 GMT'}, {'version': 'v2', 'created': 'Tue, 22 Feb 2022 03:14:45 GMT'}, {'version': 'v3', 'created': 'Tue, 2 Jan 2024 18:15:23 GMT'}]",2024-01-03,"[['Rieser', 'Jennifer M', ''], ['Chong', 'Baxi', ''], ['Gong', 'Chaohui', ''], ['Astley', 'Henry C', ''], ['Schiebel', 'Perrin E', ''], ['Diaz', 'Kelimar', ''], ['Pierce', 'Christopher', ''], ['Lu', 'Hang', ''], ['Hatton', 'Ross L', ''], ['Choset', 'Howie', ''], ['Goldman', 'Daniel I', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
32,1907.02677,Jos\'e Camacho Prof.,"Jos\'e Camacho, Katarzyna Wasielewska, Rasmus Bro, David Kotz","Interpretable Feature Learning in Multivariate Big Data Analysis for
  Network Monitoring",,"IEEE Transactions on Network and Service Management, 2024",10.1109/TNSM.2024.3368501,21 (3),cs.NI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There is an increasing interest in the development of new data-driven models
useful to assess the performance of communication networks. For many
applications, like network monitoring and troubleshooting, a data model is of
little use if it cannot be interpreted by a human operator. In this paper, we
present an extension of the Multivariate Big Data Analysis (MBDA) methodology,
a recently proposed interpretable data analysis tool. In this extension, we
propose a solution to the automatic derivation of features, a cornerstone step
for the application of MBDA when the amount of data is massive. The resulting
network monitoring approach allows us to detect and diagnose disparate network
anomalies, with a data-analysis workflow that combines the advantages of
interpretable and interactive models with the power of parallel processing. We
apply the extended MBDA to two case studies: UGR'16, a benchmark flow-based
real-traffic dataset for anomaly detection, and Dartmouth'18, the longest and
largest Wi-Fi trace known to date.
","[{'version': 'v1', 'created': 'Fri, 5 Jul 2019 04:51:49 GMT'}, {'version': 'v2', 'created': 'Tue, 18 Apr 2023 10:41:58 GMT'}, {'version': 'v3', 'created': 'Fri, 1 Mar 2024 10:28:03 GMT'}]",2024-08-01,"[['Camacho', 'José', ''], ['Wasielewska', 'Katarzyna', ''], ['Bro', 'Rasmus', ''], ['Kotz', 'David', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
33,1907.03802,Carlos Rodr\'iguez - Pardo,Carlos Rodr\'iguez-Pardo and Hakan Bilen,Personalised aesthetics with residual adapters,"12 pages, 4 figures. In Iberian Conference on Pattern Recognition and
  Image Analysis proceedings",,10.1007/978-3-030-31332-6_44,,cs.CV cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The use of computational methods to evaluate aesthetics in photography has
gained interest in recent years due to the popularization of convolutional
neural networks and the availability of new annotated datasets. Most studies in
this area have focused on designing models that do not take into account
individual preferences for the prediction of the aesthetic value of pictures.
We propose a model based on residual learning that is capable of learning
subjective, user specific preferences over aesthetics in photography, while
surpassing the state-of-the-art methods and keeping a limited number of
user-specific parameters in the model. Our model can also be used for picture
enhancement, and it is suitable for content-based or hybrid recommender systems
in which the amount of computational resources is limited.
","[{'version': 'v1', 'created': 'Mon, 8 Jul 2019 18:40:16 GMT'}]",2024-03-29,"[['Rodríguez-Pardo', 'Carlos', ''], ['Bilen', 'Hakan', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
34,1907.05765,Luca Mossina,Andrea Lodi and Luca Mossina and Emmanuel Rachelson,"Learning to Handle Parameter Perturbations in Combinatorial
  Optimization: an Application to Facility Location",,,10.1016/j.ejtl.2020.100023,,math.OC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present an approach to couple the resolution of Combinatorial Optimization
problems with methods from Machine Learning, applied to the single source,
capacitated, facility location problem. Our study is framed in the context
where a reference facility location optimization problem is given. Assuming
there exist data for many variations of the reference problem (historical or
simulated) along with their optimal solution, we study how one can exploit
these to make predictions about an unseen new instance. We demonstrate how a
classifier can be built from these data to determine whether the solution to
the reference problem still applies to a new instance. In case the reference
solution is partially applicable, we build a regressor indicating the magnitude
of the expected change, and conversely how much of it can be kept for the new
instance. This insight, derived from a priori information, is expressed via an
additional constraint in the original mathematical programming formulation. We
present an empirical evaluation and discuss the benefits, drawbacks and
perspectives of such an approach. Although presented through the application to
the facility location problem, the approach developed here is general and
explores a new perspective on the exploitation of past experience in
combinatorial optimization.
","[{'version': 'v1', 'created': 'Fri, 12 Jul 2019 14:25:51 GMT'}]",2024-04-09,"[['Lodi', 'Andrea', ''], ['Mossina', 'Luca', ''], ['Rachelson', 'Emmanuel', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
35,1907.06592,Paschalis Bizopoulos,"Paschalis Bizopoulos, and Dimitrios Koutsouris",Sparsely Activated Networks,"10 pages, 5 figures, 4 algorithms, 4 tables, submission to IEEE
  Transactions on Neural Networks and Learning Systems",,10.1109/TNNLS.2020.2984514,,cs.LG cs.CV stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Previous literature on unsupervised learning focused on designing structural
priors with the aim of learning meaningful features. However, this was done
without considering the description length of the learned representations which
is a direct and unbiased measure of the model complexity. In this paper, first
we introduce the $\varphi$ metric that evaluates unsupervised models based on
their reconstruction accuracy and the degree of compression of their internal
representations. We then present and define two activation functions (Identity,
ReLU) as base of reference and three sparse activation functions (top-k
absolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize
the previously defined $\varphi$. We lastly present Sparsely Activated Networks
(SANs) that consist of kernels with shared weights that, during encoding, are
convolved with the input and then passed through a sparse activation function.
During decoding, the same weights are convolved with the sparse activation map
and subsequently the partial reconstructions from each weight are summed to
reconstruct the input. We compare SANs using the five previously defined
activation functions on a variety of datasets (Physionet, UCI-epilepsy, MNIST,
FMNIST) and show that models that are selected using $\varphi$ have small
description representation length and consist of interpretable kernels.
","[{'version': 'v1', 'created': 'Fri, 12 Jul 2019 08:01:47 GMT'}, {'version': 'v10', 'created': 'Mon, 13 Nov 2023 18:15:30 GMT'}, {'version': 'v11', 'created': 'Thu, 4 Apr 2024 11:47:25 GMT'}, {'version': 'v2', 'created': 'Tue, 22 Oct 2019 14:24:02 GMT'}, {'version': 'v3', 'created': 'Sun, 2 Feb 2020 13:05:55 GMT'}, {'version': 'v4', 'created': 'Wed, 3 Feb 2021 16:25:28 GMT'}, {'version': 'v5', 'created': 'Wed, 18 Aug 2021 13:43:43 GMT'}, {'version': 'v6', 'created': 'Thu, 19 Aug 2021 10:27:59 GMT'}, {'version': 'v7', 'created': 'Sun, 29 Aug 2021 08:25:01 GMT'}, {'version': 'v8', 'created': 'Mon, 21 Mar 2022 18:25:33 GMT'}, {'version': 'v9', 'created': 'Mon, 16 May 2022 18:27:36 GMT'}]",2024-04-05,"[['Bizopoulos', 'Paschalis', ''], ['Koutsouris', 'Dimitrios', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
36,1908.06486,Cencheng Shen,"Cencheng Shen, Jaewon Chung, Ronak Mehta, Ting Xu, Joshua T.
  Vogelstein",Independence Testing for Temporal Data,19 pages main + 6 pages appendix,"Transactions on Machine Learning Research, 2024",,,stat.ML cs.LG stat.ME,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Temporal data are increasingly prevalent in modern data science. A
fundamental question is whether two time series are related or not. Existing
approaches often have limitations, such as relying on parametric assumptions,
detecting only linear associations, and requiring multiple tests and
corrections. While many non-parametric and universally consistent dependence
measures have recently been proposed, directly applying them to temporal data
can inflate the p-value and result in an invalid test. To address these
challenges, this paper introduces the temporal dependence statistic with block
permutation to test independence between temporal data. Under proper
assumptions, the proposed procedure is asymptotically valid and universally
consistent for testing independence between stationary time series, and capable
of estimating the optimal dependence lag that maximizes the dependence.
Moreover, it is compatible with a rich family of distance and kernel based
dependence measures, eliminates the need for multiple testing, and exhibits
excellent testing power in various simulation settings.
","[{'version': 'v1', 'created': 'Sun, 18 Aug 2019 17:19:16 GMT'}, {'version': 'v2', 'created': 'Fri, 15 Nov 2019 23:29:57 GMT'}, {'version': 'v3', 'created': 'Fri, 15 May 2020 00:50:32 GMT'}, {'version': 'v4', 'created': 'Mon, 5 Feb 2024 20:16:15 GMT'}, {'version': 'v5', 'created': 'Mon, 27 May 2024 23:15:09 GMT'}]",2024-05-29,"[['Shen', 'Cencheng', ''], ['Chung', 'Jaewon', ''], ['Mehta', 'Ronak', ''], ['Xu', 'Ting', ''], ['Vogelstein', 'Joshua T.', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
37,1909.02373,Yanbin Liu,"Yanbin Liu, Makoto Yamada, Yao-Hung Hubert Tsai, Tam Le, Ruslan
  Salakhutdinov, Yi Yang","LSMI-Sinkhorn: Semi-supervised Mutual Information Estimation with
  Optimal Transport",ECML/PKDD 2021,,10.1007/978-3-030-86486-6_40,,stat.ML cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Estimating mutual information is an important statistics and machine learning
problem. To estimate the mutual information from data, a common practice is
preparing a set of paired samples $\{(\mathbf{x}_i,\mathbf{y}_i)\}_{i=1}^n
\stackrel{\mathrm{i.i.d.}}{\sim} p(\mathbf{x},\mathbf{y})$. However, in many
situations, it is difficult to obtain a large number of data pairs. To address
this problem, we propose the semi-supervised Squared-loss Mutual Information
(SMI) estimation method using a small number of paired samples and the
available unpaired ones. We first represent SMI through the density ratio
function, where the expectation is approximated by the samples from marginals
and its assignment parameters. The objective is formulated using the optimal
transport problem and quadratic programming. Then, we introduce the
Least-Squares Mutual Information with Sinkhorn (LSMI-Sinkhorn) algorithm for
efficient optimization. Through experiments, we first demonstrate that the
proposed method can estimate the SMI without a large number of paired samples.
Then, we show the effectiveness of the proposed LSMI-Sinkhorn algorithm on
various types of machine learning problems such as image matching and photo
album summarization. Code can be found at
https://github.com/csyanbin/LSMI-Sinkhorn.
","[{'version': 'v1', 'created': 'Thu, 5 Sep 2019 12:58:20 GMT'}, {'version': 'v2', 'created': 'Fri, 11 Sep 2020 07:54:10 GMT'}, {'version': 'v3', 'created': 'Sun, 27 Jun 2021 06:34:41 GMT'}]",2024-07-16,"[['Liu', 'Yanbin', ''], ['Yamada', 'Makoto', ''], ['Tsai', 'Yao-Hung Hubert', ''], ['Le', 'Tam', ''], ['Salakhutdinov', 'Ruslan', ''], ['Yang', 'Yi', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
38,1909.03577,Christopher Jung,"Christopher Jung, Katrina Ligett, Seth Neel, Aaron Roth, Saeed
  Sharifi-Malvajerdi, Moshe Shenfeld",A New Analysis of Differential Privacy's Generalization Guarantees,,,,,cs.LG cs.CR stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We give a new proof of the ""transfer theorem"" underlying adaptive data
analysis: that any mechanism for answering adaptively chosen statistical
queries that is differentially private and sample-accurate is also accurate
out-of-sample. Our new proof is elementary and gives structural insights that
we expect will be useful elsewhere. We show: 1) that differential privacy
ensures that the expectation of any query on the posterior distribution on
datasets induced by the transcript of the interaction is close to its true
value on the data distribution, and 2) sample accuracy on its own ensures that
any query answer produced by the mechanism is close to its posterior
expectation with high probability. This second claim follows from a thought
experiment in which we imagine that the dataset is resampled from the posterior
distribution after the mechanism has committed to its answers. The transfer
theorem then follows by summing these two bounds, and in particular, avoids the
""monitor argument"" used to derive high probability bounds in prior work. An
upshot of our new proof technique is that the concrete bounds we obtain are
substantially better than the best previously known bounds, even though the
improvements are in the constants, rather than the asymptotics (which are known
to be tight). As we show, our new bounds outperform the naive
""sample-splitting"" baseline at dramatically smaller dataset sizes compared to
the previous state of the art, bringing techniques from this literature closer
to practicality.
","[{'version': 'v1', 'created': 'Mon, 9 Sep 2019 00:49:03 GMT'}, {'version': 'v2', 'created': 'Tue, 4 Jun 2024 03:08:58 GMT'}]",2024-06-05,"[['Jung', 'Christopher', ''], ['Ligett', 'Katrina', ''], ['Neel', 'Seth', ''], ['Roth', 'Aaron', ''], ['Sharifi-Malvajerdi', 'Saeed', ''], ['Shenfeld', 'Moshe', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
39,1909.04559,Frederik Mallmann-Trenn,Nancy Lynch and Frederik Mallmann-Trenn,Learning Hierarchically Structured Concepts,,,,,cs.AI cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study the question of how concepts that have structure get represented in
the brain. Specifically, we introduce a model for hierarchically structured
concepts and we show how a biologically plausible neural network can recognize
these concepts, and how it can learn them in the first place. Our main goal is
to introduce a general framework for these tasks and prove formally how both
(recognition and learning) can be achieved.
  We show that both tasks can be accomplished even in presence of noise. For
learning, we analyze Oja's rule formally, a well-known biologically-plausible
rule for adjusting the weights of synapses. We complement the learning results
with lower bounds asserting that, in order to recognize concepts of a certain
hierarchical depth, neural networks must have a corresponding number of layers.
","[{'version': 'v1', 'created': 'Tue, 10 Sep 2019 15:11:38 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Feb 2020 20:07:22 GMT'}, {'version': 'v3', 'created': 'Thu, 10 Sep 2020 11:10:16 GMT'}, {'version': 'v4', 'created': 'Sun, 17 Jan 2021 08:23:09 GMT'}, {'version': 'v5', 'created': 'Wed, 1 Sep 2021 07:57:38 GMT'}, {'version': 'v6', 'created': 'Tue, 27 Feb 2024 13:25:45 GMT'}]",2024-02-28,"[['Lynch', 'Nancy', ''], ['Mallmann-Trenn', 'Frederik', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
40,1909.05006,Sanzo Miyazawa,Sanzo Miyazawa,"Boltzmann machine learning and regularization methods for inferring
  evolutionary fields and couplings from a multiple sequence alignment","In arXiv:1909.05006v3 the values of selective temperature for protein
  PF00153, $T_s$ in Table 5 and in the section 2.8, and folding free energy for
  PF00595, and in the v4 the method for soft-thresholding were corrected; shown
  in red. The v2 was published in the IEEE/ACM Transactions on Computational
  Biology and Bioinformatics. The program is available from
  https://gitlab.com/sanzo.miyazawa/BM/","IEEE/ACM Transactions on Computational Biology and Bioinformatics,
  2022 Jan-Feb;19(1):328-342",10.1109/TCBB.2020.2993232,,q-bio.PE cond-mat.stat-mech cs.LG q-bio.BM stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The inverse Potts problem to infer a Boltzmann distribution for homologous
protein sequences from their single-site and pairwise amino acid frequencies
recently attracts a great deal of attention in the studies of protein structure
and evolution. We study regularization and learning methods and how to tune
regularization parameters to correctly infer interactions in Boltzmann machine
learning. Using $L_2$ regularization for fields, group $L_1$ for couplings is
shown to be very effective for sparse couplings in comparison with $L_2$ and
$L_1$. Two regularization parameters are tuned to yield equal values for both
the sample and ensemble averages of evolutionary energy. Both averages smoothly
change and converge, but their learning profiles are very different between
learning methods. The Adam method is modified to make stepsize proportional to
the gradient for sparse couplings. It is shown by first inferring interactions
from protein sequences and then from Monte Carlo samples that the fields and
couplings can be well recovered, but that recovering the pairwise correlations
in the resolution of a total energy is harder for the natural proteins than for
the protein-like sequences. Selective temperature for folding/structural
constrains in protein evolution is also estimated.
","[{'version': 'v1', 'created': 'Tue, 10 Sep 2019 08:41:19 GMT'}, {'version': 'v2', 'created': 'Tue, 26 May 2020 04:47:06 GMT'}, {'version': 'v3', 'created': 'Fri, 18 Jun 2021 05:41:55 GMT'}, {'version': 'v4', 'created': 'Sat, 2 Oct 2021 02:18:28 GMT'}, {'version': 'v5', 'created': 'Sun, 21 Jul 2024 09:39:03 GMT'}]",2024-07-23,"[['Miyazawa', 'Sanzo', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
41,1909.10228,Zhigang Yao,Zhigang Yao and Yuqing Xia,Manifold Fitting under Unbounded Noise,,,,,stat.ML cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There has been an emerging trend in non-Euclidean statistical analysis of
aiming to recover a low dimensional structure, namely a manifold, underlying
the high dimensional data. Recovering the manifold requires the noise to be of
certain concentration. Existing methods address this problem by constructing an
approximated manifold based on the tangent space estimation at each sample
point. Although theoretical convergence for these methods is guaranteed, either
the samples are noiseless or the noise is bounded. However, if the noise is
unbounded, which is a common scenario, the tangent space estimation at the
noisy samples will be blurred. Fitting a manifold from the blurred tangent
space might increase the inaccuracy. In this paper, we introduce a new
manifold-fitting method, by which the output manifold is constructed by
directly estimating the tangent spaces at the projected points on the
underlying manifold, rather than at the sample points, to decrease the error
caused by the noise. Assuming the noise is unbounded, our new method provides
theoretical convergence in high probability, in terms of the upper bound of the
distance between the estimated and underlying manifold. The smoothness of the
estimated manifold is also evaluated by bounding the supremum of twice
difference above. Numerical simulations are provided to validate our
theoretical findings and demonstrate the advantages of our method over other
relevant manifold fitting methods. Finally, our method is applied to real data
examples.
","[{'version': 'v1', 'created': 'Mon, 23 Sep 2019 08:55:41 GMT'}, {'version': 'v2', 'created': 'Thu, 12 Jan 2023 02:08:28 GMT'}, {'version': 'v3', 'created': 'Sun, 9 Jun 2024 06:22:48 GMT'}]",2024-06-11,"[['Yao', 'Zhigang', ''], ['Xia', 'Yuqing', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
42,1910.04331,Haoran Dou,"Haoran Dou, Xin Yang, Jikuan Qian, Wufeng Xue, Hao Qin, Xu Wang,
  Lequan Yu, Shujun Wang, Yi Xiong, Pheng-Ann Heng, Dong Ni","Agent with Warm Start and Active Termination for Plane Localization in
  3D Ultrasound","9 pages, 5 figures, 1 table. Accepted by MICCAI 2019 (oral)",,,,eess.IV cs.CV cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Standard plane localization is crucial for ultrasound (US) diagnosis. In
prenatal US, dozens of standard planes are manually acquired with a 2D probe.
It is time-consuming and operator-dependent. In comparison, 3D US containing
multiple standard planes in one shot has the inherent advantages of less
user-dependency and more efficiency. However, manual plane localization in US
volume is challenging due to the huge search space and large fetal posture
variation. In this study, we propose a novel reinforcement learning (RL)
framework to automatically localize fetal brain standard planes in 3D US. Our
contribution is two-fold. First, we equip the RL framework with a
landmark-aware alignment module to provide warm start and strong spatial bounds
for the agent actions, thus ensuring its effectiveness. Second, instead of
passively and empirically terminating the agent inference, we propose a
recurrent neural network based strategy for active termination of the agent's
interaction procedure. This improves both the accuracy and efficiency of the
localization system. Extensively validated on our in-house large dataset, our
approach achieves the accuracy of 3.4mm/9.6{\deg} and 2.7mm/9.1{\deg} for the
transcerebellar and transthalamic plane localization, respectively. Ourproposed
RL framework is general and has the potential to improve the efficiency and
standardization of US scanning.
","[{'version': 'v1', 'created': 'Thu, 10 Oct 2019 02:21:52 GMT'}, {'version': 'v2', 'created': 'Sun, 3 Mar 2024 12:01:18 GMT'}]",2024-03-05,"[['Dou', 'Haoran', ''], ['Yang', 'Xin', ''], ['Qian', 'Jikuan', ''], ['Xue', 'Wufeng', ''], ['Qin', 'Hao', ''], ['Wang', 'Xu', ''], ['Yu', 'Lequan', ''], ['Wang', 'Shujun', ''], ['Xiong', 'Yi', ''], ['Heng', 'Pheng-Ann', ''], ['Ni', 'Dong', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
43,1910.04935,Haoran Dou,"Xin Yang, Wenlong Shi, Haoran Dou, Jikuan Qian, Yi Wang, Wufeng Xue,
  Shengli Li, Dong Ni, Pheng-Ann Heng",FetusMap: Fetal Pose Estimation in 3D Ultrasound,"9 pages, 6 figures, 2 tables. Accepted by MICCAI 2019",,,,cs.CV cs.LG eess.IV,http://creativecommons.org/licenses/by/4.0/,"  The 3D ultrasound (US) entrance inspires a multitude of automated prenatal
examinations. However, studies about the structuralized description of the
whole fetus in 3D US are still rare. In this paper, we propose to estimate the
3D pose of fetus in US volumes to facilitate its quantitative analyses in
global and local scales. Given the great challenges in 3D US, including the
high volume dimension, poor image quality, symmetric ambiguity in anatomical
structures and large variations of fetal pose, our contribution is three-fold.
(i) This is the first work about 3D pose estimation of fetus in the literature.
We aim to extract the skeleton of whole fetus and assign different
segments/joints with correct torso/limb labels. (ii) We propose a
self-supervised learning (SSL) framework to finetune the deep network to form
visually plausible pose predictions. Specifically, we leverage the
landmark-based registration to effectively encode case-adaptive anatomical
priors and generate evolving label proxy for supervision. (iii) To enable our
3D network perceive better contextual cues with higher resolution input under
limited computing resource, we further adopt the gradient check-pointing (GCP)
strategy to save GPU memory and improve the prediction. Extensively validated
on a large 3D US dataset, our method tackles varying fetal poses and achieves
promising results. 3D pose estimation of fetus has potentials in serving as a
map to provide navigation for many advanced studies.
","[{'version': 'v1', 'created': 'Fri, 11 Oct 2019 01:45:09 GMT'}, {'version': 'v2', 'created': 'Sun, 3 Mar 2024 12:08:37 GMT'}]",2024-03-05,"[['Yang', 'Xin', ''], ['Shi', 'Wenlong', ''], ['Dou', 'Haoran', ''], ['Qian', 'Jikuan', ''], ['Wang', 'Yi', ''], ['Xue', 'Wufeng', ''], ['Li', 'Shengli', ''], ['Ni', 'Dong', ''], ['Heng', 'Pheng-Ann', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
44,1910.05421,Amine Remita,Amine M. Remita and Abdoulaye Banir\'e Diallo,"Statistical Linear Models in Virus Genomic Alignment-free
  Classification: Application to Hepatitis C Viruses","Accepted as a regular paper for publication in IEEE BIBM 2019 [v3:
  Fix indices in Markov classifier]","2019 IEEE International Conference on Bioinformatics and
  Biomedicine (BIBM), San Diego, CA, USA, 2019, pp. 474-481",10.1109/BIBM47256.2019.8983375,,cs.LG q-bio.GN stat.ML,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Viral sequence classification is an important task in pathogen detection,
epidemiological surveys and evolutionary studies. Statistical learning methods
are widely used to classify and identify viral sequences in samples from
environments. These methods face several challenges associated with the nature
and properties of viral genomes such as recombination, mutation rate and
diversity. Also, new generations of sequencing technologies rise other
difficulties by generating massive amounts of fragmented sequences. While
linear classifiers are often used to classify viruses, there is a lack of
exploration of the accuracy space of existing models in the context of
alignment free approaches. In this study, we present an exhaustive assessment
procedure exploring the power of linear classifiers in genotyping and subtyping
partial and complete genomes. It is applied to the Hepatitis C viruses (HCV).
Several variables are considered in this investigation such as classifier types
(generative and discriminative) and their hyper-parameters (smoothing value and
regularization penalty function), the classification task (genotyping and
subtyping), the length of the tested sequences (partial and complete) and the
length of k-mer words. Overall, several classifiers perform well given a set of
precise combination of the experimental variables mentioned above. Finally, we
provide the procedure and benchmark data to allow for more robust assessment of
classification from virus genomes.
","[{'version': 'v1', 'created': 'Fri, 11 Oct 2019 21:40:24 GMT'}, {'version': 'v2', 'created': 'Wed, 6 Nov 2019 18:23:53 GMT'}, {'version': 'v3', 'created': 'Tue, 28 May 2024 21:23:11 GMT'}]",2024-05-30,"[['Remita', 'Amine M.', ''], ['Diallo', 'Abdoulaye Baniré', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
45,1910.13988,Roman Goldenberg,"Dror Simon, Miriam Farber, Roman Goldenberg","Auto-Annotation Quality Prediction for Semi-Supervised Learning with
  Ensembles","10 pages, 1 figure, 5 tables","2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW)",10.1109/CVPRW50498.2020.00465,,cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Auto-annotation by ensemble of models is an efficient method of learning on
unlabeled data. Wrong or inaccurate annotations generated by the ensemble may
lead to performance degradation of the trained model. To deal with this problem
we propose filtering the auto-labeled data using a trained model that predicts
the quality of the annotation from the degree of consensus between ensemble
models. Using semantic segmentation as an example, we show the advantage of the
proposed auto-annotation filtering over training on data contaminated with
inaccurate labels.
  Moreover, our experimental results show that in the case of semantic
segmentation, the performance of a state-of-the-art model can be achieved by
training it with only a fraction (30$\%$) of the original manually labeled data
set, and replacing the rest with the auto-annotated, quality filtered labels.
","[{'version': 'v1', 'created': 'Wed, 30 Oct 2019 17:10:21 GMT'}]",2024-03-14,"[['Simon', 'Dror', ''], ['Farber', 'Miriam', ''], ['Goldenberg', 'Roman', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
46,1911.00119,Chengcheng Wan,"Chengcheng Wan, Muhammad Santriaji, Eri Rogers, Henry Hoffmann,
  Michael Maire, Shan Lu",ALERT: Accurate Learning for Energy and Timeliness,,"USENIX ATC, 2020",10.1145/3485730.3493446,,cs.PF cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  An increasing number of software applications incorporate runtime Deep Neural
Networks (DNNs) to process sensor data and return inference results to humans.
Effective deployment of DNNs in these interactive scenarios requires meeting
latency and accuracy constraints while minimizing energy, a problem exacerbated
by common system dynamics. Prior approaches handle dynamics through either (1)
system-oblivious DNN adaptation, which adjusts DNN latency/accuracy tradeoffs,
or (2) application-oblivious system adaptation, which adjusts resources to
change latency/energy tradeoffs. In contrast, this paper improves on the
state-of-the-art by coordinating application- and system-level adaptation.
ALERT, our runtime scheduler, uses a probabilistic model to detect
environmental volatility and then simultaneously select both a DNN and a system
resource configuration to meet latency, accuracy, and energy constraints. We
evaluate ALERT on CPU and GPU platforms for image and speech tasks in dynamic
environments. ALERT's holistic approach achieves more than 13% energy
reduction, and 27% error reduction over prior approaches that adapt solely at
the application or system level. Furthermore, ALERT incurs only 3% more energy
consumption and 2% higher DNN-inference error than an oracle scheme with
perfect application and system knowledge.
","[{'version': 'v1', 'created': 'Thu, 31 Oct 2019 21:40:42 GMT'}, {'version': 'v2', 'created': 'Thu, 28 May 2020 12:33:53 GMT'}]",2024-07-09,"[['Wan', 'Chengcheng', ''], ['Santriaji', 'Muhammad', ''], ['Rogers', 'Eri', ''], ['Hoffmann', 'Henry', ''], ['Maire', 'Michael', ''], ['Lu', 'Shan', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
47,1911.04766,Tobias Geibinger,"Tobias Geibinger, Florian Mischek and Nysret Musliu","Investigating Constraint Programming and Hybrid Methods for Real World
  Industrial Test Laboratory Scheduling",,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper we deal with a complex real world scheduling problem closely
related to the well-known Resource-Constrained Project Scheduling Problem
(RCPSP). The problem concerns industrial test laboratories in which a large
number of tests has to be performed by qualified personnel using specialised
equipment, while respecting deadlines and other constraints. We present
different constraint programming models and search strategies for this problem.
Furthermore, we propose a Very Large Neighborhood Search approach based on our
CP methods. Our models are evaluated using CP solvers and a MIP solver both on
real-world test laboratory data and on a set of generated instances of
different sizes based on the real-world data. Further, we compare the exact
approaches with VLNS and a Simulated Annealing heuristic. We could find
feasible solutions for all instances and several optimal solutions and we show
that using VLNS we can improve upon the results of the other approaches.
","[{'version': 'v1', 'created': 'Tue, 12 Nov 2019 10:03:16 GMT'}, {'version': 'v2', 'created': 'Thu, 23 Sep 2021 11:52:55 GMT'}, {'version': 'v3', 'created': 'Wed, 7 Dec 2022 10:16:33 GMT'}, {'version': 'v4', 'created': 'Wed, 14 Aug 2024 19:34:51 GMT'}]",2024-08-16,"[['Geibinger', 'Tobias', ''], ['Mischek', 'Florian', ''], ['Musliu', 'Nysret', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
48,1911.08756,Jarom\'ir Janisch,"Jarom\'ir Janisch, Tom\'a\v{s} Pevn\'y and Viliam Lis\'y",Classification with Costly Features in Hierarchical Deep Sets,"formerly Hierarchical Multiple-Instance Data Classification with
  Costly Features; RL4RealLife @ ICML2021; code available at
  https://github.com/jaromiru/rcwcf",,10.1007/s10994-024-06565-4,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Classification with Costly Features (CwCF) is a classification problem that
includes the cost of features in the optimization criteria. Individually for
each sample, its features are sequentially acquired to maximize accuracy while
minimizing the acquired features' cost. However, existing approaches can only
process data that can be expressed as vectors of fixed length. In real life,
the data often possesses rich and complex structure, which can be more
precisely described with formats such as XML or JSON. The data is hierarchical
and often contains nested lists of objects. In this work, we extend an existing
deep reinforcement learning-based algorithm with hierarchical deep sets and
hierarchical softmax, so that it can directly process this data. The extended
method has greater control over which features it can acquire and, in
experiments with seven datasets, we show that this leads to superior
performance. To showcase the real usage of the new method, we apply it to a
real-life problem of classifying malicious web domains, using an online
service.
","[{'version': 'v1', 'created': 'Wed, 20 Nov 2019 08:15:09 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Feb 2020 13:20:38 GMT'}, {'version': 'v3', 'created': 'Mon, 14 Sep 2020 13:21:42 GMT'}, {'version': 'v4', 'created': 'Mon, 26 Jul 2021 13:59:18 GMT'}, {'version': 'v5', 'created': 'Wed, 3 Aug 2022 15:38:27 GMT'}, {'version': 'v6', 'created': 'Thu, 29 Feb 2024 15:30:15 GMT'}]",2024-07-17,"[['Janisch', 'Jaromír', ''], ['Pevný', 'Tomáš', ''], ['Lisý', 'Viliam', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
49,1911.10522,Fabien Geyer,Fabien Geyer and Steffen Bondorf,"On the Robustness of Deep Learning-predicted Contention Models for
  Network Calculus",,,10.1109/ISCC50000.2020.9219693,,cs.NI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The network calculus (NC) analysis takes a simple model consisting of a
network of schedulers and data flows crossing them. A number of analysis
""building blocks"" can then be applied to capture the model without imposing
pessimistic assumptions like self-contention on tandems of servers. Yet, adding
pessimism cannot always be avoided. To compute the best bound on a single
flow's end-to-end delay thus boils down to finding the least pessimistic
contention models for all tandems of schedulers in the network - and an
exhaustive search can easily become a very resource intensive task. The
literature proposes a promising solution to this dilemma: a heuristic making
use of machine learning (ML) predictions inside the NC analysis.
  While results of this work were promising in terms of delay bound quality and
computational effort, there is little to no insight on when a prediction is
made or if the trained algorithm can achieve similarly striking results in
networks vastly differing from its training data. In this paper, we address
these pending questions. We evaluate the influence of the training data and its
features on accuracy, impact and scalability. Additionally, we contribute an
extension of the method by predicting the best $n$ contention model
alternatives in order to achieve increased robustness for its application
outside the training data. Our numerical evaluation shows that good accuracy
can still be achieved on large networks although we restrict the training to
networks that are two orders of magnitude smaller.
","[{'version': 'v1', 'created': 'Sun, 24 Nov 2019 13:00:41 GMT'}, {'version': 'v2', 'created': 'Fri, 17 Jul 2020 04:34:46 GMT'}]",2024-01-17,"[['Geyer', 'Fabien', ''], ['Bondorf', 'Steffen', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
50,1912.04968,David Ahmedt-Aristizabal,"David Ahmedt-Aristizabal, Tharindu Fernando, Simon Denman, Lars
  Petersson, Matthew J. Aburn, Clinton Fookes",Neural Memory Networks for Seizure Type Classification,"Proceedings of the IEEE International Conference of Engineering in
  Medicine and Biology Society. 2020",,10.1109/EMBC44109.2020.9175641,,eess.SP cs.LG cs.NE stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Classification of seizure type is a key step in the clinical process for
evaluating an individual who presents with seizures. It determines the course
of clinical diagnosis and treatment, and its impact stretches beyond the
clinical domain to epilepsy research and the development of novel therapies.
Automated identification of seizure type may facilitate understanding of the
disease, and seizure detection and prediction has been the focus of recent
research that has sought to exploit the benefits of machine learning and deep
learning architectures. Nevertheless, there is not yet a definitive solution
for automating the classification of seizure type, a task that must currently
be performed by an expert epileptologist. Inspired by recent advances in neural
memory networks (NMNs), we introduce a novel approach for the classification of
seizure type using electrophysiological data. We first explore the performance
of traditional deep learning techniques which use convolutional and recurrent
neural networks, and enhance these architectures by using external memory
modules with trainable neural plasticity. We show that our model achieves a
state-of-the-art weighted F1 score of 0.945 for seizure type classification on
the TUH EEG Seizure Corpus with the IBM TUSZ preprocessed data. This work
highlights the potential of neural memory networks to support the field of
epilepsy research, along with biomedical research and signal analysis more
broadly.
","[{'version': 'v1', 'created': 'Tue, 10 Dec 2019 20:27:40 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Jan 2020 02:04:44 GMT'}]",2024-03-06,"[['Ahmedt-Aristizabal', 'David', ''], ['Fernando', 'Tharindu', ''], ['Denman', 'Simon', ''], ['Petersson', 'Lars', ''], ['Aburn', 'Matthew J.', ''], ['Fookes', 'Clinton', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
51,1912.05467,Xun Wang Dr,"Rumeng Li, Xun Wang, Hong Yu","MetaMT,a MetaLearning Method Leveraging Multiple Domain Data for Low
  Resource Machine Translation",,,10.1609/aaai.v34i05.6339,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Manipulating training data leads to robust neural models for MT.
","[{'version': 'v1', 'created': 'Wed, 11 Dec 2019 17:05:18 GMT'}]",2024-02-23,"[['Li', 'Rumeng', ''], ['Wang', 'Xun', ''], ['Yu', 'Hong', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
52,1912.09818,Leon Sixt,"Leon Sixt, Maximilian Granz, Tim Landgraf",When Explanations Lie: Why Many Modified BP Attributions Fail,Published in ICML 2020 - Updated Proof,,,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Attribution methods aim to explain a neural network's prediction by
highlighting the most relevant image areas. A popular approach is to
backpropagate (BP) a custom relevance score using modified rules, rather than
the gradient. We analyze an extensive set of modified BP methods: Deep Taylor
Decomposition, Layer-wise Relevance Propagation (LRP), Excitation BP,
PatternAttribution, DeepLIFT, Deconv, RectGrad, and Guided BP. We find
empirically that the explanations of all mentioned methods, except for
DeepLIFT, are independent of the parameters of later layers. We provide
theoretical insights for this surprising behavior and also analyze why DeepLIFT
does not suffer from this limitation. Empirically, we measure how information
of later layers is ignored by using our new metric, cosine similarity
convergence (CSC). The paper provides a framework to assess the faithfulness of
new and existing modified BP methods theoretically and empirically. For code
see: https://github.com/berleon/when-explanations-lie
","[{'version': 'v1', 'created': 'Fri, 20 Dec 2019 13:46:31 GMT'}, {'version': 'v2', 'created': 'Mon, 23 Dec 2019 07:00:17 GMT'}, {'version': 'v3', 'created': 'Wed, 19 Feb 2020 17:45:40 GMT'}, {'version': 'v4', 'created': 'Fri, 21 Feb 2020 16:39:10 GMT'}, {'version': 'v5', 'created': 'Thu, 7 May 2020 19:24:58 GMT'}, {'version': 'v6', 'created': 'Thu, 13 Aug 2020 14:12:07 GMT'}, {'version': 'v7', 'created': 'Mon, 19 Feb 2024 15:45:50 GMT'}]",2024-02-20,"[['Sixt', 'Leon', ''], ['Granz', 'Maximilian', ''], ['Landgraf', 'Tim', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
53,1912.12615,Anna Knezevic,"Anna Knezevic, Nikolai Dokuchaev","Approximating intractable short ratemodel distribution with neural
  network",Incorrect methodology for generation of stochastic scenarios,,,,stat.ML cs.LG q-fin.MF,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose an algorithm which predicts each subsequent time step relative to
the previous timestep of intractable short rate model (when adjusted for drift
and overall distribution of previous percentile result) and show that the
method achieves superior outcomes to the unbiased estimate both on the trained
dataset and different validation data.
","[{'version': 'v1', 'created': 'Sun, 29 Dec 2019 09:08:49 GMT'}, {'version': 'v10', 'created': 'Wed, 6 Mar 2024 06:07:26 GMT'}, {'version': 'v11', 'created': 'Fri, 12 Apr 2024 08:58:33 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Jan 2020 05:48:04 GMT'}, {'version': 'v3', 'created': 'Tue, 14 Jan 2020 04:00:33 GMT'}, {'version': 'v4', 'created': 'Sat, 18 Jan 2020 06:52:32 GMT'}, {'version': 'v5', 'created': 'Wed, 5 Feb 2020 01:17:21 GMT'}, {'version': 'v6', 'created': 'Tue, 11 Feb 2020 06:42:10 GMT'}, {'version': 'v7', 'created': 'Sun, 23 Feb 2020 00:56:00 GMT'}, {'version': 'v8', 'created': 'Wed, 28 Feb 2024 12:36:36 GMT'}, {'version': 'v9', 'created': 'Fri, 1 Mar 2024 05:32:46 GMT'}]",2024-04-15,"[['Knezevic', 'Anna', ''], ['Dokuchaev', 'Nikolai', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
54,1912.12761,Hussain Mohammed Kabir Mr,"H M Dipu Kabir, Abbas Khosravi, Abdollah Kavousi-Fard, Saeid
  Nahavandi, Dipti Srinivasan",Optimal Uncertainty-guided Neural Network Training,,"Applied Soft Computing, 2021",10.1016/j.asoc.2020.106878,,stat.ML cs.LG,http://creativecommons.org/publicdomain/zero/1.0/,"  The neural network (NN)-based direct uncertainty quantification (UQ) methods
have achieved the state of the art performance since the first inauguration,
known as the lower-upper-bound estimation (LUBE) method. However,
currently-available cost functions for uncertainty guided NN training are not
always converging and all converged NNs are not generating optimized prediction
intervals (PIs). Moreover, several groups have proposed different quality
criteria for PIs. These raise a question about their relative effectiveness.
Most of the existing cost functions of uncertainty guided NN training are not
customizable and the convergence of training is uncertain. Therefore, in this
paper, we propose a highly customizable smooth cost function for developing NNs
to construct optimal PIs. The optimized average width of PIs, PI-failure
distances and the PI coverage probability (PICP) are computed for the test
dataset. The performance of the proposed method is examined for the wind power
generation and the electricity demand data. Results show that the proposed
method reduces variation in the quality of PIs, accelerates the training, and
improves convergence probability from 99.2% to 99.8%.
","[{'version': 'v1', 'created': 'Mon, 30 Dec 2019 00:03:28 GMT'}]",2024-07-16,"[['Kabir', 'H M Dipu', ''], ['Khosravi', 'Abbas', ''], ['Kavousi-Fard', 'Abdollah', ''], ['Nahavandi', 'Saeid', ''], ['Srinivasan', 'Dipti', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
55,2001.01258,Vegard Antun,"Nina M. Gottschling, Vegard Antun, Anders C. Hansen and Ben Adcock","The troublesome kernel -- On hallucinations, no free lunches and the
  accuracy-stability trade-off in inverse problems",,,,,cs.LG cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Methods inspired by Artificial Intelligence (AI) are starting to
fundamentally change computational science and engineering through breakthrough
performances on challenging problems. However, reliability and trustworthiness
of such techniques is a major concern. In inverse problems in imaging, the
focus of this paper, there is increasing empirical evidence that methods may
suffer from hallucinations, i.e., false, but realistic-looking artifacts;
instability, i.e., sensitivity to perturbations in the data; and unpredictable
generalization, i.e., excellent performance on some images, but significant
deterioration on others. This paper provides a theoretical foundation for these
phenomena. We give mathematical explanations for how and when such effects
arise in arbitrary reconstruction methods, with several of our results taking
the form of `no free lunch' theorems. Specifically, we show that (i) methods
that overperform on a single image can wrongly transfer details from one image
to another, creating a hallucination, (ii) methods that overperform on two or
more images can hallucinate or be unstable, (iii) optimizing the
accuracy-stability trade-off is generally difficult, (iv) hallucinations and
instabilities, if they occur, are not rare events, and may be encouraged by
standard training, (v) it may be impossible to construct optimal reconstruction
maps for certain problems. Our results trace these effects to the kernel of the
forward operator whenever it is nontrivial, but also apply to the case when the
forward operator is ill-conditioned. Based on these insights, our work aims to
spur research into new ways to develop robust and reliable AI-based methods for
inverse problems in imaging.
","[{'version': 'v1', 'created': 'Sun, 5 Jan 2020 15:30:23 GMT'}, {'version': 'v2', 'created': 'Tue, 10 Jan 2023 14:09:43 GMT'}, {'version': 'v3', 'created': 'Thu, 14 Dec 2023 19:27:50 GMT'}, {'version': 'v4', 'created': 'Tue, 18 Jun 2024 19:18:47 GMT'}]",2024-06-21,"[['Gottschling', 'Nina M.', ''], ['Antun', 'Vegard', ''], ['Hansen', 'Anders C.', ''], ['Adcock', 'Ben', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
56,2001.04794,Francesco Bardozzo,"Francesco Bardozzo, Pietro Lio', Roberto Tagliaferri","A machine learning approach to investigate regulatory control circuits
  in bacterial metabolic pathways","5 pages, 3 figures",,10.1093/bioinformatics/btaa966,,q-bio.MN cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, a machine learning approach for identifying the multi-omics
metabolic regulatory control circuits inside the pathways is described.
Therefore, the identification of bacterial metabolic pathways that are more
regulated than others in term of their multi-omics follows from the analysis of
these circuits . This is a consequence of the alternation of the omic values of
codon usage and protein abundance along with the circuits. In this work, the
E.Coli's Glycolysis and its multi-omic circuit features are shown as an
example.
","[{'version': 'v1', 'created': 'Mon, 13 Jan 2020 11:04:26 GMT'}]",2024-04-09,"[['Bardozzo', 'Francesco', ''], [""Lio'"", 'Pietro', ''], ['Tagliaferri', 'Roberto', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
57,2001.04816,Zhenlin Wang,"Z. Wang, X. Huan, K. Garikipati","Variational system identification of the partial differential equations
  governing microstructure evolution in materials: Inference over sparse and
  spatially unrelated data",,"Computer Methods in Applied Mechanics and Engineering 377 (2021)
  113706",10.1016/j.cma.2021.113706,,cs.CE cs.NA math.NA physics.data-an,http://creativecommons.org/licenses/by/4.0/,"  Pattern formation is a widely observed phenomenon in diverse fields including
materials physics, developmental biology and ecology, among many others. The
physics underlying the patterns is specific to the mechanisms, and is encoded
by partial differential equations (PDEs). With the aim of discovering hidden
physics, we have previously presented a variational approach to identifying
such systems of PDEs in the face of noisy data at varying fidelities (Computer
Methods in Applied Mechanics and Engineering, 353:201-216, 2019). Here, we
extend our variational system identification methods to address the challenges
presented by image data on microstructures in materials physics. PDEs are
formally posed as initial and boundary value problems over combinations of time
intervals and spatial domains whose evolution is either fixed or can be
tracked. However, the vast majority of microscopy techniques for evolving
microstructure in a given material system deliver micrographs of pattern
evolution over domains that bear no relation with each other at different time
instants. The temporal resolution can rarely capture the fastest time scales
that dominate the early dynamics, and noise abounds. Furthermore, data for
evolution of the same phenomenon in a material system may well be obtained from
different physical specimens. Against this backdrop of spatially unrelated,
sparse and multi-source data, we exploit the variational framework to make
judicious choices of weighting functions and identify PDE operators from the
dynamics. A consistency condition arises for parsimonious inference of a
minimal set of the spatial operators at steady state. It is complemented by a
confirmation test that provides a sharp condition for acceptance of the
inferred operators. The entire framework is demonstrated on synthetic data that
reflect the characteristics of the experimental material microscopy images.
","[{'version': 'v1', 'created': 'Sat, 11 Jan 2020 18:19:50 GMT'}, {'version': 'v2', 'created': 'Thu, 9 Apr 2020 16:22:19 GMT'}, {'version': 'v3', 'created': 'Fri, 10 Apr 2020 01:36:57 GMT'}, {'version': 'v4', 'created': 'Fri, 29 Jan 2021 20:21:50 GMT'}]",2024-03-28,"[['Wang', 'Z.', ''], ['Huan', 'X.', ''], ['Garikipati', 'K.', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
58,2001.05371,Wolfgang Stammer,"Patrick Schramowski, Wolfgang Stammer, Stefano Teso, Anna Brugger,
  Xiaoting Shao, Hans-Georg Luigs, Anne-Katrin Mahlein, Kristian Kersting","Making deep neural networks right for the right scientific reasons by
  interacting with their explanations",arXiv admin note: text overlap with arXiv:1805.08578,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep neural networks have shown excellent performances in many real-world
applications. Unfortunately, they may show ""Clever Hans""-like behavior --
making use of confounding factors within datasets -- to achieve high
performance. In this work, we introduce the novel learning setting of
""explanatory interactive learning"" (XIL) and illustrate its benefits on a plant
phenotyping research task. XIL adds the scientist into the training loop such
that she interactively revises the original model via providing feedback on its
explanations. Our experimental results demonstrate that XIL can help avoiding
Clever Hans moments in machine learning and encourages (or discourages, if
appropriate) trust into the underlying model.
","[{'version': 'v1', 'created': 'Wed, 15 Jan 2020 15:20:55 GMT'}, {'version': 'v2', 'created': 'Fri, 31 Jan 2020 11:59:54 GMT'}, {'version': 'v3', 'created': 'Fri, 19 Jun 2020 13:38:58 GMT'}, {'version': 'v4', 'created': 'Tue, 5 Mar 2024 12:49:00 GMT'}]",2024-03-06,"[['Schramowski', 'Patrick', ''], ['Stammer', 'Wolfgang', ''], ['Teso', 'Stefano', ''], ['Brugger', 'Anna', ''], ['Shao', 'Xiaoting', ''], ['Luigs', 'Hans-Georg', ''], ['Mahlein', 'Anne-Katrin', ''], ['Kersting', 'Kristian', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
59,2001.05989,Vladimir Vovk,Vladimir Vovk,Cross-conformal e-prediction,"8 pages. This version: exposition improved; proof of Proposition 4
  added",,,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This note discusses a simple modification of cross-conformal prediction
inspired by recent work on e-values. The precursor of conformal prediction
developed in the 1990s by Gammerman, Vapnik, and Vovk was also based on
e-values and is called conformal e-prediction in this note. Replacing e-values
by p-values led to conformal prediction, which has important advantages over
conformal e-prediction without obvious disadvantages. The situation with
cross-conformal prediction is, however, different: whereas for cross-conformal
prediction validity is only an empirical fact (and can be broken with excessive
randomization), this note draws the reader's attention to the obvious fact that
cross-conformal e-prediction enjoys a guaranteed property of validity.
","[{'version': 'v1', 'created': 'Thu, 16 Jan 2020 18:41:17 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Jun 2024 11:02:53 GMT'}]",2024-06-28,"[['Vovk', 'Vladimir', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
60,2001.06047,Assaf Marron,"Assaf Marron, Lior Limonad, Sarah Pollack, and David Harel","Expecting the Unexpected: Developing Autonomous-System Design Principles
  for Reacting to Unpredicted Events and Conditions",6 pages; 1 figure,"2020 IEEE/ACM 15th International Symposium on Software Engineering
  for Adaptive and Self-Managing Systems (SEAMS)",10.1145/3387939.3391607,,cs.SE cs.AI cs.HC cs.LG cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  When developing autonomous systems, engineers and other stakeholders make
great effort to prepare the system for all foreseeable events and conditions.
However, these systems are still bound to encounter events and conditions that
were not considered at design time. For reasons like safety, cost, or ethics,
it is often highly desired that these new situations be handled correctly upon
first encounter. In this paper we first justify our position that there will
always exist unpredicted events and conditions, driven among others by: new
inventions in the real world; the diversity of world-wide system deployments
and uses; and, the non-negligible probability that multiple seemingly unlikely
events, which may be neglected at design time, will not only occur, but occur
together. We then argue that despite this unpredictability property, handling
these events and conditions is indeed possible. Hence, we offer and exemplify
design principles that when applied in advance, can enable systems to deal, in
the future, with unpredicted circumstances. We conclude with a discussion of
how this work and a broader theoretical study of the unexpected can contribute
toward a foundation of engineering principles for developing trustworthy
next-generation autonomous systems.
","[{'version': 'v1', 'created': 'Thu, 16 Jan 2020 19:39:01 GMT'}, {'version': 'v2', 'created': 'Thu, 23 Jan 2020 12:30:23 GMT'}, {'version': 'v3', 'created': 'Sat, 25 Jan 2020 13:39:32 GMT'}]",2024-08-13,"[['Marron', 'Assaf', ''], ['Limonad', 'Lior', ''], ['Pollack', 'Sarah', ''], ['Harel', 'David', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
61,2001.11704,Shay Moran,Noga Alon and Alon Gonen and Elad Hazan and Shay Moran,Boosting Simple Learners,Journal version,"TheoretiCS, Volume 2 (June 19, 2023) theoretics:9253",10.46298/theoretics.23.8,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Boosting is a celebrated machine learning approach which is based on the idea
of combining weak and moderately inaccurate hypotheses to a strong and accurate
one. We study boosting under the assumption that the weak hypotheses belong to
a class of bounded capacity. This assumption is inspired by the common
convention that weak hypotheses are ""rules-of-thumbs"" from an ""easy-to-learn
class"". (Schapire and Freund~'12, Shalev-Shwartz and Ben-David '14.) Formally,
we assume the class of weak hypotheses has a bounded VC dimension. We focus on
two main questions: (i) Oracle Complexity: How many weak hypotheses are needed
to produce an accurate hypothesis? We design a novel boosting algorithm and
demonstrate that it circumvents a classical lower bound by Freund and Schapire
('95, '12). Whereas the lower bound shows that $\Omega({1}/{\gamma^2})$ weak
hypotheses with $\gamma$-margin are sometimes necessary, our new method
requires only $\tilde{O}({1}/{\gamma})$ weak hypothesis, provided that they
belong to a class of bounded VC dimension. Unlike previous boosting algorithms
which aggregate the weak hypotheses by majority votes, the new boosting
algorithm uses more complex (""deeper"") aggregation rules. We complement this
result by showing that complex aggregation rules are in fact necessary to
circumvent the aforementioned lower bound. (ii) Expressivity: Which tasks can
be learned by boosting weak hypotheses from a bounded VC class? Can complex
concepts that are ""far away"" from the class be learned? Towards answering the
first question we {introduce combinatorial-geometric parameters which capture
expressivity in boosting.} As a corollary we provide an affirmative answer to
the second question for well-studied classes, including half-spaces and
decision stumps. Along the way, we establish and exploit connections with
Discrepancy Theory.
","[{'version': 'v1', 'created': 'Fri, 31 Jan 2020 08:34:56 GMT'}, {'version': 'v2', 'created': 'Tue, 5 May 2020 04:37:17 GMT'}, {'version': 'v3', 'created': 'Thu, 8 Apr 2021 07:10:41 GMT'}, {'version': 'v4', 'created': 'Sun, 27 Mar 2022 08:17:49 GMT'}, {'version': 'v5', 'created': 'Tue, 9 Aug 2022 10:31:54 GMT'}, {'version': 'v6', 'created': 'Thu, 22 Sep 2022 06:48:18 GMT'}, {'version': 'v7', 'created': 'Thu, 29 Dec 2022 08:22:57 GMT'}, {'version': 'v8', 'created': 'Fri, 16 Jun 2023 14:06:37 GMT'}]",2024-02-14,"[['Alon', 'Noga', ''], ['Gonen', 'Alon', ''], ['Hazan', 'Elad', ''], ['Moran', 'Shay', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
62,2002.01444,Jakub Marecek,Quan Zhou and Jakub Marecek,"Learning of Linear Dynamical Systems as a Non-Commutative Polynomial
  Optimization Problem","14 pages, 4 figures; retitled to reflect the title of the the
  published version",IEEE Transactions on Automatic Control (2024),10.1109/TAC.2023.3313351,,math.OC cs.LG cs.SY eess.SY stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There has been much recent progress in forecasting the next observation of a
linear dynamical system (LDS), which is known as the improper learning, as well
as in the estimation of its system matrices, which is known as the proper
learning of LDS. We present an approach to proper learning of LDS, which in
spite of the non-convexity of the problem, guarantees global convergence of
numerical solutions to a least-squares estimator. We present promising
computational results.
","[{'version': 'v1', 'created': 'Tue, 4 Feb 2020 18:08:49 GMT'}, {'version': 'v2', 'created': 'Thu, 5 Mar 2020 21:07:10 GMT'}, {'version': 'v3', 'created': 'Thu, 18 Jun 2020 14:16:42 GMT'}, {'version': 'v4', 'created': 'Sun, 5 Feb 2023 22:49:48 GMT'}, {'version': 'v5', 'created': 'Thu, 7 Sep 2023 08:32:50 GMT'}, {'version': 'v6', 'created': 'Tue, 27 Feb 2024 15:15:10 GMT'}]",2024-02-28,"[['Zhou', 'Quan', ''], ['Marecek', 'Jakub', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
63,2002.01987,Maxim Raginsky,Belinda Tzen and Maxim Raginsky,"Function approximation by neural nets in the mean-field regime: Entropic
  regularization and controlled McKean-Vlasov dynamics",30 pages; note the change of title,,,,cs.LG math.OC math.PR stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the problem of function approximation by two-layer neural nets
with random weights that are ""nearly Gaussian"" in the sense of Kullback-Leibler
divergence. Our setting is the mean-field limit, where the finite population of
neurons in the hidden layer is replaced by a continuous ensemble. We show that
the problem can be phrased as global minimization of a free energy functional
on the space of (finite-length) paths over probability measures on the weights.
This functional trades off the $L^2$ approximation risk of the terminal measure
against the KL divergence of the path with respect to an isotropic Brownian
motion prior. We characterize the unique global minimizer and examine the
dynamics in the space of probability measures over weights that can achieve it.
In particular, we show that the optimal path-space measure corresponds to the
F\""ollmer drift, the solution to a McKean-Vlasov optimal control problem
closely related to the classic Schr\""odinger bridge problem. While the
F\""ollmer drift cannot in general be obtained in closed form, thus limiting its
potential algorithmic utility, we illustrate the viability of the mean-field
Langevin diffusion as a finite-time approximation under various conditions on
entropic regularization. Specifically, we show that it closely tracks the
F\""ollmer drift when the regularization is such that the minimizing density is
log-concave.
","[{'version': 'v1', 'created': 'Wed, 5 Feb 2020 20:50:33 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Feb 2020 07:11:07 GMT'}, {'version': 'v3', 'created': 'Mon, 23 Mar 2020 21:47:47 GMT'}, {'version': 'v4', 'created': 'Sat, 22 Jun 2024 16:57:27 GMT'}]",2024-06-25,"[['Tzen', 'Belinda', ''], ['Raginsky', 'Maxim', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
64,2002.03102,Anuraganand Sharma Dr,Anuraganand Sharma,"A Constraint Driven Solution Model for Discrete Domains with a Case
  Study of Exam Timetabling Problems",41 pages in double space,"not yet published, 2024",,,cs.NE cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
","[{'version': 'v1', 'created': 'Sat, 8 Feb 2020 06:53:38 GMT'}]",2024-02-13,"[['Sharma', 'Anuraganand', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
65,2002.0691,Angelos Chatzimparmpas,"Angelos Chatzimparmpas, Rafael M. Martins, Andreas Kerren",t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections,"This manuscript is published in the IEEE Transactions on
  Visualization and Computer Graphics Journal (IEEE TVCG)","IEEE TVCG 2020, 26(8), 2696-2714",10.1109/TVCG.2020.2986996,,cs.LG cs.HC stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  t-Distributed Stochastic Neighbor Embedding (t-SNE) for the visualization of
multidimensional data has proven to be a popular approach, with successful
applications in a wide range of domains. Despite their usefulness, t-SNE
projections can be hard to interpret or even misleading, which hurts the
trustworthiness of the results. Understanding the details of t-SNE itself and
the reasons behind specific patterns in its output may be a daunting task,
especially for non-experts in dimensionality reduction. In this work, we
present t-viSNE, an interactive tool for the visual exploration of t-SNE
projections that enables analysts to inspect different aspects of their
accuracy and meaning, such as the effects of hyper-parameters, distance and
neighborhood preservation, densities and costs of specific neighborhoods, and
the correlations between dimensions and visual patterns. We propose a coherent,
accessible, and well-integrated collection of different views for the
visualization of t-SNE projections. The applicability and usability of t-viSNE
are demonstrated through hypothetical usage scenarios with real data sets.
Finally, we present the results of a user study where the tool's effectiveness
was evaluated. By bringing to light information that would normally be lost
after running t-SNE, we hope to support analysts in using t-SNE and making its
results better understandable.
","[{'version': 'v1', 'created': 'Mon, 17 Feb 2020 12:22:34 GMT'}, {'version': 'v2', 'created': 'Sun, 5 Apr 2020 09:37:40 GMT'}, {'version': 'v3', 'created': 'Fri, 18 Sep 2020 05:12:47 GMT'}, {'version': 'v4', 'created': 'Tue, 1 Dec 2020 20:40:37 GMT'}, {'version': 'v5', 'created': 'Thu, 18 Apr 2024 16:03:37 GMT'}]",2024-04-19,"[['Chatzimparmpas', 'Angelos', ''], ['Martins', 'Rafael M.', ''], ['Kerren', 'Andreas', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
66,2002.08136,Daniel Molina Dr.,"Daniel Molina and Javier Poyatos and Javier Del Ser and Salvador
  Garc\'ia and Amir Hussain and Francisco Herrera","Comprehensive Taxonomies of Nature- and Bio-inspired Optimization:
  Inspiration versus Algorithmic Behavior, Critical Analysis and
  Recommendations (from 2020 to 2024)","89 pages, 9 figures",Cognitive Computation 12:5 (2020) 897-939,10.1007/s12559-020-09730-8,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In recent years, bio-inspired optimization methods, which mimic biological
processes to solve complex problems, have gained popularity in recent
literature. The proliferation of proposals prove the growing interest in this
field. The increase in nature- and bio-inspired algorithms, applications, and
guidelines highlights growing interest in this field. However, the exponential
rise in the number of bio-inspired algorithms poses a challenge to the future
trajectory of this research domain. Along the five versions of this document,
the number of approaches grows incessantly, and where having a new biological
description takes precedence over real problem-solving. This document presents
two comprehensive taxonomies. One based on principles of biological similarity,
and the other one based on operational aspects associated with the iteration of
population models that initially have a biological inspiration. Therefore,
these taxonomies enable researchers to categorize existing algorithmic
developments into well-defined classes, considering two criteria: the source of
inspiration, and the behavior exhibited by each algorithm. Using these
taxonomies, we classify 518 algorithms based on nature-inspired and
bio-inspired principles. Each algorithm within these categories is thoroughly
examined, allowing for a critical synthesis of design trends and similarities,
and identifying the most analogous classical algorithm for each proposal. From
our analysis, we conclude that a poor relationship is often found between the
natural inspiration of an algorithm and its behavior. Furthermore, similarities
in terms of behavior between different algorithms are greater than what is
claimed in their public disclosure: specifically, we show that more than
one-fourth of the reviewed solvers are versions of classical algorithms. The
conclusions from the analysis of the algorithms lead to several learned
lessons.
","[{'version': 'v1', 'created': 'Wed, 19 Feb 2020 12:34:45 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Feb 2020 09:27:38 GMT'}, {'version': 'v3', 'created': 'Fri, 30 Apr 2021 13:54:37 GMT'}, {'version': 'v4', 'created': 'Sat, 7 May 2022 12:08:01 GMT'}, {'version': 'v5', 'created': 'Wed, 17 Apr 2024 07:59:26 GMT'}]",2024-04-18,"[['Molina', 'Daniel', ''], ['Poyatos', 'Javier', ''], ['Del Ser', 'Javier', ''], ['García', 'Salvador', ''], ['Hussain', 'Amir', ''], ['Herrera', 'Francisco', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
67,2002.1079,Yifan Hu,"Yifan Hu, Siqi Zhang, Xin Chen, Niao He","Biased Stochastic First-Order Methods for Conditional Stochastic
  Optimization and Applications in Meta Learning",The updated version is the camera-ready version in NeurIPS 2020,"Advances in Neural Information Processing Systems 33 (NeurIPS
  2020)",,,math.OC cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Conditional stochastic optimization covers a variety of applications ranging
from invariant learning and causal inference to meta-learning. However,
constructing unbiased gradient estimators for such problems is challenging due
to the composition structure. As an alternative, we propose a biased stochastic
gradient descent (BSGD) algorithm and study the bias-variance tradeoff under
different structural assumptions. We establish the sample complexities of BSGD
for strongly convex, convex, and weakly convex objectives under smooth and
non-smooth conditions. Our lower bound analysis shows that the sample
complexities of BSGD cannot be improved for general convex objectives and
nonconvex objectives except for smooth nonconvex objectives with Lipschitz
continuous gradient estimator. For this special setting, we propose an
accelerated algorithm called biased SpiderBoost (BSpiderBoost) that matches the
lower bound complexity. We further conduct numerical experiments on invariant
logistic regression and model-agnostic meta-learning to illustrate the
performance of BSGD and BSpiderBoost.
","[{'version': 'v1', 'created': 'Tue, 25 Feb 2020 10:57:38 GMT'}, {'version': 'v2', 'created': 'Sun, 2 Jun 2024 12:38:17 GMT'}]",2024-06-04,"[['Hu', 'Yifan', ''], ['Zhang', 'Siqi', ''], ['Chen', 'Xin', ''], ['He', 'Niao', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
68,2003.07011,Sunkyu Yu,"Sunkyu Yu, Xianji Piao, Namkyoo Park","Machine learning identifies scale-free properties in disordered
  materials","44 pages, 15 figures","Nat. Commun. 11, 4842 (2020)",10.1038/s41467-020-18653-9,,physics.optics cond-mat.dis-nn physics.comp-ph physics.data-an,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The vast amount of design freedom in disordered systems expands the parameter
space for signal processing, allowing for unique signal flows that are
distinguished from those in regular systems. However, this large degree of
freedom has hindered the deterministic design of disordered systems for target
functionalities. Here, we employ a machine learning (ML) approach for
predicting and designing wave-matter interactions in disordered structures,
thereby identifying scale-free properties for waves. To abstract and map the
features of wave behaviours and disordered structures, we develop
disorder-to-localization and localization-to-disorder convolutional neural
networks (CNNs). Each CNN enables the instantaneous prediction of wave
localization in disordered structures and the instantaneous generation of
disordered structures from given localizations. We demonstrate that
CNN-generated disordered structures have scale-free properties with heavy tails
and hub atoms, which exhibit an increase of multiple orders of magnitude in
robustness to accidental defects, such as material or structural imperfection.
Our results verify the critical role of ML network structures in determining
ML-generated real-space structures, which can be used in the design of
defect-immune and efficiently tunable devices.
","[{'version': 'v1', 'created': 'Mon, 16 Mar 2020 04:06:16 GMT'}, {'version': 'v2', 'created': 'Thu, 2 Apr 2020 02:55:10 GMT'}]",2024-04-11,"[['Yu', 'Sunkyu', ''], ['Piao', 'Xianji', ''], ['Park', 'Namkyoo', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
69,2003.07292,Jamieson Warner,"J. Warner, A. Devaraj, and R. Miikkulainen",Using context to adapt to sensor drift,,,,,physics.ins-det cs.LG cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Lifelong development allows animals and machines to adapt to changes in the
environment as well as in their own systems, such as wear and tear in sensors
and actuators. An important use case of such adaptation is industrial
odor-sensing. Metal-oxide-based sensors can be used to detect gaseous compounds
in the air; however, the gases interact with the sensors, causing their
responses to change over time in a process called sensor drift. Sensor drift is
irreversible and requires frequent recalibration with additional data. This
paper demonstrates that an adaptive system that represents the drift as context
for the skill of odor sensing achieves the same goal automatically. After it is
trained on the history of changes, a neural network predicts future contexts,
allowing the context+skill sensing system to adapt to sensor drift. Evaluated
on an industrial dataset of gas-sensor drift, the approach performed better
than standard drift-naive and ensembling methods. In this way, the
context+skill system emulates the natural ability of animal olfaction systems
to adapt to a changing world, and demonstrates how it can be effective in
real-world applications.
","[{'version': 'v1', 'created': 'Mon, 16 Mar 2020 15:53:11 GMT'}, {'version': 'v2', 'created': 'Fri, 22 May 2020 19:09:26 GMT'}, {'version': 'v3', 'created': 'Thu, 11 Apr 2024 19:20:02 GMT'}]",2024-04-15,"[['Warner', 'J.', ''], ['Devaraj', 'A.', ''], ['Miikkulainen', 'R.', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
70,2003.08433,George Amariucai,"Abhishek Jana, Bipin Paudel, Md Kamruzzaman Sarker, Monireh Ebrahimi,
  Pascal Hitzler and George T Amariucai","Neural Fuzzy Extractors: A Secure Way to Use Artificial Neural Networks
  for Biometric User Authentication","8 pages, 5 figures","Proceedings on Privacy Enhancing Technologies, 2022, volume 4,
  pages 86-104",10.56553/popets-2022-0100,,cs.CR cs.HC cs.LG,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Powered by new advances in sensor development and artificial intelligence,
the decreasing cost of computation, and the pervasiveness of handheld
computation devices, biometric user authentication (and identification) is
rapidly becoming ubiquitous. Modern approaches to biometric authentication,
based on sophisticated machine learning techniques, cannot avoid storing either
trained-classifier details or explicit user biometric data, thus exposing
users' credentials to falsification. In this paper, we introduce a secure way
to handle user-specific information involved with the use of vector-space
classifiers or artificial neural networks for biometric authentication. Our
proposed architecture, called a Neural Fuzzy Extractor (NFE), allows the
coupling of pre-existing classifiers with fuzzy extractors, through a
artificial-neural-network-based buffer called an expander, with minimal or no
performance degradation. The NFE thus offers all the performance advantages of
modern deep-learning-based classifiers, and all the security of standard fuzzy
extractors. We demonstrate the NFE retrofit to a classic artificial neural
network for a simple scenario of fingerprint-based user authentication.
","[{'version': 'v1', 'created': 'Wed, 18 Mar 2020 18:48:25 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Dec 2023 00:22:29 GMT'}]",2024-01-19,"[['Jana', 'Abhishek', ''], ['Paudel', 'Bipin', ''], ['Sarker', 'Md Kamruzzaman', ''], ['Ebrahimi', 'Monireh', ''], ['Hitzler', 'Pascal', ''], ['Amariucai', 'George T', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
71,2003.10699,Dominik Kowald PhD,"Dominik Kowald, Elisabeth Lex, Markus Schedl","Utilizing Human Memory Processes to Model Genre Preferences for
  Personalized Music Recommendations",Dominik Kowald and Elisabeth Lex contributed equally to this work,HUMANIZE Workshop @ IUI'2020,,,cs.IR,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we introduce a psychology-inspired approach to model and
predict the music genre preferences of different groups of users by utilizing
human memory processes. These processes describe how humans access information
units in their memory by considering the factors of (i) past usage frequency,
(ii) past usage recency, and (iii) the current context. Using a publicly
available dataset of more than a billion music listening records shared on the
music streaming platform Last.fm, we find that our approach provides
significantly better prediction accuracy results than various baseline
algorithms for all evaluated user groups, i.e., (i) low-mainstream music
listeners, (ii) medium-mainstream music listeners, and (iii) high-mainstream
music listeners. Furthermore, our approach is based on a simple psychological
model, which contributes to the transparency and explainability of the
calculated predictions.
","[{'version': 'v1', 'created': 'Tue, 24 Mar 2020 07:40:33 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Jul 2023 09:56:30 GMT'}, {'version': 'v3', 'created': 'Thu, 15 Feb 2024 09:56:19 GMT'}]",2024-02-16,"[['Kowald', 'Dominik', ''], ['Lex', 'Elisabeth', ''], ['Schedl', 'Markus', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
72,2004.05839,Simone Garatti,Marco C. Campi and Simone Garatti,"A Theory of the Risk for Optimization with Relaxation and its
  Application to Support Vector Machines",https://www.jmlr.org/papers/v22/21-0641.html,"Journal of Machine Learning Research 22(288):1-38, 2021",,,cs.LG cs.SY eess.SY math.OC stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper we consider optimization with relaxation, an ample paradigm to
make data-driven designs. This approach was previously considered by the same
authors of this work in Garatti and Campi (2019), a study that revealed a
deep-seated connection between two concepts: risk (probability of not
satisfying a new, out-of-sample, constraint) and complexity (according to a
definition introduced in paper Garatti and Campi (2019)). This connection was
shown to have profound implications in applications because it implied that the
risk can be estimated from the complexity, a quantity that can be measured from
the data without any knowledge of the data-generation mechanism. In the present
work we establish new results. First, we expand the scope of Garatti and Campi
(2019) so as to embrace a more general setup that covers various algorithms in
machine learning. Then, we study classical support vector methods - including
SVM (Support Vector Machine), SVR (Support Vector Regression) and SVDD (Support
Vector Data Description) - and derive new results for the ability of these
methods to generalize. All results are valid for any finite size of the data
set. When the sample size tends to infinity, we establish the unprecedented
result that the risk approaches the ratio between the complexity and the
cardinality of the data sample, regardless of the value of the complexity.
","[{'version': 'v1', 'created': 'Mon, 13 Apr 2020 09:38:25 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Sep 2020 10:34:10 GMT'}, {'version': 'v3', 'created': 'Tue, 20 Oct 2020 19:26:04 GMT'}, {'version': 'v4', 'created': 'Mon, 8 Jan 2024 11:00:50 GMT'}]",2024-01-09,"[['Campi', 'Marco C.', ''], ['Garatti', 'Simone', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
73,2004.12908,Jayanta Dey,"Joshua T. Vogelstein, Jayanta Dey, Hayden S. Helm, Will LeVine, Ronak
  D. Mehta, Tyler M. Tomita, Haoyin Xu, Ali Geisa, Qingyang Wang, Gido M. van
  de Ven, Chenyu Gao, Weiwei Yang, Bryan Tower, Jonathan Larson, Christopher M.
  White, and Carey E. Priebe",A Simple Lifelong Learning Approach,,,,,cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
","[{'version': 'v1', 'created': 'Mon, 27 Apr 2020 16:16:30 GMT'}, {'version': 'v10', 'created': 'Tue, 21 Sep 2021 00:50:21 GMT'}, {'version': 'v11', 'created': 'Thu, 9 Dec 2021 18:17:20 GMT'}, {'version': 'v12', 'created': 'Tue, 11 Jan 2022 04:25:38 GMT'}, {'version': 'v13', 'created': 'Sat, 14 May 2022 14:40:41 GMT'}, {'version': 'v14', 'created': 'Wed, 5 Oct 2022 01:34:23 GMT'}, {'version': 'v15', 'created': 'Sat, 3 Dec 2022 15:33:44 GMT'}, {'version': 'v16', 'created': 'Thu, 23 Mar 2023 00:08:05 GMT'}, {'version': 'v17', 'created': 'Sun, 25 Jun 2023 13:46:43 GMT'}, {'version': 'v18', 'created': 'Fri, 2 Feb 2024 17:10:42 GMT'}, {'version': 'v19', 'created': 'Tue, 11 Jun 2024 17:04:38 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Apr 2020 17:42:48 GMT'}, {'version': 'v3', 'created': 'Mon, 29 Jun 2020 19:10:05 GMT'}, {'version': 'v4', 'created': 'Thu, 9 Jul 2020 19:22:48 GMT'}, {'version': 'v5', 'created': 'Thu, 20 Aug 2020 14:34:24 GMT'}, {'version': 'v6', 'created': 'Wed, 3 Mar 2021 15:46:10 GMT'}, {'version': 'v7', 'created': 'Mon, 14 Jun 2021 15:35:21 GMT'}, {'version': 'v8', 'created': 'Fri, 20 Aug 2021 22:29:54 GMT'}, {'version': 'v9', 'created': 'Sat, 18 Sep 2021 15:04:04 GMT'}]",2024-06-12,"[['Vogelstein', 'Joshua T.', ''], ['Dey', 'Jayanta', ''], ['Helm', 'Hayden S.', ''], ['LeVine', 'Will', ''], ['Mehta', 'Ronak D.', ''], ['Tomita', 'Tyler M.', ''], ['Xu', 'Haoyin', ''], ['Geisa', 'Ali', ''], ['Wang', 'Qingyang', ''], ['van de Ven', 'Gido M.', ''], ['Gao', 'Chenyu', ''], ['Yang', 'Weiwei', ''], ['Tower', 'Bryan', ''], ['Larson', 'Jonathan', ''], ['White', 'Christopher M.', ''], ['Priebe', 'Carey E.', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
74,2005.01886,Vladimir Pestov,Vladimir G. Pestov,"A learning problem whose consistency is equivalent to the non-existence
  of real-valued measurable cardinals","16 pp., journal macros","Addendum was revised and published as a separate paper: On a
  result of K P. Hart about non-existence of measurable solutions to the
  discrete expectation maximization problem, Comment. Math. Univ. Carolin. 64
  (2023), 353--358",,,cs.LG math.LO stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We show that the $k$-nearest neighbour learning rule is universally
consistent in a metric space $X$ if and only if it is universally consistent in
every separable subspace of $X$ and the density of $X$ is less than every
real-measurable cardinal. In particular, the $k$-NN classifier is universally
consistent in every metric space whose separable subspaces are sigma-finite
dimensional in the sense of Nagata and Preiss if and only if there are no
real-valued measurable cardinals. The latter assumption is relatively
consistent with ZFC, however the consistency of the existence of such cardinals
cannot be proved within ZFC. Our results were inspired by an example sketched
by C\'erou and Guyader in 2006 at an intuitive level of rigour.
","[{'version': 'v1', 'created': 'Mon, 4 May 2020 23:40:28 GMT'}]",2024-05-03,"[['Pestov', 'Vladimir G.', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
75,2005.04735,Dan Shiebler,Dan Shiebler,Categorical Stochastic Processes and Likelihood,,"Compositionality, Volume 3 (2021) (April 14, 2021)
  compositionality:13511",10.32408/compositionality-3-1,,cs.AI math.CT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work we take a Category Theoretic perspective on the relationship
between probabilistic modeling and function approximation. We begin by defining
two extensions of function composition to stochastic process subordination: one
based on the co-Kleisli category under the comonad (Omega x -) and one based on
the parameterization of a category with a Lawvere theory. We show how these
extensions relate to the category Stoch and other Markov Categories. Next, we
apply the Para construction to extend stochastic processes to parameterized
statistical models and we define a way to compose the likelihood functions of
these models. We conclude with a demonstration of how the Maximum Likelihood
Estimation procedure defines an identity-on-objects functor from the category
of statistical models to the category of Learners. Code to accompany this paper
can be found at
https://github.com/dshieble/Categorical_Stochastic_Processes_and_Likelihood
","[{'version': 'v1', 'created': 'Sun, 10 May 2020 18:00:56 GMT'}, {'version': 'v2', 'created': 'Thu, 3 Sep 2020 03:43:39 GMT'}, {'version': 'v3', 'created': 'Tue, 2 Mar 2021 06:10:23 GMT'}, {'version': 'v4', 'created': 'Thu, 8 Apr 2021 20:56:59 GMT'}, {'version': 'v5', 'created': 'Sun, 9 Jan 2022 13:05:22 GMT'}]",2024-08-07,"[['Shiebler', 'Dan', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
76,2005.07842,Hao Peng,"Hao Peng, Wei Wang, Pei Chen, Rui Liu","DEFM: Delay E mbedding based Forecast Machine for Time Series
  Forecasting by Spatiotemporal Information Transformation","28 pages, 5 figures",Chaos 1 April 2024; 34 (4): 043112,10.1063/5.0181791,,eess.SP cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Making accurate forecasts for a complex system is a challenge in various
practical applications. The major difficulty in solving such a problem concerns
nonlinear spatiotemporal dynamics with time-varying characteristics. Takens'
delay embedding theory provides a way to transform high-dimensional spatial
information into temporal information. In this work, by combining delay
embedding theory and deep learning techniques, we propose a novel framework,
Delay-Embedding-based Forecast Machine (DEFM), to predict the future values of
a target variable in a self-supervised and multistep-ahead manner based on
high-dimensional observations. With a three-module spatiotemporal architecture,
the DEFM leverages deep neural networks to effectively extract both the
spatially and temporally associated information from the observed time series
even with time-varying parameters or additive noise. The DEFM can accurately
predict future information by transforming spatiotemporal information to the
delay embeddings of a target variable. The efficacy and precision of the DEFM
are substantiated through applications in three spatiotemporally chaotic
systems: a 90-dimensional (90D) coupled Lorenz system, the Lorenz 96 system,
and the Kuramoto-Sivashinsky (KS) equation with inhomogeneity. Additionally,
the performance of the DEFM is evaluated on six real-world datasets spanning
various fields. Comparative experiments with five prediction methods illustrate
the superiority and robustness of the DEFM and show the great potential of the
DEFM in temporal information mining and forecasting
","[{'version': 'v1', 'created': 'Sat, 16 May 2020 01:48:33 GMT'}, {'version': 'v2', 'created': 'Sun, 7 Apr 2024 02:55:23 GMT'}]",2024-04-09,"[['Peng', 'Hao', ''], ['Wang', 'Wei', ''], ['Chen', 'Pei', ''], ['Liu', 'Rui', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
77,2005.07972,Matteo Fontana,"Matteo Fontana, Gianluca Zeni, Simone Vantini",Conformal Prediction: a Unified Review of Theory and New Challenges,forthcoming on Bernoulli,Bernoulli 29(1): 1-23 (February 2023),10.3150/21-BEJ1447,,cs.LG econ.EM stat.ME stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work we provide a review of basic ideas and novel developments about
Conformal Prediction -- an innovative distribution-free, non-parametric
forecasting method, based on minimal assumptions -- that is able to yield in a
very straightforward way predictions sets that are valid in a statistical sense
also in in the finite sample case. The in-depth discussion provided in the
paper covers the theoretical underpinnings of Conformal Prediction, and then
proceeds to list the more advanced developments and adaptations of the original
idea.
","[{'version': 'v1', 'created': 'Sat, 16 May 2020 12:38:19 GMT'}, {'version': 'v2', 'created': 'Fri, 29 Jul 2022 14:06:18 GMT'}]",2024-02-01,"[['Fontana', 'Matteo', ''], ['Zeni', 'Gianluca', ''], ['Vantini', 'Simone', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
78,2005.0946,Carole Adam,Carole Adam,"VigiFlood: evaluating the impact of a change of perspective on flood
  vigilance","IDRIM journal, 2020",,10.5595/001c.17844,,cs.CY cs.AI cs.MA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Emergency managers receive communication training about the importance of
being 'first, right and credible', and taking into account the psychology of
their audience and their particular reasoning under stress and risk. But we
believe that citizens should be similarly trained about how to deal with risk
communication. In particular, such messages necessarily carry a part of
uncertainty since most natural risks are difficult to accurately forecast ahead
of time. Yet, citizens should keep trusting the emergency communicators even
after they made forecasting errors in the past.
  We have designed a serious game called Vigiflood, based on a real case study
of flash floods hitting the South West of France in October 2018. In this game,
the user changes perspective by taking the role of an emergency communicator,
having to set the level of vigilance to alert the population, based on
uncertain clues. Our hypothesis is that this change of perspective can improve
the player's awareness and response to future flood vigilance announcements. We
evaluated this game through an online survey where people were asked to answer
a questionnaire about flood risk awareness and behavioural intentions before
and after playing the game, in order to assess its impact.
","[{'version': 'v1', 'created': 'Tue, 19 May 2020 14:05:11 GMT'}]",2024-04-16,"[['Adam', 'Carole', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
79,2005.10638,Alexandre Emerick,"Smith W. A. Canchumuni, Jose D. B. Castro, J\'ulia Potratz, Alexandre
  A. Emerick and Marco Aurelio C. Pacheco","Recent Developments Combining Ensemble Smoother and Deep Generative
  Networks for Facies History Matching","46 pages, 26 figures",,10.1007/s10596-020-10015-0,,cs.LG eess.IV stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Ensemble smoothers are among the most successful and efficient techniques
currently available for history matching. However, because these methods rely
on Gaussian assumptions, their performance is severely degraded when the prior
geology is described in terms of complex facies distributions. Inspired by the
impressive results obtained by deep generative networks in areas such as image
and video generation, we started an investigation focused on the use of
autoencoders networks to construct a continuous parameterization for facies
models. In our previous publication, we combined a convolutional variational
autoencoder (VAE) with the ensemble smoother with multiple data assimilation
(ES-MDA) for history matching production data in models generated with
multiple-point geostatistics. Despite the good results reported in our previous
publication, a major limitation of the designed parameterization is the fact
that it does not allow applying distance-based localization during the ensemble
smoother update, which limits its application in large-scale problems.
  The present work is a continuation of this research project focusing in two
aspects: firstly, we benchmark seven different formulations, including VAE,
generative adversarial network (GAN), Wasserstein GAN, variational
auto-encoding GAN, principal component analysis (PCA) with cycle GAN, PCA with
transfer style network and VAE with style loss. These formulations are tested
in a synthetic history matching problem with channelized facies. Secondly, we
propose two strategies to allow the use of distance-based localization with the
deep learning parameterizations.
","[{'version': 'v1', 'created': 'Fri, 8 May 2020 21:32:42 GMT'}]",2024-06-11,"[['Canchumuni', 'Smith W. A.', ''], ['Castro', 'Jose D. B.', ''], ['Potratz', 'Júlia', ''], ['Emerick', 'Alexandre A.', ''], ['Pacheco', 'Marco Aurelio C.', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
80,2005.117,Zequn Liu,"Zequn Liu, Ruiyi Zhang, Yiping Song, Wei Ju, Ming Zhang","When does MAML Work the Best? An Empirical Study on Model-Agnostic
  Meta-Learning in NLP Applications",,,,,cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Model-Agnostic Meta-Learning (MAML), a model-agnostic meta-learning method,
is successfully employed in NLP applications including few-shot text
classification and multi-domain low-resource language generation. Many
impacting factors, including data quantity, similarity among tasks, and the
balance between general language model and task-specific adaptation, can affect
the performance of MAML in NLP, but few works have thoroughly studied them. In
this paper, we conduct an empirical study to investigate these impacting
factors and conclude when MAML works the best based on the experimental
results.
","[{'version': 'v1', 'created': 'Sun, 24 May 2020 09:29:36 GMT'}, {'version': 'v2', 'created': 'Wed, 24 Apr 2024 05:06:27 GMT'}]",2024-04-25,"[['Liu', 'Zequn', ''], ['Zhang', 'Ruiyi', ''], ['Song', 'Yiping', ''], ['Ju', 'Wei', ''], ['Zhang', 'Ming', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
81,2005.14064,Jinglin Zhang,"Jinglin Zhang, Wenjun Xu, Hui Gao, Miao Pan, Zhu Han, and Ping Zhang","Codebook-Based Beam Tracking for Conformal ArrayEnabled UAV MmWave
  Networks",,,,,eess.SP cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Millimeter wave (mmWave) communications can potentially meet the high
data-rate requirements of unmanned aerial vehicle (UAV) networks. However, as
the prerequisite of mmWave communications, the narrow directional beam tracking
is very challenging because of the three-dimensional (3D) mobility and attitude
variation of UAVs. Aiming to address the beam tracking difficulties, we propose
to integrate the conformal array (CA) with the surface of each UAV, which
enables the full spatial coverage and the agile beam tracking in highly dynamic
UAV mmWave networks. More specifically, the key contributions of our work are
three-fold. 1) A new mmWave beam tracking framework is established for the
CA-enabled UAV mmWave network. 2) A specialized hierarchical codebook is
constructed to drive the directional radiating element (DRE)-covered
cylindrical conformal array (CCA), which contains both the angular beam pattern
and the subarray pattern to fully utilize the potential of the CA. 3) A
codebook-based multiuser beam tracking scheme is proposed, where the Gaussian
process machine learning enabled UAV position/attitude predication is developed
to improve the beam tracking efficiency in conjunction with the tracking-error
aware adaptive beamwidth control. Simulation results validate the effectiveness
of the proposed codebook-based beam tracking scheme in the CA-enabled UAV
mmWave network, and demonstrate the advantages of CA over the conventional
planner array in terms of spectrum efficiency and outage probability in the
highly dynamic scenarios.
","[{'version': 'v1', 'created': 'Thu, 28 May 2020 14:57:23 GMT'}, {'version': 'v2', 'created': 'Mon, 8 Apr 2024 07:15:19 GMT'}]",2024-04-09,"[['Zhang', 'Jinglin', ''], ['Xu', 'Wenjun', ''], ['Gao', 'Hui', ''], ['Pan', 'Miao', ''], ['Han', 'Zhu', ''], ['Zhang', 'Ping', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
82,2006.04945,Joanna Henzel,Joanna Henzel and Marek Sikora,"Gradient Boosting Application in Forecasting of Performance Indicators
  Values for Measuring the Efficiency of Promotions in FMCG Retail","8 pages, 3 figures",https://annals-csis.org/proceedings/2020/drp/pdf/118.pdf,10.15439/2020F118,,cs.CY cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In the paper, a problem of forecasting promotion efficiency is raised. The
authors propose a new approach, using the gradient boosting method for this
task. Six performance indicators are introduced to capture the promotion
effect. For each of them, within predefined groups of products, a model was
trained. A description of using these models for forecasting and optimising
promotion efficiency is provided. Data preparation and hyperparameters tuning
processes are also described. The experiments were performed for three groups
of products from a large grocery company.
","[{'version': 'v1', 'created': 'Sat, 30 May 2020 20:08:01 GMT'}]",2024-03-22,"[['Henzel', 'Joanna', ''], ['Sikora', 'Marek', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
83,2006.05259,David W. Romero,"David W. Romero, Erik J. Bekkers, Jakub M. Tomczak, Mark Hoogendoorn","Wavelet Networks: Scale-Translation Equivariant Learning From Raw
  Time-Series",,,,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Leveraging the symmetries inherent to specific data domains for the
construction of equivariant neural networks has lead to remarkable improvements
in terms of data efficiency and generalization. However, most existing research
focuses on symmetries arising from planar and volumetric data, leaving a
crucial data source largely underexplored: time-series. In this work, we fill
this gap by leveraging the symmetries inherent to time-series for the
construction of equivariant neural network. We identify two core symmetries:
*scale and translation*, and construct scale-translation equivariant neural
networks for time-series learning. Intriguingly, we find that scale-translation
equivariant mappings share strong resemblance with the wavelet transform.
Inspired by this resemblance, we term our networks Wavelet Networks, and show
that they perform nested non-linear wavelet-like time-frequency transforms.
Empirical results show that Wavelet Networks outperform conventional CNNs on
raw waveforms, and match strongly engineered spectrogram techniques across
several tasks and time-series types, including audio, environmental sounds, and
electrical signals. Our code is publicly available at
https://github.com/dwromero/wavelet_networks.
","[{'version': 'v1', 'created': 'Tue, 9 Jun 2020 13:50:34 GMT'}, {'version': 'v2', 'created': 'Sun, 21 Jan 2024 11:58:49 GMT'}]",2024-01-23,"[['Romero', 'David W.', ''], ['Bekkers', 'Erik J.', ''], ['Tomczak', 'Jakub M.', ''], ['Hoogendoorn', 'Mark', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
84,2006.07647,Ivan Smirnov,"Ivan Smirnov, Florian Lemmerich, Markus Strohmaier","Quota-based debiasing can decrease representation of already
  underrepresented groups",,,10.1098/rsos.210821,,cs.CY cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many important decisions in societies such as school admissions, hiring, or
elections are based on the selection of top-ranking individuals from a larger
pool of candidates. This process is often subject to biases, which typically
manifest as an under-representation of certain groups among the selected or
accepted individuals. The most common approach to this issue is debiasing, for
example via the introduction of quotas that ensure proportional representation
of groups with respect to a certain, often binary attribute. Cases include
quotas for women on corporate boards or ethnic quotas in elections. This,
however, has the potential to induce changes in representation with respect to
other attributes. For the case of two correlated binary attributes we show that
quota-based debiasing based on a single attribute can worsen the representation
of already underrepresented groups and decrease overall fairness of selection.
We use several data sets from a broad range of domains from recidivism risk
assessments to scientific citations to assess this effect in real-world
settings. Our results demonstrate the importance of including all relevant
attributes in debiasing procedures and that more efforts need to be put into
eliminating the root causes of inequalities as purely numerical solutions such
as quota-based debiasing might lead to unintended consequences.
","[{'version': 'v1', 'created': 'Sat, 13 Jun 2020 14:26:42 GMT'}]",2024-07-02,"[['Smirnov', 'Ivan', ''], ['Lemmerich', 'Florian', ''], ['Strohmaier', 'Markus', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
85,2006.09441,Henry Chan,"Henry Chan, Youssef S.G. Nashed, Saugat Kandel, Stephan Hruszkewycz,
  Subramanian Sankaranarayanan, Ross J. Harder, Mathew J. Cherukara",Real-time 3D Nanoscale Coherent Imaging via Physics-aware Deep Learning,,,10.1063/5.0031486,,eess.IV cond-mat.mtrl-sci cs.LG physics.app-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
","[{'version': 'v1', 'created': 'Tue, 16 Jun 2020 18:35:32 GMT'}]",2024-06-12,"[['Chan', 'Henry', ''], ['Nashed', 'Youssef S. G.', ''], ['Kandel', 'Saugat', ''], ['Hruszkewycz', 'Stephan', ''], ['Sankaranarayanan', 'Subramanian', ''], ['Harder', 'Ross J.', ''], ['Cherukara', 'Mathew J.', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
86,2006.12075,Mingyi Shi,"Mingyi Shi, Kfir Aberman, Andreas Aristidou, Taku Komura, Dani
  Lischinski, Daniel Cohen-Or, Baoquan Chen","MotioNet: 3D Human Motion Reconstruction from Monocular Video with
  Skeleton Consistency","Accepted to Transactions on Graphics (ToG) 2020. Project page:
  {https://rubbly.cn/publications/motioNet} Video:
  {https://youtu.be/8YubchlzvFA}","ACM Transaction on Graphics, 40(1), Article 1, 2020",10.1145/3407659,,cs.CV cs.GR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce MotioNet, a deep neural network that directly reconstructs the
motion of a 3D human skeleton from monocular video.While previous methods rely
on either rigging or inverse kinematics (IK) to associate a consistent skeleton
with temporally coherent joint rotations, our method is the first data-driven
approach that directly outputs a kinematic skeleton, which is a complete,
commonly used, motion representation. At the crux of our approach lies a deep
neural network with embedded kinematic priors, which decomposes sequences of 2D
joint positions into two separate attributes: a single, symmetric, skeleton,
encoded by bone lengths, and a sequence of 3D joint rotations associated with
global root positions and foot contact labels. These attributes are fed into an
integrated forward kinematics (FK) layer that outputs 3D positions, which are
compared to a ground truth. In addition, an adversarial loss is applied to the
velocities of the recovered rotations, to ensure that they lie on the manifold
of natural joint rotations. The key advantage of our approach is that it learns
to infer natural joint rotations directly from the training data, rather than
assuming an underlying model, or inferring them from joint positions using a
data-agnostic IK solver. We show that enforcing a single consistent skeleton
along with temporally coherent joint rotations constrains the solution space,
leading to a more robust handling of self-occlusions and depth ambiguities.
","[{'version': 'v1', 'created': 'Mon, 22 Jun 2020 08:50:09 GMT'}]",2024-05-14,"[['Shi', 'Mingyi', ''], ['Aberman', 'Kfir', ''], ['Aristidou', 'Andreas', ''], ['Komura', 'Taku', ''], ['Lischinski', 'Dani', ''], ['Cohen-Or', 'Daniel', ''], ['Chen', 'Baoquan', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
87,2006.13456,Yuta Shikuri,Yuta Shikuri,Likelihood-Free Gaussian Process for Regression,There were errors in the proposed method,,,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Gaussian process regression can flexibly represent the posterior distribution
of an interest parameter given sufficient information on the likelihood.
However, in some cases, we have little knowledge regarding the probability
model. For example, when investing in a financial instrument, the probability
model of cash flow is generally unknown. In this paper, we propose a novel
framework called the likelihood-free Gaussian process (LFGP), which allows
representation of the posterior distributions of interest parameters for
scalable problems without directly setting their likelihood functions. The LFGP
establishes clusters in which the value of the interest parameter can be
considered approximately identical, and it approximates the likelihood of the
interest parameter in each cluster to a Gaussian using the asymptotic normality
of the maximum likelihood estimator. We expect that the proposed framework will
contribute significantly to likelihood-free modeling, particularly by reducing
the assumptions for the probability model and the computational costs for
scalable problems.
","[{'version': 'v1', 'created': 'Wed, 24 Jun 2020 03:38:41 GMT'}, {'version': 'v2', 'created': 'Sun, 13 Sep 2020 01:43:12 GMT'}, {'version': 'v3', 'created': 'Sun, 3 Dec 2023 14:51:05 GMT'}, {'version': 'v4', 'created': 'Thu, 23 May 2024 02:48:08 GMT'}]",2024-05-24,"[['Shikuri', 'Yuta', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
88,2006.14347,Xi Li,"Jiabao Cui, Xuewei Li, Bin Li, Hanbin Zhao, Bourahla Omar, and Xi Li",Epoch-evolving Gaussian Process Guided Learning,,,,,cs.LG cs.CV stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose a novel learning scheme called epoch-evolving
Gaussian Process Guided Learning (GPGL), which aims at characterizing the
correlation information between the batch-level distribution and the global
data distribution. Such correlation information is encoded as context labels
and needs renewal every epoch. With the guidance of the context label and
ground truth label, GPGL scheme provides a more efficient optimization through
updating the model parameters with a triangle consistency loss. Furthermore,
our GPGL scheme can be further generalized and naturally applied to the current
deep models, outperforming the existing batch-based state-of-the-art models on
mainstream datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) remarkably.
","[{'version': 'v1', 'created': 'Thu, 25 Jun 2020 12:45:17 GMT'}, {'version': 'v2', 'created': 'Tue, 12 Mar 2024 13:00:14 GMT'}]",2024-03-13,"[['Cui', 'Jiabao', ''], ['Li', 'Xuewei', ''], ['Li', 'Bin', ''], ['Zhao', 'Hanbin', ''], ['Omar', 'Bourahla', ''], ['Li', 'Xi', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
89,2006.14774,Kumar Vijay Mishra,"Jiawei Liu, Kumar Vijay Mishra and Mohammad Saquib","Co-Designing Statistical MIMO Radar and In-band Full-Duplex Multi-User
  MIMO Communications -- Part I: Signal Processing","23 pages, 5 figures, 2 tables",,,,eess.SP cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider a spectral sharing problem in which a statistical (or widely
distributed) multiple-input multiple-output (MIMO) radar and an in-band
full-duplex (IBFD) multi-user MIMO (MU-MIMO) communications system concurrently
operate within the same frequency band. Prior works on joint
MIMO-radar-MIMO-communications (MRMC) systems largely focus on either colocated
MIMO radars, half-duplex MIMO communications, single-user scenarios, omit
practical constraints (clutter, uplink [UL]/downlink [DL] transmit powers,
UL/DL quality-of-service, and peak-to-average-power ratio), or MRMC
co-existence that employs separate transmit/receive units. The purpose of this
and companion papers (Part II and III) is to co-design an MRMC framework that
addresses all of these issues. In this paper, we propose signal processing for
a distributed IBFD MRMC, where radar receiver is designed to additionally
exploit the downlink communications signals reflected from a radar target.
Extensive numerical experiments show that our methods improve radar target
detection over conventional codes and yield a higher achievable data rate than
standard precoders. The following companion paper (Part II) describes the
theory and procedure of our algorithm to solve the non-convex design problem.
The final companion paper (Part II) considers the case of multiple targets and
examines the tracking performance of our MRMC system.
","[{'version': 'v1', 'created': 'Fri, 26 Jun 2020 03:22:24 GMT'}, {'version': 'v2', 'created': 'Thu, 7 Oct 2021 00:54:13 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Jan 2022 00:34:51 GMT'}, {'version': 'v4', 'created': 'Fri, 2 Sep 2022 17:24:11 GMT'}, {'version': 'v5', 'created': 'Thu, 28 Mar 2024 03:24:11 GMT'}]",2024-03-29,"[['Liu', 'Jiawei', ''], ['Mishra', 'Kumar Vijay', ''], ['Saquib', 'Mohammad', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
90,2006.15524,Xi Li,"Hanbin Zhao, Yongjian Fu, Mintong Kang, Qi Tian, Fei Wu, Xi Li","MgSvF: Multi-Grained Slow vs. Fast Framework for Few-Shot
  Class-Incremental Learning",,,,,cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As a challenging problem, few-shot class-incremental learning (FSCIL)
continually learns a sequence of tasks, confronting the dilemma between slow
forgetting of old knowledge and fast adaptation to new knowledge. In this
paper, we concentrate on this ""slow vs. fast"" (SvF) dilemma to determine which
knowledge components to be updated in a slow fashion or a fast fashion, and
thereby balance old-knowledge preservation and new-knowledge adaptation. We
propose a multi-grained SvF learning strategy to cope with the SvF dilemma from
two different grains: intra-space (within the same feature space) and
inter-space (between two different feature spaces). The proposed strategy
designs a novel frequency-aware regularization to boost the intra-space SvF
capability, and meanwhile develops a new feature space composition operation to
enhance the inter-space SvF learning performance. With the multi-grained SvF
learning strategy, our method outperforms the state-of-the-art approaches by a
large margin.
","[{'version': 'v1', 'created': 'Sun, 28 Jun 2020 06:12:49 GMT'}, {'version': 'v2', 'created': 'Sun, 20 Dec 2020 11:55:55 GMT'}, {'version': 'v3', 'created': 'Tue, 2 Mar 2021 16:47:28 GMT'}, {'version': 'v4', 'created': 'Tue, 12 Mar 2024 12:53:14 GMT'}]",2024-03-13,"[['Zhao', 'Hanbin', ''], ['Fu', 'Yongjian', ''], ['Kang', 'Mintong', ''], ['Tian', 'Qi', ''], ['Wu', 'Fei', ''], ['Li', 'Xi', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
91,2007.03747,Christoph Muehlmann,"Christoph Muehlmann, Klaus Nordhausen, Mengxi Yi","On Cokriging, Neural Networks, and Spatial Blind Source Separation for
  Multivariate Spatial Prediction",,"IEEE Geoscience and Remote Sensing Letters, 18, 1931-1935, 2021",10.1109/LGRS.2020.3011549,,eess.SP cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multivariate measurements taken at irregularly sampled locations are a common
form of data, for example in geochemical analysis of soil. In practical
considerations predictions of these measurements at unobserved locations are of
great interest. For standard multivariate spatial prediction methods it is
mandatory to not only model spatial dependencies but also cross-dependencies
which makes it a demanding task. Recently, a blind source separation approach
for spatial data was suggested. When using this spatial blind source separation
method prior the actual spatial prediction, modelling of spatial
cross-dependencies is avoided, which in turn simplifies the spatial prediction
task significantly. In this paper we investigate the use of spatial blind
source separation as a pre-processing tool for spatial prediction and compare
it with predictions from Cokriging and neural networks in an extensive
simulation study as well as a geochemical dataset.
","[{'version': 'v1', 'created': 'Wed, 1 Jul 2020 10:59:45 GMT'}]",2024-04-12,"[['Muehlmann', 'Christoph', ''], ['Nordhausen', 'Klaus', ''], ['Yi', 'Mengxi', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
92,2007.05943,Sandor Szedmak,"Sandor Szedmak (1) Eric Bach (1) ((1) Department of Computer Science,
  Aalto University)",On the generalization of Tanimoto-type kernels to real valued functions,"Pages 12, 3 PDF figures, uses arxiv.sty",,,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The Tanimoto kernel (Jaccard index) is a well known tool to describe the
similarity between sets of binary attributes. It has been extended to the case
when the attributes are nonnegative real values. This paper introduces a more
general Tanimoto kernel formulation which allows to measure the similarity of
arbitrary real-valued functions. This extension is constructed by unifying the
representation of the attributes via properly chosen sets. After deriving the
general form of the kernel, explicit feature representation is extracted from
the kernel function, and a simply way of including general kernels into the
Tanimoto kernel is shown. Finally, the kernel is also expressed as a quotient
of piecewise linear functions, and a smooth approximation is provided.
","[{'version': 'v1', 'created': 'Sun, 12 Jul 2020 09:02:27 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Feb 2024 15:47:20 GMT'}]",2024-02-27,"[['Szedmak', 'Sandor', ''], ['Bach', 'Eric', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
93,2007.06169,Tetsuya Kaji,"Tetsuya Kaji, Elena Manresa, Guillaume Pouliot",An Adversarial Approach to Structural Estimation,"56 pages, 3 tables, 11 figures","Econometrica, 91(6), 2041-2063, November 2023",10.3982/ECTA18707,,econ.EM cs.LG math.ST stat.ME stat.ML stat.TH,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a new simulation-based estimation method, adversarial estimation,
for structural models. The estimator is formulated as the solution to a minimax
problem between a generator (which generates simulated observations using the
structural model) and a discriminator (which classifies whether an observation
is simulated). The discriminator maximizes the accuracy of its classification
while the generator minimizes it. We show that, with a sufficiently rich
discriminator, the adversarial estimator attains parametric efficiency under
correct specification and the parametric rate under misspecification. We
advocate the use of a neural network as a discriminator that can exploit
adaptivity properties and attain fast rates of convergence. We apply our method
to the elderly's saving decision model and show that our estimator uncovers the
bequest motive as an important source of saving across the wealth distribution,
not only for the rich.
","[{'version': 'v1', 'created': 'Mon, 13 Jul 2020 03:31:02 GMT'}, {'version': 'v2', 'created': 'Wed, 16 Feb 2022 18:06:48 GMT'}, {'version': 'v3', 'created': 'Tue, 1 Nov 2022 02:49:35 GMT'}]",2024-01-09,"[['Kaji', 'Tetsuya', ''], ['Manresa', 'Elena', ''], ['Pouliot', 'Guillaume', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
94,2007.07181,Harshit Sharma,"Harshit Sharma, Harsh K. Gandhi, Apoorv Jain","Towards Credit-Fraud Detection via Sparsely Varying Gaussian
  Approximations",,,10.1109/ICMLANT50963.2020.9355980,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fraudulent activities are an expensive problem for many financial
institutions, costing billions of dollars to corporations annually. More
commonly occurring activities in this regard are credit card frauds. In this
context, the credit card fraud detection concept has been developed over the
lines of incorporating the uncertainty in our prediction system to ensure
better judgment in such a crucial task. We propose to use a sparse Gaussian
classification method to work with the large data-set and use the concept of
pseudo or inducing inputs. We perform the same with different sets of kernels
and the different number of inducing data points to show the best accuracy was
obtained with the selection of RBF kernel with a higher number of inducing
points. Our approach was able to work over large financial data given the
stochastic nature of our method employed and also good test accuracy with low
variance over the prediction suggesting confidence and robustness in our model.
Using the methodologies of Bayesian learning techniques with the incorporated
inducing points phenomenon, are successfully able to obtain a healthy accuracy
and a high confidence score.
","[{'version': 'v1', 'created': 'Tue, 14 Jul 2020 16:56:06 GMT'}]",2024-06-27,"[['Sharma', 'Harshit', ''], ['Gandhi', 'Harsh K.', ''], ['Jain', 'Apoorv', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
95,2007.07268,Roberto Bigazzi,"Roberto Bigazzi, Federico Landi, Marcella Cornia, Silvia Cascianelli,
  Lorenzo Baraldi, Rita Cucchiara",Explore and Explain: Self-supervised Navigation and Recounting,ICPR 2020,,10.1109/ICPR48806.2021.9412628,,cs.CV cs.AI cs.CL cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Embodied AI has been recently gaining attention as it aims to foster the
development of autonomous and intelligent agents. In this paper, we devise a
novel embodied setting in which an agent needs to explore a previously unknown
environment while recounting what it sees during the path. In this context, the
agent needs to navigate the environment driven by an exploration goal, select
proper moments for description, and output natural language descriptions of
relevant objects and scenes. Our model integrates a novel self-supervised
exploration module with penalty, and a fully-attentive captioning model for
explanation. Also, we investigate different policies for selecting proper
moments for explanation, driven by information coming from both the environment
and the navigation. Experiments are conducted on photorealistic environments
from the Matterport3D dataset and investigate the navigation and explanation
capabilities of the agent as well as the role of their interactions.
","[{'version': 'v1', 'created': 'Tue, 14 Jul 2020 18:00:49 GMT'}]",2024-04-16,"[['Bigazzi', 'Roberto', ''], ['Landi', 'Federico', ''], ['Cornia', 'Marcella', ''], ['Cascianelli', 'Silvia', ''], ['Baraldi', 'Lorenzo', ''], ['Cucchiara', 'Rita', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
96,2007.07796,George Chen,"George H. Chen, Linhong Li, Ren Zuo, Amanda Coston, Jeremy C. Weiss","Neural Topic Models with Survival Supervision: Jointly Predicting
  Time-to-Event Outcomes and Learning How Clinical Features Relate","Accepted at the Artificial Intelligence in Medicine journal;
  preliminary conference version (see earlier arXiv draft) appeared in the
  International Conference on Artificial Intelligence in Medicine (AIME 2020)",,,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
","[{'version': 'v1', 'created': 'Wed, 15 Jul 2020 16:20:04 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Jun 2024 01:45:55 GMT'}]",2024-06-06,"[['Chen', 'George H.', ''], ['Li', 'Linhong', ''], ['Zuo', 'Ren', ''], ['Coston', 'Amanda', ''], ['Weiss', 'Jeremy C.', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
97,2007.10734,Iksung Kang,"Iksung Kang, Alexandre Goy, George Barbastathis","Limited-angle tomographic reconstruction of dense layered objects by
  dynamical machine learning","12 pages, 7 figures, 2 tables",,10.1038/s41377-021-00512-x,,eess.IV cs.LG physics.optics,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Limited-angle tomography of strongly scattering quasi-transparent objects is
a challenging, highly ill-posed problem with practical implications in medical
and biological imaging, manufacturing, automation, and environmental and food
security. Regularizing priors are necessary to reduce artifacts by improving
the condition of such problems. Recently, it was shown that one effective way
to learn the priors for strongly scattering yet highly structured 3D objects,
e.g. layered and Manhattan, is by a static neural network [Goy et al, Proc.
Natl. Acad. Sci. 116, 19848-19856 (2019)]. Here, we present a radically
different approach where the collection of raw images from multiple angles is
viewed analogously to a dynamical system driven by the object-dependent forward
scattering operator. The sequence index in angle of illumination plays the role
of discrete time in the dynamical system analogy. Thus, the imaging problem
turns into a problem of nonlinear system identification, which also suggests
dynamical learning as better fit to regularize the reconstructions. We devised
a recurrent neural network (RNN) architecture with a novel split-convolutional
gated recurrent unit (SC-GRU) as the fundamental building block. Through
comprehensive comparison of several quantitative metrics, we show that the
dynamic method improves upon previous static approaches with fewer artifacts
and better overall reconstruction fidelity.
","[{'version': 'v1', 'created': 'Tue, 21 Jul 2020 11:48:22 GMT'}]",2024-08-15,"[['Kang', 'Iksung', ''], ['Goy', 'Alexandre', ''], ['Barbastathis', 'George', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
98,2007.14244,Gabriel Paludo Licks,Gabriel Paludo Licks and Felipe Meneguzzi,Automated Database Indexing using Model-free Reinforcement Learning,"8 pages, 5 figures (some have subfigures), 1 table",,10.1007/s10489-020-01674-8,,cs.DB cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Configuring databases for efficient querying is a complex task, often carried
out by a database administrator. Solving the problem of building indexes that
truly optimize database access requires a substantial amount of database and
domain knowledge, the lack of which often results in wasted space and memory
for irrelevant indexes, possibly jeopardizing database performance for querying
and certainly degrading performance for updating. We develop an architecture to
solve the problem of automatically indexing a database by using reinforcement
learning to optimize queries by indexing data throughout the lifetime of a
database. In our experimental evaluation, our architecture shows superior
performance compared to related work on reinforcement learning and genetic
algorithms, maintaining near-optimal index configurations and efficiently
scaling to large databases.
","[{'version': 'v1', 'created': 'Sat, 25 Jul 2020 14:36:55 GMT'}]",2024-04-12,"[['Licks', 'Gabriel Paludo', ''], ['Meneguzzi', 'Felipe', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
99,2008.02046,Peter Rousseeuw,"Joachim Schreurs, Iwein Vranckx, Mia Hubert, Johan A.K. Suykens, Peter
  J. Rousseeuw",Outlier detection in non-elliptical data by kernel MRCD,,"Statistics and Computing, 2021, Volume 31, article 66",10.1007/s11222-021-10041-7,,stat.ML cs.LG stat.CO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The minimum regularized covariance determinant method (MRCD) is a robust
estimator for multivariate location and scatter, which detects outliers by
fitting a robust covariance matrix to the data. Its regularization ensures that
the covariance matrix is well-conditioned in any dimension. The MRCD assumes
that the non-outlying observations are roughly elliptically distributed, but
many datasets are not of that form. Moreover, the computation time of MRCD
increases substantially when the number of variables goes up, and nowadays
datasets with many variables are common. The proposed Kernel Minimum
Regularized Covariance Determinant (KMRCD) estimator addresses both issues. It
is not restricted to elliptical data because it implicitly computes the MRCD
estimates in a kernel induced feature space. A fast algorithm is constructed
that starts from kernel-based initial estimates and exploits the kernel trick
to speed up the subsequent computations. Based on the KMRCD estimates, a rule
is proposed to flag outliers. The KMRCD algorithm performs well in simulations,
and is illustrated on real-life data.
","[{'version': 'v1', 'created': 'Wed, 5 Aug 2020 11:09:08 GMT'}, {'version': 'v2', 'created': 'Mon, 29 Mar 2021 22:23:14 GMT'}]",2024-07-08,"[['Schreurs', 'Joachim', ''], ['Vranckx', 'Iwein', ''], ['Hubert', 'Mia', ''], ['Suykens', 'Johan A. K.', ''], ['Rousseeuw', 'Peter J.', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
100,2008.04267,Suyash Gupta,"Maxime Cauchois, Suyash Gupta, Alnur Ali and John C. Duchi",Robust Validation: Confident Predictions Even When Distributions Shift,"Published in the Journal of the American Statistical Association
  (JASA 2024)",,10.1080/01621459.2023.2298037,,stat.ML cs.LG stat.ME,http://creativecommons.org/licenses/by/4.0/,"  While the traditional viewpoint in machine learning and statistics assumes
training and testing samples come from the same population, practice belies
this fiction. One strategy -- coming from robust statistics and optimization --
is thus to build a model robust to distributional perturbations. In this paper,
we take a different approach to describe procedures for robust predictive
inference, where a model provides uncertainty estimates on its predictions
rather than point predictions. We present a method that produces prediction
sets (almost exactly) giving the right coverage level for any test distribution
in an $f$-divergence ball around the training population. The method, based on
conformal inference, achieves (nearly) valid coverage in finite samples, under
only the condition that the training data be exchangeable. An essential
component of our methodology is to estimate the amount of expected future data
shift and build robustness to it; we develop estimators and prove their
consistency for protection and validity of uncertainty estimates under shifts.
By experimenting on several large-scale benchmark datasets, including Recht et
al.'s CIFAR-v4 and ImageNet-V2 datasets, we provide complementary empirical
results that highlight the importance of robust predictive validity.
","[{'version': 'v1', 'created': 'Mon, 10 Aug 2020 17:09:16 GMT'}, {'version': 'v2', 'created': 'Wed, 1 Mar 2023 12:14:52 GMT'}, {'version': 'v3', 'created': 'Thu, 4 Jul 2024 23:42:03 GMT'}]",2024-07-08,"[['Cauchois', 'Maxime', ''], ['Gupta', 'Suyash', ''], ['Ali', 'Alnur', ''], ['Duchi', 'John C.', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
101,2008.05825,"Thorsten Gl\""usenkamp","Thorsten Gl\""usenkamp","Unifying supervised learning and VAEs -- coverage, systematics and
  goodness-of-fit in normalizing-flow based neural network models for
  astro-particle reconstructions",,"Eur. Phys. J. C 84, 163 (2024)",10.1140/epjc/s10052-024-12473-7,,cs.LG astro-ph.HE astro-ph.IM hep-ex stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural-network based predictions of event properties in astro-particle
physics are getting more and more common. However, in many cases the result is
just utilized as a point prediction. Statistical uncertainties, coverage,
systematic uncertainties or a goodness-of-fit measure are often not calculated.
Here we describe a certain choice of training and network architecture that
allows to incorporate all these properties into a single network model. We show
that a KL-divergence objective of the joint distribution of data and labels
allows to unify supervised learning and variational autoencoders (VAEs) under
one umbrella of stochastic variational inference. The unification motivates an
extended supervised learning scheme which allows to calculate a goodness-of-fit
p-value for the neural network model. Conditional normalizing flows amortized
with a neural network are crucial in this construction. We discuss how to
calculate coverage probabilities without numerical integration for specific
""base-ordered"" contours that are unique to normalizing flows. Furthermore we
show how systematic uncertainties can be included via effective marginalization
during training. The proposed extended supervised training incorporates (1)
coverage calculation, (2) systematics and (3) a goodness-of-fit measure in a
single machine-learning model. There are in principle no constraints on the
shape of the involved distributions, in fact the machinery works with complex
multi-modal distributions defined on product spaces like $\mathbb{R}^n \times
\mathbb{S}^m$. The coverage calculation, however, requires care in its
interpretation when the distributions are too degenerate. We see great
potential for exploiting this per-event information in event selections or for
fast astronomical alerts which require uncertainty guarantees.
","[{'version': 'v1', 'created': 'Thu, 13 Aug 2020 11:28:57 GMT'}, {'version': 'v2', 'created': 'Fri, 28 Aug 2020 18:27:11 GMT'}, {'version': 'v3', 'created': 'Sun, 14 Aug 2022 20:47:10 GMT'}, {'version': 'v4', 'created': 'Tue, 3 Oct 2023 17:00:23 GMT'}, {'version': 'v5', 'created': 'Sun, 14 Jan 2024 14:45:36 GMT'}]",2024-03-14,"[['Glüsenkamp', 'Thorsten', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
102,2008.05966,Manaar Alam,"Manaar Alam and Sayandeep Saha and Debdeep Mukhopadhyay and Sandip
  Kundu",Deep-Lock: Secure Authorization for Deep Neural Networks,,,,,cs.LG cs.CR stat.ML,http://creativecommons.org/licenses/by/4.0/,"  Trained Deep Neural Network (DNN) models are considered valuable Intellectual
Properties (IP) in several business models. Prevention of IP theft and
unauthorized usage of such DNN models has been raised as of significant concern
by industry. In this paper, we address the problem of preventing unauthorized
usage of DNN models by proposing a generic and lightweight key-based
model-locking scheme, which ensures that a locked model functions correctly
only upon applying the correct secret key. The proposed scheme, known as
Deep-Lock, utilizes S-Boxes with good security properties to encrypt each
parameter of a trained DNN model with secret keys generated from a master key
via a key scheduling algorithm. The resulting dense network of encrypted
weights is found robust against model fine-tuning attacks. Finally, Deep-Lock
does not require any intervention in the structure and training of the DNN
models, making it applicable for all existing software and hardware
implementations of DNN.
","[{'version': 'v1', 'created': 'Thu, 13 Aug 2020 15:22:49 GMT'}, {'version': 'v2', 'created': 'Sun, 18 Feb 2024 18:32:50 GMT'}]",2024-02-20,"[['Alam', 'Manaar', ''], ['Saha', 'Sayandeep', ''], ['Mukhopadhyay', 'Debdeep', ''], ['Kundu', 'Sandip', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
103,2008.07007,Kacper Sokol,Kacper Sokol and Peter Flach,Interpretable Representations in Explainable AI: From Theory to Practice,"Published in the *Special Issue on Explainable and Interpretable
  Machine Learning and Data Mining* of the Springer *Data Mining and Knowledge
  Discovery* journal",,10.1007/s10618-024-01010-5,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Interpretable representations are the backbone of many explainers that target
black-box predictive systems based on artificial intelligence and machine
learning algorithms. They translate the low-level data representation necessary
for good predictive performance into high-level human-intelligible concepts
used to convey the explanatory insights. Notably, the explanation type and its
cognitive complexity are directly controlled by the interpretable
representation, tweaking which allows to target a particular audience and use
case. However, many explainers built upon interpretable representations
overlook their merit and fall back on default solutions that often carry
implicit assumptions, thereby degrading the explanatory power and reliability
of such techniques. To address this problem, we study properties of
interpretable representations that encode presence and absence of
human-comprehensible concepts. We demonstrate how they are operationalised for
tabular, image and text data; discuss their assumptions, strengths and
weaknesses; identify their core building blocks; and scrutinise their
configuration and parameterisation. In particular, this in-depth analysis
allows us to pinpoint their explanatory properties, desiderata and scope for
(malicious) manipulation in the context of tabular data where a linear model is
used to quantify the influence of interpretable concepts on a black-box
prediction. Our findings lead to a range of recommendations for designing
trustworthy interpretable representations; specifically, the benefits of
class-aware (supervised) discretisation of tabular data, e.g., with decision
trees, and sensitivity of image interpretable representations to segmentation
granularity and occlusion colour.
","[{'version': 'v1', 'created': 'Sun, 16 Aug 2020 21:44:03 GMT'}, {'version': 'v2', 'created': 'Fri, 30 Sep 2022 19:40:32 GMT'}, {'version': 'v3', 'created': 'Sat, 23 Dec 2023 14:00:43 GMT'}, {'version': 'v4', 'created': 'Fri, 26 Apr 2024 09:22:34 GMT'}]",2024-04-29,"[['Sokol', 'Kacper', ''], ['Flach', 'Peter', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
104,2008.07361,Luis John,"Luis H. John, Jan A. Kors, Jenna M. Reps, Patrick B. Ryan, Peter R.
  Rijnbeek","Logistic regression models for patient-level prediction based on massive
  observational data: Do we need all data?",,"International Journal of Medical Informatics, Volume 163, July
  2022, Article number 104762",10.1016/j.ijmedinf.2022.104762,,stat.AP cs.LG stat.ME stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
","[{'version': 'v1', 'created': 'Fri, 14 Aug 2020 11:00:13 GMT'}, {'version': 'v2', 'created': 'Wed, 24 Jul 2024 09:56:06 GMT'}]",2024-07-25,"[['John', 'Luis H.', ''], ['Kors', 'Jan A.', ''], ['Reps', 'Jenna M.', ''], ['Ryan', 'Patrick B.', ''], ['Rijnbeek', 'Peter R.', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
105,2008.07902,Guoli Wu,Guoli Wu and Jingya Zhang and Junqiang Song,Bayesian geoacoustic inversion using mixture density network,"The manuscript has been submitted to the SEG (GEOPHYSICS) for
  publication consideration",,,,stat.ML cs.LG physics.geo-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Bayesian geoacoustic inversion problems are conventionally solved by Markov
chain Monte Carlo methods or its variants, which are computationally expensive.
This paper extends the classic Bayesian geoacoustic inversion framework by
deriving important geoacoustic statistics of Bayesian geoacoustic inversion
from the multidimensional posterior probability density (PPD) using the mixture
density network (MDN) theory. These statistics make it convenient to train the
network directly on the whole parameter space and get the multidimensional PPD
of model parameters. The present approach provides a much more efficient way to
solve geoacoustic inversion problems in Bayesian inference framework. The
network is trained on a simulated dataset of surface-wave dispersion curves
with shear-wave velocities as labels and tested on both synthetic and real data
cases. The results show that the network gives reliable predictions and has
good generalization performance on unseen data. Once trained, the network can
rapidly (within seconds) give a fully probabilistic solution which is
comparable to Monte Carlo methods. It provides an promising approach for
real-time inversion.
","[{'version': 'v1', 'created': 'Tue, 18 Aug 2020 13:02:40 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Aug 2020 08:00:52 GMT'}, {'version': 'v3', 'created': 'Sun, 17 Jan 2021 02:52:35 GMT'}, {'version': 'v4', 'created': 'Sat, 6 Jul 2024 14:29:20 GMT'}]",2024-07-09,"[['Wu', 'Guoli', ''], ['Zhang', 'Jingya', ''], ['Song', 'Junqiang', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
106,2008.08718,Yaroslav Averyanov,Yaroslav Averyanov and Alain Celisse,"Minimum discrepancy principle strategy for choosing $k$ in $k$-NN
  regression","Bias-variance reference rule was modified in Eq. (14); new optimality
  result in population norm was added in Corollary 1",,,,stat.ML cs.LG math.ST stat.TH,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a novel data-driven strategy to choose the hyperparameter $k$ in
the $k$-NN regression estimator without using any hold-out data. We treat the
problem of choosing the hyperparameter as an iterative procedure (over $k$) and
propose using an easily implemented in practice strategy based on the idea of
early stopping and the minimum discrepancy principle. This model selection
strategy is proven to be minimax-optimal over some smoothness function classes,
for instance, the Lipschitz functions class on a bounded domain. The novel
method often improves statistical performance on artificial and real-world data
sets in comparison to other model selection strategies, such as the Hold-out
method, 5-fold cross-validation, and AIC criterion. The novelty of the strategy
comes from reducing the computational time of the model selection procedure
while preserving the statistical (minimax) optimality of the resulting
estimator. More precisely, given a sample of size $n$, if one should choose $k$
among $\left\{ 1, \ldots, n \right\}$, and $\left\{ f^1, \ldots, f^n \right\}$
are the estimators of the regression function, the minimum discrepancy
principle requires the calculation of a fraction of the estimators, while this
is not the case for the generalized cross-validation, Akaike's AIC criteria, or
Lepskii principle.
","[{'version': 'v1', 'created': 'Thu, 20 Aug 2020 00:13:19 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Jan 2021 15:11:35 GMT'}, {'version': 'v3', 'created': 'Fri, 26 Feb 2021 11:53:33 GMT'}, {'version': 'v4', 'created': 'Wed, 5 May 2021 11:33:16 GMT'}, {'version': 'v5', 'created': 'Mon, 6 May 2024 20:37:12 GMT'}, {'version': 'v6', 'created': 'Tue, 11 Jun 2024 17:15:26 GMT'}, {'version': 'v7', 'created': 'Mon, 8 Jul 2024 15:43:59 GMT'}, {'version': 'v8', 'created': 'Wed, 17 Jul 2024 17:28:01 GMT'}]",2024-07-18,"[['Averyanov', 'Yaroslav', ''], ['Celisse', 'Alain', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
107,2008.09569,Suvodeep Majumder,"Suvodeep Majumder, Pranav Mody, Tim Menzies",Revisiting Process versus Product Metrics: a Large Scale Analysis,"36 pages, 12 figures and 5 tables","Empirical Software Engineering, Volume 27, Issue 3, May 2022",10.1007/s10664-021-10068-4,,cs.SE cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Numerous methods can build predictive models from software data. However,
what methods and conclusions should we endorse as we move from analytics
in-the-small (dealing with a handful of projects) to analytics in-the-large
(dealing with hundreds of projects)?
  To answer this question, we recheck prior small-scale results (about process
versus product metrics for defect prediction and the granularity of metrics)
using 722,471 commits from 700 Github projects. We find that some analytics
in-the-small conclusions still hold when scaling up to analytics in-the-large.
For example, like prior work, we see that process metrics are better predictors
for defects than product metrics (best process/product-based learners
respectively achieve recalls of 98\%/44\% and AUCs of 95\%/54\%, median
values).
  That said, we warn that it is unwise to trust metric importance results from
analytics in-the-small studies since those change dramatically when moving to
analytics in-the-large. Also, when reasoning in-the-large about hundreds of
projects, it is better to use predictions from multiple models (since single
model predictions can become confused and exhibit a high variance).
","[{'version': 'v1', 'created': 'Fri, 21 Aug 2020 16:26:22 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Oct 2020 19:23:22 GMT'}, {'version': 'v3', 'created': 'Tue, 26 Oct 2021 13:50:46 GMT'}]",2024-02-19,"[['Majumder', 'Suvodeep', ''], ['Mody', 'Pranav', ''], ['Menzies', 'Tim', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
108,2008.09926,Anuraganand Sharma Dr,Anuraganand Sharma,"Optimistic variants of single-objective bilevel optimization for
  evolutionary algorithms","preprint: 15 pages, published: 20 pages","International Journal of Computational Intelligence and
  Applications (IJCIA), 2020, pp. 2050020-1 to 2050020-20",10.1142/S1469026820500200,"Vol. 19, No. 3",cs.NE cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
","[{'version': 'v1', 'created': 'Sat, 22 Aug 2020 23:12:07 GMT'}]",2024-02-13,"[['Sharma', 'Anuraganand', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
109,2008.11945,Yongquan Yang,Yongquan Yang,"Moderately Supervised Learning: Definition, Framework and Generality",This is the final published version (33 pages),Artificial Intelligence Review (2024),10.1007/s10462-023-10654-6,,cs.CV cs.LG eess.IV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Learning with supervision has achieved remarkable success in numerous
artificial intelligence (AI) applications. In the current literature, by
referring to the properties of the labels prepared for the training dataset,
learning with supervision is categorized as supervised learning (SL) and weakly
supervised learning (WSL). SL concerns the situation where the training data
set is assigned with ideal (complete, exact and accurate) labels, while WSL
concerns the situation where the training data set is assigned with non-ideal
(incomplete, inexact or inaccurate) labels. However, various solutions for SL
tasks have shown that the given labels are not always easy to learn, and the
transformation from the given labels to easy-to-learn targets can significantly
affect the performance of the final SL solutions. Without considering the
properties of the transformation from the given labels to easy-to-learn
targets, the definition of SL conceals some details that can be critical to
building the appropriate solutions for specific SL tasks. Thus, for engineers
in the AI application field, it is desirable to reveal these details
systematically. This article attempts to achieve this goal by expanding the
categorization of SL and investigating the sub-type moderately supervised
learning (MSL) that concerns the situation where the given labels are ideal,
but due to the simplicity in annotation, careful designs are required to
transform the given labels into easy-to-learn targets. From the perspectives of
the definition, framework and generality, we conceptualize MSL to present a
complete fundamental basis to systematically analyse MSL tasks. At meantime,
revealing the relation between the conceptualization of MSL and the
mathematicians' vision, this paper as well establishes a tutorial for AI
application engineers to refer to viewing a problem to be solved from the
mathematicians' vision.
","[{'version': 'v1', 'created': 'Thu, 27 Aug 2020 06:53:53 GMT'}, {'version': 'v2', 'created': 'Mon, 9 May 2022 03:30:23 GMT'}, {'version': 'v3', 'created': 'Wed, 18 May 2022 06:53:23 GMT'}, {'version': 'v4', 'created': 'Tue, 25 Apr 2023 05:41:57 GMT'}, {'version': 'v5', 'created': 'Mon, 24 Jul 2023 01:23:36 GMT'}, {'version': 'v6', 'created': 'Wed, 31 Jan 2024 04:45:32 GMT'}]",2024-02-12,"[['Yang', 'Yongquan', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
110,2009.04614,Kun Fang,"Kun Fang, Fanghui Liu, Xiaolin Huang and Jie Yang",End-to-end Kernel Learning via Generative Random Fourier Features,Accepted by Pattern Recognition,,10.1016/j.patcog.2022.109057,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Random Fourier features (RFFs) provide a promising way for kernel learning in
a spectral case. Current RFFs-based kernel learning methods usually work in a
two-stage way. In the first-stage process, learning the optimal feature map is
often formulated as a target alignment problem, which aims to align the learned
kernel with the pre-defined target kernel (usually the ideal kernel). In the
second-stage process, a linear learner is conducted with respect to the mapped
random features. Nevertheless, the pre-defined kernel in target alignment is
not necessarily optimal for the generalization of the linear learner. Instead,
in this paper, we consider a one-stage process that incorporates the kernel
learning and linear learner into a unifying framework. To be specific, a
generative network via RFFs is devised to implicitly learn the kernel, followed
by a linear classifier parameterized as a full-connected layer. Then the
generative network and the classifier are jointly trained by solving the
empirical risk minimization (ERM) problem to reach a one-stage solution. This
end-to-end scheme naturally allows deeper features, in correspondence to a
multi-layer structure, and shows superior generalization performance over the
classical two-stage, RFFs-based methods in real-world classification tasks.
Moreover, inspired by the randomized resampling mechanism of the proposed
method, its enhanced adversarial robustness is investigated and experimentally
verified.
","[{'version': 'v1', 'created': 'Thu, 10 Sep 2020 00:27:39 GMT'}, {'version': 'v2', 'created': 'Fri, 10 Dec 2021 08:55:10 GMT'}, {'version': 'v3', 'created': 'Tue, 14 Jun 2022 08:02:10 GMT'}, {'version': 'v4', 'created': 'Tue, 22 Nov 2022 07:52:06 GMT'}, {'version': 'v5', 'created': 'Tue, 16 Jan 2024 02:54:58 GMT'}]",2024-01-17,"[['Fang', 'Kun', ''], ['Liu', 'Fanghui', ''], ['Huang', 'Xiaolin', ''], ['Yang', 'Jie', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
111,2009.0812,Kim Hammar,Kim Hammar and Rolf Stadler,"Finding Effective Security Strategies through Reinforcement Learning and
  Self-Play",,"2020 16th International Conference on Network and Service
  Management (CNSM)",10.23919/CNSM50824.2020.9269092,,cs.LG cs.CR cs.NI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a method to automatically find security strategies for the use
case of intrusion prevention. Following this method, we model the interaction
between an attacker and a defender as a Markov game and let attack and defense
strategies evolve through reinforcement learning and self-play without human
intervention. Using a simple infrastructure configuration, we demonstrate that
effective security strategies can emerge from self-play. This shows that
self-play, which has been applied in other domains with great success, can be
effective in the context of network security. Inspection of the converged
policies show that the emerged policies reflect common-sense knowledge and are
similar to strategies of humans. Moreover, we address known challenges of
reinforcement learning in this domain and present an approach that uses
function approximation, an opponent pool, and an autoregressive policy
representation. Through evaluations we show that our method is superior to two
baseline methods but that policy convergence in self-play remains a challenge.
","[{'version': 'v1', 'created': 'Thu, 17 Sep 2020 07:41:27 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Oct 2020 16:22:54 GMT'}]",2024-04-23,"[['Hammar', 'Kim', ''], ['Stadler', 'Rolf', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
112,2009.10128,Mickael Arcos,"Micka\""el Arcos","Claraprint: a chord and melody based fingerprint for western classical
  music cover detection",,,,,cs.IR,http://creativecommons.org/licenses/by/4.0/,"  Cover song detection has been an active field in the Music Information
Retrieval (MIR) community during the past decades. Most of the research
community focused in solving it for a wide range of music genres with diverse
characteristics. Western classical music, a genre heavily based on the
recording of ""cover songs"", or musical works, represents a large heritage,
offering immediate application for an efficient fingerprint algorithm. We
propose an engineering approach for retrieving a cover song from a reference
database thanks to a fingerprint designed for classical musical works. We open
a new data set to encourage the scientific community to use it for further
researches regarding this genre.
","[{'version': 'v1', 'created': 'Mon, 21 Sep 2020 18:46:00 GMT'}, {'version': 'v2', 'created': 'Mon, 15 Apr 2024 14:22:57 GMT'}]",2024-04-16,"[['Arcos', 'Mickaël', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
113,2009.10303,Ricardo Baptista,"Ricardo Baptista, Youssef Marzouk, Olivier Zahm",On the representation and learning of monotone triangular transport maps,"40 pages, 9 figures, 3 tables","Foundations of Computational Mathematics, 2023",10.1007/s10208-023-09630-x,,stat.ML cs.LG math.FA stat.CO stat.ME,http://creativecommons.org/licenses/by/4.0/,"  Transportation of measure provides a versatile approach for modeling complex
probability distributions, with applications in density estimation, Bayesian
inference, generative modeling, and beyond. Monotone triangular transport
maps$\unicode{x2014}$approximations of the Knothe$\unicode{x2013}$Rosenblatt
(KR) rearrangement$\unicode{x2014}$are a canonical choice for these tasks. Yet
the representation and parameterization of such maps have a significant impact
on their generality and expressiveness, and on properties of the optimization
problem that arises in learning a map from data (e.g., via maximum likelihood
estimation). We present a general framework for representing monotone
triangular maps via invertible transformations of smooth functions. We
establish conditions on the transformation such that the associated
infinite-dimensional minimization problem has no spurious local minima, i.e.,
all local minima are global minima; and we show for target distributions
satisfying certain tail conditions that the unique global minimizer corresponds
to the KR map. Given a sample from the target, we then propose an adaptive
algorithm that estimates a sparse semi-parametric approximation of the
underlying KR map. We demonstrate how this framework can be applied to joint
and conditional density estimation, likelihood-free inference, and structure
learning of directed graphical models, with stable generalization performance
across a range of sample sizes.
","[{'version': 'v1', 'created': 'Tue, 22 Sep 2020 03:41:45 GMT'}, {'version': 'v2', 'created': 'Fri, 8 Jul 2022 14:20:22 GMT'}, {'version': 'v3', 'created': 'Sat, 24 Feb 2024 23:13:34 GMT'}]",2024-02-27,"[['Baptista', 'Ricardo', ''], ['Marzouk', 'Youssef', ''], ['Zahm', 'Olivier', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
114,2010.02318,Tianfan Fu,"Tianfan Fu, Cao Xiao, Xinhao Li, Lucas M. Glass, Jimeng Sun",MIMOSA: Multi-constraint Molecule Sampling for Molecule Optimization,Accepted by AAAI 2021,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
","[{'version': 'v1', 'created': 'Mon, 5 Oct 2020 20:18:42 GMT'}, {'version': 'v2', 'created': 'Fri, 11 Dec 2020 19:06:13 GMT'}, {'version': 'v3', 'created': 'Sun, 27 Feb 2022 19:26:48 GMT'}, {'version': 'v4', 'created': 'Sun, 30 Jun 2024 23:58:07 GMT'}]",2024-07-02,"[['Fu', 'Tianfan', ''], ['Xiao', 'Cao', ''], ['Li', 'Xinhao', ''], ['Glass', 'Lucas M.', ''], ['Sun', 'Jimeng', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
115,2010.05784,Haoxuan Wang,"Haoxuan Wang, Zhiding Yu, Yisong Yue, Anima Anandkumar, Anqi Liu,
  Junchi Yan","Learning Calibrated Uncertainties for Domain Shift: A Distributionally
  Robust Learning Approach",IJCAI 2023,,,,cs.LG cs.CV,http://creativecommons.org/licenses/by/4.0/,"  We propose a framework for learning calibrated uncertainties under domain
shifts, where the source (training) distribution differs from the target (test)
distribution. We detect such domain shifts via a differentiable density ratio
estimator and train it together with the task network, composing an adjusted
softmax predictive form concerning domain shift. In particular, the density
ratio estimation reflects the closeness of a target (test) sample to the source
(training) distribution. We employ it to adjust the uncertainty of prediction
in the task network. This idea of using the density ratio is based on the
distributionally robust learning (DRL) framework, which accounts for the domain
shift by adversarial risk minimization. We show that our proposed method
generates calibrated uncertainties that benefit downstream tasks, such as
unsupervised domain adaptation (UDA) and semi-supervised learning (SSL). On
these tasks, methods like self-training and FixMatch use uncertainties to
select confident pseudo-labels for re-training. Our experiments show that the
introduction of DRL leads to significant improvements in cross-domain
performance. We also show that the estimated density ratios align with human
selection frequencies, suggesting a positive correlation with a proxy of human
perceived uncertainties.
","[{'version': 'v1', 'created': 'Thu, 8 Oct 2020 02:10:54 GMT'}, {'version': 'v2', 'created': 'Mon, 20 Sep 2021 18:08:45 GMT'}, {'version': 'v3', 'created': 'Sun, 26 Dec 2021 23:21:37 GMT'}, {'version': 'v4', 'created': 'Tue, 6 Feb 2024 03:53:05 GMT'}]",2024-02-07,"[['Wang', 'Haoxuan', ''], ['Yu', 'Zhiding', ''], ['Yue', 'Yisong', ''], ['Anandkumar', 'Anima', ''], ['Liu', 'Anqi', ''], ['Yan', 'Junchi', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
116,2010.1486,Dennis Forster,"Simon Damm, Dennis Forster, Dmytro Velychko, Zhenwen Dai, Asja
  Fischer, J\""org L\""ucke","The ELBO of Variational Autoencoders Converges to a Sum of Three
  Entropies",,"Proceedings of the 26th International Conference on Artificial
  Intelligence and Statistics (AISTATS), PMLR 206:3931-3960, 2023",,,stat.ML cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The central objective function of a variational autoencoder (VAE) is its
variational lower bound (the ELBO). Here we show that for standard (i.e.,
Gaussian) VAEs the ELBO converges to a value given by the sum of three
entropies: the (negative) entropy of the prior distribution, the expected
(negative) entropy of the observable distribution, and the average entropy of
the variational distributions (the latter is already part of the ELBO). Our
derived analytical results are exact and apply for small as well as for
intricate deep networks for encoder and decoder. Furthermore, they apply for
finitely and infinitely many data points and at any stationary point (including
local maxima and saddle points). The result implies that the ELBO can for
standard VAEs often be computed in closed-form at stationary points while the
original ELBO requires numerical approximations of integrals. As a main
contribution, we provide the proof that the ELBO for VAEs is at stationary
points equal to entropy sums. Numerical experiments then show that the obtained
analytical results are sufficiently precise also in those vicinities of
stationary points that are reached in practice. Furthermore, we discuss how the
novel entropy form of the ELBO can be used to analyze and understand learning
behavior. More generally, we believe that our contributions can be useful for
future theoretical and practical studies on VAE learning as they provide novel
information on those points in parameters space that optimization of VAEs
converges to.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 10:13:28 GMT'}, {'version': 'v2', 'created': 'Sun, 13 Jun 2021 19:33:01 GMT'}, {'version': 'v3', 'created': 'Tue, 15 Jun 2021 17:38:52 GMT'}, {'version': 'v4', 'created': 'Thu, 3 Nov 2022 12:30:07 GMT'}, {'version': 'v5', 'created': 'Thu, 20 Apr 2023 08:00:35 GMT'}]",2024-04-30,"[['Damm', 'Simon', ''], ['Forster', 'Dennis', ''], ['Velychko', 'Dmytro', ''], ['Dai', 'Zhenwen', ''], ['Fischer', 'Asja', ''], ['Lücke', 'Jörg', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
117,2010.15011,Yuli Slavutsky,"Yuli Slavutsky, Yuval Benjamini",Predicting Classification Accuracy When Adding New Unobserved Classes,,"International Conference on Learning Representations (ICLR), 2021",,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multiclass classifiers are often designed and evaluated only on a sample from
the classes on which they will eventually be applied. Hence, their final
accuracy remains unknown. In this work we study how a classifier's performance
over the initial class sample can be used to extrapolate its expected accuracy
on a larger, unobserved set of classes. For this, we define a measure of
separation between correct and incorrect classes that is independent of the
number of classes: the ""reversed ROC"" (rROC), which is obtained by replacing
the roles of classes and data-points in the common ROC. We show that the
classification accuracy is a function of the rROC in multiclass classifiers,
for which the learned representation of data from the initial class sample
remains unchanged when new classes are added. Using these results we formulate
a robust neural-network-based algorithm, ""CleaneX"", which learns to estimate
the accuracy of such classifiers on arbitrarily large sets of classes. Unlike
previous methods, our method uses both the observed accuracies of the
classifier and densities of classification scores, and therefore achieves
remarkably better predictions than current state-of-the-art methods on both
simulations and real datasets of object detection, face recognition, and brain
decoding.
","[{'version': 'v1', 'created': 'Wed, 28 Oct 2020 14:37:25 GMT'}, {'version': 'v2', 'created': 'Thu, 28 Jan 2021 10:56:27 GMT'}, {'version': 'v3', 'created': 'Tue, 9 Mar 2021 14:38:37 GMT'}]",2024-05-29,"[['Slavutsky', 'Yuli', ''], ['Benjamini', 'Yuval', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
118,2010.16271,Wouter van Loon,"Wouter van Loon, Marjolein Fokkema, Botond Szabo, Mark de Rooij",View selection in multi-view stacking: Choosing the meta-learner,"47 pages, 17 figures. Accepted manuscript",Advances in Data Analysis and Classification (2024),10.1007/s11634-024-00587-5,,stat.ML cs.LG stat.ME,http://creativecommons.org/licenses/by/4.0/,"  Multi-view stacking is a framework for combining information from different
views (i.e. different feature sets) describing the same set of objects. In this
framework, a base-learner algorithm is trained on each view separately, and
their predictions are then combined by a meta-learner algorithm. In a previous
study, stacked penalized logistic regression, a special case of multi-view
stacking, has been shown to be useful in identifying which views are most
important for prediction. In this article we expand this research by
considering seven different algorithms to use as the meta-learner, and
evaluating their view selection and classification performance in simulations
and two applications on real gene-expression data sets. Our results suggest
that if both view selection and classification accuracy are important to the
research at hand, then the nonnegative lasso, nonnegative adaptive lasso and
nonnegative elastic net are suitable meta-learners. Exactly which among these
three is to be preferred depends on the research context. The remaining four
meta-learners, namely nonnegative ridge regression, nonnegative forward
selection, stability selection and the interpolating predictor, show little
advantages in order to be preferred over the other three.
","[{'version': 'v1', 'created': 'Fri, 30 Oct 2020 13:45:14 GMT'}, {'version': 'v2', 'created': 'Mon, 29 Jan 2024 13:36:23 GMT'}, {'version': 'v3', 'created': 'Mon, 15 Apr 2024 09:39:57 GMT'}]",2024-04-16,"[['van Loon', 'Wouter', ''], ['Fokkema', 'Marjolein', ''], ['Szabo', 'Botond', ''], ['de Rooij', 'Mark', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
119,2011.00613,Yansong Gao Mr.,Yansong Gao and Pratik Chaudhari,An Information-Geometric Distance on the Space of Tasks,,,,"Proc. of the International Conference of Machine Learning (ICML)
  2021",cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper prescribes a distance between learning tasks modeled as joint
distributions on data and labels. Using tools in information geometry, the
distance is defined to be the length of the shortest weight trajectory on a
Riemannian manifold as a classifier is fitted on an interpolated task. The
interpolated task evolves from the source to the target task using an optimal
transport formulation. This distance, which we call the ""coupled transfer
distance"" can be compared across different classifier architectures. We develop
an algorithm to compute the distance which iteratively transports the marginal
on the data of the source task to that of the target task while updating the
weights of the classifier to track this evolving data distribution. We develop
theory to show that our distance captures the intuitive idea that a good
transfer trajectory is the one that keeps the generalization gap small during
transfer, in particular at the end on the target task. We perform thorough
empirical validation and analysis across diverse image classification datasets
to show that the coupled transfer distance correlates strongly with the
difficulty of fine-tuning.
","[{'version': 'v1', 'created': 'Sun, 1 Nov 2020 19:48:39 GMT'}, {'version': 'v2', 'created': 'Thu, 25 Feb 2021 03:33:30 GMT'}]",2024-05-07,"[['Gao', 'Yansong', ''], ['Chaudhari', 'Pratik', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
120,2011.02281,Sebastian Neumayer,Johannes Hertrich and Sebastian Neumayer and Gabriele Steidl,Convolutional Proximal Neural Networks and Plug-and-Play Algorithms,,"Linear Algebra and its Applications, vol 631, pp. 203-234, 2024",10.1016/j.laa.2021.09.004,,math.OC cs.LG eess.SP,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we introduce convolutional proximal neural networks (cPNNs),
which are by construction averaged operators. For filters of full length, we
propose a stochastic gradient descent algorithm on a submanifold of the Stiefel
manifold to train cPNNs. In case of filters with limited length, we design
algorithms for minimizing functionals that approximate the orthogonality
constraints imposed on the operators by penalizing the least squares distance
to the identity operator. Then, we investigate how scaled cPNNs with a
prescribed Lipschitz constant can be used for denoising signals and images,
where the achieved quality depends on the Lipschitz constant. Finally, we apply
cPNN based denoisers within a Plug-and-Play (PnP) framework and provide
convergence results for the corresponding PnP forward-backward splitting
algorithm based on an oracle construction.
","[{'version': 'v1', 'created': 'Wed, 4 Nov 2020 13:32:46 GMT'}]",2024-08-13,"[['Hertrich', 'Johannes', ''], ['Neumayer', 'Sebastian', ''], ['Steidl', 'Gabriele', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
121,2011.03715,Marzieh Ajirak,"Marzieh Ajirak, Cassandra Heiselman, Anna Fuchs, Mia Heiligenstein,
  Kimberly Herrera, Diana Garretto, Petar Djuric","Bayesian Nonparametric Dimensionality Reduction of Categorical Data for
  Predicting Severity of COVID-19 in Pregnant Women",,,10.23919/EUSIPCO54536.2021.9616021,,stat.AP cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The coronavirus disease (COVID-19) has rapidly spread throughout the world
and while pregnant women present the same adverse outcome rates, they are
underrepresented in clinical research. We collected clinical data of 155
test-positive COVID-19 pregnant women at Stony Brook University Hospital. Many
of these collected data are of multivariate categorical type, where the number
of possible outcomes grows exponentially as the dimension of data increases. We
modeled the data within the unsupervised Bayesian framework and mapped them
into a lower-dimensional space using latent Gaussian processes. The latent
features in the lower dimensional space were further used for predicting if a
pregnant woman would be admitted to a hospital due to COVID-19 or would remain
with mild symptoms. We compared the prediction accuracy with the dummy/one-hot
encoding of categorical data and found that the latent Gaussian process had
better accuracy.
","[{'version': 'v1', 'created': 'Sat, 7 Nov 2020 07:11:29 GMT'}, {'version': 'v2', 'created': 'Mon, 27 Sep 2021 15:58:54 GMT'}]",2024-03-13,"[['Ajirak', 'Marzieh', ''], ['Heiselman', 'Cassandra', ''], ['Fuchs', 'Anna', ''], ['Heiligenstein', 'Mia', ''], ['Herrera', 'Kimberly', ''], ['Garretto', 'Diana', ''], ['Djuric', 'Petar', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
122,2011.05533,Matthew Marge,"Matthew Marge, Carol Espy-Wilson, Nigel Ward","Spoken Language Interaction with Robots: Research Issues and
  Recommendations, Report from the NSF Future Directions Workshop","35 pages, 6 figures, Final report from the NSF Future Directions
  Workshop on Speech for Robotics, held in October 2019, College Park, MD.
  Workshop website: https://isr.umd.edu/2019-SFRW",,10.1016/j.csl.2021.101255,,cs.RO cs.CL cs.HC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With robotics rapidly advancing, more effective human-robot interaction is
increasingly needed to realize the full potential of robots for society. While
spoken language must be part of the solution, our ability to provide spoken
language interaction capabilities is still very limited. The National Science
Foundation accordingly convened a workshop, bringing together speech, language,
and robotics researchers to discuss what needs to be done. The result is this
report, in which we identify key scientific and engineering advances needed.
  Our recommendations broadly relate to eight general themes. First, meeting
human needs requires addressing new challenges in speech technology and user
experience design. Second, this requires better models of the social and
interactive aspects of language use. Third, for robustness, robots need
higher-bandwidth communication with users and better handling of uncertainty,
including simultaneous consideration of multiple hypotheses and goals. Fourth,
more powerful adaptation methods are needed, to enable robots to communicate in
new environments, for new tasks, and with diverse user populations, without
extensive re-engineering or the collection of massive training data. Fifth,
since robots are embodied, speech should function together with other
communication modalities, such as gaze, gesture, posture, and motion. Sixth,
since robots operate in complex environments, speech components need access to
rich yet efficient representations of what the robot knows about objects,
locations, noise sources, the user, and other humans. Seventh, since robots
operate in real time, their speech and language processing components must
also. Eighth, in addition to more research, we need more work on infrastructure
and resources, including shareable software modules and internal interfaces,
inexpensive hardware, baseline systems, and diverse corpora.
","[{'version': 'v1', 'created': 'Wed, 11 Nov 2020 03:45:34 GMT'}]",2024-03-25,"[['Marge', 'Matthew', ''], ['Espy-Wilson', 'Carol', ''], ['Ward', 'Nigel', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
123,2011.06702,Cheng Chen,"Cheng Chen, Junjie Yang, Yi Zhou","Neural Network Training Techniques Regularize Optimization Trajectory:
  An Empirical Study","9 pages, 16 figures, this paper has been accepted as a short paper by
  the conference of IEEE-bigdata-2020","2020 IEEE International Conference on Big Data (Big Data),
  Atlanta, GA, USA, 2020, pp. 141-146",10.1109/BigData50022.2020.9378359,,cs.LG math.OC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modern deep neural network (DNN) trainings utilize various training
techniques, e.g., nonlinear activation functions, batch normalization,
skip-connections, etc. Despite their effectiveness, it is still mysterious how
they help accelerate DNN trainings in practice. In this paper, we provide an
empirical study of the regularization effect of these training techniques on
DNN optimization. Specifically, we find that the optimization trajectories of
successful DNN trainings consistently obey a certain regularity principle that
regularizes the model update direction to be aligned with the trajectory
direction. Theoretically, we show that such a regularity principle leads to a
convergence guarantee in nonconvex optimization and the convergence rate
depends on a regularization parameter. Empirically, we find that DNN trainings
that apply the training techniques achieve a fast convergence and obey the
regularity principle with a large regularization parameter, implying that the
model updates are well aligned with the trajectory. On the other hand, DNN
trainings without the training techniques have slow convergence and obey the
regularity principle with a small regularization parameter, implying that the
model updates are not well aligned with the trajectory. Therefore, different
training techniques regularize the model update direction via the regularity
principle to facilitate the convergence.
","[{'version': 'v1', 'created': 'Fri, 13 Nov 2020 00:26:43 GMT'}]",2024-03-05,"[['Chen', 'Cheng', ''], ['Yang', 'Junjie', ''], ['Zhou', 'Yi', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
124,2011.06716,Sha Lu,"Sha Lu, Lin Liu, Kui Yu, Thuc Duy Le, Jixue Liu, Jiuyong Li","Dependency-based Anomaly Detection: a General Framework and
  Comprehensive Evaluation",,,,,cs.LG cs.AI,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Anomaly detection is crucial for understanding unusual behaviors in data, as
anomalies offer valuable insights. This paper introduces Dependency-based
Anomaly Detection (DepAD), a general framework that utilizes variable
dependencies to uncover meaningful anomalies with better interpretability.
DepAD reframes unsupervised anomaly detection as supervised feature selection
and prediction tasks, which allows users to tailor anomaly detection algorithms
to their specific problems and data. We extensively evaluate representative
off-the-shelf techniques for the DepAD framework. Two DepAD algorithms emerge
as all-rounders and superior performers in handling a wide range of datasets
compared to nine state-of-the-art anomaly detection methods. Additionally, we
demonstrate that DepAD algorithms provide new and insightful interpretations
for detected anomalies.
","[{'version': 'v1', 'created': 'Fri, 13 Nov 2020 01:39:44 GMT'}, {'version': 'v2', 'created': 'Wed, 17 Apr 2024 05:44:10 GMT'}]",2024-04-18,"[['Lu', 'Sha', ''], ['Liu', 'Lin', ''], ['Yu', 'Kui', ''], ['Le', 'Thuc Duy', ''], ['Liu', 'Jixue', ''], ['Li', 'Jiuyong', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
125,2011.07119,Nicola Bastianello,Nicola Bastianello,tvopt: A Python Framework for Time-Varying Optimization,"Code available here: https://github.com/nicola-bastianello/tvopt --
  IEEE CDC'21 paper",,10.1109/CDC45484.2021.9683695,,cs.MS cs.LG math.OC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper introduces tvopt, a Python framework for prototyping and
benchmarking time-varying (or online) optimization algorithms. The paper first
describes the theoretical approach that informed the development of tvopt. Then
it discusses the different components of the framework and their use for
modeling and solving time-varying optimization problems. In particular, tvopt
provides functionalities for defining both centralized and distributed online
problems, and a collection of built-in algorithms to solve them, for example
gradient-based methods, ADMM and other splitting methods. Moreover, the
framework implements prediction strategies to improve the accuracy of the
online solvers. The paper then proposes some numerical results on a benchmark
problem and discusses their implementation using tvopt. The code for tvopt is
available at https://github.com/nicola-bastianello/tvopt.
","[{'version': 'v1', 'created': 'Thu, 12 Nov 2020 16:14:09 GMT'}, {'version': 'v2', 'created': 'Wed, 8 Sep 2021 13:42:27 GMT'}]",2024-05-07,"[['Bastianello', 'Nicola', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
126,2011.08174,Davide Viviano Mr.,"Davide Viviano, Jess Rudder",Policy design in experiments with unknown interference,,,,,econ.EM cs.LG stat.ME,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper studies experimental designs for estimation and inference on
policies with spillover effects. Units are organized into a finite number of
large clusters and interact in unknown ways within each cluster. First, we
introduce a single-wave experiment that, by varying the randomization across
cluster pairs, estimates the marginal effect of a change in treatment
probabilities, taking spillover effects into account. Using the marginal
effect, we propose a test for policy optimality. Second, we design a
multiple-wave experiment to estimate welfare-maximizing treatment rules. We
provide strong theoretical guarantees and an implementation in a large-scale
field experiment.
","[{'version': 'v1', 'created': 'Mon, 16 Nov 2020 18:58:54 GMT'}, {'version': 'v2', 'created': 'Tue, 17 Nov 2020 18:57:10 GMT'}, {'version': 'v3', 'created': 'Thu, 21 Jan 2021 18:58:15 GMT'}, {'version': 'v4', 'created': 'Wed, 23 Jun 2021 17:42:04 GMT'}, {'version': 'v5', 'created': 'Tue, 16 Nov 2021 05:22:25 GMT'}, {'version': 'v6', 'created': 'Thu, 24 Feb 2022 18:52:57 GMT'}, {'version': 'v7', 'created': 'Wed, 19 Apr 2023 20:29:47 GMT'}, {'version': 'v8', 'created': 'Thu, 28 Dec 2023 11:12:04 GMT'}, {'version': 'v9', 'created': 'Fri, 3 May 2024 15:45:42 GMT'}]",2024-05-06,"[['Viviano', 'Davide', ''], ['Rudder', 'Jess', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
127,2011.08388,Puneet Kumar,Puneet Kumar and Balasubramanian Raman,"Domain Adaptation based Interpretable Image Emotion Recognition using
  Facial Expression Recognition",,,,,cs.CV cs.LG,http://creativecommons.org/licenses/by/4.0/,"  A domain adaptation technique has been proposed in this paper to identify the
emotions in generic images containing facial & non-facial objects and non-human
components. It addresses the challenge of the insufficient availability of
pre-trained models and well-annotated datasets for image emotion recognition
(IER). It starts with proposing a facial emotion recognition (FER) system and
then moves on to adapting it for image emotion recognition. First, a
deep-learning-based FER system has been proposed that classifies a given facial
image into discrete emotion classes. Further, an image recognition system has
been proposed that adapts the proposed FER system to recognize the emotions
portrayed by images using domain adaptation. It classifies the generic images
into 'happy,' 'sad,' 'hate,' and 'anger' classes. A novel interpretability
approach, Divide and Conquer based Shap (DnCShap), has also been proposed to
interpret the highly relevant visual features for emotion recognition. The
proposed system's architecture has been decided through ablation studies, and
the experiments are conducted on four FER and four IER datasets. The proposed
IER system has shown an emotion classification accuracy of 59.61% for the IAPSa
dataset, 57.83% for the ArtPhoto dataset, 67.93% for the FI dataset, and 55.13%
for the EMOTIC dataset. The important visual features leading to a particular
emotion class have been identified, and the embedding plots for various emotion
classes have been analyzed to explain the proposed system's predictions.
","[{'version': 'v1', 'created': 'Tue, 17 Nov 2020 02:55:16 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Feb 2024 13:23:08 GMT'}]",2024-02-08,"[['Kumar', 'Puneet', ''], ['Raman', 'Balasubramanian', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
128,2011.10577,Luisa Lucie-Smith,"Luisa Lucie-Smith, Hiranya V. Peiris, Andrew Pontzen, Brian Nord,
  Jeyan Thiyagalingam",Deep learning insights into cosmological structure formation,"17 pages, 10 figures. Accepted in PRD","Phys. Rev. D 109, 063524 (2024)",10.1103/PhysRevD.109.063524,,astro-ph.CO astro-ph.IM cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The evolution of linear initial conditions present in the early universe into
extended halos of dark matter at late times can be computed using cosmological
simulations. However, a theoretical understanding of this complex process
remains elusive; in particular, the role of anisotropic information in the
initial conditions in establishing the final mass of dark matter halos remains
a long-standing puzzle. Here, we build a deep learning framework to investigate
this question. We train a three-dimensional convolutional neural network (CNN)
to predict the mass of dark matter halos from the initial conditions, and
quantify in full generality the amounts of information in the isotropic and
anisotropic aspects of the initial density field about final halo masses. We
find that anisotropies add a small, albeit statistically significant amount of
information over that contained within spherical averages of the density field
about final halo mass. However, the overall scatter in the final mass
predictions does not change qualitatively with this additional information,
only decreasing from 0.9 dex to 0.7 dex. Given such a small improvement, our
results demonstrate that isotropic aspects of the initial density field
essentially saturate the relevant information about final halo mass. Therefore,
instead of searching for information directly encoded in initial conditions
anisotropies, a more promising route to accurate, fast halo mass predictions is
to add approximate dynamical information based e.g. on perturbation theory.
More broadly, our results indicate that deep learning frameworks can provide a
powerful tool for extracting physical insight into cosmological structure
formation.
","[{'version': 'v1', 'created': 'Fri, 20 Nov 2020 19:00:00 GMT'}, {'version': 'v2', 'created': 'Mon, 25 Oct 2021 09:01:21 GMT'}, {'version': 'v3', 'created': 'Fri, 1 Mar 2024 11:17:17 GMT'}]",2024-03-26,"[['Lucie-Smith', 'Luisa', ''], ['Peiris', 'Hiranya V.', ''], ['Pontzen', 'Andrew', ''], ['Nord', 'Brian', ''], ['Thiyagalingam', 'Jeyan', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
129,2011.10905,Peter Schindler,"Peter Schindler, Evan R. Antoniuk, Gowoon Cheon, Yanbing Zhu, Evan J.
  Reed","Discovery of stable surfaces with extreme work functions by
  high-throughput density functional theory and machine learning","13 pages, 5 figures",,10.1002/adfm.202401764,,cond-mat.mtrl-sci physics.comp-ph physics.data-an,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The work function is the key surface property that determines how much energy
is required for an electron to escape the surface of a material. This property
is crucial for thermionic energy conversion, band alignment in
heterostructures, and electron emission devices. Here, we present a
high-throughput workflow using density functional theory (DFT) to calculate the
work function and cleavage energy of 33,631 slabs (58,332 work functions) that
we created from 3,716 bulk materials, including up to ternary compounds. The
number of materials for which we calculated surface properties surpasses the
previously largest database, the Materials Project, by a factor of $\sim$27. On
the tail ends of the work function distribution we identify 34 and 56 surfaces
with an ultra-low (<2 eV) and ultra-high (>7 eV) work function, respectively.
Further, we discover that the $(100)$-Ba-O surface of BaMoO$_3$ and the
$(001)$-F surface of Ag$_2$F have record-low (1.25 eV) and record-high (9.06
eV) steady-state work functions without requiring coatings, respectively. Based
on this database we develop a physics-based approach to featurize surfaces and
use supervised machine learning to predict the work function. We find that
physical choice of features improves prediction performance far more than
choice of model. Our random forest model achieves a mean absolute test error of
0.09 eV, which is more than 6 times better than the baseline and comparable to
the accuracy of DFT. This surrogate model enables rapid predictions of the work
function ($\sim 10^5$ faster than DFT) across a vast chemical space and
facilitates the discovery of material surfaces with extreme work functions for
energy conversion, electronic applications, and contacts in 2-dimensional
devices.
","[{'version': 'v1', 'created': 'Sun, 22 Nov 2020 01:12:27 GMT'}, {'version': 'v2', 'created': 'Sat, 6 Jan 2024 00:54:00 GMT'}]",2024-04-08,"[['Schindler', 'Peter', ''], ['Antoniuk', 'Evan R.', ''], ['Cheon', 'Gowoon', ''], ['Zhu', 'Yanbing', ''], ['Reed', 'Evan J.', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
130,2011.11877,Ruizhe Zhang,"Baihe Huang, Zhao Song, Runzhou Tao, Junze Yin, Ruizhe Zhang, Danyang
  Zhuo",InstaHide's Sample Complexity When Mixing Two Private Images,,,,,cs.LG cs.CC cs.CR cs.DS stat.ML,http://creativecommons.org/licenses/by/4.0/,"  Training neural networks usually require large numbers of sensitive training
data, and how to protect the privacy of training data has thus become a
critical topic in deep learning research. InstaHide is a state-of-the-art
scheme to protect training data privacy with only minor effects on test
accuracy, and its security has become a salient question. In this paper, we
systematically study recent attacks on InstaHide and present a unified
framework to understand and analyze these attacks. We find that existing
attacks either do not have a provable guarantee or can only recover a single
private image. On the current InstaHide challenge setup, where each InstaHide
image is a mixture of two private images, we present a new algorithm to recover
all the private images with a provable guarantee and optimal sample complexity.
In addition, we also provide a computational hardness result on retrieving all
InstaHide images. Our results demonstrate that InstaHide is not
information-theoretically secure but computationally secure in the worst case,
even when mixing two private images.
","[{'version': 'v1', 'created': 'Tue, 24 Nov 2020 03:41:03 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Feb 2024 03:14:09 GMT'}]",2024-02-07,"[['Huang', 'Baihe', ''], ['Song', 'Zhao', ''], ['Tao', 'Runzhou', ''], ['Yin', 'Junze', ''], ['Zhang', 'Ruizhe', ''], ['Zhuo', 'Danyang', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
131,2011.12216,Shuang Li,"Shuang Li, Yilun Du, Gido M. van de Ven, Igor Mordatch",Energy-Based Models for Continual Learning,"Project page:
  https://energy-based-model.github.io/Energy-Based-Models-for-Continual-Learning","Proceedings of The 1st Conference on Lifelong Learning Agents
  (CoLLAs), 2022, pp. 1-22",,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We motivate Energy-Based Models (EBMs) as a promising model class for
continual learning problems. Instead of tackling continual learning via the use
of external memory, growing models, or regularization, EBMs change the
underlying training objective to cause less interference with previously
learned information. Our proposed version of EBMs for continual learning is
simple, efficient, and outperforms baseline methods by a large margin on
several benchmarks. Moreover, our proposed contrastive divergence-based
training objective can be combined with other continual learning methods,
resulting in substantial boosts in their performance. We further show that EBMs
are adaptable to a more general continual learning setting where the data
distribution changes without the notion of explicitly delineated tasks. These
observations point towards EBMs as a useful building block for future continual
learning methods.
","[{'version': 'v1', 'created': 'Tue, 24 Nov 2020 17:08:13 GMT'}, {'version': 'v2', 'created': 'Thu, 18 Feb 2021 05:00:33 GMT'}, {'version': 'v3', 'created': 'Sun, 18 Dec 2022 23:06:52 GMT'}]",2024-04-26,"[['Li', 'Shuang', ''], ['Du', 'Yilun', ''], ['van de Ven', 'Gido M.', ''], ['Mordatch', 'Igor', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
132,2011.13714,Aishwarya Jadhav,Aishwarya Jadhav,Detection of Malaria Vector Breeding Habitats using Topographic Models,"Presented at ML4D Workshop, NeurIPS 2020. Proceedings available:
  arXiv:2101.04347. Also presented at MLPH Workshop, NeurIPS 2020. Awarded Best
  Paper. Recording available
  https://slideslive.com/38938356/detection-of-malaria-vector-breedding-habitats-using-topographic-models",,,,cs.LG cs.CV eess.IV,http://creativecommons.org/licenses/by/4.0/,"  Treatment of stagnant water bodies that act as a breeding site for malarial
vectors is a fundamental step in most malaria elimination campaigns. However,
identification of such water bodies over large areas is expensive,
labour-intensive and time-consuming and hence, challenging in countries with
limited resources. Practical models that can efficiently locate water bodies
can target the limited resources by greatly reducing the area that needs to be
scanned by the field workers. To this end, we propose a practical topographic
model based on easily available, global, high-resolution DEM data to predict
locations of potential vector-breeding water sites. We surveyed the Obuasi
region of Ghana to assess the impact of various topographic features on
different types of water bodies and uncover the features that significantly
influence the formation of aquatic habitats. We further evaluate the
effectiveness of multiple models. Our best model significantly outperforms
earlier attempts that employ topographic variables for detection of small water
sites, even the ones that utilize additional satellite imagery data and
demonstrates robustness across different settings.
","[{'version': 'v1', 'created': 'Fri, 27 Nov 2020 12:59:55 GMT'}, {'version': 'v2', 'created': 'Tue, 16 Jul 2024 10:05:39 GMT'}]",2024-07-17,"[['Jadhav', 'Aishwarya', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
133,2011.14721,Veronica Alvarez,"Ver\'onica \'Alvarez, Santiago Mazuelas, and Jos\'e A. Lozano",Probabilistic Load Forecasting Based on Adaptive Online Learning,"\c{opyright} 2021 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works",,10.1109/TPWRS.2021.3050837,,cs.LG stat.ML,http://creativecommons.org/licenses/by/4.0/,"  Load forecasting is crucial for multiple energy management tasks such as
scheduling generation capacity, planning supply and demand, and minimizing
energy trade costs. Such relevance has increased even more in recent years due
to the integration of renewable energies, electric cars, and microgrids.
Conventional load forecasting techniques obtain single-value load forecasts by
exploiting consumption patterns of past load demand. However, such techniques
cannot assess intrinsic uncertainties in load demand, and cannot capture
dynamic changes in consumption patterns. To address these problems, this paper
presents a method for probabilistic load forecasting based on the adaptive
online learning of hidden Markov models. We propose learning and forecasting
techniques with theoretical guarantees, and experimentally assess their
performance in multiple scenarios. In particular, we develop adaptive online
learning techniques that update model parameters recursively, and sequential
prediction techniques that obtain probabilistic forecasts using the most recent
parameters. The performance of the method is evaluated using multiple datasets
corresponding with regions that have different sizes and display assorted
time-varying consumption patterns. The results show that the proposed method
can significantly improve the performance of existing techniques for a wide
range of scenarios.
","[{'version': 'v1', 'created': 'Mon, 30 Nov 2020 12:02:26 GMT'}, {'version': 'v2', 'created': 'Fri, 8 Jan 2021 15:40:39 GMT'}, {'version': 'v3', 'created': 'Fri, 15 Jan 2021 09:57:28 GMT'}, {'version': 'v4', 'created': 'Thu, 15 Aug 2024 07:37:47 GMT'}]",2024-08-16,"[['Álvarez', 'Verónica', ''], ['Mazuelas', 'Santiago', ''], ['Lozano', 'José A.', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
134,2011.14814,Gal Lifshitz,Gal Lifshitz and Dan Raviv,Cost Function Unrolling in Unsupervised Optical Flow,,"IEEE Transactions on Pattern Analysis and Machine Intelligence (
  Volume: 46, Issue: 2, February 2024)",10.1109/TPAMI.2023.3327156,,cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Steepest descent algorithms, which are commonly used in deep learning, use
the gradient as the descent direction, either as-is or after a direction shift
using preconditioning. In many scenarios calculating the gradient is
numerically hard due to complex or non-differentiable cost functions,
specifically next to singular points. In this work we focus on the derivation
of the Total Variation semi-norm commonly used in unsupervised cost functions.
Specifically, we derive a differentiable proxy to the hard L1 smoothness
constraint in a novel iterative scheme which we refer to as Cost Unrolling.
Producing more accurate gradients during training, our method enables finer
predictions of a given DNN model through improved convergence, without
modifying its architecture or increasing computational complexity. We
demonstrate our method in the unsupervised optical flow task. Replacing the L1
smoothness constraint with our unrolled cost during the training of a well
known baseline, we report improved results on both MPI Sintel and KITTI 2015
unsupervised optical flow benchmarks. Particularly, we report EPE reduced by up
to 15.82% on occluded pixels, where the smoothness constraint is dominant,
enabling the detection of much sharper motion edges.
","[{'version': 'v1', 'created': 'Mon, 30 Nov 2020 14:10:03 GMT'}, {'version': 'v2', 'created': 'Wed, 24 Nov 2021 10:09:18 GMT'}, {'version': 'v3', 'created': 'Sun, 26 May 2024 20:49:27 GMT'}]",2024-05-28,"[['Lifshitz', 'Gal', ''], ['Raviv', 'Dan', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
135,2012.00822,Haozheng Luo,"Haozheng Luo, Ruiyang Qin, Chenwei Xu, Guo Ye, and Zening Luo",Open-Ended Multi-Modal Relational Reasoning for Video Question Answering,"2023 32nd IEEE International Conference on Robot and Human
  Interactive Communication (RO-MAN)",,10.1109/RO-MAN57019.2023.10309342,,cs.AI cs.HC cs.RO,http://creativecommons.org/licenses/by/4.0/,"  In this paper, we introduce a robotic agent specifically designed to analyze
external environments and address participants' questions. The primary focus of
this agent is to assist individuals using language-based interactions within
video-based scenes. Our proposed method integrates video recognition technology
and natural language processing models within the robotic agent. We investigate
the crucial factors affecting human-robot interactions by examining pertinent
issues arising between participants and robot agents. Methodologically, our
experimental findings reveal a positive relationship between trust and
interaction efficiency. Furthermore, our model demonstrates a 2\% to 3\%
performance enhancement in comparison to other benchmark methods.
","[{'version': 'v1', 'created': 'Tue, 1 Dec 2020 20:49:59 GMT'}, {'version': 'v2', 'created': 'Mon, 7 Dec 2020 03:31:34 GMT'}, {'version': 'v3', 'created': 'Fri, 23 Jun 2023 03:23:03 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Jun 2024 06:12:52 GMT'}]",2024-06-12,"[['Luo', 'Haozheng', ''], ['Qin', 'Ruiyang', ''], ['Xu', 'Chenwei', ''], ['Ye', 'Guo', ''], ['Luo', 'Zening', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
136,2012.02134,Abiy Tasissa,"Pranay Tankala, Abiy Tasissa, James M. Murphy, Demba Ba",K-Deep Simplex: Deep Manifold Learning via Local Dictionaries,"33 pages, 17 figures. This expanded version includes detailed
  numerical experiments in the supplementary material. Theorem 3 is a new
  stability result. The sections have been reorganized, and additional details
  have been provided for clarity",,,,cs.LG cs.IT eess.SP math.IT math.OC,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  We propose K-Deep Simplex(KDS) which, given a set of data points, learns a
dictionary comprising synthetic landmarks, along with representation
coefficients supported on a simplex. KDS employs a local weighted $\ell_1$
penalty that encourages each data point to represent itself as a convex
combination of nearby landmarks. We solve the proposed optimization program
using alternating minimization and design an efficient, interpretable
autoencoder using algorithm unrolling. We theoretically analyze the proposed
program by relating the weighted $\ell_1$ penalty in KDS to a weighted $\ell_0$
program. Assuming that the data are generated from a Delaunay triangulation, we
prove the equivalence of the weighted $\ell_1$ and weighted $\ell_0$ programs.
We further show the stability of the representation coefficients under mild
geometrical assumptions. If the representation coefficients are fixed, we prove
that the sub-problem of minimizing over the dictionary yields a unique
solution. Further, we show that low-dimensional representations can be
efficiently obtained from the covariance of the coefficient matrix. Experiments
show that the algorithm is highly efficient and performs competitively on
synthetic and real data sets.
","[{'version': 'v1', 'created': 'Thu, 3 Dec 2020 18:13:26 GMT'}, {'version': 'v2', 'created': 'Thu, 25 Feb 2021 04:20:48 GMT'}, {'version': 'v3', 'created': 'Sat, 14 Jan 2023 20:17:18 GMT'}, {'version': 'v4', 'created': 'Tue, 30 Jul 2024 23:39:47 GMT'}]",2024-08-01,"[['Tankala', 'Pranay', ''], ['Tasissa', 'Abiy', ''], ['Murphy', 'James M.', ''], ['Ba', 'Demba', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
137,2012.04171,Joshua Chang,"Joshua C. Chang, Patrick Fletcher, Jungmin Han, Ted L. Chang,
  Shashaank Vattikuti, Bart Desmet, Ayah Zirikly, Carson C. Chow","Sparse encoding for more-interpretable feature-selecting representations
  in probabilistic matrix factorization",Fixed typo in Eq 2,,,ICLR 2021,cs.LG q-bio.QM stat.ML,http://creativecommons.org/publicdomain/zero/1.0/,"  Dimensionality reduction methods for count data are critical to a wide range
of applications in medical informatics and other fields where model
interpretability is paramount. For such data, hierarchical Poisson matrix
factorization (HPF) and other sparse probabilistic non-negative matrix
factorization (NMF) methods are considered to be interpretable generative
models. They consist of sparse transformations for decoding their learned
representations into predictions. However, sparsity in representation decoding
does not necessarily imply sparsity in the encoding of representations from the
original data features. HPF is often incorrectly interpreted in the literature
as if it possesses encoder sparsity. The distinction between decoder sparsity
and encoder sparsity is subtle but important. Due to the lack of encoder
sparsity, HPF does not possess the column-clustering property of classical NMF
-- the factor loading matrix does not sufficiently define how each factor is
formed from the original features. We address this deficiency by
self-consistently enforcing encoder sparsity, using a generalized additive
model (GAM), thereby allowing one to relate each representation coordinate to a
subset of the original data features. In doing so, the method also gains the
ability to perform feature selection. We demonstrate our method on simulated
data and give an example of how encoder sparsity is of practical use in a
concrete application of representing inpatient comorbidities in Medicare
patients.
","[{'version': 'v1', 'created': 'Tue, 8 Dec 2020 02:27:22 GMT'}, {'version': 'v2', 'created': 'Thu, 17 Dec 2020 18:19:54 GMT'}, {'version': 'v3', 'created': 'Tue, 29 Dec 2020 19:08:55 GMT'}]",2024-03-07,"[['Chang', 'Joshua C.', ''], ['Fletcher', 'Patrick', ''], ['Han', 'Jungmin', ''], ['Chang', 'Ted L.', ''], ['Vattikuti', 'Shashaank', ''], ['Desmet', 'Bart', ''], ['Zirikly', 'Ayah', ''], ['Chow', 'Carson C.', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
138,2012.04576,Dwight Nwaigwe,"Dwight Nwaigwe, Marek Rychlik","On the existence of the maximum likelihood estimate and convergence rate
  under gradient descent for multi-class logistic regression",We included relevant background literature on this matter,,,,cs.LG math.ST stat.TH,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We revisit the problem of the existence of the maximum likelihood estimate
for multi-class logistic regression. We show that one method of ensuring its
existence is by assigning positive probability to every class in the sample
dataset. The notion of data separability is not needed, which is in contrast to
the classical set up of multi-class logistic regression in which each data
sample belongs to one class. We also provide a general and constructive
estimate of the convergence rate to the maximum likelihood estimate when
gradient descent is used as the optimizer. Our estimate involves bounding the
condition number of the Hessian of the maximum likelihood function. The
approaches used in this article rely on a simple operator-theoretic framework.
","[{'version': 'v1', 'created': 'Tue, 8 Dec 2020 17:21:34 GMT'}, {'version': 'v2', 'created': 'Sun, 10 Jan 2021 16:38:37 GMT'}, {'version': 'v3', 'created': 'Mon, 15 Mar 2021 04:32:19 GMT'}, {'version': 'v4', 'created': 'Fri, 5 Nov 2021 00:53:53 GMT'}, {'version': 'v5', 'created': 'Wed, 8 May 2024 05:31:36 GMT'}]",2024-05-09,"[['Nwaigwe', 'Dwight', ''], ['Rychlik', 'Marek', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
139,2012.0764,Rifkat Davronov Rifkat,Rifkat Davronova and Fatima Adilovab,A Comparative Analysis of the Ensemble Methods for Drug Design,"11 pages, 2 figures, 7 tables",,10.1063/5.0057487,,cs.LG cs.IT math.IT q-bio.QM,http://creativecommons.org/licenses/by/4.0/,"  Quantitative structure-activity relationship (QSAR) is a computer modeling
technique for identifying relationships between the structural properties of
chemical compounds and biological activity. QSAR modeling is necessary for drug
discovery, but it has many limitations. Ensemble-based machine learning
approaches have been used to overcome limitations and generate reliable
predictions. Ensemble learning creates a set of diverse models and combines
them. In our comparative analysis, each ensemble algorithm was paired with each
of the basic algorithms, but the basic algorithms were also investigated
separately. In this configuration, 57 algorithms were developed and compared on
4 different datasets. Thus, a technique for complex ensemble method is proposed
that builds diversified models and integrates them. The proposed individual
models did not show impressive results as a unified model, but it was
considered the most important predictor when combined. We assessed whether
ensembles always give better results than individual algorithms. The Python
code written to get experimental results in this article has been uploaded to
Github (https://github.com/rifqat/Comparative-Analysis).
","[{'version': 'v1', 'created': 'Fri, 11 Dec 2020 05:27:20 GMT'}]",2024-06-19,"[['Davronova', 'Rifkat', ''], ['Adilovab', 'Fatima', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
140,2012.07691,Thiago B. Burghi,"Thiago B. Burghi, Maarten Schoukens, Rodolphe Sepulchre",System identification of biophysical neuronal models,"Slightly extended pre-print of the paper to be presented at the 59th
  Conference on Decision and Control, held remotely between December 14-18,
  2020","Proceedings of the 2020 59th IEEE Conference on Decision and
  Control (CDC), Jeju, South Korea",10.1109/CDC42340.2020.9304363,,q-bio.NC cs.LG cs.SY eess.SY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  After sixty years of quantitative biophysical modeling of neurons, the
identification of neuronal dynamics from input-output data remains a
challenging problem, primarily due to the inherently nonlinear nature of
excitable behaviors. By reformulating the problem in terms of the
identification of an operator with fading memory, we explore a simple approach
based on a parametrization given by a series interconnection of Generalized
Orthonormal Basis Functions (GOBFs) and static Artificial Neural Networks. We
show that GOBFs are particularly well-suited to tackle the identification
problem, and provide a heuristic for selecting GOBF poles which addresses the
ultra-sensitivity of neuronal behaviors. The method is illustrated on the
identification of a bursting model from the crab stomatogastric ganglion.
","[{'version': 'v1', 'created': 'Mon, 14 Dec 2020 16:41:27 GMT'}]",2024-02-29,"[['Burghi', 'Thiago B.', ''], ['Schoukens', 'Maarten', ''], ['Sepulchre', 'Rodolphe', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
141,2012.08545,Eliu Huerta,"E. A. Huerta, Asad Khan, Xiaobo Huang, Minyang Tian, Maksim Levental,
  Ryan Chard, Wei Wei, Maeve Heflin, Daniel S. Katz, Volodymyr Kindratenko,
  Dawei Mu, Ben Blaiszik and Ian Foster","Accelerated, Scalable and Reproducible AI-driven Gravitational Wave
  Detection","17 pages, 5 figures; v2: 12 pages, 6 figures. Accepted to Nature
  Astronomy. See also the Behind the Paper blog in Nature Astronomy
  ""https://astronomycommunity.nature.com/posts/from-disruption-to-sustained-innovation-artificial-intelligence-for-gravitational-wave-astrophysics""","Nat Astron 5, 1062-1068 (2021)",10.1038/s41550-021-01405-0,,gr-qc astro-ph.IM cs.AI cs.DC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The development of reusable artificial intelligence (AI) models for wider use
and rigorous validation by the community promises to unlock new opportunities
in multi-messenger astrophysics. Here we develop a workflow that connects the
Data and Learning Hub for Science, a repository for publishing AI models, with
the Hardware Accelerated Learning (HAL) cluster, using funcX as a universal
distributed computing service. Using this workflow, an ensemble of four openly
available AI models can be run on HAL to process an entire month's worth
(August 2017) of advanced Laser Interferometer Gravitational-Wave Observatory
data in just seven minutes, identifying all four all four binary black hole
mergers previously identified in this dataset and reporting no
misclassifications. This approach combines advances in AI, distributed
computing, and scientific data infrastructure to open new pathways to conduct
reproducible, accelerated, data-driven discovery.
","[{'version': 'v1', 'created': 'Tue, 15 Dec 2020 19:00:29 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Jul 2021 21:44:56 GMT'}]",2024-06-25,"[['Huerta', 'E. A.', ''], ['Khan', 'Asad', ''], ['Huang', 'Xiaobo', ''], ['Tian', 'Minyang', ''], ['Levental', 'Maksim', ''], ['Chard', 'Ryan', ''], ['Wei', 'Wei', ''], ['Heflin', 'Maeve', ''], ['Katz', 'Daniel S.', ''], ['Kindratenko', 'Volodymyr', ''], ['Mu', 'Dawei', ''], ['Blaiszik', 'Ben', ''], ['Foster', 'Ian', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
142,2012.11554,Yufeng Zhang,"Zhuoran Yang, Yufeng Zhang, Yongxin Chen, Zhaoran Wang","Variational Transport: A Convergent Particle-BasedAlgorithm for
  Distributional Optimization","58 pages, add acknowledgement",,,,cs.LG math.OC math.ST stat.ML stat.TH,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We consider the optimization problem of minimizing a functional defined over
a family of probability distributions, where the objective functional is
assumed to possess a variational form. Such a distributional optimization
problem arises widely in machine learning and statistics, with Monte-Carlo
sampling, variational inference, policy optimization, and generative
adversarial network as examples. For this problem, we propose a novel
particle-based algorithm, dubbed as variational transport, which approximately
performs Wasserstein gradient descent over the manifold of probability
distributions via iteratively pushing a set of particles. Specifically, we
prove that moving along the geodesic in the direction of functional gradient
with respect to the second-order Wasserstein distance is equivalent to applying
a pushforward mapping to a probability distribution, which can be approximated
accurately by pushing a set of particles. Specifically, in each iteration of
variational transport, we first solve the variational problem associated with
the objective functional using the particles, whose solution yields the
Wasserstein gradient direction. Then we update the current distribution by
pushing each particle along the direction specified by such a solution. By
characterizing both the statistical error incurred in estimating the
Wasserstein gradient and the progress of the optimization algorithm, we prove
that when the objective function satisfies a functional version of the
Polyak-\L{}ojasiewicz (PL) (Polyak, 1963) and smoothness conditions,
variational transport converges linearly to the global minimum of the objective
functional up to a certain statistical error, which decays to zero sublinearly
as the number of particles goes to infinity.
","[{'version': 'v1', 'created': 'Mon, 21 Dec 2020 18:33:13 GMT'}, {'version': 'v2', 'created': 'Mon, 1 Apr 2024 03:56:23 GMT'}]",2024-04-02,"[['Yang', 'Zhuoran', ''], ['Zhang', 'Yufeng', ''], ['Chen', 'Yongxin', ''], ['Wang', 'Zhaoran', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
143,2101.00203,Anish Madan,"Anish Madan, Ranjitha Prasad","B-SMALL: A Bayesian Neural Network approach to Sparse Model-Agnostic
  Meta-Learning",,,,,cs.LG cs.AI cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
","[{'version': 'v1', 'created': 'Fri, 1 Jan 2021 09:19:48 GMT'}]",2024-03-13,"[['Madan', 'Anish', ''], ['Prasad', 'Ranjitha', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
144,2101.02404,Mitchell Krock,"Mitchell Krock, William Kleiber, Dorit Hammerling, and Stephen Becker","Modeling massive highly-multivariate nonstationary spatial data with the
  basis graphical lasso",,"Journal of Computational and Graphical Statistics, 32(4),
  1472-1487 (2023)",10.1080/10618600.2023.2174126,,stat.ME cs.LG stat.ML,http://creativecommons.org/licenses/by/4.0/,"  We propose a new modeling framework for highly-multivariate spatial processes
that synthesizes ideas from recent multiscale and spectral approaches with
graphical models. The basis graphical lasso writes a univariate Gaussian
process as a linear combination of basis functions weighted with entries of a
Gaussian graphical vector whose graph is estimated from optimizing an $\ell_1$
penalized likelihood. This paper extends the setting to a multivariate Gaussian
process where the basis functions are weighted with Gaussian graphical vectors.
We motivate a model where the basis functions represent different levels of
resolution and the graphical vectors for each level are assumed to be
independent. Using an orthogonal basis grants linear complexity and memory
usage in the number of spatial locations, the number of basis functions, and
the number of realizations. An additional fusion penalty encourages a
parsimonious conditional independence structure in the multilevel graphical
model. We illustrate our method on a large climate ensemble from the National
Center for Atmospheric Research's Community Atmosphere Model that involves 40
spatial processes.
","[{'version': 'v1', 'created': 'Thu, 7 Jan 2021 07:01:54 GMT'}, {'version': 'v2', 'created': 'Wed, 9 Jun 2021 05:15:53 GMT'}]",2024-07-08,"[['Krock', 'Mitchell', ''], ['Kleiber', 'William', ''], ['Hammerling', 'Dorit', ''], ['Becker', 'Stephen', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
145,2101.04645,Jan-Philipp Schulze,"J.-P. Schulze, P. Sperl, K. B\""ottinger","Double-Adversarial Activation Anomaly Detection: Adversarial
  Autoencoders are Anomaly Generators",Accepted at IJCNN 2022,,10.1109/IJCNN55064.2022.9892896,,cs.LG cs.CR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
","[{'version': 'v1', 'created': 'Tue, 12 Jan 2021 18:07:34 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Apr 2021 17:05:36 GMT'}, {'version': 'v3', 'created': 'Sun, 6 Mar 2022 13:14:07 GMT'}, {'version': 'v4', 'created': 'Mon, 23 May 2022 16:59:49 GMT'}, {'version': 'v5', 'created': 'Sun, 14 Jan 2024 17:28:57 GMT'}]",2024-01-17,"[['Schulze', 'J. -P.', ''], ['Sperl', 'P.', ''], ['Böttinger', 'K.', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
146,2101.05129,Anirban Chaudhuri,"Anirban Chaudhuri, Boris Kramer, Matthew Norton, Johannes O. Royset,
  Karen Willcox",Certifiable Risk-Based Engineering Design Optimization,,"AIAA Journal, 60(2), pp.551-565, 2022",10.2514/1.J060539,,math.OC cs.CE physics.data-an stat.CO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Reliable, risk-averse design of complex engineering systems with optimized
performance requires dealing with uncertainties. A conventional approach is to
add safety margins to a design that was obtained from deterministic
optimization. Safer engineering designs require appropriate cost and constraint
function definitions that capture the \textit{risk} associated with unwanted
system behavior in the presence of uncertainties. The paper proposes two
notions of certifiability. The first is based on accounting for the magnitude
of failure to ensure data-informed conservativeness. The second is the ability
to provide optimization convergence guarantees by preserving convexity.
Satisfying these notions leads to \textit{certifiable} risk-based design
optimization (CRiBDO). In the context of CRiBDO, risk measures based on
superquantile (a.k.a.\ conditional value-at-risk) and buffered probability of
failure are analyzed. CRiBDO is contrasted with reliability-based design
optimization (RBDO), where uncertainties are accounted for via the probability
of failure, through a structural and a thermal design problem. A reformulation
of the short column structural design problem leading to a convex CRiBDO
problem is presented. The CRiBDO formulations capture more information about
the problem to assign the appropriate conservativeness, exhibit superior
optimization convergence by preserving properties of underlying functions, and
alleviate the adverse effects of choosing hard failure thresholds required in
RBDO.
","[{'version': 'v1', 'created': 'Wed, 13 Jan 2021 15:23:15 GMT'}, {'version': 'v2', 'created': 'Tue, 13 Jul 2021 18:30:17 GMT'}]",2024-06-18,"[['Chaudhuri', 'Anirban', ''], ['Kramer', 'Boris', ''], ['Norton', 'Matthew', ''], ['Royset', 'Johannes O.', ''], ['Willcox', 'Karen', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
147,2101.127,Susan Stepney,"Matthew Dale, David Griffin, Richard F. L. Evans, Sarah Jenkins, Simon
  O'Keefe, Angelika Sebald, Susan Stepney, Fernando Torre, Martin Trefzer",Reservoir Computing with Magnetic Thin Films,"30 pages, 10 figures, updated and clarified","International Journal of Unconventional Computing, 19(1):63-92,
  2024",,,cs.ET cond-mat.mtrl-sci cs.AR cs.LG cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Advances in artificial intelligence are driven by technologies inspired by
the brain, but these technologies are orders of magnitude less powerful and
energy efficient than biological systems. Inspired by the nonlinear dynamics of
neural networks, new unconventional computing hardware has emerged with the
potential to exploit natural phenomena and gain efficiency, in a similar manner
to biological systems. Physical reservoir computing demonstrates this with a
variety of unconventional systems, from optical-based to memristive systems.
Reservoir computers provide a nonlinear projection of the task input into a
high-dimensional feature space by exploiting the system's internal dynamics. A
trained readout layer then combines features to perform tasks, such as pattern
recognition and time-series analysis. Despite progress, achieving
state-of-the-art performance without external signal processing to the
reservoir remains challenging. Here we perform an initial exploration of three
magnetic materials in thin-film geometries via microscale simulation. Our
results reveal that basic spin properties of magnetic films generate the
required nonlinear dynamics and memory to solve machine learning tasks
(although there would be practical challenges in exploiting these particular
materials in physical implementations). The method of exploration can be
applied to other materials, so this work opens up the possibility of testing
different materials, from relatively simple (alloys) to significantly complex
(antiferromagnetic reservoirs).
","[{'version': 'v1', 'created': 'Fri, 29 Jan 2021 17:37:17 GMT'}, {'version': 'v2', 'created': 'Mon, 30 Oct 2023 15:23:42 GMT'}]",2024-03-05,"[['Dale', 'Matthew', ''], ['Griffin', 'David', ''], ['Evans', 'Richard F. L.', ''], ['Jenkins', 'Sarah', ''], [""O'Keefe"", 'Simon', ''], ['Sebald', 'Angelika', ''], ['Stepney', 'Susan', ''], ['Torre', 'Fernando', ''], ['Trefzer', 'Martin', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
148,2102.01466,Anthony Devaux,"Anthony Devaux (BPH), Robin Genuer (BPH, SISTM), Karine P\'er\`es
  (BPH), C\'ecile Proust-Lima (BPH)","Individual dynamic prediction of clinical endpoint from large
  dimensional longitudinal biomarker history: a landmark approach",,,10.1186/s12874-022-01660-3,,stat.ML cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
","[{'version': 'v1', 'created': 'Tue, 2 Feb 2021 12:36:18 GMT'}, {'version': 'v2', 'created': 'Fri, 21 Jan 2022 08:59:58 GMT'}]",2024-07-17,"[['Devaux', 'Anthony', '', 'BPH'], ['Genuer', 'Robin', '', 'BPH, SISTM'], ['Pérès', 'Karine', '', 'BPH'], ['Proust-Lima', 'Cécile', '', 'BPH']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
149,2102.01538,Yuanpeng He,"Yuanpeng He, Lijian Li, Tianxiang Zhan","A Matrix-based Distance of Pythagorean Fuzzy Set and its Application in
  Medical Diagnosis",31 pages,,,,cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The pythagorean fuzzy set (PFS) which is developed based on intuitionistic
fuzzy set, is more efficient in elaborating and disposing uncertainties in
indeterminate situations, which is a very reason of that PFS is applied in
various kinds of fields. How to measure the distance between two pythagorean
fuzzy sets is still an open issue. Mnay kinds of methods have been proposed to
present the of the question in former reaserches. However, not all of existing
methods can accurately manifest differences among pythagorean fuzzy sets and
satisfy the property of similarity. And some other kinds of methods neglect the
relationship among three variables of pythagorean fuzzy set. To addrees the
proplem, a new method of measuring distance is proposed which meets the
requirements of axiom of distance measurement and is able to indicate the
degree of distinction of PFSs well. Then some numerical examples are offered to
to verify that the method of measuring distances can avoid the situation that
some counter? intuitive and irrational results are produced and is more
effective, reasonable and advanced than other similar methods. Besides, the
proposed method of measuring distances between PFSs is applied in a real
environment of application which is the medical diagnosis and is compared with
other previous methods to demonstrate its superiority and efficiency. And the
feasibility of the proposed method in handling uncertainties in practice is
also proved at the same time.
","[{'version': 'v1', 'created': 'Sun, 31 Jan 2021 15:59:09 GMT'}, {'version': 'v2', 'created': 'Thu, 23 May 2024 12:59:12 GMT'}]",2024-05-24,"[['He', 'Yuanpeng', ''], ['Li', 'Lijian', ''], ['Zhan', 'Tianxiang', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
150,2102.0178,Daniel Severin Dr.,"Mauro Lucci, Daniel Sever\'in, Paula Zabala","A metaheuristic for crew scheduling in a pickup-and-delivery problem
  with time windows",,"Intl. Trans. in Op. Res., vol. 30, 2023, pp. 970-1001",10.1111/itor.13096,,cs.AI cs.DM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A vehicle routing and crew scheduling problem (VRCSP) consists of
simultaneously planning the routes of a fleet of vehicles and scheduling the
crews, where the vehicle-crew correspondence is not fixed through time. This
allows a greater planning flexibility and a more efficient use of the fleet,
but in counterpart, a high synchronisation is demanded. In this work, we
present a VRCSP where pickup-and-delivery requests with time windows have to be
fulfilled over a given planning horizon by using trucks and drivers. Crews can
be composed of 1 or 2 drivers and any of them can be relieved in a given set of
locations. Moreover, they are allowed to travel among locations with
non-company shuttles, at an additional cost that is minimised. As our problem
considers distinct routes for trucks and drivers, we have an additional
flexibility not contemplated in other previous VRCSP given in the literature
where a crew is handled as an indivisible unit. We tackle this problem with a
two-stage sequential approach: a set of truck routes is computed in the first
stage and a set of driver routes consistent with the truck routes is obtained
in the second one. We design and evaluate the performance of a metaheuristic
based algorithm for the latter stage. Our algorithm is mainly a GRASP with a
perturbation procedure that allows reusing solutions already found in case the
search for new solutions becomes difficult. This procedure together with other
to repair infeasible solutions allow us to find high-quality solutions on
instances of 100 requests spread across 15 cities with a fleet of 12-32 trucks
(depending on the planning horizon) in less than an hour. We also conclude that
the possibility of carrying an additional driver leads to a decrease of the
cost of external shuttles by about 60% on average with respect to individual
crews and, in some cases, to remove this cost completely.
","[{'version': 'v1', 'created': 'Tue, 2 Feb 2021 22:14:10 GMT'}]",2024-07-11,"[['Lucci', 'Mauro', ''], ['Severín', 'Daniel', ''], ['Zabala', 'Paula', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
151,2102.04363,Bart Paul Gerard Van Parys,Bart P.G. Van Parys,Efficient Data-Driven Optimization with Noisy Data,,,10.1016/j.orl.2024.107089,,math.OC cs.LG math.ST stat.TH,http://creativecommons.org/licenses/by/4.0/,"  Classical Kullback-Leibler or entropic distances are known to enjoy certain
desirable statistical properties in the context of decision-making with
noiseless data. However, in most practical situations the data available to a
decision maker is subject to a certain amount of measurement noise. We hence
study here data-driven prescription problems in which the data is corrupted by
a known noise source. We derive efficient data-driven formulations in this
noisy regime and indicate that they enjoy an entropic optimal transport
interpretation. Finally, we show that these efficient robust formulations are
tractable in several interesting settings by exploiting a classical
representation result by Strassen.
","[{'version': 'v1', 'created': 'Mon, 8 Feb 2021 17:12:30 GMT'}, {'version': 'v2', 'created': 'Fri, 11 Mar 2022 00:41:02 GMT'}, {'version': 'v3', 'created': 'Mon, 10 Jul 2023 15:08:31 GMT'}, {'version': 'v4', 'created': 'Fri, 23 Feb 2024 10:47:23 GMT'}]",2024-02-26,"[['Van Parys', 'Bart P. G.', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
152,2102.05542,Antoine Houdard,"Antoine Houdard and Arthur Leclaire and Nicolas Papadakis and Julien
  Rabin","On the Existence of Optimal Transport Gradient for Learning Generative
  Models",,Transactions on Machine Learning Research (2023),,,stat.ML cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The use of optimal transport cost for learning generative models has become
popular with Wasserstein Generative Adversarial Networks (WGAN). Training of
WGAN relies on a theoretical background: the calculation of the gradient of the
optimal transport cost with respect to the generative model parameters. We
first demonstrate that such gradient may not be defined, which can result in
numerical instabilities during gradient-based optimization. We address this
issue by stating a valid differentiation theorem in the case of entropic
regularized transport and specify conditions under which existence is ensured.
By exploiting the discrete nature of empirical data, we formulate the gradient
in a semi-discrete setting and propose an algorithm for the optimization of the
generative model parameters. Finally, we illustrate numerically the advantage
of the proposed framework.
","[{'version': 'v1', 'created': 'Wed, 10 Feb 2021 16:28:20 GMT'}]",2024-04-04,"[['Houdard', 'Antoine', ''], ['Leclaire', 'Arthur', ''], ['Papadakis', 'Nicolas', ''], ['Rabin', 'Julien', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
153,2102.06701,Yasaman Bahri,"Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, Utkarsh Sharma",Explaining Neural Scaling Laws,"11 pages, 3 figures + Supplement (expanded). This version to appear
  in PNAS",PNAS 121 (27) e2311878121 (2024),10.1073/pnas.2311878121,,cs.LG cond-mat.dis-nn stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The population loss of trained deep neural networks often follows precise
power-law scaling relations with either the size of the training dataset or the
number of parameters in the network. We propose a theory that explains the
origins of and connects these scaling laws. We identify variance-limited and
resolution-limited scaling behavior for both dataset and model size, for a
total of four scaling regimes. The variance-limited scaling follows simply from
the existence of a well-behaved infinite data or infinite width limit, while
the resolution-limited regime can be explained by positing that models are
effectively resolving a smooth data manifold. In the large width limit, this
can be equivalently obtained from the spectrum of certain kernels, and we
present evidence that large width and large dataset resolution-limited scaling
exponents are related by a duality. We exhibit all four scaling regimes in the
controlled setting of large random feature and pretrained models and test the
predictions empirically on a range of standard architectures and datasets. We
also observe several empirical relationships between datasets and scaling
exponents under modifications of task and architecture aspect ratio. Our work
provides a taxonomy for classifying different scaling regimes, underscores that
there can be different mechanisms driving improvements in loss, and lends
insight into the microscopic origins of and relationships between scaling
exponents.
","[{'version': 'v1', 'created': 'Fri, 12 Feb 2021 18:57:46 GMT'}, {'version': 'v2', 'created': 'Mon, 29 Apr 2024 00:55:09 GMT'}]",2024-06-28,"[['Bahri', 'Yasaman', ''], ['Dyer', 'Ethan', ''], ['Kaplan', 'Jared', ''], ['Lee', 'Jaehoon', ''], ['Sharma', 'Utkarsh', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
154,2102.0903,Ngoc Toan Nguyen,"Marten van Dijk, Nhuong V. Nguyen, Toan N. Nguyen, Lam M. Nguyen and
  Phuong Ha Nguyen",Proactive DP: A Multple Target Optimization Framework for DP-SGD,"arXiv admin note: text overlap with arXiv:2007.09208, changes in
  contents and title",,,,cs.LG math.OC stat.ML,http://creativecommons.org/licenses/by/4.0/,"  We introduce a multiple target optimization framework for DP-SGD referred to
as pro-active DP. In contrast to traditional DP accountants, which are used to
track the expenditure of privacy budgets, the pro-active DP scheme allows one
to a-priori select parameters of DP-SGD based on a fixed privacy budget (in
terms of $\epsilon$ and $\delta$) in such a way to optimize the anticipated
utility (test accuracy) the most. To achieve this objective, we first propose
significant improvements to the moment account method, presenting a closed-form
$(\epsilon,\delta)$-DP guarantee that connects all parameters in the DP-SGD
setup. We show that DP-SGD is $(\epsilon<0.5,\delta=1/N)$-DP if
$\sigma=\sqrt{2(\epsilon +\ln(1/\delta))/\epsilon}$ with $T$ at least $\approx
2k^2/\epsilon$ and $(2/e)^2k^2-1/2\geq \ln(N)$, where $T$ is the total number
of rounds, and $K=kN$ is the total number of gradient computations where $k$
measures $K$ in number of epochs of size $N$ of the local data set. We prove
that our expression is close to tight in that if $T$ is more than a constant
factor $\approx 4$ smaller than the lower bound $\approx 2k^2/\epsilon$, then
the $(\epsilon,\delta)$-DP guarantee is violated. The above DP guarantee can be
enhanced in thatDP-SGD is $(\epsilon, \delta)$-DP if $\sigma =
\sqrt{2(\epsilon+\ln(1/\delta))/\epsilon}$ with $T$ at least $\approx
2k^2/\epsilon$ together with two additional, less intuitive, conditions that
allow larger $\epsilon\geq 0.5$. Our DP theory allows us to create a utility
graph and DP calculator. These tools link privacy and utility objectives and
search for optimal experiment setups, efficiently taking into account both
accuracy and privacy objectives, as well as implementation goals. We furnish a
comprehensive implementation flow of our proactive DP, with rigorous
experiments to showcase the proof-of-concept.
","[{'version': 'v1', 'created': 'Wed, 17 Feb 2021 21:19:39 GMT'}, {'version': 'v10', 'created': 'Tue, 4 Jun 2024 07:41:47 GMT'}, {'version': 'v2', 'created': 'Mon, 14 Jun 2021 21:05:08 GMT'}, {'version': 'v3', 'created': 'Tue, 19 Oct 2021 23:07:08 GMT'}, {'version': 'v4', 'created': 'Tue, 1 Feb 2022 00:27:02 GMT'}, {'version': 'v5', 'created': 'Fri, 6 Jan 2023 17:40:58 GMT'}, {'version': 'v6', 'created': 'Fri, 13 Jan 2023 08:13:13 GMT'}, {'version': 'v7', 'created': 'Sun, 5 Mar 2023 18:44:52 GMT'}, {'version': 'v8', 'created': 'Tue, 30 May 2023 04:19:24 GMT'}, {'version': 'v9', 'created': 'Fri, 24 Nov 2023 11:25:40 GMT'}]",2024-06-05,"[['van Dijk', 'Marten', ''], ['Nguyen', 'Nhuong V.', ''], ['Nguyen', 'Toan N.', ''], ['Nguyen', 'Lam M.', ''], ['Nguyen', 'Phuong Ha', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
155,2102.09111,Dan Li,"Dan Li, Dariush Fooladivanda, Sonia Martinez","Online Optimization and Ambiguity-based Learning of Distributionally
  Uncertain Dynamic Systems",,IEEE Transactions on Automatic Control 2024,10.1109/TAC.2024.3396378,,eess.SY cs.LG cs.SY math.DS math.OC stat.AP stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper proposes a novel approach to construct data-driven online
solutions to optimization problems (P) subject to a class of distributionally
uncertain dynamical systems. The introduced framework allows for the
simultaneous learning of distributional system uncertainty via a parameterized,
control-dependent ambiguity set using a finite historical data set, and its use
to make online decisions with probabilistic regret function bounds. Leveraging
the merits of Machine Learning, the main technical approach relies on the
theory of Distributional Robust Optimization (DRO), to hedge against
uncertainty and provide less conservative results than standard Robust
Optimization approaches. Starting from recent results that describe ambiguity
sets via parameterized, and control-dependent empirical distributions as well
as ambiguity radii, we first present a tractable reformulation of the
corresponding optimization problem while maintaining the probabilistic
guarantees. We then specialize these problems to the cases of 1) optimal
one-stage control of distributionally uncertain nonlinear systems, and 2)
resource allocation under distributional uncertainty. A novelty of this work is
that it extends DRO to online optimization problems subject to a
distributionally uncertain dynamical system constraint, handled via a
control-dependent ambiguity set that leads to online-tractable optimization
with probabilistic guarantees on regret bounds. Further, we introduce an online
version of Nesterov's accelerated-gradient algorithm, and analyze its
performance to solve this class of problems via dissipativity theory.
","[{'version': 'v1', 'created': 'Thu, 18 Feb 2021 01:49:06 GMT'}, {'version': 'v2', 'created': 'Sun, 21 Jul 2024 18:11:23 GMT'}]",2024-07-23,"[['Li', 'Dan', ''], ['Fooladivanda', 'Dariush', ''], ['Martinez', 'Sonia', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
156,2102.09407,Quentin Delfosse,"Quentin Delfosse, Patrick Schramowski, Martin Mundt, Alejandro Molina
  and Kristian Kersting",Adaptive Rational Activations to Boost Deep Reinforcement Learning,"Main paper: 9 pages, References: 4 pages, Appendix: 11 pages. Main
  paper: 5 figures, Appendix: 6 figures, 6 tables. Rational Activation
  Functions repository: https://github.com/k4ntz/activation-functions Rational
  Reinforcement Learning: https://github.com/ml-research/rational_rl",,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Latest insights from biology show that intelligence not only emerges from the
connections between neurons but that individual neurons shoulder more
computational responsibility than previously anticipated. This perspective
should be critical in the context of constantly changing distinct reinforcement
learning environments, yet current approaches still primarily employ static
activation functions. In this work, we motivate why rationals are suitable for
adaptable activation functions and why their inclusion into neural networks is
crucial. Inspired by recurrence in residual networks, we derive a condition
under which rational units are closed under residual connections and formulate
a naturally regularised version: the recurrent-rational. We demonstrate that
equipping popular algorithms with (recurrent-)rational activations leads to
consistent improvements on Atari games, especially turning simple DQN into a
solid approach, competitive to DDQN and Rainbow.
","[{'version': 'v1', 'created': 'Thu, 18 Feb 2021 14:53:12 GMT'}, {'version': 'v2', 'created': 'Thu, 4 Nov 2021 14:05:07 GMT'}, {'version': 'v3', 'created': 'Sat, 29 Jan 2022 19:42:25 GMT'}, {'version': 'v4', 'created': 'Mon, 4 Mar 2024 15:22:32 GMT'}, {'version': 'v5', 'created': 'Sat, 16 Mar 2024 12:40:45 GMT'}]",2024-03-19,"[['Delfosse', 'Quentin', ''], ['Schramowski', 'Patrick', ''], ['Mundt', 'Martin', ''], ['Molina', 'Alejandro', ''], ['Kersting', 'Kristian', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
157,2102.12974,Nicola Rares Franco,"Michela C. Massi, Nicola R. Franco, Francesca Ieva, Andrea Manzoni,
  Anna Maria Paganoni, Paolo Zunino",Learning High-Order Interactions via Targeted Pattern Search,,,10.1371/journal.pone.0281618,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Logistic Regression (LR) is a widely used statistical method in empirical
binary classification studies. However, real-life scenarios oftentimes share
complexities that prevent from the use of the as-is LR model, and instead
highlight the need to include high-order interactions to capture data
variability. This becomes even more challenging because of: (i) datasets
growing wider, with more and more variables; (ii) studies being typically
conducted in strongly imbalanced settings; (iii) samples going from very large
to extremely small; (iv) the need of providing both predictive models and
interpretable results. In this paper we present a novel algorithm, Learning
high-order Interactions via targeted Pattern Search (LIPS), to select
interaction terms of varying order to include in a LR model for an imbalanced
binary classification task when input data are categorical. LIPS's rationale
stems from the duality between item sets and categorical interactions. The
algorithm relies on an interaction learning step based on a well-known frequent
item set mining algorithm, and a novel dissimilarity-based interaction
selection step that allows the user to specify the number of interactions to be
included in the LR model. In addition, we particularize two variants (Scores
LIPS and Clusters LIPS), that can address even more specific needs. Through a
set of experiments we validate our algorithm and prove its wide applicability
to real-life research scenarios, showing that it outperforms a benchmark
state-of-the-art algorithm.
","[{'version': 'v1', 'created': 'Tue, 23 Feb 2021 11:13:22 GMT'}]",2024-05-15,"[['Massi', 'Michela C.', ''], ['Franco', 'Nicola R.', ''], ['Ieva', 'Francesca', ''], ['Manzoni', 'Andrea', ''], ['Paganoni', 'Anna Maria', ''], ['Zunino', 'Paolo', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
158,2102.13273,Joaquim Dias Garcia,"Joaquim Dias Garcia, Alexandre Street, Tito Homem-de-Mello and
  Francisco D. Mu\~noz","Application-Driven Learning: A Closed-Loop Prediction and Optimization
  Approach Applied to Dynamic Reserves and Demand Forecasting",,,,,math.OC cs.LG cs.SY eess.SY stat.ME stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Forecasting and decision-making are generally modeled as two sequential steps
with no feedback, following an open-loop approach. In this paper, we present
application-driven learning, a new closed-loop framework in which the processes
of forecasting and decision-making are merged and co-optimized through a
bilevel optimization problem. We present our methodology in a general format
and prove that the solution converges to the best estimator in terms of the
expected cost of the selected application. Then, we propose two solution
methods: an exact method based on the KKT conditions of the second-level
problem and a scalable heuristic approach suitable for decomposition methods.
The proposed methodology is applied to the relevant problem of defining dynamic
reserve requirements and conditional load forecasts, offering an alternative
approach to current ad hoc procedures implemented in industry practices. We
benchmark our methodology with the standard sequential least-squares forecast
and dispatch planning process. We apply the proposed methodology to an
illustrative system and to a wide range of instances, from dozens of buses to
large-scale realistic systems with thousands of buses. Our results show that
the proposed methodology is scalable and yields consistently better performance
than the standard open-loop approach.
","[{'version': 'v1', 'created': 'Fri, 26 Feb 2021 02:43:28 GMT'}, {'version': 'v2', 'created': 'Thu, 11 Mar 2021 17:59:38 GMT'}, {'version': 'v3', 'created': 'Mon, 14 Jun 2021 13:39:52 GMT'}, {'version': 'v4', 'created': 'Thu, 10 Nov 2022 02:29:27 GMT'}, {'version': 'v5', 'created': 'Mon, 8 Apr 2024 05:37:01 GMT'}]",2024-04-09,"[['Garcia', 'Joaquim Dias', ''], ['Street', 'Alexandre', ''], ['Homem-de-Mello', 'Tito', ''], ['Muñoz', 'Francisco D.', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
159,2103.00686,Divya Mahajan,"Muhammad Adnan, Yassaman Ebrahimzadeh Maboud, Divya Mahajan, Prashant
  J. Nair","Accelerating Recommendation System Training by Leveraging Popular
  Choices",,"Proceedings of the VLDB Endowment, 2022",10.14778/3485450.3485462,,cs.IR cs.AI cs.AR cs.LG,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Recommender models are commonly used to suggest relevant items to a user for
e-commerce and online advertisement-based applications. These models use
massive embedding tables to store numerical representation of items' and users'
categorical variables (memory intensive) and employ neural networks (compute
intensive) to generate final recommendations. Training these large-scale
recommendation models is evolving to require increasing data and compute
resources. The highly parallel neural networks portion of these models can
benefit from GPU acceleration however, large embedding tables often cannot fit
in the limited-capacity GPU device memory. Hence, this paper deep dives into
the semantics of training data and obtains insights about the feature access,
transfer, and usage patterns of these models. We observe that, due to the
popularity of certain inputs, the accesses to the embeddings are highly skewed
with a few embedding entries being accessed up to 10000x more. This paper
leverages this asymmetrical access pattern to offer a framework, called FAE,
and proposes a hot-embedding aware data layout for training recommender models.
This layout utilizes the scarce GPU memory for storing the highly accessed
embeddings, thus reduces the data transfers from CPU to GPU. At the same time,
FAE engages the GPU to accelerate the executions of these hot embedding
entries. Experiments on production-scale recommendation models with real
datasets show that FAE reduces the overall training time by 2.3x and 1.52x in
comparison to XDL CPU-only and XDL CPU-GPU execution while maintaining baseline
accuracy
","[{'version': 'v1', 'created': 'Mon, 1 Mar 2021 01:43:26 GMT'}, {'version': 'v2', 'created': 'Tue, 2 Mar 2021 19:16:36 GMT'}, {'version': 'v3', 'created': 'Tue, 28 Sep 2021 19:08:26 GMT'}]",2024-03-19,"[['Adnan', 'Muhammad', ''], ['Maboud', 'Yassaman Ebrahimzadeh', ''], ['Mahajan', 'Divya', ''], ['Nair', 'Prashant J.', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
160,2103.01861,Idan Amit,"Idan Amit, Nili Ben Ezra, Dror G. Feitelson",Follow Your Nose -- Which Code Smells are Worth Chasing?,,,,,cs.SE cs.LG,http://creativecommons.org/licenses/by/4.0/,"  The common use case of code smells assumes causality: Identify a smell,
remove it, and by doing so improve the code. We empirically investigate their
fitness to this use. We present a list of properties that code smells should
have if they indeed cause lower quality. We evaluated the smells in 31,687 Java
files from 677 GitHub repositories, all the repositories with 200+ commits in
2019. We measured the influence of smells on four metrics for quality,
productivity, and bug detection efficiency. Out of 151 code smells computed by
the CheckStyle smell detector, less than 20% were found to be potentially
causal, and only a handful are rather robust. The strongest smells deal with
simplicity, defensive programming, and abstraction. Files without the
potentially causal smells are 50% more likely to be of high quality.
Unfortunately, most smells are not removed, and developers tend to remove the
easy ones and not the effective ones.
","[{'version': 'v1', 'created': 'Tue, 2 Mar 2021 16:55:46 GMT'}, {'version': 'v2', 'created': 'Mon, 15 Jan 2024 15:52:39 GMT'}]",2024-01-17,"[['Amit', 'Idan', ''], ['Ezra', 'Nili Ben', ''], ['Feitelson', 'Dror G.', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
161,2103.02543,Patrizio Frosini,"Pasquale Cascarano, Patrizio Frosini, Nicola Quercioli and Amir Saki","On the geometric and Riemannian structure of the spaces of group
  equivariant non-expansive operators","21 pages, 1 figure. The introduction has been extended and a section
  on the group's action on the space of GENEOs has been added. Some minor fixes
  are made. The text has been simplified and made clearer",,,,math.DG cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Group equivariant non-expansive operators have been recently proposed as
basic components in topological data analysis and deep learning. In this paper
we study some geometric properties of the spaces of group equivariant operators
and show how a space $\mathcal{F}$ of group equivariant non-expansive operators
can be endowed with the structure of a Riemannian manifold, so making available
the use of gradient descent methods for the minimization of cost functions on
$\mathcal{F}$. As an application of this approach, we also describe a procedure
to select a finite set of representative group equivariant non-expansive
operators in the considered manifold.
","[{'version': 'v1', 'created': 'Wed, 3 Mar 2021 17:29:25 GMT'}, {'version': 'v2', 'created': 'Sun, 31 Dec 2023 08:55:23 GMT'}]",2024-01-02,"[['Cascarano', 'Pasquale', ''], ['Frosini', 'Patrizio', ''], ['Quercioli', 'Nicola', ''], ['Saki', 'Amir', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
162,2103.03864,Krzysztof Maziarz,"Krzysztof Maziarz, Henry Jackson-Flux, Pashmina Cameron, Finton
  Sirockin, Nadine Schneider, Nikolaus Stiefl, Marwin Segler, Marc Brockschmidt",Learning to Extend Molecular Scaffolds with Structural Motifs,"Published at the 10th International Conference on Learning
  Representations (ICLR 2022)",,,,cs.LG q-bio.QM,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advancements in deep learning-based modeling of molecules promise to
accelerate in silico drug discovery. A plethora of generative models is
available, building molecules either atom-by-atom and bond-by-bond or
fragment-by-fragment. However, many drug discovery projects require a fixed
scaffold to be present in the generated molecule, and incorporating that
constraint has only recently been explored. Here, we propose MoLeR, a
graph-based model that naturally supports scaffolds as initial seed of the
generative procedure, which is possible because it is not conditioned on the
generation history. Our experiments show that MoLeR performs comparably to
state-of-the-art methods on unconstrained molecular optimization tasks, and
outperforms them on scaffold-based tasks, while being an order of magnitude
faster to train and sample from than existing approaches. Furthermore, we show
the influence of a number of seemingly minor design choices on the overall
performance.
","[{'version': 'v1', 'created': 'Fri, 5 Mar 2021 18:28:49 GMT'}, {'version': 'v2', 'created': 'Fri, 11 Jun 2021 17:58:07 GMT'}, {'version': 'v3', 'created': 'Tue, 14 Dec 2021 18:55:50 GMT'}, {'version': 'v4', 'created': 'Mon, 25 Apr 2022 17:45:58 GMT'}, {'version': 'v5', 'created': 'Sun, 12 May 2024 12:47:40 GMT'}]",2024-05-14,"[['Maziarz', 'Krzysztof', ''], ['Jackson-Flux', 'Henry', ''], ['Cameron', 'Pashmina', ''], ['Sirockin', 'Finton', ''], ['Schneider', 'Nadine', ''], ['Stiefl', 'Nikolaus', ''], ['Segler', 'Marwin', ''], ['Brockschmidt', 'Marc', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
163,2103.05621,Yehuda Dar,"Yehuda Dar, Daniel LeJeune, Richard G. Baraniuk","The Common Intuition to Transfer Learning Can Win or Lose: Case Studies
  for Linear Regression",,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
","[{'version': 'v1', 'created': 'Tue, 9 Mar 2021 18:46:01 GMT'}, {'version': 'v2', 'created': 'Thu, 10 Feb 2022 18:26:46 GMT'}, {'version': 'v3', 'created': 'Wed, 23 Aug 2023 16:54:03 GMT'}, {'version': 'v4', 'created': 'Fri, 31 May 2024 14:35:18 GMT'}]",2024-06-03,"[['Dar', 'Yehuda', ''], ['LeJeune', 'Daniel', ''], ['Baraniuk', 'Richard G.', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
164,2103.05753,Bradly Alicea,"Bradly Alicea, Rishabh Chakrabarty, Stefan Dvoretskii, Akshara Gopi,
  Avery Lim, and Jesse Parent","Continual Developmental Neurosimulation Using Embodied Computational
  Agents","35 pages, 9 figures",,,,q-bio.NC cs.AI cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
","[{'version': 'v1', 'created': 'Sun, 7 Mar 2021 07:22:49 GMT'}, {'version': 'v2', 'created': 'Sat, 28 Oct 2023 06:30:17 GMT'}, {'version': 'v3', 'created': 'Fri, 12 Jul 2024 06:10:30 GMT'}]",2024-07-15,"[['Alicea', 'Bradly', ''], ['Chakrabarty', 'Rishabh', ''], ['Dvoretskii', 'Stefan', ''], ['Gopi', 'Akshara', ''], ['Lim', 'Avery', ''], ['Parent', 'Jesse', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
165,2103.11509,Berk Iskender,"Berk Iskender, Yoram Bresler",Scatter Correction in X-ray CT by Physics-Inspired Deep Learning,,,10.1109/TCI.2022.3226300,,physics.med-ph cs.LG eess.IV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
","[{'version': 'v1', 'created': 'Sun, 21 Mar 2021 22:51:20 GMT'}, {'version': 'v2', 'created': 'Sun, 23 Jan 2022 04:58:43 GMT'}, {'version': 'v3', 'created': 'Wed, 2 Feb 2022 04:46:10 GMT'}]",2024-01-30,"[['Iskender', 'Berk', ''], ['Bresler', 'Yoram', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
166,2103.11856,Tapio Pahikkala,"Tapio Pahikkala, Parisa Movahedi, Ileana Montoya, Havu Miikonen,
  Stephan Foldes, Antti Airola, Laszlo Major",A Link between Coding Theory and Cross-Validation with Applications,,,,,cs.LG cs.IT math.CO math.IT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  How many different binary classification problems a single learning algorithm
can solve on a fixed data with exactly zero or at most a given number of
cross-validation errors? While the number in the former case is known to be
limited by the no-free-lunch theorem, we show that the exact answers are given
by the theory of error detecting codes. As a case study, we focus on the AUC
performance measure and leave-pair-out cross-validation (LPOCV), in which every
possible pair of data with different class labels is held out at a time. We
show that the maximal number of classification problems with fixed class
proportion, for which a learning algorithm can achieve zero LPOCV error, equals
the maximal number of code words in a constant weight code (CWC), with certain
technical properties. We then generalize CWCs by introducing light CWCs, and
prove an analogous result for nonzero LPOCV errors and light CWCs. Moreover, we
prove both upper and lower bounds on the maximal numbers of code words in light
CWCs. Finally, as an immediate practical application, we develop new LPOCV
based randomization tests for learning algorithms that generalize the classical
Wilcoxon-Mann-Whitney U test.
","[{'version': 'v1', 'created': 'Mon, 22 Mar 2021 13:57:45 GMT'}, {'version': 'v2', 'created': 'Thu, 25 Jan 2024 08:55:05 GMT'}, {'version': 'v3', 'created': 'Fri, 9 Feb 2024 09:48:46 GMT'}]",2024-02-12,"[['Pahikkala', 'Tapio', ''], ['Movahedi', 'Parisa', ''], ['Montoya', 'Ileana', ''], ['Miikonen', 'Havu', ''], ['Foldes', 'Stephan', ''], ['Airola', 'Antti', ''], ['Major', 'Laszlo', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
167,2103.12653,Fabio Marcos De Abreu Santos,"Fabio Santos, Igor Wiese, Bianca Trinkenreich, Igor Steinmacher, Anita
  Sarma and Marco Gerosa",Can I Solve It? Identifying APIs Required to Complete OSS Task,,"Mining Software Repositories Conference (MSR 2021), May 2021",10.1109/MSR52588.2021.00047,,cs.SE cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Open Source Software projects add labels to open issues to help contributors
choose tasks. However, manually labeling issues is time-consuming and
error-prone. Current automatic approaches for creating labels are mostly
limited to classifying issues as a bug/non-bug. In this paper, we investigate
the feasibility and relevance of labeling issues with the domain of the APIs
required to complete the tasks. We leverage the issues' description and the
project history to build prediction models, which resulted in precision up to
82% and recall up to 97.8%. We also ran a user study (n=74) to assess these
labels' relevancy to potential contributors. The results show that the labels
were useful to participants in choosing tasks, and the API-domain labels were
selected more often than the existing architecture-based labels. Our results
can inspire the creation of tools to automatically label issues, helping
developers to find tasks that better match their skills.
","[{'version': 'v1', 'created': 'Tue, 23 Mar 2021 16:16:09 GMT'}]",2024-01-24,"[['Santos', 'Fabio', ''], ['Wiese', 'Igor', ''], ['Trinkenreich', 'Bianca', ''], ['Steinmacher', 'Igor', ''], ['Sarma', 'Anita', ''], ['Gerosa', 'Marco', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
168,2103.1289,Lucas Barcelos,"Lucas Barcelos, Alexander Lambert, Rafael Oliveira, Paulo Borges,
  Byron Boots and Fabio Ramos",Dual Online Stein Variational Inference for Control and Dynamics,Corresponding author: lucas.barcelos@sydney.edu.au,,,,cs.RO cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Model predictive control (MPC) schemes have a proven track record for
delivering aggressive and robust performance in many challenging control tasks,
coping with nonlinear system dynamics, constraints, and observational noise.
Despite their success, these methods often rely on simple control
distributions, which can limit their performance in highly uncertain and
complex environments. MPC frameworks must be able to accommodate changing
distributions over system parameters, based on the most recent measurements. In
this paper, we devise an implicit variational inference algorithm able to
estimate distributions over model parameters and control inputs on-the-fly. The
method incorporates Stein Variational gradient descent to approximate the
target distributions as a collection of particles, and performs updates based
on a Bayesian formulation. This enables the approximation of complex
multi-modal posterior distributions, typically occurring in challenging and
realistic robot navigation tasks. We demonstrate our approach on both simulated
and real-world experiments requiring real-time execution in the face of
dynamically changing environments.
","[{'version': 'v1', 'created': 'Tue, 23 Mar 2021 23:26:09 GMT'}]",2024-01-24,"[['Barcelos', 'Lucas', ''], ['Lambert', 'Alexander', ''], ['Oliveira', 'Rafael', ''], ['Borges', 'Paulo', ''], ['Boots', 'Byron', ''], ['Ramos', 'Fabio', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
169,2103.1501,Max Simchowitz,"Tyler Westenbroek, Max Simchowitz, Michael I. Jordan, S. Shankar
  Sastry","On the Stability of Nonlinear Receding Horizon Control: A Geometric
  Perspective",,,,,math.OC cs.LG cs.SY eess.SY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  %!TEX root = LCSS_main_max.tex
  The widespread adoption of nonlinear Receding Horizon Control (RHC)
strategies by industry has led to more than 30 years of intense research
efforts to provide stability guarantees for these methods. However, current
theoretical guarantees require that each (generally nonconvex) planning problem
can be solved to (approximate) global optimality, which is an unrealistic
requirement for the derivative-based local optimization methods generally used
in practical implementations of RHC. This paper takes the first step towards
understanding stability guarantees for nonlinear RHC when the inner planning
problem is solved to first-order stationary points, but not necessarily global
optima. Special attention is given to feedback linearizable systems, and a
mixture of positive and negative results are provided. We establish that, under
certain strong conditions, first-order solutions to RHC exponentially stabilize
linearizable systems. Surprisingly, these conditions can hold even in
situations where there may be \textit{spurious local minima.} Crucially, this
guarantee requires that state costs applied to the planning problems are in a
certain sense `compatible' with the global geometry of the system, and a simple
counter-example demonstrates the necessity of this condition. These results
highlight the need to rethink the role of global geometry in the context of
optimization-based control.
","[{'version': 'v1', 'created': 'Sat, 27 Mar 2021 22:59:37 GMT'}, {'version': 'v2', 'created': 'Sun, 11 Sep 2022 18:03:54 GMT'}, {'version': 'v3', 'created': 'Thu, 25 Jan 2024 19:02:39 GMT'}]",2024-01-29,"[['Westenbroek', 'Tyler', ''], ['Simchowitz', 'Max', ''], ['Jordan', 'Michael I.', ''], ['Sastry', 'S. Shankar', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
170,2104.00032,"Moritz B\""ohle","Moritz B\""ohle and Mario Fritz and Bernt Schiele","Convolutional Dynamic Alignment Networks for Interpretable
  Classifications",Published at CVRP 2021 (oral),,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce a new family of neural network models called Convolutional
Dynamic Alignment Networks (CoDA-Nets), which are performant classifiers with a
high degree of inherent interpretability. Their core building blocks are
Dynamic Alignment Units (DAUs), which linearly transform their input with
weight vectors that dynamically align with task-relevant patterns. As a result,
CoDA-Nets model the classification prediction through a series of
input-dependent linear transformations, allowing for linear decomposition of
the output into individual input contributions. Given the alignment of the
DAUs, the resulting contribution maps align with discriminative input patterns.
These model-inherent decompositions are of high visual quality and outperform
existing attribution methods under quantitative metrics. Further, CoDA-Nets
constitute performant classifiers, achieving on par results to ResNet and VGG
models on e.g. CIFAR-10 and TinyImagenet.
","[{'version': 'v1', 'created': 'Wed, 31 Mar 2021 18:03:53 GMT'}, {'version': 'v2', 'created': 'Mon, 15 Jan 2024 08:33:14 GMT'}]",2024-01-17,"[['Böhle', 'Moritz', ''], ['Fritz', 'Mario', ''], ['Schiele', 'Bernt', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
171,2104.0017,Robik Shrestha,"Robik Shrestha, Kushal Kafle and Christopher Kanan",Are Bias Mitigation Techniques for Deep Learning Effective?,"Published in WACV 2022 under the title ""An Investigation of Critical
  Issues in Bias Mitigation Techniques""",,,,cs.LG cs.AI cs.CV stat.ML,http://creativecommons.org/licenses/by/4.0/,"  A critical problem in deep learning is that systems learn inappropriate
biases, resulting in their inability to perform well on minority groups. This
has led to the creation of multiple algorithms that endeavor to mitigate bias.
However, it is not clear how effective these methods are. This is because study
protocols differ among papers, systems are tested on datasets that fail to test
many forms of bias, and systems have access to hidden knowledge or are tuned
specifically to the test set. To address this, we introduce an improved
evaluation protocol, sensible metrics, and a new dataset, which enables us to
ask and answer critical questions about bias mitigation algorithms. We evaluate
seven state-of-the-art algorithms using the same network architecture and
hyperparameter selection policy across three benchmark datasets. We introduce a
new dataset called Biased MNIST that enables assessment of robustness to
multiple bias sources. We use Biased MNIST and a visual question answering
(VQA) benchmark to assess robustness to hidden biases. Rather than only tuning
to the test set distribution, we study robustness across different tuning
distributions, which is critical because for many applications the test
distribution may not be known during development. We find that algorithms
exploit hidden biases, are unable to scale to multiple forms of bias, and are
highly sensitive to the choice of tuning set. Based on our findings, we implore
the community to adopt more rigorous assessment of future bias mitigation
methods. All data, code, and results are publicly available at:
https://github.com/erobic/bias-mitigators.
","[{'version': 'v1', 'created': 'Thu, 1 Apr 2021 00:14:45 GMT'}, {'version': 'v2', 'created': 'Fri, 22 Oct 2021 19:56:52 GMT'}, {'version': 'v3', 'created': 'Mon, 15 Apr 2024 01:03:11 GMT'}, {'version': 'v4', 'created': 'Tue, 23 Apr 2024 13:42:45 GMT'}]",2024-04-24,"[['Shrestha', 'Robik', ''], ['Kafle', 'Kushal', ''], ['Kanan', 'Christopher', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
172,2104.00864,Daniel Olds,"Phillip M. Maffettone, Aidan C. Daly, Daniel Olds","Constrained non-negative matrix factorization enabling real-time
  insights of $\textit{in situ}$ and high-throughput experiments","This article has been submitted to Applied Physics Reviews. After it
  is published, it will be found at https://aip.scitation.org/journal/are.
  Copyright (2021) Phillip M. Maffettone, Aiden C. Daly, Daniel Olds",,10.1063/5.0052859,,physics.app-ph cond-mat.mtrl-sci cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Non-negative Matrix Factorization (NMF) methods offer an appealing
unsupervised learning method for real-time analysis of streaming spectral data
in time-sensitive data collection, such as $\textit{in situ}$ characterization
of materials. However, canonical NMF methods are optimized to reconstruct a
full dataset as closely as possible, with no underlying requirement that the
reconstruction produces components or weights representative of the true
physical processes. In this work, we demonstrate how constraining NMF weights
or components, provided as known or assumed priors, can provide significant
improvement in revealing true underlying phenomena. We present a PyTorch based
method for efficiently applying constrained NMF and demonstrate this on several
synthetic examples. When applied to streaming experimentally measured spectral
data, an expert researcher-in-the-loop can provide and dynamically adjust the
constraints. This set of interactive priors to the NMF model can, for example,
contain known or identified independent components, as well as functional
expectations about the mixing of components. We demonstrate this application on
measured X-ray diffraction and pair distribution function data from $\textit{in
situ}$ beamline experiments. Details of the method are described, and general
guidance provided to employ constrained NMF in extraction of critical
information and insights during $\textit{in situ}$ and high-throughput
experiments.
","[{'version': 'v1', 'created': 'Fri, 2 Apr 2021 03:04:24 GMT'}]",2024-06-12,"[['Maffettone', 'Phillip M.', ''], ['Daly', 'Aidan C.', ''], ['Olds', 'Daniel', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
173,2104.02206,Morgan Talbot,"Morgan B. Talbot, Rushikesh Zawar, Rohil Badkundri, Mengmi Zhang,
  Gabriel Kreiman",Tuned Compositional Feature Replays for Efficient Stream Learning,"Copyright 2023 IEEE. The journal version of this article is hosted at
  https://ieeexplore.ieee.org/document/10373937 and
  https://klab.tch.harvard.edu/publications/PDFs/gk8019.pdf",,10.1109/TNNLS.2023.3344085,,cs.CV cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Our brains extract durable, generalizable knowledge from transient
experiences of the world. Artificial neural networks come nowhere close to this
ability. When tasked with learning to classify objects by training on
non-repeating video frames in temporal order (online stream learning), models
that learn well from shuffled datasets catastrophically forget old knowledge
upon learning new stimuli. We propose a new continual learning algorithm,
Compositional Replay Using Memory Blocks (CRUMB), which mitigates forgetting by
replaying feature maps reconstructed by combining generic parts. CRUMB
concatenates trainable and re-usable ""memory block"" vectors to compositionally
reconstruct feature map tensors in convolutional neural networks. Storing the
indices of memory blocks used to reconstruct new stimuli enables memories of
the stimuli to be replayed during later tasks. This reconstruction mechanism
also primes the neural network to minimize catastrophic forgetting by biasing
it towards attending to information about object shapes more than information
about image textures, and stabilizes the network during stream learning by
providing a shared feature-level basis for all training examples. These
properties allow CRUMB to outperform an otherwise identical algorithm that
stores and replays raw images, while occupying only 3.6% as much memory. We
stress-tested CRUMB alongside 13 competing methods on 7 challenging datasets.
To address the limited number of existing online stream learning datasets, we
introduce 2 new benchmarks by adapting existing datasets for stream learning.
With only 3.7-4.1% as much memory and 15-43% as much runtime, CRUMB mitigates
catastrophic forgetting more effectively than the state-of-the-art. Our code is
available at https://github.com/MorganBDT/crumb.git.
","[{'version': 'v1', 'created': 'Tue, 6 Apr 2021 00:53:01 GMT'}, {'version': 'v2', 'created': 'Wed, 7 Apr 2021 01:25:33 GMT'}, {'version': 'v3', 'created': 'Sat, 25 Sep 2021 15:41:24 GMT'}, {'version': 'v4', 'created': 'Tue, 23 Nov 2021 15:31:55 GMT'}, {'version': 'v5', 'created': 'Sat, 26 Nov 2022 18:13:10 GMT'}, {'version': 'v6', 'created': 'Mon, 6 Mar 2023 20:32:23 GMT'}, {'version': 'v7', 'created': 'Wed, 25 Oct 2023 21:51:26 GMT'}, {'version': 'v8', 'created': 'Tue, 2 Jan 2024 16:12:32 GMT'}]",2024-01-03,"[['Talbot', 'Morgan B.', ''], ['Zawar', 'Rushikesh', ''], ['Badkundri', 'Rohil', ''], ['Zhang', 'Mengmi', ''], ['Kreiman', 'Gabriel', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
174,2104.02493,Ole Schumann,"Ole Schumann, Markus Hahn, Nicolas Scheiner, Fabio Weishaupt, Julius
  F. Tilly, J\""urgen Dickmann, Christian W\""ohler","RadarScenes: A Real-World Radar Point Cloud Data Set for Automotive
  Applications",,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A new automotive radar data set with measurements and point-wise annotations
from more than four hours of driving is presented. Data provided by four series
radar sensors mounted on one test vehicle were recorded and the individual
detections of dynamic objects were manually grouped to clusters and labeled
afterwards. The purpose of this data set is to enable the development of novel
(machine learning-based) radar perception algorithms with the focus on moving
road users. Images of the recorded sequences were captured using a documentary
camera. For the evaluation of future object detection and classification
algorithms, proposals for score calculation are made so that researchers can
evaluate their algorithms on a common basis. Additional information as well as
download instructions can be found on the website of the data set:
www.radar-scenes.com.
","[{'version': 'v1', 'created': 'Tue, 6 Apr 2021 13:22:23 GMT'}, {'version': 'v2', 'created': 'Sun, 18 Feb 2024 10:43:09 GMT'}]",2024-02-20,"[['Schumann', 'Ole', ''], ['Hahn', 'Markus', ''], ['Scheiner', 'Nicolas', ''], ['Weishaupt', 'Fabio', ''], ['Tilly', 'Julius F.', ''], ['Dickmann', 'Jürgen', ''], ['Wöhler', 'Christian', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
175,2104.03158,Jean Pauphilet,"Dimitris Bertsimas, Arthur Delarue, Jean Pauphilet","Simple Imputation Rules for Prediction with Missing Data: Contrasting
  Theoretical Guarantees with Empirical Performance",,,,,stat.ML cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Missing data is a common issue in real-world datasets. This paper studies the
performance of impute-then-regress pipelines by contrasting theoretical and
empirical evidence. We establish the asymptotic consistency of such pipelines
for a broad family of imputation methods. While common sense suggests that a
`good' imputation method produces datasets that are plausible, we show, on the
contrary, that, as far as prediction is concerned, crude can be good. Among
others, we find that mode-impute is asymptotically sub-optimal, while
mean-impute is asymptotically optimal. We then exhaustively assess the validity
of these theoretical conclusions on a large corpus of synthetic, semi-real, and
real datasets. While the empirical evidence we collect mostly supports our
theoretical findings, it also highlights gaps between theory and practice and
opportunities for future research, regarding the relevance of the MAR
assumption, the complex interdependency between the imputation and regression
tasks, and the need for realistic synthetic data generation models.
","[{'version': 'v1', 'created': 'Wed, 7 Apr 2021 14:45:14 GMT'}, {'version': 'v2', 'created': 'Wed, 5 Oct 2022 14:40:56 GMT'}, {'version': 'v3', 'created': 'Fri, 2 Feb 2024 07:58:40 GMT'}]",2024-02-05,"[['Bertsimas', 'Dimitris', ''], ['Delarue', 'Arthur', ''], ['Pauphilet', 'Jean', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
176,2104.03361,Javier Gonzalez-Trejo,"Javier A. Gonz\'alez-Trejo, Diego A. Mercado-Ravell","Monitoring Social-distance in Wide Areas during Pandemics: a Density Map
  and Segmentation Approach",Video: https://youtu.be/TwzBMKg7h_U,,,,cs.CV cs.LG eess.IV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With the relaxation of the containment measurements around the globe,
monitoring the social distancing in crowded public places is of grate
importance to prevent a new massive wave of COVID-19 infections. Recent works
in that matter have limited themselves by detecting social distancing in
corridors up to small crowds by detecting each person individually considering
the full body in the image. In this work, we propose a new framework for
monitoring the social-distance using end-to-end Deep Learning, to detect crowds
violating the social-distance in wide areas where important occlusions may be
present. Our framework consists in the creation of a new ground truth based on
the ground truth density maps and the proposal of two different solutions, a
density-map-based and a segmentation-based, to detect the crowds violating the
social-distance constrain. We assess the results of both approaches by using
the generated ground truth from the PET2009 and CityStreet datasets. We show
that our framework performs well at providing the zones where people are not
following the social-distance even when heavily occluded or far away from one
camera.
","[{'version': 'v1', 'created': 'Wed, 7 Apr 2021 19:26:26 GMT'}]",2024-04-02,"[['González-Trejo', 'Javier A.', ''], ['Mercado-Ravell', 'Diego A.', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
177,2104.04026,Petr M\'anek,"Petr M\'anek (1 and 2), Graham Van Goffrier (1), Vignesh Gopakumar
  (3), Nikolaos Nikolaou (1), Jonathan Shimwell (3) and Ingo Waldmann (1) ((1)
  Department of Physics and Astronomy, University College London, London, UK,
  (2) Institute of Experimental and Applied Physics, Czech Technical
  University, Prague, Czech Republic, (3) UK Atomic Energy Authority, Culham
  Science Centre, Abingdon, UK)",Fast Regression of the Tritium Breeding Ratio in Fusion Reactors,"13 pages, 8 figures",Mach. Learn.: Sci. Technol. 4 015008 (2023),10.1088/2632-2153/acb2b3,363718,physics.comp-ph cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  The tritium breeding ratio (TBR) is an essential quantity for the design of
modern and next-generation D-T fueled nuclear fusion reactors. Representing the
ratio between tritium fuel generated in breeding blankets and fuel consumed
during reactor runtime, the TBR depends on reactor geometry and material
properties in a complex manner. In this work, we explored the training of
surrogate models to produce a cheap but high-quality approximation for a Monte
Carlo TBR model in use at the UK Atomic Energy Authority. We investigated
possibilities for dimensional reduction of its feature space, reviewed 9
families of surrogate models for potential applicability, and performed
hyperparameter optimisation. Here we present the performance and scaling
properties of these models, the fastest of which, an artificial neural network,
demonstrated $R^2=0.985$ and a mean prediction time of $0.898\ \mu\mathrm{s}$,
representing a relative speedup of $8\cdot 10^6$ with respect to the expensive
MC model. We further present a novel adaptive sampling algorithm,
Quality-Adaptive Surrogate Sampling, capable of interfacing with any of the
individually studied surrogates. Our preliminary testing on a toy TBR theory
has demonstrated the efficacy of this algorithm for accelerating the surrogate
modelling process.
","[{'version': 'v1', 'created': 'Thu, 8 Apr 2021 19:55:42 GMT'}, {'version': 'v2', 'created': 'Mon, 12 Sep 2022 17:15:39 GMT'}]",2024-01-08,"[['Mánek', 'Petr', '', '1 and 2'], ['Van Goffrier', 'Graham', ''], ['Gopakumar', 'Vignesh', ''], ['Nikolaou', 'Nikolaos', ''], ['Shimwell', 'Jonathan', ''], ['Waldmann', 'Ingo', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
178,2104.07324,Shayan Hashemi,"Shayan Hashemi, Mika M\""antyl\""a",OneLog: Towards End-to-End Training in Software Log Anomaly Detection,,,10.1007/s10515-024-00428-x,,cs.SE cs.LG,http://creativecommons.org/licenses/by/4.0/,"  With the growth of online services, IoT devices, and DevOps-oriented software
development, software log anomaly detection is becoming increasingly important.
Prior works mainly follow a traditional four-staged architecture (Preprocessor,
Parser, Vectorizer, and Classifier). This paper proposes OneLog, which utilizes
a single Deep Neural Network (DNN) instead of multiple separate components.
OneLog harnesses Convolutional Neural Networks (CNN) at the character level to
take digits, numbers, and punctuations, which were removed in prior works, into
account alongside the main natural language text. We evaluate our approach in
six message- and sequence-based data sets: HDFS, Hadoop, BGL, Thunderbird,
Spirit, and Liberty. We experiment with Onelog with single-, multi-, and
cross-project setups. Onelog offers state-of-the-art performance in our
datasets. Onelog can utilize multi-project datasets simultaneously during
training, which suggests our model can generalize between datasets.
Multi-project training also improves Onelog performance making it ideal when
limited training data is available for an individual project. We also found
that cross-project anomaly detection is possible with a single project pair
(Liberty and Spirit). Analysis of model internals shows that one log has
multiple modes of detecting anomalies and that the model learns manually
validated parsing rules for the log messages. We conclude that character-based
CNNs are a promising approach toward end-to-end learning in log anomaly
detection. They offer good performance and generalization over multiple
datasets. We will make our scripts publicly available upon the acceptance of
this paper.
","[{'version': 'v1', 'created': 'Thu, 15 Apr 2021 09:23:32 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Feb 2024 17:07:34 GMT'}]",2024-08-06,"[['Hashemi', 'Shayan', ''], ['Mäntylä', 'Mika', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
179,2104.09029,Siamak Layeghy,"Siamak Layeghy, Marcus Gallagher, Marius Portmann",Benchmarking the Benchmark -- Analysis of Synthetic NIDS Datasets,"25 pages, 13 figures",,10.1016/j.jisa.2023.103689,,cs.NI cs.CR cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Network Intrusion Detection Systems (NIDSs) are an increasingly important
tool for the prevention and mitigation of cyber attacks. A number of labelled
synthetic datasets generated have been generated and made publicly available by
researchers, and they have become the benchmarks via which new ML-based NIDS
classifiers are being evaluated. Recently published results show excellent
classification performance with these datasets, increasingly approaching 100
percent performance across key evaluation metrics such as accuracy, F1 score,
etc. Unfortunately, we have not yet seen these excellent academic research
results translated into practical NIDS systems with such near-perfect
performance. This motivated our research presented in this paper, where we
analyse the statistical properties of the benign traffic in three of the more
recent and relevant NIDS datasets, (CIC, UNSW, ...). As a comparison, we
consider two datasets obtained from real-world production networks, one from a
university network and one from a medium size Internet Service Provider (ISP).
Our results show that the two real-world datasets are quite similar among
themselves in regards to most of the considered statistical features. Equally,
the three synthetic datasets are also relatively similar within their group.
However, and most importantly, our results show a distinct difference of most
of the considered statistical features between the three synthetic datasets and
the two real-world datasets. Since ML relies on the basic assumption of
training and test datasets being sampled from the same distribution, this
raises the question of how well the performance results of ML-classifiers
trained on the considered synthetic datasets can translate and generalise to
real-world networks. We believe this is an interesting and relevant question
which provides motivation for further research in this space.
","[{'version': 'v1', 'created': 'Mon, 19 Apr 2021 03:17:37 GMT'}]",2024-01-09,"[['Layeghy', 'Siamak', ''], ['Gallagher', 'Marcus', ''], ['Portmann', 'Marius', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
180,2104.10751,Hakan Akyuz,"Tabea E. R\""ober, Adia C. Lumadjeng, M. Hakan Aky\""uz, \c{S}. \.Ilker
  Birbil","Rule Generation for Classification: Scalability, Interpretability, and
  Fairness",,,,,cs.LG stat.ML,http://creativecommons.org/licenses/by/4.0/,"  We introduce a new rule-based optimization method for classification with
constraints. The proposed method leverages column generation for linear
programming, and hence, is scalable to large datasets. The resulting pricing
subproblem is shown to be NP-Hard. We recourse to a decision tree-based
heuristic and solve a proxy pricing subproblem for acceleration. The method
returns a set of rules along with their optimal weights indicating the
importance of each rule for learning. We address interpretability and fairness
by assigning cost coefficients to the rules and introducing additional
constraints. In particular, we focus on local interpretability and generalize
separation criterion in fairness to multiple sensitive attributes and classes.
We test the performance of the proposed methodology on a collection of datasets
and present a case study to elaborate on its different aspects. The proposed
rule-based learning method exhibits a good compromise between local
interpretability and fairness on the one side, and accuracy on the other side.
","[{'version': 'v1', 'created': 'Wed, 21 Apr 2021 20:31:28 GMT'}, {'version': 'v2', 'created': 'Sun, 4 Dec 2022 17:26:37 GMT'}, {'version': 'v3', 'created': 'Wed, 30 Aug 2023 15:51:05 GMT'}, {'version': 'v4', 'created': 'Sun, 12 May 2024 12:51:09 GMT'}]",2024-05-14,"[['Röber', 'Tabea E.', ''], ['Lumadjeng', 'Adia C.', ''], ['Akyüz', 'M. Hakan', ''], ['Birbil', 'Ş. İlker', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
181,2104.12582,Heather Williams,"Heather M. Williams, Roman V. Yampolskiy",Understanding and Avoiding AI Failures: A Practical Guide,,,,,cs.CY cs.AI,http://creativecommons.org/licenses/by/4.0/,"  As AI technologies increase in capability and ubiquity, AI accidents are
becoming more common. Based on normal accident theory, high reliability theory,
and open systems theory, we create a framework for understanding the risks
associated with AI applications. In addition, we also use AI safety principles
to quantify the unique risks of increased intelligence and human-like qualities
in AI. Together, these two fields give a more complete picture of the risks of
contemporary AI. By focusing on system properties near accidents instead of
seeking a root cause of accidents, we identify where attention should be paid
to safety for current generation AI systems.
","[{'version': 'v1', 'created': 'Thu, 22 Apr 2021 17:05:27 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Apr 2021 12:31:55 GMT'}, {'version': 'v3', 'created': 'Wed, 28 Apr 2021 21:33:11 GMT'}, {'version': 'v4', 'created': 'Mon, 11 Mar 2024 18:58:33 GMT'}]",2024-03-13,"[['Williams', 'Heather M.', ''], ['Yampolskiy', 'Roman V.', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
182,2104.13894,Abiy Tasissa,"Abiy Tasissa, Pranay Tankala and Demba Ba",Weighed l1 on the simplex: Compressive sensing meets locality,"7 pages, 2 figures. The proof of theorem 1 in v1 does not hold true
  in general without additional assumptions. This version fixes this problem.
  For more details, we refer the interested reader to arXiv:2012.02134 which is
  the journal version of the workshop paper v1",,,,eess.SP cs.IT cs.LG math.IT math.OC,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Sparse manifold learning algorithms combine techniques in manifold learning
and sparse optimization to learn features that could be utilized for downstream
tasks. The standard setting of compressive sensing can not be immediately
applied to this setup. Due to the intrinsic geometric structure of data,
dictionary atoms might be redundant and do not satisfy the restricted isometry
property or coherence condition. In addition, manifold learning emphasizes
learning local geometry which is not reflected in a standard $\ell_1$
minimization problem. We propose weighted $\ell_0$ and weighted $\ell_1$
metrics that encourage representation via neighborhood atoms suited for
dictionary based manifold learning. Assuming that the data is generated from
Delaunay triangulation, we show the equivalence of weighted $\ell_0$ and
weighted $\ell_1$. We discuss an optimization program that learns the
dictionaries and sparse coefficients and demonstrate the utility of our
regularization on synthetic and real datasets.
","[{'version': 'v1', 'created': 'Wed, 28 Apr 2021 17:26:29 GMT'}, {'version': 'v2', 'created': 'Fri, 2 Aug 2024 05:05:48 GMT'}]",2024-08-05,"[['Tasissa', 'Abiy', ''], ['Tankala', 'Pranay', ''], ['Ba', 'Demba', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
183,2105.02487,Boxin Zhao,"Boxin Zhao, Percy S. Zhai, Y. Samuel Wang, Mladen Kolar","High-dimensional Functional Graphical Model Structure Learning via
  Neighborhood Selection Approach",,,,,stat.ML cs.LG stat.ME,http://creativecommons.org/licenses/by/4.0/,"  Undirected graphical models are widely used to model the conditional
independence structure of vector-valued data. However, in many modern
applications, for example those involving EEG and fMRI data, observations are
more appropriately modeled as multivariate random functions rather than
vectors. Functional graphical models have been proposed to model the
conditional independence structure of such functional data. We propose a
neighborhood selection approach to estimate the structure of Gaussian
functional graphical models, where we first estimate the neighborhood of each
node via a function-on-function regression and subsequently recover the entire
graph structure by combining the estimated neighborhoods. Our approach only
requires assumptions on the conditional distributions of random functions, and
we estimate the conditional independence structure directly. We thus circumvent
the need for a well-defined precision operator that may not exist when the
functions are infinite dimensional. Additionally, the neighborhood selection
approach is computationally efficient and can be easily parallelized. The
statistical consistency of the proposed method in the high-dimensional setting
is supported by both theory and experimental results. In addition, we study the
effect of the choice of the function basis used for dimensionality reduction in
an intermediate step. We give a heuristic criterion for choosing a function
basis and motivate two practically useful choices, which we justify by both
theory and experiments.
","[{'version': 'v1', 'created': 'Thu, 6 May 2021 07:38:50 GMT'}, {'version': 'v2', 'created': 'Tue, 29 Mar 2022 02:22:18 GMT'}, {'version': 'v3', 'created': 'Fri, 26 Jan 2024 02:28:28 GMT'}]",2024-01-29,"[['Zhao', 'Boxin', ''], ['Zhai', 'Percy S.', ''], ['Wang', 'Y. Samuel', ''], ['Kolar', 'Mladen', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
184,2105.02813,Subhayan De,"Subhayan De, Bhuiyan Shameem Mahmood Ebna Hai, Alireza Doostan, Markus
  Bause","Prediction of Ultrasonic Guided Wave Propagation in Solid-fluid and
  their Interface under Uncertainty using Machine Learning","21 pages, 13 figures",Journal of Engineering Mechanics (2021),10.1061/(ASCE)EM.1943-7889.0002038,,eess.SP cs.LG physics.flu-dyn,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Structural health monitoring (SHM) systems use the non-destructive testing
principle for damage identification. As part of SHM, the propagation of
ultrasonic guided waves (UGWs) is tracked and analyzed for the changes in the
associated wave pattern. These changes help identify the location of a
structural damage, if any. We advance existing research by accounting for
uncertainty in the material and geometric properties of a structure. The
physics model used in this study comprises of a monolithically coupled system
of acoustic and elastic wave equations, known as the wave propagation in
fluid-solid and their interface (WpFSI) problem. As the UGWs propagate in the
solid, fluid, and their interface, the wave signal displacement measurements
are contrasted against the benchmark pattern. For the numerical solution, we
develop an efficient algorithm that successfully addresses the inherent
complexity of solving the multiphysics problem under uncertainty. We present a
procedure that uses Gaussian process regression and convolutional neural
network for predicting the UGW propagation in a solid-fluid and their interface
under uncertainty. First, a set of training images for different realizations
of the uncertain parameters of the inclusion inside the structure is generated
using a monolithically-coupled system of acoustic and elastic wave equations.
Next, Gaussian processes trained with these images are used for predicting the
propagated wave with convolutional neural networks for further enhancement to
produce high-quality images of the wave patterns for new realizations of the
uncertainty. The results indicate that the proposed approach provides an
accurate prediction for the WpFSI problem in the presence of uncertainty.
","[{'version': 'v1', 'created': 'Tue, 30 Mar 2021 01:05:14 GMT'}]",2024-01-15,"[['De', 'Subhayan', ''], ['Hai', 'Bhuiyan Shameem Mahmood Ebna', ''], ['Doostan', 'Alireza', ''], ['Bause', 'Markus', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
185,2105.03607,Gowtham S Seenivasaharagavan,"Gowtham S Seenivasaharagavan, Milan Korda, Hassan Arbabi and Igor
  Mezi\'c",Clarifying the effect of mean subtraction on Dynamic Mode Decomposition,"39 pages, 7 figures",,,,math.DS eess.SP physics.data-an,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Any autonomous nonlinear dynamical system can be viewed as a superposition of
infinitely many linear processes, through the so-called Koopman mode
decomposition. Its data-driven approximation- Dynamic Mode Decomposition (DMD)-
has been extensively developed and deployed across a plethora of fields. In
this work, we study the effect of subtracting the temporal mean on the DMD
approximation, for observables possessing only a finite number of Koopman
modes.
  Pre-processing time-sequential training data by removing the temporal mean
has been a point of contention in the Companion matrix formulation of DMD. This
stems from the potential of said pre-processing to render DMD equivalent to a
temporal Discrete Fourier Transform (DFT). We prove that this equivalence is
impossible when the training data is linearly consistent and the order of the
DMD approximation exceeds the number of Koopman modes. Since model order and
training set size are synonymous in this variant of DMD, the parity of DMD and
DFT can, therefore, be indicative of inadequate training data.
","[{'version': 'v1', 'created': 'Sat, 8 May 2021 06:05:25 GMT'}, {'version': 'v2', 'created': 'Mon, 21 Jun 2021 04:04:30 GMT'}, {'version': 'v3', 'created': 'Fri, 3 Jun 2022 18:45:10 GMT'}, {'version': 'v4', 'created': 'Tue, 21 Mar 2023 00:29:44 GMT'}, {'version': 'v5', 'created': 'Thu, 6 Jun 2024 06:31:13 GMT'}]",2024-06-07,"[['Seenivasaharagavan', 'Gowtham S', ''], ['Korda', 'Milan', ''], ['Arbabi', 'Hassan', ''], ['Mezić', 'Igor', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
186,2105.06251,"Eike Stadtl\""ander","Eike Stadtl\""ander, Tam\'as Horv\'ath, Stefan Wrobel",Learning Weakly Convex Sets in Metric Spaces,"completely revised version, currently under review",,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
","[{'version': 'v1', 'created': 'Mon, 10 May 2021 23:00:02 GMT'}, {'version': 'v2', 'created': 'Tue, 19 Mar 2024 18:02:11 GMT'}]",2024-03-21,"[['Stadtländer', 'Eike', ''], ['Horváth', 'Tamás', ''], ['Wrobel', 'Stefan', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
187,2105.11598,Jackson Shields,"Jackson Shields, Oscar Pizarro, Stefan B. Williams",Feature Space Exploration For Planning Initial Benthic AUV Surveys,,"Field Robotics (2023), 3, 652-686",10.55417/fr.2023021,,cs.RO cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Special-purpose Autonomous Underwater Vehicles (AUVs) are utilised for
benthic (seafloor) surveys, where the vehicle collects optical imagery of the
seafloor. Due to the small-sensor footprint of the cameras and the vast areas
to be surveyed, these AUVs can not feasibly collect full coverage imagery of
areas larger than a few tens of thousands of square meters. Therefore it is
necessary for AUV paths to sample the surveys areas sparsely, yet effectively.
Broad-scale acoustic bathymetric data is readily available over large areas,
and is often a useful prior of seafloor cover. As such, prior bathymetry can be
used to guide AUV data collection. This research proposes methods for planning
initial AUV surveys that efficiently explore a feature space representation of
the bathymetry, in order to sample from a diverse set of bathymetric terrain.
This will enable the AUV to visit areas that likely contain unique habitats and
are representative of the entire survey site. We propose several information
gathering planners that utilise a feature space exploration reward, to plan
freeform paths or to optimise the placement of a survey template. The
suitability of these methods to plan AUV surveys is evaluated based on the
coverage of the feature space and also the ability to visit all classes of
benthic habitat on the initial dive. Informative planners based on
Rapidly-expanding Random Trees (RRT) and Monte-Carlo Tree Search (MCTS) were
found to be the most effective. This is a valuable tool for AUV surveys as it
increases the utility of initial dives. It also delivers a comprehensive
training set to learn a relationship between acoustic bathymetry and
visually-derived seafloor classifications.
","[{'version': 'v1', 'created': 'Tue, 25 May 2021 01:20:18 GMT'}, {'version': 'v2', 'created': 'Fri, 29 Dec 2023 00:53:35 GMT'}]",2024-01-01,"[['Shields', 'Jackson', ''], ['Pizarro', 'Oscar', ''], ['Williams', 'Stefan B.', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
188,2105.12436,Chi Zhang,"Chi Zhang (1), Christian Berger (1), Marco Dozza (2) ((1) Department
  of Computer Science and Engineering, University of Gothenburg, Gothenburg,
  Sweden, (2) Department of Maritime Sciences and Mechanics, Chalmers
  University of Technology, Gothenburg, Sweden)","Social-IWSTCNN: A Social Interaction-Weighted Spatio-Temporal
  Convolutional Neural Network for Pedestrian Trajectory Prediction in Urban
  Traffic Scenarios","8 pages, 4 figures. Accepted in IEEE Intelligent Vehicles Symposium
  (IV), 2021","2021 IEEE Intelligent Vehicles Symposium (IV), Nagoya, Japan,
  2021, pp. 1515-1522",10.1109/IV48863.2021.9575958,,cs.CV cs.AI cs.LG,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Pedestrian trajectory prediction in urban scenarios is essential for
automated driving. This task is challenging because the behavior of pedestrians
is influenced by both their own history paths and the interactions with others.
Previous research modeled these interactions with pooling mechanisms or
aggregating with hand-crafted attention weights. In this paper, we present the
Social Interaction-Weighted Spatio-Temporal Convolutional Neural Network
(Social-IWSTCNN), which includes both the spatial and the temporal features. We
propose a novel design, namely the Social Interaction Extractor, to learn the
spatial and social interaction features of pedestrians. Most previous works
used ETH and UCY datasets which include five scenes but do not cover urban
traffic scenarios extensively for training and evaluation. In this paper, we
use the recently released large-scale Waymo Open Dataset in urban traffic
scenarios, which includes 374 urban training scenes and 76 urban testing scenes
to analyze the performance of our proposed algorithm in comparison to the
state-of-the-art (SOTA) models. The results show that our algorithm outperforms
SOTA algorithms such as Social-LSTM, Social-GAN, and Social-STGCNN on both
Average Displacement Error (ADE) and Final Displacement Error (FDE).
Furthermore, our Social-IWSTCNN is 54.8 times faster in data pre-processing
speed, and 4.7 times faster in total test speed than the current best SOTA
algorithm Social-STGCNN.
","[{'version': 'v1', 'created': 'Wed, 26 May 2021 09:53:19 GMT'}]",2024-03-20,"[['Zhang', 'Chi', ''], ['Berger', 'Christian', ''], ['Dozza', 'Marco', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
189,2105.12833,Ayaan Haque,"Sajiv Shah, Ayaan Haque, Fei Liu","Simulated Data Generation Through Algorithmic Force Coefficient
  Estimation for AI-Based Robotic Projectile Launch Modeling",not relevant work,,,,cs.RO cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
","[{'version': 'v1', 'created': 'Sun, 9 May 2021 18:47:45 GMT'}, {'version': 'v2', 'created': 'Fri, 28 May 2021 02:03:32 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Jul 2021 21:30:40 GMT'}, {'version': 'v4', 'created': 'Fri, 26 Jan 2024 21:42:30 GMT'}]",2024-01-30,"[['Shah', 'Sajiv', ''], ['Haque', 'Ayaan', ''], ['Liu', 'Fei', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
190,2105.13637,Daogao Liu,"Yin Tat Lee, Daogao Liu, Zhou Lu",The Power of Sampling: Dimension-free Risk Bounds in Private ERM,We add the dimension-independent upper bounds results,,,,cs.LG cs.CR math.OC,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Differentially private empirical risk minimization (DP-ERM) is a fundamental
problem in private optimization. While the theory of DP-ERM is well-studied, as
large-scale models become prevalent, traditional DP-ERM methods face new
challenges, including (1) the prohibitive dependence on the ambient dimension,
(2) the highly non-smooth objective functions, (3) costly first-order gradient
oracles. Such challenges demand rethinking existing DP-ERM methodologies. In
this work, we show that the regularized exponential mechanism combined with
existing samplers can address these challenges altogether: under the standard
unconstrained domain and low-rank gradients assumptions, our algorithm can
achieve rank-dependent risk bounds for non-smooth convex objectives using only
zeroth order oracles, which was not accomplished by prior methods. This
highlights the power of sampling in differential privacy. We further construct
lower bounds, demonstrating that when gradients are full-rank, there is no
separation between the constrained and unconstrained settings. Our lower bound
is derived from a general black-box reduction from unconstrained to the
constrained domain and an improved lower bound in the constrained setting,
which might be of independent interest.
","[{'version': 'v1', 'created': 'Fri, 28 May 2021 07:28:24 GMT'}, {'version': 'v2', 'created': 'Wed, 9 Feb 2022 19:58:53 GMT'}, {'version': 'v3', 'created': 'Fri, 22 Apr 2022 20:42:48 GMT'}, {'version': 'v4', 'created': 'Mon, 3 Jun 2024 21:31:18 GMT'}]",2024-06-05,"[['Lee', 'Yin Tat', ''], ['Liu', 'Daogao', ''], ['Lu', 'Zhou', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
191,2105.15106,Shuanghong Shen,"Shuanghong Shen, Qi Liu, Zhenya Huang, Yonghe Zheng, Minghao Yin,
  Minjuan Wang, and Enhong Chen","A Survey of Knowledge Tracing: Models, Variants, and Applications",22 pages,IEEE Transactions on Learning Technologies. 17(2024)1898-1919,10.1109/TLT.2024.3383325,,cs.CY cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modern online education has the capacity to provide intelligent educational
services by automatically analyzing substantial amounts of student behavioral
data. Knowledge Tracing (KT) is one of the fundamental tasks for student
behavioral data analysis, aiming to monitor students' evolving knowledge state
during their problem-solving process. In recent years, a substantial number of
studies have concentrated on this rapidly growing field, significantly
contributing to its advancements. In this survey, we will conduct a thorough
investigation of these progressions. Firstly, we present three types of
fundamental KT models with distinct technical routes. Subsequently, we review
extensive variants of the fundamental KT models that consider more stringent
learning assumptions. Moreover, the development of KT cannot be separated from
its applications, thereby we present typical KT applications in various
scenarios. To facilitate the work of researchers and practitioners in this
field, we have developed two open-source algorithm libraries: EduData that
enables the download and preprocessing of KT-related datasets, and EduKTM that
provides an extensible and unified implementation of existing mainstream KT
models. Finally, we discuss potential directions for future research in this
rapidly growing field. We hope that the current survey will assist both
researchers and practitioners in fostering the development of KT, thereby
benefiting a broader range of students.
","[{'version': 'v1', 'created': 'Thu, 6 May 2021 13:05:55 GMT'}, {'version': 'v2', 'created': 'Tue, 1 Jun 2021 04:43:30 GMT'}, {'version': 'v3', 'created': 'Thu, 9 Feb 2023 09:40:45 GMT'}, {'version': 'v4', 'created': 'Thu, 11 Apr 2024 02:57:17 GMT'}]",2024-07-16,"[['Shen', 'Shuanghong', ''], ['Liu', 'Qi', ''], ['Huang', 'Zhenya', ''], ['Zheng', 'Yonghe', ''], ['Yin', 'Minghao', ''], ['Wang', 'Minjuan', ''], ['Chen', 'Enhong', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
192,2106.00906,Daniel McKenzie,"Daniel McKenzie, Howard Heaton, Qiuwei Li, Samy Wu Fung, Stanley
  Osher, Wotao Yin",Operator Splitting for Learning to Predict Equilibria in Convex Games,To appear in SIMODS,,,,cs.LG cs.GT math.OC,http://creativecommons.org/licenses/by/4.0/,"  Systems of competing agents can often be modeled as games. Assuming
rationality, the most likely outcomes are given by an equilibrium (e.g. a Nash
equilibrium). In many practical settings, games are influenced by context, i.e.
additional data beyond the control of any agent (e.g. weather for traffic and
fiscal policy for market economies). Often the exact game mechanics are
unknown, yet vast amounts of historical data consisting of (context,
equilibrium) pairs are available, raising the possibility of learning a solver
which predicts the equilibria given only the context. We introduce Nash Fixed
Point Networks (N-FPNs), a class of neural networks that naturally output
equilibria. Crucially, N- FPNs employ a constraint decoupling scheme to handle
complicated agent action sets while avoiding expensive projections.
Empirically, we find N-FPNs are compatible with the recently developed
Jacobian-Free Backpropagation technique for training implicit networks, making
them significantly faster and easier to train than prior models. Our
experiments show N-FPNs are capable of scaling to problems orders of magnitude
larger than existing learned game solvers.
","[{'version': 'v1', 'created': 'Wed, 2 Jun 2021 02:55:46 GMT'}, {'version': 'v2', 'created': 'Thu, 3 Feb 2022 20:40:23 GMT'}, {'version': 'v3', 'created': 'Wed, 8 Nov 2023 22:00:48 GMT'}, {'version': 'v4', 'created': 'Tue, 11 Jun 2024 23:32:53 GMT'}]",2024-06-13,"[['McKenzie', 'Daniel', ''], ['Heaton', 'Howard', ''], ['Li', 'Qiuwei', ''], ['Fung', 'Samy Wu', ''], ['Osher', 'Stanley', ''], ['Yin', 'Wotao', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
193,2106.02225,Shufeng Kong,"Shufeng Kong, Dan Guevarra, Carla P. Gomes, John M. Gregoire","Materials Representation and Transfer Learning for Multi-Property
  Prediction",This is accepted at the Applied Physics Reviews journal,,10.1063/5.0047066,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The adoption of machine learning in materials science has rapidly transformed
materials property prediction. Hurdles limiting full capitalization of recent
advancements in machine learning include the limited development of methods to
learn the underlying interactions of multiple elements, as well as the
relationships among multiple properties, to facilitate property prediction in
new composition spaces. To address these issues, we introduce the Hierarchical
Correlation Learning for Multi-property Prediction (H-CLMP) framework that
seamlessly integrates (i) prediction using only a material's composition, (ii)
learning and exploitation of correlations among target properties in
multi-target regression, and (iii) leveraging training data from tangential
domains via generative transfer learning. The model is demonstrated for
prediction of spectral optical absorption of complex metal oxides spanning 69
3-cation metal oxide composition spaces. H-CLMP accurately predicts non-linear
composition-property relationships in composition spaces for which no training
data is available, which broadens the purview of machine learning to the
discovery of materials with exceptional properties. This achievement results
from the principled integration of latent embedding learning, property
correlation learning, generative transfer learning, and attention models. The
best performance is obtained using H-CLMP with Transfer learning (H-CLMP(T))
wherein a generative adversarial network is trained on computational density of
states data and deployed in the target domain to augment prediction of optical
absorption from composition. H-CLMP(T) aggregates multiple knowledge sources
with a framework that is well-suited for multi-target regression across the
physical sciences.
","[{'version': 'v1', 'created': 'Fri, 4 Jun 2021 03:00:34 GMT'}, {'version': 'v2', 'created': 'Mon, 7 Jun 2021 05:36:36 GMT'}, {'version': 'v3', 'created': 'Fri, 18 Jun 2021 03:03:23 GMT'}]",2024-06-12,"[['Kong', 'Shufeng', ''], ['Guevarra', 'Dan', ''], ['Gomes', 'Carla P.', ''], ['Gregoire', 'John M.', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
194,2106.02626,Gabriel B\'ena,"Gabriel B\'ena, Dan F. M. Goodman",Dynamics of specialization in neural modules under resource constraints,,,,,q-bio.NC cs.AI cs.LG cs.NE,http://creativecommons.org/licenses/by/4.0/,"  It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
","[{'version': 'v1', 'created': 'Fri, 4 Jun 2021 17:39:36 GMT'}, {'version': 'v2', 'created': 'Thu, 27 Jul 2023 17:19:03 GMT'}, {'version': 'v3', 'created': 'Sat, 30 Sep 2023 17:09:57 GMT'}, {'version': 'v4', 'created': 'Tue, 3 Oct 2023 08:25:16 GMT'}, {'version': 'v5', 'created': 'Sat, 18 May 2024 14:10:05 GMT'}]",2024-05-21,"[['Béna', 'Gabriel', ''], ['Goodman', 'Dan F. M.', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
195,2106.03027,Rahul Ramesh,"Rahul Ramesh, Pratik Chaudhari","Model Zoo: A Growing ""Brain"" That Learns Continually",,,,"Proc. of the International Conference of Learning Representations
  (ICLR) 2022",cs.LG,http://creativecommons.org/licenses/by/4.0/,"  This paper argues that continual learning methods can benefit by splitting
the capacity of the learner across multiple models. We use statistical learning
theory and experimental analysis to show how multiple tasks can interact with
each other in a non-trivial fashion when a single model is trained on them. The
generalization error on a particular task can improve when it is trained with
synergistic tasks, but can also deteriorate when trained with competing tasks.
This theory motivates our method named Model Zoo which, inspired from the
boosting literature, grows an ensemble of small models, each of which is
trained during one episode of continual learning. We demonstrate that Model Zoo
obtains large gains in accuracy on a variety of continual learning benchmark
problems. Code is available at
https://github.com/grasp-lyrl/modelzoo_continual.
","[{'version': 'v1', 'created': 'Sun, 6 Jun 2021 04:25:09 GMT'}, {'version': 'v2', 'created': 'Wed, 8 Dec 2021 21:48:21 GMT'}, {'version': 'v3', 'created': 'Wed, 15 Jun 2022 22:16:33 GMT'}]",2024-05-07,"[['Ramesh', 'Rahul', ''], ['Chaudhari', 'Pratik', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
196,2106.03395,Laurens Sluijterman,"Laurens Sluijterman, Eric Cator, Tom Heskes","How to Evaluate Uncertainty Estimates in Machine Learning for
  Regression?","14 pages, 10 figures",,10.1016/j.neunet.2024.106203,,stat.ML cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As neural networks become more popular, the need for accompanying uncertainty
estimates increases. There are currently two main approaches to test the
quality of these estimates. Most methods output a density. They can be compared
by evaluating their loglikelihood on a test set. Other methods output a
prediction interval directly. These methods are often tested by examining the
fraction of test points that fall inside the corresponding prediction
intervals. Intuitively both approaches seem logical. However, we demonstrate
through both theoretical arguments and simulations that both ways of evaluating
the quality of uncertainty estimates have serious flaws. Firstly, both
approaches cannot disentangle the separate components that jointly create the
predictive uncertainty, making it difficult to evaluate the quality of the
estimates of these components. Secondly, a better loglikelihood does not
guarantee better prediction intervals, which is what the methods are often used
for in practice. Moreover, the current approach to test prediction intervals
directly has additional flaws. We show why it is fundamentally flawed to test a
prediction or confidence interval on a single test set. At best, marginal
coverage is measured, implicitly averaging out overconfident and underconfident
predictions. A much more desirable property is pointwise coverage, requiring
the correct coverage for each prediction. We demonstrate through practical
examples that these effects can result in favoring a method, based on the
predictive uncertainty, that has undesirable behaviour of the confidence or
prediction intervals. Finally, we propose a simulation-based testing approach
that addresses these problems while still allowing easy comparison between
different methods.
","[{'version': 'v1', 'created': 'Mon, 7 Jun 2021 07:47:46 GMT'}, {'version': 'v2', 'created': 'Thu, 3 Aug 2023 12:53:40 GMT'}]",2024-06-05,"[['Sluijterman', 'Laurens', ''], ['Cator', 'Eric', ''], ['Heskes', 'Tom', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
197,2106.04618,Laurens Bliek,"Laurens Bliek, Arthur Guijt, Rickard Karlsson, Sicco Verwer, Mathijs
  de Weerdt","EXPObench: Benchmarking Surrogate-based Optimisation Algorithms on
  Expensive Black-box Functions",33 pages,,10.1016/j.asoc.2023.110744,,cs.LG cs.NE math.OC,http://creativecommons.org/licenses/by/4.0/,"  Surrogate algorithms such as Bayesian optimisation are especially designed
for black-box optimisation problems with expensive objectives, such as
hyperparameter tuning or simulation-based optimisation. In the literature,
these algorithms are usually evaluated with synthetic benchmarks which are well
established but have no expensive objective, and only on one or two real-life
applications which vary wildly between papers. There is a clear lack of
standardisation when it comes to benchmarking surrogate algorithms on
real-life, expensive, black-box objective functions. This makes it very
difficult to draw conclusions on the effect of algorithmic contributions and to
give substantial advice on which method to use when. A new benchmark library,
EXPObench, provides first steps towards such a standardisation. The library is
used to provide an extensive comparison of six different surrogate algorithms
on four expensive optimisation problems from different real-life applications.
This has led to new insights regarding the relative importance of exploration,
the evaluation time of the objective, and the used model. We also provide rules
of thumb for which surrogate algorithm to use in which situation. A further
contribution is that we make the algorithms and benchmark problem instances
publicly available, contributing to more uniform analysis of surrogate
algorithms. Most importantly, we include the performance of the six algorithms
on all evaluated problem instances. This results in a unique new dataset that
lowers the bar for researching new methods as the number of expensive
evaluations required for comparison is significantly reduced.
","[{'version': 'v1', 'created': 'Tue, 8 Jun 2021 18:17:42 GMT'}, {'version': 'v2', 'created': 'Thu, 1 Dec 2022 16:37:41 GMT'}]",2024-03-14,"[['Bliek', 'Laurens', ''], ['Guijt', 'Arthur', ''], ['Karlsson', 'Rickard', ''], ['Verwer', 'Sicco', ''], ['de Weerdt', 'Mathijs', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
198,2106.0541,Hadi Hojjati,"Hadi Hojjati, Narges Armanfard","DASVDD: Deep Autoencoding Support Vector Data Descriptor for Anomaly
  Detection",,"IEEE Transactions on Knowledge and Data Engineering (Early
  Access), 2023, 1-12",10.1109/TKDE.2023.3328882,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
","[{'version': 'v1', 'created': 'Wed, 9 Jun 2021 21:57:41 GMT'}, {'version': 'v2', 'created': 'Wed, 17 Nov 2021 22:15:35 GMT'}, {'version': 'v3', 'created': 'Wed, 15 Nov 2023 03:11:04 GMT'}, {'version': 'v4', 'created': 'Sat, 20 Jan 2024 20:37:22 GMT'}]",2024-01-23,"[['Hojjati', 'Hadi', ''], ['Armanfard', 'Narges', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
199,2106.06049,Sowmya S Sundaram,"Deepak P, Sowmya S Sundaram",FiSH: Fair Spatial Hotspots,,"Data Mining and Knowledge Discovery 37, 1374 - 1403 (2023)",10.1007/s10618-022-00887-4,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Pervasiveness of tracking devices and enhanced availability of spatially
located data has deepened interest in using them for various policy
interventions, through computational data analysis tasks such as spatial hot
spot detection. In this paper, we consider, for the first time to our best
knowledge, fairness in detecting spatial hot spots. We motivate the need for
ensuring fairness through statistical parity over the collective population
covered across chosen hot spots. We then characterize the task of identifying a
diverse set of solutions in the noteworthiness-fairness trade-off spectrum, to
empower the user to choose a trade-off justified by the policy domain. Being a
novel task formulation, we also develop a suite of evaluation metrics for fair
hot spots, motivated by the need to evaluate pertinent aspects of the task. We
illustrate the computational infeasibility of identifying fair hot spots using
naive and/or direct approaches and devise a method, codenamed {\it FiSH}, for
efficiently identifying high-quality, fair and diverse sets of spatial hot
spots. FiSH traverses the tree-structured search space using heuristics that
guide it towards identifying effective and fair sets of spatial hot spots.
Through an extensive empirical analysis over a real-world dataset from the
domain of human development, we illustrate that FiSH generates high-quality
solutions at fast response times.
","[{'version': 'v1', 'created': 'Tue, 1 Jun 2021 10:29:03 GMT'}]",2024-04-18,"[['P', 'Deepak', ''], ['Sundaram', 'Sowmya S', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
200,2106.06682,Senwei Liang,Senwei Liang and Shixiao W. Jiang and John Harlim and Haizhao Yang,Solving PDEs on Unknown Manifolds with Machine Learning,,,,,math.NA cs.LG cs.NA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper proposes a mesh-free computational framework and machine learning
theory for solving elliptic PDEs on unknown manifolds, identified with point
clouds, based on diffusion maps (DM) and deep learning. The PDE solver is
formulated as a supervised learning task to solve a least-squares regression
problem that imposes an algebraic equation approximating a PDE (and boundary
conditions if applicable). This algebraic equation involves a graph-Laplacian
type matrix obtained via DM asymptotic expansion, which is a consistent
estimator of second-order elliptic differential operators. The resulting
numerical method is to solve a highly non-convex empirical risk minimization
problem subjected to a solution from a hypothesis space of neural networks
(NNs). In a well-posed elliptic PDE setting, when the hypothesis space consists
of neural networks with either infinite width or depth, we show that the global
minimizer of the empirical loss function is a consistent solution in the limit
of large training data. When the hypothesis space is a two-layer neural
network, we show that for a sufficiently large width, gradient descent can
identify a global minimizer of the empirical loss function. Supporting
numerical examples demonstrate the convergence of the solutions, ranging from
simple manifolds with low and high co-dimensions, to rough surfaces with and
without boundaries. We also show that the proposed NN solver can robustly
generalize the PDE solution on new data points with generalization errors that
are almost identical to the training errors, superseding a Nystrom-based
interpolation method.
","[{'version': 'v1', 'created': 'Sat, 12 Jun 2021 03:55:15 GMT'}, {'version': 'v2', 'created': 'Fri, 10 Jun 2022 17:48:38 GMT'}, {'version': 'v3', 'created': 'Fri, 6 Oct 2023 23:03:17 GMT'}, {'version': 'v4', 'created': 'Tue, 27 Feb 2024 18:29:18 GMT'}]",2024-02-28,"[['Liang', 'Senwei', ''], ['Jiang', 'Shixiao W.', ''], ['Harlim', 'John', ''], ['Yang', 'Haizhao', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
201,2106.06957,Nan Liu,"Feng Xie, Yilin Ning, Han Yuan, Benjamin Alan Goldstein, Marcus Eng
  Hock Ong, Nan Liu, Bibhas Chakraborty","AutoScore-Survival: Developing interpretable machine learning-based
  time-to-event scores with right-censored survival data",,,10.1016/j.jbi.2021.103959,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
","[{'version': 'v1', 'created': 'Sun, 13 Jun 2021 10:21:45 GMT'}]",2024-06-11,"[['Xie', 'Feng', ''], ['Ning', 'Yilin', ''], ['Yuan', 'Han', ''], ['Goldstein', 'Benjamin Alan', ''], ['Ong', 'Marcus Eng Hock', ''], ['Liu', 'Nan', ''], ['Chakraborty', 'Bibhas', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
202,2106.0716,Kim Hammar,Kim Hammar and Rolf Stadler,Learning Intrusion Prevention Policies through Optimal Stopping,Fix typos,"2021 17th International Conference on Network and Service
  Management (CNSM)",10.23919/CNSM52442.2021.9615542,,cs.AI cs.CR cs.LG cs.NI,http://creativecommons.org/licenses/by-sa/4.0/,"  We study automated intrusion prevention using reinforcement learning. In a
novel approach, we formulate the problem of intrusion prevention as an optimal
stopping problem. This formulation allows us insight into the structure of the
optimal policies, which turn out to be threshold based. Since the computation
of the optimal defender policy using dynamic programming is not feasible for
practical cases, we approximate the optimal policy through reinforcement
learning in a simulation environment. To define the dynamics of the simulation,
we emulate the target infrastructure and collect measurements. Our evaluations
show that the learned policies are close to optimal and that they indeed can be
expressed using thresholds.
","[{'version': 'v1', 'created': 'Mon, 14 Jun 2021 04:45:37 GMT'}, {'version': 'v2', 'created': 'Fri, 30 Jul 2021 16:06:36 GMT'}, {'version': 'v3', 'created': 'Tue, 10 Aug 2021 06:17:40 GMT'}, {'version': 'v4', 'created': 'Sat, 21 Aug 2021 12:20:33 GMT'}, {'version': 'v5', 'created': 'Wed, 25 Aug 2021 17:41:11 GMT'}, {'version': 'v6', 'created': 'Fri, 3 Sep 2021 14:53:05 GMT'}, {'version': 'v7', 'created': 'Thu, 9 Sep 2021 15:53:29 GMT'}, {'version': 'v8', 'created': 'Thu, 10 Feb 2022 15:55:16 GMT'}]",2024-04-23,"[['Hammar', 'Kim', ''], ['Stadler', 'Rolf', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
203,2106.08748,Suman Sapkota,Suman Sapkota and Binod Bhattarai,Input Invex Neural Network,"45 pages, 23 figures, 10 tables",,,,cs.LG cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Connected decision boundaries are useful in several tasks like image
segmentation, clustering, alpha-shape or defining a region in nD-space.
However, the machine learning literature lacks methods for generating connected
decision boundaries using neural networks. Thresholding an invex function, a
generalization of a convex function, generates such decision boundaries. This
paper presents two methods for constructing invex functions using neural
networks. The first approach is based on constraining a neural network with
Gradient Clipped-Gradient Penality (GCGP), where we clip and penalise the
gradients. In contrast, the second one is based on the relationship of the
invex function to the composition of invertible and convex functions. We employ
connectedness as a basic interpretation method and create connected
region-based classifiers. We show that multiple connected set based classifiers
can approximate any classification function. In the experiments section, we use
our methods for classification tasks using an ensemble of 1-vs-all models as
well as using a single multiclass model on small-scale datasets. The
experiments show that connected set-based classifiers do not pose any
disadvantage over ordinary neural network classifiers, but rather, enhance
their interpretability. We also did an extensive study on the properties of
invex function and connected sets for interpretability and network morphism
with experiments on toy and real-world data sets. Our study suggests that invex
function is fundamental to understanding and applying locality and
connectedness of input space which is useful for various downstream tasks.
","[{'version': 'v1', 'created': 'Wed, 16 Jun 2021 12:48:55 GMT'}, {'version': 'v2', 'created': 'Wed, 31 Aug 2022 17:16:00 GMT'}, {'version': 'v3', 'created': 'Mon, 6 Feb 2023 08:05:39 GMT'}, {'version': 'v4', 'created': 'Sat, 3 Aug 2024 16:36:56 GMT'}]",2024-08-06,"[['Sapkota', 'Suman', ''], ['Bhattarai', 'Binod', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
204,2106.08903,Johannes Gasteiger,"Johannes Gasteiger, Florian Becker, Stephan G\""unnemann",GemNet: Universal Directional Graph Neural Networks for Molecules,"Published as a conference paper at NeurIPS 2021. Author name changed
  from Johannes Klicpera to Johannes Gasteiger",,,,physics.comp-ph cs.LG physics.chem-ph stat.ML,http://creativecommons.org/licenses/by/4.0/,"  Effectively predicting molecular interactions has the potential to accelerate
molecular dynamics by multiple orders of magnitude and thus revolutionize
chemical simulations. Graph neural networks (GNNs) have recently shown great
successes for this task, overtaking classical methods based on fixed molecular
kernels. However, they still appear very limited from a theoretical
perspective, since regular GNNs cannot distinguish certain types of graphs. In
this work we close this gap between theory and practice. We show that GNNs with
spherical representations are indeed universal approximators for predictions
that are invariant to translation, and equivariant to permutation and rotation.
We then discretize such GNNs via directed edge embeddings and two-hop message
passing, and incorporate multiple structural improvements to arrive at the
geometric message passing neural network (GemNet). We demonstrate the benefits
of the proposed changes in multiple ablation studies. GemNet outperforms
previous models on the COLL, MD17, and OC20 datasets by 34%, 41%, and 20%,
respectively, and performs especially well on the most challenging molecules.
Our implementation is available online.
","[{'version': 'v1', 'created': 'Wed, 2 Jun 2021 15:44:55 GMT'}, {'version': 'v10', 'created': 'Sat, 22 Jun 2024 14:11:10 GMT'}, {'version': 'v2', 'created': 'Tue, 22 Jun 2021 15:37:29 GMT'}, {'version': 'v3', 'created': 'Sun, 24 Oct 2021 19:50:19 GMT'}, {'version': 'v4', 'created': 'Mon, 8 Nov 2021 19:00:09 GMT'}, {'version': 'v5', 'created': 'Mon, 29 Nov 2021 19:25:58 GMT'}, {'version': 'v6', 'created': 'Tue, 11 Jan 2022 10:39:51 GMT'}, {'version': 'v7', 'created': 'Tue, 18 Jan 2022 11:11:54 GMT'}, {'version': 'v8', 'created': 'Fri, 21 Jan 2022 18:42:22 GMT'}, {'version': 'v9', 'created': 'Tue, 5 Apr 2022 12:18:48 GMT'}]",2024-06-25,"[['Gasteiger', 'Johannes', ''], ['Becker', 'Florian', ''], ['Günnemann', 'Stephan', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
205,2106.09499,Walter Del Pozzo,"Alessandro Martini, Stefano Schmidt, Gregory Ashton, Walter Del Pozzo","Maximum Entropy Spectral Analysis: an application to gravitational waves
  data analysis","15 pages, 11 figures",,,,stat.ME astro-ph.IM physics.data-an,http://creativecommons.org/licenses/by/4.0/,"  The Maximum Entropy Spectral Analysis (MESA) method, developed by Burg,
offers a powerful tool for spectral estimation of a time-series. It relies on
Jaynes' maximum entropy principle, allowing the spectrum of a stochastic
process to be inferred using the coefficients of an autoregressive process
AR($p$) of order $p$. A closed-form recursive solution provides estimates for
both the autoregressive coefficients and the order $p$ of the process. We
provide a ready-to-use implementation of this algorithm in a Python package
called \texttt{memspectrum}, characterized through power spectral density (PSD)
analysis on synthetic data with known PSD and comparisons of different criteria
for stopping the recursion. Additionally, we compare the performance of our
implementation with the ubiquitous Welch algorithm, using synthetic data
generated from the GW150914 strain spectrum released by the LIGO-Virgo-Kagra
collaboration. Our findings indicate that Burg's method provides PSD estimates
with systematically lower variance and bias. This is particularly manifest in
the case of a small (O($5000$)) number of data points, making Burg's method
most suitable to work in this regime. Since this is close to the typical length
of analysed gravitational waves data, improving the estimate of the PSD in this
regime leads to more reliable posterior profiles for the system under study. We
conclude our investigation by utilising MESA, and its particularly easy
parametrisation where the only free parameter is the order $p$ of the AR
process, to marginalise over the interferometers noise PSD in conjunction with
inferring the parameters of GW150914.
","[{'version': 'v1', 'created': 'Thu, 17 Jun 2021 13:48:57 GMT'}, {'version': 'v2', 'created': 'Mon, 1 Jul 2024 13:18:50 GMT'}]",2024-07-02,"[['Martini', 'Alessandro', ''], ['Schmidt', 'Stefano', ''], ['Ashton', 'Gregory', ''], ['Del Pozzo', 'Walter', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
206,2106.10025,Daekyung Lee,"Daekyung Lee, Wonguk Cho, Heetae Kim, Gunn Kim, Hyeong-Chai Jeong,
  Beom Jun Kim",Unveiling Node Mass through Self-Consistent Gravity Model,"18 pages, 4 figures, 1 table",,,,physics.data-an,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The gravity model, inspired by Newton's law of universal gravitation, has
long served as a primary tool for interpreting trade flows between countries,
using a country's economic `mass' as a key determinant. Despite its wide
application, the definition of `mass' within this model remains ambiguous. It
is often approximated using indicators like GDP, which may not accurately
reflect a country's true trade potential. Here, we introduce a data-driven,
self-consistent numerical approach that redefines `mass' from a static proxy to
a dynamic attribute inferred directly from flow data. We infer mass
distribution and interaction nature through our method, mirroring Newton's
approach to understanding gravity. Our methodology accurately identifies
predefined embeddings and reconstructs system attributes when applied to
synthetic flow data, demonstrating its strong predictive power and
adaptability. Further application to real-world trade networks yields critical
insights, revealing the spatial spectrum of trade flows and the economic mass
of countries, two key features unexplored in depth by existing models. Our
methodology not only enables accurate reconstruction of the original flow but
also allows for a deep understanding of the unique capabilities of each node
within the network. This study marks a significant shift in the understanding
and application of the gravity model, providing a more comprehensive tool for
analyzing complex systems and uncovering new insights into various fields,
including global trade, traffic engineering, epidemic disease prevention, and
infrastructure design.
","[{'version': 'v1', 'created': 'Fri, 18 Jun 2021 10:02:48 GMT'}, {'version': 'v2', 'created': 'Wed, 23 Jun 2021 04:42:57 GMT'}, {'version': 'v3', 'created': 'Mon, 5 Aug 2024 08:34:20 GMT'}]",2024-08-06,"[['Lee', 'Daekyung', ''], ['Cho', 'Wonguk', ''], ['Kim', 'Heetae', ''], ['Kim', 'Gunn', ''], ['Jeong', 'Hyeong-Chai', ''], ['Kim', 'Beom Jun', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
207,2106.10479,Yang Tan,"Yang Tan, Yang Li, Shao-Lun Huang",Practical Transferability Estimation for Image Classification Tasks,"This paper is not the latest version. Please refer to
  Transferability-Guided Cross-Domain Cross-Task Transfer Learning (IEEE
  TNNLS'24) for more
  details.https://ieeexplore.ieee.org/abstract/document/10420486",,,,cs.CV cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Transferability estimation is an essential problem in transfer learning to
predict how good the performance is when transferring a source model (or source
task) to a target task. Recent analytical transferability metrics have been
widely used for source model selection and multi-task learning. A major
challenge is how to make transfereability estimation robust under the
cross-domain cross-task settings. The recently proposed OTCE score solves this
problem by considering both domain and task differences, with the help of
transfer experiences on auxiliary tasks, which causes an efficiency overhead.
In this work, we propose a practical transferability metric called JC-NCE score
that dramatically improves the robustness of the task difference estimation in
OTCE, thus removing the need for auxiliary tasks. Specifically, we build the
joint correspondences between source and target data via solving an optimal
transport problem with a ground cost considering both the sample distance and
label distance, and then compute the transferability score as the negative
conditional entropy of the matched labels. Extensive validations under the
intra-dataset and inter-dataset transfer settings demonstrate that our JC-NCE
score outperforms the auxiliary-task free version of OTCE for 7% and 12%,
respectively, and is also more robust than other existing transferability
metrics on average.
","[{'version': 'v1', 'created': 'Sat, 19 Jun 2021 11:59:11 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Jun 2021 10:26:37 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Feb 2024 06:29:09 GMT'}]",2024-03-01,"[['Tan', 'Yang', ''], ['Li', 'Yang', ''], ['Huang', 'Shao-Lun', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
208,2106.10761,Moshe Shenfeld,Moshe Shenfeld and Katrina Ligett,Generalization in the Face of Adaptivity: A Bayesian Perspective,,"Advances in Neural Information Processing Systems, 36 (2024)",,,cs.LG stat.ML,http://creativecommons.org/licenses/by/4.0/,"  Repeated use of a data sample via adaptively chosen queries can rapidly lead
to overfitting, wherein the empirical evaluation of queries on the sample
significantly deviates from their mean with respect to the underlying data
distribution. It turns out that simple noise addition algorithms suffice to
prevent this issue, and differential privacy-based analysis of these algorithms
shows that they can handle an asymptotically optimal number of queries.
However, differential privacy's worst-case nature entails scaling such noise to
the range of the queries even for highly-concentrated queries, or introducing
more complex algorithms.
  In this paper, we prove that straightforward noise-addition algorithms
already provide variance-dependent guarantees that also extend to unbounded
queries. This improvement stems from a novel characterization that illuminates
the core problem of adaptive data analysis. We show that the harm of adaptivity
results from the covariance between the new query and a Bayes factor-based
measure of how much information about the data sample was encoded in the
responses given to past queries. We then leverage this characterization to
introduce a new data-dependent stability notion that can bound this covariance.
","[{'version': 'v1', 'created': 'Sun, 20 Jun 2021 22:06:44 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Jun 2023 20:24:58 GMT'}, {'version': 'v3', 'created': 'Wed, 3 Apr 2024 19:39:10 GMT'}]",2024-04-26,"[['Shenfeld', 'Moshe', ''], ['Ligett', 'Katrina', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
209,2106.10901,Quim Motger,"Quim Motger, Xavier Franch and Jordi Marco","Software-Based Dialogue Systems: Survey, Taxonomy and Challenges",,,10.1145/3527450,,cs.CL cs.SE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The use of natural language interfaces in the field of human-computer
interaction is undergoing intense study through dedicated scientific and
industrial research. The latest contributions in the field, including deep
learning approaches like recurrent neural networks, the potential of
context-aware strategies and user-centred design approaches, have brought back
the attention of the community to software-based dialogue systems, generally
known as conversational agents or chatbots. Nonetheless, and given the novelty
of the field, a generic, context-independent overview on the current state of
research of conversational agents covering all research perspectives involved
is missing. Motivated by this context, this paper reports a survey of the
current state of research of conversational agents through a systematic
literature review of secondary studies. The conducted research is designed to
develop an exhaustive perspective through a clear presentation of the
aggregated knowledge published by recent literature within a variety of
domains, research focuses and contexts. As a result, this research proposes a
holistic taxonomy of the different dimensions involved in the conversational
agents' field, which is expected to help researchers and to lay the groundwork
for future research in the field of natural language interfaces.
","[{'version': 'v1', 'created': 'Mon, 21 Jun 2021 07:41:44 GMT'}, {'version': 'v2', 'created': 'Tue, 6 Feb 2024 10:22:52 GMT'}]",2024-02-07,"[['Motger', 'Quim', ''], ['Franch', 'Xavier', ''], ['Marco', 'Jordi', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
210,2106.11469,Johannes Blaschke P,"Johannes P. Blaschke (1), Aaron S. Brewster (2), Daniel W. Paley (2),
  Derek Mendez (2), Asmit Bhowmick (2), Nicholas K. Sauter (2), Wilko Kr\""oger
  (3), Murali Shankar (3), Bjoern Enders (1), Deborah Bard (1) ((1) National
  Energy Research Scientific Computing Center, Lawrence Berkeley National
  Laboratory, USA, (2) Molecular Biophysics and Integrated Bioimaging Division,
  Lawrence Berkeley National Laboratory, USA, (3) SLAC National Accelerator
  Laboratory, USA)","Real-Time XFEL Data Analysis at SLAC and NERSC: a Trial Run of Nascent
  Exascale Experimental Data Analysis",,,10.1002/cpe.8019,,cs.DC physics.data-an,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  X-ray scattering experiments using Free Electron Lasers (XFELs) are a
powerful tool to determine the molecular structure and function of unknown
samples (such as COVID-19 viral proteins). XFEL experiments are a challenge to
computing in two ways: i) due to the high cost of running XFELs, a fast
turnaround time from data acquisition to data analysis is essential to make
informed decisions on experimental protocols; ii) data collection rates are
growing exponentially, requiring new scalable algorithms. Here we report our
experiences analyzing data from two experiments at the Linac Coherent Light
Source (LCLS) during September 2020. Raw data were analyzed on NERSC's Cori
XC40 system, using the Superfacility paradigm: our workflow automatically moves
raw data between LCLS and NERSC, where it is analyzed using the software
package CCTBX. We achieved real time data analysis with a turnaround time from
data acquisition to full molecular reconstruction in as little as 10 min --
sufficient time for the experiment's operators to make informed decisions. By
hosting the data analysis on Cori, and by automating LCLS-NERSC
interoperability, we achieved a data analysis rate which matches the data
acquisition rate. Completing data analysis with 10 mins is a first for XFEL
experiments and an important milestone if we are to keep up with data
collection trends.
","[{'version': 'v1', 'created': 'Tue, 22 Jun 2021 01:09:12 GMT'}, {'version': 'v2', 'created': 'Mon, 16 Aug 2021 22:03:26 GMT'}, {'version': 'v3', 'created': 'Mon, 1 Jan 2024 02:45:43 GMT'}]",2024-03-21,"[['Blaschke', 'Johannes P.', ''], ['Brewster', 'Aaron S.', ''], ['Paley', 'Daniel W.', ''], ['Mendez', 'Derek', ''], ['Bhowmick', 'Asmit', ''], ['Sauter', 'Nicholas K.', ''], ['Kröger', 'Wilko', ''], ['Shankar', 'Murali', ''], ['Enders', 'Bjoern', ''], ['Bard', 'Deborah', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
211,2106.1193,Albin Soutif--Cormerais,"Albin Soutif--Cormerais, Marc Masana, Joost van de Weijer,
  Bart{\l}omiej Twardowski",On the importance of cross-task features for class-incremental learning,"Accepted at the Theory and Foundation of Continual Learning Workshop,
  International Conference on Machine Learning (ICML) 2021. Supplementary
  material included. V4 includes additional results of linear probing of
  intermediate representations that were added to the supplementary",,,,cs.LG cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In class-incremental learning, an agent with limited resources needs to learn
a sequence of classification tasks, forming an ever growing classification
problem, with the constraint of not being able to access data from previous
tasks. The main difference with task-incremental learning, where a task-ID is
available at inference time, is that the learner also needs to perform
cross-task discrimination, i.e. distinguish between classes that have not been
seen together. Approaches to tackle this problem are numerous and mostly make
use of an external memory (buffer) of non-negligible size. In this paper, we
ablate the learning of cross-task features and study its influence on the
performance of basic replay strategies used for class-IL. We also define a new
forgetting measure for class-incremental learning, and see that forgetting is
not the principal cause of low performance. Our experimental results show that
future algorithms for class-incremental learning should not only prevent
forgetting, but also aim to improve the quality of the cross-task features, and
the knowledge transfer between tasks. This is especially important when tasks
contain limited amount of data.
","[{'version': 'v1', 'created': 'Tue, 22 Jun 2021 17:03:15 GMT'}, {'version': 'v2', 'created': 'Fri, 30 Jul 2021 10:20:36 GMT'}, {'version': 'v3', 'created': 'Wed, 29 Sep 2021 07:28:32 GMT'}, {'version': 'v4', 'created': 'Tue, 28 May 2024 11:44:03 GMT'}]",2024-05-29,"[['Soutif--Cormerais', 'Albin', ''], ['Masana', 'Marc', ''], ['van de Weijer', 'Joost', ''], ['Twardowski', 'Bartłomiej', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
212,2106.12382,Xinyi Wang,"Xinyi Wang, Lang Tong","Innovations Autoencoder and its Application in One-class Anomalous
  Sequence Detection",,"The Journal of Machine Learning Research, Vol 23, Issue 1, pp.
  2347-2373, 2022",,,stat.ML cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  An innovations sequence of a time series is a sequence of independent and
identically distributed random variables with which the original time series
has a causal representation. The innovation at a time is statistically
independent of the history of the time series. As such, it represents the new
information contained at present but not in the past. Because of its simple
probability structure, an innovations sequence is the most efficient signature
of the original. Unlike the principle or independent component analysis
representations, an innovations sequence preserves not only the complete
statistical properties but also the temporal order of the original time series.
An long-standing open problem is to find a computationally tractable way to
extract an innovations sequence of non-Gaussian processes. This paper presents
a deep learning approach, referred to as Innovations Autoencoder (IAE), that
extracts innovations sequences using a causal convolutional neural network. An
application of IAE to the one-class anomalous sequence detection problem with
unknown anomaly and anomaly-free models is also presented.
","[{'version': 'v1', 'created': 'Wed, 23 Jun 2021 13:24:17 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Jul 2021 15:37:42 GMT'}]",2024-05-01,"[['Wang', 'Xinyi', ''], ['Tong', 'Lang', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
213,2106.13082,Robert Rosenbaum,Robert Rosenbaum,On the relationship between predictive coding and backpropagation,,,10.1371/journal.pone.0266102,,q-bio.NC cs.LG cs.NE,http://creativecommons.org/licenses/by/4.0/,"  Artificial neural networks are often interpreted as abstract models of
biological neuronal networks, but they are typically trained using the
biologically unrealistic backpropagation algorithm and its variants. Predictive
coding has been proposed as a potentially more biologically realistic
alternative to backpropagation for training neural networks. This manuscript
reviews and extends recent work on the mathematical relationship between
predictive coding and backpropagation for training feedforward artificial
neural networks on supervised learning tasks. Implications of these results for
the interpretation of predictive coding and deep neural networks as models of
biological learning are discussed along with a repository of functions,
Torch2PC, for performing predictive coding with PyTorch neural network models.
","[{'version': 'v1', 'created': 'Sun, 20 Jun 2021 18:22:50 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Jun 2021 11:13:15 GMT'}, {'version': 'v3', 'created': 'Thu, 21 Oct 2021 20:19:55 GMT'}, {'version': 'v4', 'created': 'Fri, 18 Feb 2022 12:28:20 GMT'}, {'version': 'v5', 'created': 'Tue, 16 Apr 2024 19:10:00 GMT'}, {'version': 'v6', 'created': 'Tue, 23 Apr 2024 19:17:10 GMT'}]",2024-04-25,"[['Rosenbaum', 'Robert', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
214,2106.13083,Stefano Forti,"Giuseppe Bisicchia, Stefano Forti, Antonio Brogi",A Declarative Goal-oriented Framework for Smart Environments with LPaaS,,"CEUR Workshop Proceedings Volume 3002, Pages 143 - 157",,,eess.SP cs.AI cs.LO cs.SY eess.SY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Smart environments powered by the Internet of Things aim at improving our
daily lives by automatically tuning ambient parameters (e.g. temperature,
interior light) and by achieving energy savings through self-managing
cyber-physical systems. Commercial solutions, however, only permit setting
simple target goals on those parameters and do not consider mediating
conflicting goals among different users and/or system administrators, and
feature limited compatibility across different IoT verticals. In this article,
we propose a declarative framework to represent smart environments, user-set
goals and customisable mediation policies to reconcile contrasting goals
encompassing multiple IoT systems. An open-source Prolog prototype of the
framework is showcased over two lifelike motivating examples.
","[{'version': 'v1', 'created': 'Fri, 18 Jun 2021 14:03:20 GMT'}]",2024-01-01,"[['Bisicchia', 'Giuseppe', ''], ['Forti', 'Stefano', ''], ['Brogi', 'Antonio', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
215,2106.15775,Motoya Ohnishi,"Motoya Ohnishi, Isao Ishikawa, Kendall Lowrey, Masahiro Ikeda, Sham
  Kakade, Yoshinobu Kawahara",Koopman Spectrum Nonlinear Regulators and Efficient Online Learning,"41 pages, 21 figures","Transactions on Machine Learning Research
  (https://openreview.net/forum?id=thfoUZugvS), 2024",,,cs.LG cs.RO cs.SY eess.SY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most modern reinforcement learning algorithms optimize a cumulative
single-step cost along a trajectory. The optimized motions are often
'unnatural', representing, for example, behaviors with sudden accelerations
that waste energy and lack predictability. In this work, we present a novel
paradigm of controlling nonlinear systems via the minimization of the Koopman
spectrum cost: a cost over the Koopman operator of the controlled dynamics.
This induces a broader class of dynamical behaviors that evolve over stable
manifolds such as nonlinear oscillators, closed loops, and smooth movements. We
demonstrate that some dynamics characterizations that are not possible with a
cumulative cost are feasible in this paradigm, which generalizes the classical
eigenstructure and pole assignments to nonlinear decision making. Moreover, we
present a sample efficient online learning algorithm for our problem that
enjoys a sub-linear regret bound under some structural assumptions.
","[{'version': 'v1', 'created': 'Wed, 30 Jun 2021 02:07:39 GMT'}, {'version': 'v2', 'created': 'Tue, 2 Jul 2024 08:53:08 GMT'}]",2024-07-03,"[['Ohnishi', 'Motoya', ''], ['Ishikawa', 'Isao', ''], ['Lowrey', 'Kendall', ''], ['Ikeda', 'Masahiro', ''], ['Kakade', 'Sham', ''], ['Kawahara', 'Yoshinobu', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
216,2106.15802,Xu Geng,"Zhengfei Zheng, Xu Geng, and Hai Yang","CityNet: A Comprehensive Multi-Modal Urban Dataset for Advanced Research
  in Urban Computing",,,,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Data-driven approaches have emerged as a popular tool for addressing
challenges in urban computing. However, current research efforts have primarily
focused on limited data sources, which fail to capture the complexity of urban
data arising from multiple entities and their interconnections. Therefore, a
comprehensive and multifaceted dataset is required to enable more extensive
studies in urban computing. In this paper, we present CityNet, a multi-modal
urban dataset that incorporates various data, including taxi trajectory,
traffic speed, point of interest (POI), road network, wind, rain, temperature,
and more, from seven cities. We categorize this comprehensive data into three
streams: mobility data, geographical data, and meteorological data. We begin by
detailing the generation process and basic properties of CityNet. Additionally,
we conduct extensive data mining and machine learning experiments, including
spatio-temporal predictions, transfer learning, and reinforcement learning, to
facilitate the use of CityNet. Our experimental results provide benchmarks for
various tasks and methods, and also reveal internal correlations among cities
and tasks within CityNet that can be leveraged to improve spatiotemporal
forecasting performance. Based on our benchmarking results and the correlations
uncovered, we believe that CityNet can significantly contribute to the field of
urban computing by enabling research on advanced topics.
","[{'version': 'v1', 'created': 'Wed, 30 Jun 2021 04:05:51 GMT'}, {'version': 'v2', 'created': 'Wed, 10 Apr 2024 14:11:50 GMT'}]",2024-04-11,"[['Zheng', 'Zhengfei', ''], ['Geng', 'Xu', ''], ['Yang', 'Hai', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
217,2106.16192,Sagar Pal,"Sagar Pal, Cesar Pairetti, Marco Crialesi-Esposito, Daniel Fuster,
  St\'ephane Zaleski","Statistics of drops generated from ensembles of randomly corrugated
  ligaments",,,,,physics.flu-dyn physics.comp-ph physics.data-an,http://creativecommons.org/licenses/by/4.0/,"  The size of drops generated by the capillary-driven disintegration of liquid
ligaments plays a fundamental role in several important natural phenomena,
ranging from heat and mass transfer at the ocean-atmosphere interface to
pathogen transmission. The inherent non-linearity of the equations governing
the ligament destabilization leads to significant differences in the resulting
drop sizes, owing to small fluctuations in the myriad initial conditions.
Previous experiments and simulations reveal a variety of drop size
distributions, corresponding to competing underlying physical interpretations.
Here, we perform numerical simulations of individual ligaments, the
deterministic breakup of which is triggered by random initial surface
corrugations. The simulations are grouped in a large ensemble, each
corresponding to a random initial configuration. The resulting probability
distributions reveal three stable drop sizes, generated via a sequence of two
distinct stages of breakup. Four different distributions are tested,
volume-based Poisson, Gaussian, Gamma and Log-Normal. Depending on the time,
range of droplet sizes and criteria for success, each distribution has
successes and failures. However the Log-Normal distribution roughly describes
the data when fitting both the primary peak and the tail of the distribution
while the number of droplets generated is the highest, while the Gamma and
Log-Normal distributions perform equally well when fitting the tail. The study
demonstrates a precisely controllable and reproducible framework, which can be
employed to investigate the mechanisms responsible for the polydispersity of
drop sizes found in complex fluid fragmentation scenarios.
","[{'version': 'v1', 'created': 'Wed, 30 Jun 2021 16:41:19 GMT'}, {'version': 'v2', 'created': 'Thu, 19 Aug 2021 16:13:45 GMT'}, {'version': 'v3', 'created': 'Mon, 3 Jun 2024 10:37:06 GMT'}]",2024-06-04,"[['Pal', 'Sagar', ''], ['Pairetti', 'Cesar', ''], ['Crialesi-Esposito', 'Marco', ''], ['Fuster', 'Daniel', ''], ['Zaleski', 'Stéphane', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
218,2107.00363,Nicolas Dewolf,"Nicolas Dewolf, Bernard De Baets, Willem Waegeman",Valid prediction intervals for regression problems,"Minor correction (bibliography and typo in Fig. 3). Thanks to Dr.
  Mar\'ia Moreno de Castro for spotting this typo",,10.1007/s10462-022-10178-5,,stat.ML cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Over the last few decades, various methods have been proposed for estimating
prediction intervals in regression settings, including Bayesian methods,
ensemble methods, direct interval estimation methods and conformal prediction
methods. An important issue is the calibration of these methods: the generated
prediction intervals should have a predefined coverage level, without being
overly conservative. In this work, we review the above four classes of methods
from a conceptual and experimental point of view. Results on benchmark data
sets from various domains highlight large fluctuations in performance from one
data set to another. These observations can be attributed to the violation of
certain assumptions that are inherent to some classes of methods. We illustrate
how conformal prediction can be used as a general calibration procedure for
methods that deliver poor results without a calibration step.
","[{'version': 'v1', 'created': 'Thu, 1 Jul 2021 10:59:36 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Dec 2021 08:29:54 GMT'}, {'version': 'v3', 'created': 'Tue, 8 Mar 2022 16:00:34 GMT'}, {'version': 'v4', 'created': 'Mon, 1 Apr 2024 12:30:49 GMT'}]",2024-04-02,"[['Dewolf', 'Nicolas', ''], ['De Baets', 'Bernard', ''], ['Waegeman', 'Willem', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
219,2107.02896,Javier Velasco-Mata,"Javier Velasco-Mata, V\'ictor Gonz\'alez-Castro, Eduardo Fidalgo,
  Enrique Alegre","Efficient Detection of Botnet Traffic by features selection and Decision
  Trees",Submitted to IEEE Access,,10.1109/access.2021.3108222,,cs.CR cs.LG,http://creativecommons.org/publicdomain/zero/1.0/,"  Botnets are one of the online threats with the biggest presence, causing
billionaire losses to global economies. Nowadays, the increasing number of
devices connected to the Internet makes it necessary to analyze large amounts
of network traffic data. In this work, we focus on increasing the performance
on botnet traffic classification by selecting those features that further
increase the detection rate. For this purpose we use two feature selection
techniques, Information Gain and Gini Importance, which led to three
pre-selected subsets of five, six and seven features. Then, we evaluate the
three feature subsets along with three models, Decision Tree, Random Forest and
k-Nearest Neighbors. To test the performance of the three feature vectors and
the three models we generate two datasets based on the CTU-13 dataset, namely
QB-CTU13 and EQB-CTU13. We measure the performance as the macro averaged F1
score over the computational time required to classify a sample. The results
show that the highest performance is achieved by Decision Trees using a five
feature set which obtained a mean F1 score of 85% classifying each sample in an
average time of 0.78 microseconds.
","[{'version': 'v1', 'created': 'Wed, 30 Jun 2021 11:55:12 GMT'}]",2024-07-02,"[['Velasco-Mata', 'Javier', ''], ['González-Castro', 'Víctor', ''], ['Fidalgo', 'Eduardo', ''], ['Alegre', 'Enrique', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
220,2107.04312,Paraskevi Nousi,"Paraskevi Nousi, Styliani-Christina Fragkouli, Nikolaos Passalis,
  Panagiotis Iosif, Theocharis Apostolatos, George Pappas, Nikolaos
  Stergioulas, Anastasios Tefas","Autoencoder-driven Spiral Representation Learning for Gravitational Wave
  Surrogate Modelling",,"Neurocomputing 491, 67-77 (2022)",10.1016/j.neucom.2022.03.052,VIR-0678B-21,cs.LG astro-ph.HE gr-qc,http://creativecommons.org/licenses/by/4.0/,"  Recently, artificial neural networks have been gaining momentum in the field
of gravitational wave astronomy, for example in surrogate modelling of
computationally expensive waveform models for binary black hole inspiral and
merger. Surrogate modelling yields fast and accurate approximations of
gravitational waves and neural networks have been used in the final step of
interpolating the coefficients of the surrogate model for arbitrary waveforms
outside the training sample. We investigate the existence of underlying
structures in the empirical interpolation coefficients using autoencoders. We
demonstrate that when the coefficient space is compressed to only two
dimensions, a spiral structure appears, wherein the spiral angle is linearly
related to the mass ratio. Based on this finding, we design a spiral module
with learnable parameters, that is used as the first layer in a neural network,
which learns to map the input space to the coefficients. The spiral module is
evaluated on multiple neural network architectures and consistently achieves
better speed-accuracy trade-off than baseline models. A thorough experimental
study is conducted and the final result is a surrogate model which can evaluate
millions of input parameters in a single forward pass in under 1ms on a desktop
GPU, while the mismatch between the corresponding generated waveforms and the
ground-truth waveforms is better than the compared baseline methods. We
anticipate the existence of analogous underlying structures and corresponding
computational gains also in the case of spinning black hole binaries.
","[{'version': 'v1', 'created': 'Fri, 9 Jul 2021 09:03:08 GMT'}]",2024-07-09,"[['Nousi', 'Paraskevi', ''], ['Fragkouli', 'Styliani-Christina', ''], ['Passalis', 'Nikolaos', ''], ['Iosif', 'Panagiotis', ''], ['Apostolatos', 'Theocharis', ''], ['Pappas', 'George', ''], ['Stergioulas', 'Nikolaos', ''], ['Tefas', 'Anastasios', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
221,2107.07232,Alexandre Verine,"Alexandre Verine, Benjamin Negrevergne, Fabrice Rossi, Yann Chevaleyre",On the expressivity of bi-Lipschitz normalizing flows,,"Proceedings of The 14th Asian Conference on Machine Learning, PMLR
  189:1054-1069, 2023",,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  An invertible function is bi-Lipschitz if both the function and its inverse
have bounded Lipschitz constants. Nowadays, most Normalizing Flows are
bi-Lipschitz by design or by training to limit numerical errors (among other
things). In this paper, we discuss the expressivity of bi-Lipschitz Normalizing
Flows and identify several target distributions that are difficult to
approximate using such models. Then, we characterize the expressivity of
bi-Lipschitz Normalizing Flows by giving several lower bounds on the Total
Variation distance between these particularly unfavorable distributions and
their best possible approximation. Finally, we discuss potential remedies which
include using more complex latent distributions.
","[{'version': 'v1', 'created': 'Thu, 15 Jul 2021 10:13:46 GMT'}, {'version': 'v2', 'created': 'Mon, 7 Feb 2022 18:25:29 GMT'}, {'version': 'v3', 'created': 'Thu, 7 Mar 2024 17:54:39 GMT'}]",2024-03-08,"[['Verine', 'Alexandre', ''], ['Negrevergne', 'Benjamin', ''], ['Rossi', 'Fabrice', ''], ['Chevaleyre', 'Yann', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
222,2107.0802,Yiye Jiang,"Yiye Jiang, J\'er\'emie Bigot and Sofian Maabout",Online Graph Topology Learning from Matrix-valued Time Series,,,,,stat.ML cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
","[{'version': 'v1', 'created': 'Fri, 16 Jul 2021 17:21:14 GMT'}, {'version': 'v2', 'created': 'Sat, 18 Feb 2023 14:39:54 GMT'}, {'version': 'v3', 'created': 'Thu, 1 Feb 2024 16:05:28 GMT'}]",2024-02-02,"[['Jiang', 'Yiye', ''], ['Bigot', 'Jérémie', ''], ['Maabout', 'Sofian', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
223,2107.09801,Borko Bo\v{s}kovi\'c,"Borko Bo\v{s}kovi\'c, Janez Brest","Two-phase Optimization of Binary Sequences with Low Peak Sidelobe Level
  Value","8 pages, 4 figures, 5 tables","Borko Bo\v{s}kovi\'c, Janez Brest, Two-phase optimization of
  binary sequences with low peak sidelobe level value, Expert Systems with
  Applications, Volume 251, 2024",10.1016/j.eswa.2024.124032,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The search for binary sequences with low peak sidelobe level value represents
a formidable computational problem. To locate better sequences for this
problem, we designed a stochastic algorithm that uses two fitness functions. In
these fitness functions, the value of the autocorrelation function has a
different impact on the final fitness value. It is defined with the value of
the exponent over the autocorrelation function values. Each function is used in
the corresponding optimization phase, and the optimization process switches
between these two phases until the stopping condition is satisfied. The
proposed algorithm was implemented using the compute unified device
architecture and therefore allowed us to exploit the computational power of
graphics processing units. This algorithm was tested on sequences with lengths
$L = 2^m - 1$, for $14 \le m \le 20$. From the obtained results it is evident
that the usage of two fitness functions improved the efficiency of the
algorithm significantly, new-best known solutions were achieved, and the
achieved PSL values were significantly less than $\sqrt{L}$.
","[{'version': 'v1', 'created': 'Wed, 30 Jun 2021 13:59:55 GMT'}]",2024-06-12,"[['Bošković', 'Borko', ''], ['Brest', 'Janez', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
224,2107.09951,Nan Liu,"Feng Xie, Han Yuan, Yilin Ning, Marcus Eng Hock Ong, Mengling Feng,
  Wynne Hsu, Bibhas Chakraborty, Nan Liu","Deep learning for temporal data representation in electronic health
  records: A systematic review of challenges and methodologies",,,10.1016/j.jbi.2021.103980,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Objective: Temporal electronic health records (EHRs) can be a wealth of
information for secondary uses, such as clinical events prediction or chronic
disease management. However, challenges exist for temporal data representation.
We therefore sought to identify these challenges and evaluate novel
methodologies for addressing them through a systematic examination of deep
learning solutions.
  Methods: We searched five databases (PubMed, EMBASE, the Institute of
Electrical and Electronics Engineers [IEEE] Xplore Digital Library, the
Association for Computing Machinery [ACM] digital library, and Web of Science)
complemented with hand-searching in several prestigious computer science
conference proceedings. We sought articles that reported deep learning
methodologies on temporal data representation in structured EHR data from
January 1, 2010, to August 30, 2020. We summarized and analyzed the selected
articles from three perspectives: nature of time series, methodology, and model
implementation.
  Results: We included 98 articles related to temporal data representation
using deep learning. Four major challenges were identified, including data
irregularity, data heterogeneity, data sparsity, and model opacity. We then
studied how deep learning techniques were applied to address these challenges.
Finally, we discuss some open challenges arising from deep learning.
  Conclusion: Temporal EHR data present several major challenges for clinical
prediction modeling and data utilization. To some extent, current deep learning
solutions can address these challenges. Future studies can consider designing
comprehensive and integrated solutions. Moreover, researchers should
incorporate additional clinical domain knowledge into study designs and enhance
the interpretability of the model to facilitate its implementation in clinical
practice.
","[{'version': 'v1', 'created': 'Wed, 21 Jul 2021 09:00:40 GMT'}]",2024-06-11,"[['Xie', 'Feng', ''], ['Yuan', 'Han', ''], ['Ning', 'Yilin', ''], ['Ong', 'Marcus Eng Hock', ''], ['Feng', 'Mengling', ''], ['Hsu', 'Wynne', ''], ['Chakraborty', 'Bibhas', ''], ['Liu', 'Nan', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
225,2107.10715,Michael Timothy Bennett,"Michael Timothy Bennett, Yoshihiro Maruyama","Philosophical Specification of Empathetic Ethical Artificial
  Intelligence",To appear in IEEE Transactions in Cognitive and Developmental Systems,"IEEE Transactions on Cognitive and Developmental Systems, vol. 14,
  no. 2, pp. 292-300, June 2022",10.1109/TCDS.2021.3099945,,cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
","[{'version': 'v1', 'created': 'Thu, 22 Jul 2021 14:37:46 GMT'}]",2024-04-30,"[['Bennett', 'Michael Timothy', ''], ['Maruyama', 'Yoshihiro', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
226,2107.10955,Xiaodong Li,"Xingmei Lou, Yu Hu, Xiaodong Li",Learning Linear Polytree Structural Equation Models,"35 pages, 5 figures, 4 tables",,,,stat.ML cs.LG math.ST stat.ME stat.TH,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We are interested in the problem of learning the directed acyclic graph (DAG)
when data are generated from a linear structural equation model (SEM) and the
causal structure can be characterized by a polytree. Under the Gaussian
polytree models, we study sufficient conditions on the sample sizes for the
well-known Chow-Liu algorithm to exactly recover both the skeleton and the
equivalence class of the polytree, which is uniquely represented by a CPDAG. On
the other hand, necessary conditions on the required sample sizes for both
skeleton and CPDAG recovery are also derived in terms of information-theoretic
lower bounds, which match the respective sufficient conditions and thereby give
a sharp characterization of the difficulty of these tasks. We also consider the
problem of inverse correlation matrix estimation under the linear polytree
models, and establish the estimation error bound in terms of the dimension and
the total number of v-structures. We also consider an extension of group linear
polytree models, in which each node represents a group of variables. Our
theoretical findings are illustrated by comprehensive numerical simulations,
and experiments on benchmark data also demonstrate the robustness of polytree
learning when the true graphical structures can only be approximated by
polytrees.
","[{'version': 'v1', 'created': 'Thu, 22 Jul 2021 23:22:20 GMT'}, {'version': 'v2', 'created': 'Wed, 19 Jan 2022 00:08:44 GMT'}, {'version': 'v3', 'created': 'Wed, 2 Feb 2022 04:30:25 GMT'}, {'version': 'v4', 'created': 'Tue, 14 May 2024 09:00:19 GMT'}]",2024-05-15,"[['Lou', 'Xingmei', ''], ['Hu', 'Yu', ''], ['Li', 'Xiaodong', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
227,2107.11972,Liang Zeng,"Liang Zeng, Lei Wang, Hui Niu, Ruchen Zhang, Ling Wang, Jian Li","Trade When Opportunity Comes: Price Movement Forecasting via
  Locality-Aware Attention and Iterative Refinement Labeling",,,,,cs.LG cs.AI cs.CE q-fin.ST,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Price movement forecasting, aimed at predicting financial asset trends based
on current market information, has achieved promising advancements through
machine learning (ML) methods. Most existing ML methods, however, struggle with
the extremely low signal-to-noise ratio and stochastic nature of financial
data, often mistaking noises for real trading signals without careful selection
of potentially profitable samples. To address this issue, we propose LARA, a
novel price movement forecasting framework with two main components:
Locality-Aware Attention (LA-Attention) and Iterative Refinement Labeling
(RA-Labeling). (1) LA-Attention, enhanced by metric learning techniques,
automatically extracts the potentially profitable samples through masked
attention scheme and task-specific distance metrics. (2) RA-Labeling further
iteratively refines the noisy labels of potentially profitable samples, and
combines the learned predictors robust to the unseen and noisy samples. In a
set of experiments on three real-world financial markets: stocks,
cryptocurrencies, and ETFs, LARA significantly outperforms several machine
learning based methods on the Qlib quantitative investment platform. Extensive
ablation studies confirm LARA's superior ability in capturing more reliable
trading opportunities.
","[{'version': 'v1', 'created': 'Mon, 26 Jul 2021 05:52:42 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Jun 2022 14:14:54 GMT'}, {'version': 'v3', 'created': 'Mon, 15 May 2023 13:17:58 GMT'}, {'version': 'v4', 'created': 'Wed, 10 Jul 2024 07:05:51 GMT'}]",2024-07-11,"[['Zeng', 'Liang', ''], ['Wang', 'Lei', ''], ['Niu', 'Hui', ''], ['Zhang', 'Ruchen', ''], ['Wang', 'Ling', ''], ['Li', 'Jian', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
228,2107.12365,Yuling Yan,"Yuling Yan, Yuxin Chen, Jianqing Fan",Inference for Heteroskedastic PCA with Missing Data,accepted to Annals of Statistics,,,,math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper studies how to construct confidence regions for principal
component analysis (PCA) in high dimension, a problem that has been vastly
under-explored. While computing measures of uncertainty for nonlinear/nonconvex
estimators is in general difficult in high dimension, the challenge is further
compounded by the prevalent presence of missing data and heteroskedastic noise.
We propose a novel approach to performing valid inference on the principal
subspace under a spiked covariance model with missing data, on the basis of an
estimator called HeteroPCA (Zhang et al., 2022). We develop non-asymptotic
distributional guarantees for HeteroPCA, and demonstrate how these can be
invoked to compute both confidence regions for the principal subspace and
entrywise confidence intervals for the spiked covariance matrix. Our inference
procedures are fully data-driven and adaptive to heteroskedastic random noise,
without requiring prior knowledge about the noise levels.
","[{'version': 'v1', 'created': 'Mon, 26 Jul 2021 17:59:01 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Feb 2024 17:22:09 GMT'}]",2024-02-29,"[['Yan', 'Yuling', ''], ['Chen', 'Yuxin', ''], ['Fan', 'Jianqing', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
229,2107.13509,Upol Ehsan,"Upol Ehsan, Samir Passi, Q. Vera Liao, Larry Chan, I-Hsiang Lee,
  Michael Muller, Mark O. Riedl",The Who in XAI: How AI Background Shapes Perceptions of AI Explanations,,ACM CHI 2024,10.1145/3613904.3642474,,cs.HC cs.AI cs.CY,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  Explainability of AI systems is critical for users to take informed actions.
Understanding ""who"" opens the black-box of AI is just as important as opening
it. We conduct a mixed-methods study of how two different groups--people with
and without AI background--perceive different types of AI explanations.
Quantitatively, we share user perceptions along five dimensions. Qualitatively,
we describe how AI background can influence interpretations, elucidating the
differences through lenses of appropriation and cognitive heuristics. We find
that (1) both groups showed unwarranted faith in numbers for different reasons
and (2) each group found value in different explanations beyond their intended
design. Carrying critical implications for the field of XAI, our findings
showcase how AI generated explanations can have negative consequences despite
best intentions and how that could lead to harmful manipulation of trust. We
propose design interventions to mitigate them.
","[{'version': 'v1', 'created': 'Wed, 28 Jul 2021 17:32:04 GMT'}, {'version': 'v2', 'created': 'Tue, 5 Mar 2024 20:33:44 GMT'}]",2024-03-07,"[['Ehsan', 'Upol', ''], ['Passi', 'Samir', ''], ['Liao', 'Q. Vera', ''], ['Chan', 'Larry', ''], ['Lee', 'I-Hsiang', ''], ['Muller', 'Michael', ''], ['Riedl', 'Mark O.', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
230,2107.14796,Charles L. B\'erub\'e,Charles L. B\'erub\'e and Pierre B\'erub\'e,Data-driven modeling of time-domain induced polarization,"38 pages, 11 figures, 3 tables, 1 appendix. Manuscript submitted to
  SEG Geophysics for review. Original manuscript uploaded to arxiv.org in
  accordance with SEG's Preprint Policy (
  https://library.seg.org/page/policies/preprints ). For associated code, see
  https://doi.org/10.5281/zenodo.5148538",Geophysics Volume 87 Issue 3 May 2022,10.1190/geo2021-0497.1,,cs.LG physics.geo-ph,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
","[{'version': 'v1', 'created': 'Fri, 30 Jul 2021 17:54:44 GMT'}]",2024-02-14,"[['Bérubé', 'Charles L.', ''], ['Bérubé', 'Pierre', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
231,2108.00138,Bibek Poudel,"Bibek Poudel, Thomas Watson, Weizi Li","Learning to Control DC Motor for Micromobility in Real Time with
  Reinforcement Learning","7 pages, 3 figures",,10.1109/ITSC55140.2022.9921919,,cs.LG cs.RO,http://creativecommons.org/licenses/by/4.0/,"  Autonomous micromobility has been attracting the attention of researchers and
practitioners in recent years. A key component of many micro-transport vehicles
is the DC motor, a complex dynamical system that is continuous and non-linear.
Learning to quickly control the DC motor in the presence of disturbances and
uncertainties is desired for various applications that require robustness and
stability. Techniques to accomplish this task usually rely on a mathematical
system model, which is often insufficient to anticipate the effects of
time-varying and interrelated sources of non-linearities. While some model-free
approaches have been successful at the task, they rely on massive interactions
with the system and are trained in specialized hardware in order to fit a
highly parameterized controller. In this work, we learn to steer a DC motor via
sample-efficient reinforcement learning. Using data collected from hardware
interactions in the real world, we additionally build a simulator to experiment
with a wide range of parameters and learning strategies. With the best
parameters found, we learn an effective control policy in one minute and 53
seconds on a simulation and in 10 minutes and 35 seconds on a physical system.
","[{'version': 'v1', 'created': 'Sat, 31 Jul 2021 03:24:36 GMT'}, {'version': 'v2', 'created': 'Sat, 29 Jan 2022 14:10:56 GMT'}, {'version': 'v3', 'created': 'Sat, 19 Mar 2022 15:08:54 GMT'}, {'version': 'v4', 'created': 'Sat, 30 Jul 2022 23:14:20 GMT'}]",2024-03-12,"[['Poudel', 'Bibek', ''], ['Watson', 'Thomas', ''], ['Li', 'Weizi', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
232,2108.0049,Fernando Llorente Fern\'andez,"F. Llorente, L. Martino, J. Read, D. Delgado","A survey of Monte Carlo methods for noisy and costly densities with
  application to reinforcement learning",,International Statistical Review. 2024,10.1111/insr.12573,,cs.LG stat.CO stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This survey gives an overview of Monte Carlo methodologies using surrogate
models, for dealing with densities which are intractable, costly, and/or noisy.
This type of problem can be found in numerous real-world scenarios, including
stochastic optimization and reinforcement learning, where each evaluation of a
density function may incur some computationally-expensive or even physical
(real-world activity) cost, likely to give different results each time. The
surrogate model does not incur this cost, but there are important trade-offs
and considerations involved in the choice and design of such methodologies. We
classify the different methodologies into three main classes and describe
specific instances of algorithms under a unified notation. A modular scheme
which encompasses the considered methods is also presented. A range of
application scenarios is discussed, with special attention to the
likelihood-free setting and reinforcement learning. Several numerical
comparisons are also provided.
","[{'version': 'v1', 'created': 'Sun, 1 Aug 2021 16:47:15 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Sep 2021 11:37:21 GMT'}]",2024-06-18,"[['Llorente', 'F.', ''], ['Martino', 'L.', ''], ['Read', 'J.', ''], ['Delgado', 'D.', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
233,2108.06663,Vinod Kumar Chauhan,"Vinod Kumar Chauhan, Sukhdeep Singh and Anuj Sharma","HCR-Net: A deep learning based script independent handwritten character
  recognition network",accepted to Multimedia Tools and Applications,,10.1007/s11042-024-18655-5,,cs.CV cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Handwritten character recognition (HCR) remains a challenging pattern
recognition problem despite decades of research, and lacks research on script
independent recognition techniques. {\color{black}This is mainly because of
similar character structures, different handwriting styles, diverse scripts,
handcrafted feature extraction techniques, unavailability of data and code, and
the development of script-specific deep learning techniques. To address these
limitations, we have proposed a script independent deep learning network for
HCR research, called HCR-Net, that sets a new research direction for the field.
HCR-Net is based on a novel transfer learning approach for HCR, which
\textit{partly utilizes} feature extraction layers of a pre-trained network.}
Due to transfer learning and image augmentation, HCR-Net provides faster and
computationally efficient training, better performance and generalizations, and
can work with small datasets. HCR-Net is extensively evaluated on 40 publicly
available datasets of Bangla, Punjabi, Hindi, English, Swedish, Urdu, Farsi,
Tibetan, Kannada, Malayalam, Telugu, Marathi, Nepali and Arabic languages, and
established 26 new benchmark results while performed close to the best results
in the rest cases. HCR-Net showed performance improvements up to 11\% against
the existing results and achieved a fast convergence rate showing up to 99\% of
final performance in the very first epoch. HCR-Net significantly outperformed
the state-of-the-art transfer learning techniques and also reduced the number
of trainable parameters by 34\% as compared with the corresponding pre-trained
network. To facilitate reproducibility and further advancements of HCR
research, the complete code is publicly released at
\url{https://github.com/jmdvinodjmd/HCR-Net}.
","[{'version': 'v1', 'created': 'Sun, 15 Aug 2021 05:48:07 GMT'}, {'version': 'v2', 'created': 'Thu, 14 Apr 2022 09:49:49 GMT'}, {'version': 'v3', 'created': 'Tue, 31 Jan 2023 19:47:50 GMT'}, {'version': 'v4', 'created': 'Sat, 17 Feb 2024 15:35:50 GMT'}]",2024-02-27,"[['Chauhan', 'Vinod Kumar', ''], ['Singh', 'Sukhdeep', ''], ['Sharma', 'Anuj', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
234,2108.07222,Suheng Xu,"Suheng Xu, Alexander S. McLeod, Xinzhong Chen, Daniel J. Rizzo, Bjarke
  S. Jessen, Ziheng Yao, Zhiyuan Sun, Sara Shabani, Abhay N. Pasupathy, Andrew
  J. Millis, Cory R. Dean, James C. Hone, Mengkun Liu, D. N. Basov",Deep learning analysis of polaritonic waves images,,"ACS Nano 15, 11, 18182-18191(2020)",10.1021/acsnano.1c07011,,cond-mat.mtrl-sci physics.data-an physics.optics,http://creativecommons.org/licenses/by/4.0/,"  Deep learning (DL) is an emerging analysis tool across sciences and
engineering. Encouraged by the successes of DL in revealing quantitative trends
in massive imaging data, we applied this approach to nano-scale deeply
sub-diffractional images of propagating polaritonic waves in complex materials.
We developed a practical protocol for the rapid regression of images that
quantifies the wavelength and the quality factor of polaritonic waves utilizing
the convolutional neural network (CNN). Using simulated near-field images as
training data, the CNN can be made to simultaneously extract polaritonic
characteristics and materials parameters in a timescale that is at least three
orders of magnitude faster than common fitting/processing procedures. The
CNN-based analysis was validated by examining the experimental near-field
images of charge-transfer plasmon polaritons at Graphene/{\alpha}-RuCl3
interfaces. Our work provides a general framework for extracting quantitative
information from images generated with a variety of scanning probe methods.
","[{'version': 'v1', 'created': 'Wed, 11 Aug 2021 02:33:41 GMT'}, {'version': 'v2', 'created': 'Wed, 10 Jul 2024 05:26:52 GMT'}]",2024-07-11,"[['Xu', 'Suheng', ''], ['McLeod', 'Alexander S.', ''], ['Chen', 'Xinzhong', ''], ['Rizzo', 'Daniel J.', ''], ['Jessen', 'Bjarke S.', ''], ['Yao', 'Ziheng', ''], ['Sun', 'Zhiyuan', ''], ['Shabani', 'Sara', ''], ['Pasupathy', 'Abhay N.', ''], ['Millis', 'Andrew J.', ''], ['Dean', 'Cory R.', ''], ['Hone', 'James C.', ''], ['Liu', 'Mengkun', ''], ['Basov', 'D. N.', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
235,2108.07749,Joshua Yao-Yu Lin,"Joshua Yao-Yu Lin, Sneh Pandya, Devanshi Pratap, Xin Liu, Matias
  Carrasco Kind, Volodymyr Kindratenko",AGNet: Weighing Black Holes with Deep Learning,"8 pages, 7 figures, 1 table, Accepted by MNRAS","Monthly Notices of the Royal Astronomical Society, 2022;, stac3339",10.1093/mnras/stac3339,,astro-ph.GA astro-ph.HE cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Supermassive black holes (SMBHs) are ubiquitously found at the centers of
most massive galaxies. Measuring SMBH mass is important for understanding the
origin and evolution of SMBHs. However, traditional methods require
spectroscopic data which is expensive to gather. We present an algorithm that
weighs SMBHs using quasar light time series, circumventing the need for
expensive spectra. We train, validate, and test neural networks that directly
learn from the Sloan Digital Sky Survey (SDSS) Stripe 82 light curves for a
sample of $38,939$ spectroscopically confirmed quasars to map out the nonlinear
encoding between SMBH mass and multi-color optical light curves. We find a
1$\sigma$ scatter of 0.37 dex between the predicted SMBH mass and the fiducial
virial mass estimate based on SDSS single-epoch spectra, which is comparable to
the systematic uncertainty in the virial mass estimate. Our results have direct
implications for more efficient applications with future observations from the
Vera C. Rubin Observatory. Our code, \textsf{AGNet}, is publicly available at
\url{https://github.com/snehjp2/AGNet}.
","[{'version': 'v1', 'created': 'Tue, 17 Aug 2021 16:45:11 GMT'}, {'version': 'v2', 'created': 'Tue, 22 Nov 2022 03:27:59 GMT'}]",2024-04-23,"[['Lin', 'Joshua Yao-Yu', ''], ['Pandya', 'Sneh', ''], ['Pratap', 'Devanshi', ''], ['Liu', 'Xin', ''], ['Kind', 'Matias Carrasco', ''], ['Kindratenko', 'Volodymyr', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
236,2108.08481,Zongyi Li,"Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli,
  Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar",Neural Operator: Learning Maps Between Function Spaces,,"The Journal of Machine Learning Research (2023), Volume 24, Issue
  1, Article No 89, pp 4061-4157",10.5555/3648699.3648788,,cs.LG cs.NA math.NA,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The classical development of neural networks has primarily focused on
learning mappings between finite dimensional Euclidean spaces or finite sets.
We propose a generalization of neural networks to learn operators, termed
neural operators, that map between infinite dimensional function spaces. We
formulate the neural operator as a composition of linear integral operators and
nonlinear activation functions. We prove a universal approximation theorem for
our proposed neural operator, showing that it can approximate any given
nonlinear continuous operator. The proposed neural operators are also
discretization-invariant, i.e., they share the same model parameters among
different discretization of the underlying function spaces. Furthermore, we
introduce four classes of efficient parameterization, viz., graph neural
operators, multi-pole graph neural operators, low-rank neural operators, and
Fourier neural operators. An important application for neural operators is
learning surrogate maps for the solution operators of partial differential
equations (PDEs). We consider standard PDEs such as the Burgers, Darcy
subsurface flow, and the Navier-Stokes equations, and show that the proposed
neural operators have superior performance compared to existing machine
learning based methodologies, while being several orders of magnitude faster
than conventional PDE solvers.
","[{'version': 'v1', 'created': 'Thu, 19 Aug 2021 03:56:49 GMT'}, {'version': 'v2', 'created': 'Mon, 6 Sep 2021 04:56:53 GMT'}, {'version': 'v3', 'created': 'Thu, 16 Dec 2021 06:18:21 GMT'}, {'version': 'v4', 'created': 'Thu, 6 Oct 2022 17:02:08 GMT'}, {'version': 'v5', 'created': 'Fri, 7 Apr 2023 17:30:37 GMT'}, {'version': 'v6', 'created': 'Thu, 2 May 2024 17:19:54 GMT'}]",2024-05-03,"[['Kovachki', 'Nikola', ''], ['Li', 'Zongyi', ''], ['Liu', 'Burigede', ''], ['Azizzadenesheli', 'Kamyar', ''], ['Bhattacharya', 'Kaushik', ''], ['Stuart', 'Andrew', ''], ['Anandkumar', 'Anima', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
237,2108.08677,Saber Salehkaleybar,"Arsalan Sharifnassab, Saber Salehkaleybar, S. Jamaloddin Golestani","Order Optimal Bounds for One-Shot Federated Learning over non-Convex
  Loss Functions",,,,,cs.LG,http://creativecommons.org/licenses/by/4.0/,"  We consider the problem of federated learning in a one-shot setting in which
there are $m$ machines, each observing $n$ sample functions from an unknown
distribution on non-convex loss functions. Let $F:[-1,1]^d\to\mathbb{R}$ be the
expected loss function with respect to this unknown distribution. The goal is
to find an estimate of the minimizer of $F$. Based on its observations, each
machine generates a signal of bounded length $B$ and sends it to a server. The
server collects signals of all machines and outputs an estimate of the
minimizer of $F$. We show that the expected loss of any algorithm is lower
bounded by $\max\big(1/(\sqrt{n}(mB)^{1/d}), 1/\sqrt{mn}\big)$, up to a
logarithmic factor. We then prove that this lower bound is order optimal in $m$
and $n$ by presenting a distributed learning algorithm, called Multi-Resolution
Estimator for Non-Convex loss function (MRE-NC), whose expected loss matches
the lower bound for large $mn$ up to polylogarithmic factors.
","[{'version': 'v1', 'created': 'Thu, 19 Aug 2021 13:38:43 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Dec 2022 10:05:00 GMT'}, {'version': 'v3', 'created': 'Tue, 6 Feb 2024 12:06:44 GMT'}]",2024-02-07,"[['Sharifnassab', 'Arsalan', ''], ['Salehkaleybar', 'Saber', ''], ['Golestani', 'S. Jamaloddin', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
238,2108.12056,Shawn Beaulieu,"Shawn L. Beaulieu, Jeff Clune, Nick Cheney",Continual learning under domain transfer with sparse synaptic bursting,,,,,cs.LG cs.AI cs.CV,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Existing machines are functionally specific tools that were made for easy
prediction and control. Tomorrow's machines may be closer to biological systems
in their mutability, resilience, and autonomy. But first they must be capable
of learning and retaining new information without being exposed to it
arbitrarily often. Past efforts to engineer such systems have sought to build
or regulate artificial neural networks using disjoint sets of weights that are
uniquely sensitive to specific tasks or inputs. This has not yet enabled
continual learning over long sequences of previously unseen data without
corrupting existing knowledge: a problem known as catastrophic forgetting. In
this paper, we introduce a system that can learn sequentially over previously
unseen datasets (ImageNet, CIFAR-100) with little forgetting over time. This is
done by controlling the activity of weights in a convolutional neural network
on the basis of inputs using top-down regulation generated by a second
feed-forward neural network. We find that our method learns continually under
domain transfer with sparse bursts of activity in weights that are recycled
across tasks, rather than by maintaining task-specific modules. Sparse synaptic
bursting is found to balance activity and suppression such that new functions
can be learned without corrupting extant knowledge, thus mirroring the balance
of order and disorder in systems at the edge of chaos. This behavior emerges
during a prior pre-training (or 'meta-learning') phase in which regulated
synapses are selectively disinhibited, or grown, from an initial state of
uniform suppression through prediction error minimization.
","[{'version': 'v1', 'created': 'Thu, 26 Aug 2021 22:53:27 GMT'}, {'version': 'v2', 'created': 'Tue, 16 Nov 2021 16:29:29 GMT'}, {'version': 'v3', 'created': 'Mon, 22 Nov 2021 17:27:43 GMT'}, {'version': 'v4', 'created': 'Sun, 5 Dec 2021 16:36:59 GMT'}, {'version': 'v5', 'created': 'Fri, 21 Jan 2022 15:33:59 GMT'}, {'version': 'v6', 'created': 'Sat, 12 Mar 2022 02:00:12 GMT'}, {'version': 'v7', 'created': 'Sun, 8 May 2022 16:48:56 GMT'}, {'version': 'v8', 'created': 'Tue, 20 Sep 2022 15:58:57 GMT'}, {'version': 'v9', 'created': 'Tue, 16 Jan 2024 18:55:17 GMT'}]",2024-01-17,"[['Beaulieu', 'Shawn L.', ''], ['Clune', 'Jeff', ''], ['Cheney', 'Nick', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
239,2108.13172,Thomas Witten,"Yunxiang Song, Thomas A. Witten",Stochastic synchronization induced by noise,"10 pages, 6 figures. v4 makes several small revisions for clarity",,10.1103/PhysRevE.106.044207,,cond-mat.stat-mech nlin.CD physics.bio-ph physics.data-an,http://creativecommons.org/licenses/by/4.0/,"  Random perturbations applied in tandem to an ensemble of oscillating objects
can synchronize their motion. We study multiple copies of an arbitrary
dynamical system in a stable limit cycle, described via a standard phase
reduction picture. The copies differ only in their arbitrary phases $\phi$.
Weak, randomly-timed external impulses applied to all the copies can
synchronize these phases over time. Beyond a threshold strength there is no
such convergence to a common phase. Instead, the synchronization becomes
erratic: successive impulses produce stochastic fluctuations in the phase
distribution $q(\phi)$, ranging from near-perfect to near-random
synchronization. Here we show that the sampled entropies of these phase
distributions themselves form a steady-state ensemble, whose average can be
made arbitrarily negative by tuning the impulse strength. A random-walk
description of the entropy's evolution accounts for the observed exponential
distribution of entropies and for the stochastic synchronization phenomenon.
","[{'version': 'v1', 'created': 'Mon, 16 Aug 2021 19:39:56 GMT'}, {'version': 'v2', 'created': 'Mon, 6 Sep 2021 21:54:33 GMT'}, {'version': 'v3', 'created': 'Thu, 23 Sep 2021 01:41:31 GMT'}, {'version': 'v4', 'created': 'Tue, 31 May 2022 17:27:45 GMT'}]",2024-02-23,"[['Song', 'Yunxiang', ''], ['Witten', 'Thomas A.', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
240,2108.13577,Xianghao Zhan,"Xianghao Zhan, Yuzhe Liu, Nicholas J. Cecchi, Olivier Gevaert, Michael
  M. Zeineh, Gerald A. Grant, David B. Camarillo","Rapidly and accurately estimating brain strain and strain rate across
  head impact types with transfer learning and data fusion","14 pages, 6 figures",,10.1109/TBME.2024.3354192,,cs.LG physics.med-ph q-bio.QM q-bio.TO,http://creativecommons.org/licenses/by/4.0/,"  Brain strain and strain rate are effective in predicting traumatic brain
injury (TBI) caused by head impacts. However, state-of-the-art finite element
modeling (FEM) demands considerable computational time in the computation,
limiting its application in real-time TBI risk monitoring. To accelerate,
machine learning head models (MLHMs) were developed, and the model accuracy was
found to decrease when the training/test datasets were from different head
impacts types. However, the size of dataset for specific impact types may not
be enough for model training. To address the computational cost of FEM, the
limited strain rate prediction, and the generalizability of MLHMs to on-field
datasets, we propose data fusion and transfer learning to develop a series of
MLHMs to predict the maximum principal strain (MPS) and maximum principal
strain rate (MPSR). We trained and tested the MLHMs on 13,623 head impacts from
simulations, American football, mixed martial arts, car crash, and compared
against the models trained on only simulations or only on-field impacts. The
MLHMs developed with transfer learning are significantly more accurate in
estimating MPS and MPSR than other models, with a mean absolute error (MAE)
smaller than 0.03 in predicting MPS and smaller than 7 (1/s) in predicting MPSR
on all impact datasets. The MLHMs can be applied to various head impact types
for rapidly and accurately calculating brain strain and strain rate. Besides
the clinical applications in real-time brain strain and strain rate monitoring,
this model helps researchers estimate the brain strain and strain rate caused
by head impacts more efficiently than FEM.
","[{'version': 'v1', 'created': 'Tue, 31 Aug 2021 01:45:20 GMT'}]",2024-06-04,"[['Zhan', 'Xianghao', ''], ['Liu', 'Yuzhe', ''], ['Cecchi', 'Nicholas J.', ''], ['Gevaert', 'Olivier', ''], ['Zeineh', 'Michael M.', ''], ['Grant', 'Gerald A.', ''], ['Camarillo', 'David B.', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
241,2108.13969,Haoran Duan,"Haoran Duan, Fan Wan, Rui Sun, Zeyu Wang, Varun Ojha, Yu Guan, Hubert
  P. H. Shum, Bingzhang Hu, Yang Long",Semi-Supervised Crowd Counting from Unlabeled Data,,,,,cs.CV cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Automatic Crowd behavior analysis can be applied to effectively help the
daily transportation statistics and planning, which helps the smart city
construction. As one of the most important keys, crowd counting has drawn
increasing attention. Recent works achieved promising performance but relied on
the supervised paradigm with expensive crowd annotations. To alleviate the
annotation cost in real-world transportation scenarios, in this work we
proposed a semi-supervised learning framework $S^{4}\textit{Crowd}$, which can
leverage both unlabeled/labeled data for robust crowd counting. In the
unsupervised pathway, two \textit{self-supervised losses} were proposed to
simulate the crowd variations such as scale, illumination, based on which
supervised information pseudo labels were generated and gradually refined. We
also proposed a crowd-driven recurrent unit \textit{Gated-Crowd-Recurrent-Unit
(GCRU)}, which can preserve discriminant crowd information by extracting
second-order statistics, yielding pseudo labels with improved quality. A joint
loss including both unsupervised/supervised information was proposed, and a
dynamic weighting strategy was employed to balance the importance of the
unsupervised loss and supervised loss at different training stages. We
conducted extensive experiments on four popular crowd counting datasets in
semi-supervised settings. Experimental results supported the effectiveness of
each proposed component in our $S^{4}$Crowd framework. Our method achieved
competitive performance in semi-supervised learning approaches on these crowd
counting datasets.
","[{'version': 'v1', 'created': 'Tue, 31 Aug 2021 16:51:00 GMT'}, {'version': 'v2', 'created': 'Sun, 27 Feb 2022 19:05:37 GMT'}, {'version': 'v3', 'created': 'Tue, 26 Mar 2024 16:13:26 GMT'}]",2024-03-27,"[['Duan', 'Haoran', ''], ['Wan', 'Fan', ''], ['Sun', 'Rui', ''], ['Wang', 'Zeyu', ''], ['Ojha', 'Varun', ''], ['Guan', 'Yu', ''], ['Shum', 'Hubert P. H.', ''], ['Hu', 'Bingzhang', ''], ['Long', 'Yang', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
242,2109.00046,Lijun Sun Mr,"Mengying Lei, Aurelie Labbe, Lijun Sun","Scalable Spatiotemporally Varying Coefficient Modelling with Bayesian
  Kernelized Tensor Regression",,Bayesian Analysis (2024),10.1214/24-BA1428,,stat.ML cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As a regression technique in spatial statistics, the spatiotemporally varying
coefficient model (STVC) is an important tool for discovering nonstationary and
interpretable response-covariate associations over both space and time.
However, it is difficult to apply STVC for large-scale spatiotemporal analyses
due to its high computational cost. To address this challenge, we summarize the
spatiotemporally varying coefficients using a third-order tensor structure and
propose to reformulate the spatiotemporally varying coefficient model as a
special low-rank tensor regression problem. The low-rank decomposition can
effectively model the global patterns of large data sets with a substantially
reduced number of parameters. To further incorporate the local spatiotemporal
dependencies, we use Gaussian process (GP) priors on the spatial and temporal
factor matrices. We refer to the overall framework as Bayesian Kernelized
Tensor Regression (BKTR), and kernelized tensor factorization can be considered
a new and scalable approach to modeling multivariate spatiotemporal processes
with a low-rank covariance structure. For model inference, we develop an
efficient Markov chain Monte Carlo (MCMC) algorithm, which uses Gibbs sampling
to update factor matrices and slice sampling to update kernel hyperparameters.
We conduct extensive experiments on both synthetic and real-world data sets,
and our results confirm the superior performance and efficiency of BKTR for
model estimation and parameter inference.
","[{'version': 'v1', 'created': 'Tue, 31 Aug 2021 19:22:23 GMT'}, {'version': 'v2', 'created': 'Tue, 15 Feb 2022 17:40:51 GMT'}, {'version': 'v3', 'created': 'Fri, 27 Jan 2023 21:51:57 GMT'}, {'version': 'v4', 'created': 'Sat, 13 Apr 2024 18:25:28 GMT'}]",2024-05-17,"[['Lei', 'Mengying', ''], ['Labbe', 'Aurelie', ''], ['Sun', 'Lijun', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
243,2109.00783,Daesoo Lee,"Daesoo Lee, Erlend Aune",Computer Vision Self-supervised Learning Methods on Time Series,,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Self-supervised learning (SSL) has had great success in both computer vision.
Most of the current mainstream computer vision SSL frameworks are based on
Siamese network architecture. These approaches often rely on cleverly crafted
loss functions and training setups to avoid feature collapse. In this study, we
evaluate if those computer-vision SSL frameworks are also effective on a
different modality (\textit{i.e.,} time series). The effectiveness is
experimented and evaluated on the UCR and UEA archives, and we show that the
computer vision SSL frameworks can be effective even for time series. In
addition, we propose a new method that improves on the recently proposed VICReg
method. Our method improves on a \textit{covariance} term proposed in VICReg,
and in addition we augment the head of the architecture by an iterative
normalization layer that accelerates the convergence of the model.
","[{'version': 'v1', 'created': 'Thu, 2 Sep 2021 08:45:53 GMT'}, {'version': 'v2', 'created': 'Fri, 3 Dec 2021 12:57:40 GMT'}, {'version': 'v3', 'created': 'Mon, 18 Jul 2022 13:20:13 GMT'}, {'version': 'v4', 'created': 'Fri, 26 Jan 2024 22:16:07 GMT'}]",2024-01-30,"[['Lee', 'Daesoo', ''], ['Aune', 'Erlend', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
244,2109.01281,Michael Timothy Bennett,Michael Timothy Bennett,Symbol Emergence and The Solutions to Any Task,Accepted to the 14th conference on Artificial General Intelligence,"Proceedings of the 14th International Conference on Artificial
  General Intelligence. 2021. Lecture Notes in Computer Science, vol 13154.
  Springer. pp. 30-40",10.1007/978-3-030-93758-4_4,,cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  The following defines intent, an arbitrary task and its solutions, and then
argues that an agent which always constructs what is called an Intensional
Solution would qualify as artificial general intelligence. We then explain how
natural language may emerge and be acquired by such an agent, conferring the
ability to model the intent of other individuals labouring under similar
compulsions, because an abstract symbol system and the solution to a task are
one and the same.
","[{'version': 'v1', 'created': 'Fri, 3 Sep 2021 02:44:35 GMT'}, {'version': 'v2', 'created': 'Mon, 4 Oct 2021 08:42:05 GMT'}]",2024-04-30,"[['Bennett', 'Michael Timothy', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
245,2109.03459,Youngjune Lee,Youngjune Lee and Kee-Eung Kim,"Dual Correction Strategy for Ranking Distillation in Top-N Recommender
  System",CIKM 2021,,10.1145/3459637.3482093,,cs.IR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge Distillation (KD), which transfers the knowledge of a well-trained
large model (teacher) to a small model (student), has become an important area
of research for practical deployment of recommender systems. Recently, Relaxed
Ranking Distillation (RRD) has shown that distilling the ranking information in
the recommendation list significantly improves the performance. However, the
method still has limitations in that 1) it does not fully utilize the
prediction errors of the student model, which makes the training not fully
efficient, and 2) it only distills the user-side ranking information, which
provides an insufficient view under the sparse implicit feedback. This paper
presents Dual Correction strategy for Distillation (DCD), which transfers the
ranking information from the teacher model to the student model in a more
efficient manner. Most importantly, DCD uses the discrepancy between the
teacher model and the student model predictions to decide which knowledge to be
distilled. By doing so, DCD essentially provides the learning guidance tailored
to ""correcting"" what the student model has failed to accurately predict. This
process is applied for transferring the ranking information from the user-side
as well as the item-side to address sparse implicit user feedback. Our
experiments show that the proposed method outperforms the state-of-the-art
baselines, and ablation studies validate the effectiveness of each component.
","[{'version': 'v1', 'created': 'Wed, 8 Sep 2021 07:00:45 GMT'}, {'version': 'v2', 'created': 'Tue, 5 Sep 2023 09:06:10 GMT'}, {'version': 'v3', 'created': 'Tue, 3 Oct 2023 14:55:06 GMT'}, {'version': 'v4', 'created': 'Wed, 15 May 2024 05:24:19 GMT'}]",2024-05-16,"[['Lee', 'Youngjune', ''], ['Kim', 'Kee-Eung', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
246,2109.03958,Duong Nguyen,Duong Nguyen and Ronan Fablet,"TrAISformer -- A Transformer Network with Sparse Augmented Data
  Representation and Cross Entropy Loss for AIS-based Vessel Trajectory
  Prediction",,,10.1109/ACCESS.2024.3349957,,cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Vessel trajectory prediction plays a pivotal role in numerous maritime
applications and services. While the Automatic Identification System (AIS)
offers a rich source of information to address this task, forecasting vessel
trajectory using AIS data remains challenging, even for modern machine learning
techniques, because of the inherent heterogeneous and multimodal nature of
motion data. In this paper, we propose a novel approach to tackle these
challenges. We introduce a discrete, high-dimensional representation of AIS
data and a new loss function designed to explicitly address heterogeneity and
multimodality. The proposed model-referred to as TrAISformer-is a modified
transformer network that extracts long-term temporal patterns in AIS vessel
trajectories in the proposed enriched space to forecast the positions of
vessels several hours ahead. We report experimental results on real, publicly
available AIS data. TrAISformer significantly outperforms state-of-the-art
methods, with an average prediction performance below 10 nautical miles up to
~10 hours.
","[{'version': 'v1', 'created': 'Wed, 8 Sep 2021 22:44:33 GMT'}, {'version': 'v2', 'created': 'Thu, 5 Jan 2023 20:37:47 GMT'}, {'version': 'v3', 'created': 'Sun, 14 May 2023 13:47:01 GMT'}, {'version': 'v4', 'created': 'Wed, 3 Jan 2024 14:22:51 GMT'}]",2024-01-09,"[['Nguyen', 'Duong', ''], ['Fablet', 'Ronan', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
247,2109.06911,Mohammed Amine Bennouna,Amine Bennouna and Bart P.G. Van Parys,"Learning and Decision-Making with Data: Optimal Formulations and Phase
  Transitions",,,,,stat.ML cs.LG math.OC math.ST stat.TH,http://creativecommons.org/licenses/by/4.0/,"  We study the problem of designing optimal learning and decision-making
formulations when only historical data is available. Prior work typically
commits to a particular class of data-driven formulation and subsequently tries
to establish out-of-sample performance guarantees. We take here the opposite
approach. We define first a sensible yard stick with which to measure the
quality of any data-driven formulation and subsequently seek to find an optimal
such formulation. Informally, any data-driven formulation can be seen to
balance a measure of proximity of the estimated cost to the actual cost while
guaranteeing a level of out-of-sample performance. Given an acceptable level of
out-of-sample performance, we construct explicitly a data-driven formulation
that is uniformly closer to the true cost than any other formulation enjoying
the same out-of-sample performance. We show the existence of three distinct
out-of-sample performance regimes (a superexponential regime, an exponential
regime and a subexponential regime) between which the nature of the optimal
data-driven formulation experiences a phase transition. The optimal data-driven
formulations can be interpreted as a classically robust formulation in the
superexponential regime, an entropic distributionally robust formulation in the
exponential regime and finally a variance penalized formulation in the
subexponential regime. This final observation unveils a surprising connection
between these three, at first glance seemingly unrelated, data-driven
formulations which until now remained hidden.
","[{'version': 'v1', 'created': 'Tue, 14 Sep 2021 18:20:15 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Sep 2022 17:40:19 GMT'}, {'version': 'v3', 'created': 'Mon, 11 Mar 2024 21:28:38 GMT'}]",2024-03-13,"[['Bennouna', 'Amine', ''], ['Van Parys', 'Bart P. G.', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
248,2109.08792,Alex Chohlas-Wood,"Alex Chohlas-Wood, Madison Coots, Henry Zhu, Emma Brunskill, Sharad
  Goel","Learning to be Fair: A Consequentialist Approach to Equitable
  Decision-Making",,,,,cs.LG cs.CY,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In an attempt to make algorithms fair, the machine learning literature has
largely focused on equalizing decisions, outcomes, or error rates across race
or gender groups. To illustrate, consider a hypothetical government rideshare
program that provides transportation assistance to low-income people with
upcoming court dates. Following this literature, one might allocate rides to
those with the highest estimated treatment effect per dollar, while
constraining spending to be equal across race groups. That approach, however,
ignores the downstream consequences of such constraints, and, as a result, can
induce unexpected harms. For instance, if one demographic group lives farther
from court, enforcing equal spending would necessarily mean fewer total rides
provided, and potentially more people penalized for missing court. Here we
present an alternative framework for designing equitable algorithms that
foregrounds the consequences of decisions. In our approach, one first elicits
stakeholder preferences over the space of possible decisions and the resulting
outcomes--such as preferences for balancing spending parity against court
appearance rates. We then optimize over the space of decision policies, making
trade-offs in a way that maximizes the elicited utility. To do so, we develop
an algorithm for efficiently learning these optimal policies from data for a
large family of expressive utility functions. In particular, we use a
contextual bandit algorithm to explore the space of policies while solving a
convex optimization problem at each step to estimate the best policy based on
the available information. This consequentialist paradigm facilitates a more
holistic approach to equitable decision-making.
","[{'version': 'v1', 'created': 'Sat, 18 Sep 2021 00:30:43 GMT'}, {'version': 'v2', 'created': 'Wed, 16 Feb 2022 23:21:16 GMT'}, {'version': 'v3', 'created': 'Wed, 1 Feb 2023 17:01:51 GMT'}, {'version': 'v4', 'created': 'Mon, 12 Feb 2024 20:17:04 GMT'}]",2024-02-14,"[['Chohlas-Wood', 'Alex', ''], ['Coots', 'Madison', ''], ['Zhu', 'Henry', ''], ['Brunskill', 'Emma', ''], ['Goel', 'Sharad', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
249,2109.09506,Junfeng Hu,"Junfeng Hu, Yuxuan Liang, Zhencheng Fan, Li Liu, Yifang Yin, Roger
  Zimmermann",Decoupling Long- and Short-Term Patterns in Spatiotemporal Inference,,,10.1109/TNNLS.2023.3293814,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Sensors are the key to environmental monitoring, which impart benefits to
smart cities in many aspects, such as providing real-time air quality
information to assist human decision-making. However, it is impractical to
deploy massive sensors due to the expensive costs, resulting in sparse data
collection. Therefore, how to get fine-grained data measurement has long been a
pressing issue. In this paper, we aim to infer values at non-sensor locations
based on observations from available sensors (termed spatiotemporal inference),
where capturing spatiotemporal relationships among the data plays a critical
role. Our investigations reveal two significant insights that have not been
explored by previous works. Firstly, data exhibits distinct patterns at both
long- and short-term temporal scales, which should be analyzed separately.
Secondly, short-term patterns contain more delicate relations including those
across spatial and temporal dimensions simultaneously, while long-term patterns
involve high-level temporal trends. Based on these observations, we propose to
decouple the modeling of short-term and long-term patterns. Specifically, we
introduce a joint spatiotemporal graph attention network to learn the relations
across space and time for short-term patterns. Furthermore, we propose a graph
recurrent network with a time skip strategy to alleviate the gradient vanishing
problem and model the long-term dependencies. Experimental results on four
public real-world datasets demonstrate that our method effectively captures
both long- and short-term relations, achieving state-of-the-art performance
against existing methods.
","[{'version': 'v1', 'created': 'Thu, 16 Sep 2021 03:06:31 GMT'}, {'version': 'v2', 'created': 'Mon, 25 Oct 2021 01:45:55 GMT'}, {'version': 'v3', 'created': 'Tue, 23 Apr 2024 14:18:38 GMT'}]",2024-04-24,"[['Hu', 'Junfeng', ''], ['Liang', 'Yuxuan', ''], ['Fan', 'Zhencheng', ''], ['Liu', 'Li', ''], ['Yin', 'Yifang', ''], ['Zimmermann', 'Roger', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
250,2109.09824,Geri Skenderi,"Geri Skenderi, Christian Joppi, Matteo Denitto, Marco Cristani","Well Googled is Half Done: Multimodal Forecasting of New Fashion Product
  Sales with Image-based Google Trends",Accepted by the Wiley Journal of Forecasting,,10.1002/for.3104,,cs.CV cs.LG,http://creativecommons.org/licenses/by/4.0/,"  New fashion product sales forecasting is a challenging problem that involves
many business dynamics and cannot be solved by classical forecasting
approaches. In this paper, we investigate the effectiveness of systematically
probing exogenous knowledge in the form of Google Trends time series and
combining it with multi-modal information related to a brand-new fashion item,
in order to effectively forecast its sales despite the lack of past data. In
particular, we propose a neural network-based approach, where an encoder learns
a representation of the exogenous time series, while the decoder forecasts the
sales based on the Google Trends encoding and the available visual and metadata
information. Our model works in a non-autoregressive manner, avoiding the
compounding effect of large first-step errors. As a second contribution, we
present VISUELLE, a publicly available dataset for the task of new fashion
product sales forecasting, containing multimodal information for 5577 real, new
products sold between 2016-2019 from Nunalie, an Italian fast-fashion company.
The dataset is equipped with images of products, metadata, related sales, and
associated Google Trends. We use VISUELLE to compare our approach against
state-of-the-art alternatives and several baselines, showing that our neural
network-based approach is the most accurate in terms of both percentage and
absolute error. It is worth noting that the addition of exogenous knowledge
boosts the forecasting accuracy by 1.5% in terms of Weighted Absolute
Percentage Error (WAPE), revealing the importance of exploiting informative
external information. The code and dataset are both available at
https://github.com/HumaticsLAB/GTM-Transformer.
","[{'version': 'v1', 'created': 'Mon, 20 Sep 2021 20:15:08 GMT'}, {'version': 'v2', 'created': 'Fri, 24 Sep 2021 07:17:51 GMT'}, {'version': 'v3', 'created': 'Fri, 8 Oct 2021 09:33:18 GMT'}, {'version': 'v4', 'created': 'Tue, 26 Oct 2021 07:47:50 GMT'}, {'version': 'v5', 'created': 'Thu, 15 Sep 2022 12:06:59 GMT'}, {'version': 'v6', 'created': 'Sun, 14 Jan 2024 18:23:37 GMT'}]",2024-03-28,"[['Skenderi', 'Geri', ''], ['Joppi', 'Christian', ''], ['Denitto', 'Matteo', ''], ['Cristani', 'Marco', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
251,2109.10241,Zhi-Wei Wang,Zhi-Wei Wang and Samuel L. Braunstein,Life in a random universe: Sciama's argument reconsidered,"9 pages, 4 figures, pulished on The Astrophysical Journal",ApJ 962 55 (2024),10.3847/1538-4357/ad1994,,physics.hist-ph astro-ph.CO cs.AI gr-qc physics.data-an,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Random sampling in high dimensions has successfully been applied to phenomena
as diverse as nuclear resonances, neural networks and black hole evaporation.
Here we revisit an elegant argument by the British physicist Dennis Sciama,
which demonstrated that were our universe random, it would almost certainly
have a negligible chance for life. Under plausible assumptions, we show that a
random universe can masquerade as `intelligently designed,' with the
fundamental constants instead appearing to be fined tuned to be achieve the
highest probability for life to occur. For our universe, this mechanism may
only require there to be around a dozen currently unknown fundamental
constants. We speculate on broader applications for the mechanism we uncover.
","[{'version': 'v1', 'created': 'Fri, 10 Sep 2021 23:15:31 GMT'}, {'version': 'v2', 'created': 'Thu, 23 Sep 2021 03:11:09 GMT'}, {'version': 'v3', 'created': 'Mon, 12 Dec 2022 21:10:43 GMT'}, {'version': 'v4', 'created': 'Thu, 22 Feb 2024 13:35:10 GMT'}]",2024-02-23,"[['Wang', 'Zhi-Wei', ''], ['Braunstein', 'Samuel L.', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
252,2109.11762,William Won,"William Won, Saeed Rashidi, Sudarshan Srinivasan, Tushar Krishna","LIBRA: Enabling Workload-aware Multi-dimensional Network Topology
  Optimization for Distributed Training of Large AI Models","Contains 10 main pages, 21 figures, 3 tables","Proceedings of the 2024 IEEE International Symposium on
  Performance Analysis of Systems and Software (ISPASS '24)",10.1109/ispass61541.2024.00028,,cs.DC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As model sizes in machine learning continue to scale, distributed training is
necessary to accommodate model weights within each device and to reduce
training time. However, this comes with the expense of increased communication
overhead due to the exchange of gradients and activations, which become the
critical bottleneck of the end-to-end training process. In this work, we
motivate the design of multi-dimensional networks within machine learning
systems as a cost-efficient mechanism to enhance overall network bandwidth. We
also identify that optimal bandwidth allocation is pivotal for
multi-dimensional networks to ensure efficient resource utilization. We
introduce LIBRA, a framework specifically focused on optimizing
multi-dimensional fabric architectures. Through case studies, we demonstrate
the value of LIBRA, both in architecting optimized fabrics under diverse
constraints and in enabling co-optimization opportunities.
","[{'version': 'v1', 'created': 'Fri, 24 Sep 2021 06:22:28 GMT'}, {'version': 'v2', 'created': 'Sun, 5 May 2024 05:53:40 GMT'}]",2024-05-07,"[['Won', 'William', ''], ['Rashidi', 'Saeed', ''], ['Srinivasan', 'Sudarshan', ''], ['Krishna', 'Tushar', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
253,2109.12512,Yule Wang,"Yule Wang, Qiang Luo, Yue Ding, Yunzhe Li, Dong Wang, Hongbo Deng","DemiNet: Dependency-Aware Multi-Interest Network with Self-Supervised
  Graph Learning for Click-Through Rate Prediction",,,,,cs.IR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose a novel model named DemiNet (short for
DEpendency-Aware Multi-Interest Network) to address the above two issues. To be
specific, we first consider various dependency types between item nodes and
perform dependency-aware heterogeneous attention for denoising and obtaining
accurate sequence item representations. Secondly, for multiple interests
extraction, multi-head attention is conducted on top of the graph embedding. To
filter out noisy inter-item correlations and enhance the robustness of
extracted interests, self-supervised interest learning is introduced to the
above two steps. Thirdly, to aggregate the multiple interests, interest experts
corresponding to different interest routes give rating scores respectively,
while a specialized network assigns the confidence of each score. Experimental
results on three real-world datasets demonstrate that the proposed DemiNet
significantly improves the overall recommendation performance over several
state-of-the-art baselines. Further studies verify the efficacy and
interpretability benefits brought by the fine-grained user interest modeling.
","[{'version': 'v1', 'created': 'Sun, 26 Sep 2021 07:10:45 GMT'}, {'version': 'v2', 'created': 'Sat, 9 Mar 2024 19:53:03 GMT'}]",2024-03-12,"[['Wang', 'Yule', ''], ['Luo', 'Qiang', ''], ['Ding', 'Yue', ''], ['Li', 'Yunzhe', ''], ['Wang', 'Dong', ''], ['Deng', 'Hongbo', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
254,2109.13004,"Moritz B\""ohle","Moritz B\""ohle, Mario Fritz, Bernt Schiele","Optimising for Interpretability: Convolutional Dynamic Alignment
  Networks","Extension of ""Convolutional Dynamic Alignment Networks for
  Interpretable Classifications"" (B\""ohle et al., CVPR 2021). arXiv admin note:
  substantial text overlap with arXiv:2104.00032","Published in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (Volume 45, Issue: 6, 01 June 2023, Page(s): 7625 - 7638)",10.1109/TPAMI.2022.3226041,,stat.ML cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce a new family of neural network models called Convolutional
Dynamic Alignment Networks (CoDA Nets), which are performant classifiers with a
high degree of inherent interpretability. Their core building blocks are
Dynamic Alignment Units (DAUs), which are optimised to transform their inputs
with dynamically computed weight vectors that align with task-relevant
patterns. As a result, CoDA Nets model the classification prediction through a
series of input-dependent linear transformations, allowing for linear
decomposition of the output into individual input contributions. Given the
alignment of the DAUs, the resulting contribution maps align with
discriminative input patterns. These model-inherent decompositions are of high
visual quality and outperform existing attribution methods under quantitative
metrics. Further, CoDA Nets constitute performant classifiers, achieving on par
results to ResNet and VGG models on e.g. CIFAR-10 and TinyImagenet. Lastly,
CoDA Nets can be combined with conventional neural network models to yield
powerful classifiers that more easily scale to complex datasets such as
Imagenet whilst exhibiting an increased interpretable depth, i.e., the output
can be explained well in terms of contributions from intermediate layers within
the network.
","[{'version': 'v1', 'created': 'Mon, 27 Sep 2021 12:39:46 GMT'}, {'version': 'v2', 'created': 'Mon, 15 Jan 2024 08:44:20 GMT'}]",2024-01-17,"[['Böhle', 'Moritz', ''], ['Fritz', 'Mario', ''], ['Schiele', 'Bernt', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
255,2109.13078,Georgios D. Barmparis,"N. Almazova, G. D. Barmparis and G. P. Tsironis",Analysis of chaotic dynamical systems with autoencoders,,,10.1063/5.0055673,,cs.NE cs.AI nlin.CD,http://creativecommons.org/licenses/by/4.0/,"  We focus on chaotic dynamical systems and analyze their time series with the
use of autoencoders, i.e., configurations of neural networks that map identical
output to input. This analysis results in the determination of the latent space
dimension of each system and thus determines the minimal number of nodes
necessary to capture the essential information contained in the chaotic time
series. The constructed chaotic autoencoders generate similar maximal Lyapunov
exponents as the original chaotic systems and thus encompass their essential
dynamical information.
","[{'version': 'v1', 'created': 'Wed, 22 Sep 2021 11:18:57 GMT'}]",2024-06-19,"[['Almazova', 'N.', ''], ['Barmparis', 'G. D.', ''], ['Tsironis', 'G. P.', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
256,2109.14082,Rachel Luo,"Rachel Luo, Shengjia Zhao, Jonathan Kuck, Boris Ivanovic, Silvio
  Savarese, Edward Schmerling, Marco Pavone",Sample-Efficient Safety Assurances using Conformal Prediction,"International Journal of Robotics Research, 2023",,10.1177/02783649231221580,,cs.RO cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  When deploying machine learning models in high-stakes robotics applications,
the ability to detect unsafe situations is crucial. Early warning systems can
provide alerts when an unsafe situation is imminent (in the absence of
corrective action). To reliably improve safety, these warning systems should
have a provable false negative rate; i.e. of the situations that are unsafe,
fewer than $\epsilon$ will occur without an alert. In this work, we present a
framework that combines a statistical inference technique known as conformal
prediction with a simulator of robot/environment dynamics, in order to tune
warning systems to provably achieve an $\epsilon$ false negative rate using as
few as $1/\epsilon$ data points. We apply our framework to a driver warning
system and a robotic grasping application, and empirically demonstrate
guaranteed false negative rate while also observing low false detection
(positive) rate.
","[{'version': 'v1', 'created': 'Tue, 28 Sep 2021 23:00:30 GMT'}, {'version': 'v2', 'created': 'Fri, 25 Feb 2022 19:59:31 GMT'}, {'version': 'v3', 'created': 'Tue, 27 Sep 2022 23:17:47 GMT'}, {'version': 'v4', 'created': 'Mon, 20 Feb 2023 00:16:48 GMT'}, {'version': 'v5', 'created': 'Tue, 2 Jan 2024 18:23:59 GMT'}]",2024-01-03,"[['Luo', 'Rachel', ''], ['Zhao', 'Shengjia', ''], ['Kuck', 'Jonathan', ''], ['Ivanovic', 'Boris', ''], ['Savarese', 'Silvio', ''], ['Schmerling', 'Edward', ''], ['Pavone', 'Marco', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
257,2109.14501,Jayanta Dey,"Jayanta Dey, Ali Geisa, Ronak Mehta, Tyler M. Tomita, Hayden S. Helm,
  Haoyin Xu, Eric Eaton, Jeffery Dick, Carey E. Priebe, Joshua T. Vogelstein",Towards a theory of out-of-distribution learning,,,,,stat.ML cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Learning is a process wherein a learning agent enhances its performance
through exposure of experience or data. Throughout this journey, the agent may
encounter diverse learning environments. For example, data may be presented to
the leaner all at once, in multiple batches, or sequentially. Furthermore, the
distribution of each data sample could be either identical and independent
(iid) or non-iid. Additionally, there may exist computational and space
constraints for the deployment of the learning algorithms. The complexity of a
learning task can vary significantly, depending on the learning setup and the
constraints imposed upon it. However, it is worth noting that the current
literature lacks formal definitions for many of the in-distribution and
out-of-distribution learning paradigms. Establishing proper and universally
agreed-upon definitions for these learning setups is essential for thoroughly
exploring the evolution of ideas across different learning scenarios and
deriving generalized mathematical bounds for these learners. In this paper, we
aim to address this issue by proposing a chronological approach to defining
different learning tasks using the provably approximately correct (PAC)
learning framework. We will start with in-distribution learning and progress to
recently proposed lifelong or continual learning. We employ consistent
terminology and notation to demonstrate how each of these learning frameworks
represents a specific instance of a broader, more generalized concept of
learnability. Our hope is that this work will inspire a universally agreed-upon
approach to quantifying different types of learning, fostering greater
understanding and progress in the field.
","[{'version': 'v1', 'created': 'Wed, 29 Sep 2021 15:35:16 GMT'}, {'version': 'v2', 'created': 'Thu, 7 Oct 2021 17:46:04 GMT'}, {'version': 'v3', 'created': 'Wed, 24 Nov 2021 18:18:39 GMT'}, {'version': 'v4', 'created': 'Thu, 6 Jan 2022 16:46:24 GMT'}, {'version': 'v5', 'created': 'Fri, 7 Jun 2024 17:24:36 GMT'}]",2024-06-10,"[['Dey', 'Jayanta', ''], ['Geisa', 'Ali', ''], ['Mehta', 'Ronak', ''], ['Tomita', 'Tyler M.', ''], ['Helm', 'Hayden S.', ''], ['Xu', 'Haoyin', ''], ['Eaton', 'Eric', ''], ['Dick', 'Jeffery', ''], ['Priebe', 'Carey E.', ''], ['Vogelstein', 'Joshua T.', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
258,2110.00453,Federico Tavella,"Federico Tavella, Aphrodite Galata, Angelo Cangelosi",Phonology Recognition in American Sign Language,5 pages,,10.1109/ICASSP43922.2022.9747212,,cs.CL cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Inspired by recent developments in natural language processing, we propose a
novel approach to sign language processing based on phonological properties
validated by American Sign Language users. By taking advantage of datasets
composed of phonological data and people speaking sign language, we use a
pretrained deep model based on mesh reconstruction to extract the 3D
coordinates of the signers keypoints. Then, we train standard statistical and
deep machine learning models in order to assign phonological classes to each
temporal sequence of coordinates.
  Our paper introduces the idea of exploiting the phonological properties
manually assigned by sign language users to classify videos of people
performing signs by regressing a 3D mesh. We establish a new baseline for this
problem based on the statistical distribution of 725 different signs. Our
best-performing models achieve a micro-averaged F1-score of 58% for the major
location class and 70% for the sign type using statistical and deep learning
algorithms, compared to their corresponding baselines of 35% and 39%.
","[{'version': 'v1', 'created': 'Fri, 1 Oct 2021 14:38:47 GMT'}]",2024-07-25,"[['Tavella', 'Federico', ''], ['Galata', 'Aphrodite', ''], ['Cangelosi', 'Angelo', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
259,2110.01806,Teresa Head-Gordon,"Jie Li, Oufan Zhang, Yingze Wang, Kunyang Sun, Xingyi Guan, Dorian
  Bagni, Mojtaba Haghighatlari, Fiona L. Kearns, Conor Parks, Rommie E.Amaro,
  Teresa Head-Gordon","Mining for Potent Inhibitors through Artificial Intelligence and
  Physics: A Unified Methodology for Ligand Based and Structure Based Drug
  Design",,,,,q-bio.BM physics.bio-ph physics.chem-ph physics.data-an,http://creativecommons.org/licenses/by/4.0/,"  The viability of a new drug molecule is a time and resource intensive task
that makes computer-aided assessments a vital approach to rapid drug discovery.
Here we develop a machine learning algorithm, iMiner, that generates novel
inhibitor molecules for target proteins by combining deep reinforcement
learning with real-time 3D molecular docking using AutoDock Vina, thereby
simultaneously creating chemical novelty while constraining molecules for shape
and molecular compatibility with target active sites. Moreover, through the use
of various types of reward functions, we can generate new molecules that are
chemically similar to a target ligand, which can be grown from known protein
bound fragments, as well as to create molecules that enforce interactions with
target residues in the protein active site. The iMiner algorithm is embedded in
a composite workflow that filters out Pan-assay interference compounds,
Lipinski rule violations, and poor synthetic accessibility, with options for
cross-validation against other docking scoring functions and automation of a
molecular dynamics simulation to measure pose stability. Because our approach
only relies on the structure of the target protein, iMiner can be easily
adapted for future development of other inhibitors or small molecule
therapeutics of any target protein.
","[{'version': 'v1', 'created': 'Tue, 5 Oct 2021 03:45:15 GMT'}, {'version': 'v2', 'created': 'Wed, 10 Jan 2024 18:14:37 GMT'}]",2024-01-11,"[['Li', 'Jie', ''], ['Zhang', 'Oufan', ''], ['Wang', 'Yingze', ''], ['Sun', 'Kunyang', ''], ['Guan', 'Xingyi', ''], ['Bagni', 'Dorian', ''], ['Haghighatlari', 'Mojtaba', ''], ['Kearns', 'Fiona L.', ''], ['Parks', 'Conor', ''], ['Amaro', 'Rommie E.', ''], ['Head-Gordon', 'Teresa', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
260,2110.01808,Robert McLachlan,"Stephen R Marsland, Robert I McLachlan, and Christopher Tuffley","Parametric study of E. coli incidence with reference to the New Zealand
  freshwater standards and the Manawat\=u-Whanganui region",,"ANZIAM J. 59 (MINZ2017) pp. M63-M125, 2022",10.21914/anziamj.v59.16838,,stat.AP physics.data-an,http://creativecommons.org/licenses/by/4.0/,"  The New Zealand National Policy Statement for Freshwater Management 2020 sets
several targets for freshwater quality, six of which are measurements of
rivers; others relate to lakes. Each regional council is required to monitor
freshwater quality and to respond as prescribed in order to meet the targets.
One target of particular public interest is based on four criteria determined
from recent E. coli readings, and concerns the health risk of swimming in a
river. However, the inherent variability of the data makes it difficult to
determine the water quality state and trend reliably, particularly using
traditional methods based on percentiles. Therefore, in this study we return to
the parametric lognormal model of E. coli distribution, from which the official
criteria were developed. We interpret the classification system in terms of the
parametric model and show that the parametric model can reduce uncertainty and
can incorporate more useful information, especially from very high E. coli
readings, and is suitable for censored data. We apply the parametric model for
state and trend to 135 sites in the Manawat\=u-Whanganui region.
","[{'version': 'v1', 'created': 'Tue, 5 Oct 2021 03:52:26 GMT'}]",2024-07-23,"[['Marsland', 'Stephen R', ''], ['McLachlan', 'Robert I', ''], ['Tuffley', 'Christopher', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
261,2110.01831,Michael Timothy Bennett,"Michael Timothy Bennett, Yoshihiro Maruyama","The Artificial Scientist: Logicist, Emergentist, and Universalist
  Approaches to Artificial General Intelligence",Accepted to the 14th Conference on Artificial General Intelligence,"Proceedings of the 14th International Conference on Artificial
  General Intelligence. 2021. Lecture Notes in Computer Science, vol 13154.
  Springer. pp. 45-54",10.1007/978-3-030-93758-4_6,,cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  We attempt to define what is necessary to construct an Artificial Scientist,
explore and evaluate several approaches to artificial general intelligence
(AGI) which may facilitate this, conclude that a unified or hybrid approach is
necessary and explore two theories that satisfy this requirement to some
degree.
","[{'version': 'v1', 'created': 'Tue, 5 Oct 2021 05:58:23 GMT'}]",2024-04-30,"[['Bennett', 'Michael Timothy', ''], ['Maruyama', 'Yoshihiro', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
262,2110.01835,Michael Timothy Bennett,Michael Timothy Bennett,"Compression, The Fermi Paradox and Artificial Super-Intelligence","Short paper accepted to the 14th Conference on Artificial General
  Intelligence","Proceedings of the 14th International Conference on Artificial
  General Intelligence. 2021. Lecture Notes in Computer Science, vol 13154.
  Springer. pp. 41-44",10.1007/978-3-030-93758-4_5,,cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
","[{'version': 'v1', 'created': 'Tue, 5 Oct 2021 06:17:02 GMT'}]",2024-04-30,"[['Bennett', 'Michael Timothy', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
263,2110.03105,Marlene Berke,"Marlene D. Berke, Zhangir Azerbayev, Mario Belledonne, Zenna Tavares,
  Julian Jara-Ettinger","MetaCOG: A Hierarchical Probabilistic Model for Learning Meta-Cognitive
  Visual Representations","23 pages, 7 figures, UAI 2024",,,,cs.AI cs.CV,http://creativecommons.org/licenses/by/4.0/,"  Humans have the capacity to question what we see and to recognize when our
vision is unreliable (e.g., when we realize that we are experiencing a visual
illusion). Inspired by this capacity, we present MetaCOG: a hierarchical
probabilistic model that can be attached to a neural object detector to monitor
its outputs and determine their reliability. MetaCOG achieves this by learning
a probabilistic model of the object detector's performance via Bayesian
inference -- i.e., a meta-cognitive representation of the network's propensity
to hallucinate or miss different object categories. Given a set of video frames
processed by an object detector, MetaCOG performs joint inference over the
underlying 3D scene and the detector's performance, grounding inference on a
basic assumption of object permanence. Paired with three neural object
detectors, we show that MetaCOG accurately recovers each detector's performance
parameters and improves the overall system's accuracy. We additionally show
that MetaCOG is robust to varying levels of error in object detector outputs,
showing proof-of-concept for a novel approach to the problem of detecting and
correcting errors in vision systems when ground-truth is not available.
","[{'version': 'v1', 'created': 'Wed, 6 Oct 2021 23:37:21 GMT'}, {'version': 'v2', 'created': 'Mon, 21 Feb 2022 16:10:21 GMT'}, {'version': 'v3', 'created': 'Tue, 29 Aug 2023 18:15:10 GMT'}, {'version': 'v4', 'created': 'Tue, 9 Jul 2024 02:02:24 GMT'}]",2024-07-10,"[['Berke', 'Marlene D.', ''], ['Azerbayev', 'Zhangir', ''], ['Belledonne', 'Mario', ''], ['Tavares', 'Zenna', ''], ['Jara-Ettinger', 'Julian', '']]",4,4_predicting_datasets_supervised_dataset,"predicting, datasets, supervised, dataset, prediction, data, predict, temporal, models, forecasting","predicting (0.35), datasets (0.34), supervised (0.33), dataset (0.33), prediction (0.31), data (0.31), predict (0.30), temporal (0.29), models (0.28), forecasting (0.26)","  We present a neural network framework for learning a survival model to
predict a time-to-event outcome while simultaneously learning a topic model
that reveals feature relationships. In particular, we model each subject as a
distribution over ""topics"", where a topic could, for instance, correspond to an
age group, a disorder, or a disease. The presence of a topic in a subject means
that specific clinical features are more likely to appear for the subject.
Topics encode information about related features and are learned in a
supervised manner to predict a time-to-event outcome. Our framework supports
combining many different topic and survival models; training the resulting
joint survival-topic model readily scales to large datasets using standard
neural net optimizers with minibatch gradient descent. For example, a special
case is to combine LDA with a Cox model, in which case a subject's distribution
over topics serves as the input feature vector to the Cox model. We explain how
to address practical implementation issues that arise when applying these
neural survival-supervised topic models to clinical data, including how to
visualize results to assist clinical interpretation. We study the effectiveness
of our proposed framework on seven clinical datasets on predicting time until
death as well as hospital ICU length of stay, where we find that neural
survival-supervised topic models achieve competitive accuracy with existing
approaches while yielding interpretable clinical topics that explain feature
relationships. Our code is available at:
https://github.com/georgehc/survival-topics
,   The individual data collected throughout patient follow-up constitute crucial
information for assessing the risk of a clinical event, and eventually for
adapting a therapeutic strategy. Joint models and landmark models have been
proposed to compute individual dynamic predictions from repeated measures to
one or two markers. However, they hardly extend to the case where the complete
patient history includes much more repeated markers possibly. Our objective was
thus to propose a solution for the dynamic prediction of a health event that
may exploit repeated measures of a possibly large number of markers. We
combined a landmark approach extended to endogenous markers history with
machine learning methods adapted to survival data. Each marker trajectory is
modeled using the information collected up to landmark time, and summary
variables that best capture the individual trajectories are derived. These
summaries and additional covariates are then included in different prediction
methods. To handle a possibly large dimensional history, we rely on machine
learning methods adapted to survival data, namely regularized regressions and
random survival forests, to predict the event from the landmark time, and we
show how they can be combined into a superlearner. Then, the performances are
evaluated by cross-validation using estimators of Brier Score and the area
under the Receiver Operating Characteristic curve adapted to censored data. We
demonstrate in a simulation study the benefits of machine learning survival
methods over standard survival models, especially in the case of numerous
and/or nonlinear relationships between the predictors and the event. We then
applied the methodology in two prediction contexts: a clinical context with the
prediction of death for patients with primary biliary cholangitis, and a public
health context with the prediction of death in the general elderly population
at different ages. Our methodology, implemented in R, enables the prediction of
an event using the entire longitudinal patient history, even when the number of
repeated markers is large. Although introduced with mixed models for the
repeated markers and methods for a single right censored time-to-event, our
method can be used with any other appropriate modeling technique for the
markers and can be easily extended to competing risks setting.
,   Scoring systems are highly interpretable and widely used to evaluate
time-to-event outcomes in healthcare research. However, existing time-to-event
scores are predominantly created ad-hoc using a few manually selected variables
based on clinician's knowledge, suggesting an unmet need for a robust and
efficient generic score-generating method.
  AutoScore was previously developed as an interpretable machine learning score
generator, integrated both machine learning and point-based scores in the
strong discriminability and accessibility. We have further extended it to
time-to-event data and developed AutoScore-Survival, for automatically
generating time-to-event scores with right-censored survival data. Random
survival forest provides an efficient solution for selecting variables, and Cox
regression was used for score weighting. We illustrated our method in a
real-life study of 90-day mortality of patients in intensive care units and
compared its performance with survival models (i.e., Cox) and the random
survival forest.
  The AutoScore-Survival-derived scoring model was more parsimonious than
survival models built using traditional variable selection methods (e.g.,
penalized likelihood approach and stepwise variable selection), and its
performance was comparable to survival models using the same set of variables.
Although AutoScore-Survival achieved a comparable integrated area under the
curve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores
generated are favorable in clinical applications because they are easier to
compute and interpret.
  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use
machine learning-based clinical score generator to studies of time-to-event
outcomes. It provides a systematic guideline to facilitate the future
development of time-to-event scores for clinical applications.
"
264,2110.03173,Yiyang Zhao,"Yiyang Zhao, Linnan Wang, Kevin Yang, Tianjun Zhang, Tian Guo,
  Yuandong Tian",Multi-objective Optimization by Learning Space Partitions,,International Conference on Learning Representations 2022,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In contrast to single-objective optimization (SOO), multi-objective
optimization (MOO) requires an optimizer to find the Pareto frontier, a subset
of feasible solutions that are not dominated by other feasible solutions. In
this paper, we propose LaMOO, a novel multi-objective optimizer that learns a
model from observed samples to partition the search space and then focus on
promising regions that are likely to contain a subset of the Pareto frontier.
The partitioning is based on the dominance number, which measures ""how close"" a
data point is to the Pareto frontier among existing samples. To account for
possible partition errors due to limited samples and model mismatch, we
leverage Monte Carlo Tree Search (MCTS) to exploit promising regions while
exploring suboptimal regions that may turn out to contain good solutions later.
Theoretically, we prove the efficacy of learning space partitioning via LaMOO
under certain assumptions. Empirically, on the HyperVolume (HV) benchmark, a
popular MOO metric, LaMOO substantially outperforms strong baselines on
multiple real-world MOO tasks, by up to 225% in sample efficiency for neural
architecture search on Nasbench201, and up to 10% for molecular design.
","[{'version': 'v1', 'created': 'Thu, 7 Oct 2021 03:56:19 GMT'}, {'version': 'v2', 'created': 'Sun, 10 Oct 2021 17:10:32 GMT'}, {'version': 'v3', 'created': 'Sat, 12 Mar 2022 02:22:05 GMT'}, {'version': 'v4', 'created': 'Mon, 19 Sep 2022 04:47:59 GMT'}]",2024-08-13,"[['Zhao', 'Yiyang', ''], ['Wang', 'Linnan', ''], ['Yang', 'Kevin', ''], ['Zhang', 'Tianjun', ''], ['Guo', 'Tian', ''], ['Tian', 'Yuandong', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
265,2110.03443,Jann Spiess,"Laura Blattner, Scott Nelson, Jann Spiess",Unpacking the Black Box: Regulating Algorithmic Decisions,,,,,econ.GN cs.AI cs.LG q-fin.EC stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  What should regulators of complex algorithms regulate? We propose a model of
oversight over 'black-box' algorithms used in high-stakes applications such as
lending, medical testing, or hiring. In our model, a regulator is limited in
how much she can learn about a black-box model deployed by an agent with
misaligned preferences. The regulator faces two choices: first, whether to
allow for the use of complex algorithms; and second, which key properties of
algorithms to regulate. We show that limiting agents to algorithms that are
simple enough to be fully transparent is inefficient as long as the
misalignment is limited and complex algorithms have sufficiently better
performance than simple ones. Allowing for complex algorithms can improve
welfare, but the gains depend on how the regulator regulates them. Regulation
that focuses on the overall average behavior of algorithms, for example based
on standard explainer tools, will generally be inefficient. Targeted regulation
that focuses on the source of incentive misalignment, e.g., excess false
positives or racial disparities, can provide second-best solutions. We provide
empirical support for our theoretical findings using an application in consumer
lending, where we document that complex models regulated based on
context-specific explanation tools outperform simple, fully transparent models.
This gain from complex models represents a Pareto improvement across our
empirical applications that is preferred both by the lender and from the
perspective of the financial regulator.
","[{'version': 'v1', 'created': 'Tue, 5 Oct 2021 23:20:25 GMT'}, {'version': 'v2', 'created': 'Fri, 14 Jul 2023 17:55:10 GMT'}, {'version': 'v3', 'created': 'Fri, 31 May 2024 23:47:21 GMT'}]",2024-06-04,"[['Blattner', 'Laura', ''], ['Nelson', 'Scott', ''], ['Spiess', 'Jann', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
266,2110.05365,Peter S\'uken\'ik,"Peter S\'uken\'ik, Aleksei Kuvshinov, Stephan G\""unnemann",Intriguing Properties of Input-dependent Randomized Smoothing,,,,,cs.LG cs.AI stat.ML,http://creativecommons.org/licenses/by-sa/4.0/,"  Randomized smoothing is currently considered the state-of-the-art method to
obtain certifiably robust classifiers. Despite its remarkable performance, the
method is associated with various serious problems such as ""certified accuracy
waterfalls"", certification vs.\ accuracy trade-off, or even fairness issues.
Input-dependent smoothing approaches have been proposed with intention of
overcoming these flaws. However, we demonstrate that these methods lack formal
guarantees and so the resulting certificates are not justified. We show that in
general, the input-dependent smoothing suffers from the curse of
dimensionality, forcing the variance function to have low semi-elasticity. On
the other hand, we provide a theoretical and practical framework that enables
the usage of input-dependent smoothing even in the presence of the curse of
dimensionality, under strict restrictions. We present one concrete design of
the smoothing variance function and test it on CIFAR10 and MNIST. Our design
mitigates some of the problems of classical smoothing and is formally
underlined, yet further improvement of the design is still necessary.
","[{'version': 'v1', 'created': 'Mon, 11 Oct 2021 15:50:49 GMT'}, {'version': 'v2', 'created': 'Sun, 19 Jun 2022 14:39:14 GMT'}, {'version': 'v3', 'created': 'Fri, 8 Mar 2024 18:10:06 GMT'}]",2024-03-11,"[['Súkeník', 'Peter', ''], ['Kuvshinov', 'Aleksei', ''], ['Günnemann', 'Stephan', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
267,2110.06742,Amber Cassimon,"Amber Cassimon, Reinout Eyckerman, Siegfried Mercelis, Steven Latr\'e,
  Peter Hellinckx","A Review of the Deep Sea Treasure problem as a Multi-Objective
  Reinforcement Learning Benchmark","10 pages, 4 figures; Fixed Supplementary Materials PDF",,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, the authors investigate the Deep Sea Treasure (DST) problem as
proposed by Vamplew et al. Through a number of proofs, the authors show the
original DST problem to be quite basic, and not always representative of
practical Multi-Objective Optimization problems. In an attempt to bring theory
closer to practice, the authors propose an alternative, improved version of the
DST problem, and prove that some of the properties that simplify the original
DST problem no longer hold. The authors also provide a reference implementation
and perform a comparison between their implementation, and other existing
open-source implementations of the problem. Finally, the authors also provide a
complete Pareto-front for their new DST problem.
","[{'version': 'v1', 'created': 'Wed, 13 Oct 2021 14:21:21 GMT'}, {'version': 'v2', 'created': 'Fri, 15 Oct 2021 08:47:15 GMT'}, {'version': 'v3', 'created': 'Tue, 26 Oct 2021 07:40:04 GMT'}, {'version': 'v4', 'created': 'Tue, 21 May 2024 11:30:00 GMT'}]",2024-05-22,"[['Cassimon', 'Amber', ''], ['Eyckerman', 'Reinout', ''], ['Mercelis', 'Siegfried', ''], ['Latré', 'Steven', ''], ['Hellinckx', 'Peter', '']]",6,6_optimization_optimisation_constraints_constraint,"optimization, optimisation, constraints, constraint, scheduling, algorithms, planning, algorithm, search, benchmark","optimization (0.51), optimisation (0.50), constraints (0.48), constraint (0.46), scheduling (0.41), algorithms (0.39), planning (0.36), algorithm (0.31), search (0.31), benchmark (0.28)","  Molecule optimization is a fundamental task for accelerating drug discovery,
with the goal of generating new valid molecules that maximize multiple drug
properties while maintaining similarity to the input molecule. Existing
generative models and reinforcement learning approaches made initial success,
but still face difficulties in simultaneously optimizing multiple drug
properties. To address such challenges, we propose the MultI-constraint
MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule
as an initial guess and sample molecules from the target distribution. MIMOSA
first pretrains two property agnostic graph neural networks (GNNs) for molecule
topology and substructure-type prediction, where a substructure can be either
atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and
employs three basic substructure operations (add, replace, delete) to generate
new molecules and associated weights. The weights can encode multiple
constraints including similarity and drug property constraints, upon which we
select promising molecules for next iteration. MIMOSA enables flexible encoding
of multiple property- and similarity-constraints and can efficiently generate
new molecules that satisfy various property constraints and achieved up to
49.6% relative improvement over the best baseline in terms of success rate. The
code repository (including readme file, data preprocessing and model
construction, evaluation) is available https://github.com/futianfan/MIMOSA.
,   Single-objective bilevel optimization is a specialized form of constraint
optimization problems where one of the constraints is an optimization problem
itself. These problems are typically non-convex and strongly NP-Hard. Recently,
there has been an increased interest from the evolutionary computation
community to model bilevel problems due to its applicability in the real-world
applications for decision-making problems. In this work, a partial nested
evolutionary approach with a local heuristic search has been proposed to solve
the benchmark problems and have outstanding results. This approach relies on
the concept of intermarriage-crossover in search of feasible regions by
exploiting information from the constraints. A new variant has also been
proposed to the commonly used convergence approaches, i.e., optimistic and
pessimistic. It is called extreme optimistic approach. The experimental results
demonstrate the algorithm converges differently to known optimum solutions with
the optimistic variants. Optimistic approach also outperforms pessimistic
approach. Comparative statistical analysis of our approach with other recently
published partial to complete evolutionary approaches demonstrates very
competitive results.
,   Many science and engineering applications require finding solutions to
planning and optimization problems by satisfying a set of constraints. These
constraint problems (CPs) are typically NP-complete and can be formalized as
constraint satisfaction problems (CSPs) or constraint optimization problems
(COPs). Evolutionary algorithms (EAs) are good solvers for optimization
problems ubiquitous in various problem domains, however traditional operators
for EAs are 'blind' to constraints or generally use problem dependent objective
functions; as they do not exploit information from the constraints in search
for solutions. A variation of EA, Intelligent constraint handling evolutionary
algorithm (ICHEA), has been demonstrated to be a versatile constraints-guided
EA for continuous constrained problems in our earlier works in (Sharma and
Sharma, 2012) where it extracts information from constraints and exploits it in
the evolutionary search to make the search more efficient. In this paper ICHEA
has been demonstrated to solve benchmark exam timetabling problems, a classic
COP. The presented approach demonstrates competitive results with other
state-of-the-art approaches in EAs in terms of quality of solutions. ICHEA
first uses its inter-marriage crossover operator to satisfy all the given
constraints incrementally and then uses combination of traditional and enhanced
operators to optimize the solution. Generally CPs solved by EAs are problem
dependent penalty based fitness functions. We also proposed a generic
preference based solution model that does not require a problem dependent
fitness function, however currently it only works for mutually exclusive
constraints.
"
268,2110.07478,Nan Wu,David B Dunson and Nan Wu,Inferring Manifolds From Noisy Data Using Gaussian Processes,"51 pages, 20 figures",,,,stat.ML cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
","[{'version': 'v1', 'created': 'Thu, 14 Oct 2021 15:50:38 GMT'}, {'version': 'v2', 'created': 'Sun, 2 Oct 2022 18:01:27 GMT'}, {'version': 'v3', 'created': 'Fri, 24 May 2024 22:35:29 GMT'}]",2024-05-28,"[['Dunson', 'David B', ''], ['Wu', 'Nan', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
269,2110.08626,Alexey Vasyukov,"A. Stankevich, I. Nechepurenko, A. Shevchenko, L. Gremyachikh, A.
  Ustyuzhanin, A. Vasyukov","Learning velocity model for complex media with deep convolutional neural
  networks","14 pages, 6 figures, 6 tables",,10.1134/S1995080224010499,,cs.LG cs.SD eess.AS,http://creativecommons.org/licenses/by-sa/4.0/,"  The paper considers the problem of velocity model acquisition for a complex
media based on boundary measurements. The acoustic model is used to describe
the media. We used an open-source dataset of velocity distributions to compare
the presented results with the previous works directly. Forward modeling is
performed using the grid-characteristic numerical method. The inverse problem
is solved using deep convolutional neural networks. Modifications for a
baseline UNet architecture are proposed to improve both structural similarity
index measure quantitative correspondence of the velocity profiles with the
ground truth. We evaluate our enhancements and demonstrate the statistical
significance of the results.
","[{'version': 'v1', 'created': 'Sat, 16 Oct 2021 17:52:08 GMT'}]",2024-05-14,"[['Stankevich', 'A.', ''], ['Nechepurenko', 'I.', ''], ['Shevchenko', 'A.', ''], ['Gremyachikh', 'L.', ''], ['Ustyuzhanin', 'A.', ''], ['Vasyukov', 'A.', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
270,2110.0924,Nardine Osman,"Carles Sierra and Nardine Osman and Pablo Noriega and Jordi
  Sabater-Mir and Antoni Perell\'o",Value alignment: a formal approach,"accepted paper at the Responsible Artificial Intelligence Agents
  Workshop, of the 18th International Conference on Autonomous Agents and
  MultiAgent Systems (AAMAS 2019)","Responsible Artificial Intelligence Agents Workshop (RAIA) at
  AAMAS 2019",,,cs.AI,http://creativecommons.org/licenses/by-nc-nd/4.0/,"  principles that should govern autonomous AI systems. It essentially states
that a system's goals and behaviour should be aligned with human values. But
how to ensure value alignment? In this paper we first provide a formal model to
represent values through preferences and ways to compute value aggregations;
i.e. preferences with respect to a group of agents and/or preferences with
respect to sets of values. Value alignment is then defined, and computed, for a
given norm with respect to a given value through the increase/decrease that it
results in the preferences of future states of the world. We focus on norms as
it is norms that govern behaviour, and as such, the alignment of a given system
with a given value will be dictated by the norms the system follows.
","[{'version': 'v1', 'created': 'Mon, 18 Oct 2021 12:40:04 GMT'}]",2024-02-08,"[['Sierra', 'Carles', ''], ['Osman', 'Nardine', ''], ['Noriega', 'Pablo', ''], ['Sabater-Mir', 'Jordi', ''], ['Perelló', 'Antoni', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
271,2110.11281,Amir Dahari,"Amir Dahari, Steve Kench, Isaac Squires, Samuel J. Cooper","Fusion of complementary 2D and 3D mesostructural datasets using
  generative adversarial networks",,,10.1002/aenm.202202407,,cs.CV cs.LG,http://creativecommons.org/licenses/by/4.0/,"  Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
","[{'version': 'v1', 'created': 'Thu, 21 Oct 2021 17:07:57 GMT'}, {'version': 'v2', 'created': 'Fri, 22 Oct 2021 06:51:45 GMT'}, {'version': 'v3', 'created': 'Fri, 30 Sep 2022 14:47:45 GMT'}]",2024-08-07,"[['Dahari', 'Amir', ''], ['Kench', 'Steve', ''], ['Squires', 'Isaac', ''], ['Cooper', 'Samuel J.', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
272,2110.11334,Jingkang Yang,"Jingkang Yang, Kaiyang Zhou, Yixuan Li, Ziwei Liu",Generalized Out-of-Distribution Detection: A Survey,"Feel free to comment on our Overleaf manuscript:
  https://www.overleaf.com/9899719915wmccvdtwpkct#c25192",,,,cs.CV cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Out-of-distribution (OOD) detection is critical to ensuring the reliability
and safety of machine learning systems. For instance, in autonomous driving, we
would like the driving system to issue an alert and hand over the control to
humans when it detects unusual scenes or objects that it has never seen during
training time and cannot make a safe decision. The term, OOD detection, first
emerged in 2017 and since then has received increasing attention from the
research community, leading to a plethora of methods developed, ranging from
classification-based to density-based to distance-based ones. Meanwhile,
several other problems, including anomaly detection (AD), novelty detection
(ND), open set recognition (OSR), and outlier detection (OD), are closely
related to OOD detection in terms of motivation and methodology. Despite common
goals, these topics develop in isolation, and their subtle differences in
definition and problem setting often confuse readers and practitioners. In this
survey, we first present a unified framework called generalized OOD detection,
which encompasses the five aforementioned problems, i.e., AD, ND, OSR, OOD
detection, and OD. Under our framework, these five problems can be seen as
special cases or sub-tasks, and are easier to distinguish. We then review each
of these five areas by summarizing their recent technical developments, with a
special focus on OOD detection methodologies. We conclude this survey with open
challenges and potential research directions.
","[{'version': 'v1', 'created': 'Thu, 21 Oct 2021 17:59:41 GMT'}, {'version': 'v2', 'created': 'Wed, 3 Aug 2022 10:46:12 GMT'}, {'version': 'v3', 'created': 'Tue, 23 Jan 2024 07:36:33 GMT'}]",2024-01-24,"[['Yang', 'Jingkang', ''], ['Zhou', 'Kaiyang', ''], ['Li', 'Yixuan', ''], ['Liu', 'Ziwei', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
273,2110.11385,Sahisnu Mazumder,"Bing Liu, Eric Robertson, Scott Grigsby, Sahisnu Mazumder",Self-Initiated Open World Learning for Autonomous AI Agents,Published in AAAI 2022 Spring Symposium Series,,,,cs.AI cs.HC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  As more and more AI agents are used in practice, it is time to think about
how to make these agents fully autonomous so that they can learn by themselves
in a self-motivated and self-supervised manner rather than being retrained
periodically on the initiation of human engineers using expanded training data.
As the real-world is an open environment with unknowns or novelties, detecting
novelties or unknowns, characterizing them, accommodating or adapting to them,
gathering ground-truth training data, and incrementally learning the
unknowns/novelties are critical to making the agent more and more knowledgeable
and powerful over time. The key challenge is how to automate the process so
that it is carried out on the agent's own initiative and through its own
interactions with humans and the environment. Since an AI agent usually has a
performance task, characterizing each novelty becomes critical and necessary so
that the agent can formulate an appropriate response to adapt its behavior to
accommodate the novelty and to learn from it to improve the agent's adaptation
capability and task performance. The process goes continually without
termination. This paper proposes a theoretic framework for this learning
paradigm to promote the research of building Self-initiated Open world Learning
(SOL) agents. An example SOL agent is also described.
","[{'version': 'v1', 'created': 'Thu, 21 Oct 2021 18:11:02 GMT'}, {'version': 'v2', 'created': 'Fri, 11 Feb 2022 01:10:40 GMT'}, {'version': 'v3', 'created': 'Thu, 29 Feb 2024 04:50:25 GMT'}]",2024-03-01,"[['Liu', 'Bing', ''], ['Robertson', 'Eric', ''], ['Grigsby', 'Scott', ''], ['Mazumder', 'Sahisnu', '']]",8,8_ai_robots_intelligence_autonomous,"ai, robots, intelligence, autonomous, robot, agent, intelligent, cognitive, communication, agents","ai (0.56), robots (0.46), intelligence (0.45), autonomous (0.42), robot (0.42), agent (0.42), intelligent (0.40), cognitive (0.40), communication (0.40), agents (0.40)","  The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment.
,   As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.
,   In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.
"
274,2110.12484,Xinyu Piao,"XinYu Piao, DoangJoo Synn, JooYoung Park and Jong-Kook Kim","Enabling Large Batch Size Training for DNN Models Beyond the Memory
  Limit While Maintaining Performance",Published in IEEE Access,"IEEE Access (2023), Volume: 11, Page(s): 102981 - 102990",10.1109/ACCESS.2023.3312572,,cs.LG cs.DC,http://creativecommons.org/licenses/by/4.0/,"  Recent deep learning models are difficult to train using a large batch size,
because commodity machines may not have enough memory to accommodate both the
model and a large data batch size. The batch size is one of the
hyper-parameters used in the training model, and it is dependent on and is
limited by the target machine memory capacity because the batch size can only
fit into the remaining memory after the model is uploaded. Moreover, the data
item size is also an important factor because if each data item size is larger
then the batch size that can fit into the remaining memory becomes smaller.
This paper proposes a method called Micro-Batch Processing (MBP) to address
this problem. This method helps deep learning models to train by providing a
batch processing method that splits a batch into a size that can fit in the
remaining memory and processes them sequentially. After processing the small
batches individually, a loss normalization algorithm based on the gradient
accumulation is used to maintain the performance. The purpose of our method is
to allow deep learning models to train using larger batch sizes that exceed the
memory capacity of a system without increasing the memory size or using
multiple devices (GPUs).
","[{'version': 'v1', 'created': 'Sun, 24 Oct 2021 16:38:05 GMT'}, {'version': 'v2', 'created': 'Wed, 30 Nov 2022 04:23:50 GMT'}, {'version': 'v3', 'created': 'Tue, 2 Jul 2024 13:33:39 GMT'}]",2024-07-03,"[['Piao', 'XinYu', ''], ['Synn', 'DoangJoo', ''], ['Park', 'JooYoung', ''], ['Kim', 'Jong-Kook', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
275,2110.14048,Bo Liu,Bo Liu and Xingchao Liu and Xiaojie Jin and Peter Stone and Qiang Liu,Conflict-Averse Gradient Descent for Multi-task Learning,"20 pages, 6 figures, Conference on Neural Information Processing
  Systems, 2021",,,,cs.LG cs.AI,http://creativecommons.org/licenses/by/4.0/,"  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
","[{'version': 'v1', 'created': 'Tue, 26 Oct 2021 22:03:51 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Feb 2024 04:18:38 GMT'}]",2024-02-22,"[['Liu', 'Bo', ''], ['Liu', 'Xingchao', ''], ['Jin', 'Xiaojie', ''], ['Stone', 'Peter', ''], ['Liu', 'Qiang', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
276,2110.14053,Wenxi Wang,"Wenxi Wang, Yang Hu, Mohit Tiwari, Sarfraz Khurshid, Kenneth McMillan,
  Risto Miikkulainen",NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks,Paper has been accepted by ICLR'24,,,,cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Propositional satisfiability (SAT) is an NP-complete problem that impacts
many research fields, such as planning, verification, and security. Mainstream
modern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL)
algorithm. Recent work aimed to enhance CDCL SAT solvers using Graph Neural
Networks (GNNs). However, so far this approach either has not made solving more
effective, or required substantial GPU resources for frequent online model
inferences. Aiming to make GNN improvements practical, this paper proposes an
approach called NeuroBack, which builds on two insights: (1) predicting phases
(i.e., values) of variables appearing in the majority (or even all) of the
satisfying assignments are essential for CDCL SAT solving, and (2) it is
sufficient to query the neural model only once for the predictions before the
SAT solving starts. Once trained, the offline model inference allows NeuroBack
to execute exclusively on the CPU, removing its reliance on GPU resources. To
train NeuroBack, a new dataset called DataBack containing 120,286 data samples
is created. NeuroBack is implemented as an enhancement to a state-of-the-art
SAT solver called Kissat. As a result, it allowed Kissat to solve up to 5.2%
and 7.4% more problems on two recent SAT competition problem sets, SATCOMP-2022
and SATCOMP-2023, respectively. NeuroBack therefore shows how machine learning
can be harnessed to improve SAT solving in an effective and practical manner.
","[{'version': 'v1', 'created': 'Tue, 26 Oct 2021 22:08:22 GMT'}, {'version': 'v2', 'created': 'Thu, 28 Oct 2021 02:57:28 GMT'}, {'version': 'v3', 'created': 'Thu, 9 Jun 2022 19:13:56 GMT'}, {'version': 'v4', 'created': 'Wed, 4 Oct 2023 01:31:48 GMT'}, {'version': 'v5', 'created': 'Fri, 27 Oct 2023 16:30:44 GMT'}, {'version': 'v6', 'created': 'Tue, 28 Nov 2023 21:05:18 GMT'}, {'version': 'v7', 'created': 'Wed, 8 May 2024 18:23:10 GMT'}]",2024-05-10,"[['Wang', 'Wenxi', ''], ['Hu', 'Yang', ''], ['Tiwari', 'Mohit', ''], ['Khurshid', 'Sarfraz', ''], ['McMillan', 'Kenneth', ''], ['Miikkulainen', 'Risto', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
277,2110.14961,Miltiadis Kofinas,"Miltiadis Kofinas, Naveen Shankar Nagaraja, Efstratios Gavves","Roto-translated Local Coordinate Frames For Interacting Dynamical
  Systems",In NeurIPS 2021. Source code: https://github.com/mkofinas/locs,,,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modelling interactions is critical in learning complex dynamical systems,
namely systems of interacting objects with highly non-linear and time-dependent
behaviour. A large class of such systems can be formalized as
$\textit{geometric graphs}$, $\textit{i.e.}$, graphs with nodes positioned in
the Euclidean space given an $\textit{arbitrarily}$ chosen global coordinate
system, for instance vehicles in a traffic scene. Notwithstanding the arbitrary
global coordinate system, the governing dynamics of the respective dynamical
systems are invariant to rotations and translations, also known as
$\textit{Galilean invariance}$. As ignoring these invariances leads to worse
generalization, in this work we propose local coordinate frames per node-object
to induce roto-translation invariance to the geometric graph of the interacting
dynamical system. Further, the local coordinate frames allow for a natural
definition of anisotropic filtering in graph neural networks. Experiments in
traffic scenes, 3D motion capture, and colliding particles demonstrate that the
proposed approach comfortably outperforms the recent state-of-the-art.
","[{'version': 'v1', 'created': 'Thu, 28 Oct 2021 09:03:37 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Dec 2022 17:19:13 GMT'}, {'version': 'v3', 'created': 'Wed, 20 Mar 2024 16:45:00 GMT'}]",2024-03-21,"[['Kofinas', 'Miltiadis', ''], ['Nagaraja', 'Naveen Shankar', ''], ['Gavves', 'Efstratios', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
278,2110.15188,Bastian Rieck,"Michael F. Adamer, Edward De Brouwer, Leslie O'Bray, Bastian Rieck",The magnitude vector of images,,,10.1007/s41468-024-00182-9,,cs.LG cs.CV math.AT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The magnitude of a finite metric space has recently emerged as a novel
invariant quantity, allowing to measure the effective size of a metric space.
Despite encouraging first results demonstrating the descriptive abilities of
the magnitude, such as being able to detect the boundary of a metric space, the
potential use cases of magnitude remain under-explored. In this work, we
investigate the properties of the magnitude on images, an important data
modality in many machine learning applications. By endowing each individual
images with its own metric space, we are able to define the concept of
magnitude on images and analyse the individual contribution of each pixel with
the magnitude vector. In particular, we theoretically show that the previously
known properties of boundary detection translate to edge detection abilities in
images. Furthermore, we demonstrate practical use cases of magnitude for
machine learning applications and propose a novel magnitude model that consists
of a computationally efficient magnitude computation and a learnable metric. By
doing so, we address the computational hurdle that used to make magnitude
impractical for many applications and open the way for the adoption of
magnitude in machine learning research.
","[{'version': 'v1', 'created': 'Thu, 28 Oct 2021 15:05:08 GMT'}, {'version': 'v2', 'created': 'Fri, 7 Oct 2022 16:07:33 GMT'}]",2024-07-08,"[['Adamer', 'Michael F.', ''], ['De Brouwer', 'Edward', ''], [""O'Bray"", 'Leslie', ''], ['Rieck', 'Bastian', '']]",1,1_learning_regularization_classifiers_forgetting,"learning, regularization, classifiers, forgetting, classification, learn, training, memory, trained, recognition","learning (0.47), regularization (0.40), classifiers (0.39), forgetting (0.36), classification (0.36), learn (0.36), training (0.34), memory (0.34), trained (0.34), recognition (0.32)","  In lifelong learning, data are used to improve performance not only on the
present task, but also on past and future (unencountered) tasks. While typical
transfer learning algorithms can improve performance on future tasks, their
performance on prior tasks degrades upon learning new tasks (called
forgetting). Many recent approaches for continual or lifelong learning have
attempted to maintain performance on old tasks given new tasks. But striving to
avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning
should be to use data to improve performance on both future tasks (forward
transfer) and past tasks (backward transfer). In this paper, we show that a
simple approach -- representation ensembling -- demonstrates both forward and
backward transfer in a variety of simulated and benchmark data scenarios,
including tabular, vision (CIFAR-100, 5-dataset, Split Mini-Imagenet, and
Food1k), and speech (spoken digit), in contrast to various reference
algorithms, which typically failed to transfer either forward or backward, or
both. Moreover, our proposed approach can flexibly operate with or without a
computational budget.
,   We study a fundamental transfer learning process from source to target linear
regression tasks, including overparameterized settings where there are more
learned parameters than data samples. The target task learning is addressed by
using its training data together with the parameters previously computed for
the source task. We define a transfer learning approach to the target task as a
linear regression optimization with a regularization on the distance between
the to-be-learned target parameters and the already-learned source parameters.
We analytically characterize the generalization performance of our transfer
learning approach and demonstrate its ability to resolve the peak in
generalization errors in double descent phenomena of the minimum L2-norm
solution to linear regression. Moreover, we show that for sufficiently related
tasks, the optimally tuned transfer learning approach can outperform the
optimally tuned ridge regression method, even when the true parameter vector
conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate
that transfer learning can beat the minimum mean square error (MMSE) solution
of the independent target task. Our results emphasize the ability of transfer
learning to extend the solution space to the target task and, by that, to have
an improved MMSE solution. We formulate the linear MMSE solution to our
transfer learning setting and point out its key differences from the common
design philosophy to transfer learning.
,   There is a growing interest in the learning-to-learn paradigm, also known as
meta-learning, where models infer on new tasks using a few training examples.
Recently, meta-learning based methods have been widely used in few-shot
classification, regression, reinforcement learning, and domain adaptation. The
model-agnostic meta-learning (MAML) algorithm is a well-known algorithm that
obtains model parameter initialization at meta-training phase. In the meta-test
phase, this initialization is rapidly adapted to new tasks by using gradient
descent. However, meta-learning models are prone to overfitting since there are
insufficient training tasks resulting in over-parameterized models with poor
generalization performance for unseen tasks. In this paper, we propose a
Bayesian neural network based MAML algorithm, which we refer to as the B-SMALL
algorithm. The proposed framework incorporates a sparse variational loss term
alongside the loss function of MAML, which uses a sparsifying approximated KL
divergence as a regularizer. We demonstrate the performance of B-MAML using
classification and regression tasks, and highlight that training a sparsifying
BNN using MAML indeed improves the parameter footprint of the model while
performing at par or even outperforming the MAML approach. We also illustrate
applicability of our approach in distributed sensor networks, where sparsity
and meta-learning can be beneficial.
"
279,2110.15447,SayedHassan Khatoonabadi,"SayedHassan Khatoonabadi, Diego Elias Costa, Rabe Abdalkareem, Emad
  Shihab","On Wasted Contributions: Understanding the Dynamics of
  Contributor-Abandoned Pull Requests","Manuscript accepted for publication in ACM Transactions on Software
  Engineering and Methodology (TOSEM)",,10.1145/3530785,,cs.SE cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Pull-based development has enabled numerous volunteers to contribute to
open-source projects with fewer barriers. Nevertheless, a considerable amount
of pull requests (PRs) with valid contributions are abandoned by their
contributors, wasting the effort and time put in by both the contributors and
maintainers. To better understand the underlying dynamics of
contributor-abandoned PRs, we conduct a mixed-methods study using both
quantitative and qualitative methods. We curate a dataset consisting of 265,325
PRs including 4,450 abandoned ones from ten popular and mature GitHub projects
and measure 16 features characterizing PRs, contributors, review processes, and
projects. Using statistical and machine learning techniques, we find that
complex PRs, novice contributors, and lengthy reviews have a higher probability
of abandonment and the rate of PR abandonment fluctuates alongside the
projects' maturity or workload. To identify why contributors abandon their PRs,
we also manually examine a random sample of 354 abandoned PRs. We observe that
the most frequent abandonment reasons are related to the obstacles faced by
contributors, followed by the hurdles imposed by maintainers during the review
process. Finally, we survey the top core maintainers of the studied projects to
understand their perspectives on dealing with PR abandonment and on our
findings.
","[{'version': 'v1', 'created': 'Thu, 28 Oct 2021 21:51:14 GMT'}, {'version': 'v2', 'created': 'Fri, 20 May 2022 17:35:48 GMT'}]",2024-08-14,"[['Khatoonabadi', 'SayedHassan', ''], ['Costa', 'Diego Elias', ''], ['Abdalkareem', 'Rabe', ''], ['Shihab', 'Emad', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
280,2110.15702,Chinmaya Kumar Dehury Dr.,"Chinmaya Kumar Dehury, Shivananda Poojara, Satish Narayana Srirama","DeF-DReL: Systematic Deployment of Serverless Functions in Fog and Cloud
  environments using Deep Reinforcement Learning",This preprint version is currently under-consideration to submit,,10.1016/j.asoc.2023.111179,,cs.DC cs.AI,http://creativecommons.org/licenses/by/4.0/,"  Fog computing is introduced by shifting cloud resources towards the users'
proximity to mitigate the limitations possessed by cloud computing. Fog
environment made its limited resource available to a large number of users to
deploy their serverless applications, composed of several serverless functions.
One of the primary intentions behind introducing the fog environment is to
fulfil the demand of latency and location-sensitive serverless applications
through its limited resources. The recent research mainly focuses on assigning
maximum resources to such applications from the fog node and not taking full
advantage of the cloud environment. This introduces a negative impact in
providing the resources to a maximum number of connected users. To address this
issue, in this paper, we investigated the optimum percentage of a user's
request that should be fulfilled by fog and cloud. As a result, we proposed
DeF-DReL, a Systematic Deployment of Serverless Functions in Fog and Cloud
environments using Deep Reinforcement Learning, using several real-life
parameters, such as distance and latency of the users from nearby fog node,
user's priority, the priority of the serverless applications and their resource
demand, etc. The performance of the DeF-DReL algorithm is further compared with
recent related algorithms. From the simulation and comparison results, its
superiority over other algorithms and its applicability to the real-life
scenario can be clearly observed.
","[{'version': 'v1', 'created': 'Fri, 29 Oct 2021 12:10:54 GMT'}, {'version': 'v2', 'created': 'Fri, 5 Nov 2021 01:00:55 GMT'}, {'version': 'v3', 'created': 'Mon, 18 Apr 2022 13:33:48 GMT'}]",2024-02-09,"[['Dehury', 'Chinmaya Kumar', ''], ['Poojara', 'Shivananda', ''], ['Srirama', 'Satish Narayana', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
281,2111.00289,Kim Hammar,Kim Hammar and Rolf Stadler,Intrusion Prevention through Optimal Stopping,"Preprint; Submitted to IEEE for review. major revision 1/4 2022.
  arXiv admin note: substantial text overlap with arXiv:2106.07160","IEEE Transactions on Network and Service Management ( Volume: 19,
  Issue: 3, September 2022)",10.1109/TNSM.2022.3176781,,cs.LG cs.AI cs.CR cs.NI,http://creativecommons.org/licenses/by-sa/4.0/,"  We study automated intrusion prevention using reinforcement learning.
Following a novel approach, we formulate the problem of intrusion prevention as
an (optimal) multiple stopping problem. This formulation gives us insight into
the structure of optimal policies, which we show to have threshold properties.
For most practical cases, it is not feasible to obtain an optimal defender
policy using dynamic programming. We therefore develop a reinforcement learning
approach to approximate an optimal threshold policy. We introduce T-SPSA, an
efficient reinforcement learning algorithm that learns threshold policies
through stochastic approximation. We show that T-SPSA outperforms
state-of-the-art algorithms for our use case. Our overall method for learning
and validating policies includes two systems: a simulation system where
defender policies are incrementally learned and an emulation system where
statistics are produced that drive simulation runs and where learned policies
are evaluated. We show that this approach can produce effective defender
policies for a practical IT infrastructure.
","[{'version': 'v1', 'created': 'Sat, 30 Oct 2021 17:03:28 GMT'}, {'version': 'v2', 'created': 'Mon, 17 Jan 2022 12:40:49 GMT'}, {'version': 'v3', 'created': 'Mon, 24 Jan 2022 20:23:07 GMT'}, {'version': 'v4', 'created': 'Mon, 31 Jan 2022 17:14:58 GMT'}, {'version': 'v5', 'created': 'Sat, 5 Feb 2022 20:06:54 GMT'}, {'version': 'v6', 'created': 'Thu, 17 Feb 2022 19:55:49 GMT'}, {'version': 'v7', 'created': 'Fri, 1 Apr 2022 07:30:34 GMT'}]",2024-04-23,"[['Hammar', 'Kim', ''], ['Stadler', 'Rolf', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
282,2111.01014,Pavel Kolesnichenko Dr,Pavel V. Kolesnichenko and Donatas Zigmantas,"Neural-network-powered pulse reconstruction from one-dimensional
  interferometric cross-correlation traces","24 pages, 4 figures",,10.1364/OE.479638,,physics.optics physics.data-an physics.ins-det,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Any ultrafast optical spectroscopy experiment is usually accompanied by the
necessary routine of ultrashort-pulse characterisation. The majority of pulse
characterisation approaches solve either a one-dimensional (e.g. via
interferometry) or a two-dimensional (e.g. via frequency-resolved measurements)
problem. Solution of the two-dimensional pulse-retrieval problem is generally
more consistent due to problem's over-determined nature. In contrast, the
one-dimensional pulse-retrieval problem is impossible to solve unambiguously as
ultimately imposed by the fundamental theorem of algebra. In cases where
additional constraints are involved, the one-dimensional problem may be
possible to solve, however, existing iterative algorithms lack generality, and
often stagnate for complicated pulse shapes. Here we use a deep neural network
to unambiguously solve a constrained one-dimensional pulse-retrieval problem
and show the potential of fast, reliable, and complete pulse characterisation
using interferometric cross-correlation time traces (determined by the pulses
with partial spectral overlap).
","[{'version': 'v1', 'created': 'Mon, 1 Nov 2021 15:19:32 GMT'}]",2024-01-30,"[['Kolesnichenko', 'Pavel V.', ''], ['Zigmantas', 'Donatas', '']]",7,7_imaging_cnn_convolutional_neural,"imaging, cnn, convolutional, neural, deep, learning, scatter, 3d, scattering, images","imaging (0.38), cnn (0.35), convolutional (0.33), neural (0.33), deep (0.33), learning (0.30), scatter (0.28), 3d (0.26), scattering (0.25), images (0.25)","  Scatter due to interaction of photons with the imaged object is a fundamental
problem in X-ray Computed Tomography (CT). It manifests as various artifacts in
the reconstruction, making its abatement or correction critical for image
quality. Despite success in specific settings, hardware-based methods require
modification in the hardware, or increase in the scan time or dose. This
accounts for the great interest in software-based methods, including
Monte-Carlo based scatter estimation, analytical-numerical, and kernel-based
methods, with data-driven learning-based approaches demonstrated recently. In
this work, two novel physics-inspired deep-learning-based methods, PhILSCAT and
OV-PhILSCAT, are proposed. The methods estimate and correct for the scatter in
the acquired projection measurements. Different from previous works, they
incorporate both an initial reconstruction of the object of interest and the
scatter-corrupted measurements related to it, and use a deep neural network
architecture and cost function, both specifically tailored to the problem.
Numerical experiments with data generated by Monte-Carlo simulations of the
imaging of phantoms reveal consistent improvement over a recent purely
projection-domain deep neural network scatter correction method.
,   Modelling the impact of a material's mesostructure on device level
performance typically requires access to 3D image data containing all the
relevant information to define the geometry of the simulation domain. This
image data must include sufficient contrast between phases to distinguish each
material, be of high enough resolution to capture the key details, but also
have a large enough field-of-view to be representative of the material in
general. It is rarely possible to obtain data with all of these properties from
a single imaging technique. In this paper, we present a method for combining
information from pairs of distinct but complementary imaging techniques in
order to accurately reconstruct the desired multi-phase, high resolution,
representative, 3D images. Specifically, we use deep convolutional generative
adversarial networks to implement super-resolution, style transfer and
dimensionality expansion. To demonstrate the widespread applicability of this
tool, two pairs of datasets are used to validate the quality of the volumes
generated by fusing the information from paired imaging techniques. Three key
mesostructural metrics are calculated in each case to show the accuracy of this
method. Having confidence in the accuracy of our method, we then demonstrate
its power by applying to a real data pair from a lithium ion battery electrode,
where the required 3D high resolution image data is not available anywhere in
the literature. We believe this approach is superior to previously reported
statistical material reconstruction methods both in terms of its fidelity and
ease of use. Furthermore, much of the data required to train this algorithm
already exists in the literature, waiting to be combined. As such, our
open-access code could precipitate a step change by generating the hard to
obtain high quality image volumes necessary to simulate behaviour at the
mesoscale.
,   Phase retrieval, the problem of recovering lost phase information from
measured intensity alone, is an inverse problem that is widely faced in various
imaging modalities ranging from astronomy to nanoscale imaging. The current
process of phase recovery is iterative in nature. As a result, the image
formation is time-consuming and computationally expensive, precluding real-time
imaging. Here, we use 3D nanoscale X-ray imaging as a representative example to
develop a deep learning model to address this phase retrieval problem. We
introduce 3D-CDI-NN, a deep convolutional neural network and differential
programming framework trained to predict 3D structure and strain solely from
input 3D X-ray coherent scattering data. Our networks are designed to be
""physics-aware"" in multiple aspects; in that the physics of x-ray scattering
process is explicitly enforced in the training of the network, and the training
data are drawn from atomistic simulations that are representative of the
physics of the material. We further refine the neural network prediction
through a physics-based optimization procedure to enable maximum accuracy at
lowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction
pattern to real-space structure and strain hundreds of times faster than
traditional iterative phase retrieval methods, with negligible loss in
accuracy. Our integrated machine learning and differential programming solution
to the phase retrieval problem is broadly applicable across inverse problems in
other application areas.
"
283,2111.02164,Micha{\l} Romaszewski,"Micha{\l} Cholewa, Micha{\l} Romaszewski, Przemys{\l}aw G{\l}omb","Data structure > labels? Unsupervised heuristics for SVM hyperparameter
  estimation",,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Classification is one of the main areas of pattern recognition research, and
within it, Support Vector Machine (SVM) is one of the most popular methods
outside of field of deep learning -- and a de-facto reference for many Machine
Learning approaches. Its performance is determined by parameter selection,
which is usually achieved by a time-consuming grid search cross-validation
procedure (GSCV). That method, however relies on the availability and quality
of labelled examples and thus, when those are limited can be hindered. To
address that problem, there exist several unsupervised heuristics that take
advantage of the characteristics of the dataset for selecting parameters
instead of using class label information. While an order of magnitude faster,
they are scarcely used under the assumption that their results are
significantly worse than those of grid search. To challenge that assumption, we
have proposed improved heuristics for SVM parameter selection and tested it
against GSCV and state of the art heuristics on over 30 standard classification
datasets. The results show not only its advantage over state-of-art heuristics
but also that it is statistically no worse than GSCV.
","[{'version': 'v1', 'created': 'Wed, 3 Nov 2021 12:04:03 GMT'}, {'version': 'v2', 'created': 'Thu, 22 Feb 2024 08:05:10 GMT'}]",2024-02-23,"[['Cholewa', 'Michał', ''], ['Romaszewski', 'Michał', ''], ['Głomb', 'Przemysław', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
284,2111.02168,Alexandra Hotti,"Alexandra Hotti, Riccardo Sven Risuleo, Stefan Magureanu, Aref Moradi,
  Jens Lagergren","The Klarna Product Page Dataset: Web Element Nomination with Graph
  Neural Networks and Large Language Models","12 pages, 8 figures, 3 tables, under review",,,,cs.LG cs.CL cs.CV cs.HC cs.IR,http://creativecommons.org/licenses/by-nc-sa/4.0/,"  Web automation holds the potential to revolutionize how users interact with
the digital world, offering unparalleled assistance and simplifying tasks via
sophisticated computational methods. Central to this evolution is the web
element nomination task, which entails identifying unique elements on webpages.
Unfortunately, the development of algorithmic designs for web automation is
hampered by the scarcity of comprehensive and realistic datasets that reflect
the complexity faced by real-world applications on the Web. To address this, we
introduce the Klarna Product Page Dataset, a comprehensive and diverse
collection of webpages that surpasses existing datasets in richness and
variety. The dataset features 51,701 manually labeled product pages from 8,175
e-commerce websites across eight geographic regions, accompanied by a dataset
of rendered page screenshots. To initiate research on the Klarna Product Page
Dataset, we empirically benchmark a range of Graph Neural Networks (GNNs) on
the web element nomination task. We make three important contributions. First,
we found that a simple Convolutional GNN (GCN) outperforms complex
state-of-the-art nomination methods. Second, we introduce a training refinement
procedure that involves identifying a small number of relevant elements from
each page using the aforementioned GCN. These elements are then passed to a
large language model for the final nomination. This procedure significantly
improves the nomination accuracy by 16.8 percentage points on our challenging
dataset, without any need for fine-tuning. Finally, in response to another
prevalent challenge in this field - the abundance of training methodologies
suitable for element nomination - we introduce the Challenge Nomination
Training Procedure, a novel training approach that further boosts nomination
accuracy.
","[{'version': 'v1', 'created': 'Wed, 3 Nov 2021 12:13:52 GMT'}, {'version': 'v2', 'created': 'Tue, 9 Nov 2021 15:17:14 GMT'}, {'version': 'v3', 'created': 'Tue, 25 Oct 2022 14:27:11 GMT'}, {'version': 'v4', 'created': 'Fri, 23 Feb 2024 19:22:23 GMT'}]",2024-02-27,"[['Hotti', 'Alexandra', ''], ['Risuleo', 'Riccardo Sven', ''], ['Magureanu', 'Stefan', ''], ['Moradi', 'Aref', ''], ['Lagergren', 'Jens', '']]",0,0_anomaly_anomalies_datasets_dataset,"anomaly, anomalies, datasets, dataset, features, learning, neural, deep, detection, dnn","anomaly (0.44), anomalies (0.35), datasets (0.33), dataset (0.32), features (0.29), learning (0.28), neural (0.28), deep (0.28), detection (0.26), dnn (0.26)","  Semi-supervised anomaly detection aims to detect anomalies from normal
samples using a model that is trained on normal data. With recent advancements
in deep learning, researchers have designed efficient deep anomaly detection
methods. Existing works commonly use neural networks to map the data into a
more informative representation and then apply an anomaly detection algorithm.
In this paper, we propose a method, DASVDD, that jointly learns the parameters
of an autoencoder while minimizing the volume of an enclosing hyper-sphere on
its latent representation. We propose an anomaly score which is a combination
of autoencoder's reconstruction error and the distance from the center of the
enclosing hypersphere in the latent representation. Minimizing this anomaly
score aids us in learning the underlying distribution of the normal class
during training. Including the reconstruction error in the anomaly score
ensures that DASVDD does not suffer from the common hypersphere collapse issue
since the DASVDD model does not converge to the trivial solution of mapping all
inputs to a constant point in the latent representation. Experimental
evaluations on several benchmark datasets show that the proposed method
outperforms the commonly used state-of-the-art anomaly detection algorithms
while maintaining robust performance across different anomaly classes.
,   In many real-world AD applications including computer security and fraud
prevention, the anomaly detector must be configurable by the human analyst to
minimize the effort on false positives. One important way to configure the
detector is by providing true labels (nominal or anomaly) for a few instances.
Recent work on active anomaly discovery has shown that greedily querying the
top-scoring instance and tuning the weights of ensemble detectors based on
label feedback allows us to quickly discover true anomalies.
  This paper makes four main contributions to improve the state-of-the-art in
anomaly discovery using tree-based ensembles. First, we provide an important
insight that explains the practical successes of unsupervised tree-based
ensembles and active learning based on greedy query selection strategy. We also
present empirical results on real-world data to support our insights and
theoretical analysis to support active learning. Second, we develop a novel
batch active learning algorithm to improve the diversity of discovered
anomalies based on a formalism called compact description to describe the
discovered anomalies. Third, we develop a novel active learning algorithm to
handle streaming data setting. We present a data drift detection algorithm that
not only detects the drift robustly, but also allows us to take corrective
actions to adapt the anomaly detector in a principled manner. Fourth, we
present extensive experiments to evaluate our insights and our tree-based
active anomaly discovery algorithms in both batch and streaming data settings.
Our results show that active learning allows us to discover significantly more
anomalies than state-of-the-art unsupervised baselines, our batch active
learning algorithm discovers diverse anomalies, and our algorithms under the
streaming-data setup are competitive with the batch setup.
,   Anomaly detection is a challenging task for machine learning algorithms due
to the inherent class imbalance. It is costly and time-demanding to manually
analyse the observed data, thus usually only few known anomalies if any are
available. Inspired by generative models and the analysis of the hidden
activations of neural networks, we introduce a novel unsupervised anomaly
detection method called DA3D. Here, we use adversarial autoencoders to generate
anomalous counterexamples based on the normal data only. These artificial
anomalies used during training allow the detection of real, yet unseen
anomalies. With our novel generative approach, we transform the unsupervised
task of anomaly detection to a supervised one, which is more tractable by
machine learning and especially deep learning methods. DA3D surpasses the
performance of state-of-the-art anomaly detection methods in a purely
data-driven way, where no domain knowledge is required.
"
285,2111.03706,Mirko Goldmann,"Mirko Goldmann, Claudio R. Mirasso, Ingo Fischer, Miguel C. Soriano","Learn one size to infer all: Exploiting translational symmetries in
  delay-dynamical and spatio-temporal systems using scalable neural networks",,,,,cs.LG nlin.AO,http://creativecommons.org/licenses/by/4.0/,"  We design scalable neural networks adapted to translational symmetries in
dynamical systems, capable of inferring untrained high-dimensional dynamics for
different system sizes. We train these networks to predict the dynamics of
delay-dynamical and spatio-temporal systems for a single size. Then, we drive
the networks by their own predictions. We demonstrate that by scaling the size
of the trained network, we can predict the complex dynamics for larger or
smaller system sizes. Thus, the network learns from a single example and, by
exploiting symmetry properties, infers entire bifurcation diagrams.
","[{'version': 'v1', 'created': 'Fri, 5 Nov 2021 19:09:50 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Sep 2022 13:21:01 GMT'}, {'version': 'v3', 'created': 'Fri, 5 Jul 2024 08:51:37 GMT'}]",2024-07-08,"[['Goldmann', 'Mirko', ''], ['Mirasso', 'Claudio R.', ''], ['Fischer', 'Ingo', ''], ['Soriano', 'Miguel C.', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
286,2111.04597,Ye Tian,Ye Tian and Yang Feng,Neyman-Pearson Multi-class Classification via Cost-sensitive Learning,"114 pages, 18 figures",,,,stat.ML cs.LG stat.ME,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Most existing classification methods aim to minimize the overall
misclassification error rate. However, in applications such as loan default
prediction, different types of errors can have varying consequences. To address
this asymmetry issue, two popular paradigms have been developed: the
Neyman-Pearson (NP) paradigm and the cost-sensitive (CS) paradigm. Previous
studies on the NP paradigm have primarily focused on the binary case, while the
multi-class NP problem poses a greater challenge due to its unknown
feasibility. In this work, we tackle the multi-class NP problem by establishing
a connection with the CS problem via strong duality and propose two algorithms.
We extend the concept of NP oracle inequalities, crucial in binary
classifications, to NP oracle properties in the multi-class context. Our
algorithms satisfy these NP oracle properties under certain conditions.
Furthermore, we develop practical algorithms to assess the feasibility and
strong duality in multi-class NP problems, which can offer practitioners the
landscape of a multi-class NP problem with various target error levels.
Simulations and real data studies validate the effectiveness of our algorithms.
To our knowledge, this is the first study to address the multi-class NP problem
with theoretical guarantees. The proposed algorithms have been implemented in
the R package \texttt{npcs}, which is available on CRAN.
","[{'version': 'v1', 'created': 'Mon, 8 Nov 2021 16:09:39 GMT'}, {'version': 'v2', 'created': 'Mon, 23 Jan 2023 20:46:51 GMT'}, {'version': 'v3', 'created': 'Mon, 29 Apr 2024 04:59:11 GMT'}, {'version': 'v4', 'created': 'Thu, 1 Aug 2024 14:38:06 GMT'}]",2024-08-02,"[['Tian', 'Ye', ''], ['Feng', 'Yang', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
287,2111.04746,Max Hopkins,"Max Hopkins, Daniel M. Kane, Shachar Lovett, Gaurav Mahajan",Realizable Learning is All You Need,,"TheoretiCS, Volume 3 (February 6, 2024) theoretics:10093",10.46298/theoretics.24.2,,cs.LG stat.ML,http://creativecommons.org/licenses/by/4.0/,"  The equivalence of realizable and agnostic learnability is a fundamental
phenomenon in learning theory. With variants ranging from classical settings
like PAC learning and regression to recent trends such as adversarially robust
learning, it's surprising that we still lack a unified theory; traditional
proofs of the equivalence tend to be disparate, and rely on strong
model-specific assumptions like uniform convergence and sample compression.
  In this work, we give the first model-independent framework explaining the
equivalence of realizable and agnostic learnability: a three-line blackbox
reduction that simplifies, unifies, and extends our understanding across a wide
variety of settings. This includes models with no known characterization of
learnability such as learning with arbitrary distributional assumptions and
more general loss functions, as well as a host of other popular settings such
as robust learning, partial learning, fair learning, and the statistical query
model.
  More generally, we argue that the equivalence of realizable and agnostic
learning is actually a special case of a broader phenomenon we call property
generalization: any desirable property of a learning algorithm (e.g. noise
tolerance, privacy, stability) that can be satisfied over finite hypothesis
classes extends (possibly in some variation) to any learnable hypothesis class.
","[{'version': 'v1', 'created': 'Mon, 8 Nov 2021 19:00:00 GMT'}, {'version': 'v2', 'created': 'Sun, 25 Sep 2022 08:34:25 GMT'}, {'version': 'v3', 'created': 'Fri, 3 Feb 2023 12:06:15 GMT'}, {'version': 'v4', 'created': 'Sat, 3 Feb 2024 00:55:16 GMT'}]",2024-08-07,"[['Hopkins', 'Max', ''], ['Kane', 'Daniel M.', ''], ['Lovett', 'Shachar', ''], ['Mahajan', 'Gaurav', '']]",2,2_classification_learning_boosting_prediction,"classification, learning, boosting, prediction, complexity, optimal, algorithms, datasets, classes, accuracy","classification (0.43), learning (0.40), boosting (0.38), prediction (0.37), complexity (0.32), optimal (0.31), algorithms (0.31), datasets (0.29), classes (0.29), accuracy (0.27)","  The goal of multi-task learning is to enable more efficient learning than
single task learning by sharing model structures for a diverse set of tasks. A
standard multi-task learning objective is to minimize the average loss across
all tasks. While straightforward, using this objective often results in much
worse final performance for each task than learning them independently. A major
challenge in optimizing a multi-task model is the conflicting gradients, where
gradients of different task objectives are not well aligned so that following
the average gradient direction can be detrimental to specific tasks'
performance. Previous work has proposed several heuristics to manipulate the
task gradients for mitigating this problem. But most of them lack convergence
guarantee and/or could converge to any Pareto-stationary point. In this paper,
we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the
average loss function, while leveraging the worst local improvement of
individual tasks to regularize the algorithm trajectory. CAGrad balances the
objectives automatically and still provably converges to a minimum over the
average loss. It includes the regular gradient descent (GD) and the multiple
gradient descent algorithm (MGDA) in the multi-objective optimization (MOO)
literature as special cases. On a series of challenging multi-task supervised
learning and reinforcement learning tasks, CAGrad achieves improved performance
over prior state-of-the-art multi-objective gradient manipulation methods.
,   One of the central problems studied in the theory of machine learning is the
question of whether, for a given class of hypotheses, it is possible to
efficiently find a {consistent} hypothesis, i.e., which has zero training
error. While problems involving {\em convex} hypotheses have been extensively
studied, the question of whether efficient learning is possible for non-convex
hypotheses composed of possibly several disconnected regions is still less
understood. Although it has been shown quite a while ago that efficient
learning of weakly convex hypotheses, a parameterized relaxation of convex
hypotheses, is possible for the special case of Boolean functions, the question
of whether this idea can be developed into a generic paradigm has not been
studied yet. In this paper, we provide a positive answer and show that the
consistent hypothesis finding problem can indeed be solved in polynomial time
for a broad class of weakly convex hypotheses over metric spaces. To this end,
we propose a general domain-independent algorithm for finding consistent weakly
convex hypotheses and prove sufficient conditions for its efficiency that
characterize the corresponding hypothesis classes. To illustrate our general
algorithm and its properties, we discuss several non-trivial learning examples
to demonstrate how it can be used to efficiently solve the corresponding
consistent hypothesis finding problem. Without the weak convexity constraint,
these problems are known to be computationally intractable. We then proceed to
show that the general idea of our algorithm can even be extended to the case of
extensional weakly convex hypotheses, as it naturally arise, e.g., when
performing vertex classification in graphs. We prove that using our extended
algorithm, the problem can be solved in polynomial time provided the distances
in the domain can be computed efficiently.
,   Objective: Provide guidance on sample size considerations for developing
predictive models by empirically establishing the adequate sample size, which
balances the competing objectives of improving model performance and reducing
model complexity as well as computational requirements.
  Materials and Methods: We empirically assess the effect of sample size on
prediction performance and model complexity by generating learning curves for
81 prediction problems (23 outcomes predicted in a depression cohort, 58
outcomes predicted in a hypertension cohort) in three large observational
health databases, requiring training of 17,248 prediction models. The adequate
sample size was defined as the sample size for which the performance of a model
equalled the maximum model performance minus a small threshold value.
  Results: The adequate sample size achieves a median reduction of the number
of observations of 9.5%, 37.3%, 58.5%, and 78.5% for the thresholds of 0.001,
0.005, 0.01, and 0.02, respectively. The median reduction of the number of
predictors in the models was 8.6%, 32.2%, 48.2%, and 68.3% for the thresholds
of 0.001, 0.005, 0.01, and 0.02, respectively.
  Discussion: Based on our results a conservative, yet significant, reduction
in sample size and model complexity can be estimated for future prediction
work. Though, if a researcher is willing to generate a learning curve a much
larger reduction of the model complexity may be possible as suggested by a
large outcome-dependent variability.
  Conclusion: Our results suggest that in most cases only a fraction of the
available data was sufficient to produce a model close to the performance of
one developed on the full data set, but with a substantially reduced model
complexity.
"
288,2111.06464,Lukasz Kucinski,"{\L}ukasz Kuci\'nski, Tomasz Korbak, Pawe{\l} Ko{\l}odziej, Piotr
  Mi{\l}o\'s","Catalytic Role Of Noise And Necessity Of Inductive Biases In The
  Emergence Of Compositional Communication",NeurIPS 2021,,,,cs.LG cs.AI cs.CL,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Communication is compositional if complex signals can be represented as a
combination of simpler subparts. In this paper, we theoretically show that
inductive biases on both the training framework and the data are needed to
develop a compositional communication. Moreover, we prove that compositionality
spontaneously arises in the signaling games, where agents communicate over a
noisy channel. We experimentally confirm that a range of noise levels, which
depends on the model and the data, indeed promotes compositionality. Finally,
we provide a comprehensive study of this dependence and report results in terms
of recently studied compositionality metrics: topographical similarity,
conflict count, and context independence.
","[{'version': 'v1', 'created': 'Thu, 11 Nov 2021 21:15:21 GMT'}, {'version': 'v2', 'created': 'Wed, 3 Apr 2024 15:39:55 GMT'}]",2024-04-04,"[['Kuciński', 'Łukasz', ''], ['Korbak', 'Tomasz', ''], ['Kołodziej', 'Paweł', ''], ['Miłoś', 'Piotr', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
289,2111.07897,Jitendra Tugnait,Jitendra K. Tugnait,"On Sparse High-Dimensional Graphical Model Learning For Dependent Time
  Series","20 pages, 5 figures. Published in Signal Processing. Latest version
  (June 4, 2024) corrects some typos","Signal Processing, vol. 197, pp. 1-18, Aug. 2022, Article 108539",10.1016/j.sigpro.2022.108539,,eess.SP cs.LG stat.ML,http://creativecommons.org/licenses/by/4.0/,"  We consider the problem of inferring the conditional independence graph (CIG)
of a sparse, high-dimensional stationary multivariate Gaussian time series. A
sparse-group lasso-based frequency-domain formulation of the problem based on
frequency-domain sufficient statistic for the observed time series is
presented. We investigate an alternating direction method of multipliers (ADMM)
approach for optimization of the sparse-group lasso penalized log-likelihood.
We provide sufficient conditions for convergence in the Frobenius norm of the
inverse PSD estimators to the true value, jointly across all frequencies, where
the number of frequencies are allowed to increase with sample size. This
results also yields a rate of convergence. We also empirically investigate
selection of the tuning parameters based on Bayesian information criterion, and
illustrate our approach using numerical examples utilizing both synthetic and
real data.
","[{'version': 'v1', 'created': 'Mon, 15 Nov 2021 16:52:02 GMT'}, {'version': 'v2', 'created': 'Sun, 13 Mar 2022 14:20:17 GMT'}, {'version': 'v3', 'created': 'Tue, 4 Jun 2024 18:14:13 GMT'}]",2024-06-06,"[['Tugnait', 'Jitendra K.', '']]",3,3_manifold_models_gaussian_dimensional,"manifold, models, gaussian, dimensional, sparse, data, modeling, multivariate, learning, estimation","manifold (0.39), models (0.36), gaussian (0.35), dimensional (0.32), sparse (0.32), data (0.31), modeling (0.31), multivariate (0.31), learning (0.30), estimation (0.30)","  This paper is concerned with the statistical analysis of matrix-valued time
series. These are data collected over a network of sensors (typically a set of
spatial locations) along time, where a vector of features is observed per time
instant per sensor. Thus each sensor is characterized by a vectorial time
series. We would like to identify the dependency structure among these sensors
and represent it by a graph. When there is only one feature per sensor, the
vector auto-regressive models have been widely adapted to infer the structure
of Granger causality. The resulting graph is referred to as causal graph. Our
first contribution is then extending VAR models to matrix-variate models to
serve the purpose of graph learning. Secondly, we propose two online procedures
respectively in low and high dimensions, which can update quickly the estimates
of coefficients when new samples arrive. In particular in high dimensional
regime, a novel Lasso-type is introduced and we develop its homotopy algorithms
for the online learning. We also provide an adaptive tuning procedure for the
regularization parameter. Lastly, we consider that, the application of AR
models onto data usually requires detrending the raw data, however, this step
is forbidden in online context. Therefore, we augment the proposed AR models by
incorporating trend as extra parameter, and then adapt the online algorithms to
the augmented data models, which allow us to simultaneously learn the graph and
trend from streaming samples. In this work, we consider primarily the periodic
trend. Numerical experiments using both synthetic and real data are performed,
whose results support the effectiveness of the proposed methods.
,   In analyzing complex datasets, it is often of interest to infer lower
dimensional structure underlying the higher dimensional observations. As a
flexible class of nonlinear structures, it is common to focus on Riemannian
manifolds. Most existing manifold learning algorithms replace the original data
with lower dimensional coordinates without providing an estimate of the
manifold in the observation space or using the manifold to denoise the original
data. This article proposes a new methodology for addressing these problems,
allowing interpolation of the estimated manifold between fitted data points.
The proposed approach is motivated by novel theoretical properties of local
covariance matrices constructed from noisy samples on a manifold. Our results
enable us to turn a global manifold reconstruction problem into a local
regression problem, allowing application of Gaussian processes for
probabilistic manifold reconstruction. In addition to theory justifying the
algorithm, we provide simulated and real data examples to illustrate the
performance.
,   We present a novel approach for data-driven modeling of the time-domain
induced polarization (IP) phenomenon using variational autoencoders (VAE). VAEs
are Bayesian neural networks that aim to learn a latent statistical
distribution to encode extensive data sets as lower dimension representations.
We collected 1 600 319 IP decay curves in various regions of Canada, the United
States and Kazakhstan, and compiled them to train a deep VAE. The proposed deep
learning approach is strictly unsupervised and data-driven: it does not require
manual processing or ground truth labeling of IP data. Moreover, our VAE
approach avoids the pitfalls of IP parametrization with the empirical Cole-Cole
and Debye decomposition models, simple power-law models, or other sophisticated
mechanistic models. We demonstrate four applications of VAEs to model and
process IP data: (1) representative synthetic data generation, (2) unsupervised
Bayesian denoising and data uncertainty estimation, (3) quantitative evaluation
of the signal-to-noise ratio, and (4) automated outlier detection. We also
interpret the IP compilation's latent representation and reveal a strong
correlation between its first dimension and the average chargeability of IP
decays. Finally, we experiment with varying VAE latent space dimensions and
demonstrate that a single real-valued scalar parameter contains sufficient
information to encode our extensive IP data compilation. This new finding
suggests that modeling time-domain IP data using mathematical models governed
by more than one free parameter is ambiguous, whereas modeling only the average
chargeability is justified. A pre-trained implementation of our model --
readily applicable to new IP data from any geolocation -- is available as
open-source Python code for the applied geophysics community.
"
290,2111.08014,Kirill Pavlenko,Anatoly Dymarsky and Kirill Pavlenko,Tensor network to learn the wavefunction of data,,,10.1103/PhysRevResearch.4.043111,,quant-ph cs.LG,http://creativecommons.org/licenses/by/4.0/,"  How many different ways are there to handwrite digit 3? To quantify this
question imagine extending a dataset of handwritten digits MNIST by sampling
additional images until they start repeating. We call the collection of all
resulting images of digit 3 the ""full set."" To study the properties of the full
set we introduce a tensor network architecture which simultaneously
accomplishes both classification (discrimination) and sampling tasks.
Qualitatively, our trained network represents the indicator function of the
full set. It therefore can be used to characterize the data itself. We
illustrate that by studying the full sets associated with the digits of MNIST.
  Using quantum mechanical interpretation of our network we characterize the
full set by calculating its entanglement entropy. We also study its geometric
properties such as mean Hamming distance, effective dimension, and size. The
latter answers the question above -- the total number of black and white threes
written MNIST style is $2^{72}$.
","[{'version': 'v1', 'created': 'Mon, 15 Nov 2021 19:00:01 GMT'}]",2024-02-02,"[['Dymarsky', 'Anatoly', ''], ['Pavlenko', 'Kirill', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
291,2111.08108,Minh Phuong Nguyen,Chandrajit Bajaj and Minh Nguyen,"Physics-informed neural networks via stochastic Hamiltonian dynamics
  learning","To be published in Springer series ""Lecture Notes in Networks and
  Systems""","Lecture Notes in Networks and Systems, vol 1066. Springer, Cham,
  2024",10.1007/978-3-031-66428-1_11,,math.OC cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we propose novel learning frameworks to tackle optimal control
problems by applying the Pontryagin maximum principle and then solving for a
Hamiltonian dynamical system. Applying the Pontryagin maximum principle to the
original optimal control problem shifts the learning focus to reduced
Hamiltonian dynamics and corresponding adjoint variables. Then, the reduced
Hamiltonian networks can be learned by going backwards in time and then
minimizing loss function deduced from the Pontryagin maximum principle's
conditions. The learning process is further improved by progressively learning
a posterior distribution of the reduced Hamiltonians. This is achieved through
utilizing a variational autoencoder which leads to more effective path
exploration process. We apply our learning frameworks called NeuralPMP to
various control tasks and obtain competitive results.
","[{'version': 'v1', 'created': 'Mon, 15 Nov 2021 22:13:43 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Feb 2024 18:58:53 GMT'}, {'version': 'v3', 'created': 'Fri, 26 Apr 2024 05:10:50 GMT'}]",2024-08-13,"[['Bajaj', 'Chandrajit', ''], ['Nguyen', 'Minh', '']]",5,5_neural_dynamics_networks_dynamical,"neural, dynamics, networks, dynamical, neuronal, trajectories, robots, robotic, models, learning","neural (0.46), dynamics (0.45), networks (0.40), dynamical (0.40), neuronal (0.39), trajectories (0.36), robots (0.35), robotic (0.35), models (0.34), learning (0.33)","  There is much to learn through synthesis of Developmental Biology, Cognitive
Science and Computational Modeling. Our path forward involves a design for
developmentally-inspired learning agents based on Braitenberg Vehicles.
Continual developmental neurosimulation allows us to consider the role of
developmental trajectories in bridging the related phenomena of nervous system
morphogenesis, developmental learning, and plasticity. Being closely tied to
continual learning, our approach is tightly integrated with developmental
embodiment, and can be implemented using a type of agent called developmental
Braitenberg Vehicles (dBVs). dBVs begin their lives as a set of undefined
structures that transform into agent-based systems including a body, sensors,
effectors, and nervous system. This phenotype is characterized in terms of
developmental timing: with distinct morphogenetic, critical, and acquisition
(developmental learning) periods. We further propose that network morphogenesis
can be accomplished using a genetic algorithmic approach, while developmental
learning can be implemented using a number of computational methodologies. This
approach provides a framework for adaptive agent behavior that might result
from a developmental approach: namely by exploiting critical periods or growth
and acquisition, an explicitly embodied network architecture, and a distinction
between the assembly of neuronal networks and active learning on these
networks. In conclusion, we will consider agent learning and development at
different timescales, from very short (<100ms) intervals to long-term
evolution. The development, evolution, and learning in an embodied agent-based
approach is key to an integrative view of biologically-inspired intelligence.
,   Modeling of non-rigid object launching and manipulation is complex
considering the wide range of dynamics affecting trajectory, many of which may
be unknown. Using physics models can be inaccurate because they cannot account
for unknown factors and the effects of the deformation of the object as it is
launched; moreover, deriving force coefficients for these models is not
possible without extensive experimental testing. Recently, advancements in
data-powered artificial intelligence methods have allowed learnable models and
systems to emerge. It is desirable to train a model for launch prediction on a
robot, as deep neural networks can account for immeasurable dynamics. However,
the inability to collect large amounts of experimental data decreases
performance of deep neural networks. Through estimating force coefficients, the
accepted physics models can be leveraged to produce adequate supplemental data
to artificially increase the size of the training set, yielding improved neural
networks. In this paper, we introduce a new framework for algorithmic
estimation of force coefficients for non-rigid object launching, which can be
generalized to other domains, in order to generate large datasets. We implement
a novel training algorithm and objective for our deep neural network to
accurately model launch trajectory of non-rigid objects and predict whether
they will hit a series of targets. Our experimental results demonstrate the
effectiveness of using simulated data from force coefficient estimation and
shows the importance of simulated data for training an effective neural
network.
,   It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn't necessarily hold. We
then systematically tested which features of the environment and network do
lead to the emergence of specialization. We used a simple toy environment, task
and network, allowing us precise control, and show that in this setup, several
distinct measures of specialization give qualitatively similar results. We
further find that in this setup (1) specialization can only emerge in
environments where features of that environment are meaningfully separable, (2)
specialization preferentially emerges when the network is strongly
resource-constrained, and (3) these findings are qualitatively similar across
the different variations of network architectures that we tested, but that the
quantitative relationships depend on the precise architecture. Finally, we show
that functional specialization varies dynamically across time, and demonstrate
that these dynamics depend on both the timing and bandwidth of information flow
in the network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligence in situations of real-world complexity, from biology to
brain-inspired neuromorphic systems. We propose that thoroughly stress testing
candidate definitions of functional modularity in simplified scenarios before
extending to more complex data, network models and electrophysiological
recordings is likely to be a fruitful approach.
"
