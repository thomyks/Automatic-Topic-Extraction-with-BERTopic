Topic,Count,Name,Representation,Aspect1,Representative_Docs,Human_Readable_Topic,Higher_Topic_Label,Highest_Topic_Label,Final_Label
-1,20553,-1_features_classification_learning_datasets,"['features', 'classification', 'learning', 'datasets', 'models', 'ai', 'networks', 'neural', 'trained', 'dataset']","['data', 'learning', 'models', 'model', 'training', 'performance', 'tasks', 'approach', 'large', 'image']","['  With the significant advancements of Large Language Models (LLMs) in the\nfield of Natural Language Processing (NLP), the development of image-text\nmultimodal models has garnered widespread attention. Current surveys on\nimage-text multimodal models mainly focus on representative models or\napplication domains, but lack a review on how general technical models\ninfluence the development of domain-specific models, which is crucial for\ndomain researchers. Based on this, this paper first reviews the technological\nevolution of image-text multimodal models, from early explorations of feature\nspace to visual language encoding structures, and then to the latest large\nmodel architectures. Next, from the perspective of technological evolution, we\nexplain how the development of general image-text multimodal technologies\npromotes the progress of multimodal technologies in the biomedical field, as\nwell as the importance and complexity of specific datasets in the biomedical\ndomain. Then, centered on the tasks of image-text multimodal models, we analyze\ntheir common components and challenges. After that, we summarize the\narchitecture, components, and data of general image-text multimodal models, and\nintroduce the applications and improvements of image-text multimodal models in\nthe biomedical field. Finally, we categorize the challenges faced in the\ndevelopment and application of general models into external factors and\nintrinsic factors, further refining them into 2 external factors and 5\nintrinsic factors, and propose targeted solutions, providing guidance for\nfuture research directions. For more details and data, please visit our GitHub\npage: \\url{https://github.com/i2vec/A-survey-on-image-text-multimodal-models}.\n', '  Recently, large language models (LLMs) have notably positioned them as\ncapable tools for addressing complex optimization challenges. Despite this\nrecognition, a predominant limitation of existing LLM-based optimization\nmethods is their struggle to capture the relationships among decision variables\nwhen relying exclusively on numerical text prompts, especially in\nhigh-dimensional problems. Keeping this in mind, we first propose to enhance\nthe optimization performance using multimodal LLM capable of processing both\ntextual and visual prompts for deeper insights of the processed optimization\nproblem. This integration allows for a more comprehensive understanding of\noptimization problems, akin to human cognitive processes. We have developed a\nmultimodal LLM-based optimization framework that simulates human\nproblem-solving workflows, thereby offering a more nuanced and effective\nanalysis. The efficacy of this method is evaluated through extensive empirical\nstudies focused on a well-known combinatorial optimization problem, i.e.,\ncapacitated vehicle routing problem. The results are compared against those\nobtained from the LLM-based optimization algorithms that rely solely on textual\nprompts, demonstrating the significant advantages of our multimodal approach.\n', '  Achieving a universally high accuracy in object detection is quite\nchallenging, and the mainstream focus in the industry currently lies on\ndetecting specific classes of objects. However, deploying one or multiple\nobject detection networks requires a certain amount of GPU memory for training\nand storage capacity for inference. This presents challenges in terms of how to\neffectively coordinate multiple object detection tasks under\nresource-constrained conditions. This paper introduces a lightweight\nfine-tuning strategy called Calibration side tuning, which integrates aspects\nof adapter tuning and side tuning to adapt the successful techniques employed\nin transformers for use with ResNet. The Calibration side tuning architecture\nthat incorporates maximal transition calibration, utilizing a small number of\nadditional parameters to enhance network performance while maintaining a smooth\ntraining process. Furthermore, this paper has conducted an analysis on multiple\nfine-tuning strategies and have implemented their application within ResNet,\nthereby expanding the research on fine-tuning strategies for object detection\nnetworks. Besides, this paper carried out extensive experiments using five\nbenchmark datasets. The experimental results demonstrated that this method\noutperforms other compared state-of-the-art techniques, and a better balance\nbetween the complexity and performance of the finetune schemes is achieved.\n']",Multimodal Models for Image-Text Analysis,,,
0,602,0_medical_clinical_patients_medicine,"['medical', 'clinical', 'patients', 'medicine', 'med', 'clinician', 'physician', 'doctor', 'annotated', 'physicians']","['medical', 'clinical', 'biomedical', 'patient', 'healthcare', 'notes', 'health', 'patients', 'records', 'extraction']","[""  The integration of Artificial Intelligence (AI), especially Large Language\nModels (LLMs), into the clinical diagnosis process offers significant potential\nto improve the efficiency and accessibility of medical care. While LLMs have\nshown some promise in the medical domain, their application in clinical\ndiagnosis remains underexplored, especially in real-world clinical practice,\nwhere highly sophisticated, patient-specific decisions need to be made. Current\nevaluations of LLMs in this field are often narrow in scope, focusing on\nspecific diseases or specialties and employing simplified diagnostic tasks. To\nbridge this gap, we introduce CliBench, a novel benchmark developed from the\nMIMIC IV dataset, offering a comprehensive and realistic assessment of LLMs'\ncapabilities in clinical diagnosis. This benchmark not only covers diagnoses\nfrom a diverse range of medical cases across various specialties but also\nincorporates tasks of clinical significance: treatment procedure\nidentification, lab test ordering and medication prescriptions. Supported by\nstructured output ontologies, CliBench enables a precise and multi-granular\nevaluation, offering an in-depth understanding of LLM's capability on diverse\nclinical tasks of desired granularity. We conduct a zero-shot evaluation of\nleading LLMs to assess their proficiency in clinical decision-making. Our\npreliminary results shed light on the potential and limitations of current LLMs\nin clinical settings, providing valuable insights for future advancements in\nLLM-powered healthcare.\n"", '  Large Language Models (LLMs) have demonstrated surprising performance across\nvarious natural language processing tasks. Recently, medical LLMs enhanced with\ndomain-specific knowledge have exhibited excellent capabilities in medical\nconsultation and diagnosis. These models can smoothly simulate doctor-patient\ndialogues and provide professional medical advice. Most medical LLMs are\ndeveloped through continued training of open-source general LLMs, which require\nsignificantly fewer computational resources than training LLMs from scratch.\nAdditionally, this approach offers better protection of patient privacy\ncompared to API-based solutions. This survey systematically explores how to\ntrain medical LLMs based on general LLMs. It covers: (a) how to acquire\ntraining corpus and construct customized medical training sets, (b) how to\nchoose a appropriate training paradigm, (c) how to choose a suitable evaluation\nbenchmark, and (d) existing challenges and promising future research directions\nare discussed. This survey can provide guidance for the development of LLMs\nfocused on various medical applications, such as medical education, diagnostic\nplanning, and clinical assistants.\n', '  Large language models (LLMs) have emerged as powerful tools with\ntransformative potential across numerous domains, including healthcare and\nmedicine. In the medical domain, LLMs hold promise for tasks ranging from\nclinical decision support to patient education. However, evaluating the\nperformance of LLMs in medical contexts presents unique challenges due to the\ncomplex and critical nature of medical information. This paper provides a\ncomprehensive overview of the landscape of medical LLM evaluation, synthesizing\ninsights from existing studies and highlighting evaluation data sources, task\nscenarios, and evaluation methods. Additionally, it identifies key challenges\nand opportunities in medical LLM evaluation, emphasizing the need for continued\nresearch and innovation to ensure the responsible integration of LLMs into\nclinical practice.\n']",Medical Language Models for Clinical Diagnosis,Applications of Large Language Models in Healthcare,Large Language Models,Large Language Models
0,602,0_medical_clinical_patients_medicine,"['medical', 'clinical', 'patients', 'medicine', 'med', 'clinician', 'physician', 'doctor', 'annotated', 'physicians']","['medical', 'clinical', 'biomedical', 'patient', 'healthcare', 'notes', 'health', 'patients', 'records', 'extraction']","[""  The integration of Artificial Intelligence (AI), especially Large Language\nModels (LLMs), into the clinical diagnosis process offers significant potential\nto improve the efficiency and accessibility of medical care. While LLMs have\nshown some promise in the medical domain, their application in clinical\ndiagnosis remains underexplored, especially in real-world clinical practice,\nwhere highly sophisticated, patient-specific decisions need to be made. Current\nevaluations of LLMs in this field are often narrow in scope, focusing on\nspecific diseases or specialties and employing simplified diagnostic tasks. To\nbridge this gap, we introduce CliBench, a novel benchmark developed from the\nMIMIC IV dataset, offering a comprehensive and realistic assessment of LLMs'\ncapabilities in clinical diagnosis. This benchmark not only covers diagnoses\nfrom a diverse range of medical cases across various specialties but also\nincorporates tasks of clinical significance: treatment procedure\nidentification, lab test ordering and medication prescriptions. Supported by\nstructured output ontologies, CliBench enables a precise and multi-granular\nevaluation, offering an in-depth understanding of LLM's capability on diverse\nclinical tasks of desired granularity. We conduct a zero-shot evaluation of\nleading LLMs to assess their proficiency in clinical decision-making. Our\npreliminary results shed light on the potential and limitations of current LLMs\nin clinical settings, providing valuable insights for future advancements in\nLLM-powered healthcare.\n"", '  Large Language Models (LLMs) have demonstrated surprising performance across\nvarious natural language processing tasks. Recently, medical LLMs enhanced with\ndomain-specific knowledge have exhibited excellent capabilities in medical\nconsultation and diagnosis. These models can smoothly simulate doctor-patient\ndialogues and provide professional medical advice. Most medical LLMs are\ndeveloped through continued training of open-source general LLMs, which require\nsignificantly fewer computational resources than training LLMs from scratch.\nAdditionally, this approach offers better protection of patient privacy\ncompared to API-based solutions. This survey systematically explores how to\ntrain medical LLMs based on general LLMs. It covers: (a) how to acquire\ntraining corpus and construct customized medical training sets, (b) how to\nchoose a appropriate training paradigm, (c) how to choose a suitable evaluation\nbenchmark, and (d) existing challenges and promising future research directions\nare discussed. This survey can provide guidance for the development of LLMs\nfocused on various medical applications, such as medical education, diagnostic\nplanning, and clinical assistants.\n', '  Large language models (LLMs) have emerged as powerful tools with\ntransformative potential across numerous domains, including healthcare and\nmedicine. In the medical domain, LLMs hold promise for tasks ranging from\nclinical decision support to patient education. However, evaluating the\nperformance of LLMs in medical contexts presents unique challenges due to the\ncomplex and critical nature of medical information. This paper provides a\ncomprehensive overview of the landscape of medical LLM evaluation, synthesizing\ninsights from existing studies and highlighting evaluation data sources, task\nscenarios, and evaluation methods. Additionally, it identifies key challenges\nand opportunities in medical LLM evaluation, emphasizing the need for continued\nresearch and innovation to ensure the responsible integration of LLMs into\nclinical practice.\n']",Medical Language Models for Clinical Diagnosis,Applications of Large Language Models in Healthcare,Large Language Models,Large Language Models
1,597,1_recommender_recommenders_personalized_conversational,"['recommender', 'recommenders', 'personalized', 'conversational', 'recommendation', 'recommendations', 'factorization', 'embeddings', 'attention', 'ranking']","['recommendation', 'item', 'recommender', 'user', 'items', 'recommendations', 'users', 'preferences', 'collaborative', 'sequential']","[""  Recently, large language models (LLMs) have shown great potential in\nrecommender systems, either improving existing recommendation models or serving\nas the backbone. However, there exists a large semantic gap between LLMs and\nrecommender systems, since items to be recommended are often indexed by\ndiscrete identifiers (item ID) out of the LLM's vocabulary. In essence, LLMs\ncapture language semantics while recommender systems imply collaborative\nsemantics, making it difficult to sufficiently leverage the model capacity of\nLLMs for recommendation. To address this challenge, in this paper, we propose a\nnew LLM-based recommendation model called LC-Rec, which can better integrate\nlanguage and collaborative semantics for recommender systems. Our approach can\ndirectly generate items from the entire item set for recommendation, without\nrelying on candidate items. Specifically, we make two major contributions in\nour approach. For item indexing, we design a learning-based vector quantization\nmethod with uniform semantic mapping, which can assign meaningful and\nnon-conflicting IDs (called item indices) for items. For alignment tuning, we\npropose a series of specially designed tuning tasks to enhance the integration\nof collaborative semantics in LLMs. Our fine-tuning tasks enforce LLMs to\ndeeply integrate language and collaborative semantics (characterized by the\nlearned item indices), so as to achieve an effective adaptation to recommender\nsystems. Extensive experiments demonstrate the effectiveness of our method,\nshowing that our approach can outperform a number of competitive baselines\nincluding traditional recommenders and existing LLM-based recommenders. Our\ncode is available at https://github.com/RUCAIBox/LC-Rec/.\n"", '  Conversational recommender system (CRS), which combines the techniques of\ndialogue system and recommender system, has obtained increasing interest\nrecently. In contrast to traditional recommender system, it learns the user\npreference better through interactions (i.e. conversations), and then further\nboosts the recommendation performance. However, existing studies on CRS ignore\nto address the relationship among attributes, users, and items effectively,\nwhich might lead to inappropriate questions and inaccurate recommendations. In\nthis view, we propose a knowledge graph based conversational recommender system\n(referred as KG-CRS). Specifically, we first integrate the user-item graph and\nitem-attribute graph into a dynamic graph, i.e., dynamically changing during\nthe dialogue process by removing negative items or attributes. We then learn\ninformative embedding of users, items, and attributes by also considering\npropagation through neighbors on the graph. Extensive experiments on three real\ndatasets validate the superiority of our method over the state-of-the-art\napproaches in terms of both the recommendation and conversation tasks.\n', '  Addressing the challenges related to data sparsity, cold-start problems, and\ndiversity in recommendation systems is both crucial and demanding. Many current\nsolutions leverage knowledge graphs to tackle these issues by combining both\nitem-based and user-item collaborative signals. A common trend in these\napproaches focuses on improving ranking performance at the cost of escalating\nmodel complexity, reducing diversity, and complicating the task. It is\nessential to provide recommendations that are both personalized and diverse,\nrather than solely relying on achieving high rank-based performance, such as\nClick-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task\nlearning approach, training on user-item and item-item interactions. We apply\nitem-based contrastive learning on descriptive text, sampling positive and\nnegative pairs based on item metadata. Our approach allows the model to better\nunderstand the relationships between entities within the knowledge graph by\nutilizing semantic information from text. It leads to more accurate, relevant,\nand diverse user recommendations and a benefit that extends even to cold-start\nusers who have few interactions with items. We perform extensive experiments on\ntwo widely used datasets to validate the effectiveness of our approach. Our\nfindings demonstrate that jointly training user-item interactions and\nitem-based signals using synopsis text is highly effective. Furthermore, our\nresults provide evidence that item-based contrastive learning enhances the\nquality of entity embeddings, as indicated by metrics such as uniformity and\nalignment.\n']",Recommender Systems with Advanced Techniques,Advances in Recommender Systems,Recommender Systems and Personalization,Recommender Systems and Personalization
2,573,2_translations_translators_multilingual_lingual,"['translations', 'translators', 'multilingual', 'lingual', 'translating', 'bilingual', 'translation', 'monolingual', 'translated', 'languages']","['translation', 'languages', 'multilingual', 'lingual', 'resource', 'translations', 'cross', 'language', 'monolingual', 'parallel']","['  The field of cross-lingual sentence embeddings has recently experienced\nsignificant advancements, but research concerning low-resource languages has\nlagged due to the scarcity of parallel corpora. This paper shows that\ncross-lingual word representation in low-resource languages is notably\nunder-aligned with that in high-resource languages in current models. To\naddress this, we introduce a novel framework that explicitly aligns words\nbetween English and eight low-resource languages, utilizing off-the-shelf word\nalignment models. This framework incorporates three primary training\nobjectives: aligned word prediction and word translation ranking, along with\nthe widely used translation ranking. We evaluate our approach through\nexperiments on the bitext retrieval task, which demonstrate substantial\nimprovements on sentence embeddings in low-resource languages. In addition, the\ncompetitive performance of the proposed model across a broader range of tasks\nin high-resource languages underscores its practicality.\n', '  Multilingual generative models obtain remarkable cross-lingual in-context\nlearning capabilities through pre-training on large-scale corpora. However,\nthey still exhibit a performance bias toward high-resource languages and learn\nisolated distributions of multilingual sentence representations, which may\nhinder knowledge transfer across languages. To bridge this gap, we propose a\nsimple yet effective cross-lingual alignment framework exploiting pairs of\ntranslation sentences. It aligns the internal sentence representations across\ndifferent languages via multilingual contrastive learning and aligns outputs by\nfollowing cross-lingual instructions in the target language. Experimental\nresults show that even with less than 0.1 {\\textperthousand} of pre-training\ntokens, our alignment framework significantly boosts the cross-lingual\nabilities of generative language models and mitigates the performance gap.\nFurther analyses reveal that it results in a better internal multilingual\nrepresentation distribution of multilingual models.\n', ""  Large Language Models (LLMs) demonstrate strong machine translation\ncapabilities on languages they are trained on. However, the impact of factors\nbeyond training data size on translation performance remains a topic of debate,\nespecially concerning languages not directly encountered during training. Our\nstudy delves into Llama2's translation capabilities. By modeling a linear\nrelationship between linguistic feature distances and machine translation\nscores, we ask ourselves if there are potentially better central languages for\nLLMs other than English. Our experiments show that the 7B Llama2 model yields\nabove 10 BLEU when translating into all languages it has seen, which rarely\nhappens for languages it has not seen. Most translation improvements into\nunseen languages come from scaling up the model size rather than instruction\ntuning or increasing shot count. Furthermore, our correlation analysis reveals\nthat syntactic similarity is not the only linguistic factor that strongly\ncorrelates with machine translation scores. Interestingly, we discovered that\nunder specific circumstances, some languages (e.g. Swedish, Catalan), despite\nhaving significantly less training data, exhibit comparable correlation levels\nto English. These insights challenge the prevailing landscape of LLMs,\nsuggesting that models centered around languages other than English could\nprovide a more efficient foundation for multilingual applications.\n""]",Cross-Lingual Language Models and Translation,Multilingual Natural Language Processing,Natural Language Processing,Natural Language Processing
3,571,3_robotic_robot_robotics_robots,"['robotic', 'robot', 'robotics', 'robots', 'grasping', 'manipulator', 'teleoperation', 'embodied', 'grasp', 'humanoid']","['robot', 'manipulation', 'robotic', 'robots', 'objects', 'object', 'navigation', 'grasping', 'tactile', 'instructions']","[""  Large Language Models (LLMs) have been recently used in robot applications\nfor grounding LLM common-sense reasoning with the robot's perception and\nphysical abilities. In humanoid robots, memory also plays a critical role in\nfostering real-world embodiment and facilitating long-term interactive\ncapabilities, especially in multi-task setups where the robot must remember\nprevious task states, environment states, and executed actions. In this paper,\nwe address incorporating memory processes with LLMs for generating cross-task\nrobot actions, while the robot effectively switches between tasks. Our proposed\ndual-layered architecture features two LLMs, utilizing their complementary\nskills of reasoning and following instructions, combined with a memory model\ninspired by human cognition. Our results show a significant improvement in\nperformance over a baseline of five robotic tasks, demonstrating the potential\nof integrating memory with LLMs for combining the robot's action and perception\nfor adaptive task execution.\n"", '  Considering how to make the model accurately understand and follow natural\nlanguage instructions and perform actions consistent with world knowledge is a\nkey challenge in robot manipulation. This mainly includes human fuzzy\ninstruction reasoning and the following of physical knowledge. Therefore, the\nembodied intelligence agent must have the ability to model world knowledge from\ntraining data. However, most existing vision and language robot manipulation\nmethods mainly operate in less realistic simulator and language settings and\nlack explicit modeling of world knowledge. To bridge this gap, we introduce a\nnovel and simple robot manipulation framework, called Surfer. It is based on\nthe world model, treats robot manipulation as a state transfer of the visual\nscene, and decouples it into two parts: action and scene. Then, the\ngeneralization ability of the model on new instructions and new scenes is\nenhanced by explicit modeling of the action and scene prediction in multi-modal\ninformation. In addition to the framework, we also built a robot manipulation\nsimulator that supports full physics execution based on the MuJoCo physics\nengine. It can automatically generate demonstration training data and test\ndata, effectively reducing labor costs. To conduct a comprehensive and\nsystematic evaluation of the robot manipulation model in terms of language\nunderstanding and physical execution, we also created a robotic manipulation\nbenchmark with progressive reasoning tasks, called SeaWave. It contains 4\nlevels of progressive reasoning tasks and can provide a standardized testing\nplatform for embedded AI agents in multi-modal environments. On average, Surfer\nachieved a success rate of 54.74% on the defined four levels of manipulation\ntasks, exceeding the best baseline performance of 47.64%.\n', '  To substantially enhance robot intelligence, there is a pressing need to\ndevelop a large model that enables general-purpose robots to proficiently\nundertake a broad spectrum of manipulation tasks, akin to the versatile\ntask-planning ability exhibited by LLMs. The vast diversity in objects, robots,\nand manipulation tasks presents huge challenges. Our work introduces a\ncomprehensive framework to develop a foundation model for general robotic\nmanipulation that formalizes a manipulation task as contact synthesis.\nSpecifically, our model takes as input object and robot manipulator point\nclouds, object physical attributes, target motions, and manipulation region\nmasks. It outputs contact points on the object and associated contact forces or\npost-contact motions for robots to achieve the desired manipulation task. We\nperform extensive experiments both in the simulation and real-world settings,\nmanipulating articulated rigid objects, rigid objects, and deformable objects\nthat vary in dimensionality, ranging from one-dimensional objects like ropes to\ntwo-dimensional objects like cloth and extending to three-dimensional objects\nsuch as plasticine. Our model achieves average success rates of around 90\\%.\nSupplementary materials and videos are available on our project website at\nhttps://manifoundationmodel.github.io/.\n']",Robot Manipulation and Embodied Intelligence,Robotics and Artificial Intelligence,Robotics and Artificial Intelligence,Robotics and Artificial Intelligence
4,569,4_cnn_segmentation_cnns_segmentations,"['cnn', 'segmentation', 'cnns', 'segmentations', 'segmenting', 'supervised', 'deep', 'convolutional', 'attention', 'ultrasound']","['segmentation', 'cancer', 'tumor', 'medical', 'images', 'breast', 'diagnosis', 'pathology', 'tumors', 'lesion']","[""  In this study, the main objective is to develop an algorithm capable of\nidentifying and delineating tumor regions in breast ultrasound (BUS) and\nmammographic images. The technique employs two advanced deep learning\narchitectures, namely U-Net and pretrained SAM, for tumor segmentation. The\nU-Net model is specifically designed for medical image segmentation and\nleverages its deep convolutional neural network framework to extract meaningful\nfeatures from input images. On the other hand, the pretrained SAM architecture\nincorporates a mechanism to capture spatial dependencies and generate\nsegmentation results. Evaluation is conducted on a diverse dataset containing\nannotated tumor regions in BUS and mammographic images, covering both benign\nand malignant tumors. This dataset enables a comprehensive assessment of the\nalgorithm's performance across different tumor types. Results demonstrate that\nthe U-Net model outperforms the pretrained SAM architecture in accurately\nidentifying and segmenting tumor regions in both BUS and mammographic images.\nThe U-Net exhibits superior performance in challenging cases involving\nirregular shapes, indistinct boundaries, and high tumor heterogeneity. In\ncontrast, the pretrained SAM architecture exhibits limitations in accurately\nidentifying tumor areas, particularly for malignant tumors and objects with\nweak boundaries or complex shapes. These findings highlight the importance of\nselecting appropriate deep learning architectures tailored for medical image\nsegmentation. The U-Net model showcases its potential as a robust and accurate\ntool for tumor detection, while the pretrained SAM architecture suggests the\nneed for further improvements to enhance segmentation performance.\n"", '  Tumor segmentation from multi-modal brain MRI images is a challenging task\ndue to the limited samples, high variance in shapes and uneven distribution of\ntumor morphology. The performance of automated medical image segmentation has\nbeen significant improvement by the recent advances in deep learning. However,\nthe model predictions have not yet reached the desired level for clinical use\nin terms of accuracy and generalizability. In order to address the distinct\nproblems presented in Challenges 1, 2, and 3 of BraTS 2023, we have constructed\nan optimization framework based on a 3D U-Net model for brain tumor\nsegmentation. This framework incorporates a range of techniques, including\nvarious pre-processing and post-processing techniques, and transfer learning.\nOn the validation datasets, this multi-modality brain tumor segmentation\nframework achieves an average lesion-wise Dice score of 0.79, 0.72, 0.74 on\nChallenges 1, 2, 3 respectively.\n', '  Ultrasound imaging plays a critical role in the early detection of breast\ncancer. Accurate identification and segmentation of lesions are essential steps\nin clinical practice, requiring methods to assist physicians in lesion\nsegmentation. However, ultrasound lesion segmentation models based on\nsupervised learning require extensive manual labeling, which is both\ntime-consuming and labor-intensive. In this study, we present a novel framework\nfor weakly supervised lesion segmentation in early breast ultrasound images.\nOur method uses morphological enhancement and class activation map (CAM)-guided\nlocalization. Finally, we employ the Segment Anything Model (SAM), a computer\nvision foundation model, for detailed segmentation. This approach does not\nrequire pixel-level annotation, thereby reducing the cost of data annotation.\nThe performance of our method is comparable to supervised learning methods that\nrequire manual annotations, achieving a Dice score of 74.39% and outperforming\ncomparative supervised models in terms of Hausdorff distance in the BUSI\ndataset. These results demonstrate that our framework effectively integrates\nweakly supervised learning with SAM, providing a promising solution for breast\ncancer image analysis. The code for this study is available at:\nhttps://github.com/YueXin18/MorSeg-CAM-SAM.\n']",Deep Learning for Medical Image Segmentation,Deep Learning for Medical Imaging and Analysis,Deep Learning for Medical Imaging and Analysis,Deep Learning for Medical Imaging and Analysis
5,560,5_quantum_qubits_qubit_qcnns,"['quantum', 'qubits', 'qubit', 'qcnns', 'qcnn', 'grover', 'entanglement', 'qnns', 'qiskit', 'qnn']","['quantum', 'classical', 'circuit', 'qubit', 'qubits', 'computers', 'computing', 'states', 'gates', 'entanglement']","['  This paper provides an introduction to quantum machine learning, exploring\nthe potential benefits of using quantum computing principles and algorithms\nthat may improve upon classical machine learning approaches. Quantum computing\nutilizes particles governed by quantum mechanics for computational purposes,\nleveraging properties like superposition and entanglement for information\nrepresentation and manipulation. Quantum machine learning applies these\nprinciples to enhance classical machine learning models, potentially reducing\nnetwork size and training time on quantum hardware. The paper covers basic\nquantum mechanics principles, including superposition, phase space, and\nentanglement, and introduces the concept of quantum gates that exploit these\nproperties. It also reviews classical deep learning concepts, such as\nartificial neural networks, gradient descent, and backpropagation, before\ndelving into trainable quantum circuits as neural networks. An example problem\ndemonstrates the potential advantages of quantum neural networks, and the\nappendices provide detailed derivations. The paper aims to help researchers new\nto quantum mechanics and machine learning develop their expertise more\nefficiently.\n', '  Graph states are used to represent mathematical graphs as quantum states on\nquantum computers. They can be formulated through stabilizer codes or directly\nquantum gates and quantum states. In this paper we show that a quantum graph\nneural network model can be understood and realized based on graph states. We\nshow that they can be used either as a parameterized quantum circuits to\nrepresent neural networks or as an underlying structure to construct graph\nneural networks on quantum computers.\n', '  In this work, quantum transformers are designed and analysed in detail by\nextending the state-of-the-art classical transformer neural network\narchitectures known to be very performant in natural language processing and\nimage analysis. Building upon the previous work, which uses parametrised\nquantum circuits for data loading and orthogonal neural layers, we introduce\nthree types of quantum transformers for training and inference, including a\nquantum transformer based on compound matrices, which guarantees a theoretical\nadvantage of the quantum attention mechanism compared to their classical\ncounterpart both in terms of asymptotic run time and the number of model\nparameters. These quantum architectures can be built using shallow quantum\ncircuits and produce qualitatively different classification models. The three\nproposed quantum attention layers vary on the spectrum between closely\nfollowing the classical transformers and exhibiting more quantum\ncharacteristics. As building blocks of the quantum transformer, we propose a\nnovel method for loading a matrix as quantum states as well as two new\ntrainable quantum orthogonal layers adaptable to different levels of\nconnectivity and quality of quantum computers. We performed extensive\nsimulations of the quantum transformers on standard medical image datasets that\nshowed competitively, and at times better performance compared to the classical\nbenchmarks, including the best-in-class classical vision transformers. The\nquantum transformers we trained on these small-scale datasets require fewer\nparameters compared to standard classical benchmarks. Finally, we implemented\nour quantum transformers on superconducting quantum computers and obtained\nencouraging results for up to six qubit experiments.\n']",Quantum Machine Learning and Neural Networks,Quantum and Neuromorphic Computing,Computational Models of Complex Systems and Processes,Computational Models of Complex Systems and Processes
6,559,6_adversarial_adversarially_adversary_attacks,"['adversarial', 'adversarially', 'adversary', 'attacks', 'attacker', 'adversaries', 'backdoors', 'defenses', 'classifiers', 'threats']","['adversarial', 'attacks', 'attack', 'robustness', 'backdoor', 'defense', 'perturbations', 'examples', 'robust', 'clean']","[""  Deep neural networks (DNNs) are easily fooled by adversarial perturbations\nthat are imperceptible to humans. Adversarial training, a process where\nadversarial examples are added to the training set, is the current\nstate-of-the-art defense against adversarial attacks, but it lowers the model's\naccuracy on clean inputs, is computationally expensive, and offers less\nrobustness to natural noise. In contrast, energy-based models (EBMs), which\nwere designed for efficient implementation in neuromorphic hardware and\nphysical systems, incorporate feedback connections from each layer to the\nprevious layer, yielding a recurrent, deep-attractor architecture which we\nhypothesize should make them naturally robust. Our work is the first to explore\nthe robustness of EBMs to both natural corruptions and adversarial attacks,\nwhich we do using the CIFAR-10 and CIFAR-100 datasets. We demonstrate that EBMs\nare more robust than transformers and display comparable robustness to\nadversarially-trained DNNs on gradient-based (white-box) attacks, query-based\n(black-box) attacks, and natural perturbations without sacrificing clean\naccuracy, and without the need for adversarial training or additional training\ntechniques.\n"", '  As deep learning (DL) models are increasingly being integrated into our\neveryday lives, ensuring their safety by making them robust against adversarial\nattacks has become increasingly critical. DL models have been found to be\nsusceptible to adversarial attacks which can be achieved by introducing small,\ntargeted perturbations to disrupt the input data. Adversarial training has been\npresented as a mitigation strategy which can result in more robust models. This\nadversarial robustness comes with additional computational costs required to\ndesign adversarial attacks during training. The two objectives -- adversarial\nrobustness and computational efficiency -- then appear to be in conflict of\neach other. In this work, we explore the effects of two different model\ncompression methods -- structured weight pruning and quantization -- on\nadversarial robustness. We specifically explore the effects of fine-tuning on\ncompressed models, and present the trade-off between standard fine-tuning and\nadversarial fine-tuning. Our results show that compression does not inherently\nlead to loss in model robustness and adversarial fine-tuning of a compressed\nmodel can yield large improvement to the robustness performance of models. We\npresent experiments on two benchmark datasets showing that adversarial\nfine-tuning of compressed models can achieve robustness performance comparable\nto adversarially trained models, while also improving computational efficiency.\n', '  The rapid advancement of artificial intelligence within the realm of\ncybersecurity raises significant security concerns. The vulnerability of deep\nlearning models in adversarial attacks is one of the major issues. In\nadversarial machine learning, malicious users try to fool the deep learning\nmodel by inserting adversarial perturbation inputs into the model during its\ntraining or testing phase. Subsequently, it reduces the model confidence score\nand results in incorrect classifications. The novel key contribution of the\nresearch is to empirically test the black-box adversarial transferability\nphenomena in cyber attack detection systems. It indicates that the adversarial\nperturbation input generated through the surrogate model has a similar impact\non the target model in producing the incorrect classification. To empirically\nvalidate this phenomenon, surrogate and target models are used. The adversarial\nperturbation inputs are generated based on the surrogate-model for which the\nhacker has complete information. Based on these adversarial perturbation\ninputs, both surrogate and target models are evaluated during the inference\nphase. We have done extensive experimentation over the CICDDoS-2019 dataset,\nand the results are classified in terms of various performance metrics like\naccuracy, precision, recall, and f1-score. The findings indicate that any deep\nlearning model is highly susceptible to adversarial attacks, even if the\nattacker does not have access to the internal details of the target model. The\nresults also indicate that white-box adversarial attacks have a severe impact\ncompared to black-box adversarial attacks. There is a need to investigate and\nexplore adversarial defence techniques to increase the robustness of the deep\nlearning models against adversarial attacks.\n']",Adversarial Attacks on Deep Learning Models,Adversarial Attacks and Defenses in AI and Machine Learning Models,Adversarial Attacks and Vulnerabilities in AI and Machine Learning,Adversarial Attacks and Vulnerabilities in AI and Machine Learning
7,554,7_agent_agents_planning_ai,"['agent', 'agents', 'planning', 'ai', 'planner', 'automation', 'planners', 'tasks', 'plans', 'autonomous']","['agents', 'agent', 'planning', 'game', 'games', 'actions', 'plan', 'web', 'action', 'environment']","[""  While Large Language Models (LLMs) have demonstrated impressive\naccomplishments in both reasoning and planning, their abilities in multi-agent\ncollaborations remains largely unexplored. This study evaluates LLM-based\nagents in a multi-agent cooperative text game with Theory of Mind (ToM)\ninference tasks, comparing their performance with Multi-Agent Reinforcement\nLearning (MARL) and planning-based baselines. We observed evidence of emergent\ncollaborative behaviors and high-order Theory of Mind capabilities among\nLLM-based agents. Our results reveal limitations in LLM-based agents' planning\noptimization due to systematic failures in managing long-horizon contexts and\nhallucination about the task state. We explore the use of explicit belief state\nrepresentations to mitigate these issues, finding that it enhances task\nperformance and the accuracy of ToM inferences for LLM-based agents.\n"", '  Intelligent agents stand out as a potential path toward artificial general\nintelligence (AGI). Thus, researchers have dedicated significant effort to\ndiverse implementations for them. Benefiting from recent progress in large\nlanguage models (LLMs), LLM-based agents that use universal natural language as\nan interface exhibit robust generalization capabilities across various\napplications -- from serving as autonomous general-purpose task assistants to\napplications in coding, social, and economic domains, LLM-based agents offer\nextensive exploration opportunities. This paper surveys current research to\nprovide an in-depth overview of LLM-based intelligent agents within\nsingle-agent and multi-agent systems. It covers their definitions, research\nframeworks, and foundational components such as their composition, cognitive\nand planning methods, tool utilization, and responses to environmental\nfeedback. We also delve into the mechanisms of deploying LLM-based agents in\nmulti-agent systems, including multi-role collaboration, message passing, and\nstrategies to alleviate communication issues between agents. The discussions\nalso shed light on popular datasets and application scenarios. We conclude by\nenvisioning prospects for LLM-based agents, considering the evolving landscape\nof AI and natural language processing.\n', '  Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents.\n']",Large Language Model-based Intelligent Agents for Planning and Collaboration,Applications of Large Language Models,Large Language Models,Large Language Models
7,554,7_agent_agents_planning_ai,"['agent', 'agents', 'planning', 'ai', 'planner', 'automation', 'planners', 'tasks', 'plans', 'autonomous']","['agents', 'agent', 'planning', 'game', 'games', 'actions', 'plan', 'web', 'action', 'environment']","[""  While Large Language Models (LLMs) have demonstrated impressive\naccomplishments in both reasoning and planning, their abilities in multi-agent\ncollaborations remains largely unexplored. This study evaluates LLM-based\nagents in a multi-agent cooperative text game with Theory of Mind (ToM)\ninference tasks, comparing their performance with Multi-Agent Reinforcement\nLearning (MARL) and planning-based baselines. We observed evidence of emergent\ncollaborative behaviors and high-order Theory of Mind capabilities among\nLLM-based agents. Our results reveal limitations in LLM-based agents' planning\noptimization due to systematic failures in managing long-horizon contexts and\nhallucination about the task state. We explore the use of explicit belief state\nrepresentations to mitigate these issues, finding that it enhances task\nperformance and the accuracy of ToM inferences for LLM-based agents.\n"", '  Intelligent agents stand out as a potential path toward artificial general\nintelligence (AGI). Thus, researchers have dedicated significant effort to\ndiverse implementations for them. Benefiting from recent progress in large\nlanguage models (LLMs), LLM-based agents that use universal natural language as\nan interface exhibit robust generalization capabilities across various\napplications -- from serving as autonomous general-purpose task assistants to\napplications in coding, social, and economic domains, LLM-based agents offer\nextensive exploration opportunities. This paper surveys current research to\nprovide an in-depth overview of LLM-based intelligent agents within\nsingle-agent and multi-agent systems. It covers their definitions, research\nframeworks, and foundational components such as their composition, cognitive\nand planning methods, tool utilization, and responses to environmental\nfeedback. We also delve into the mechanisms of deploying LLM-based agents in\nmulti-agent systems, including multi-role collaboration, message passing, and\nstrategies to alleviate communication issues between agents. The discussions\nalso shed light on popular datasets and application scenarios. We conclude by\nenvisioning prospects for LLM-based agents, considering the evolving landscape\nof AI and natural language processing.\n', '  Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents.\n']",Large Language Model-based Intelligent Agents for Planning and Collaboration,Applications of Large Language Models,Large Language Models,Large Language Models
8,509,8_molecular_molecule_molecules_ligands,"['molecular', 'molecule', 'molecules', 'ligands', 'proteins', 'ligand', 'protein', 'receptor', 'modeling', 'models']","['protein', 'molecular', 'molecules', 'drug', 'molecule', 'proteins', 'docking', 'chemical', 'discovery', 'ligand']","[""  Molecular representation learning is pivotal for various molecular property\nprediction tasks related to drug discovery. Robust and accurate benchmarks are\nessential for refining and validating current methods. Existing molecular\nproperty benchmarks derived from wet experiments, however, face limitations\nsuch as data volume constraints, unbalanced label distribution, and noisy\nlabels. To address these issues, we construct a large-scale and precise\nmolecular representation dataset of approximately 140,000 small molecules,\nmeticulously designed to capture an extensive array of chemical, physical, and\nbiological properties, derived through a robust computational ligand-target\nbinding analysis pipeline. We conduct extensive experiments on various deep\nlearning models, demonstrating that our dataset offers significant\nphysicochemical interpretability to guide model development and design.\nNotably, the dataset's properties are linked to binding affinity metrics,\nproviding additional insights into model performance in drug-target interaction\ntasks. We believe this dataset will serve as a more accurate and reliable\nbenchmark for molecular representation learning, thereby expediting progress in\nthe field of artificial intelligence-driven drug discovery.\n"", ""  The rise of cost involved with drug discovery and current speed of which they\nare discover, underscore the need for more efficient structure-based drug\ndesign (SBDD) methods. We employ Generative Flow Networks (GFlowNets), to\neffectively explore the vast combinatorial space of drug-like molecules, which\ntraditional virtual screening methods fail to cover. We introduce a novel\nmodification to the GFlowNet framework by incorporating trigonometrically\nconsistent embeddings, previously utilized in tasks involving protein\nconformation and protein-ligand interactions, to enhance the model's ability to\ngenerate molecules tailored to specific protein pockets. We have modified the\nexisting protein conditioning used by GFlowNets, blending geometric information\nfrom both protein and ligand embeddings to achieve more geometrically\nconsistent embeddings. Experiments conducted using CrossDocked2020 demonstrated\nan improvement in the binding affinity between generated molecules and protein\npockets for both single and multi-objective tasks, compared to previous work.\nAdditionally, we propose future work aimed at further increasing the geometric\ninformation captured in protein-ligand interactions.\n"", ""  Diffusion models have emerged as powerful tools for molecular generation,\nparticularly in the context of 3D molecular structures. Inspired by\nnon-equilibrium statistical physics, these models can generate 3D molecular\nstructures with specific properties or requirements crucial to drug discovery.\nDiffusion models were particularly successful at learning 3D molecular\ngeometries' complex probability distributions and their corresponding chemical\nand physical properties through forward and reverse diffusion processes. This\nreview focuses on the technical implementation of diffusion models tailored for\n3D molecular generation. It compares the performance, evaluation methods, and\nimplementation details of various diffusion models used for molecular\ngeneration tasks. We cover strategies for atom and bond representation,\narchitectures of reverse diffusion denoising networks, and challenges\nassociated with generating stable 3D molecular structures. This review also\nexplores the applications of diffusion models in $\\textit{de novo}$ drug design\nand related areas of computational chemistry, such as structure-based drug\ndesign, including target-specific molecular generation, molecular docking, and\nmolecular dynamics of protein-ligand complexes. We also cover conditional\ngeneration on physical properties, conformation generation, and fragment-based\ndrug design. By summarizing the state-of-the-art diffusion models for 3D\nmolecular generation, this review sheds light on their role in advancing drug\ndiscovery as well as their current limitations.\n""]",Molecular Representation Learning for Drug Discovery,Computational Methods for Molecular and Materials Science,Computational Biology and Chemistry,Computational Biology and Chemistry
9,499,9_reasoning_prompting_thinking_prompts,"['reasoning', 'prompting', 'thinking', 'prompts', 'inference', 'deductive', 'prompt', 'tasks', 'knowledge', 'logic']","['reasoning', 'math', 'thought', 'mathematical', 'logical', 'prompting', 'chain', 'abilities', 'step', 'problems']","['  Scaling up language models to billions of parameters has opened up\npossibilities for in-context learning, allowing instruction tuning and few-shot\nlearning on tasks that the model was not specifically trained for. This has\nachieved breakthrough performance on language tasks such as translation,\nsummarization, and question-answering. Furthermore, in addition to these\nassociative ""System 1"" tasks, recent advances in Chain-of-thought prompt\nlearning have demonstrated strong ""System 2"" reasoning abilities, answering a\nquestion in the field of artificial general intelligence whether LLMs can\nreason. The field started with the question whether LLMs can solve grade school\nmath word problems. This paper reviews the rapidly expanding field of\nprompt-based reasoning with LLMs. Our taxonomy identifies different ways to\ngenerate, evaluate, and control multi-step reasoning. We provide an in-depth\ncoverage of core approaches and open problems, and we propose a research agenda\nfor the near future. Finally, we highlight the relation between reasoning and\nprompt-based learning, and we discuss the relation between reasoning,\nsequential decision processes, and reinforcement learning. We find that\nself-improvement, self-reflection, and some metacognitive abilities of the\nreasoning processes are possible through the judicious use of prompts. True\nself-improvement and self-reasoning, to go from reasoning with LLMs to\nreasoning by LLMs, remains future work.\n', '  Chain-of-thought (CoT) prompting can guide language models to engage in\ncomplex multi-step reasoning. The quality of provided demonstrations\nsignificantly impacts the success of downstream inference tasks. While existing\nautomated methods prioritize accuracy and semantics in these demonstrations, we\nshow that the underlying reasoning patterns play a more crucial role in such\ntasks. In this paper, we propose Pattern-Aware CoT, a prompting method that\nconsiders the diversity of demonstration patterns. By incorporating patterns\nsuch as step length and reasoning process within intermediate steps, PA-CoT\neffectively mitigates the issue of bias induced by demonstrations and enables\nbetter generalization to diverse scenarios. We conduct experiments on nine\nreasoning benchmark tasks using two open-source LLMs. The results show that our\nmethod substantially enhances reasoning performance and exhibits robustness to\nerrors. The code will be made publicly available.\n', ""  Chain of Thought (CoT) is significant in improving the reasoning abilities of\nlarge language models (LLMs). However, the correlation between the\neffectiveness of CoT and the length of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we have conducted several empirical\nexperiments to explore the relations. Specifically, we design experiments that\nexpand and compress the rationale reasoning steps within CoT demonstrations\nwhile keeping all other factors constant. We have the following key findings.\nFirst, the results indicate that lengthening the reasoning steps in prompts,\neven without adding new information into the prompt, considerably enhances\nLLMs' reasoning abilities across multiple datasets. Alternatively, shortening\nthe reasoning steps, even while preserving the key information, significantly\ndiminishes the reasoning abilities of models. This finding highlights the\nimportance of the number of steps in CoT prompts and provides practical\nguidance to make better use of LLMs' potential in complex problem-solving\nscenarios. Second, we also investigated the relationship between the\nperformance of CoT and the rationales used in demonstrations. Surprisingly, the\nresult shows that even incorrect rationales can yield favorable outcomes if\nthey maintain the requisite length of inference. Third, we observed that the\nadvantages of increasing reasoning steps are task-dependent: simpler tasks\nrequire fewer steps, whereas complex tasks gain significantly from longer\ninference sequences. The code is available at\nhttps://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models\n""]",Reasoning with Large Language Models through Prompting,Advances in Large Language Models,Large Language Models,Large Language Models
9,499,9_reasoning_prompting_thinking_prompts,"['reasoning', 'prompting', 'thinking', 'prompts', 'inference', 'deductive', 'prompt', 'tasks', 'knowledge', 'logic']","['reasoning', 'math', 'thought', 'mathematical', 'logical', 'prompting', 'chain', 'abilities', 'step', 'problems']","['  Scaling up language models to billions of parameters has opened up\npossibilities for in-context learning, allowing instruction tuning and few-shot\nlearning on tasks that the model was not specifically trained for. This has\nachieved breakthrough performance on language tasks such as translation,\nsummarization, and question-answering. Furthermore, in addition to these\nassociative ""System 1"" tasks, recent advances in Chain-of-thought prompt\nlearning have demonstrated strong ""System 2"" reasoning abilities, answering a\nquestion in the field of artificial general intelligence whether LLMs can\nreason. The field started with the question whether LLMs can solve grade school\nmath word problems. This paper reviews the rapidly expanding field of\nprompt-based reasoning with LLMs. Our taxonomy identifies different ways to\ngenerate, evaluate, and control multi-step reasoning. We provide an in-depth\ncoverage of core approaches and open problems, and we propose a research agenda\nfor the near future. Finally, we highlight the relation between reasoning and\nprompt-based learning, and we discuss the relation between reasoning,\nsequential decision processes, and reinforcement learning. We find that\nself-improvement, self-reflection, and some metacognitive abilities of the\nreasoning processes are possible through the judicious use of prompts. True\nself-improvement and self-reasoning, to go from reasoning with LLMs to\nreasoning by LLMs, remains future work.\n', '  Chain-of-thought (CoT) prompting can guide language models to engage in\ncomplex multi-step reasoning. The quality of provided demonstrations\nsignificantly impacts the success of downstream inference tasks. While existing\nautomated methods prioritize accuracy and semantics in these demonstrations, we\nshow that the underlying reasoning patterns play a more crucial role in such\ntasks. In this paper, we propose Pattern-Aware CoT, a prompting method that\nconsiders the diversity of demonstration patterns. By incorporating patterns\nsuch as step length and reasoning process within intermediate steps, PA-CoT\neffectively mitigates the issue of bias induced by demonstrations and enables\nbetter generalization to diverse scenarios. We conduct experiments on nine\nreasoning benchmark tasks using two open-source LLMs. The results show that our\nmethod substantially enhances reasoning performance and exhibits robustness to\nerrors. The code will be made publicly available.\n', ""  Chain of Thought (CoT) is significant in improving the reasoning abilities of\nlarge language models (LLMs). However, the correlation between the\neffectiveness of CoT and the length of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we have conducted several empirical\nexperiments to explore the relations. Specifically, we design experiments that\nexpand and compress the rationale reasoning steps within CoT demonstrations\nwhile keeping all other factors constant. We have the following key findings.\nFirst, the results indicate that lengthening the reasoning steps in prompts,\neven without adding new information into the prompt, considerably enhances\nLLMs' reasoning abilities across multiple datasets. Alternatively, shortening\nthe reasoning steps, even while preserving the key information, significantly\ndiminishes the reasoning abilities of models. This finding highlights the\nimportance of the number of steps in CoT prompts and provides practical\nguidance to make better use of LLMs' potential in complex problem-solving\nscenarios. Second, we also investigated the relationship between the\nperformance of CoT and the rationales used in demonstrations. Surprisingly, the\nresult shows that even incorrect rationales can yield favorable outcomes if\nthey maintain the requisite length of inference. Third, we observed that the\nadvantages of increasing reasoning steps are task-dependent: simpler tasks\nrequire fewer steps, whereas complex tasks gain significantly from longer\ninference sequences. The code is available at\nhttps://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models\n""]",Reasoning with Large Language Models through Prompting,Advances in Large Language Models,Large Language Models,Large Language Models
9,499,9_reasoning_prompting_thinking_prompts,"['reasoning', 'prompting', 'thinking', 'prompts', 'inference', 'deductive', 'prompt', 'tasks', 'knowledge', 'logic']","['reasoning', 'math', 'thought', 'mathematical', 'logical', 'prompting', 'chain', 'abilities', 'step', 'problems']","['  Scaling up language models to billions of parameters has opened up\npossibilities for in-context learning, allowing instruction tuning and few-shot\nlearning on tasks that the model was not specifically trained for. This has\nachieved breakthrough performance on language tasks such as translation,\nsummarization, and question-answering. Furthermore, in addition to these\nassociative ""System 1"" tasks, recent advances in Chain-of-thought prompt\nlearning have demonstrated strong ""System 2"" reasoning abilities, answering a\nquestion in the field of artificial general intelligence whether LLMs can\nreason. The field started with the question whether LLMs can solve grade school\nmath word problems. This paper reviews the rapidly expanding field of\nprompt-based reasoning with LLMs. Our taxonomy identifies different ways to\ngenerate, evaluate, and control multi-step reasoning. We provide an in-depth\ncoverage of core approaches and open problems, and we propose a research agenda\nfor the near future. Finally, we highlight the relation between reasoning and\nprompt-based learning, and we discuss the relation between reasoning,\nsequential decision processes, and reinforcement learning. We find that\nself-improvement, self-reflection, and some metacognitive abilities of the\nreasoning processes are possible through the judicious use of prompts. True\nself-improvement and self-reasoning, to go from reasoning with LLMs to\nreasoning by LLMs, remains future work.\n', '  Chain-of-thought (CoT) prompting can guide language models to engage in\ncomplex multi-step reasoning. The quality of provided demonstrations\nsignificantly impacts the success of downstream inference tasks. While existing\nautomated methods prioritize accuracy and semantics in these demonstrations, we\nshow that the underlying reasoning patterns play a more crucial role in such\ntasks. In this paper, we propose Pattern-Aware CoT, a prompting method that\nconsiders the diversity of demonstration patterns. By incorporating patterns\nsuch as step length and reasoning process within intermediate steps, PA-CoT\neffectively mitigates the issue of bias induced by demonstrations and enables\nbetter generalization to diverse scenarios. We conduct experiments on nine\nreasoning benchmark tasks using two open-source LLMs. The results show that our\nmethod substantially enhances reasoning performance and exhibits robustness to\nerrors. The code will be made publicly available.\n', ""  Chain of Thought (CoT) is significant in improving the reasoning abilities of\nlarge language models (LLMs). However, the correlation between the\neffectiveness of CoT and the length of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we have conducted several empirical\nexperiments to explore the relations. Specifically, we design experiments that\nexpand and compress the rationale reasoning steps within CoT demonstrations\nwhile keeping all other factors constant. We have the following key findings.\nFirst, the results indicate that lengthening the reasoning steps in prompts,\neven without adding new information into the prompt, considerably enhances\nLLMs' reasoning abilities across multiple datasets. Alternatively, shortening\nthe reasoning steps, even while preserving the key information, significantly\ndiminishes the reasoning abilities of models. This finding highlights the\nimportance of the number of steps in CoT prompts and provides practical\nguidance to make better use of LLMs' potential in complex problem-solving\nscenarios. Second, we also investigated the relationship between the\nperformance of CoT and the rationales used in demonstrations. Surprisingly, the\nresult shows that even incorrect rationales can yield favorable outcomes if\nthey maintain the requisite length of inference. Third, we observed that the\nadvantages of increasing reasoning steps are task-dependent: simpler tasks\nrequire fewer steps, whereas complex tasks gain significantly from longer\ninference sequences. The code is available at\nhttps://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models\n""]",Reasoning with Large Language Models through Prompting,Advances in Large Language Models,Large Language Models,Large Language Models
9,499,9_reasoning_prompting_thinking_prompts,"['reasoning', 'prompting', 'thinking', 'prompts', 'inference', 'deductive', 'prompt', 'tasks', 'knowledge', 'logic']","['reasoning', 'math', 'thought', 'mathematical', 'logical', 'prompting', 'chain', 'abilities', 'step', 'problems']","['  Scaling up language models to billions of parameters has opened up\npossibilities for in-context learning, allowing instruction tuning and few-shot\nlearning on tasks that the model was not specifically trained for. This has\nachieved breakthrough performance on language tasks such as translation,\nsummarization, and question-answering. Furthermore, in addition to these\nassociative ""System 1"" tasks, recent advances in Chain-of-thought prompt\nlearning have demonstrated strong ""System 2"" reasoning abilities, answering a\nquestion in the field of artificial general intelligence whether LLMs can\nreason. The field started with the question whether LLMs can solve grade school\nmath word problems. This paper reviews the rapidly expanding field of\nprompt-based reasoning with LLMs. Our taxonomy identifies different ways to\ngenerate, evaluate, and control multi-step reasoning. We provide an in-depth\ncoverage of core approaches and open problems, and we propose a research agenda\nfor the near future. Finally, we highlight the relation between reasoning and\nprompt-based learning, and we discuss the relation between reasoning,\nsequential decision processes, and reinforcement learning. We find that\nself-improvement, self-reflection, and some metacognitive abilities of the\nreasoning processes are possible through the judicious use of prompts. True\nself-improvement and self-reasoning, to go from reasoning with LLMs to\nreasoning by LLMs, remains future work.\n', '  Chain-of-thought (CoT) prompting can guide language models to engage in\ncomplex multi-step reasoning. The quality of provided demonstrations\nsignificantly impacts the success of downstream inference tasks. While existing\nautomated methods prioritize accuracy and semantics in these demonstrations, we\nshow that the underlying reasoning patterns play a more crucial role in such\ntasks. In this paper, we propose Pattern-Aware CoT, a prompting method that\nconsiders the diversity of demonstration patterns. By incorporating patterns\nsuch as step length and reasoning process within intermediate steps, PA-CoT\neffectively mitigates the issue of bias induced by demonstrations and enables\nbetter generalization to diverse scenarios. We conduct experiments on nine\nreasoning benchmark tasks using two open-source LLMs. The results show that our\nmethod substantially enhances reasoning performance and exhibits robustness to\nerrors. The code will be made publicly available.\n', ""  Chain of Thought (CoT) is significant in improving the reasoning abilities of\nlarge language models (LLMs). However, the correlation between the\neffectiveness of CoT and the length of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we have conducted several empirical\nexperiments to explore the relations. Specifically, we design experiments that\nexpand and compress the rationale reasoning steps within CoT demonstrations\nwhile keeping all other factors constant. We have the following key findings.\nFirst, the results indicate that lengthening the reasoning steps in prompts,\neven without adding new information into the prompt, considerably enhances\nLLMs' reasoning abilities across multiple datasets. Alternatively, shortening\nthe reasoning steps, even while preserving the key information, significantly\ndiminishes the reasoning abilities of models. This finding highlights the\nimportance of the number of steps in CoT prompts and provides practical\nguidance to make better use of LLMs' potential in complex problem-solving\nscenarios. Second, we also investigated the relationship between the\nperformance of CoT and the rationales used in demonstrations. Surprisingly, the\nresult shows that even incorrect rationales can yield favorable outcomes if\nthey maintain the requisite length of inference. Third, we observed that the\nadvantages of increasing reasoning steps are task-dependent: simpler tasks\nrequire fewer steps, whereas complex tasks gain significantly from longer\ninference sequences. The code is available at\nhttps://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models\n""]",Reasoning with Large Language Models through Prompting,Advances in Large Language Models,Large Language Models,Large Language Models
9,499,9_reasoning_prompting_thinking_prompts,"['reasoning', 'prompting', 'thinking', 'prompts', 'inference', 'deductive', 'prompt', 'tasks', 'knowledge', 'logic']","['reasoning', 'math', 'thought', 'mathematical', 'logical', 'prompting', 'chain', 'abilities', 'step', 'problems']","['  Scaling up language models to billions of parameters has opened up\npossibilities for in-context learning, allowing instruction tuning and few-shot\nlearning on tasks that the model was not specifically trained for. This has\nachieved breakthrough performance on language tasks such as translation,\nsummarization, and question-answering. Furthermore, in addition to these\nassociative ""System 1"" tasks, recent advances in Chain-of-thought prompt\nlearning have demonstrated strong ""System 2"" reasoning abilities, answering a\nquestion in the field of artificial general intelligence whether LLMs can\nreason. The field started with the question whether LLMs can solve grade school\nmath word problems. This paper reviews the rapidly expanding field of\nprompt-based reasoning with LLMs. Our taxonomy identifies different ways to\ngenerate, evaluate, and control multi-step reasoning. We provide an in-depth\ncoverage of core approaches and open problems, and we propose a research agenda\nfor the near future. Finally, we highlight the relation between reasoning and\nprompt-based learning, and we discuss the relation between reasoning,\nsequential decision processes, and reinforcement learning. We find that\nself-improvement, self-reflection, and some metacognitive abilities of the\nreasoning processes are possible through the judicious use of prompts. True\nself-improvement and self-reasoning, to go from reasoning with LLMs to\nreasoning by LLMs, remains future work.\n', '  Chain-of-thought (CoT) prompting can guide language models to engage in\ncomplex multi-step reasoning. The quality of provided demonstrations\nsignificantly impacts the success of downstream inference tasks. While existing\nautomated methods prioritize accuracy and semantics in these demonstrations, we\nshow that the underlying reasoning patterns play a more crucial role in such\ntasks. In this paper, we propose Pattern-Aware CoT, a prompting method that\nconsiders the diversity of demonstration patterns. By incorporating patterns\nsuch as step length and reasoning process within intermediate steps, PA-CoT\neffectively mitigates the issue of bias induced by demonstrations and enables\nbetter generalization to diverse scenarios. We conduct experiments on nine\nreasoning benchmark tasks using two open-source LLMs. The results show that our\nmethod substantially enhances reasoning performance and exhibits robustness to\nerrors. The code will be made publicly available.\n', ""  Chain of Thought (CoT) is significant in improving the reasoning abilities of\nlarge language models (LLMs). However, the correlation between the\neffectiveness of CoT and the length of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we have conducted several empirical\nexperiments to explore the relations. Specifically, we design experiments that\nexpand and compress the rationale reasoning steps within CoT demonstrations\nwhile keeping all other factors constant. We have the following key findings.\nFirst, the results indicate that lengthening the reasoning steps in prompts,\neven without adding new information into the prompt, considerably enhances\nLLMs' reasoning abilities across multiple datasets. Alternatively, shortening\nthe reasoning steps, even while preserving the key information, significantly\ndiminishes the reasoning abilities of models. This finding highlights the\nimportance of the number of steps in CoT prompts and provides practical\nguidance to make better use of LLMs' potential in complex problem-solving\nscenarios. Second, we also investigated the relationship between the\nperformance of CoT and the rationales used in demonstrations. Surprisingly, the\nresult shows that even incorrect rationales can yield favorable outcomes if\nthey maintain the requisite length of inference. Third, we observed that the\nadvantages of increasing reasoning steps are task-dependent: simpler tasks\nrequire fewer steps, whereas complex tasks gain significantly from longer\ninference sequences. The code is available at\nhttps://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models\n""]",Reasoning with Large Language Models through Prompting,Advances in Large Language Models,Large Language Models,Large Language Models
9,499,9_reasoning_prompting_thinking_prompts,"['reasoning', 'prompting', 'thinking', 'prompts', 'inference', 'deductive', 'prompt', 'tasks', 'knowledge', 'logic']","['reasoning', 'math', 'thought', 'mathematical', 'logical', 'prompting', 'chain', 'abilities', 'step', 'problems']","['  Scaling up language models to billions of parameters has opened up\npossibilities for in-context learning, allowing instruction tuning and few-shot\nlearning on tasks that the model was not specifically trained for. This has\nachieved breakthrough performance on language tasks such as translation,\nsummarization, and question-answering. Furthermore, in addition to these\nassociative ""System 1"" tasks, recent advances in Chain-of-thought prompt\nlearning have demonstrated strong ""System 2"" reasoning abilities, answering a\nquestion in the field of artificial general intelligence whether LLMs can\nreason. The field started with the question whether LLMs can solve grade school\nmath word problems. This paper reviews the rapidly expanding field of\nprompt-based reasoning with LLMs. Our taxonomy identifies different ways to\ngenerate, evaluate, and control multi-step reasoning. We provide an in-depth\ncoverage of core approaches and open problems, and we propose a research agenda\nfor the near future. Finally, we highlight the relation between reasoning and\nprompt-based learning, and we discuss the relation between reasoning,\nsequential decision processes, and reinforcement learning. We find that\nself-improvement, self-reflection, and some metacognitive abilities of the\nreasoning processes are possible through the judicious use of prompts. True\nself-improvement and self-reasoning, to go from reasoning with LLMs to\nreasoning by LLMs, remains future work.\n', '  Chain-of-thought (CoT) prompting can guide language models to engage in\ncomplex multi-step reasoning. The quality of provided demonstrations\nsignificantly impacts the success of downstream inference tasks. While existing\nautomated methods prioritize accuracy and semantics in these demonstrations, we\nshow that the underlying reasoning patterns play a more crucial role in such\ntasks. In this paper, we propose Pattern-Aware CoT, a prompting method that\nconsiders the diversity of demonstration patterns. By incorporating patterns\nsuch as step length and reasoning process within intermediate steps, PA-CoT\neffectively mitigates the issue of bias induced by demonstrations and enables\nbetter generalization to diverse scenarios. We conduct experiments on nine\nreasoning benchmark tasks using two open-source LLMs. The results show that our\nmethod substantially enhances reasoning performance and exhibits robustness to\nerrors. The code will be made publicly available.\n', ""  Chain of Thought (CoT) is significant in improving the reasoning abilities of\nlarge language models (LLMs). However, the correlation between the\neffectiveness of CoT and the length of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we have conducted several empirical\nexperiments to explore the relations. Specifically, we design experiments that\nexpand and compress the rationale reasoning steps within CoT demonstrations\nwhile keeping all other factors constant. We have the following key findings.\nFirst, the results indicate that lengthening the reasoning steps in prompts,\neven without adding new information into the prompt, considerably enhances\nLLMs' reasoning abilities across multiple datasets. Alternatively, shortening\nthe reasoning steps, even while preserving the key information, significantly\ndiminishes the reasoning abilities of models. This finding highlights the\nimportance of the number of steps in CoT prompts and provides practical\nguidance to make better use of LLMs' potential in complex problem-solving\nscenarios. Second, we also investigated the relationship between the\nperformance of CoT and the rationales used in demonstrations. Surprisingly, the\nresult shows that even incorrect rationales can yield favorable outcomes if\nthey maintain the requisite length of inference. Third, we observed that the\nadvantages of increasing reasoning steps are task-dependent: simpler tasks\nrequire fewer steps, whereas complex tasks gain significantly from longer\ninference sequences. The code is available at\nhttps://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models\n""]",Reasoning with Large Language Models through Prompting,Advances in Large Language Models,Large Language Models,Large Language Models
10,492,10_compilers_programming_coding_compiler,"['compilers', 'programming', 'coding', 'compiler', 'code', 'programmers', 'snippets', 'developers', 'programmer', 'apis']","['code', 'software', 'programming', 'program', 'programs', 'developers', 'generation', 'bug', 'repair', 'bugs']","['  Large language models for code (i.e., code LLMs) have shown strong code\nunderstanding and generation capabilities. To evaluate the capabilities of code\nLLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval\nand ClassEval). Code reasoning is one of the most essential abilities of code\nLLMs, but existing benchmarks for code reasoning are not sufficient. Typically,\nthey focus on predicting the input and output of a program, ignoring the\nevaluation of the intermediate behavior during program execution, as well as\nthe logical consistency (e.g., the model should not give the correct output if\nthe prediction of execution path is wrong) when performing the reasoning. To\naddress these problems, in this paper, we propose a framework, namely REval,\nfor evaluating code reasoning abilities and consistency of code LLMs with\nprogram execution. We utilize existing code benchmarks and adapt them to new\nbenchmarks within our framework. A large-scale empirical study is conducted and\nmost LLMs show unsatisfactory performance on both Runtime Behavior Reasoning\n(i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation\n(i.e., an average IC score of 10.3). Evaluation results of current code LLMs\nreflect the urgent need for the community to strengthen the code reasoning\ncapability of code LLMs. Our code, data, and \\newname leaderboard are available\nat https://r-eval.github.io.\n', ""  Code Large Language Models (Code LLMs) have demonstrated outstanding\nperformance in code-related tasks. Several instruction tuning approaches have\nbeen proposed to boost the code generation performance of pre-trained Code\nLLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with\nself-evaluating for code generation. It learns diverse instruction targets and\ncombines a code evaluation objective to enhance its code generation ability.\nOur model achieves superior performance on the HumanEval and MBPP benchmarks,\ndemonstrating new insights for future code instruction tuning work. Our key\nfindings are: (1) Augmenting more diverse responses with distinct reasoning\npaths increases the code capability of LLMs. (2) Improving one's ability to\nevaluate the correctness of code solutions also enhances their ability to\ncreate it.\n"", '  Context. Nowadays, 83% of software developers use Large Language Models\n(LLMs) to generate code. LLMs recently became essential to increase the\nproductivity of software developers and decrease the time and cost of software\ndevelopment. Developers ranging from novices to experts use LLM tools not only\nto detect and patch bugs, but also to integrate generated code into their\nsoftware. However, as of today there is no objective assessment of the energy\nefficiency of the source code generated by LLM tools. Released in August 2023,\nCode Llama is one of the most recent LLM tools.\n  Goal. In this paper, we present an empirical study that assesses the energy\nefficiency of Code Llama with respect to human-written source code.\n  Method. We design an experiment involving three human-written benchmarks\nimplemented in C++, JavaScript, and Python. We ask Code Llama to generate the\ncode of the benchmarks using different prompts and temperatures. Therefore, we\nexecute both implementations and profile their energy efficiency.\n  Results. Our study shows that the energy efficiency of code generated by Code\nLlama is heavily-dependent on the chosen programming language and the specific\ncode problem at hand. Also, human implementations tend to be more energy\nefficient overall, with generated JavaScript code outperforming its human\ncounterpart. Moreover, explicitly asking Code Llama to generate\nenergy-efficient code results in an equal or worse energy efficiency, as well\nas using different temperatures seems not to affect the energy efficiency of\ngenerated code.\n  Conclusions. According to our results, code generated using Code Llama does\nnot guarantee energy efficiency, even when prompted to do so. Therefore,\nsoftware developers should evaluate the energy efficiency of generated code\nbefore integrating it into the software system under development.\n']",Code Generation and Evaluation for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
10,492,10_compilers_programming_coding_compiler,"['compilers', 'programming', 'coding', 'compiler', 'code', 'programmers', 'snippets', 'developers', 'programmer', 'apis']","['code', 'software', 'programming', 'program', 'programs', 'developers', 'generation', 'bug', 'repair', 'bugs']","['  Large language models for code (i.e., code LLMs) have shown strong code\nunderstanding and generation capabilities. To evaluate the capabilities of code\nLLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval\nand ClassEval). Code reasoning is one of the most essential abilities of code\nLLMs, but existing benchmarks for code reasoning are not sufficient. Typically,\nthey focus on predicting the input and output of a program, ignoring the\nevaluation of the intermediate behavior during program execution, as well as\nthe logical consistency (e.g., the model should not give the correct output if\nthe prediction of execution path is wrong) when performing the reasoning. To\naddress these problems, in this paper, we propose a framework, namely REval,\nfor evaluating code reasoning abilities and consistency of code LLMs with\nprogram execution. We utilize existing code benchmarks and adapt them to new\nbenchmarks within our framework. A large-scale empirical study is conducted and\nmost LLMs show unsatisfactory performance on both Runtime Behavior Reasoning\n(i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation\n(i.e., an average IC score of 10.3). Evaluation results of current code LLMs\nreflect the urgent need for the community to strengthen the code reasoning\ncapability of code LLMs. Our code, data, and \\newname leaderboard are available\nat https://r-eval.github.io.\n', ""  Code Large Language Models (Code LLMs) have demonstrated outstanding\nperformance in code-related tasks. Several instruction tuning approaches have\nbeen proposed to boost the code generation performance of pre-trained Code\nLLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with\nself-evaluating for code generation. It learns diverse instruction targets and\ncombines a code evaluation objective to enhance its code generation ability.\nOur model achieves superior performance on the HumanEval and MBPP benchmarks,\ndemonstrating new insights for future code instruction tuning work. Our key\nfindings are: (1) Augmenting more diverse responses with distinct reasoning\npaths increases the code capability of LLMs. (2) Improving one's ability to\nevaluate the correctness of code solutions also enhances their ability to\ncreate it.\n"", '  Context. Nowadays, 83% of software developers use Large Language Models\n(LLMs) to generate code. LLMs recently became essential to increase the\nproductivity of software developers and decrease the time and cost of software\ndevelopment. Developers ranging from novices to experts use LLM tools not only\nto detect and patch bugs, but also to integrate generated code into their\nsoftware. However, as of today there is no objective assessment of the energy\nefficiency of the source code generated by LLM tools. Released in August 2023,\nCode Llama is one of the most recent LLM tools.\n  Goal. In this paper, we present an empirical study that assesses the energy\nefficiency of Code Llama with respect to human-written source code.\n  Method. We design an experiment involving three human-written benchmarks\nimplemented in C++, JavaScript, and Python. We ask Code Llama to generate the\ncode of the benchmarks using different prompts and temperatures. Therefore, we\nexecute both implementations and profile their energy efficiency.\n  Results. Our study shows that the energy efficiency of code generated by Code\nLlama is heavily-dependent on the chosen programming language and the specific\ncode problem at hand. Also, human implementations tend to be more energy\nefficient overall, with generated JavaScript code outperforming its human\ncounterpart. Moreover, explicitly asking Code Llama to generate\nenergy-efficient code results in an equal or worse energy efficiency, as well\nas using different temperatures seems not to affect the energy efficiency of\ngenerated code.\n  Conclusions. According to our results, code generated using Code Llama does\nnot guarantee energy efficiency, even when prompted to do so. Therefore,\nsoftware developers should evaluate the energy efficiency of generated code\nbefore integrating it into the software system under development.\n']",Code Generation and Evaluation for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
10,492,10_compilers_programming_coding_compiler,"['compilers', 'programming', 'coding', 'compiler', 'code', 'programmers', 'snippets', 'developers', 'programmer', 'apis']","['code', 'software', 'programming', 'program', 'programs', 'developers', 'generation', 'bug', 'repair', 'bugs']","['  Large language models for code (i.e., code LLMs) have shown strong code\nunderstanding and generation capabilities. To evaluate the capabilities of code\nLLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval\nand ClassEval). Code reasoning is one of the most essential abilities of code\nLLMs, but existing benchmarks for code reasoning are not sufficient. Typically,\nthey focus on predicting the input and output of a program, ignoring the\nevaluation of the intermediate behavior during program execution, as well as\nthe logical consistency (e.g., the model should not give the correct output if\nthe prediction of execution path is wrong) when performing the reasoning. To\naddress these problems, in this paper, we propose a framework, namely REval,\nfor evaluating code reasoning abilities and consistency of code LLMs with\nprogram execution. We utilize existing code benchmarks and adapt them to new\nbenchmarks within our framework. A large-scale empirical study is conducted and\nmost LLMs show unsatisfactory performance on both Runtime Behavior Reasoning\n(i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation\n(i.e., an average IC score of 10.3). Evaluation results of current code LLMs\nreflect the urgent need for the community to strengthen the code reasoning\ncapability of code LLMs. Our code, data, and \\newname leaderboard are available\nat https://r-eval.github.io.\n', ""  Code Large Language Models (Code LLMs) have demonstrated outstanding\nperformance in code-related tasks. Several instruction tuning approaches have\nbeen proposed to boost the code generation performance of pre-trained Code\nLLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with\nself-evaluating for code generation. It learns diverse instruction targets and\ncombines a code evaluation objective to enhance its code generation ability.\nOur model achieves superior performance on the HumanEval and MBPP benchmarks,\ndemonstrating new insights for future code instruction tuning work. Our key\nfindings are: (1) Augmenting more diverse responses with distinct reasoning\npaths increases the code capability of LLMs. (2) Improving one's ability to\nevaluate the correctness of code solutions also enhances their ability to\ncreate it.\n"", '  Context. Nowadays, 83% of software developers use Large Language Models\n(LLMs) to generate code. LLMs recently became essential to increase the\nproductivity of software developers and decrease the time and cost of software\ndevelopment. Developers ranging from novices to experts use LLM tools not only\nto detect and patch bugs, but also to integrate generated code into their\nsoftware. However, as of today there is no objective assessment of the energy\nefficiency of the source code generated by LLM tools. Released in August 2023,\nCode Llama is one of the most recent LLM tools.\n  Goal. In this paper, we present an empirical study that assesses the energy\nefficiency of Code Llama with respect to human-written source code.\n  Method. We design an experiment involving three human-written benchmarks\nimplemented in C++, JavaScript, and Python. We ask Code Llama to generate the\ncode of the benchmarks using different prompts and temperatures. Therefore, we\nexecute both implementations and profile their energy efficiency.\n  Results. Our study shows that the energy efficiency of code generated by Code\nLlama is heavily-dependent on the chosen programming language and the specific\ncode problem at hand. Also, human implementations tend to be more energy\nefficient overall, with generated JavaScript code outperforming its human\ncounterpart. Moreover, explicitly asking Code Llama to generate\nenergy-efficient code results in an equal or worse energy efficiency, as well\nas using different temperatures seems not to affect the energy efficiency of\ngenerated code.\n  Conclusions. According to our results, code generated using Code Llama does\nnot guarantee energy efficiency, even when prompted to do so. Therefore,\nsoftware developers should evaluate the energy efficiency of generated code\nbefore integrating it into the software system under development.\n']",Code Generation and Evaluation for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
10,492,10_compilers_programming_coding_compiler,"['compilers', 'programming', 'coding', 'compiler', 'code', 'programmers', 'snippets', 'developers', 'programmer', 'apis']","['code', 'software', 'programming', 'program', 'programs', 'developers', 'generation', 'bug', 'repair', 'bugs']","['  Large language models for code (i.e., code LLMs) have shown strong code\nunderstanding and generation capabilities. To evaluate the capabilities of code\nLLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval\nand ClassEval). Code reasoning is one of the most essential abilities of code\nLLMs, but existing benchmarks for code reasoning are not sufficient. Typically,\nthey focus on predicting the input and output of a program, ignoring the\nevaluation of the intermediate behavior during program execution, as well as\nthe logical consistency (e.g., the model should not give the correct output if\nthe prediction of execution path is wrong) when performing the reasoning. To\naddress these problems, in this paper, we propose a framework, namely REval,\nfor evaluating code reasoning abilities and consistency of code LLMs with\nprogram execution. We utilize existing code benchmarks and adapt them to new\nbenchmarks within our framework. A large-scale empirical study is conducted and\nmost LLMs show unsatisfactory performance on both Runtime Behavior Reasoning\n(i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation\n(i.e., an average IC score of 10.3). Evaluation results of current code LLMs\nreflect the urgent need for the community to strengthen the code reasoning\ncapability of code LLMs. Our code, data, and \\newname leaderboard are available\nat https://r-eval.github.io.\n', ""  Code Large Language Models (Code LLMs) have demonstrated outstanding\nperformance in code-related tasks. Several instruction tuning approaches have\nbeen proposed to boost the code generation performance of pre-trained Code\nLLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with\nself-evaluating for code generation. It learns diverse instruction targets and\ncombines a code evaluation objective to enhance its code generation ability.\nOur model achieves superior performance on the HumanEval and MBPP benchmarks,\ndemonstrating new insights for future code instruction tuning work. Our key\nfindings are: (1) Augmenting more diverse responses with distinct reasoning\npaths increases the code capability of LLMs. (2) Improving one's ability to\nevaluate the correctness of code solutions also enhances their ability to\ncreate it.\n"", '  Context. Nowadays, 83% of software developers use Large Language Models\n(LLMs) to generate code. LLMs recently became essential to increase the\nproductivity of software developers and decrease the time and cost of software\ndevelopment. Developers ranging from novices to experts use LLM tools not only\nto detect and patch bugs, but also to integrate generated code into their\nsoftware. However, as of today there is no objective assessment of the energy\nefficiency of the source code generated by LLM tools. Released in August 2023,\nCode Llama is one of the most recent LLM tools.\n  Goal. In this paper, we present an empirical study that assesses the energy\nefficiency of Code Llama with respect to human-written source code.\n  Method. We design an experiment involving three human-written benchmarks\nimplemented in C++, JavaScript, and Python. We ask Code Llama to generate the\ncode of the benchmarks using different prompts and temperatures. Therefore, we\nexecute both implementations and profile their energy efficiency.\n  Results. Our study shows that the energy efficiency of code generated by Code\nLlama is heavily-dependent on the chosen programming language and the specific\ncode problem at hand. Also, human implementations tend to be more energy\nefficient overall, with generated JavaScript code outperforming its human\ncounterpart. Moreover, explicitly asking Code Llama to generate\nenergy-efficient code results in an equal or worse energy efficiency, as well\nas using different temperatures seems not to affect the energy efficiency of\ngenerated code.\n  Conclusions. According to our results, code generated using Code Llama does\nnot guarantee energy efficiency, even when prompted to do so. Therefore,\nsoftware developers should evaluate the energy efficiency of generated code\nbefore integrating it into the software system under development.\n']",Code Generation and Evaluation for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
10,492,10_compilers_programming_coding_compiler,"['compilers', 'programming', 'coding', 'compiler', 'code', 'programmers', 'snippets', 'developers', 'programmer', 'apis']","['code', 'software', 'programming', 'program', 'programs', 'developers', 'generation', 'bug', 'repair', 'bugs']","['  Large language models for code (i.e., code LLMs) have shown strong code\nunderstanding and generation capabilities. To evaluate the capabilities of code\nLLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval\nand ClassEval). Code reasoning is one of the most essential abilities of code\nLLMs, but existing benchmarks for code reasoning are not sufficient. Typically,\nthey focus on predicting the input and output of a program, ignoring the\nevaluation of the intermediate behavior during program execution, as well as\nthe logical consistency (e.g., the model should not give the correct output if\nthe prediction of execution path is wrong) when performing the reasoning. To\naddress these problems, in this paper, we propose a framework, namely REval,\nfor evaluating code reasoning abilities and consistency of code LLMs with\nprogram execution. We utilize existing code benchmarks and adapt them to new\nbenchmarks within our framework. A large-scale empirical study is conducted and\nmost LLMs show unsatisfactory performance on both Runtime Behavior Reasoning\n(i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation\n(i.e., an average IC score of 10.3). Evaluation results of current code LLMs\nreflect the urgent need for the community to strengthen the code reasoning\ncapability of code LLMs. Our code, data, and \\newname leaderboard are available\nat https://r-eval.github.io.\n', ""  Code Large Language Models (Code LLMs) have demonstrated outstanding\nperformance in code-related tasks. Several instruction tuning approaches have\nbeen proposed to boost the code generation performance of pre-trained Code\nLLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with\nself-evaluating for code generation. It learns diverse instruction targets and\ncombines a code evaluation objective to enhance its code generation ability.\nOur model achieves superior performance on the HumanEval and MBPP benchmarks,\ndemonstrating new insights for future code instruction tuning work. Our key\nfindings are: (1) Augmenting more diverse responses with distinct reasoning\npaths increases the code capability of LLMs. (2) Improving one's ability to\nevaluate the correctness of code solutions also enhances their ability to\ncreate it.\n"", '  Context. Nowadays, 83% of software developers use Large Language Models\n(LLMs) to generate code. LLMs recently became essential to increase the\nproductivity of software developers and decrease the time and cost of software\ndevelopment. Developers ranging from novices to experts use LLM tools not only\nto detect and patch bugs, but also to integrate generated code into their\nsoftware. However, as of today there is no objective assessment of the energy\nefficiency of the source code generated by LLM tools. Released in August 2023,\nCode Llama is one of the most recent LLM tools.\n  Goal. In this paper, we present an empirical study that assesses the energy\nefficiency of Code Llama with respect to human-written source code.\n  Method. We design an experiment involving three human-written benchmarks\nimplemented in C++, JavaScript, and Python. We ask Code Llama to generate the\ncode of the benchmarks using different prompts and temperatures. Therefore, we\nexecute both implementations and profile their energy efficiency.\n  Results. Our study shows that the energy efficiency of code generated by Code\nLlama is heavily-dependent on the chosen programming language and the specific\ncode problem at hand. Also, human implementations tend to be more energy\nefficient overall, with generated JavaScript code outperforming its human\ncounterpart. Moreover, explicitly asking Code Llama to generate\nenergy-efficient code results in an equal or worse energy efficiency, as well\nas using different temperatures seems not to affect the energy efficiency of\ngenerated code.\n  Conclusions. According to our results, code generated using Code Llama does\nnot guarantee energy efficiency, even when prompted to do so. Therefore,\nsoftware developers should evaluate the energy efficiency of generated code\nbefore integrating it into the software system under development.\n']",Code Generation and Evaluation for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
10,492,10_compilers_programming_coding_compiler,"['compilers', 'programming', 'coding', 'compiler', 'code', 'programmers', 'snippets', 'developers', 'programmer', 'apis']","['code', 'software', 'programming', 'program', 'programs', 'developers', 'generation', 'bug', 'repair', 'bugs']","['  Large language models for code (i.e., code LLMs) have shown strong code\nunderstanding and generation capabilities. To evaluate the capabilities of code\nLLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval\nand ClassEval). Code reasoning is one of the most essential abilities of code\nLLMs, but existing benchmarks for code reasoning are not sufficient. Typically,\nthey focus on predicting the input and output of a program, ignoring the\nevaluation of the intermediate behavior during program execution, as well as\nthe logical consistency (e.g., the model should not give the correct output if\nthe prediction of execution path is wrong) when performing the reasoning. To\naddress these problems, in this paper, we propose a framework, namely REval,\nfor evaluating code reasoning abilities and consistency of code LLMs with\nprogram execution. We utilize existing code benchmarks and adapt them to new\nbenchmarks within our framework. A large-scale empirical study is conducted and\nmost LLMs show unsatisfactory performance on both Runtime Behavior Reasoning\n(i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation\n(i.e., an average IC score of 10.3). Evaluation results of current code LLMs\nreflect the urgent need for the community to strengthen the code reasoning\ncapability of code LLMs. Our code, data, and \\newname leaderboard are available\nat https://r-eval.github.io.\n', ""  Code Large Language Models (Code LLMs) have demonstrated outstanding\nperformance in code-related tasks. Several instruction tuning approaches have\nbeen proposed to boost the code generation performance of pre-trained Code\nLLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with\nself-evaluating for code generation. It learns diverse instruction targets and\ncombines a code evaluation objective to enhance its code generation ability.\nOur model achieves superior performance on the HumanEval and MBPP benchmarks,\ndemonstrating new insights for future code instruction tuning work. Our key\nfindings are: (1) Augmenting more diverse responses with distinct reasoning\npaths increases the code capability of LLMs. (2) Improving one's ability to\nevaluate the correctness of code solutions also enhances their ability to\ncreate it.\n"", '  Context. Nowadays, 83% of software developers use Large Language Models\n(LLMs) to generate code. LLMs recently became essential to increase the\nproductivity of software developers and decrease the time and cost of software\ndevelopment. Developers ranging from novices to experts use LLM tools not only\nto detect and patch bugs, but also to integrate generated code into their\nsoftware. However, as of today there is no objective assessment of the energy\nefficiency of the source code generated by LLM tools. Released in August 2023,\nCode Llama is one of the most recent LLM tools.\n  Goal. In this paper, we present an empirical study that assesses the energy\nefficiency of Code Llama with respect to human-written source code.\n  Method. We design an experiment involving three human-written benchmarks\nimplemented in C++, JavaScript, and Python. We ask Code Llama to generate the\ncode of the benchmarks using different prompts and temperatures. Therefore, we\nexecute both implementations and profile their energy efficiency.\n  Results. Our study shows that the energy efficiency of code generated by Code\nLlama is heavily-dependent on the chosen programming language and the specific\ncode problem at hand. Also, human implementations tend to be more energy\nefficient overall, with generated JavaScript code outperforming its human\ncounterpart. Moreover, explicitly asking Code Llama to generate\nenergy-efficient code results in an equal or worse energy efficiency, as well\nas using different temperatures seems not to affect the energy efficiency of\ngenerated code.\n  Conclusions. According to our results, code generated using Code Llama does\nnot guarantee energy efficiency, even when prompted to do so. Therefore,\nsoftware developers should evaluate the energy efficiency of generated code\nbefore integrating it into the software system under development.\n']",Code Generation and Evaluation for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
11,471,11_causal_causality_causally_confounders,"['causal', 'causality', 'causally', 'confounders', 'unobserved', 'inference', 'discovery', 'observational', 'data', 'inferring']","['causal', 'treatment', 'variables', 'observational', 'causality', 'discovery', 'effects', 'effect', 'identifiability', 'interventions']","['  The ability to understand causality from data is one of the major milestones\nof human-level intelligence. Causal Discovery (CD) algorithms can identify the\ncause-effect relationships among the variables of a system from related\nobservational data with certain assumptions. Over the years, several methods\nhave been developed primarily based on the statistical properties of data to\nuncover the underlying causal mechanism. In this study, we present an extensive\ndiscussion on the methods designed to perform causal discovery from both\nindependent and identically distributed (I.I.D.) data and time series data. For\nthis purpose, we first introduce the common terminologies used in causal\ndiscovery literature and then provide a comprehensive discussion of the\nalgorithms designed to identify causal relations in different settings. We\nfurther discuss some of the benchmark datasets available for evaluating the\nalgorithmic performance, off-the-shelf tools or software packages to perform\ncausal discovery readily, and the common metrics used to evaluate these\nmethods. We also evaluate some widely used causal discovery algorithms on\nmultiple benchmark datasets and compare their performances. Finally, we\nconclude by discussing the research challenges and the applications of causal\ndiscovery algorithms in multiple areas of interest.\n', ""  Causal discovery from observational data holds great promise, but existing\nmethods rely on strong assumptions about the underlying causal structure, often\nrequiring full observability of all relevant variables. We tackle these\nchallenges by leveraging the score function $\\nabla \\log p(X)$ of observed\nvariables for causal discovery and propose the following contributions. First,\nwe generalize the existing results of identifiability with the score to\nadditive noise models with minimal requirements on the causal mechanisms.\nSecond, we establish conditions for inferring causal relations from the score\neven in the presence of hidden variables; this result is two-faced: we\ndemonstrate the score's potential as an alternative to conditional independence\ntests to infer the equivalence class of causal graphs with hidden variables,\nand we provide the necessary conditions for identifying direct causes in latent\nvariable models. Building on these insights, we propose a flexible algorithm\nfor causal discovery across linear, nonlinear, and latent variable models,\nwhich we empirically validate.\n"", '  This paper proposes techniques to enhance the performance of non-causal\nmodels for causal inference using data from randomized experiments. In domains\nlike advertising, customer retention, and precision medicine, non-causal models\nthat predict outcomes under no intervention are often used to score individuals\nand rank them according to the expected effectiveness of an intervention (e.g,\nan ad, a retention incentive, a nudge). However, these scores may not perfectly\ncorrespond to intervention effects due to the inherent non-causal nature of the\nmodels. To address this limitation, we propose causal fine-tuning and effect\ncalibration, two techniques that leverage experimental data to refine the\noutput of non-causal models for different causal tasks, including effect\nestimation, effect ordering, and effect classification. They are underpinned by\ntwo key advantages. First, they can effectively integrate the predictive\ncapabilities of general non-causal models with the requirements of a causal\ntask in a specific context, allowing decision makers to support diverse causal\napplications with a ""foundational"" scoring model. Second, through simulations\nand an empirical example, we demonstrate that they can outperform the\nalternative of building a causal-effect model from scratch, particularly when\nthe available experimental data is limited and the non-causal scores already\ncapture substantial information about the relative sizes of causal effects.\nOverall, this research underscores the practical advantages of combining\nexperimental data with non-causal models to support causal applications.\n']",Causal Discovery and Inference from Data,Causal Analysis and Counterfactual Reasoning,Causal Analysis and Reasoning,Causal Analysis and Reasoning
12,457,12_retrieval_semantic_search_relevance,"['retrieval', 'semantic', 'search', 'relevance', 'corpus', 'ranking', 'recall', 'retrieved', 'indexing', 'retrievers']","['retrieval', 'documents', 'query', 'document', 'ranking', 'relevance', 'queries', 'answering', 'passage', 'question']","['  With the rapid development of large-scale language models,\nRetrieval-Augmented Generation (RAG) has been widely adopted. However, existing\nRAG paradigms are inevitably influenced by erroneous retrieval information,\nthereby reducing the reliability and correctness of generated results.\nTherefore, to improve the relevance of retrieval information, this study\nproposes a method that replaces traditional retrievers with GPT-3.5, leveraging\nits vast corpus knowledge to generate retrieval information. We also propose a\nweb retrieval based method to implement fine-grained knowledge retrieval,\nUtilizing the powerful reasoning capability of GPT-3.5 to realize semantic\npartitioning of problem.In order to mitigate the illusion of GPT retrieval and\nreduce noise in Web retrieval,we proposes a multi-source retrieval framework,\nnamed MSRAG, which combines GPT retrieval with web retrieval. Experiments on\nmultiple knowledge-intensive QA datasets demonstrate that the proposed\nframework in this study performs better than existing RAG framework in\nenhancing the overall efficiency and accuracy of QA systems.\n', '  The retrieval-augmented generation (RAG) enables retrieval of relevant\ninformation from an external knowledge source and allows large language models\n(LLMs) to answer queries over previously unseen document collections. However,\nit was demonstrated that traditional RAG applications perform poorly in\nanswering multi-hop questions, which require retrieving and reasoning over\nmultiple elements of supporting evidence. We introduce a new method called\nMulti-Meta-RAG, which uses database filtering with LLM-extracted metadata to\nimprove the RAG selection of the relevant documents from various sources,\nrelevant to the question. While database filtering is specific to a set of\nquestions from a particular domain and format, we found out that Multi-Meta-RAG\ngreatly improves the results on the MultiHop-RAG benchmark. The code is\navailable at https://github.com/mxpoliakov/Multi-Meta-RAG.\n', '  Retrieval-Augmented Generation (RAG) has recently demonstrated the\nperformance of Large Language Models (LLMs) in the knowledge-intensive tasks\nsuch as Question-Answering (QA). RAG expands the query context by incorporating\nexternal knowledge bases to enhance the response accuracy. However, it would be\ninefficient to access LLMs multiple times for each query and unreliable to\nretrieve all the relevant documents by a single query. We have found that even\nthough there is low relevance between some critical documents and query, it is\npossible to retrieve the remaining documents by combining parts of the\ndocuments with the query. To mine the relevance, a two-stage retrieval\nframework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is\nproposed to improve document retrieval recall and the accuracy of answers while\nmaintaining efficiency. Additionally, a compact classifier is applied to two\ndifferent selection strategies to determine the contribution of the retrieved\ndocuments to answering the query and retrieve the relatively relevant\ndocuments. Meanwhile, DR-RAG call the LLMs only once, which significantly\nimproves the efficiency of the experiment. The experimental results on\nmulti-hop QA datasets show that DR-RAG can significantly improve the accuracy\nof the answers and achieve new progress in QA systems.\n']",Retrieval-Augmented Generation (RAG) Methods,Information Retrieval and Knowledge Generation in Research Publications,Information Retrieval and Knowledge Systems,Information Retrieval and Knowledge Systems
13,376,13_multimodal_visual_captioning_mllm,"['multimodal', 'visual', 'captioning', 'mllm', 'modality', 'mllms', 'comprehension', 'textual', 'text', 'answering']","['visual', 'multimodal', 'reasoning', 'vision', 'image', 'question', 'answering', 'modal', 'instruction', 'questions']","[""  In production, multi-modal large language models (MLLMs) are expected to\nsupport multi-turn queries of interchanging image and text modalities. However,\nthe current MLLMs trained with visual-question-answering (VQA) datasets could\nsuffer from degradation, as VQA datasets lack the diversity and complexity of\nthe original text instruction datasets which the underlying language model had\nbeen trained with. To address this challenging degradation, we first collect a\nlightweight (6k entries) VQA preference dataset where answers were annotated by\nGemini for 5 quality metrics in a granular fashion, and investigate standard\nSupervised Fine-tuning, rejection sampling, Direct Preference Optimization\n(DPO), and SteerLM. Our findings indicate that the with DPO we are able to\nsurpass instruction-following capabilities of the language model, achieving a\n6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite\nsmall data scale. This enhancement in textual instruction proficiency\ncorrelates with boosted visual instruction performance (+4.9\\% on MM-Vet, +6\\%\non LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks\ncompared to previous RLHF approach. In conclusion, we propose a\ndistillation-based multi-modal alignment model with fine-grained annotations on\na small dataset that reconciles the textual and visual performance of MLLMs,\nrestoring and boosting language capability after visual instruction tuning.\n"", '  Connecting text and visual modalities plays an essential role in generative\nintelligence. For this reason, inspired by the success of large language\nmodels, significant research efforts are being devoted to the development of\nMultimodal Large Language Models (MLLMs). These models can seamlessly integrate\nvisual and textual modalities, while providing a dialogue-based interface and\ninstruction-following capabilities. In this paper, we provide a comprehensive\nreview of recent visual-based MLLMs, analyzing their architectural choices,\nmultimodal alignment strategies, and training techniques. We also conduct a\ndetailed analysis of these models across a wide range of tasks, including\nvisual grounding, image generation and editing, visual understanding, and\ndomain-specific applications. Additionally, we compile and describe training\ndatasets and evaluation benchmarks, conducting comparisons among existing\nmodels in terms of performance and computational requirements. Overall, this\nsurvey offers a comprehensive overview of the current state of the art, laying\nthe groundwork for future MLLMs.\n', '  Recent advancements in Chain-of-Thought (CoT) and related rationale-based\nworks have significantly improved the performance of Large Language Models\n(LLMs) in complex reasoning tasks. With the evolution of Multimodal Large\nLanguage Models (MLLMs), enhancing their capability to tackle complex\nmultimodal reasoning problems is a crucial frontier. However, incorporating\nmultimodal rationales in CoT has yet to be thoroughly investigated. We propose\nthe Image-of-Thought (IoT) prompting method, which helps MLLMs to extract\nvisual rationales step-by-step. Specifically, IoT prompting can automatically\ndesign critical visual information extraction operations based on the input\nimages and questions. Each step of visual information refinement identifies\nspecific visual rationales that support answers to complex visual reasoning\nquestions. Beyond the textual CoT, IoT simultaneously utilizes visual and\ntextual rationales to help MLLMs understand complex multimodal information. IoT\nprompting has improved zero-shot visual reasoning performance across various\nvisual understanding tasks in different MLLMs. Moreover, the step-by-step\nvisual feature explanations generated by IoT prompting elucidate the visual\nreasoning process, aiding in analyzing the cognitive processes of large\nmultimodal models\n']",Multimodal Large Language Models (MLLMs),Multimodal Large Language Models (MLLMs),Multimodal Learning and Vision-Language Models,Multimodal Learning
13,376,13_multimodal_visual_captioning_mllm,"['multimodal', 'visual', 'captioning', 'mllm', 'modality', 'mllms', 'comprehension', 'textual', 'text', 'answering']","['visual', 'multimodal', 'reasoning', 'vision', 'image', 'question', 'answering', 'modal', 'instruction', 'questions']","[""  In production, multi-modal large language models (MLLMs) are expected to\nsupport multi-turn queries of interchanging image and text modalities. However,\nthe current MLLMs trained with visual-question-answering (VQA) datasets could\nsuffer from degradation, as VQA datasets lack the diversity and complexity of\nthe original text instruction datasets which the underlying language model had\nbeen trained with. To address this challenging degradation, we first collect a\nlightweight (6k entries) VQA preference dataset where answers were annotated by\nGemini for 5 quality metrics in a granular fashion, and investigate standard\nSupervised Fine-tuning, rejection sampling, Direct Preference Optimization\n(DPO), and SteerLM. Our findings indicate that the with DPO we are able to\nsurpass instruction-following capabilities of the language model, achieving a\n6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite\nsmall data scale. This enhancement in textual instruction proficiency\ncorrelates with boosted visual instruction performance (+4.9\\% on MM-Vet, +6\\%\non LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks\ncompared to previous RLHF approach. In conclusion, we propose a\ndistillation-based multi-modal alignment model with fine-grained annotations on\na small dataset that reconciles the textual and visual performance of MLLMs,\nrestoring and boosting language capability after visual instruction tuning.\n"", '  Connecting text and visual modalities plays an essential role in generative\nintelligence. For this reason, inspired by the success of large language\nmodels, significant research efforts are being devoted to the development of\nMultimodal Large Language Models (MLLMs). These models can seamlessly integrate\nvisual and textual modalities, while providing a dialogue-based interface and\ninstruction-following capabilities. In this paper, we provide a comprehensive\nreview of recent visual-based MLLMs, analyzing their architectural choices,\nmultimodal alignment strategies, and training techniques. We also conduct a\ndetailed analysis of these models across a wide range of tasks, including\nvisual grounding, image generation and editing, visual understanding, and\ndomain-specific applications. Additionally, we compile and describe training\ndatasets and evaluation benchmarks, conducting comparisons among existing\nmodels in terms of performance and computational requirements. Overall, this\nsurvey offers a comprehensive overview of the current state of the art, laying\nthe groundwork for future MLLMs.\n', '  Recent advancements in Chain-of-Thought (CoT) and related rationale-based\nworks have significantly improved the performance of Large Language Models\n(LLMs) in complex reasoning tasks. With the evolution of Multimodal Large\nLanguage Models (MLLMs), enhancing their capability to tackle complex\nmultimodal reasoning problems is a crucial frontier. However, incorporating\nmultimodal rationales in CoT has yet to be thoroughly investigated. We propose\nthe Image-of-Thought (IoT) prompting method, which helps MLLMs to extract\nvisual rationales step-by-step. Specifically, IoT prompting can automatically\ndesign critical visual information extraction operations based on the input\nimages and questions. Each step of visual information refinement identifies\nspecific visual rationales that support answers to complex visual reasoning\nquestions. Beyond the textual CoT, IoT simultaneously utilizes visual and\ntextual rationales to help MLLMs understand complex multimodal information. IoT\nprompting has improved zero-shot visual reasoning performance across various\nvisual understanding tasks in different MLLMs. Moreover, the step-by-step\nvisual feature explanations generated by IoT prompting elucidate the visual\nreasoning process, aiding in analyzing the cognitive processes of large\nmultimodal models\n']",Multimodal Large Language Models (MLLMs),Multimodal Large Language Models (MLLMs),Multimodal Learning and Vision-Language Models,Multimodal Learning
14,366,14_forecasting_forecasts_forecast_lstm,"['forecasting', 'forecasts', 'forecast', 'lstm', 'prediction', 'predictive', 'seasonal', 'future', 'attention', 'models']","['series', 'forecasting', 'time', 'multivariate', 'temporal', 'dependencies', 'term', 'transformer', 'long', 'attention']","['  The rapid development of time series forecasting research has brought many\ndeep learning-based modules in this field. However, despite the increasing\namount of new forecasting architectures, it is still unclear if we have\nleveraged the full potential of these existing modules within a properly\ndesigned architecture. In this work, we propose a novel hierarchical neural\narchitecture search approach for time series forecasting tasks. With the design\nof a hierarchical search space, we incorporate many architecture types designed\nfor forecasting tasks and allow for the efficient combination of different\nforecasting architecture modules. Results on long-term-time-series-forecasting\ntasks show that our approach can search for lightweight high-performing\nforecasting architectures across different forecasting tasks.\n', '  Large language models (LLMs) are being applied to time series tasks,\nparticularly time series forecasting. However, are language models actually\nuseful for time series? After a series of ablation studies on three recent and\npopular LLM-based time series forecasting methods, we find that removing the\nLLM component or replacing it with a basic attention layer does not degrade the\nforecasting results -- in most cases the results even improved. We also find\nthat despite their significant computational cost, pretrained LLMs do no better\nthan models trained from scratch, do not represent the sequential dependencies\nin time series, and do not assist in few-shot settings. Additionally, we\nexplore time series encoders and reveal that patching and attention structures\nperform similarly to state-of-the-art LLM-based forecasters.\n', ""  Large language models (LLMs) have been applied in many fields and have\ndeveloped rapidly in recent years. As a classic machine learning task, time\nseries forecasting has recently been boosted by LLMs. Recent works treat large\nlanguage models as \\emph{zero-shot} time series reasoners without further\nfine-tuning, which achieves remarkable performance. However, there are some\nunexplored research problems when applying LLMs for time series forecasting\nunder the zero-shot setting. For instance, the LLMs' preferences for the input\ntime series are less understood. In this paper, by comparing LLMs with\ntraditional time series forecasting models, we observe many interesting\nproperties of LLMs in the context of time series forecasting. First, our study\nshows that LLMs perform well in predicting time series with clear patterns and\ntrends, but face challenges with datasets lacking periodicity. This observation\ncan be explained by the ability of LLMs to recognize the underlying period\nwithin datasets, which is supported by our experiments. In addition, the input\nstrategy is investigated, and it is found that incorporating external knowledge\nand adopting natural language paraphrases substantially improve the predictive\nperformance of LLMs for time series. Overall, our study contributes insight\ninto LLMs' advantages and limitations in time series forecasting under\ndifferent conditions.\n""]",Time Series Forecasting with Deep Learning,Time Series Analysis and Prediction,Predictive Modeling and Forecasting,Predictive Modeling and Forecasting
15,362,15_highway_driving_autonomous_traffic,"['highway', 'driving', 'autonomous', 'traffic', 'road', 'vehicles', 'lane', 'planning', 'vehicle', 'cars']","['driving', 'autonomous', 'traffic', 'vehicles', 'vehicle', 'lane', 'safety', 'road', 'scenarios', 'car']","['  Reinforcement learning has been demonstrated to outperform even the best\nhumans in complex domains like video games. However, running reinforcement\nlearning experiments on the required scale for autonomous driving is extremely\ndifficult. Building a large scale reinforcement learning system and\ndistributing it across many GPUs is challenging. Gathering experience during\ntraining on real world vehicles is prohibitive from a safety and scalability\nperspective. Therefore, an efficient and realistic driving simulator is\nrequired that uses a large amount of data from real-world driving. We bring\nthese capabilities together and conduct large-scale reinforcement learning\nexperiments for autonomous driving. We demonstrate that our policy performance\nimproves with increasing scale. Our best performing policy reduces the failure\nrate by 64% while improving the rate of driving progress by 25% compared to the\npolicies produced by state-of-the-art machine learning for autonomous driving.\n', '  Autonomous driving technology can improve traffic safety and reduce traffic\naccidents. In addition, it improves traffic flow, reduces congestion, saves\nenergy and increases travel efficiency. In the relatively mature automatic\ndriving technology, the automatic driving function is divided into several\nmodules: perception, decision-making, planning and control, and a reasonable\ndivision of labor can improve the stability of the system. Therefore,\nautonomous vehicles need to have the ability to predict the trajectory of\nsurrounding vehicles in order to make reasonable decision planning and safety\nmeasures to improve driving safety. By using deep learning method, a\nsafety-sensitive deep learning model based on short term memory (LSTM) network\nis proposed. This model can alleviate the shortcomings of current automatic\ndriving trajectory planning, and the output trajectory not only ensures high\naccuracy but also improves safety. The cell state simulation algorithm\nsimulates the trackability of the trajectory generated by this model. The\nresearch results show that compared with the traditional model-based method,\nthe trajectory prediction method based on LSTM network has obvious advantages\nin predicting the trajectory in the long time domain. The intention recognition\nmodule considering interactive information has higher prediction and accuracy,\nand the algorithm results show that the trajectory is very smooth based on the\npremise of safe prediction and efficient lane change. And autonomous vehicles\ncan efficiently and safely complete lane changes.\n', ""  Pedestrians' safety is a crucial factor in assessing autonomous driving\nscenarios. However, pedestrian safety evaluation is rarely considered by\nexisting autonomous driving simulation platforms. This paper proposes a\npedestrian safety evaluation method for autonomous driving, in which not only\nthe collision events but also the conflict events together with the\ncharacteristics of pedestrians are fully considered. Moreover, to apply the\npedestrian safety evaluation system, we construct a high-fidelity simulation\nframework embedded with pedestrian safety-critical characteristics. We\ndemonstrate our simulation framework and pedestrian safety evaluation with a\ncomparative experiment with two kinds of autonomous driving perception\nalgorithms -- single-vehicle perception and vehicle-to-infrastructure (V2I)\ncooperative perception. The results show that our framework can evaluate\ndifferent autonomous driving algorithms with detailed and quantitative\npedestrian safety indexes. To this end, the proposed simulation method and\nframework can be used to access different autonomous driving algorithms and\nevaluate pedestrians' safety performance in future autonomous driving\nsimulations, which can inspire more pedestrian-friendly autonomous driving\nalgorithms.\n""]",Autonomous Driving and Traffic Safety,Autonomous Systems and Safety Assessment,Autonomous Systems and Safety Assessment,Autonomous Systems and Safety Assessment
16,349,16_optimizers_sgd_optimizer_gradient,"['optimizers', 'sgd', 'optimizer', 'gradient', 'gradients', 'adaptive', 'optimization', 'stochastic', 'adam', 'hessian']","['convergence', 'gradient', 'stochastic', 'convex', 'descent', 'momentum', 'rate', 'nonconvex', 'optimization', 'order']","['  For nonconvex objective functions, including deep neural networks, stochastic\ngradient descent (SGD) with momentum has fast convergence and excellent\ngeneralizability, but a theoretical explanation for this is lacking. In\ncontrast to previous studies that defined the stochastic noise that occurs\nduring optimization as the variance of the stochastic gradient, we define it as\nthe gap between the search direction of the optimizer and the steepest descent\ndirection and show that its level dominates generalizability of the model. We\nalso show that the stochastic noise in SGD with momentum smoothes the objective\nfunction, the degree of which is determined by the learning rate, the batch\nsize, the momentum factor, the variance of the stochastic gradient, and the\nupper bound of the gradient norm. By numerically deriving the stochastic noise\nlevel in SGD and SGD with momentum, we provide theoretical findings that help\nexplain the training dynamics of SGD with momentum, which were not explained by\nprevious studies on convergence and stability. We also provide experimental\nresults supporting our assertion that model generalizability depends on the\nstochastic noise level.\n', '  It is known that the standard stochastic gradient descent (SGD) optimization\nmethod, as well as accelerated and adaptive SGD optimization methods such as\nthe Adam optimizer fail to converge if the learning rates do not converge to\nzero (as, for example, in the situation of constant learning rates). Numerical\nsimulations often use human-tuned deterministic learning rate schedules or\nsmall constant learning rates. The default learning rate schedules for SGD\noptimization methods in machine learning implementation frameworks such as\nTensorFlow and Pytorch are constant learning rates. In this work we propose and\nstudy a learning-rate-adaptive approach for SGD optimization methods in which\nthe learning rate is adjusted based on empirical estimates for the values of\nthe objective function of the considered optimization problem (the function\nthat one intends to minimize). In particular, we propose a\nlearning-rate-adaptive variant of the Adam optimizer and implement it in case\nof several neural network learning problems, particularly, in the context of\ndeep learning approximation methods for partial differential equations such as\ndeep Kolmogorov methods, physics-informed neural networks, and deep Ritz\nmethods. In each of the presented learning problems the proposed\nlearning-rate-adaptive variant of the Adam optimizer faster reduces the value\nof the objective function than the Adam optimizer with the default learning\nrate. For a simple class of quadratic minimization problems we also rigorously\nprove that a learning-rate-adaptive variant of the SGD optimization method\nconverges to the minimizer of the considered minimization problem. Our\nconvergence proof is based on an analysis of the laws of invariant measures of\nthe SGD method as well as on a more general convergence analysis for SGD with\nrandom but predictable learning rates which we develop in this work.\n', '  Adaptive gradient-descent optimizers are the standard choice for training\nneural network models. Despite their faster convergence than gradient-descent\nand remarkable performance in practice, the adaptive optimizers are not as well\nunderstood as vanilla gradient-descent. A reason is that the dynamic update of\nthe learning rate that helps in faster convergence of these methods also makes\ntheir analysis intricate. Particularly, the simple gradient-descent method\nconverges at a linear rate for a class of optimization problems, whereas the\npractically faster adaptive gradient methods lack such a theoretical guarantee.\nThe Polyak-{\\L}ojasiewicz (PL) inequality is the weakest known class, for which\nlinear convergence of gradient-descent and its momentum variants has been\nproved. Therefore, in this paper, we prove that AdaGrad and Adam, two\nwell-known adaptive gradient methods, converge linearly when the cost function\nis smooth and satisfies the PL inequality. Our theoretical framework follows a\nsimple and unified approach, applicable to both batch and stochastic gradients,\nwhich can potentially be utilized in analyzing linear convergence of other\nvariants of Adam.\n']",Stochastic Gradient Descent Optimizers,Optimization Methods for Distributed Deep Learning,Deep Learning Optimization and Training,Deep Learning Optimization and Security
17,335,17_neural_networks_regularization_neurons,"['neural', 'networks', 'regularization', 'neurons', 'relu', 'gradient', 'resnets', 'layers', 'weights', 'approximation']","['relu', 'activation', 'networks', 'neural', 'functions', 'layer', 'function', 'network', 'descent', 'linear']","['  We investigate the expressivity and learning dynamics of bias-free ReLU\nnetworks. We firstly show that two-layer bias-free ReLU networks have limited\nexpressivity: the only odd function two-layer bias-free ReLU networks can\nexpress is a linear one. We then show that, under symmetry conditions on the\ndata, these networks have the same learning dynamics as linear networks. This\nallows us to give closed-form time-course solutions to certain two-layer\nbias-free ReLU networks, which has not been done for nonlinear networks outside\nthe lazy learning regime. While deep bias-free ReLU networks are more\nexpressive than their two-layer counterparts, they still share a number of\nsimilarities with deep linear networks. These similarities enable us to\nleverage insights from linear networks, leading to a novel understanding of\nbias-free ReLU networks. Overall, our results show that some properties\nestablished for bias-free ReLU networks arise due to equivalence to linear\nnetworks, and suggest that including bias or considering asymmetric data are\navenues to engage with nonlinear behaviors.\n', '  Appropriate weight initialization settings, along with the ReLU activation\nfunction, have become cornerstones of modern deep learning, enabling the\ntraining and deployment of highly effective and efficient neural network models\nacross diverse areas of artificial intelligence. The problem of\n\\textquotedblleft dying ReLU,"" where ReLU neurons become inactive and yield\nzero output, presents a significant challenge in the training of deep neural\nnetworks with ReLU activation function. Theoretical research and various\nmethods have been introduced to address the problem. However, even with these\nmethods and research, training remains challenging for extremely deep and\nnarrow feedforward networks with ReLU activation function. In this paper, we\npropose a novel weight initialization method to address this issue. We\nestablish several properties of our initial weight matrix and demonstrate how\nthese properties enable the effective propagation of signal vectors. Through a\nseries of experiments and comparisons with existing methods, we demonstrate the\neffectiveness of the novel initialization method.\n', '  In this paper, we investigate the expressivity and approximation properties\nof deep neural networks employing the ReLU$^k$ activation function for $k \\geq\n2$. Although deep ReLU networks can approximate polynomials effectively, deep\nReLU$^k$ networks have the capability to represent higher-degree polynomials\nprecisely. Our initial contribution is a comprehensive, constructive proof for\npolynomial representation using deep ReLU$^k$ networks. This allows us to\nestablish an upper bound on both the size and count of network parameters.\nConsequently, we are able to demonstrate a suboptimal approximation rate for\nfunctions from Sobolev spaces as well as for analytic functions. Additionally,\nthrough an exploration of the representation power of deep ReLU$^k$ networks\nfor shallow networks, we reveal that deep ReLU$^k$ networks can approximate\nfunctions from a range of variation spaces, extending beyond those generated\nsolely by the ReLU$^k$ activation function. This finding demonstrates the\nadaptability of deep ReLU$^k$ networks in approximating functions within\nvarious variation spaces.\n']",ReLU Neural Networks Properties and Approximation,Deep Learning Theory and Foundations,Machine Learning and Artificial Intelligence,Machine Learning and Intelligent Systems
18,329,18_reward_reinforcement_rewards_supervised,"['reward', 'reinforcement', 'rewards', 'supervised', 'learning', 'trained', 'language', 'preference', 'ranking', 'sampling']","['preference', 'reward', 'alignment', 'preferences', 'feedback', 'human', 'reinforcement', 'policy', 'responses', 'optimization']","['  RLHF has emerged as a pivotal step in aligning language models with human\nobjectives and values. It typically involves learning a reward model from human\npreference data and then using reinforcement learning to update the generative\nmodel accordingly. Conversely, Direct Preference Optimization (DPO) directly\noptimizes the generative model with preference data, skipping reinforcement\nlearning. However, both RLHF and DPO assume uniform preferences, overlooking\nthe reality of diverse human annotators. This paper presents a new method to\nalign generative models with varied human preferences. We propose an\nExpectation-Maximization adaptation to DPO, generating a mixture of models\nbased on latent preference types of the annotators. We then introduce a min-max\nregret ensemble learning model to produce a single generative method to\nminimize worst-case regret among annotator subgroups with similar latent\nfactors. Our algorithms leverage the simplicity of DPO while accommodating\ndiverse preferences. Experimental results validate the effectiveness of our\napproach in producing equitable generative policies.\n', '  Aligning human preference and value is an important requirement for building\ncontemporary foundation models and embodied AI. However, popular approaches\nsuch as reinforcement learning with human feedback (RLHF) break down the task\ninto successive stages, such as supervised fine-tuning (SFT), reward modeling\n(RM), and reinforcement learning (RL), each performing one specific learning\ntask. Such a sequential approach results in serious issues such as significant\nunder-utilization of data and distribution mismatch between the learned reward\nmodel and generated policy, which eventually lead to poor alignment\nperformance. We develop a single stage approach named Alignment with Integrated\nHuman Feedback (AIHF), capable of integrating both human preference and\ndemonstration to train reward models and the policy. The proposed approach\nadmits a suite of efficient algorithms, which can easily reduce to, and\nleverage, popular alignment algorithms such as RLHF and Directly Policy\nOptimization (DPO), and only requires minor changes to the existing alignment\npipelines. We demonstrate the efficiency of the proposed solutions with\nextensive experiments involving alignment problems in LLMs and robotic control\nproblems in MuJoCo. We observe that the proposed solutions outperform the\nexisting alignment algorithms such as RLHF and DPO by large margins, especially\nwhen the amount of high-quality preference data is relatively limited.\n', '  The success of reinforcement learning from human feedback (RLHF) in language\nmodel alignment is strongly dependent on the quality of the underlying reward\nmodel. In this paper, we present a novel approach to improve reward model\nquality by generating synthetic preference data, thereby augmenting the\ntraining dataset with on-policy, high-quality preference pairs. Motivated by\nthe promising results of Best-of-N sampling strategies in language model\ntraining, we extend their application to reward model training. This results in\na self-training strategy to generate preference pairs by selecting the best and\nworst candidates in a pool of responses to a given query. Empirically, we find\nthat this approach improves the performance of any reward model, with an effect\ncomparable to the addition of a similar quantity of human preference data. This\nwork opens up new avenues of research for improving RLHF for language model\nalignment, by offering synthetic preference generation as a solution to reward\nmodeling challenges.\n']",Aligning Language Models with Human Preferences,Advances in Large Language Models,Large Language Models,Large Language Models
18,329,18_reward_reinforcement_rewards_supervised,"['reward', 'reinforcement', 'rewards', 'supervised', 'learning', 'trained', 'language', 'preference', 'ranking', 'sampling']","['preference', 'reward', 'alignment', 'preferences', 'feedback', 'human', 'reinforcement', 'policy', 'responses', 'optimization']","['  RLHF has emerged as a pivotal step in aligning language models with human\nobjectives and values. It typically involves learning a reward model from human\npreference data and then using reinforcement learning to update the generative\nmodel accordingly. Conversely, Direct Preference Optimization (DPO) directly\noptimizes the generative model with preference data, skipping reinforcement\nlearning. However, both RLHF and DPO assume uniform preferences, overlooking\nthe reality of diverse human annotators. This paper presents a new method to\nalign generative models with varied human preferences. We propose an\nExpectation-Maximization adaptation to DPO, generating a mixture of models\nbased on latent preference types of the annotators. We then introduce a min-max\nregret ensemble learning model to produce a single generative method to\nminimize worst-case regret among annotator subgroups with similar latent\nfactors. Our algorithms leverage the simplicity of DPO while accommodating\ndiverse preferences. Experimental results validate the effectiveness of our\napproach in producing equitable generative policies.\n', '  Aligning human preference and value is an important requirement for building\ncontemporary foundation models and embodied AI. However, popular approaches\nsuch as reinforcement learning with human feedback (RLHF) break down the task\ninto successive stages, such as supervised fine-tuning (SFT), reward modeling\n(RM), and reinforcement learning (RL), each performing one specific learning\ntask. Such a sequential approach results in serious issues such as significant\nunder-utilization of data and distribution mismatch between the learned reward\nmodel and generated policy, which eventually lead to poor alignment\nperformance. We develop a single stage approach named Alignment with Integrated\nHuman Feedback (AIHF), capable of integrating both human preference and\ndemonstration to train reward models and the policy. The proposed approach\nadmits a suite of efficient algorithms, which can easily reduce to, and\nleverage, popular alignment algorithms such as RLHF and Directly Policy\nOptimization (DPO), and only requires minor changes to the existing alignment\npipelines. We demonstrate the efficiency of the proposed solutions with\nextensive experiments involving alignment problems in LLMs and robotic control\nproblems in MuJoCo. We observe that the proposed solutions outperform the\nexisting alignment algorithms such as RLHF and DPO by large margins, especially\nwhen the amount of high-quality preference data is relatively limited.\n', '  The success of reinforcement learning from human feedback (RLHF) in language\nmodel alignment is strongly dependent on the quality of the underlying reward\nmodel. In this paper, we present a novel approach to improve reward model\nquality by generating synthetic preference data, thereby augmenting the\ntraining dataset with on-policy, high-quality preference pairs. Motivated by\nthe promising results of Best-of-N sampling strategies in language model\ntraining, we extend their application to reward model training. This results in\na self-training strategy to generate preference pairs by selecting the best and\nworst candidates in a pool of responses to a given query. Empirically, we find\nthat this approach improves the performance of any reward model, with an effect\ncomparable to the addition of a similar quantity of human preference data. This\nwork opens up new avenues of research for improving RLHF for language model\nalignment, by offering synthetic preference generation as a solution to reward\nmodeling challenges.\n']",Aligning Language Models with Human Preferences,Advances in Large Language Models,Large Language Models,Large Language Models
18,329,18_reward_reinforcement_rewards_supervised,"['reward', 'reinforcement', 'rewards', 'supervised', 'learning', 'trained', 'language', 'preference', 'ranking', 'sampling']","['preference', 'reward', 'alignment', 'preferences', 'feedback', 'human', 'reinforcement', 'policy', 'responses', 'optimization']","['  RLHF has emerged as a pivotal step in aligning language models with human\nobjectives and values. It typically involves learning a reward model from human\npreference data and then using reinforcement learning to update the generative\nmodel accordingly. Conversely, Direct Preference Optimization (DPO) directly\noptimizes the generative model with preference data, skipping reinforcement\nlearning. However, both RLHF and DPO assume uniform preferences, overlooking\nthe reality of diverse human annotators. This paper presents a new method to\nalign generative models with varied human preferences. We propose an\nExpectation-Maximization adaptation to DPO, generating a mixture of models\nbased on latent preference types of the annotators. We then introduce a min-max\nregret ensemble learning model to produce a single generative method to\nminimize worst-case regret among annotator subgroups with similar latent\nfactors. Our algorithms leverage the simplicity of DPO while accommodating\ndiverse preferences. Experimental results validate the effectiveness of our\napproach in producing equitable generative policies.\n', '  Aligning human preference and value is an important requirement for building\ncontemporary foundation models and embodied AI. However, popular approaches\nsuch as reinforcement learning with human feedback (RLHF) break down the task\ninto successive stages, such as supervised fine-tuning (SFT), reward modeling\n(RM), and reinforcement learning (RL), each performing one specific learning\ntask. Such a sequential approach results in serious issues such as significant\nunder-utilization of data and distribution mismatch between the learned reward\nmodel and generated policy, which eventually lead to poor alignment\nperformance. We develop a single stage approach named Alignment with Integrated\nHuman Feedback (AIHF), capable of integrating both human preference and\ndemonstration to train reward models and the policy. The proposed approach\nadmits a suite of efficient algorithms, which can easily reduce to, and\nleverage, popular alignment algorithms such as RLHF and Directly Policy\nOptimization (DPO), and only requires minor changes to the existing alignment\npipelines. We demonstrate the efficiency of the proposed solutions with\nextensive experiments involving alignment problems in LLMs and robotic control\nproblems in MuJoCo. We observe that the proposed solutions outperform the\nexisting alignment algorithms such as RLHF and DPO by large margins, especially\nwhen the amount of high-quality preference data is relatively limited.\n', '  The success of reinforcement learning from human feedback (RLHF) in language\nmodel alignment is strongly dependent on the quality of the underlying reward\nmodel. In this paper, we present a novel approach to improve reward model\nquality by generating synthetic preference data, thereby augmenting the\ntraining dataset with on-policy, high-quality preference pairs. Motivated by\nthe promising results of Best-of-N sampling strategies in language model\ntraining, we extend their application to reward model training. This results in\na self-training strategy to generate preference pairs by selecting the best and\nworst candidates in a pool of responses to a given query. Empirically, we find\nthat this approach improves the performance of any reward model, with an effect\ncomparable to the addition of a similar quantity of human preference data. This\nwork opens up new avenues of research for improving RLHF for language model\nalignment, by offering synthetic preference generation as a solution to reward\nmodeling challenges.\n']",Aligning Language Models with Human Preferences,Advances in Large Language Models,Large Language Models,Large Language Models
18,329,18_reward_reinforcement_rewards_supervised,"['reward', 'reinforcement', 'rewards', 'supervised', 'learning', 'trained', 'language', 'preference', 'ranking', 'sampling']","['preference', 'reward', 'alignment', 'preferences', 'feedback', 'human', 'reinforcement', 'policy', 'responses', 'optimization']","['  RLHF has emerged as a pivotal step in aligning language models with human\nobjectives and values. It typically involves learning a reward model from human\npreference data and then using reinforcement learning to update the generative\nmodel accordingly. Conversely, Direct Preference Optimization (DPO) directly\noptimizes the generative model with preference data, skipping reinforcement\nlearning. However, both RLHF and DPO assume uniform preferences, overlooking\nthe reality of diverse human annotators. This paper presents a new method to\nalign generative models with varied human preferences. We propose an\nExpectation-Maximization adaptation to DPO, generating a mixture of models\nbased on latent preference types of the annotators. We then introduce a min-max\nregret ensemble learning model to produce a single generative method to\nminimize worst-case regret among annotator subgroups with similar latent\nfactors. Our algorithms leverage the simplicity of DPO while accommodating\ndiverse preferences. Experimental results validate the effectiveness of our\napproach in producing equitable generative policies.\n', '  Aligning human preference and value is an important requirement for building\ncontemporary foundation models and embodied AI. However, popular approaches\nsuch as reinforcement learning with human feedback (RLHF) break down the task\ninto successive stages, such as supervised fine-tuning (SFT), reward modeling\n(RM), and reinforcement learning (RL), each performing one specific learning\ntask. Such a sequential approach results in serious issues such as significant\nunder-utilization of data and distribution mismatch between the learned reward\nmodel and generated policy, which eventually lead to poor alignment\nperformance. We develop a single stage approach named Alignment with Integrated\nHuman Feedback (AIHF), capable of integrating both human preference and\ndemonstration to train reward models and the policy. The proposed approach\nadmits a suite of efficient algorithms, which can easily reduce to, and\nleverage, popular alignment algorithms such as RLHF and Directly Policy\nOptimization (DPO), and only requires minor changes to the existing alignment\npipelines. We demonstrate the efficiency of the proposed solutions with\nextensive experiments involving alignment problems in LLMs and robotic control\nproblems in MuJoCo. We observe that the proposed solutions outperform the\nexisting alignment algorithms such as RLHF and DPO by large margins, especially\nwhen the amount of high-quality preference data is relatively limited.\n', '  The success of reinforcement learning from human feedback (RLHF) in language\nmodel alignment is strongly dependent on the quality of the underlying reward\nmodel. In this paper, we present a novel approach to improve reward model\nquality by generating synthetic preference data, thereby augmenting the\ntraining dataset with on-policy, high-quality preference pairs. Motivated by\nthe promising results of Best-of-N sampling strategies in language model\ntraining, we extend their application to reward model training. This results in\na self-training strategy to generate preference pairs by selecting the best and\nworst candidates in a pool of responses to a given query. Empirically, we find\nthat this approach improves the performance of any reward model, with an effect\ncomparable to the addition of a similar quantity of human preference data. This\nwork opens up new avenues of research for improving RLHF for language model\nalignment, by offering synthetic preference generation as a solution to reward\nmodeling challenges.\n']",Aligning Language Models with Human Preferences,Advances in Large Language Models,Large Language Models,Large Language Models
18,329,18_reward_reinforcement_rewards_supervised,"['reward', 'reinforcement', 'rewards', 'supervised', 'learning', 'trained', 'language', 'preference', 'ranking', 'sampling']","['preference', 'reward', 'alignment', 'preferences', 'feedback', 'human', 'reinforcement', 'policy', 'responses', 'optimization']","['  RLHF has emerged as a pivotal step in aligning language models with human\nobjectives and values. It typically involves learning a reward model from human\npreference data and then using reinforcement learning to update the generative\nmodel accordingly. Conversely, Direct Preference Optimization (DPO) directly\noptimizes the generative model with preference data, skipping reinforcement\nlearning. However, both RLHF and DPO assume uniform preferences, overlooking\nthe reality of diverse human annotators. This paper presents a new method to\nalign generative models with varied human preferences. We propose an\nExpectation-Maximization adaptation to DPO, generating a mixture of models\nbased on latent preference types of the annotators. We then introduce a min-max\nregret ensemble learning model to produce a single generative method to\nminimize worst-case regret among annotator subgroups with similar latent\nfactors. Our algorithms leverage the simplicity of DPO while accommodating\ndiverse preferences. Experimental results validate the effectiveness of our\napproach in producing equitable generative policies.\n', '  Aligning human preference and value is an important requirement for building\ncontemporary foundation models and embodied AI. However, popular approaches\nsuch as reinforcement learning with human feedback (RLHF) break down the task\ninto successive stages, such as supervised fine-tuning (SFT), reward modeling\n(RM), and reinforcement learning (RL), each performing one specific learning\ntask. Such a sequential approach results in serious issues such as significant\nunder-utilization of data and distribution mismatch between the learned reward\nmodel and generated policy, which eventually lead to poor alignment\nperformance. We develop a single stage approach named Alignment with Integrated\nHuman Feedback (AIHF), capable of integrating both human preference and\ndemonstration to train reward models and the policy. The proposed approach\nadmits a suite of efficient algorithms, which can easily reduce to, and\nleverage, popular alignment algorithms such as RLHF and Directly Policy\nOptimization (DPO), and only requires minor changes to the existing alignment\npipelines. We demonstrate the efficiency of the proposed solutions with\nextensive experiments involving alignment problems in LLMs and robotic control\nproblems in MuJoCo. We observe that the proposed solutions outperform the\nexisting alignment algorithms such as RLHF and DPO by large margins, especially\nwhen the amount of high-quality preference data is relatively limited.\n', '  The success of reinforcement learning from human feedback (RLHF) in language\nmodel alignment is strongly dependent on the quality of the underlying reward\nmodel. In this paper, we present a novel approach to improve reward model\nquality by generating synthetic preference data, thereby augmenting the\ntraining dataset with on-policy, high-quality preference pairs. Motivated by\nthe promising results of Best-of-N sampling strategies in language model\ntraining, we extend their application to reward model training. This results in\na self-training strategy to generate preference pairs by selecting the best and\nworst candidates in a pool of responses to a given query. Empirically, we find\nthat this approach improves the performance of any reward model, with an effect\ncomparable to the addition of a similar quantity of human preference data. This\nwork opens up new avenues of research for improving RLHF for language model\nalignment, by offering synthetic preference generation as a solution to reward\nmodeling challenges.\n']",Aligning Language Models with Human Preferences,Advances in Large Language Models,Large Language Models,Large Language Models
18,329,18_reward_reinforcement_rewards_supervised,"['reward', 'reinforcement', 'rewards', 'supervised', 'learning', 'trained', 'language', 'preference', 'ranking', 'sampling']","['preference', 'reward', 'alignment', 'preferences', 'feedback', 'human', 'reinforcement', 'policy', 'responses', 'optimization']","['  RLHF has emerged as a pivotal step in aligning language models with human\nobjectives and values. It typically involves learning a reward model from human\npreference data and then using reinforcement learning to update the generative\nmodel accordingly. Conversely, Direct Preference Optimization (DPO) directly\noptimizes the generative model with preference data, skipping reinforcement\nlearning. However, both RLHF and DPO assume uniform preferences, overlooking\nthe reality of diverse human annotators. This paper presents a new method to\nalign generative models with varied human preferences. We propose an\nExpectation-Maximization adaptation to DPO, generating a mixture of models\nbased on latent preference types of the annotators. We then introduce a min-max\nregret ensemble learning model to produce a single generative method to\nminimize worst-case regret among annotator subgroups with similar latent\nfactors. Our algorithms leverage the simplicity of DPO while accommodating\ndiverse preferences. Experimental results validate the effectiveness of our\napproach in producing equitable generative policies.\n', '  Aligning human preference and value is an important requirement for building\ncontemporary foundation models and embodied AI. However, popular approaches\nsuch as reinforcement learning with human feedback (RLHF) break down the task\ninto successive stages, such as supervised fine-tuning (SFT), reward modeling\n(RM), and reinforcement learning (RL), each performing one specific learning\ntask. Such a sequential approach results in serious issues such as significant\nunder-utilization of data and distribution mismatch between the learned reward\nmodel and generated policy, which eventually lead to poor alignment\nperformance. We develop a single stage approach named Alignment with Integrated\nHuman Feedback (AIHF), capable of integrating both human preference and\ndemonstration to train reward models and the policy. The proposed approach\nadmits a suite of efficient algorithms, which can easily reduce to, and\nleverage, popular alignment algorithms such as RLHF and Directly Policy\nOptimization (DPO), and only requires minor changes to the existing alignment\npipelines. We demonstrate the efficiency of the proposed solutions with\nextensive experiments involving alignment problems in LLMs and robotic control\nproblems in MuJoCo. We observe that the proposed solutions outperform the\nexisting alignment algorithms such as RLHF and DPO by large margins, especially\nwhen the amount of high-quality preference data is relatively limited.\n', '  The success of reinforcement learning from human feedback (RLHF) in language\nmodel alignment is strongly dependent on the quality of the underlying reward\nmodel. In this paper, we present a novel approach to improve reward model\nquality by generating synthetic preference data, thereby augmenting the\ntraining dataset with on-policy, high-quality preference pairs. Motivated by\nthe promising results of Best-of-N sampling strategies in language model\ntraining, we extend their application to reward model training. This results in\na self-training strategy to generate preference pairs by selecting the best and\nworst candidates in a pool of responses to a given query. Empirically, we find\nthat this approach improves the performance of any reward model, with an effect\ncomparable to the addition of a similar quantity of human preference data. This\nwork opens up new avenues of research for improving RLHF for language model\nalignment, by offering synthetic preference generation as a solution to reward\nmodeling challenges.\n']",Aligning Language Models with Human Preferences,Advances in Large Language Models,Large Language Models,Large Language Models
19,308,19_transcription_voice_transcriptions_corpus,"['transcription', 'voice', 'transcriptions', 'corpus', 'wav2vec', 'speech', 'asr', 'transcribed', 'wav2vec2', 'phonetic']","['speech', 'recognition', 'automatic', 'languages', 'word', 'acoustic', 'phonetic', 'speaker', 'phoneme', 'spoken']","['  Advances in machine learning have made it possible to perform various text\nand speech processing tasks, such as automatic speech recognition (ASR), in an\nend-to-end (E2E) manner. E2E approaches utilizing pre-trained models are\ngaining attention for conserving training data and resources. However, most of\ntheir applications in ASR involve only one of either a pre-trained speech or a\nlanguage model. This paper proposes integrating a pre-trained speech\nrepresentation model and a large language model (LLM) for E2E ASR. The proposed\nmodel enables the optimization of the entire ASR process, including acoustic\nfeature extraction and acoustic and language modeling, by combining pre-trained\nmodels with a bridge network and also enables the application of remarkable\ndevelopments in LLM utilization, such as parameter-efficient domain adaptation\nand inference optimization. Experimental results demonstrate that the proposed\nmodel achieves a performance comparable to that of modern E2E ASR models by\nutilizing powerful pre-training models with the proposed integrated approach.\n', ""  In the realm of automatic speech recognition (ASR), robustness in noisy\nenvironments remains a significant challenge. Recent ASR models, such as\nWhisper, have shown promise, but their efficacy in noisy conditions can be\nfurther enhanced. This study is focused on recovering from packet loss to\nimprove the word error rate (WER) of ASR models. We propose using a front-end\nadaptation network connected to a frozen ASR model. The adaptation network is\ntrained to modify the corrupted input spectrum by minimizing the criteria of\nthe ASR model in addition to an enhancement loss function. Our experiments\ndemonstrate that the adaptation network, trained on Whisper's criteria, notably\nreduces word error rates across domains and languages in packet-loss scenarios.\nThis improvement is achieved with minimal affect to Whisper model's\nfoundational performance, underscoring our method's practicality and potential\nin enhancing ASR models in challenging acoustic environments.\n"", '  Recent advancements in supervised automatic speech recognition (ASR) have\nachieved remarkable performance, largely due to the growing availability of\nlarge transcribed speech corpora. However, most languages lack sufficient\npaired speech and text data to effectively train these systems. In this\narticle, we tackle the challenge of developing ASR systems without paired\nspeech and text corpora by proposing the removal of reliance on a phoneme\nlexicon. We explore a new research direction: word-level unsupervised ASR.\nUsing a curated speech corpus containing only high-frequency English words, our\nsystem achieves a word error rate of nearly 20% without parallel transcripts or\noracle word boundaries. Furthermore, we experimentally demonstrate that an\nunsupervised speech recognizer can emerge from joint speech-to-speech and\ntext-to-text masked token-infilling. This innovative model surpasses the\nperformance of previous unsupervised ASR models trained with direct\ndistribution matching.\n']",Automatic Speech Recognition (ASR) Systems,Speech Processing and Recognition Systems,Speech and Audio Processing,Speech and Audio Processing
20,299,20_bandit_bandits_optimal_regret,"['bandit', 'bandits', 'optimal', 'regret', 'optimality', 'reward', 'rewards', 'exploration', 'exploitation', 'guarantees']","['regret', 'bandit', 'bandits', 'arm', 'armed', 'arms', 'bounds', 'algorithm', 'contextual', 'reward']","['  Fast changing states or volatile environments pose a significant challenge to\nonline optimization, which needs to perform rapid adaptation under limited\nobservation. In this paper, we give query and regret optimal bandit algorithms\nunder the strict notion of strongly adaptive regret, which measures the maximum\nregret over any contiguous interval $I$. Due to its worst-case nature, there is\nan almost-linear $\\Omega(|I|^{1-\\epsilon})$ regret lower bound, when only one\nquery per round is allowed [Daniely el al, ICML 2015]. Surprisingly, with just\ntwo queries per round, we give Strongly Adaptive Bandit Learner (StABL) that\nachieves $\\tilde{O}(\\sqrt{n|I|})$ adaptive regret for multi-armed bandits with\n$n$ arms. The bound is tight and cannot be improved in general. Our algorithm\nleverages a multiplicative update scheme of varying stepsizes and a carefully\nchosen observation distribution to control the variance. Furthermore, we extend\nour results and provide optimal algorithms in the bandit convex optimization\nsetting. Finally, we empirically demonstrate the superior performance of our\nalgorithms under volatile environments and for downstream tasks, such as\nalgorithm selection for hyperparameter optimization.\n', '  We present substantial evidence demonstrating the benefits of integrating\nLarge Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.\nContextual bandits have been widely used in recommendation systems to generate\npersonalized suggestions based on user-specific contexts. We show that LLMs,\npre-trained on extensive corpora rich in human knowledge and preferences, can\nsimulate human behaviours well enough to jump-start contextual multi-armed\nbandits to reduce online learning regret. We propose an initialization\nalgorithm for contextual bandits by prompting LLMs to produce a pre-training\ndataset of approximate human preferences for the bandit. This significantly\nreduces online learning regret and data-gathering costs for training such\nmodels. Our approach is validated empirically through two sets of experiments\nwith different bandit setups: one which utilizes LLMs to serve as an oracle and\na real-world experiment utilizing data from a conjoint survey experiment.\n', ""  Contextual dueling bandit is used to model the bandit problems, where a\nlearner's goal is to find the best arm for a given context using observed noisy\npreference feedback over the selected arms for the past contexts. However,\nexisting algorithms assume the reward function is linear, which can be complex\nand non-linear in many real-life applications like online recommendations or\nranking web search results. To overcome this challenge, we use a neural network\nto estimate the reward function using preference feedback for the previously\nselected arms. We propose upper confidence bound- and Thompson sampling-based\nalgorithms with sub-linear regret guarantees that efficiently select arms in\neach round. We then extend our theoretical results to contextual bandit\nproblems with binary feedback, which is in itself a non-trivial contribution.\nExperimental results on the problem instances derived from synthetic datasets\ncorroborate our theoretical results.\n""]",Optimal Bandit Algorithms for Adaptive Regret Minimization,Optimization and Learning in Bandit and Game-Theoretic Settings,Decision Making and Optimization under Uncertainty,Decision Making and Optimization under Uncertainty
21,263,21_pdes_pde_learning_neural,"['pdes', 'pde', 'learning', 'neural', 'solvers', 'pinn', 'nonlinear', 'pinns', 'solver', 'networks']","['equations', 'physics', 'operator', 'differential', 'partial', 'equation', 'operators', 'boundary', 'neural', 'numerical']","['  Learning and solving governing equations of a physical system, represented by\npartial differential equations (PDEs), from data is a central challenge in a\nvariety of areas of science and engineering. Traditional numerical methods for\nsolving PDEs can be computationally expensive for complex systems and require\nthe complete PDEs of the physical system. On the other hand, current\ndata-driven machine learning methods require a large amount of data to learn a\nsurrogate model of the PDE solution operator, which could be impractical. Here,\nwe propose the first solution operator learning method that only requires one\nPDE solution, i.e., one-shot learning. By leveraging the principle of locality\nof PDEs, we consider small local domains instead of the entire computational\ndomain and define a local solution operator. The local solution operator is\nthen trained using a neural network, and utilized to predict the solution of a\nnew input function via mesh-based fixed-point iteration (FPI), meshfree\nlocal-solution-operator informed neural network (LOINN) or\nlocal-solution-operator informed neural network with correction (cLOINN). We\ntest our method on diverse PDEs, including linear or nonlinear PDEs, PDEs\ndefined on complex geometries, and PDE systems, demonstrating the effectiveness\nand generalization capabilities of our method across these varied scenarios.\n', '  Physics-informed neural networks (PINNs) have recently emerged as a promising\nway to compute the solutions of partial differential equations (PDEs) using\ndeep neural networks. However, despite their significant success in various\nfields, it remains unclear in many aspects how to effectively train PINNs if\nthe solutions of PDEs exhibit stiff behaviors or high frequencies. In this\npaper, we propose a new method for training PINNs using variable-scaling\ntechniques. This method is simple and it can be applied to a wide range of\nproblems including PDEs with rapidly-varying solutions. Throughout various\nnumerical experiments, we will demonstrate the effectiveness of the proposed\nmethod for these problems and confirm that it can significantly improve the\ntraining efficiency and performance of PINNs. Furthermore, based on the\nanalysis of the neural tangent kernel (NTK), we will provide theoretical\nevidence for this phenomenon and show that our methods can indeed improve the\nperformance of PINNs.\n', '  Physics-informed neural networks (PINNs) have attracted significant attention\nfor solving partial differential equations (PDEs) in recent years because they\nalleviate the curse of dimensionality that appears in traditional methods.\nHowever, the most disadvantage of PINNs is that one neural network corresponds\nto one PDE. In practice, we usually need to solve a class of PDEs, not just\none. With the explosive growth of deep learning, many useful techniques in\ngeneral deep learning tasks are also suitable for PINNs. Transfer learning\nmethods may reduce the cost for PINNs in solving a class of PDEs. In this\npaper, we proposed a transfer learning method of PINNs via keeping singular\nvectors and optimizing singular values (namely SVD-PINNs). Numerical\nexperiments on high dimensional PDEs (10-d linear parabolic equations and 10-d\nAllen-Cahn equations) show that SVD-PINNs work for solving a class of PDEs with\ndifferent but close right-hand-side functions.\n']",Physics-Informed Neural Networks for PDEs,Physics-Informed Machine Learning for Differential Equations and Fluid Dynamics,Machine Learning for Dynamical Systems and Differential Equations,Machine Learning for Dynamical Systems and Differential Equations
22,261,22_electroencephalogram_eeg_electroencephalography_bci,"['electroencephalogram', 'eeg', 'electroencephalography', 'bci', 'neural', 'eeg_glt', 'brain', 'bcis', 'fmri', 'electrode']","['sleep', 'brain', 'signals', 'subject', 'decoding', 'seizure', 'subjects', 'electroencephalogram', 'signal', 'activity']","['  Electroencephalogram (EEG) is a non-invasive technique to record\nbioelectrical signals. Integrating supervised deep learning techniques with EEG\nsignals has recently facilitated automatic analysis across diverse EEG-based\ntasks. However, the label issues of EEG signals have constrained the\ndevelopment of EEG-based deep models. Obtaining EEG annotations is difficult\nthat requires domain experts to guide collection and labeling, and the\nvariability of EEG signals among different subjects causes significant label\nshifts. To solve the above challenges, self-supervised learning (SSL) has been\nproposed to extract representations from unlabeled samples through\nwell-designed pretext tasks. This paper concentrates on integrating SSL\nframeworks with temporal EEG signals to achieve efficient representation and\nproposes a systematic review of the SSL for EEG signals. In this paper, 1) we\nintroduce the concept and theory of self-supervised learning and typical SSL\nframeworks. 2) We provide a comprehensive review of SSL for EEG analysis,\nincluding taxonomy, methodology, and technique details of the existing\nEEG-based SSL frameworks, and discuss the difference between these methods. 3)\nWe investigate the adaptation of the SSL approach to various downstream tasks,\nincluding the task description and related benchmark datasets. 4) Finally, we\ndiscuss the potential directions for future SSL-EEG research.\n', ""  Decoding linguistic information from non-invasive brain signals using EEG has\ngained increasing research attention due to its vast applicational potential.\nRecently, a number of works have adopted a generative-based framework to decode\nelectroencephalogram (EEG) signals into sentences by utilizing the power\ngenerative capacity of pretrained large language models (LLMs). However, this\napproach has several drawbacks that hinder the further development of\nlinguistic applications for brain-computer interfaces (BCIs). Specifically, the\nability of the EEG encoder to learn semantic information from EEG data remains\nquestionable, and the LLM decoder's tendency to generate sentences based on its\ntraining memory can be hard to avoid. These issues necessitate a novel approach\nfor converting EEG signals into sentences. In this paper, we propose a novel\ntwo-step pipeline that addresses these limitations and enhances the validity of\nlinguistic EEG decoding research. We first confirm that word-level semantic\ninformation can be learned from EEG data recorded during natural reading by\ntraining a Conformer encoder via a masked contrastive objective for word-level\nclassification. To achieve sentence decoding results, we employ a training-free\nretrieval method to retrieve sentences based on the predictions from the EEG\nencoder. Extensive experiments and ablation studies were conducted in this\npaper for a comprehensive evaluation of the proposed approach. Visualization of\nthe top prediction candidates reveals that our model effectively groups EEG\nsegments into semantic categories with similar meanings, thereby validating its\nability to learn patterns from unspoken EEG recordings. Despite the exploratory\nnature of this work, these results suggest that our method holds promise for\nproviding more reliable solutions for converting EEG signals into text.\n"", '  Electroencephalography (EEG) signals, known for convenient non-invasive\nacquisition but low signal-to-noise ratio, have recently gained substantial\nattention due to the potential to decode natural images. This paper presents a\nself-supervised framework to demonstrate the feasibility of learning image\nrepresentations from EEG signals, particularly for object recognition. The\nframework utilizes image and EEG encoders to extract features from paired image\nstimuli and EEG responses. Contrastive learning aligns these two modalities by\nconstraining their similarity. With the framework, we attain significantly\nabove-chance results on a comprehensive EEG-image dataset, achieving a top-1\naccuracy of 15.6% and a top-5 accuracy of 42.8% in challenging 200-way\nzero-shot tasks. Moreover, we perform extensive experiments to explore the\nbiological plausibility by resolving the temporal, spatial, spectral, and\nsemantic aspects of EEG signals. Besides, we introduce attention modules to\ncapture spatial correlations, providing implicit evidence of the brain activity\nperceived from EEG data. These findings yield valuable insights for neural\ndecoding and brain-computer interfaces in real-world scenarios. The code will\nbe released on https://github.com/eeyhsong/NICE-EEG.\n']",EEG Signal Analysis and Decoding Techniques,Signal Processing and Analysis in Biomedical and Geophysical Applications,Signal Processing and Analysis in Complex Environments,Signal Processing and Analysis in Complex Environments
23,260,23_forgetting_continual_learning_retention,"['forgetting', 'continual', 'learning', 'retention', 'learned', 'memory', 'forget', 'learn', 'continually', 'retaining']","['continual', 'forgetting', 'catastrophic', 'incremental', 'old', 'replay', 'classes', 'class', 'memory', 'learning']","['  One of the objectives of continual learning is to prevent catastrophic\nforgetting in learning multiple tasks sequentially, and the existing solutions\nhave been driven by the conceptualization of the plasticity-stability dilemma.\nHowever, the convergence of continual learning for each sequential task is less\nstudied so far. In this paper, we provide a convergence analysis of\nmemory-based continual learning with stochastic gradient descent and empirical\nevidence that training current tasks causes the cumulative degradation of\nprevious tasks. We propose an adaptive method for nonconvex continual learning\n(NCCL), which adjusts step sizes of both previous and current tasks with the\ngradients. The proposed method can achieve the same convergence rate as the SGD\nmethod when the catastrophic forgetting term which we define in the paper is\nsuppressed at each iteration. Further, we demonstrate that the proposed\nalgorithm improves the performance of continual learning over existing methods\nfor several image classification tasks.\n', ""  Continual learning (CL) aims to incrementally learn different tasks (such as\nclassification) in a non-stationary data stream without forgetting old ones.\nMost CL works focus on tackling catastrophic forgetting under a\nlearning-from-scratch paradigm. However, with the increasing prominence of\nfoundation models, pre-trained models equipped with informative representations\nhave become available for various downstream requirements. Several CL methods\nbased on pre-trained models have been explored, either utilizing pre-extracted\nfeatures directly (which makes bridging distribution gaps challenging) or\nincorporating adaptors (which may be subject to forgetting). In this paper, we\npropose a concise and effective approach for CL with pre-trained models. Given\nthat forgetting occurs during parameter updating, we contemplate an alternative\napproach that exploits training-free random projectors and class-prototype\naccumulation, which thus bypasses the issue. Specifically, we inject a frozen\nRandom Projection layer with nonlinear activation between the pre-trained\nmodel's feature representations and output head, which captures interactions\nbetween features with expanded dimensionality, providing enhanced linear\nseparability for class-prototype-based CL. We also demonstrate the importance\nof decorrelating the class-prototypes to reduce the distribution disparity when\nusing pre-trained representations. These techniques prove to be effective and\ncircumvent the problem of forgetting for both class- and domain-incremental\ncontinual learning. Compared to previous methods applied to pre-trained\nViT-B/16 models, we reduce final error rates by between 20% and 62% on seven\nclass-incremental benchmarks, despite not using any rehearsal memory. We\nconclude that the full potential of pre-trained models for simple, effective,\nand fast CL has not hitherto been fully tapped. Code is at\ngithub.com/RanPAC/RanPAC.\n"", '  A key challenge for machine intelligence is to learn new visual concepts\nwithout forgetting the previously acquired knowledge. Continual learning is\naimed towards addressing this challenge. However, there is a gap between\nexisting supervised continual learning and human-like intelligence, where human\nis able to learn from both labeled and unlabeled data. How unlabeled data\naffects learning and catastrophic forgetting in the continual learning process\nremains unknown. To explore these issues, we formulate a new semi-supervised\ncontinual learning method, which can be generically applied to existing\ncontinual learning models. Specifically, a novel gradient learner learns from\nlabeled data to predict gradients on unlabeled data. Hence, the unlabeled data\ncould fit into the supervised continual learning method. Different from\nconventional semi-supervised settings, we do not hypothesize that the\nunderlying classes, which are associated to the unlabeled data, are known to\nthe learning process. In other words, the unlabeled data could be very distinct\nfrom the labeled data. We evaluate the proposed method on mainstream continual\nlearning, adversarial continual learning, and semi-supervised learning tasks.\nThe proposed method achieves state-of-the-art performance on classification\naccuracy and backward transfer in the continual learning setting while\nachieving desired performance on classification accuracy in the semi-supervised\nlearning setting. This implies that the unlabeled images can enhance the\ngeneralizability of continual learning models on the predictive ability on\nunseen data and significantly alleviate catastrophic forgetting. The code is\navailable at \\url{https://github.com/luoyan407/grad_prediction.git}.\n']",Continual Learning and Catastrophic Forgetting,Continual Learning and Catastrophic Forgetting,Machine Learning Adaptation and Forgetting,Machine Learning Adaptation and Forgetting
24,239,24_privacy_private_privately_sgd,"['privacy', 'private', 'privately', 'sgd', 'differentially', 'public', 'gradients', 'leakage', 'protection', 'algorithms']","['privacy', 'private', 'differential', 'guarantees', 'utility', 'bounds', 'mechanism', 'noise', 'gradient', 'tight']","['  When analysing Differentially Private (DP) machine learning pipelines, the\npotential privacy cost of data-dependent pre-processing is frequently\noverlooked in privacy accounting. In this work, we propose a general framework\nto evaluate the additional privacy cost incurred by non-private data-dependent\npre-processing algorithms. Our framework establishes upper bounds on the\noverall privacy guarantees by utilising two new technical notions: a variant of\nDP termed Smooth DP and the bounded sensitivity of the pre-processing\nalgorithms. In addition to the generic framework, we provide explicit overall\nprivacy guarantees for multiple data-dependent pre-processing algorithms, such\nas data imputation, quantization, deduplication and PCA, when used in\ncombination with several DP algorithms. Notably, this framework is also simple\nto implement, allowing direct integration into existing DP pipelines.\n', '  Differentially private stochastic gradient descent (DP-SGD) is the standard\nalgorithm for training machine learning models under differential privacy (DP).\nThe major drawback of DP-SGD is the drop in utility which prior work has\ncomprehensively studied. However, in practice another major drawback that\nhinders the large-scale deployment is the significantly higher computational\ncost. We conduct a comprehensive empirical study to quantify the computational\ncost of training deep learning models under DP and benchmark methods that aim\nat reducing the cost. Among these are more efficient implementations of DP-SGD\nand training with lower precision. Finally, we study the scaling behaviour\nusing up to 80 GPUs.\n', ""  Machine learning (ML) models have been shown to leak private information from\ntheir training datasets. Differential Privacy (DP), typically implemented\nthrough the differential private stochastic gradient descent algorithm\n(DP-SGD), has become the standard solution to bound leakage from the models.\nDespite recent improvements, DP-SGD-based approaches for private learning still\nusually struggle in the high privacy ($\\varepsilon\\le1)$ and low data regimes,\nand when the private training datasets are imbalanced. To overcome these\nlimitations, we propose Differentially Private Prototype Learning (DPPL) as a\nnew paradigm for private transfer learning. DPPL leverages publicly pre-trained\nencoders to extract features from private data and generates DP prototypes that\nrepresent each private class in the embedding space and can be publicly\nreleased for inference. Since our DP prototypes can be obtained from only a few\nprivate training data points and without iterative noise addition, they offer\nhigh-utility predictions and strong privacy guarantees even under the notion of\npure DP. We additionally show that privacy-utility trade-offs can be further\nimproved when leveraging the public data beyond pre-training of the encoder: in\nparticular, we can privately sample our DP prototypes from the publicly\navailable data points used to train the encoder. Our experimental evaluation\nwith four state-of-the-art encoders, four vision datasets, and under different\ndata and imbalancedness regimes demonstrate DPPL's high performance under\nstrong privacy guarantees in challenging private learning setups.\n""]",Differentially Private Machine Learning,Differential Privacy in Machine Learning,Machine Learning and Data Privacy,Machine Learning and Data Privacy
25,234,25_3d_scenes_scene_depth,"['3d', 'scenes', 'scene', 'depth', 'viewpoints', 'generative', 'rendering', 'view', '3dgs', 'viewpoint']","['scene', 'view', 'scenes', 'depth', 'rendering', 'radiance', 'reconstruction', 'views', 'geometry', 'camera']","['  Editing a local region or a specific object in a 3D scene represented by a\nNeRF or consistently blending a new realistic object into the scene is\nchallenging, mainly due to the implicit nature of the scene representation. We\npresent Blended-NeRF, a robust and flexible framework for editing a specific\nregion of interest in an existing NeRF scene, based on text prompts, along with\na 3D ROI box. Our method leverages a pretrained language-image model to steer\nthe synthesis towards a user-provided text prompt, along with a 3D MLP model\ninitialized on an existing NeRF scene to generate the object and blend it into\na specified region in the original scene. We allow local editing by localizing\na 3D ROI box in the input scene, and blend the content synthesized inside the\nROI with the existing scene using a novel volumetric blending technique. To\nobtain natural looking and view-consistent results, we leverage existing and\nnew geometric priors and 3D augmentations for improving the visual fidelity of\nthe final result. We test our framework both qualitatively and quantitatively\non a variety of real 3D scenes and text prompts, demonstrating realistic\nmulti-view consistent results with much flexibility and diversity compared to\nthe baselines. Finally, we show the applicability of our framework for several\n3D editing applications, including adding new objects to a scene,\nremoving/replacing/altering existing objects, and texture conversion.\n', '  Text-driven 3D scene generation techniques have made rapid progress in recent\nyears. Their success is mainly attributed to using existing generative models\nto iteratively perform image warping and inpainting to generate 3D scenes.\nHowever, these methods heavily rely on the outputs of existing models, leading\nto error accumulation in geometry and appearance that prevent the models from\nbeing used in various scenarios (e.g., outdoor and unreal scenarios). To\naddress this limitation, we generatively refine the newly generated local views\nby querying and aggregating global 3D information, and then progressively\ngenerate the 3D scene. Specifically, we employ a tri-plane features-based NeRF\nas a unified representation of the 3D scene to constrain global 3D consistency,\nand propose a generative refinement network to synthesize new contents with\nhigher quality by exploiting the natural image prior from 2D diffusion model as\nwell as the global 3D information of the current scene. Our extensive\nexperiments demonstrate that, in comparison to previous methods, our approach\nsupports wide variety of scene generation and arbitrary camera trajectories\nwith improved visual quality and 3D consistency.\n', '  Generating 3D scenes is a challenging open problem, which requires\nsynthesizing plausible content that is fully consistent in 3D space. While\nrecent methods such as neural radiance fields excel at view synthesis and 3D\nreconstruction, they cannot synthesize plausible details in unobserved regions\nsince they lack a generative capability. Conversely, existing generative\nmethods are typically not capable of reconstructing detailed, large-scale\nscenes in the wild, as they use limited-capacity 3D scene representations,\nrequire aligned camera poses, or rely on additional regularizers. In this work,\nwe introduce the first diffusion model able to perform fast, detailed\nreconstruction and generation of real-world 3D scenes. To achieve this, we make\nthree contributions. First, we introduce a new neural scene representation,\nIB-planes, that can efficiently and accurately represent large 3D scenes,\ndynamically allocating more capacity as needed to capture details visible in\neach image. Second, we propose a denoising-diffusion framework to learn a prior\nover this novel 3D scene representation, using only 2D images without the need\nfor any additional supervision signal such as masks or depths. This supports 3D\nreconstruction and generation in a unified architecture. Third, we develop a\nprincipled approach to avoid trivial 3D solutions when integrating the\nimage-based rendering with the diffusion model, by dropping out representations\nof some images. We evaluate the model on several challenging datasets of real\nand synthetic images, and demonstrate superior results on generation, novel\nview synthesis and 3D reconstruction.\n']",3D Scene Generation and Editing,Computer-Generated 3D Content Creation,Artificial Intelligence for Creative Content Generation,Artificial Intelligence for Creative Content Generation
26,231,26_softmax_imagenet_classification_detection,"['softmax', 'imagenet', 'classification', 'detection', 'accuracy', 'datasets', 'deep', 'ood', 'robust', 'neural']","['uncertainty', 'calibration', 'distribution', 'shift', 'confidence', 'predictions', 'ensemble', 'ensembles', 'uncertainties', 'samples']","['  Out-of-distribution (OOD) learning often relies heavily on statistical\napproaches or predefined assumptions about OOD data distributions, hindering\ntheir efficacy in addressing multifaceted challenges of OOD generalization and\nOOD detection in real-world deployment environments. This paper presents a\nnovel framework for OOD learning with human feedback, which can provide\ninvaluable insights into the nature of OOD shifts and guide effective model\nadaptation. Our framework capitalizes on the freely available unlabeled data in\nthe wild that captures the environmental test-time OOD distributions under both\ncovariate and semantic shifts. To harness such data, our key idea is to\nselectively provide human feedback and label a small number of informative\nsamples from the wild data distribution, which are then used to train a\nmulti-class classifier and an OOD detector. By exploiting human feedback, we\nenhance the robustness and reliability of machine learning models, equipping\nthem with the capability to handle OOD scenarios with greater precision. We\nprovide theoretical insights on the generalization error bounds to justify our\nalgorithm. Extensive experiments show the superiority of our method,\noutperforming the current state-of-the-art by a significant margin.\n', '  Deep learning models excel when the data distribution during training aligns\nwith testing data. Yet, their performance diminishes when faced with\nout-of-distribution (OOD) samples, leading to great interest in the field of\nOOD detection. Current approaches typically assume that OOD samples originate\nfrom an unconcentrated distribution complementary to the training distribution.\nWhile this assumption is appropriate in the traditional unsupervised OOD\n(U-OOD) setting, it proves inadequate when considering the place of deployment\nof the underlying deep learning model. To better reflect this real-world\nscenario, we introduce the novel setting of continual U-OOD detection. To\ntackle this new setting, we propose a method that starts from a U-OOD detector,\nwhich is agnostic to the OOD distribution, and slowly updates during deployment\nto account for the actual OOD distribution. Our method uses a new U-OOD scoring\nfunction that combines the Mahalanobis distance with a nearest-neighbor\napproach. Furthermore, we design a confidence-scaled few-shot OOD detector that\noutperforms previous methods. We show our method greatly improves upon strong\nbaselines from related fields.\n', '  With the rapid advancement in the performance of deep neural networks (DNNs),\nthere has been significant interest in deploying and incorporating artificial\nintelligence (AI) systems into real-world scenarios. However, many DNNs lack\nthe ability to represent uncertainty, often exhibiting excessive confidence\neven when making incorrect predictions. To ensure the reliability of AI\nsystems, particularly in safety-critical cases, DNNs should transparently\nreflect the uncertainty in their predictions. In this paper, we investigate\nrobust post-hoc uncertainty calibration methods for DNNs within the context of\nmulti-class classification tasks. While previous studies have made notable\nprogress, they still face challenges in achieving robust calibration,\nparticularly in scenarios involving out-of-distribution (OOD). We identify that\nprevious methods lack adaptability to individual input data and struggle to\naccurately estimate uncertainty when processing inputs drawn from the wild\ndataset. To address this issue, we introduce a novel instance-wise calibration\nmethod based on an energy model. Our method incorporates energy scores instead\nof softmax confidence scores, allowing for adaptive consideration of DNN\nuncertainty for each prediction within a logit space. In experiments, we show\nthat the proposed method consistently maintains robust performance across the\nspectrum, spanning from in-distribution to OOD scenarios, when compared to\nother state-of-the-art methods.\n']",Out-of-Distribution Detection and Robust Deep Learning,Deep Learning for Out-of-Distribution Detection and Robustness,Deep Learning Optimization and Security,Deep Learning Methodologies
27,228,27_fairness_unfairness_discrimination_discriminatory,"['fairness', 'unfairness', 'discrimination', 'discriminatory', 'bias', 'unfair', 'biases', 'equalized', 'classifiers', 'classifier']","['fairness', 'fair', 'demographic', 'sensitive', 'discrimination', 'groups', 'bias', 'attributes', 'unfairness', 'group']","['  While significant advancements have been made in the field of fair machine\nlearning, the majority of studies focus on scenarios where the decision model\noperates on a static population. In this paper, we study fairness in dynamic\nsystems where sequential decisions are made. Each decision may shift the\nunderlying distribution of features or user behavior. We model the dynamic\nsystem through a Markov Decision Process (MDP). By acknowledging that\ntraditional fairness notions and long-term fairness are distinct requirements\nthat may not necessarily align with one another, we propose an algorithmic\nframework to integrate various fairness considerations with reinforcement\nlearning using both pre-processing and in-processing approaches. Three case\nstudies show that our method can strike a balance between traditional fairness\nnotions, long-term fairness, and utility.\n', '  Training supervised machine learning systems with a fairness loss can improve\nprediction fairness across different demographic groups. However, doing so\nrequires demographic annotations for training data, without which we cannot\nproduce debiased classifiers for most tasks. Drawing inspiration from transfer\nlearning methods, we investigate whether we can utilize demographic data from a\nrelated task to improve the fairness of a target task. We adapt a single-task\nfairness loss to a multi-task setting to exploit demographic labels from a\nrelated task in debiasing a target task and demonstrate that demographic\nfairness objectives transfer fairness within a multi-task framework.\nAdditionally, we show that this approach enables intersectional fairness by\ntransferring between two datasets with different single-axis demographics. We\nexplore different data domains to show how our loss can improve fairness\ndomains and tasks.\n', '  With the introduction of machine learning in high-stakes decision making,\nensuring algorithmic fairness has become an increasingly important problem to\nsolve. In response to this, many mathematical definitions of fairness have been\nproposed, and a variety of optimisation techniques have been developed, all\ndesigned to maximise a defined notion of fairness. However, fair solutions are\nreliant on the quality of the training data, and can be highly sensitive to\nnoise. Recent studies have shown that robustness (the ability for a model to\nperform well on unseen data) plays a significant role in the type of strategy\nthat should be used when approaching a new problem and, hence, measuring the\nrobustness of these strategies has become a fundamental problem. In this work,\nwe therefore propose a new criterion to measure the robustness of various\nfairness optimisation strategies - the robustness ratio. We conduct multiple\nextensive experiments on five bench mark fairness data sets using three of the\nmost popular fairness strategies with respect to four of the most popular\ndefinitions of fairness. Our experiments empirically show that fairness methods\nthat rely on threshold optimisation are very sensitive to noise in all the\nevaluated data sets, despite mostly outperforming other methods. This is in\ncontrast to the other two methods, which are less fair for low noise scenarios\nbut fairer for high noise ones. To the best of our knowledge, we are the first\nto quantitatively evaluate the robustness of fairness optimisation strategies.\nThis can potentially can serve as a guideline in choosing the most suitable\nfairness strategy for various data sets.\n']",Fairness in Machine Learning Systems,Fairness in Artificial Intelligence and Machine Learning,Fairness and Bias in Artificial Intelligence,Fairness and Bias in Artificial Intelligence
28,227,28_anomaly_anomalies_anomalous_detections,"['anomaly', 'anomalies', 'anomalous', 'detections', 'detection', 'detecting', 'detect', 'autoencoders', 'supervised', 'unusual']","['anomaly', 'anomalies', 'detection', 'normal', 'anomalous', 'series', 'unsupervised', 'autoencoder', 'time', 'supervised']","['  With the rapid development of the Internet, various types of anomaly traffic\nare threatening network security. We consider the problem of anomaly network\ntraffic detection and propose a three-stage anomaly detection framework using\nonly normal traffic. Our framework can generate pseudo anomaly samples without\nprior knowledge of anomalies to achieve the detection of anomaly data. Firstly,\nwe employ a reconstruction method to learn the deep representation of normal\nsamples. Secondly, these representations are normalized to a standard normal\ndistribution using a bidirectional flow module. To simulate anomaly samples, we\nadd noises to the normalized representations which are then passed through the\ngeneration direction of the bidirectional flow module. Finally, a simple\nclassifier is trained to differentiate the normal samples and pseudo anomaly\nsamples in the latent space. During inference, our framework requires only two\nmodules to detect anomalous samples, leading to a considerable reduction in\nmodel size. According to the experiments, our method achieves the state\nof-the-art results on the common benchmarking datasets of anomaly network\ntraffic detection. The code is given in the\nhttps://github.com/ZxuanDang/ATD-via-Flows.git\n', '  Unsupervised Anomaly Detection (UAD) plays a crucial role in identifying\nabnormal patterns within data without labeled examples, holding significant\npractical implications across various domains. Although the individual\ncontributions of representation learning and clustering to anomaly detection\nare well-established, their interdependencies remain under-explored due to the\nabsence of a unified theoretical framework. Consequently, their collective\npotential to enhance anomaly detection performance remains largely untapped. To\nbridge this gap, in this paper, we propose a novel probabilistic mixture model\nfor anomaly detection to establish a theoretical connection among\nrepresentation learning, clustering, and anomaly detection. By maximizing a\nnovel anomaly-aware data likelihood, representation learning and clustering can\neffectively reduce the adverse impact of anomalous data and collaboratively\nbenefit anomaly detection. Meanwhile, a theoretically substantiated anomaly\nscore is naturally derived from this framework. Lastly, drawing inspiration\nfrom gravitational analysis in physics, we have devised an improved anomaly\nscore that more effectively harnesses the combined power of representation\nlearning and clustering. Extensive experiments, involving 17 baseline methods\nacross 30 diverse datasets, validate the effectiveness and generalization\ncapability of the proposed method, surpassing state-of-the-art methods.\n', '  Transformer, as one of the most advanced neural network models in Natural\nLanguage Processing (NLP), exhibits diverse applications in the field of\nanomaly detection. To inspire research on Transformer-based anomaly detection,\nthis review offers a fresh perspective on the concept of anomaly detection. We\nexplore the current challenges of anomaly detection and provide detailed\ninsights into the operating principles of Transformer and its variants in\nanomaly detection tasks. Additionally, we delineate various application\nscenarios for Transformer-based anomaly detection models and discuss the\ndatasets and evaluation metrics employed. Furthermore, this review highlights\nthe key challenges in Transformer-based anomaly detection research and conducts\na comprehensive analysis of future research trends in this domain. The review\nincludes an extensive compilation of over 100 core references related to\nTransformer-based anomaly detection. To the best of our knowledge, this is the\nfirst comprehensive review that focuses on the research related to Transformer\nin the context of anomaly detection. We hope that this paper can provide\ndetailed technical information to researchers interested in Transformer-based\nanomaly detection tasks.\n']",Anomaly Detection Methods and Techniques,Anomaly and Outlier Detection Methods,Data Analysis and Pattern Discovery,Data Analysis and Pattern Discovery
29,214,29_musicgen_music_midi_musicrl,"['musicgen', 'music', 'midi', 'musicrl', 'songs', 'musical', 'genres', 'musicians', 'melodies', 'composers']","['music', 'musical', 'audio', 'melody', 'song', 'pitch', 'symbolic', 'lyrics', 'sound', 'songs']","['  Recent advances in text-to-music generation models have opened new avenues in\nmusical creativity. However, music generation usually involves iterative\nrefinements, and how to edit the generated music remains a significant\nchallenge. This paper introduces a novel approach to the editing of music\ngenerated by such models, enabling the modification of specific attributes,\nsuch as genre, mood and instrument, while maintaining other aspects unchanged.\nOur method transforms text editing to \\textit{latent space manipulation} while\nadding an extra constraint to enforce consistency. It seamlessly integrates\nwith existing pretrained text-to-music diffusion models without requiring\nadditional training. Experimental results demonstrate superior performance over\nboth zero-shot and certain supervised baselines in style and timbre transfer\nevaluations. Additionally, we showcase the practical applicability of our\napproach in real-world music editing scenarios.\n', ""  Symbolic Music, akin to language, can be encoded in discrete symbols. Recent\nresearch has extended the application of large language models (LLMs) such as\nGPT-4 and Llama2 to the symbolic music domain including understanding and\ngeneration. Yet scant research explores the details of how these LLMs perform\non advanced music understanding and conditioned generation, especially from the\nmulti-step reasoning perspective, which is a critical aspect in the\nconditioned, editable, and interactive human-computer co-creation process. This\nstudy conducts a thorough investigation of LLMs' capability and limitations in\nsymbolic music processing. We identify that current LLMs exhibit poor\nperformance in song-level multi-step music reasoning, and typically fail to\nleverage learned music knowledge when addressing complex musical tasks. An\nanalysis of LLMs' responses highlights distinctly their pros and cons. Our\nfindings suggest achieving advanced musical capability is not intrinsically\nobtained by LLMs, and future research should focus more on bridging the gap\nbetween music knowledge and reasoning, to improve the co-creation experience\nfor musicians.\n"", '  Controllable music generation plays a vital role in human-AI music\nco-creation. While Large Language Models (LLMs) have shown promise in\ngenerating high-quality music, their focus on autoregressive generation limits\ntheir utility in music editing tasks. To address this gap, we propose a novel\napproach leveraging a parameter-efficient heterogeneous adapter combined with a\nmasking training scheme. This approach enables autoregressive language models\nto seamlessly address music inpainting tasks. Additionally, our method\nintegrates frame-level content-based controls, facilitating track-conditioned\nmusic refinement and score-conditioned music arrangement. We apply this method\nto fine-tune MusicGen, a leading autoregressive music generation model. Our\nexperiments demonstrate promising results across multiple music editing tasks,\noffering more flexible controls for future AI-driven music editing tools. The\nsource codes and a demo page showcasing our work are available at\nhttps://kikyo-16.github.io/AIR.\n']",Music Generation and Editing with AI,Artificial Intelligence in Creative Industries,Artificial Intelligence in Creative Industries,Artificial Intelligence in Creative Industries
30,213,30_rnns_lstms_memory_softmax,"['rnns', 'lstms', 'memory', 'softmax', 'attention', 'rnn', 'neural', 'recurrent', 'transformers', 'models']","['transformers', 'attention', 'transformer', 'sequence', 'linear', 'recurrent', 'softmax', 'sequences', 'layer', 'modeling']","[""  Selective state-space models (SSMs) like Mamba overcome some of the\nshortcomings of Transformers, such as quadratic computational complexity with\nsequence length and large inference-time memory requirements from the key-value\ncache. Moreover, recent studies have shown that SSMs can match or exceed the\nlanguage modeling capabilities of Transformers, making them an attractive\nalternative. In a controlled setting (e.g., same data), however, studies so far\nhave only presented small scale experiments comparing SSMs to Transformers. To\nunderstand the strengths and weaknesses of these architectures at larger\nscales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and\nTransformer models trained on the same datasets of up to 3.5T tokens. We also\ncompare these models to a hybrid architecture consisting of 43% Mamba-2, 7%\nattention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks,\nwe answer the question of whether Mamba models can match Transformers at larger\ntraining budgets. Our results show that while pure SSMs match or exceed\nTransformers on many tasks, they lag behind Transformers on tasks which require\nstrong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook)\nor long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid\nexceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points\non average) and is predicted to be up to 8x faster when generating tokens at\ninference time. To validate long-context capabilities, we provide additional\nexperiments evaluating variants of the Mamba-2-Hybrid and Transformer extended\nto support 16K, 32K, and 128K sequences. On an additional 23 long-context\ntasks, the hybrid model continues to closely match or exceed the Transformer on\naverage. To enable further study, we release the checkpoints as well as the\ncode used to train our models as part of NVIDIA's Megatron-LM project.\n"", '  Deep neural networks based on state space models (SSMs) are attracting much\nattention in sequence modeling since their computational cost is significantly\nsmaller than that of Transformers. While the capabilities of SSMs have been\nprimarily investigated through experimental comparisons, theoretical\nunderstanding of SSMs is still limited. In particular, there is a lack of\nstatistical and quantitative evaluation of whether SSM can replace\nTransformers. In this paper, we theoretically explore in which tasks SSMs can\nbe alternatives of Transformers from the perspective of estimating\nsequence-to-sequence functions. We consider the setting where the target\nfunction has direction-dependent smoothness and prove that SSMs can estimate\nsuch functions with the same convergence rate as Transformers. Additionally, we\nprove that SSMs can estimate the target function, even if the smoothness\nchanges depending on the input sequence, as well as Transformers. Our results\nshow the possibility that SSMs can replace Transformers when estimating the\nfunctions in certain classes that appear in practice.\n', ""  Foundation models, now powering most of the exciting applications in deep\nlearning, are almost universally based on the Transformer architecture and its\ncore attention module. Many subquadratic-time architectures such as linear\nattention, gated convolution and recurrent models, and structured state space\nmodels (SSMs) have been developed to address Transformers' computational\ninefficiency on long sequences, but they have not performed as well as\nattention on important modalities such as language. We identify that a key\nweakness of such models is their inability to perform content-based reasoning,\nand make several improvements. First, simply letting the SSM parameters be\nfunctions of the input addresses their weakness with discrete modalities,\nallowing the model to selectively propagate or forget information along the\nsequence length dimension depending on the current token. Second, even though\nthis change prevents the use of efficient convolutions, we design a\nhardware-aware parallel algorithm in recurrent mode. We integrate these\nselective SSMs into a simplified end-to-end neural network architecture without\nattention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$\nhigher throughput than Transformers) and linear scaling in sequence length, and\nits performance improves on real data up to million-length sequences. As a\ngeneral sequence model backbone, Mamba achieves state-of-the-art performance\nacross several modalities such as language, audio, and genomics. On language\nmodeling, our Mamba-3B model outperforms Transformers of the same size and\nmatches Transformers twice its size, both in pretraining and downstream\nevaluation.\n""]",State Space Models vs Transformers,Advances in Sequence Modeling: Transformers and Alternatives,Advances in Sequence Modeling and Learning,Advances in Sequence Modeling and Learning
31,211,31_ai_accountability_ethics_ethical,"['ai', 'accountability', 'ethics', 'ethical', 'intelligence', 'aia', 'governance', 'oversight', 'regulations', 'responsibility']","['governance', 'risks', 'regulatory', 'ethical', 'safety', 'ethics', 'risk', 'systems', 'regulations', 'artificial']","[""  Auditing of AI systems is a promising way to understand and manage ethical\nproblems and societal risks associated with contemporary AI systems, as well as\nsome anticipated future risks. Efforts to develop standards for auditing\nArtificial Intelligence (AI) systems have therefore understandably gained\nmomentum. However, we argue that creating auditing standards is not just\ninsufficient, but actively harmful by proliferating unheeded and inconsistent\nstandards, especially in light of the rapid evolution and ethical and safety\nchallenges of AI. Instead, the paper proposes the establishment of an AI Audit\nStandards Board, responsible for developing and updating auditing methods and\nstandards in line with the evolving nature of AI technologies. Such a body\nwould ensure that auditing practices remain relevant, robust, and responsive to\nthe rapid advancements in AI. The paper argues that such a governance structure\nwould also be helpful for maintaining public trust in AI and for promoting a\nculture of safety and ethical responsibility within the AI industry.\n  Throughout the paper, we draw parallels with other industries, including\nsafety-critical industries like aviation and nuclear energy, as well as more\nprosaic ones such as financial accounting and pharmaceuticals. AI auditing\nshould emulate those fields, and extend beyond technical assessments to include\nethical considerations and stakeholder engagement, but we explain that this is\nnot enough; emulating other fields' governance mechanisms for these processes,\nand for audit standards creation, is a necessity. We also emphasize the\nimportance of auditing the entire development process of AI systems, not just\nthe final products...\n"", '  The evolution of AI is set to profoundly reshape the future. The European\nUnion, recognizing this impending prominence, has enacted the AI Act,\nregulating market access for AI-based systems. A salient feature of the Act is\nto guard democratic and humanistic values by focusing regulation on\ntransparency, explainability, and the human ability to understand and control\nAI systems. Hereby, the EU AI Act does not merely specify technological\nrequirements for AI systems. The EU issues a democratic call for human-centered\nAI systems and, in turn, an interdisciplinary research agenda for\nhuman-centered innovation in AI development. Without robust methods to assess\nAI systems and their effect on individuals and society, the EU AI Act may lead\nto repeating the mistakes of the General Data Protection Regulation of the EU\nand to rushed, chaotic, ad-hoc, and ambiguous implementation, causing more\nconfusion than lending guidance. Moreover, determined research activities in\nHuman-AI interaction will be pivotal for both regulatory compliance and the\nadvancement of AI in a manner that is both ethical and effective. Such an\napproach will ensure that AI development aligns with human values and needs,\nfostering a technology landscape that is innovative, responsible, and an\nintegral part of our society.\n', ""  As AI systems become increasingly prevalent and impactful, the need for\neffective AI governance and accountability measures is paramount. This paper\nexamines the AI governance landscape, focusing on Anthropic's Claude, a\nfoundational AI model. We analyze Claude through the lens of the NIST AI Risk\nManagement Framework and the EU AI Act, identifying potential threats and\nproposing mitigation strategies. The paper highlights the importance of\ntransparency, rigorous benchmarking, and comprehensive data handling processes\nin ensuring the responsible development and deployment of AI systems. We\nconclude by discussing the social impact of AI governance and the ethical\nconsiderations surrounding AI accountability.\n""]",AI Governance and Accountability,Artificial Intelligence Ethics and Governance,Artificial Intelligence Applications and Implications,Artificial Intelligence Applications and Implications
32,206,32_graphs_subgraph_networks_subgraphs,"['graphs', 'subgraph', 'networks', 'subgraphs', 'graph', 'nodes', 'node', 'vertices', 'neural', 'edge']","['graph', 'node', 'message', 'graphs', 'nodes', 'expressive', 'neighborhood', 'subgraph', 'networks', 'neighbors']","['  Graph neural networks (GNNs) have become the \\textit{de facto} standard for\nrepresentational learning in graphs, and have achieved state-of-the-art\nperformance in many graph-related tasks; however, it has been shown that the\nexpressive power of standard GNNs are equivalent maximally to 1-dimensional\nWeisfeiler-Lehman (1-WL) Test. Recently, there is a line of works aiming to\nenhance the expressive power of graph neural networks. One line of such works\naim at developing $K$-hop message-passing GNNs where node representation is\nupdated by aggregating information from not only direct neighbors but all\nneighbors within $K$-hop of the node. Another line of works leverages subgraph\ninformation to enhance the expressive power which is proven to be strictly more\npowerful than 1-WL test. In this work, we discuss the limitation of $K$-hop\nmessage-passing GNNs and propose \\textit{substructure encoding function} to\nuplift the expressive power of any $K$-hop message-passing GNN. We further\ninject contextualized substructure information to enhance the expressiveness of\n$K$-hop message-passing GNNs. Our method is provably more powerful than\nprevious works on $K$-hop graph neural networks and 1-WL subgraph GNNs, which\nis a specific type of subgraph based GNN models, and not less powerful than\n3-WL. Empirically, our proposed method set new state-of-the-art performance or\nachieves comparable performance for a variety of datasets. Our code is\navailable at \\url{https://github.com/tianyao-aka/Expresive_K_hop_GNNs}.\n', ""  Graph Neural Network (GNN), with the main idea of encoding graph structure\ninformation of graphs by propagation and aggregation, has developed rapidly. It\nachieved excellent performance in representation learning of multiple types of\ngraphs such as homogeneous graphs, heterogeneous graphs, and more complex\ngraphs like knowledge graphs. However, merely stacking GNN layers may not\nimprove the model's performance and can even be detrimental. For the phenomenon\nof performance degradation in deep GNNs, we propose a new perspective. Unlike\nthe popular explanations of over-smoothing or over-squashing, we think the\nissue arises from the interference of low-quality node representations during\nmessage propagation. We introduce a simple and general method, SF-GNN, to\naddress this problem. In SF-GNN, we define two representations for each node,\none is the node representation that represents the feature of the node itself,\nand the other is the message representation specifically for propagating\nmessages to neighbor nodes. A self-filter module evaluates the quality of the\nnode representation and decides whether to integrate it into the message\npropagation based on this quality assessment. Experiments on node\nclassification tasks for both homogeneous and heterogeneous graphs, as well as\nlink prediction tasks on knowledge graphs, demonstrate that our method can be\napplied to various GNN models and outperforms state-of-the-art baseline methods\nin addressing deep GNN degradation.\n"", '  Graph neural networks (GNNs) have achieved state-of-the-art performance in\ngraph representation learning. Message passing neural networks, which learn\nrepresentations through recursively aggregating information from each node and\nits neighbors, are among the most commonly-used GNNs. However, a wealth of\nstructural information of individual nodes and full graphs is often ignored in\nsuch process, which restricts the expressive power of GNNs. Various graph data\naugmentation methods that enable the message passing with richer structure\nknowledge have been introduced as one main way to tackle this issue, but they\nare often focused on individual structure features and difficult to scale up\nwith more structure features. In this work we propose a novel approach, namely\ncollective structure knowledge-augmented graph neural network (CoS-GNN), in\nwhich a new message passing method is introduced to allow GNNs to harness a\ndiverse set of node- and graph-level structure features, together with original\nnode features/attributes, in augmented graphs. In doing so, our approach\nlargely improves the structural knowledge modeling of GNNs in both node and\ngraph levels, resulting in substantially improved graph representations. This\nis justified by extensive empirical results where CoS-GNN outperforms\nstate-of-the-art models in various graph-level learning tasks, including graph\nclassification, anomaly detection, and out-of-distribution generalization.\n']",Graph Neural Networks for Enhanced Representation Learning,Graph Neural Networks and Deep Learning on Graph-Structured Data,Graph Representation Learning and Neural Networks,Graph Representation Learning and Neural Networks
33,199,33_forecast_forecasts_forecasting_meteorological,"['forecast', 'forecasts', 'forecasting', 'meteorological', 'weather', 'climate', 'ensembles', 'prediction', 'predictions', 'precipitation']","['weather', 'climate', 'precipitation', 'forecasts', 'forecast', 'forecasting', 'atmospheric', 'resolution', 'extreme', 'ensemble']","['  General circulation models (GCMs) are the foundation of weather and climate\nprediction. GCMs are physics-based simulators which combine a numerical solver\nfor large-scale dynamics with tuned representations for small-scale processes\nsuch as cloud formation. Recently, machine learning (ML) models trained on\nreanalysis data achieved comparable or better skill than GCMs for deterministic\nweather forecasting. However, these models have not demonstrated improved\nensemble forecasts, or shown sufficient stability for long-term weather and\nclimate simulations. Here we present the first GCM that combines a\ndifferentiable solver for atmospheric dynamics with ML components, and show\nthat it can generate forecasts of deterministic weather, ensemble weather and\nclimate on par with the best ML and physics-based methods. NeuralGCM is\ncompetitive with ML models for 1-10 day forecasts, and with the European Centre\nfor Medium-Range Weather Forecasts ensemble prediction for 1-15 day forecasts.\nWith prescribed sea surface temperature, NeuralGCM can accurately track climate\nmetrics such as global mean temperature for multiple decades, and climate\nforecasts with 140 km resolution exhibit emergent phenomena such as realistic\nfrequency and trajectories of tropical cyclones. For both weather and climate,\nour approach offers orders of magnitude computational savings over conventional\nGCMs. Our results show that end-to-end deep learning is compatible with tasks\nperformed by conventional GCMs, and can enhance the large-scale physical\nsimulations that are essential for understanding and predicting the Earth\nsystem.\n', ""  Operational numerical weather prediction systems consist of three fundamental\ncomponents: the global observing system for data collection, data assimilation\nfor generating initial conditions, and the forecasting model to predict future\nweather conditions. While NWP have undergone a quiet revolution, with forecast\nskills progressively improving over the past few decades, their advancement has\nslowed due to challenges such as high computational costs and the complexities\nassociated with assimilating an increasing volume of observational data and\nmanaging finer spatial grids. Advances in machine learning offer an alternative\npath towards more efficient and accurate weather forecasts. The rise of machine\nlearning based weather forecasting models has also spurred the development of\nmachine learning based DA models or even purely machine learning based weather\nforecasting systems. This paper introduces FuXi Weather, an end-to-end machine\nlearning based weather forecasting system. FuXi Weather employs specialized\ndata preprocessing and multi-modal data fusion techniques to integrate\ninformation from diverse sources under all-sky conditions, including microwave\nsounders from 3 polar-orbiting satellites and radio occultation data from\nGlobal Navigation Satellite System. Operating on a 6-hourly DA and forecasting\ncycle, FuXi Weather independently generates robust and accurate 10-day global\nweather forecasts at a spatial resolution of 0.25\\textdegree. It surpasses the\nEuropean Centre for Medium-range Weather Forecasts high-resolution forecasts in\nterms of predictability, extending the skillful forecast lead times for several\nkey weather variables such as the geopotential height at 500 hPa from 9.25 days\nto 9.5 days. The system's high computational efficiency and robust performance,\neven with limited observations, demonstrates its potential as a promising\nalternative to traditional NWP systems.\n"", ""  Weather forecasts are fundamentally uncertain, so predicting the range of\nprobable weather scenarios is crucial for important decisions, from warning the\npublic about hazardous weather, to planning renewable energy use. Here, we\nintroduce GenCast, a probabilistic weather model with greater skill and speed\nthan the top operational medium-range weather forecast in the world, the\nEuropean Centre for Medium-Range Forecasts (ECMWF)'s ensemble forecast, ENS.\nUnlike traditional approaches, which are based on numerical weather prediction\n(NWP), GenCast is a machine learning weather prediction (MLWP) method, trained\non decades of reanalysis data. GenCast generates an ensemble of stochastic\n15-day global forecasts, at 12-hour steps and 0.25 degree latitude-longitude\nresolution, for over 80 surface and atmospheric variables, in 8 minutes. It has\ngreater skill than ENS on 97.4% of 1320 targets we evaluated, and better\npredicts extreme weather, tropical cyclones, and wind power production. This\nwork helps open the next chapter in operational weather forecasting, where\ncritical weather-dependent decisions are made with greater accuracy and\nefficiency.\n""]",Machine Learning for Weather Forecasting and Climate Prediction,Climate and Weather Modeling and Prediction using Machine Learning and NLP,Environmental Modeling and Prediction using Advanced Computing and AI,Environmental Modeling and Prediction using Advanced Computing and AI
34,199,34_spiking_neuron_neuromorphic_neurons,"['spiking', 'neuron', 'neuromorphic', 'neurons', 'neural', 'neuronal', 'spike', 'dsnns', 'synapses', 'rsnns']","['spiking', 'neuromorphic', 'spike', 'neurons', 'event', 'energy', 'neuron', 'synaptic', 'hardware', 'neural']","[""  This paper explores the synergistic potential of neuromorphic and edge\ncomputing to create a versatile machine learning (ML) system tailored for\nprocessing data captured by dynamic vision sensors. We construct and train\nhybrid models, blending spiking neural networks (SNNs) and artificial neural\nnetworks (ANNs) using PyTorch and Lava frameworks. Our hybrid architecture\nintegrates an SNN for temporal feature extraction and an ANN for\nclassification. We delve into the challenges of deploying such hybrid\nstructures on hardware. Specifically, we deploy individual components on\nIntel's Neuromorphic Processor Loihi (for SNN) and Jetson Nano (for ANN). We\nalso propose an accumulator circuit to transfer data from the spiking to the\nnon-spiking domain. Furthermore, we conduct comprehensive performance analyses\nof hybrid SNN-ANN models on a heterogeneous system of neuromorphic and edge AI\nhardware, evaluating accuracy, latency, power, and energy consumption. Our\nfindings demonstrate that the hybrid spiking networks surpass the baseline ANN\nmodel across all metrics and outperform the baseline SNN model in accuracy and\nlatency.\n"", '  Brain-inspired Spiking Neural Networks (SNNs) have bio-plausibility and\nlow-power advantages over Artificial Neural Networks (ANNs). Applications of\nSNNs are currently limited to simple classification tasks because of their poor\nperformance. In this work, we focus on bridging the performance gap between\nANNs and SNNs on object detection. Our design revolves around network\narchitecture and spiking neuron. First, the overly complex module design causes\nspike degradation when the YOLO series is converted to the corresponding\nspiking version. We design a SpikeYOLO architecture to solve this problem by\nsimplifying the vanilla YOLO and incorporating meta SNN blocks. Second, object\ndetection is more sensitive to quantization errors in the conversion of\nmembrane potentials into binary spikes by spiking neurons. To address this\nchallenge, we design a new spiking neuron that activates Integer values during\ntraining while maintaining spike-driven by extending virtual timesteps during\ninference. The proposed method is validated on both static and neuromorphic\nobject detection datasets. On the static COCO dataset, we obtain 66.2% mAP@50\nand 48.9% mAP@50:95, which is +15.0% and +18.7% higher than the prior\nstate-of-the-art SNN, respectively. On the neuromorphic Gen1 dataset, we\nachieve 67.2% mAP@50, which is +2.5% greater than the ANN with equivalent\narchitecture, and the energy efficiency is improved by 5.7*. Code:\nhttps://github.com/BICLab/SpikeYOLO\n', '  Spiking neural networks (SNNs) have low power consumption and\nbio-interpretable characteristics, and are considered to have tremendous\npotential for energy-efficient computing. However, the exploration of SNNs on\nimage generation tasks remains very limited, and a unified and effective\nstructure for SNN-based generative models has yet to be proposed. In this\npaper, we explore a novel diffusion model architecture within spiking neural\nnetworks. We utilize transformer to replace the commonly used U-net structure\nin mainstream diffusion models. It can generate higher quality images with\nrelatively lower computational cost and shorter sampling time. It aims to\nprovide an empirical baseline for research of generative models based on SNNs.\nExperiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets demonstrate that our\nwork is highly competitive compared to existing SNN generative models.\n']",Spiking Neural Networks (SNNs) for Efficient Computing,Quantum and Neuromorphic Computing,Computational Models of Complex Systems and Processes,Computational Models of Complex Systems and Processes
35,192,35_safety_reinforcement_unsafe_safely,"['safety', 'reinforcement', 'unsafe', 'safely', 'safe', 'autonomous', 'control', 'learning', 'barrier', 'learned']","['safety', 'safe', 'control', 'controllers', 'controller', 'constraints', 'reinforcement', 'policy', 'barrier', 'reachability']","['  We develop provably safe and convergent reinforcement learning (RL)\nalgorithms for control of nonlinear dynamical systems, bridging the gap between\nthe hard safety guarantees of control theory and the convergence guarantees of\nRL theory. Recent advances at the intersection of control and RL follow a\ntwo-stage, safety filter approach to enforcing hard safety constraints:\nmodel-free RL is used to learn a potentially unsafe controller, whose actions\nare projected onto safe sets prescribed, for example, by a control barrier\nfunction. Though safe, such approaches lose any convergence guarantees enjoyed\nby the underlying RL methods. In this paper, we develop a single-stage,\nsampling-based approach to hard constraint satisfaction that learns RL\ncontrollers enjoying classical convergence guarantees while satisfying hard\nsafety constraints throughout training and deployment. We validate the efficacy\nof our approach in simulation, including safe control of a quadcopter in a\nchallenging obstacle avoidance problem, and demonstrate that it outperforms\nexisting benchmarks.\n', ""  Recently, safe reinforcement learning (RL) with the actor-critic structure\nfor continuous control tasks has received increasing attention. It is still\nchallenging to learn a near-optimal control policy with safety and convergence\nguarantees. Also, few works have addressed the safe RL algorithm design under\ntime-varying safety constraints. This paper proposes a safe RL algorithm for\noptimal control of nonlinear systems with time-varying state and control\nconstraints. In the proposed approach, we construct a novel barrier force-based\ncontrol policy structure to guarantee control safety. A multi-step policy\nevaluation mechanism is proposed to predict the policy's safety risk under\ntime-varying safety constraints and guide the policy to update safely.\nTheoretical results on stability and robustness are proven. Also, the\nconvergence of the actor-critic implementation is analyzed. The performance of\nthe proposed algorithm outperforms several state-of-the-art RL algorithms in\nthe simulated Safety Gym environment. Furthermore, the approach is applied to\nthe integrated path following and collision avoidance problem for two\nreal-world intelligent vehicles. A differential-drive vehicle and an\nAckermann-drive one are used to verify offline deployment and online learning\nperformance, respectively. Our approach shows an impressive sim-to-real\ntransfer capability and a satisfactory online control performance in the\nexperiment.\n"", '  Deep reinforcement learning (DRL) has demonstrated remarkable performance in\nmany continuous control tasks. However, a significant obstacle to the\nreal-world application of DRL is the lack of safety guarantees. Although DRL\nagents can satisfy system safety in expectation through reward shaping,\ndesigning agents to consistently meet hard constraints (e.g., safety\nspecifications) at every time step remains a formidable challenge. In contrast,\nexisting work in the field of safe control provides guarantees on persistent\nsatisfaction of hard safety constraints. However, these methods require\nexplicit analytical system dynamics models to synthesize safe control, which\nare typically inaccessible in DRL settings. In this paper, we present a\nmodel-free safe control algorithm, the implicit safe set algorithm, for\nsynthesizing safeguards for DRL agents that ensure provable safety throughout\ntraining. The proposed algorithm synthesizes a safety index (barrier\ncertificate) and a subsequent safe control law solely by querying a black-box\ndynamic function (e.g., a digital twin simulator). Moreover, we theoretically\nprove that the implicit safe set algorithm guarantees finite time convergence\nto the safe set and forward invariance for both continuous-time and\ndiscrete-time systems. We validate the proposed algorithm on the\nstate-of-the-art Safety Gym benchmark, where it achieves zero safety violations\nwhile gaining $95\\% \\pm 9\\%$ cumulative reward compared to state-of-the-art\nsafe DRL methods. Furthermore, the resulting algorithm scales well to\nhigh-dimensional systems with parallel computing.\n']",Safe Reinforcement Learning for Autonomous Control,Reinforcement Learning Methods and Applications,Reinforcement Learning,Reinforcement Learning
35,192,35_safety_reinforcement_unsafe_safely,"['safety', 'reinforcement', 'unsafe', 'safely', 'safe', 'autonomous', 'control', 'learning', 'barrier', 'learned']","['safety', 'safe', 'control', 'controllers', 'controller', 'constraints', 'reinforcement', 'policy', 'barrier', 'reachability']","['  We develop provably safe and convergent reinforcement learning (RL)\nalgorithms for control of nonlinear dynamical systems, bridging the gap between\nthe hard safety guarantees of control theory and the convergence guarantees of\nRL theory. Recent advances at the intersection of control and RL follow a\ntwo-stage, safety filter approach to enforcing hard safety constraints:\nmodel-free RL is used to learn a potentially unsafe controller, whose actions\nare projected onto safe sets prescribed, for example, by a control barrier\nfunction. Though safe, such approaches lose any convergence guarantees enjoyed\nby the underlying RL methods. In this paper, we develop a single-stage,\nsampling-based approach to hard constraint satisfaction that learns RL\ncontrollers enjoying classical convergence guarantees while satisfying hard\nsafety constraints throughout training and deployment. We validate the efficacy\nof our approach in simulation, including safe control of a quadcopter in a\nchallenging obstacle avoidance problem, and demonstrate that it outperforms\nexisting benchmarks.\n', ""  Recently, safe reinforcement learning (RL) with the actor-critic structure\nfor continuous control tasks has received increasing attention. It is still\nchallenging to learn a near-optimal control policy with safety and convergence\nguarantees. Also, few works have addressed the safe RL algorithm design under\ntime-varying safety constraints. This paper proposes a safe RL algorithm for\noptimal control of nonlinear systems with time-varying state and control\nconstraints. In the proposed approach, we construct a novel barrier force-based\ncontrol policy structure to guarantee control safety. A multi-step policy\nevaluation mechanism is proposed to predict the policy's safety risk under\ntime-varying safety constraints and guide the policy to update safely.\nTheoretical results on stability and robustness are proven. Also, the\nconvergence of the actor-critic implementation is analyzed. The performance of\nthe proposed algorithm outperforms several state-of-the-art RL algorithms in\nthe simulated Safety Gym environment. Furthermore, the approach is applied to\nthe integrated path following and collision avoidance problem for two\nreal-world intelligent vehicles. A differential-drive vehicle and an\nAckermann-drive one are used to verify offline deployment and online learning\nperformance, respectively. Our approach shows an impressive sim-to-real\ntransfer capability and a satisfactory online control performance in the\nexperiment.\n"", '  Deep reinforcement learning (DRL) has demonstrated remarkable performance in\nmany continuous control tasks. However, a significant obstacle to the\nreal-world application of DRL is the lack of safety guarantees. Although DRL\nagents can satisfy system safety in expectation through reward shaping,\ndesigning agents to consistently meet hard constraints (e.g., safety\nspecifications) at every time step remains a formidable challenge. In contrast,\nexisting work in the field of safe control provides guarantees on persistent\nsatisfaction of hard safety constraints. However, these methods require\nexplicit analytical system dynamics models to synthesize safe control, which\nare typically inaccessible in DRL settings. In this paper, we present a\nmodel-free safe control algorithm, the implicit safe set algorithm, for\nsynthesizing safeguards for DRL agents that ensure provable safety throughout\ntraining. The proposed algorithm synthesizes a safety index (barrier\ncertificate) and a subsequent safe control law solely by querying a black-box\ndynamic function (e.g., a digital twin simulator). Moreover, we theoretically\nprove that the implicit safe set algorithm guarantees finite time convergence\nto the safe set and forward invariance for both continuous-time and\ndiscrete-time systems. We validate the proposed algorithm on the\nstate-of-the-art Safety Gym benchmark, where it achieves zero safety violations\nwhile gaining $95\\% \\pm 9\\%$ cumulative reward compared to state-of-the-art\nsafe DRL methods. Furthermore, the resulting algorithm scales well to\nhigh-dimensional systems with parallel computing.\n']",Safe Reinforcement Learning for Autonomous Control,Reinforcement Learning Methods and Applications,Reinforcement Learning,Reinforcement Learning
36,192,36_citations_citing_citation_cited,"['citations', 'citing', 'citation', 'cited', 'cite', 'bibliometric', 'references', 'scholarly', 'attributions', 'researchers']","['citation', 'papers', 'citations', 'scientific', 'academic', 'publications', 'scholarly', 'reviews', 'literature', 'research']","['  In this contribution, we deal with seed-based information retrieval in\nnetworks of research publications. Using systematic reviews as a baseline, and\npublication data from the NIH Open Citation Collection, we compare the\nperformance of the three citation-based approaches direct citation,\nco-citation, and bibliographic coupling with respect to recall and precision\nmeasures. In addition, we include the PubMed Related Article score as well as\ncombined approaches in the comparison. We also provide a fairly comprehensive\nreview of earlier research in which citation relations have been used for\ninformation retrieval purposes. The results show an advantage for co-citation\nover bibliographic coupling and direct citation. However, combining the three\napproaches outperforms the exclusive use of co-citation in the study. The\nresults further indicate, in line with previous research, that combining\ncitation-based approaches with textual approaches enhances the performance of\nseed-based information retrieval. The results from the study may guide\napproaches combining citation-based and textual approaches in their choice of\ncitation similarity measures. We suggest that future research use more\nstructured approaches to evaluate methods for seed-based retrieval of\npublications, including comparative approaches as well as the elaboration of\ncommon data sets and baselines for evaluation.\n', '  Abstractive citation text generation is usually framed as an infilling task,\nwhere a sequence-to-sequence model is trained to generate a citation given a\nreference paper and the context window around the target; the generated\ncitation should be a brief discussion of the reference paper as it relates to\nthe citing context. However, examining a recent LED-based citation generation\nsystem, we find that many of the generated citations are generic summaries of\nthe reference papers main contribution, ignoring the citation contexts focus on\na different topic. To address this problem, we propose a simple modification to\nthe citation text generation task: the generation target is not only the\ncitation itself, but the entire context window, including the target citation.\nThis approach can be easily applied to any abstractive citation generation\nsystem, and our experimental results show that training in this way is\npreferred by human readers and allows the generation model to make use of\ncontextual clues about what topic to discuss and what stance to take.\n', '  Citation text plays a pivotal role in elucidating the connection between\nscientific documents, demanding an in-depth comprehension of the cited paper.\nConstructing citations is often time-consuming, requiring researchers to delve\ninto extensive literature and grapple with articulating relevant content. To\naddress this challenge, the field of citation text generation (CTG) has\nemerged. However, while earlier methods have primarily centered on creating\nsingle-sentence citations, practical scenarios frequently necessitate citing\nmultiple papers within a single paragraph. To bridge this gap, we propose a\nmethod that leverages Large Language Models (LLMs) to generate multi-citation\nsentences. Our approach involves a single source paper and a collection of\ntarget papers, culminating in a coherent paragraph containing multi-sentence\ncitation text. Furthermore, we introduce a curated dataset named MCG-S2ORC,\ncomposed of English-language academic research papers in Computer Science,\nshowcasing multiple citation instances. In our experiments, we evaluate three\nLLMs LLaMA, Alpaca, and Vicuna to ascertain the most effective model for this\nendeavor. Additionally, we exhibit enhanced performance by integrating\nknowledge graphs from target papers into the prompts for generating citation\ntext. This research underscores the potential of harnessing LLMs for citation\ngeneration, opening a compelling avenue for exploring the intricate connections\nbetween scientific documents.\n']",Citation Analysis and Generation in Research Publications,Information Retrieval and Knowledge Generation in Research Publications,Information Retrieval and Knowledge Systems,Information Retrieval and Knowledge Systems
37,190,37_federated_collaborative_learning_collaboratively,"['federated', 'collaborative', 'learning', 'collaboratively', 'distributed', 'generalization', 'datasets', 'classifier', 'clients', 'sharing']","['clients', 'federated', 'client', 'heterogeneity', 'local', 'server', 'global', 'personalized', 'heterogeneous', 'aggregation']","[""  Federated Learning (FL) is a machine learning paradigm that enables clients\nto jointly train a global model by aggregating the locally trained models\nwithout sharing any local training data. In practice, there can often be\nsubstantial heterogeneity (e.g., class imbalance) across the local data\ndistributions observed by each of these clients. Under such non-iid data\ndistributions across clients, FL suffers from the 'client-drift' problem where\nevery client drifts to its own local optimum. This results in slower\nconvergence and poor performance of the aggregated model. To address this\nlimitation, we propose a novel regularization technique based on adaptive\nself-distillation (ASD) for training models on the client side. Our\nregularization scheme adaptively adjusts to the client's training data based on\nthe global model entropy and the client's label distribution. The proposed\nregularization can be easily integrated atop existing, state-of-the-art FL\nalgorithms, leading to a further boost in the performance of these\noff-the-shelf methods. We theoretically explain how ASD reduces client-drift\nand also explain its generalization ability. We demonstrate the efficacy of our\napproach through extensive experiments on multiple real-world benchmarks and\nshow substantial gains in performance over state-of-the-art methods.\n"", ""  Federated Learning (FL) allows several clients to construct a common global\nmachine-learning model without having to share their data. FL, however, faces\nthe challenge of statistical heterogeneity between the client's data, which\ndegrades performance and slows down the convergence toward the global model. In\nthis paper, we provide theoretical proof that minimizing heterogeneity between\nclients facilitates the convergence of a global model for every single client.\nThis becomes particularly important under empirical concept shifts among\nclients, rather than merely considering imbalanced classes, which have been\nstudied until now. Therefore, we propose a method for knowledge transfer\nbetween clients where the server trains client-specific generators. Each\ngenerator generates samples for the corresponding client to remove the conflict\nwith other clients' models. Experiments conducted on synthetic and real data,\nalong with a theoretical study, support the effectiveness of our method in\nconstructing a well-generalizable global model by reducing the conflict between\nlocal models.\n"", ""  Knowledge distillation (KD) can enable collaborative learning among\ndistributed clients that have different model architectures and do not share\ntheir local data and model parameters with others. Each client updates its\nlocal model using the average model output/feature of all client models as the\ntarget, known as federated KD. However, existing federated KD methods often do\nnot perform well when clients' local models are trained with heterogeneous\nlocal datasets. In this paper, we propose Federated knowledge distillation\nenabled by Adversarial Learning (FedAL) to address the data heterogeneity among\nclients. First, to alleviate the local model output divergence across clients\ncaused by data heterogeneity, the server acts as a discriminator to guide\nclients' local model training to achieve consensus model outputs among clients\nthrough a min-max game between clients and the discriminator. Moreover,\ncatastrophic forgetting may happen during the clients' local training and\nglobal knowledge transfer due to clients' heterogeneous local data. Towards\nthis challenge, we design the less-forgetting regularization for both local\ntraining and global knowledge transfer to guarantee clients' ability to\ntransfer/learn knowledge to/from others. Experimental results show that FedAL\nand its variants achieve higher accuracy than other federated KD baselines.\n""]",Federated Learning for Collaborative Model Training,Federated Learning,Machine Learning and Data Privacy,Machine Learning and Data Privacy
38,188,38_optimizing_optimizer_optimization_optimisation,"['optimizing', 'optimizer', 'optimization', 'optimisation', 'optimal', 'metaheuristics', 'objective', 'pareto', 'metaheuristic', 'objectives']","['evolutionary', 'optimization', 'problems', 'objective', 'solutions', 'combinatorial', 'search', 'integer', 'fitness', 'solvers']","['  Recently, Pareto Set Learning (PSL) has been proposed for learning the entire\nPareto set using a neural network. PSL employs preference vectors to scalarize\nmultiple objectives, facilitating the learning of mappings from preference\nvectors to specific Pareto optimal solutions. Previous PSL methods have shown\ntheir effectiveness in solving artificial multi-objective optimization problems\n(MOPs) with uniform preference vector sampling. The quality of the learned\nPareto set is influenced by the sampling strategy of the preference vector, and\nthe sampling of the preference vector needs to be decided based on the Pareto\nfront shape. However, a fixed preference sampling strategy cannot\nsimultaneously adapt the Pareto front of multiple MOPs. To address this\nlimitation, this paper proposes an Evolutionary Preference Sampling (EPS)\nstrategy to efficiently sample preference vectors. Inspired by evolutionary\nalgorithms, we consider preference sampling as an evolutionary process to\ngenerate preference vectors for neural network training. We integrate the EPS\nstrategy into five advanced PSL methods. Extensive experiments demonstrate that\nour proposed method has a faster convergence speed than baseline algorithms on\n7 testing problems. Our implementation is available at\nhttps://github.com/rG223/EPS.\n', '  Multi-objective optimization problems can be found in many real-world\napplications, where the objectives often conflict each other and cannot be\noptimized by a single solution. In the past few decades, numerous methods have\nbeen proposed to find Pareto solutions that represent optimal trade-offs among\nthe objectives for a given problem. However, these existing methods could have\nhigh computational complexity or may not have good theoretical properties for\nsolving a general differentiable multi-objective optimization problem. In this\nwork, by leveraging the smooth optimization technique, we propose a lightweight\nand efficient smooth Tchebycheff scalarization approach for gradient-based\nmulti-objective optimization. It has good theoretical properties for finding\nall Pareto solutions with valid trade-off preferences, while enjoying\nsignificantly lower computational complexity compared to other methods.\nExperimental results on various real-world application problems fully\ndemonstrate the effectiveness of our proposed method.\n', '  Real-world scenarios frequently involve multi-objective data-driven\noptimization problems, characterized by unknown problem coefficients and\nmultiple conflicting objectives. Traditional two-stage methods independently\napply a machine learning model to estimate problem coefficients, followed by\ninvoking a solver to tackle the predicted optimization problem. The independent\nuse of optimization solvers and prediction models may lead to suboptimal\nperformance due to mismatches between their objectives. Recent efforts have\nfocused on end-to-end training of predictive models that use decision loss\nderived from the downstream optimization problem. However, these methods have\nprimarily focused on single-objective optimization problems, thus limiting\ntheir applicability. We aim to propose a multi-objective decision-focused\napproach to address this gap. In order to better align with the inherent\nproperties of multi-objective optimization problems, we propose a set of novel\nloss functions. These loss functions are designed to capture the discrepancies\nbetween predicted and true decision problems, considering solution space,\nobjective space, and decision quality, named landscape loss, Pareto set loss,\nand decision loss, respectively. Our experimental results demonstrate that our\nproposed method significantly outperforms traditional two-stage methods and\nmost current decision-focused methods.\n']",Multi-Objective Optimization Methods,Multi-Objective Optimization Techniques,Optimization and Design,Optimization and Design
38,188,38_optimizing_optimizer_optimization_optimisation,"['optimizing', 'optimizer', 'optimization', 'optimisation', 'optimal', 'metaheuristics', 'objective', 'pareto', 'metaheuristic', 'objectives']","['evolutionary', 'optimization', 'problems', 'objective', 'solutions', 'combinatorial', 'search', 'integer', 'fitness', 'solvers']","['  Recently, Pareto Set Learning (PSL) has been proposed for learning the entire\nPareto set using a neural network. PSL employs preference vectors to scalarize\nmultiple objectives, facilitating the learning of mappings from preference\nvectors to specific Pareto optimal solutions. Previous PSL methods have shown\ntheir effectiveness in solving artificial multi-objective optimization problems\n(MOPs) with uniform preference vector sampling. The quality of the learned\nPareto set is influenced by the sampling strategy of the preference vector, and\nthe sampling of the preference vector needs to be decided based on the Pareto\nfront shape. However, a fixed preference sampling strategy cannot\nsimultaneously adapt the Pareto front of multiple MOPs. To address this\nlimitation, this paper proposes an Evolutionary Preference Sampling (EPS)\nstrategy to efficiently sample preference vectors. Inspired by evolutionary\nalgorithms, we consider preference sampling as an evolutionary process to\ngenerate preference vectors for neural network training. We integrate the EPS\nstrategy into five advanced PSL methods. Extensive experiments demonstrate that\nour proposed method has a faster convergence speed than baseline algorithms on\n7 testing problems. Our implementation is available at\nhttps://github.com/rG223/EPS.\n', '  Multi-objective optimization problems can be found in many real-world\napplications, where the objectives often conflict each other and cannot be\noptimized by a single solution. In the past few decades, numerous methods have\nbeen proposed to find Pareto solutions that represent optimal trade-offs among\nthe objectives for a given problem. However, these existing methods could have\nhigh computational complexity or may not have good theoretical properties for\nsolving a general differentiable multi-objective optimization problem. In this\nwork, by leveraging the smooth optimization technique, we propose a lightweight\nand efficient smooth Tchebycheff scalarization approach for gradient-based\nmulti-objective optimization. It has good theoretical properties for finding\nall Pareto solutions with valid trade-off preferences, while enjoying\nsignificantly lower computational complexity compared to other methods.\nExperimental results on various real-world application problems fully\ndemonstrate the effectiveness of our proposed method.\n', '  Real-world scenarios frequently involve multi-objective data-driven\noptimization problems, characterized by unknown problem coefficients and\nmultiple conflicting objectives. Traditional two-stage methods independently\napply a machine learning model to estimate problem coefficients, followed by\ninvoking a solver to tackle the predicted optimization problem. The independent\nuse of optimization solvers and prediction models may lead to suboptimal\nperformance due to mismatches between their objectives. Recent efforts have\nfocused on end-to-end training of predictive models that use decision loss\nderived from the downstream optimization problem. However, these methods have\nprimarily focused on single-objective optimization problems, thus limiting\ntheir applicability. We aim to propose a multi-objective decision-focused\napproach to address this gap. In order to better align with the inherent\nproperties of multi-objective optimization problems, we propose a set of novel\nloss functions. These loss functions are designed to capture the discrepancies\nbetween predicted and true decision problems, considering solution space,\nobjective space, and decision quality, named landscape loss, Pareto set loss,\nand decision loss, respectively. Our experimental results demonstrate that our\nproposed method significantly outperforms traditional two-stage methods and\nmost current decision-focused methods.\n']",Multi-Objective Optimization Methods,Multi-Objective Optimization Techniques,Optimization and Design,Optimization and Design
39,187,39_particles_particle_collider_lhc,"['particles', 'particle', 'collider', 'lhc', 'qcd', 'detectors', 'quarks', 'quark', 'detector', 'lhcb']","['particle', 'detector', 'physics', 'jet', 'collider', 'calorimeter', 'energy', 'particles', 'jets', 'events']","[""  The Large Hadron Collider's high luminosity era presents major computational\nchallenges in the analysis of collision events. Large amounts of Monte Carlo\n(MC) simulation will be required to constrain the statistical uncertainties of\nthe simulated datasets below these of the experimental data. Modelling of\nhigh-energy particles propagating through the calorimeter section of the\ndetector is the most computationally intensive MC simulation task. We introduce\na technique combining recent advancements in generative models and quantum\nannealing for fast and efficient simulation of high-energy particle-calorimeter\ninteractions.\n"", '  In modern collider experiments, the quest to explore fundamental interactions\nbetween elementary particles has reached unparalleled levels of precision.\nSignatures from particle physics detectors are low-level objects (such as\nenergy depositions or tracks) encoding the physics of collisions (the final\nstate particles of hard scattering interactions). The complete simulation of\nthem in a detector is a computational and storage-intensive task. To address\nthis computational bottleneck in particle physics, alternative approaches have\nbeen developed, introducing additional assumptions and trade off accuracy for\nspeed.The field has seen a surge in interest in surrogate modeling the detector\nsimulation, fueled by the advancements in deep generative models. These models\naim to generate responses that are statistically identical to the observed\ndata. In this paper, we conduct a comprehensive and exhaustive taxonomic review\nof the existing literature on the simulation of detector signatures from both\nmethodological and application-wise perspectives. Initially, we formulate the\nproblem of detector signature simulation and discuss its different variations\nthat can be unified. Next, we classify the state-of-the-art methods into five\ndistinct categories based on their underlying model architectures, summarizing\ntheir respective generation strategies. Finally, we shed light on the\nchallenges and opportunities that lie ahead in detector signature simulation,\nsetting the stage for future research and development.\n', '  The detection of out-of-distribution data points is a common task in particle\nphysics. It is used for monitoring complex particle detectors or for\nidentifying rare and unexpected events that may be indicative of new phenomena\nor physics beyond the Standard Model. Recent advances in Machine Learning for\nanomaly detection have encouraged the utilization of such techniques on\nparticle physics problems. This review article provides an overview of the\nstate-of-the-art techniques for anomaly detection in particle physics using\nmachine learning. We discuss the challenges associated with anomaly detection\nin large and complex data sets, such as those produced by high-energy particle\ncolliders, and highlight some of the successful applications of anomaly\ndetection in particle physics experiments.\n']",Particle Physics Detector Simulation,Particle Physics and Accelerator Technologies,Particle Physics and Accelerator Technologies,Particle Physics and Accelerator Technologies
40,176,40_crystals_crystal_crystalline_molecular,"['crystals', 'crystal', 'crystalline', 'molecular', 'lattice', 'atoms', 'atom', 'alloys', 'predicting', 'molecules']","['materials', 'crystal', 'molecular', 'material', 'chemical', 'properties', 'calculations', 'atomic', 'force', 'structures']","['  Recent advances in deep learning have enabled the generation of realistic\ndata by training generative models on large datasets of text, images, and\naudio. While these models have demonstrated exceptional performance in\ngenerating novel and plausible data, it remains an open question whether they\ncan effectively accelerate scientific discovery through the data generation and\ndrive significant advancements across various scientific fields. In particular,\nthe discovery of new inorganic materials with promising properties poses a\ncritical challenge, both scientifically and for industrial applications.\nHowever, unlike textual or image data, materials, or more specifically crystal\nstructures, consist of multiple types of variables - including lattice vectors,\natom positions, and atomic species. This complexity in data give rise to a\nvariety of approaches for representing and generating such data. Consequently,\nthe design choices of generative models for crystal structures remain an open\nquestion. In this study, we explore a new type of diffusion model for the\ngenerative inverse design of crystal structures, with a backbone based on a\nTransformer architecture. We demonstrate our models are superior to previous\nmethods in their versatility for generating crystal structures with desired\nproperties. Furthermore, our empirical results suggest that the optimal\nconditioning methods vary depending on the dataset.\n', '  The calculation of electron density distribution using density functional\ntheory (DFT) in materials and molecules is central to the study of their\nquantum and macro-scale properties, yet accurate and efficient calculation\nremains a long-standing challenge. We introduce ChargE3Net, an E(3)-equivariant\ngraph neural network for predicting electron density in atomic systems.\nChargE3Net enables the learning of higher-order equivariant feature to achieve\nhigh predictive accuracy and model expressivity. We show that ChargE3Net\nexceeds the performance of prior work on diverse sets of molecules and\nmaterials. When trained on the massive dataset of over 100K materials in the\nMaterials Project database, our model is able to capture the complexity and\nvariability in the data, leading to a significant 26.7% reduction in\nself-consistent iterations when used to initialize DFT calculations on unseen\nmaterials. Furthermore, we show that non-self-consistent DFT calculations using\nour predicted charge densities yield near-DFT performance on electronic and\nthermodynamic property prediction at a fraction of the computational cost.\nFurther analysis attributes the greater predictive accuracy to improved\nmodeling of systems with high angular variations. These results illuminate a\npathway towards a machine learning-accelerated ab initio calculations for\nmaterials discovery.\n', ""  Computational prediction of stable crystal structures has a profound impact\non the large-scale discovery of novel functional materials. However, predicting\nthe crystal structure solely from a material's composition or formula is a\npromising yet challenging task, as traditional ab initio crystal structure\nprediction (CSP) methods rely on time-consuming global searches and\nfirst-principles free energy calculations. Inspired by the recent success of\ndeep learning approaches in protein structure prediction, which utilize\npairwise amino acid interactions to describe 3D structures, we present\nAlphaCrystal-II, a novel knowledge-based solution that exploits the abundant\ninter-atomic interaction patterns found in existing known crystal structures.\nAlphaCrystal-II predicts the atomic distance matrix of a target crystal\nmaterial and employs this matrix to reconstruct its 3D crystal structure. By\nleveraging the wealth of inter-atomic relationships of known crystal\nstructures, our approach demonstrates remarkable effectiveness and reliability\nin structure prediction through comprehensive experiments. This work highlights\nthe potential of data-driven methods in accelerating the discovery and design\nof new materials with tailored properties.\n""]",Crystal Structure Prediction and Materials Discovery,Computational Methods for Molecular and Materials Science,Computational Biology and Chemistry,Computational Biology and Chemistry
41,172,41_finance_financial_textual_nlp,"['finance', 'financial', 'textual', 'nlp', 'text', 'investors', 'investor', 'lexicon', 'accounting', 'investment']","['financial', 'stock', 'market', 'sentiment', 'finance', 'news', 'investors', 'companies', 'corporate', 'investment']","['  Recent releases of pre-trained Large Language Models (LLMs) have gained\nconsiderable traction, yet research on fine-tuning and employing\ndomain-specific LLMs remains scarce. This study investigates approaches for\nfine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs,\nfoundational models, and methods for domain-specific pre-training. Focusing on\nthe financial sector, it details dataset selection, preprocessing, model\nchoice, and considerations crucial for LLM fine-tuning in finance. Addressing\nthe unique characteristics of financial data, the study explores the\nconstruction of domain-specific vocabularies and considerations for security\nand regulatory compliance. In the practical application of LLM fine-tuning, the\nstudy outlines the procedure and implementation for generating domain-specific\nLLMs in finance. Various financial cases, including stock price prediction,\nsentiment analysis of financial news, automated document processing, research,\ninformation extraction, and customer service enhancement, are exemplified. The\nstudy explores the potential of LLMs in the financial domain, identifies\nlimitations, and proposes directions for improvement, contributing valuable\ninsights for future research. Ultimately, it advances natural language\nprocessing technology in business, suggesting proactive LLM utilization in\nfinancial services across industries.\n', ""  Natural language processing (NLP) has recently gained relevance within\nfinancial institutions by providing highly valuable insights into companies and\nmarkets' financial documents. However, the landscape of the financial domain\npresents extra challenges for NLP, due to the complexity of the texts and the\nuse of specific terminology. Generalist language models tend to fall short in\ntasks specifically tailored for finance, even when using large language models\n(LLMs) with great natural language understanding and generative capabilities.\nThis paper presents a study on LLM adaptation methods targeted at the financial\ndomain and with high emphasis on financial sentiment analysis. To this purpose,\ntwo foundation models with less than 1.5B parameters have been adapted using a\nwide range of strategies. We show that through careful fine-tuning on both\nfinancial documents and instructions, these foundation models can be adapted to\nthe target domain. Moreover, we observe that small LLMs have comparable\nperformance to larger scale models, while being more efficient in terms of\nparameters and data. In addition to the models, we show how to generate\nartificial instructions through LLMs to augment the number of samples of the\ninstruction dataset.\n"", ""  In recent years, Large Language Models (LLMs) like ChatGPT have seen\nconsiderable advancements and have been applied in diverse fields. Built on the\nTransformer architecture, these models are trained on extensive datasets,\nenabling them to understand and generate human language effectively. In the\nfinancial domain, the deployment of LLMs is gaining momentum. These models are\nbeing utilized for automating financial report generation, forecasting market\ntrends, analyzing investor sentiment, and offering personalized financial\nadvice. Leveraging their natural language processing capabilities, LLMs can\ndistill key insights from vast financial data, aiding institutions in making\ninformed investment choices and enhancing both operational efficiency and\ncustomer satisfaction. In this study, we provide a comprehensive overview of\nthe emerging integration of LLMs into various financial tasks. Additionally, we\nconducted holistic tests on multiple financial tasks through the combination of\nnatural language instructions. Our findings show that GPT-4 effectively follow\nprompt instructions across various financial tasks. This survey and evaluation\nof LLMs in the financial domain aim to deepen the understanding of LLMs'\ncurrent role in finance for both financial practitioners and LLM researchers,\nidentify new research and application prospects, and highlight how these\ntechnologies can be leveraged to solve practical challenges in the finance\nindustry.\n""]",Large Language Models in Finance,Applications of Large Language Models,Large Language Models,Large Language Models
41,172,41_finance_financial_textual_nlp,"['finance', 'financial', 'textual', 'nlp', 'text', 'investors', 'investor', 'lexicon', 'accounting', 'investment']","['financial', 'stock', 'market', 'sentiment', 'finance', 'news', 'investors', 'companies', 'corporate', 'investment']","['  Recent releases of pre-trained Large Language Models (LLMs) have gained\nconsiderable traction, yet research on fine-tuning and employing\ndomain-specific LLMs remains scarce. This study investigates approaches for\nfine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs,\nfoundational models, and methods for domain-specific pre-training. Focusing on\nthe financial sector, it details dataset selection, preprocessing, model\nchoice, and considerations crucial for LLM fine-tuning in finance. Addressing\nthe unique characteristics of financial data, the study explores the\nconstruction of domain-specific vocabularies and considerations for security\nand regulatory compliance. In the practical application of LLM fine-tuning, the\nstudy outlines the procedure and implementation for generating domain-specific\nLLMs in finance. Various financial cases, including stock price prediction,\nsentiment analysis of financial news, automated document processing, research,\ninformation extraction, and customer service enhancement, are exemplified. The\nstudy explores the potential of LLMs in the financial domain, identifies\nlimitations, and proposes directions for improvement, contributing valuable\ninsights for future research. Ultimately, it advances natural language\nprocessing technology in business, suggesting proactive LLM utilization in\nfinancial services across industries.\n', ""  Natural language processing (NLP) has recently gained relevance within\nfinancial institutions by providing highly valuable insights into companies and\nmarkets' financial documents. However, the landscape of the financial domain\npresents extra challenges for NLP, due to the complexity of the texts and the\nuse of specific terminology. Generalist language models tend to fall short in\ntasks specifically tailored for finance, even when using large language models\n(LLMs) with great natural language understanding and generative capabilities.\nThis paper presents a study on LLM adaptation methods targeted at the financial\ndomain and with high emphasis on financial sentiment analysis. To this purpose,\ntwo foundation models with less than 1.5B parameters have been adapted using a\nwide range of strategies. We show that through careful fine-tuning on both\nfinancial documents and instructions, these foundation models can be adapted to\nthe target domain. Moreover, we observe that small LLMs have comparable\nperformance to larger scale models, while being more efficient in terms of\nparameters and data. In addition to the models, we show how to generate\nartificial instructions through LLMs to augment the number of samples of the\ninstruction dataset.\n"", ""  In recent years, Large Language Models (LLMs) like ChatGPT have seen\nconsiderable advancements and have been applied in diverse fields. Built on the\nTransformer architecture, these models are trained on extensive datasets,\nenabling them to understand and generate human language effectively. In the\nfinancial domain, the deployment of LLMs is gaining momentum. These models are\nbeing utilized for automating financial report generation, forecasting market\ntrends, analyzing investor sentiment, and offering personalized financial\nadvice. Leveraging their natural language processing capabilities, LLMs can\ndistill key insights from vast financial data, aiding institutions in making\ninformed investment choices and enhancing both operational efficiency and\ncustomer satisfaction. In this study, we provide a comprehensive overview of\nthe emerging integration of LLMs into various financial tasks. Additionally, we\nconducted holistic tests on multiple financial tasks through the combination of\nnatural language instructions. Our findings show that GPT-4 effectively follow\nprompt instructions across various financial tasks. This survey and evaluation\nof LLMs in the financial domain aim to deepen the understanding of LLMs'\ncurrent role in finance for both financial practitioners and LLM researchers,\nidentify new research and application prospects, and highlight how these\ntechnologies can be leveraged to solve practical challenges in the finance\nindustry.\n""]",Large Language Models in Finance,Applications of Large Language Models,Large Language Models,Large Language Models
42,171,42_explainability_explanations_interpretability_ai,"['explainability', 'explanations', 'interpretability', 'ai', 'explainers', 'interpretable', 'explaining', 'explainer', 'predictive', 'predictors']","['explanations', 'explanation', 'explainability', 'interpretability', 'black', 'hoc', 'box', 'interpretable', 'explainers', 'attributions']","['  eXplainable Artificial Intelligence (XAI) aims at providing understandable\nexplanations of black box models. In this paper, we evaluate current XAI\nmethods by scoring them based on ground truth simulations and sensitivity\nanalysis. To this end, we used an Electric Arc Furnace (EAF) model to better\nunderstand the limits and robustness characteristics of XAI methods such as\nSHapley Additive exPlanations (SHAP), Local Interpretable Model-agnostic\nExplanations (LIME), as well as Averaged Local Effects (ALE) or Smooth\nGradients (SG) in a highly topical setting. These XAI methods were applied to\nvarious types of black-box models and then scored based on their correctness\ncompared to the ground-truth sensitivity of the data-generating processes using\na novel scoring evaluation methodology over a range of simulated additive\nnoise. The resulting evaluation shows that the capability of the Machine\nLearning (ML) models to capture the process accurately is, indeed, coupled with\nthe correctness of the explainability of the underlying data-generating\nprocess. We furthermore show the differences between XAI methods in their\nability to correctly predict the true sensitivity of the modeled industrial\nprocess.\n', ""  Artificial Intelligence (AI) is often an integral part of modern decision\nsupport systems. The best-performing predictive models used in AI-based\ndecision support systems lack transparency. Explainable Artificial Intelligence\n(XAI) aims to create AI systems that can explain their rationale to human\nusers. Local explanations in XAI can provide information about the causes of\nindividual predictions in terms of feature importance. However, a critical\ndrawback of existing local explanation methods is their inability to quantify\nthe uncertainty associated with a feature's importance. This paper introduces\nan extension of a feature importance explanation method, Calibrated\nExplanations, previously only supporting classification, with support for\nstandard regression and probabilistic regression, i.e., the probability that\nthe target is above an arbitrary threshold. The extension for regression keeps\nall the benefits of Calibrated Explanations, such as calibration of the\nprediction from the underlying model with confidence intervals, uncertainty\nquantification of feature importance, and allows both factual and\ncounterfactual explanations. Calibrated Explanations for standard regression\nprovides fast, reliable, stable, and robust explanations. Calibrated\nExplanations for probabilistic regression provides an entirely new way of\ncreating probabilistic explanations from any ordinary regression model,\nallowing dynamic selection of thresholds. The method is model agnostic with\neasily understood conditional rules. An implementation in Python is freely\navailable on GitHub and for installation using both pip and conda, making the\nresults in this paper easily replicable.\n"", '  Strategies based on Explainable Artificial Intelligence (XAI) have promoted\nbetter human interpretability of the results of black box models. This opens up\nthe possibility of questioning whether explanations created by XAI methods meet\nhuman expectations. The XAI methods being currently used (Ciu, Dalex, Eli5,\nLofo, Shap, and Skater) provide various forms of explanations, including global\nrankings of relevance of features, which allow for an overview of how the model\nis explained as a result of its inputs and outputs. These methods provide for\nan increase in the explainability of the model and a greater interpretability\ngrounded on the context of the problem. Intending to shed light on the\nexplanations generated by XAI methods and their interpretations, this research\naddresses a real-world classification problem related to homicide prediction,\nalready peer-validated, replicated its proposed black box model and used 6\ndifferent XAI methods to generate explanations and 6 different human experts.\nThe results were generated through calculations of correlations, comparative\nanalysis and identification of relationships between all ranks of features\nproduced. It was found that even though it is a model that is difficult to\nexplain, 75\\% of the expectations of human experts were met, with approximately\n48\\% agreement between results from XAI methods and human experts. The results\nallow for answering questions such as: ""Are the Expectation of Interpretation\ngenerated among different human experts similar?"", ""Do the different XAI\nmethods generate similar explanations for the proposed problem?"", ""Can\nexplanations generated by XAI methods meet human expectation of\nInterpretations?"", and ""Can Explanations and Expectations of Interpretation\nwork together?"".\n']",Explainable Artificial Intelligence (XAI) Methods Evaluation,Explainable Artificial Intelligence (XAI),Artificial Intelligence and Machine Learning Interpretability and Explainability,Explainable AI and Machine Learning
43,170,43_dialogs_dialogue_dialogues_dialog,"['dialogs', 'dialogue', 'dialogues', 'dialog', 'conversation', 'conversational', 'conversations', 'utterances', 'dialogstudio', 'utterance']","['dialogue', 'dialogues', 'intent', 'slot', 'dialog', 'conversational', 'turn', 'conversations', 'responses', 'conversation']","['  We study the limitations of Large Language Models (LLMs) for the task of\nresponse generation in human-machine dialogue. Several techniques have been\nproposed in the literature for different dialogue types (e.g., Open-Domain).\nHowever, the evaluations of these techniques have been limited in terms of base\nLLMs, dialogue types and evaluation metrics. In this work, we extensively\nanalyze different LLM adaptation techniques when applied to different dialogue\ntypes. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue\ntypes Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.\nWe evaluate the performance of in-context learning and fine-tuning techniques\nacross datasets selected for each dialogue type. We assess the impact of\nincorporating external knowledge to ground the generation in both scenarios of\nRetrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent\nevaluation and explainability criteria for automatic metrics and human\nevaluation protocols. Our analysis shows that there is no universal\nbest-technique for adapting large language models as the efficacy of each\ntechnique depends on both the base LLM and the specific type of dialogue. Last\nbut not least, the assessment of the best adaptation technique should include\nhuman evaluation to avoid false expectations and outcomes derived from\nautomatic metrics.\n', '  This survey provides a comprehensive review of research on multi-turn\ndialogue systems, with a particular focus on multi-turn dialogue systems based\non large language models (LLMs). This paper aims to (a) give a summary of\nexisting LLMs and approaches for adapting LLMs to downstream tasks; (b)\nelaborate recent advances in multi-turn dialogue systems, covering both\nLLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems,\nalong with datasets and evaluation metrics; (c) discuss some future emphasis\nand recent research problems arising from the development of LLMs and the\nincreasing demands on multi-turn dialogue systems.\n', '  Dialogue State Tracking (DST) is designed to monitor the evolving dialogue\nstate in the conversations and plays a pivotal role in developing task-oriented\ndialogue systems. However, obtaining the annotated data for the DST task is\nusually a costly endeavor. In this paper, we focus on employing LLMs to\ngenerate dialogue data to reduce dialogue collection and annotation costs.\nSpecifically, GPT-4 is used to simulate the user and agent interaction,\ngenerating thousands of dialogues annotated with DST labels. Then a two-stage\nfine-tuning on LLaMA 2 is performed on the generated data and the real data for\nthe DST prediction. Experimental results on two public DST benchmarks show that\nwith the generated dialogue data, our model performs better than the baseline\ntrained solely on real data. In addition, our approach is also capable of\nadapting to the dynamic demands in real-world scenarios, generating dialogues\nin new domains swiftly. After replacing dialogue segments in any domain with\nthe corresponding generated ones, the model achieves comparable performance to\nthe model trained on real data.\n']",Dialogue Systems and Large Language Models,Applications of Large Language Models,Large Language Models,Large Language Models
43,170,43_dialogs_dialogue_dialogues_dialog,"['dialogs', 'dialogue', 'dialogues', 'dialog', 'conversation', 'conversational', 'conversations', 'utterances', 'dialogstudio', 'utterance']","['dialogue', 'dialogues', 'intent', 'slot', 'dialog', 'conversational', 'turn', 'conversations', 'responses', 'conversation']","['  We study the limitations of Large Language Models (LLMs) for the task of\nresponse generation in human-machine dialogue. Several techniques have been\nproposed in the literature for different dialogue types (e.g., Open-Domain).\nHowever, the evaluations of these techniques have been limited in terms of base\nLLMs, dialogue types and evaluation metrics. In this work, we extensively\nanalyze different LLM adaptation techniques when applied to different dialogue\ntypes. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue\ntypes Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.\nWe evaluate the performance of in-context learning and fine-tuning techniques\nacross datasets selected for each dialogue type. We assess the impact of\nincorporating external knowledge to ground the generation in both scenarios of\nRetrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent\nevaluation and explainability criteria for automatic metrics and human\nevaluation protocols. Our analysis shows that there is no universal\nbest-technique for adapting large language models as the efficacy of each\ntechnique depends on both the base LLM and the specific type of dialogue. Last\nbut not least, the assessment of the best adaptation technique should include\nhuman evaluation to avoid false expectations and outcomes derived from\nautomatic metrics.\n', '  This survey provides a comprehensive review of research on multi-turn\ndialogue systems, with a particular focus on multi-turn dialogue systems based\non large language models (LLMs). This paper aims to (a) give a summary of\nexisting LLMs and approaches for adapting LLMs to downstream tasks; (b)\nelaborate recent advances in multi-turn dialogue systems, covering both\nLLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems,\nalong with datasets and evaluation metrics; (c) discuss some future emphasis\nand recent research problems arising from the development of LLMs and the\nincreasing demands on multi-turn dialogue systems.\n', '  Dialogue State Tracking (DST) is designed to monitor the evolving dialogue\nstate in the conversations and plays a pivotal role in developing task-oriented\ndialogue systems. However, obtaining the annotated data for the DST task is\nusually a costly endeavor. In this paper, we focus on employing LLMs to\ngenerate dialogue data to reduce dialogue collection and annotation costs.\nSpecifically, GPT-4 is used to simulate the user and agent interaction,\ngenerating thousands of dialogues annotated with DST labels. Then a two-stage\nfine-tuning on LLaMA 2 is performed on the generated data and the real data for\nthe DST prediction. Experimental results on two public DST benchmarks show that\nwith the generated dialogue data, our model performs better than the baseline\ntrained solely on real data. In addition, our approach is also capable of\nadapting to the dynamic demands in real-world scenarios, generating dialogues\nin new domains swiftly. After replacing dialogue segments in any domain with\nthe corresponding generated ones, the model achieves comparable performance to\nthe model trained on real data.\n']",Dialogue Systems and Large Language Models,Applications of Large Language Models,Large Language Models,Large Language Models
44,167,44_summarizers_summarizer_summarizing_summarization,"['summarizers', 'summarizer', 'summarizing', 'summarization', 'summarisation', 'summaries', 'summary', 'sentences', 'transcripts', 'paragraph']","['summarization', 'summaries', 'summary', 'abstractive', 'document', 'extractive', 'factual', 'summarisation', 'metrics', 'meeting']","['  Factual consistency is an important quality in dialogue summarization. Large\nlanguage model (LLM)-based automatic text summarization models generate more\nfactually consistent summaries compared to those by smaller pretrained language\nmodels, but they face deployment challenges in real-world applications due to\nprivacy or resource constraints. In this paper, we investigate the use of\nsymbolic knowledge distillation to improve the factual consistency of smaller\npretrained models for dialogue summarization. We employ zero-shot learning to\nextract symbolic knowledge from LLMs, generating both factually consistent\n(positive) and inconsistent (negative) summaries. We then apply two contrastive\nlearning objectives on these summaries to enhance smaller summarization models.\nExperiments with BART, PEGASUS, and Flan-T5 indicate that our approach\nsurpasses strong baselines that rely on complex data augmentation strategies.\nOur approach achieves better factual consistency while maintaining coherence,\nfluency, and relevance, as confirmed by various automatic evaluation metrics.\nWe also provide access to the data and code to facilitate future research.\n', ""  Recent studies have found that summaries generated by large language models\n(LLMs) are favored by human annotators over the original reference summaries in\ncommonly used summarization datasets. Therefore, we study an LLM-as-reference\nlearning setting for smaller text summarization models to investigate whether\ntheir performance can be substantially improved. To this end, we use LLMs as\nboth oracle summary generators for standard supervised fine-tuning and oracle\nsummary evaluators for efficient contrastive learning that leverages the LLMs'\nsupervision signals. We conduct comprehensive experiments with source news\narticles and find that (1) summarization models trained under the\nLLM-as-reference setting achieve significant performance improvement in both\nLLM and human evaluations; (2) contrastive learning outperforms standard\nsupervised fine-tuning under both low and high resource settings. Our\nexperimental results also enable a meta-analysis of LLMs' summary evaluation\ncapacities under a challenging setting, showing that LLMs are not well-aligned\nwith human evaluators. Particularly, our expert human evaluation reveals\nremaining nuanced performance gaps between LLMs and our fine-tuned models,\nwhich LLMs fail to capture. Thus, we call for further studies into both the\npotential and challenges of using LLMs in summarization model development.\n"", '  While large language models (LLMs) can already achieve strong performance on\nstandard generic summarization benchmarks, their performance on more complex\nsummarization task settings is less studied. Therefore, we benchmark LLMs on\ninstruction controllable text summarization, where the model input consists of\nboth a source article and a natural language requirement for desired summary\ncharacteristics. To this end, we curate an evaluation-only dataset for this\ntask setting and conduct human evaluations of five LLM-based systems to assess\ntheir instruction-following capabilities in controllable summarization. We then\nbenchmark LLM-based automatic evaluation for this task with 4 different\nevaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our study\nreveals that instruction controllable text summarization remains a challenging\ntask for LLMs, since (1) all LLMs evaluated still make factual and other types\nof errors in their summaries; (2) no LLM-based evaluation methods can achieve a\nstrong alignment with human annotators when judging the quality of candidate\nsummaries; (3) different LLMs show large performance gaps in summary generation\nand evaluation capabilities. We make our collected benchmark InstruSum publicly\navailable to facilitate future research in this direction.\n']",Text Summarization with Large Language Models,Large Language Models for Text Analysis and Summarization,Large Language Models,Large Language Models
44,167,44_summarizers_summarizer_summarizing_summarization,"['summarizers', 'summarizer', 'summarizing', 'summarization', 'summarisation', 'summaries', 'summary', 'sentences', 'transcripts', 'paragraph']","['summarization', 'summaries', 'summary', 'abstractive', 'document', 'extractive', 'factual', 'summarisation', 'metrics', 'meeting']","['  Factual consistency is an important quality in dialogue summarization. Large\nlanguage model (LLM)-based automatic text summarization models generate more\nfactually consistent summaries compared to those by smaller pretrained language\nmodels, but they face deployment challenges in real-world applications due to\nprivacy or resource constraints. In this paper, we investigate the use of\nsymbolic knowledge distillation to improve the factual consistency of smaller\npretrained models for dialogue summarization. We employ zero-shot learning to\nextract symbolic knowledge from LLMs, generating both factually consistent\n(positive) and inconsistent (negative) summaries. We then apply two contrastive\nlearning objectives on these summaries to enhance smaller summarization models.\nExperiments with BART, PEGASUS, and Flan-T5 indicate that our approach\nsurpasses strong baselines that rely on complex data augmentation strategies.\nOur approach achieves better factual consistency while maintaining coherence,\nfluency, and relevance, as confirmed by various automatic evaluation metrics.\nWe also provide access to the data and code to facilitate future research.\n', ""  Recent studies have found that summaries generated by large language models\n(LLMs) are favored by human annotators over the original reference summaries in\ncommonly used summarization datasets. Therefore, we study an LLM-as-reference\nlearning setting for smaller text summarization models to investigate whether\ntheir performance can be substantially improved. To this end, we use LLMs as\nboth oracle summary generators for standard supervised fine-tuning and oracle\nsummary evaluators for efficient contrastive learning that leverages the LLMs'\nsupervision signals. We conduct comprehensive experiments with source news\narticles and find that (1) summarization models trained under the\nLLM-as-reference setting achieve significant performance improvement in both\nLLM and human evaluations; (2) contrastive learning outperforms standard\nsupervised fine-tuning under both low and high resource settings. Our\nexperimental results also enable a meta-analysis of LLMs' summary evaluation\ncapacities under a challenging setting, showing that LLMs are not well-aligned\nwith human evaluators. Particularly, our expert human evaluation reveals\nremaining nuanced performance gaps between LLMs and our fine-tuned models,\nwhich LLMs fail to capture. Thus, we call for further studies into both the\npotential and challenges of using LLMs in summarization model development.\n"", '  While large language models (LLMs) can already achieve strong performance on\nstandard generic summarization benchmarks, their performance on more complex\nsummarization task settings is less studied. Therefore, we benchmark LLMs on\ninstruction controllable text summarization, where the model input consists of\nboth a source article and a natural language requirement for desired summary\ncharacteristics. To this end, we curate an evaluation-only dataset for this\ntask setting and conduct human evaluations of five LLM-based systems to assess\ntheir instruction-following capabilities in controllable summarization. We then\nbenchmark LLM-based automatic evaluation for this task with 4 different\nevaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our study\nreveals that instruction controllable text summarization remains a challenging\ntask for LLMs, since (1) all LLMs evaluated still make factual and other types\nof errors in their summaries; (2) no LLM-based evaluation methods can achieve a\nstrong alignment with human annotators when judging the quality of candidate\nsummaries; (3) different LLMs show large performance gaps in summary generation\nand evaluation capabilities. We make our collected benchmark InstruSum publicly\navailable to facilitate future research in this direction.\n']",Text Summarization with Large Language Models,Large Language Models for Text Analysis and Summarization,Large Language Models,Large Language Models
45,166,45_cropland_sensed_imagery_land,"['cropland', 'sensed', 'imagery', 'land', 'satellite', 'crop', 'vegetation', 'cnn', 'earth', 'sensing']","['satellite', 'crop', 'imagery', 'remote', 'land', 'sensing', 'soil', 'cover', 'vegetation', 'earth']","[""  Earth observation (EO) satellite missions have been providing detailed images\nabout the state of the Earth and its land cover for over 50 years. Long term\nmissions, such as NASA's Landsat, Terra, and Aqua satellites, and more\nrecently, the ESA's Sentinel missions, record images of the entire world every\nfew days. Although single images provide point-in-time data, repeated images of\nthe same area, or satellite image time series (SITS) provide information about\nthe changing state of vegetation and land use. These SITS are useful for\nmodeling dynamic processes and seasonal changes such as plant phenology. They\nhave potential benefits for many aspects of land and natural resource\nmanagement, including applications in agricultural, forest, water, and disaster\nmanagement, urban planning, and mining. However, the resulting satellite image\ntime series (SITS) are complex, incorporating information from the temporal,\nspatial, and spectral dimensions. Therefore, deep learning methods are often\ndeployed as they can analyze these complex relationships. This review presents\na summary of the state-of-the-art methods of modelling environmental,\nagricultural, and other Earth observation variables from SITS data using deep\nlearning methods. We aim to provide a resource for remote sensing experts\ninterested in using deep learning techniques to enhance Earth observation\nmodels with temporal information.\n"", '  We introduce a simple yet effective early fusion method for crop yield\nprediction that handles multiple input modalities with different temporal and\nspatial resolutions. We use high-resolution crop yield maps as ground truth\ndata to train crop and machine learning model agnostic methods at the sub-field\nlevel. We use Sentinel-2 satellite imagery as the primary modality for input\ndata with other complementary modalities, including weather, soil, and DEM\ndata. The proposed method uses input modalities available with global coverage,\nmaking the framework globally scalable. We explicitly highlight the importance\nof input modalities for crop yield prediction and emphasize that the\nbest-performing combination of input modalities depends on region, crop, and\nchosen model.\n', ""  Satellites equipped with optical sensors capture high-resolution imagery,\nproviding valuable insights into various environmental phenomena. In recent\nyears, there has been a surge of research focused on addressing some challenges\nin remote sensing, ranging from water detection in diverse landscapes to the\nsegmentation of mountainous and terrains. Ongoing investigations goals to\nenhance the precision and efficiency of satellite imagery analysis. Especially,\nthere is a growing emphasis on developing methodologies for accurate water body\ndetection, snow and clouds, important for environmental monitoring, resource\nmanagement, and disaster response. Within this context, this paper focus on the\ncloud segmentation from remote sensing imagery. Accurate remote sensing data\nanalysis can be challenging due to the presence of clouds in optical\nsensor-based applications. The quality of resulting products such as\napplications and research is directly impacted by cloud detection, which plays\na key role in the remote sensing data processing pipeline. This paper examines\nseven cutting-edge semantic segmentation and detection algorithms applied to\nclouds identification, conducting a benchmark analysis to evaluate their\narchitectural approaches and identify the most performing ones. To increase the\nmodel's adaptability, critical elements including the type of imagery and the\namount of spectral bands used during training are analyzed. Additionally, this\nresearch tries to produce machine learning algorithms that can perform cloud\nsegmentation using only a few spectral bands, including RGB and RGBN-IR\ncombinations. The model's flexibility for a variety of applications and user\nscenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as\ndatasets. This benchmark can be reproduced using the material from this github\nlink: https://github.com/toelt-llc/cloud_segmentation_comparative.\n""]",Satellite Imagery Analysis for Earth Observation,Satellite Data Analysis and Space Object Tracking,Remote Sensing and Space Data Analysis,Remote Sensing and Space Data Analysis
46,164,46_emotions_emotion_multimodal_emotional,"['emotions', 'emotion', 'multimodal', 'emotional', 'utterances', 'conversations', 'dialogues', 'affective', 'contextual', 'arousal']","['emotion', 'emotions', 'emotional', 'recognition', 'conversation', 'facial', 'speech', 'multimodal', 'affective', 'utterance']","[""  Large language models (LLMs) have demonstrated impressive performance in\nmathematical and commonsense reasoning tasks using chain-of-thought (CoT)\nprompting techniques. But can they perform emotional reasoning by concatenating\n`Let's think step-by-step' to the input prompt? In this paper we investigate\nthis question along with introducing a novel approach to zero-shot emotion\ndetection and emotional reasoning using LLMs. Existing state of the art\nzero-shot approaches rely on textual entailment models to choose the most\nappropriate emotion label for an input text. We argue that this strongly\nrestricts the model to a fixed set of labels which may not be suitable or\nsufficient for many applications where emotion analysis is required. Instead,\nwe propose framing the problem of emotion analysis as a generative\nquestion-answering (QA) task. Our approach uses a two step methodology of\ngenerating relevant context or background knowledge to answer the emotion\ndetection question step-by-step. Our paper is the first work on using a\ngenerative approach to jointly address the tasks of emotion detection and\nemotional reasoning for texts. We evaluate our approach on two popular emotion\ndetection datasets and also release the fine-grained emotion labels and\nexplanations for further training and fine-tuning of emotional reasoning\nsystems.\n"", '  The purpose of emotion recognition in conversation (ERC) is to identify the\nemotion category of an utterance based on contextual information. Previous ERC\nmethods relied on simple connections for cross-modal fusion and ignored the\ninformation differences between modalities, resulting in the model being unable\nto focus on modality-specific emotional information. At the same time, the\nshared information between modalities was not processed to generate emotions.\nInformation redundancy problem. To overcome these limitations, we propose a\ncross-modal fusion emotion prediction network based on vector connections. The\nnetwork mainly includes two stages: the multi-modal feature fusion stage based\non connection vectors and the emotion classification stage based on fused\nfeatures. Furthermore, we design a supervised inter-class contrastive learning\nmodule based on emotion labels. Experimental results confirm the effectiveness\nof the proposed method, demonstrating excellent performance on the IEMOCAP and\nMELD datasets.\n', '  In human-computer interaction, it is crucial for agents to respond to human\nby understanding their emotions. Unraveling the causes of emotions is more\nchallenging. A new task named Multimodal Emotion-Cause Pair Extraction in\nConversations is responsible for recognizing emotion and identifying causal\nexpressions. In this study, we propose a multi-stage framework to generate\nemotion and extract the emotion causal pairs given the target emotion. In the\nfirst stage, Llama-2-based InstructERC is utilized to extract the emotion\ncategory of each utterance in a conversation. After emotion recognition, a\ntwo-stream attention model is employed to extract the emotion causal pairs\ngiven the target emotion for subtask 2 while MuTEC is employed to extract\ncausal span for subtask 1. Our approach achieved first place for both of the\ntwo subtasks in the competition.\n']",Emotion Recognition in Conversations,Emotion Analysis and Recognition in Human-Computer Interaction,Human Behavior and Emotion Analysis through Language and Interaction,Human Behavior and Emotion Analysis through Language and Interaction
47,159,47_courts_judicial_court_legalsemi,"['courts', 'judicial', 'court', 'legalsemi', 'lawyers', 'lawyer', 'law', 'legalai', 'retrieval', 'jurisdiction']","['legal', 'court', 'case', 'law', 'documents', 'judicial', 'cases', 'lawyers', 'judgments', 'retrieval']","['  Legal case retrieval aims to help legal workers find relevant cases related\nto their cases at hand, which is important for the guarantee of fairness and\njustice in legal judgments. While recent advances in neural retrieval methods\nhave significantly improved the performance of open-domain retrieval tasks\n(e.g., Web search), their advantages have not been observed in legal case\nretrieval due to their thirst for annotated data. As annotating large-scale\ntraining data in legal domains is prohibitive due to the need for domain\nexpertise, traditional search techniques based on lexical matching such as\nTF-IDF, BM25, and Query Likelihood are still prevalent in legal case retrieval\nsystems. While previous studies have designed several pre-training methods for\nIR models in open-domain tasks, these methods are usually suboptimal in legal\ncase retrieval because they cannot understand and capture the key knowledge and\ndata structures in the legal corpus. To this end, we propose a novel\npre-training framework named Caseformer that enables the pre-trained models to\nlearn legal knowledge and domain-specific relevance information in legal case\nretrieval without any human-labeled data. Through three unsupervised learning\ntasks, Caseformer is able to capture the special language, document structure,\nand relevance patterns of legal case documents, making it a strong backbone for\ndownstream legal case retrieval tasks. Experimental results show that our model\nhas achieved state-of-the-art performance in both zero-shot and full-data\nfine-tuning settings. Also, experiments on both Chinese and English legal\ndatasets demonstrate that the effectiveness of Caseformer is\nlanguage-independent in legal case retrieval.\n', '  General and legal domain LLMs have demonstrated strong performance in various\ntasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are\ndefined by the experts of computer science, lacking consistency with the logic\nof legal practice, making it difficult to judge their practical capabilities.\nTo address this challenge, we are the first to build the Chinese legal LLMs\nbenchmark LAiW, based on the logic of legal practice. To align with the\nthinking process of legal experts and legal practice (syllogism), we divide the\nlegal capabilities of LLMs from easy to difficult into three levels: basic\ninformation retrieval, legal foundation inference, and complex legal\napplication. Each level contains multiple tasks to ensure a comprehensive\nevaluation. Through automated evaluation of current general and legal domain\nLLMs on our benchmark, we indicate that these LLMs may not align with the logic\nof legal practice. LLMs seem to be able to directly acquire complex legal\napplication capabilities but perform poorly in some basic tasks, which may pose\nobstacles to their practical application and acceptance by legal experts. To\nfurther confirm the complex legal application capabilities of current LLMs in\nlegal application scenarios, we also incorporate human evaluation with legal\nexperts. The results indicate that while LLMs may demonstrate strong\nperformance, they still require reinforcement of legal logic.\n', '  Recent advances in Large Language Models (LLMs) have significantly shaped the\napplications of AI in multiple fields, including the studies of legal\nintelligence. Trained on extensive legal texts, including statutes and legal\ndocuments, the legal LLMs can capture important legal knowledge/concepts\neffectively and provide important support for downstream legal applications\nsuch as legal consultancy. Yet, the dynamic nature of legal statutes and\ninterpretations also poses new challenges to the use of LLMs in legal\napplications. Particularly, how to update the legal knowledge of LLMs\neffectively and efficiently has become an important research problem in\npractice. Existing benchmarks for evaluating knowledge update methods are\nmostly designed for the open domain and cannot address the specific challenges\nof the legal domain, such as the nuanced application of new legal knowledge,\nthe complexity and lengthiness of legal regulations, and the intricate nature\nof legal reasoning. To address this gap, we introduce the Legal Knowledge\nUpdate BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for\nlegal LLMs across five dimensions. Specifically, we categorize the needs of\nknowledge updates in the legal domain with the help of legal professionals, and\nthen hire annotators from law schools to create synthetic updates to the\nChinese Criminal and Civil Code as well as sets of questions of which the\nanswers would change after the updates. Through a comprehensive evaluation of\nstate-of-the-art knowledge update methods, we reveal a notable gap between\nexisting knowledge update methods and the unique needs of the legal domain,\nemphasizing the need for further research and development of knowledge update\nmechanisms tailored for legal LLMs.\n']",Legal Case Retrieval and AI Applications,"Applications of Artificial Intelligence in Law, Patent Analysis, and Healthcare",Artificial Intelligence Applications and Implications,Artificial Intelligence Applications and Implications
48,159,48_offloading_vehicular_edge_congestion,"['offloading', 'vehicular', 'edge', 'congestion', 'scheduling', 'traffic', 'networks', 'vehicles', 'throughput', 'network']","['offloading', 'allocation', 'service', 'wireless', 'edge', 'communication', 'network', 'latency', 'computing', 'reinforcement']","['  Vehicular edge computing (VEC) is an emerging technology that enables\nvehicles to perform high-intensity tasks by executing tasks locally or\noffloading them to nearby edge devices. However, obstacles such as buildings\nmay degrade the communications and incur communication interruptions, and thus\nthe vehicle may not meet the requirement for task offloading. Reconfigurable\nintelligent surfaces (RIS) is introduced to support vehicle communication and\nprovide an alternative communication path. The system performance can be\nimproved by flexibly adjusting the phase-shift of the RIS. For RIS-assisted VEC\nsystem where tasks arrive randomly, we design a control scheme that considers\noffloading power, local power allocation and phase-shift optimization. To solve\nthis non-convex problem, we propose a new deep reinforcement learning (DRL)\nframework that employs modified multi-agent deep deterministic policy gradient\n(MADDPG) approach to optimize the power allocation for vehicle users (VUs) and\nblock coordinate descent (BCD) algorithm to optimize the phase-shift of the\nRIS. Simulation results show that our proposed scheme outperforms the\ncentralized deep deterministic policy gradient (DDPG) scheme and random scheme.\n', '  Computational offloading has become an enabling component for edge\nintelligence in mobile and smart devices. Existing offloading schemes mainly\nfocus on mobile devices and servers, while ignoring the potential network\ncongestion caused by tasks from multiple mobile devices, especially in wireless\nmulti-hop networks. To fill this gap, we propose a low-overhead,\ncongestion-aware distributed task offloading scheme by augmenting a distributed\ngreedy framework with graph-based machine learning. In simulated wireless\nmulti-hop networks with 20-110 nodes and a resource allocation scheme based on\nshortest path routing and contention-based link scheduling, our approach is\ndemonstrated to be effective in reducing congestion or unstable queues under\nthe context-agnostic baseline, while improving the execution latency over local\ncomputing.\n', '  With the increasing demand for multiple applications on internet of vehicles.\nIt requires vehicles to carry out multiple computing tasks in real time.\nHowever, due to the insufficient computing capability of vehicles themselves,\noffloading tasks to vehicular edge computing (VEC) servers and allocating\ncomputing resources to tasks becomes a challenge. In this paper, a multi task\ndigital twin (DT) VEC network is established. By using DT to develop offloading\nstrategies and resource allocation strategies for multiple tasks of each\nvehicle in a single slot, an optimization problem is constructed. To solve it,\nwe propose a multi-agent reinforcement learning method on the task offloading\nand resource allocation. Numerous experiments demonstrate that our method is\neffective compared to other benchmark algorithms.\n']",Vehicular Edge Computing and Task Offloading Optimization,Edge Computing and Artificial Intelligence for IoT and Mobile Networks,Edge Computing and Artificial Intelligence for IoT and Mobile Networks,Edge Computing and Artificial Intelligence for IoT and Mobile Networks
49,150,49_attention_generative_editing_images,"['attention', 'generative', 'editing', 'images', 'scenes', 'text', 'guided', 'image', 'visual', 'scene']","['image', 'diffusion', 'text', 'images', 'prompts', 'editing', 'layout', 'generation', 'prompt', 'object']","['  With the advancement of image-to-image diffusion models guided by text,\nsignificant progress has been made in image editing. However, a persistent\nchallenge remains in seamlessly incorporating objects into images based on\ntextual instructions, without relying on extra user-provided guidance. Text and\nimages are inherently distinct modalities, bringing out difficulties in fully\ncapturing the semantic intent conveyed through language and accurately\ntranslating that into the desired visual modifications. Therefore, text-guided\nimage editing models often produce generations with residual object attributes\nthat do not fully align with human expectations. To address this challenge, the\nmodels should comprehend the image content effectively away from a disconnect\nbetween the provided textual editing prompts and the actual modifications made\nto the image. In our paper, we propose a novel method called Locate and Forget\n(LaF), which effectively locates potential target concepts in the image for\nmodification by comparing the syntactic trees of the target prompt and scene\ndescriptions in the input image, intending to forget their existence clues in\nthe generated image. Compared to the baselines, our method demonstrates its\nsuperiority in text-guided image editing tasks both qualitatively and\nquantitatively.\n', '  The generation of high-quality human images through text-to-image (T2I)\nmethods is a significant yet challenging task. Distinct from general image\ngeneration, human image synthesis must satisfy stringent criteria related to\nhuman pose, anatomy, and alignment with textual prompts, making it particularly\ndifficult to achieve realistic results. Recent advancements in T2I generation\nbased on diffusion models have shown promise, yet challenges remain in meeting\nhuman-specific preferences. In this paper, we introduce a novel approach\ntailored specifically for human image generation utilizing Direct Preference\nOptimization (DPO). Specifically, we introduce an efficient method for\nconstructing a specialized DPO dataset for training human image generation\nmodels without the need for costly human feedback. We also propose a modified\nloss function that enhances the DPO training process by minimizing artifacts\nand improving image fidelity. Our method demonstrates its versatility and\neffectiveness in generating human images, including personalized text-to-image\ngeneration. Through comprehensive evaluations, we show that our approach\nsignificantly advances the state of human image generation, achieving superior\nresults in terms of natural anatomies, poses, and text-image alignment.\n', '  Personalized text-to-image (P-T2I) generation aims to create new, text-guided\nimages featuring the personalized subject with a few reference images. However,\nbalancing the trade-off relationship between prompt fidelity and identity\npreservation remains a critical challenge. To address the issue, we propose a\nnovel P-T2I method called Layout-and-Retouch, consisting of two stages: 1)\nlayout generation and 2) retouch. In the first stage, our step-blended\ninference utilizes the inherent sample diversity of vanilla T2I models to\nproduce diversified layout images, while also enhancing prompt fidelity. In the\nsecond stage, multi-source attention swapping integrates the context image from\nthe first stage with the reference image, leveraging the structure from the\ncontext image and extracting visual features from the reference image. This\nachieves high prompt fidelity while preserving identity characteristics.\nThrough our extensive experiments, we demonstrate that our method generates a\nwide variety of images with diverse layouts while maintaining the unique\nidentity features of the personalized objects, even with challenging text\nprompts. This versatility highlights the potential of our framework to handle\ncomplex conditions, significantly enhancing the diversity and applicability of\npersonalized image synthesis.\n']",Text-Guided Image Editing and Generation,Text-Guided Visual Content Generation and Editing,Artificial Intelligence for Creative Content Generation,Artificial Intelligence for Creative Content Generation
50,149,50_reinforcement_reward_rewards_learning,"['reinforcement', 'reward', 'rewards', 'learning', 'learned', 'exploration', 'learn', 'deepmind', 'tasks', 'rl']","['reward', 'reinforcement', 'exploration', 'rewards', 'environments', 'agents', 'meta', 'agent', 'environment', 'intrinsic']","['  Meta-reinforcement learning (meta-RL) is a promising framework for tackling\nchallenging domains requiring efficient exploration. Existing meta-RL\nalgorithms are characterized by low sample efficiency, and mostly focus on\nlow-dimensional task distributions. In parallel, model-based RL methods have\nbeen successful in solving partially observable MDPs, of which meta-RL is a\nspecial case. In this work, we leverage this success and propose a new\nmodel-based approach to meta-RL, based on elements from existing\nstate-of-the-art model-based and meta-RL methods. We demonstrate the\neffectiveness of our approach on common meta-RL benchmark domains, attaining\ngreater return with better sample efficiency (up to $15\\times$) while requiring\nvery little hyperparameter tuning. In addition, we validate our approach on a\nslate of more challenging, higher-dimensional domains, taking a step towards\nreal-world generalizing agents.\n', '  Improving sample efficiency is central to Reinforcement Learning (RL),\nespecially in environments where the rewards are sparse. Some recent approaches\nhave proposed to specify reward functions as manually designed or learned\nreward structures whose integrations in the RL algorithms are claimed to\nsignificantly improve the learning efficiency. Manually designed reward\nstructures can suffer from inaccuracy and existing automatically learning\nmethods are often computationally intractable for complex tasks. The\nintegration of inaccurate or partial reward structures in RL algorithms fail to\nlearn optimal policies. In this work, we propose an RL algorithm that can\nautomatically structure the reward function for sample efficiency, given a set\nof labels that signify subtasks. Given such minimal knowledge about the task,\nwe train a high-level policy that selects optimal sub-tasks in each state\ntogether with a low-level policy that efficiently learns to complete each\nsub-task. We evaluate our algorithm in a variety of sparse-reward environments.\nThe experiment results show that our approach significantly outperforms the\nstate-of-art baselines as the difficulty of the task increases.\n', '  Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as\npromising approaches for learning data-efficient RL algorithms tailored to a\ngiven task distribution. However, they show poor asymptotic performance and\nstruggle with out-of-distribution tasks because they rely on sequence models,\nsuch as recurrent neural networks or transformers, to process experiences\nrather than summarize them using general-purpose RL components such as value\nfunctions. In contrast, traditional RL algorithms are data-inefficient as they\ndo not use domain knowledge, but they do converge to an optimal policy in the\nlimit. We propose RL$^3$, a principled hybrid approach that incorporates\naction-values, learned per task through traditional RL, in the inputs to\nmeta-RL. We show that RL$^3$ earns greater cumulative reward in the long term,\ncompared to RL$^2$, while maintaining data-efficiency in the short term, and\ngeneralizes better to out-of-distribution tasks. Experiments are conducted on\nboth custom and benchmark discrete domains from the meta-RL literature that\nexhibit a range of short-term, long-term, and complex dependencies.\n']",Meta-Reinforcement Learning and Reward Structures,Reinforcement Learning Applications and Methodologies,Reinforcement Learning,Reinforcement Learning
51,148,51_ecg_electrocardiogram_electrocardiograms_ecgs,"['ecg', 'electrocardiogram', 'electrocardiograms', 'ecgs', 'arrhythmia', 'heartbeat', 'arrhythmias', 'electrocardiography', 'cardiac', 'recordings']","['heart', 'electrocardiogram', 'signals', 'cardiovascular', 'cardiac', 'arrhythmia', 'signal', 'monitoring', 'wearable', 'atrial']","['  Electrocardiogram (ECG) signals play a pivotal role in cardiovascular\ndiagnostics, providing essential information on the electrical activity of the\nheart. However, the inherent noise and limited resolution in ECG recordings can\nhinder accurate interpretation and diagnosis. In this paper, we propose a novel\nmodel for ECG super resolution (SR) that uses a DNAE to enhance temporal and\nfrequency information inside ECG signals. Our approach addresses the\nlimitations of traditional ECG signal processing techniques. Our model takes in\ninput 5-second length ECG windows sampled at 50 Hz (very low resolution) and it\nis able to reconstruct a denoised super-resolution signal with an x10\nupsampling rate (sampled at 500 Hz). We trained the proposed DCAE-SR on public\navailable myocardial infraction ECG signals. Our method demonstrates superior\nperformance in reconstructing high-resolution ECG signals from very\nlow-resolution signals with a sampling rate of 50 Hz. We compared our results\nwith the current deep-learning literature approaches for ECG super-resolution\nand some non-deep learning reproducible methods that can perform both\nsuper-resolution and denoising. We obtained current state-of-the-art\nperformances in super-resolution of very low resolution ECG signals frequently\ncorrupted by ECG artifacts. We were able to obtain a signal-to-noise ratio of\n12.20 dB (outperforms previous 4.68 dB), mean squared error of 0.0044\n(outperforms previous 0.0154) and root mean squared error of 4.86% (outperforms\nprevious 12.40%). In conclusion, our DCAE-SR model offers a robust (to artefact\npresence), versatile and explainable solution to enhance the quality of ECG\nsignals. This advancement holds promise in advancing the field of\ncardiovascular diagnostics, paving the way for improved patient care and\nhigh-quality clinical decisions\n', '  Within cardiovascular disease detection using deep learning applied to ECG\nsignals, the complexities of handling physiological signals have sparked\ngrowing interest in leveraging deep generative models for effective data\naugmentation. In this paper, we introduce a novel versatile approach based on\ndenoising diffusion probabilistic models for ECG synthesis, addressing three\nscenarios: (i) heartbeat generation, (ii) partial signal imputation, and (iii)\nfull heartbeat forecasting. Our approach presents the first generalized\nconditional approach for ECG synthesis, and our experimental results\ndemonstrate its effectiveness for various ECG-related tasks. Moreover, we show\nthat our approach outperforms other state-of-the-art ECG generative models and\ncan enhance the performance of state-of-the-art classifiers.\n', '  Cardiovascular disease is a major life-threatening condition that is commonly\nmonitored using electrocardiogram (ECG) signals. However, these signals are\noften contaminated by various types of noise at different intensities,\nsignificantly interfering with downstream tasks. Therefore, denoising ECG\nsignals and increasing the signal-to-noise ratio is crucial for cardiovascular\nmonitoring. In this paper, we propose a deep learning method that combines a\none-dimensional convolutional layer with transformer architecture for denoising\nECG signals. The convolutional layer processes the ECG signal by various\nkernel/patch sizes and generates an embedding called multi-scale patch\nembedding. The embedding then is used as the input of a transformer network and\nenhances the capability of the transformer for denoising the ECG signal.\n']",ECG Signal Processing and Analysis,Signal Processing and Analysis in Biomedical and Geophysical Applications,Signal Processing and Analysis in Complex Environments,Signal Processing and Analysis in Complex Environments
52,142,52_activity_activities_wearable_sensing,"['activity', 'activities', 'wearable', 'sensing', 'tracking', 'fitness', 'pose', 'recognizing', 'recognition', 'datasets']","['activity', 'sensor', 'activities', 'recognition', 'sensors', 'wearable', 'rehabilitation', 'exercise', 'inertial', 'monitoring']","['  Sensor-based human activity recognition (HAR) has been an active research\narea, owing to its applications in smart environments, assisted living,\nfitness, healthcare, etc. Recently, deep learning based end-to-end training has\nresulted in state-of-the-art performance in domains such as computer vision and\nnatural language, where large amounts of annotated data are available. However,\nlarge quantities of annotated data are not available for sensor-based HAR.\nMoreover, the real-world settings on which the HAR is performed differ in terms\nof sensor modalities, classification tasks, and target users. To address this\nproblem, transfer learning has been employed extensively. In this survey, we\nfocus on these transfer learning methods in the application domains of smart\nhome and wearables-based HAR. In particular, we provide a problem-solution\nperspective by categorizing and presenting the works in terms of their\ncontributions and the challenges they address. We also present an updated view\nof the state-of-the-art for both application domains. Based on our analysis of\n205 papers, we highlight the gaps in the literature and provide a roadmap for\naddressing them. This survey provides a reference to the HAR community, by\nsummarizing the existing works and providing a promising research agenda.\n', '  The proliferation of deep learning has significantly advanced various fields,\nyet Human Activity Recognition (HAR) has not fully capitalized on these\ndevelopments, primarily due to the scarcity of labeled datasets. Despite the\nintegration of advanced Inertial Measurement Units (IMUs) in ubiquitous\nwearable devices like smartwatches and fitness trackers, which offer\nself-labeled activity data from users, the volume of labeled data remains\ninsufficient compared to domains where deep learning has achieved remarkable\nsuccess. Addressing this gap, in this paper, we propose a novel approach to\nimprove wearable sensor-based HAR by introducing a pose-to-sensor network model\nthat generates sensor data directly from 3D skeleton pose sequences. our method\nsimultaneously trains the pose-to-sensor network and a human activity\nclassifier, optimizing both data reconstruction and activity recognition. Our\ncontributions include the integration of simultaneous training, direct\npose-to-sensor generation, and a comprehensive evaluation on the MM-Fit\ndataset. Experimental results demonstrate the superiority of our framework with\nsignificant performance improvements over baseline methods.\n', '  Human activity recognition (HAR) from on-body sensors is a core functionality\nin many AI applications: from personal health, through sports and wellness to\nIndustry 4.0. A key problem holding up progress in wearable sensor-based HAR,\ncompared to other ML areas, such as computer vision, is the unavailability of\ndiverse and labeled training data. Particularly, while there are innumerable\nannotated images available in online repositories, freely available sensor data\nis sparse and mostly unlabeled. We propose an unsupervised statistical\nfeature-guided diffusion model specifically optimized for wearable sensor-based\nhuman activity recognition with devices such as inertial measurement unit (IMU)\nsensors. The method generates synthetic labeled time-series sensor data without\nrelying on annotated training data. Thereby, it addresses the scarcity and\nannotation difficulties associated with real-world sensor data. By conditioning\nthe diffusion model on statistical information such as mean, standard\ndeviation, Z-score, and skewness, we generate diverse and representative\nsynthetic sensor data. We conducted experiments on public human activity\nrecognition datasets and compared the method to conventional oversampling and\nstate-of-the-art generative adversarial network methods. Experimental results\ndemonstrate that this can improve the performance of human activity recognition\nand outperform existing techniques.\n']",Human Activity Recognition with Wearable Sensors,Human Activity and Stance Analysis with Multimodal Sensing and AI,Human Behavior Analysis with AI and Multimodal Data,Human Behavior Analysis with AI and Multimodal Data
53,142,53_hatecheck_hatred_hateful_offensiveness,"['hatecheck', 'hatred', 'hateful', 'offensiveness', 'hate', 'bullying', 'cyberbullying', 'profanity', 'dehumanization', 'speech']","['hate', 'speech', 'offensive', 'hateful', 'media', 'cyberbullying', 'content', 'social', 'moderation', 'online']","[""  The growth of social networks makes toxic content spread rapidly. Hate speech\ndetection is a task to help decrease the number of harmful comments. With the\ndiversity in the hate speech created by users, it is necessary to interpret the\nhate speech besides detecting it. Hence, we propose a methodology to construct\na system for targeted hate speech detection from online streaming texts from\nsocial media. We first introduce the ViTHSD - a targeted hate speech detection\ndataset for Vietnamese Social Media Texts. The dataset contains 10K comments,\neach comment is labeled to specific targets with three levels: clean,\noffensive, and hate. There are 5 targets in the dataset, and each target is\nlabeled with the corresponding level manually by humans with strict annotation\nguidelines. The inter-annotator agreement obtained from the dataset is 0.45 by\nCohen's Kappa index, which is indicated as a moderate level. Then, we construct\na baseline for this task by combining the Bi-GRU-LSTM-CNN with the pre-trained\nlanguage model to leverage the power of text representation of BERTology.\nFinally, we suggest a methodology to integrate the baseline model for targeted\nhate speech detection into the online streaming system for practical\napplication in preventing hateful and offensive content on social media.\n"", ""  Hate speech on social media threatens the mental and physical well-being of\nindividuals and contributes to real-world violence. Resharing is an important\ndriver behind the spread of hate speech on social media. Yet, little is known\nabout who reshares hate speech and what their characteristics are. In this\npaper, we analyze the role of user characteristics in hate speech resharing\nacross different types of hate speech (e.g., political hate). For this, we\nproceed as follows: First, we cluster hate speech posts using large language\nmodels to identify different types of hate speech. Then we model the effects of\nuser attributes on users' probability to reshare hate speech using an\nexplainable machine learning model. To do so, we apply debiasing to control for\nselection bias in our observational social media data and further control for\nthe latent vulnerability of users to hate speech. We find that, all else equal,\nusers with fewer followers, fewer friends, fewer posts, and older accounts\nshare more hate speech. This shows that users with little social influence tend\nto share more hate speech. Further, we find substantial heterogeneity across\ndifferent types of hate speech. For example, racist and misogynistic hate is\nspread mostly by users with little social influence. In contrast, political\nanti-Trump and anti-right-wing hate is reshared by users with larger social\ninfluence. Overall, understanding the factors that drive users to share hate\nspeech is crucial for detecting individuals at risk of engaging in harmful\nbehavior and for designing effective mitigation strategies.\n"", '  Hate speech has emerged as a major problem plaguing our social spaces today.\nWhile there have been significant efforts to address this problem, existing\nmethods are still significantly limited in effectively detecting hate speech\nonline. A major limitation of existing methods is that hate speech detection is\na highly contextual problem, and these methods cannot fully capture the context\nof hate speech to make accurate predictions. Recently, large language models\n(LLMs) have demonstrated state-of-the-art performance in several natural\nlanguage tasks. LLMs have undergone extensive training using vast amounts of\nnatural language data, enabling them to grasp intricate contextual details.\nHence, they could be used as knowledge bases for context-aware hate speech\ndetection. However, a fundamental problem with using LLMs to detect hate speech\nis that there are no studies on effectively prompting LLMs for context-aware\nhate speech detection. In this study, we conduct a large-scale study of hate\nspeech detection, employing five established hate speech datasets. We discover\nthat LLMs not only match but often surpass the performance of current benchmark\nmachine learning models in identifying hate speech. By proposing four diverse\nprompting strategies that optimize the use of LLMs in detecting hate speech.\nOur study reveals that a meticulously crafted reasoning prompt can effectively\ncapture the context of hate speech by fully utilizing the knowledge base in\nLLMs, significantly outperforming existing techniques. Furthermore, although\nLLMs can provide a rich knowledge base for the contextual detection of hate\nspeech, suitable prompting strategies play a crucial role in effectively\nleveraging this knowledge base for efficient detection.\n']",Hate Speech Detection and Analysis,Social Media Misbehavior Detection and Analysis,Misbehavior and Toxicity in Online Content,Misbehavior and Toxicity in Online Content
54,141,54_offline_reinforcement_learning_learned,"['offline', 'reinforcement', 'learning', 'learned', 'reward', 'rewards', 'critic', 'exploration', 'imitation', 'learn']","['offline', 'policy', 'reinforcement', 'online', 'policies', 'value', 'behavior', 'diffusion', 'actions', 'conservative']","['  Deep generative models (DGMs) have demonstrated great success across various\ndomains, particularly in generating texts, images, and videos using models\ntrained from offline data. Similarly, data-driven decision-making and robotic\ncontrol also necessitate learning a generator function from the offline data to\nserve as the strategy or policy. In this case, applying deep generative models\nin offline policy learning exhibits great potential, and numerous studies have\nexplored in this direction. However, this field still lacks a comprehensive\nreview and so developments of different branches are relatively independent. In\nthis paper, we provide the first systematic review on the applications of deep\ngenerative models for offline policy learning. In particular, we cover five\nmainstream deep generative models, including Variational Auto-Encoders,\nGenerative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion\nModels, and their applications in both offline reinforcement learning (offline\nRL) and imitation learning (IL). Offline RL and IL are two main branches of\noffline policy learning and are widely-adopted techniques for sequential\ndecision-making. Notably, for each type of DGM-based offline policy learning,\nwe distill its fundamental scheme, categorize related works based on the usage\nof the DGM, and sort out the development process of algorithms in that field.\nSubsequent to the main content, we provide in-depth discussions on deep\ngenerative models and offline policy learning as a summary, based on which we\npresent our perspectives on future research directions. This work offers a\nhands-on reference for the research progress in deep generative models for\noffline policy learning, and aims to inspire improved DGM-based offline RL or\nIL algorithms. For convenience, we maintain a paper list on\nhttps://github.com/LucasCJYSDL/DGMs-for-Offline-Policy-Learning.\n', '  Despite recent progress in offline learning, these methods are still trained\nand tested on the same environment. In this paper, we compare the\ngeneralization abilities of widely used online and offline learning methods\nsuch as online reinforcement learning (RL), offline RL, sequence modeling, and\nbehavioral cloning. Our experiments show that offline learning algorithms\nperform worse on new environments than online learning ones. We also introduce\nthe first benchmark for evaluating generalization in offline learning,\ncollecting datasets of varying sizes and skill-levels from Procgen (2D video\ngames) and WebShop (e-commerce websites). The datasets contain trajectories for\na limited number of game levels or natural language instructions and at test\ntime, the agent has to generalize to new levels or instructions. Our\nexperiments reveal that existing offline learning algorithms struggle to match\nthe performance of online RL on both train and test environments. Behavioral\ncloning is a strong baseline, outperforming state-of-the-art offline RL and\nsequence modeling approaches when trained on data from multiple environments\nand tested on new ones. Finally, we find that increasing the diversity of the\ndata, rather than its size, improves performance on new environments for all\noffline learning algorithms. Our study demonstrates the limited generalization\nof current offline learning algorithms highlighting the need for more research\nin this area.\n', '  While imitation learning requires access to high-quality data, offline\nreinforcement learning (RL) should, in principle, perform similarly or better\nwith substantially lower data quality by using a value function. However,\ncurrent results indicate that offline RL often performs worse than imitation\nlearning, and it is often unclear what holds back the performance of offline\nRL. Motivated by this observation, we aim to understand the bottlenecks in\ncurrent offline RL algorithms. While poor performance of offline RL is\ntypically attributed to an imperfect value function, we ask: is the main\nbottleneck of offline RL indeed in learning the value function, or something\nelse? To answer this question, we perform a systematic empirical study of (1)\nvalue learning, (2) policy extraction, and (3) policy generalization in offline\nRL problems, analyzing how these components affect performance. We make two\nsurprising observations. First, we find that the choice of a policy extraction\nalgorithm significantly affects the performance and scalability of offline RL,\noften more so than the value learning objective. For instance, we show that\ncommon value-weighted behavioral cloning objectives (e.g., AWR) do not fully\nleverage the learned value function, and switching to behavior-constrained\npolicy gradient objectives (e.g., DDPG+BC) often leads to substantial\nimprovements in performance and scalability. Second, we find that a big barrier\nto improving offline RL performance is often imperfect policy generalization on\ntest-time states out of the support of the training data, rather than policy\nlearning on in-distribution states. We then show that the use of suboptimal but\nhigh-coverage data or test-time policy training techniques can address this\ngeneralization issue in practice. Specifically, we propose two simple test-time\npolicy improvement methods and show that these methods lead to better\nperformance.\n']",Offline Reinforcement Learning,Reinforcement Learning Methods and Applications,Reinforcement Learning,Reinforcement Learning
54,141,54_offline_reinforcement_learning_learned,"['offline', 'reinforcement', 'learning', 'learned', 'reward', 'rewards', 'critic', 'exploration', 'imitation', 'learn']","['offline', 'policy', 'reinforcement', 'online', 'policies', 'value', 'behavior', 'diffusion', 'actions', 'conservative']","['  Deep generative models (DGMs) have demonstrated great success across various\ndomains, particularly in generating texts, images, and videos using models\ntrained from offline data. Similarly, data-driven decision-making and robotic\ncontrol also necessitate learning a generator function from the offline data to\nserve as the strategy or policy. In this case, applying deep generative models\nin offline policy learning exhibits great potential, and numerous studies have\nexplored in this direction. However, this field still lacks a comprehensive\nreview and so developments of different branches are relatively independent. In\nthis paper, we provide the first systematic review on the applications of deep\ngenerative models for offline policy learning. In particular, we cover five\nmainstream deep generative models, including Variational Auto-Encoders,\nGenerative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion\nModels, and their applications in both offline reinforcement learning (offline\nRL) and imitation learning (IL). Offline RL and IL are two main branches of\noffline policy learning and are widely-adopted techniques for sequential\ndecision-making. Notably, for each type of DGM-based offline policy learning,\nwe distill its fundamental scheme, categorize related works based on the usage\nof the DGM, and sort out the development process of algorithms in that field.\nSubsequent to the main content, we provide in-depth discussions on deep\ngenerative models and offline policy learning as a summary, based on which we\npresent our perspectives on future research directions. This work offers a\nhands-on reference for the research progress in deep generative models for\noffline policy learning, and aims to inspire improved DGM-based offline RL or\nIL algorithms. For convenience, we maintain a paper list on\nhttps://github.com/LucasCJYSDL/DGMs-for-Offline-Policy-Learning.\n', '  Despite recent progress in offline learning, these methods are still trained\nand tested on the same environment. In this paper, we compare the\ngeneralization abilities of widely used online and offline learning methods\nsuch as online reinforcement learning (RL), offline RL, sequence modeling, and\nbehavioral cloning. Our experiments show that offline learning algorithms\nperform worse on new environments than online learning ones. We also introduce\nthe first benchmark for evaluating generalization in offline learning,\ncollecting datasets of varying sizes and skill-levels from Procgen (2D video\ngames) and WebShop (e-commerce websites). The datasets contain trajectories for\na limited number of game levels or natural language instructions and at test\ntime, the agent has to generalize to new levels or instructions. Our\nexperiments reveal that existing offline learning algorithms struggle to match\nthe performance of online RL on both train and test environments. Behavioral\ncloning is a strong baseline, outperforming state-of-the-art offline RL and\nsequence modeling approaches when trained on data from multiple environments\nand tested on new ones. Finally, we find that increasing the diversity of the\ndata, rather than its size, improves performance on new environments for all\noffline learning algorithms. Our study demonstrates the limited generalization\nof current offline learning algorithms highlighting the need for more research\nin this area.\n', '  While imitation learning requires access to high-quality data, offline\nreinforcement learning (RL) should, in principle, perform similarly or better\nwith substantially lower data quality by using a value function. However,\ncurrent results indicate that offline RL often performs worse than imitation\nlearning, and it is often unclear what holds back the performance of offline\nRL. Motivated by this observation, we aim to understand the bottlenecks in\ncurrent offline RL algorithms. While poor performance of offline RL is\ntypically attributed to an imperfect value function, we ask: is the main\nbottleneck of offline RL indeed in learning the value function, or something\nelse? To answer this question, we perform a systematic empirical study of (1)\nvalue learning, (2) policy extraction, and (3) policy generalization in offline\nRL problems, analyzing how these components affect performance. We make two\nsurprising observations. First, we find that the choice of a policy extraction\nalgorithm significantly affects the performance and scalability of offline RL,\noften more so than the value learning objective. For instance, we show that\ncommon value-weighted behavioral cloning objectives (e.g., AWR) do not fully\nleverage the learned value function, and switching to behavior-constrained\npolicy gradient objectives (e.g., DDPG+BC) often leads to substantial\nimprovements in performance and scalability. Second, we find that a big barrier\nto improving offline RL performance is often imperfect policy generalization on\ntest-time states out of the support of the training data, rather than policy\nlearning on in-distribution states. We then show that the use of suboptimal but\nhigh-coverage data or test-time policy training techniques can address this\ngeneralization issue in practice. Specifically, we propose two simple test-time\npolicy improvement methods and show that these methods lead to better\nperformance.\n']",Offline Reinforcement Learning,Reinforcement Learning Methods and Applications,Reinforcement Learning,Reinforcement Learning
55,141,55_labeling_labeled_supervised_labels,"['labeling', 'labeled', 'supervised', 'labels', 'label', 'classification', 'unlabelled', 'unlabeled', 'classifier', 'regularization']","['label', 'labels', 'pseudo', 'unlabeled', 'noisy', 'semi', 'labeled', 'class', 'supervised', 'labeling']","['  Self-supervised pretraining on unlabeled data followed by supervised\nfine-tuning on labeled data is a popular paradigm for learning from limited\nlabeled examples. We extend this paradigm to the classical positive unlabeled\n(PU) setting, where the task is to learn a binary classifier given only a few\nlabeled positive samples, and (often) a large amount of unlabeled samples\n(which could be positive or negative).\n  We first propose a simple extension of standard infoNCE family of contrastive\nlosses, to the PU setting; and show that this learns superior representations,\nas compared to existing unsupervised and supervised approaches. We then develop\na simple methodology to pseudo-label the unlabeled samples using a new\nPU-specific clustering scheme; these pseudo-labels can then be used to train\nthe final (positive vs. negative) classifier. Our method handily outperforms\nstate-of-the-art PU methods over several standard PU benchmark datasets, while\nnot requiring a-priori knowledge of any class prior (which is a common\nassumption in other PU methods). We also provide a simple theoretical analysis\nthat motivates our methods.\n', '  Deep neural models have achieved state of the art performance on a wide range\nof problems in computer science, especially in computer vision. However, deep\nneural networks often require large datasets of labeled samples to generalize\neffectively, and an important area of active research is semi-supervised\nlearning, which attempts to instead utilize large quantities of (easily\nacquired) unlabeled samples. One family of methods in this space is\npseudo-labeling, a class of algorithms that use model outputs to assign labels\nto unlabeled samples which are then used as labeled samples during training.\nSuch assigned labels, called pseudo-labels, are most commonly associated with\nthe field of semi-supervised learning. In this work we explore a broader\ninterpretation of pseudo-labels within both self-supervised and unsupervised\nmethods. By drawing the connection between these areas we identify new\ndirections when advancements in one area would likely benefit others, such as\ncurriculum learning and self-supervised regularization.\n', '  Real-world datasets usually are class-imbalanced and corrupted by label\nnoise. To solve the joint issue of long-tailed distribution and label noise,\nmost previous works usually aim to design a noise detector to distinguish the\nnoisy and clean samples. Despite their effectiveness, they may be limited in\nhandling the joint issue effectively in a unified way. In this work, we develop\na novel pseudo labeling method using class prototypes from the perspective of\ndistribution matching, which can be solved with optimal transport (OT). By\nsetting a manually-specific probability measure and using a learned transport\nplan to pseudo-label the training samples, the proposed method can reduce the\nside-effects of noisy and long-tailed data simultaneously. Then we introduce a\nsimple yet effective filter criteria by combining the observed labels and\npseudo labels to obtain a more balanced and less noisy subset for a robust\nmodel training. Extensive experiments demonstrate that our method can extract\nthis class-balanced subset with clean labels, which brings effective\nperformance gains for long-tailed classification with label noise.\n']",Semi-Supervised Learning with Pseudo-Labeling,Self-Supervised Learning and Representation Learning,Self-Supervised Representation Learning,Self-Supervised Representation Learning
56,141,56_bias_biases_genders_gender,"['bias', 'biases', 'genders', 'gender', 'gendered', 'biased', 'stereotypes', 'languages', 'language', 'demographics']","['gender', 'bias', 'biases', 'stereotypes', 'fairness', 'stereotypical', 'stereotype', 'social', 'racial', 'groups']","['  Large Language Models (LLMs) can generate biased responses. Yet previous\ndirect probing techniques contain either gender mentions or predefined gender\nstereotypes, which are challenging to comprehensively collect. Hence, we\npropose an indirect probing framework based on conditional generation. This\napproach aims to induce LLMs to disclose their gender bias even without\nexplicit gender or stereotype mentions. We explore three distinct strategies to\ndisclose explicit and implicit gender bias in LLMs. Our experiments demonstrate\nthat all tested LLMs exhibit explicit and/or implicit gender bias, even when\ngender stereotypes are not present in the inputs. In addition, an increased\nmodel size or model alignment amplifies bias in most cases. Furthermore, we\ninvestigate three methods to mitigate bias in LLMs via Hyperparameter Tuning,\nInstruction Guiding, and Debias Tuning. Remarkably, these methods prove\neffective even in the absence of explicit genders or stereotypes.\n', ""  Gender bias research has been pivotal in revealing undesirable behaviors in\nlarge language models, exposing serious gender stereotypes associated with\noccupations, and emotions. A key observation in prior work is that models\nreinforce stereotypes as a consequence of the gendered correlations that are\npresent in the training data. In this paper, we focus on bias where the effect\nfrom training data is unclear, and instead address the question: Do language\nmodels still exhibit gender bias in non-stereotypical settings? To do so, we\nintroduce UnStereoEval (USE), a novel framework tailored for investigating\ngender bias in stereotype-free scenarios. USE defines a sentence-level score\nbased on pretraining data statistics to determine if the sentence contain\nminimal word-gender associations. To systematically benchmark the fairness of\npopular language models in stereotype-free scenarios, we utilize USE to\nautomatically generate benchmarks without any gender-related language. By\nleveraging USE's sentence-level score, we also repurpose prior gender bias\nbenchmarks (Winobias and Winogender) for non-stereotypical evaluation.\nSurprisingly, we find low fairness across all 28 tested models. Concretely,\nmodels demonstrate fair behavior in only 9%-41% of stereotype-free sentences,\nsuggesting that bias does not solely stem from the presence of gender-related\nwords. These results raise important questions about where underlying model\nbiases come from and highlight the need for more systematic and comprehensive\nbias evaluation. We release the full dataset and code at\nhttps://ucinlp.github.io/unstereo-eval.\n"", '  With the growing deployment of large language models (LLMs) across various\napplications, assessing the influence of gender biases embedded in LLMs becomes\ncrucial. The topic of gender bias within the realm of natural language\nprocessing (NLP) has gained considerable focus, particularly in the context of\nEnglish. Nonetheless, the investigation of gender bias in languages other than\nEnglish is still relatively under-explored and insufficiently analyzed. In this\nwork, We examine gender bias in LLMs-generated outputs for different languages.\nWe use three measurements: 1) gender bias in selecting descriptive words given\nthe gender-related context. 2) gender bias in selecting gender-related pronouns\n(she/he) given the descriptive words. 3) gender bias in the topics of\nLLM-generated dialogues. We investigate the outputs of the GPT series of LLMs\nin various languages using our three measurement methods. Our findings revealed\nsignificant gender biases across all the languages we examined.\n']",Gender Bias in Large Language Models,Bias and Fairness in Large Language Models,Fairness and Bias in Artificial Intelligence,Fairness and Bias in Artificial Intelligence
57,138,57_satisfiability_semantics_unsatisfiable_logics,"['satisfiability', 'semantics', 'unsatisfiable', 'logics', 'propositional', 'solvers', 'logic', 'constraints', 'conjunctive', 'clauses']","['logic', 'satisfiability', 'logics', 'clause', 'semantics', 'propositional', 'solvers', 'counting', 'complete', 'formulas']","['  We formulate discussion graph semantics of first-order logic with equality\nfor reasoning about discussion and argumentation as naturally as we would\nreason about sentences. While there are a few existing proposals to use a\nformal logic for reasoning about argumentation, they are constructed bottom-up\nand specialised to the argumentation model by Dung. There is indeed a\nconspicuous lack of a formal reasoning framework for handling general\ndiscussion and argumentation models. We achieve the generality through a\ntop-down formulation of the semantics of first-order logic (with equality)\nformulas, addressing the current shortage.\n', '  The data-complexity of both satisfiability and finite satisfiability for the\ntwo-variable fragment with counting is NP-complete; the data-complexity of both\nquery-answering and finite query-answering for the two-variable guarded\nfragment with counting is co-NP-complete.\n', '  Boolean satisfiability (SAT) problems are routinely solved by SAT solvers in\nreal-life applications, yet solving time can vary drastically between solvers\nfor the same instance. This has motivated research into machine learning models\nthat can predict, for a given SAT instance, which solver to select among\nseveral options. Existing SAT solver selection methods all rely on some\nhand-picked instance features, which are costly to compute and ignore the\nstructural information in SAT graphs. In this paper we present GraSS, a novel\napproach for automatic SAT solver selection based on tripartite graph\nrepresentations of instances and a heterogeneous graph neural network (GNN)\nmodel. While GNNs have been previously adopted in other SAT-related tasks, they\ndo not incorporate any domain-specific knowledge and ignore the runtime\nvariation introduced by different clause orders. We enrich the graph\nrepresentation with domain-specific decisions, such as novel node feature\ndesign, positional encodings for clauses in the graph, a GNN architecture\ntailored to our tripartite graphs and a runtime-sensitive loss function.\nThrough extensive experiments, we demonstrate that this combination of raw\nrepresentations and domain-specific choices leads to improvements in runtime\nfor a pool of seven state-of-the-art solvers on both an industrial circuit\ndesign benchmark, and on instances from the 20-year Anniversary Track of the\n2022 SAT Competition.\n']",Formal Reasoning and Satisfiability in Logic,"Reasoning and Problem-Solving with Logic, Language, and Graphs",Artificial Intelligence and Reasoning Systems,Intelligent Systems
58,137,58_texts_text_autextification_ai,"['texts', 'text', 'autextification', 'ai', 'mixtext', 'paraphrasing', 'linguistic', 'gram2vec', 'writing', 'plagiarism']","['authorship', 'texts', 'text', 'detectors', 'detection', 'plagiarism', 'writing', 'academic', 'author', 'attribution']","['  In recent years, there have been significant advancements in the development\nof Large Language Models (LLMs). While their practical applications are now\nwidespread, their potential for misuse, such as generating fake news and\ncommitting plagiarism, has posed significant concerns. To address this issue,\ndetectors have been developed to evaluate whether a given text is\nhuman-generated or AI-generated. Among others, zero-shot detectors stand out as\neffective approaches that do not require additional training data and are often\nlikelihood-based. In chat-based applications, users commonly input prompts and\nutilize the AI-generated texts. However, zero-shot detectors typically analyze\nthese texts in isolation, neglecting the impact of the original prompts. It is\nconceivable that this approach may lead to a discrepancy in likelihood\nassessments between the text generation phase and the detection phase. So far,\nthere remains an unverified gap concerning how the presence or absence of\nprompts impacts detection accuracy for zero-shot detectors. In this paper, we\nintroduce an evaluative framework to empirically analyze the impact of prompts\non the detection accuracy of AI-generated text. We assess various zero-shot\ndetectors using both white-box detection, which leverages the prompt, and\nblack-box detection, which operates without prompt information. Our experiments\nreveal the significant influence of prompts on detection accuracy. Remarkably,\ncompared with black-box detection without prompts, the white-box methods using\nprompts demonstrate an increase in AUC of at least $0.1$ across all zero-shot\ndetectors tested. Code is available:\n\\url{https://github.com/kaito25atugich/Detector}.\n', ""  This study explores the challenge of sentence-level AI-generated text\ndetection within human-AI collaborative hybrid texts. Existing studies of\nAI-generated text detection for hybrid texts often rely on synthetic datasets.\nThese typically involve hybrid texts with a limited number of boundaries. We\ncontend that studies of detecting AI-generated content within hybrid texts\nshould cover different types of hybrid texts generated in realistic settings to\nbetter inform real-world applications. Therefore, our study utilizes the\nCoAuthor dataset, which includes diverse, realistic hybrid texts generated\nthrough the collaboration between human writers and an intelligent writing\nsystem in multi-turn interactions. We adopt a two-step, segmentation-based\npipeline: (i) detect segments within a given hybrid text where each segment\ncontains sentences of consistent authorship, and (ii) classify the authorship\nof each identified segment. Our empirical findings highlight (1) detecting\nAI-generated sentences in hybrid texts is overall a challenging task because\n(1.1) human writers' selecting and even editing AI-generated sentences based on\npersonal preferences adds difficulty in identifying the authorship of segments;\n(1.2) the frequent change of authorship between neighboring sentences within\nthe hybrid text creates difficulties for segment detectors in identifying\nauthorship-consistent segments; (1.3) the short length of text segments within\nhybrid texts provides limited stylistic cues for reliable authorship\ndetermination; (2) before embarking on the detection process, it is beneficial\nto assess the average length of segments within the hybrid text. This\nassessment aids in deciding whether (2.1) to employ a text segmentation-based\nstrategy for hybrid texts with longer segments, or (2.2) to adopt a direct\nsentence-by-sentence classification strategy for those with shorter segments.\n"", ""  With the increasing prevalence of text generated by large language models\n(LLMs), there is a growing concern about distinguishing between LLM-generated\nand human-written texts in order to prevent the misuse of LLMs, such as the\ndissemination of misleading information and academic dishonesty. Previous\nresearch has primarily focused on classifying text as either entirely\nhuman-written or LLM-generated, neglecting the detection of mixed texts that\ncontain both types of content. This paper explores LLMs' ability to identify\nboundaries in human-written and machine-generated mixed texts. We approach this\ntask by transforming it into a token classification problem and regard the\nlabel turning point as the boundary. Notably, our ensemble model of LLMs\nachieved first place in the 'Human-Machine Mixed Text Detection' sub-task of\nthe SemEval'24 Competition Task 8. Additionally, we investigate factors that\ninfluence the capability of LLMs in detecting boundaries within mixed texts,\nincluding the incorporation of extra layers on top of LLMs, combination of\nsegmentation loss, and the impact of pretraining. Our findings aim to provide\nvaluable insights for future research in this area.\n""]",Detecting AI-Generated Text in Human-AI Collaborative Writing,Natural Language Processing for Text Analysis and Generation,Natural Language Processing,Natural Language Processing
59,135,59_jailbreaks_jailbreaking_jailbreak_jailbreakbench,"['jailbreaks', 'jailbreaking', 'jailbreak', 'jailbreakbench', 'vulnerabilities', 'vulnerable', 'vulnerability', 'security', 'adversarial', 'attacks']","['jailbreak', 'harmful', 'jailbreaks', 'attack', 'attacks', 'prompts', 'safety', 'defense', 'malicious', 'success']","['  As Large Language Models (LLMs) are increasingly being deployed in\nsafety-critical applications, their vulnerability to potential jailbreaks --\nmalicious prompts that can disable the safety mechanism of LLMs -- has\nattracted growing research attention. While alignment methods have been\nproposed to protect LLMs from jailbreaks, many have found that aligned LLMs can\nstill be jailbroken by carefully crafted malicious prompts, producing content\nthat violates policy regulations. Existing jailbreak attacks on LLMs can be\ncategorized into prompt-level methods which make up stories/logic to circumvent\nsafety alignment and token-level attack methods which leverage gradient methods\nto find adversarial tokens. In this work, we introduce the concept of Ensemble\nJailbreak and explore methods that can integrate prompt-level and token-level\njailbreak into a more powerful hybrid jailbreak attack. Specifically, we\npropose a novel EnJa attack to hide harmful instructions using prompt-level\njailbreak, boost the attack success rate using a gradient-based attack, and\nconnect the two types of jailbreak attacks via a template-based connector. We\nevaluate the effectiveness of EnJa on several aligned models and show that it\nachieves a state-of-the-art attack success rate with fewer queries and is much\nstronger than any individual jailbreak.\n', '  Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential\nbut also introduce challenges related to content constraints and potential\nmisuse. Our study investigates three key research questions: (1) the number of\ndifferent prompt types that can jailbreak LLMs, (2) the effectiveness of\njailbreak prompts in circumventing LLM constraints, and (3) the resilience of\nChatGPT against these jailbreak prompts. Initially, we develop a classification\nmodel to analyze the distribution of existing prompts, identifying ten distinct\npatterns and three categories of jailbreak prompts. Subsequently, we assess the\njailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a\ndataset of 3,120 jailbreak questions across eight prohibited scenarios.\nFinally, we evaluate the resistance of ChatGPT against jailbreak prompts,\nfinding that the prompts can consistently evade the restrictions in 40 use-case\nscenarios. The study underscores the importance of prompt structures in\njailbreaking LLMs and discusses the challenges of robust jailbreak prompt\ngeneration and prevention.\n', '  Misuse of the Large Language Models (LLMs) has raised widespread concern. To\naddress this issue, safeguards have been taken to ensure that LLMs align with\nsocial ethics. However, recent findings have revealed an unsettling\nvulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By\napplying techniques, such as employing role-playing scenarios, adversarial\nexamples, or subtle subversion of safety objectives as a prompt, LLMs can\nproduce an inappropriate or even harmful response. While researchers have\nstudied several categories of jailbreak attacks, they have done so in\nisolation. To fill this gap, we present the first large-scale measurement of\nvarious jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak\nmethods from four categories, 160 questions from 16 violation categories, and\nsix popular LLMs. Our extensive experimental results demonstrate that the\noptimized jailbreak prompts consistently achieve the highest attack success\nrates, as well as exhibit robustness across different LLMs. Some jailbreak\nprompt datasets, available from the Internet, can also achieve high attack\nsuccess rates on many LLMs, such as ChatGLM3, GPT-3.5, and PaLM2. Despite the\nclaims from many organizations regarding the coverage of violation categories\nin their policies, the attack success rates from these categories remain high,\nindicating the challenges of effectively aligning LLM policies and the ability\nto counter jailbreak attacks. We also discuss the trade-off between the attack\nperformance and efficiency, as well as show that the transferability of the\njailbreak prompts is still viable, becoming an option for black-box models.\nOverall, our research highlights the necessity of evaluating different\njailbreak methods. We hope our study can provide insights for future research\non jailbreak attacks and serve as a benchmark tool for evaluating them for\npractitioners.\n']",Jailbreak Attacks on Large Language Models,"Large Language Models: Safety, Security, and Ethics",Large Language Models,Large Language Models
59,135,59_jailbreaks_jailbreaking_jailbreak_jailbreakbench,"['jailbreaks', 'jailbreaking', 'jailbreak', 'jailbreakbench', 'vulnerabilities', 'vulnerable', 'vulnerability', 'security', 'adversarial', 'attacks']","['jailbreak', 'harmful', 'jailbreaks', 'attack', 'attacks', 'prompts', 'safety', 'defense', 'malicious', 'success']","['  As Large Language Models (LLMs) are increasingly being deployed in\nsafety-critical applications, their vulnerability to potential jailbreaks --\nmalicious prompts that can disable the safety mechanism of LLMs -- has\nattracted growing research attention. While alignment methods have been\nproposed to protect LLMs from jailbreaks, many have found that aligned LLMs can\nstill be jailbroken by carefully crafted malicious prompts, producing content\nthat violates policy regulations. Existing jailbreak attacks on LLMs can be\ncategorized into prompt-level methods which make up stories/logic to circumvent\nsafety alignment and token-level attack methods which leverage gradient methods\nto find adversarial tokens. In this work, we introduce the concept of Ensemble\nJailbreak and explore methods that can integrate prompt-level and token-level\njailbreak into a more powerful hybrid jailbreak attack. Specifically, we\npropose a novel EnJa attack to hide harmful instructions using prompt-level\njailbreak, boost the attack success rate using a gradient-based attack, and\nconnect the two types of jailbreak attacks via a template-based connector. We\nevaluate the effectiveness of EnJa on several aligned models and show that it\nachieves a state-of-the-art attack success rate with fewer queries and is much\nstronger than any individual jailbreak.\n', '  Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential\nbut also introduce challenges related to content constraints and potential\nmisuse. Our study investigates three key research questions: (1) the number of\ndifferent prompt types that can jailbreak LLMs, (2) the effectiveness of\njailbreak prompts in circumventing LLM constraints, and (3) the resilience of\nChatGPT against these jailbreak prompts. Initially, we develop a classification\nmodel to analyze the distribution of existing prompts, identifying ten distinct\npatterns and three categories of jailbreak prompts. Subsequently, we assess the\njailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a\ndataset of 3,120 jailbreak questions across eight prohibited scenarios.\nFinally, we evaluate the resistance of ChatGPT against jailbreak prompts,\nfinding that the prompts can consistently evade the restrictions in 40 use-case\nscenarios. The study underscores the importance of prompt structures in\njailbreaking LLMs and discusses the challenges of robust jailbreak prompt\ngeneration and prevention.\n', '  Misuse of the Large Language Models (LLMs) has raised widespread concern. To\naddress this issue, safeguards have been taken to ensure that LLMs align with\nsocial ethics. However, recent findings have revealed an unsettling\nvulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By\napplying techniques, such as employing role-playing scenarios, adversarial\nexamples, or subtle subversion of safety objectives as a prompt, LLMs can\nproduce an inappropriate or even harmful response. While researchers have\nstudied several categories of jailbreak attacks, they have done so in\nisolation. To fill this gap, we present the first large-scale measurement of\nvarious jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak\nmethods from four categories, 160 questions from 16 violation categories, and\nsix popular LLMs. Our extensive experimental results demonstrate that the\noptimized jailbreak prompts consistently achieve the highest attack success\nrates, as well as exhibit robustness across different LLMs. Some jailbreak\nprompt datasets, available from the Internet, can also achieve high attack\nsuccess rates on many LLMs, such as ChatGLM3, GPT-3.5, and PaLM2. Despite the\nclaims from many organizations regarding the coverage of violation categories\nin their policies, the attack success rates from these categories remain high,\nindicating the challenges of effectively aligning LLM policies and the ability\nto counter jailbreak attacks. We also discuss the trade-off between the attack\nperformance and efficiency, as well as show that the transferability of the\njailbreak prompts is still viable, becoming an option for black-box models.\nOverall, our research highlights the necessity of evaluating different\njailbreak methods. We hope our study can provide insights for future research\non jailbreak attacks and serve as a benchmark tool for evaluating them for\npractitioners.\n']",Jailbreak Attacks on Large Language Models,"Large Language Models: Safety, Security, and Ethics",Large Language Models,Large Language Models
60,133,60_loras_lora_tuning_rank,"['loras', 'lora', 'tuning', 'rank', 'adapting', 'adaptation', 'lorahub', 'tuned', 'sparse', 'trained']","['rank', 'tuning', 'adaptation', 'parameter', 'fine', 'low', 'matrices', 'trainable', 'parameters', 'efficient']","['  The recent trend in scaling language models has led to a growing demand for\nparameter-efficient tuning (PEFT) methods such as LoRA (Low-Rank Adaptation).\nLoRA consistently matches or surpasses the full fine-tuning baseline with fewer\nparameters. However, handling numerous task-specific or user-specific LoRA\nmodules on top of a base model still presents significant storage challenges.\nTo address this, we introduce LoRA-XS (Low-Rank Adaptation with eXtremely Small\nnumber of parameters), a novel approach leveraging Singular Value Decomposition\n(SVD) for parameter-efficient fine-tuning. LoRA-XS introduces a small r x r\nweight matrix between frozen LoRA matrices, which are constructed by SVD of the\noriginal weight matrix. Training only r x r weight matrices ensures\nindependence from model dimensions, enabling more parameter-efficient\nfine-tuning, especially for larger models. LoRA-XS achieves a remarkable\nreduction of trainable parameters by over 100x in 7B models compared to LoRA.\nOur benchmarking across various scales, including GLUE, GSM8k, and MATH\nbenchmarks, shows that our approach outperforms LoRA and recent\nstate-of-the-art approaches like VeRA in terms of parameter efficiency while\nmaintaining competitive performance.\n', '  Low-Rank Adaptation, also known as LoRA, has emerged as a prominent method\nfor parameter-efficient fine-tuning foundation models by re-parameterizing the\noriginal matrix into the product of two low-rank matrices. Despite its\nefficiency, LoRA often yields inferior performance compared to full\nfine-tuning. In this paper, we propose LoRA-Pro to bridge this performance gap.\nFirstly, we delve into the optimization processes in LoRA and full fine-tuning.\nWe reveal that while LoRA employs low-rank approximation, it neglects to\napproximate the optimization process of full fine-tuning. To address this, we\nintroduce a novel concept called the ""equivalent gradient."" This virtual\ngradient makes the optimization process on the re-parameterized matrix\nequivalent to LoRA, which can be used to quantify the differences between LoRA\nand full fine-tuning. The equivalent gradient is derived from the gradients of\nmatrices $A$ and $B$. To narrow the performance gap, our approach minimizes the\ndifferences between the equivalent gradient and the gradient obtained from full\nfine-tuning during the optimization process. By solving this objective, we\nderive optimal closed-form solutions for updating matrices $A$ and $B$. Our\nmethod constrains the optimization process, shrinking the performance gap\nbetween LoRA and full fine-tuning. Extensive experiments on natural language\nprocessing tasks validate the effectiveness of our method.\n', '  Low-Rank Adaptation (LoRA) is currently the most commonly used\nParameter-efficient fine-tuning (PEFT) method, it introduces auxiliary\nparameters for each layer to fine-tune the pre-trained model under limited\ncomputing resources. However, it still faces resource consumption challenges\nduring training when scaling up to larger models. Most previous studies have\ntackled this issue by using pruning techniques, which involve removing LoRA\nparameters deemed unimportant. Nonetheless, these efforts only analyze LoRA\nparameter features to evaluate their importance, such as parameter count, size,\nand gradient. In fact, the output of LoRA (product of LoRA parameter and hidden\nstate), directly impacts the final results. Preliminary experiments indicate\nthat a fraction of LoRA elements possesses significantly high output values,\nsubstantially influencing the layer output. Motivated by the observation, we\npropose LoRA-drop. Concretely, LoRA-drop evaluates the importance of LoRA based\non the LoRA output. Then we retain LoRA for important layers and the other\nlayers share the same LoRA. We conduct abundant experiments with models of\ndifferent scales on NLU and NLG tasks. Results demonstrate that LoRA-drop can\nachieve performance comparable to full fine-tuning and LoRA, while retaining\n50\\% of the LoRA parameters on average.\n']",Low-Rank Adaptation for Efficient Fine-Tuning,Efficient Model Adaptation Techniques,Advanced Statistical and Machine Learning Methods,Advanced Statistical and Machine Learning Methods
61,133,61_traffic_spatiotemporal_transportation_networks,"['traffic', 'spatiotemporal', 'transportation', 'networks', 'forecast', 'citynet', 'prediction', 'forecasting', 'predicting', 'congestion']","['traffic', 'temporal', 'spatio', 'spatial', 'urban', 'transportation', 'road', 'forecasting', 'prediction', 'flow']","['  As a core technology of Intelligent Transportation System, traffic flow\nprediction has a wide range of applications. The fundamental challenge in\ntraffic flow prediction is to effectively model the complex spatial-temporal\ndependencies in traffic data. Spatial-temporal Graph Neural Network (GNN)\nmodels have emerged as one of the most promising methods to solve this problem.\nHowever, GNN-based models have three major limitations for traffic prediction:\ni) Most methods model spatial dependencies in a static manner, which limits the\nability to learn dynamic urban traffic patterns; ii) Most methods only consider\nshort-range spatial information and are unable to capture long-range spatial\ndependencies; iii) These methods ignore the fact that the propagation of\ntraffic conditions between locations has a time delay in traffic systems. To\nthis end, we propose a novel Propagation Delay-aware dynamic long-range\ntransFormer, namely PDFormer, for accurate traffic flow prediction.\nSpecifically, we design a spatial self-attention module to capture the dynamic\nspatial dependencies. Then, two graph masking matrices are introduced to\nhighlight spatial dependencies from short- and long-range views. Moreover, a\ntraffic delay-aware feature transformation module is proposed to empower\nPDFormer with the capability of explicitly modeling the time delay of spatial\ninformation propagation. Extensive experimental results on six real-world\npublic traffic datasets show that our method can not only achieve\nstate-of-the-art performance but also exhibit competitive computational\nefficiency. Moreover, we visualize the learned spatial-temporal attention map\nto make our model highly interpretable.\n', '  Spatio-temporal forecasting of traffic flow data represents a typical problem\nin the field of machine learning, impacting urban traffic management systems.\nTraditional statistical and machine learning methods cannot adequately handle\nboth the temporal and spatial dependencies in these complex traffic flow\ndatasets. A prevalent approach in the field is to combine graph convolutional\nnetworks and multi-head attention mechanisms for spatio-temporal processing.\nThis paper proposes a wavelet-based temporal attention model, namely a\nwavelet-based dynamic spatio-temporal aware graph neural network (W-DSTAGNN),\nfor tackling the traffic forecasting problem. Benchmark experiments using\nseveral statistical metrics confirm that our proposal efficiently captures\nspatio-temporal correlations and outperforms ten state-of-the-art models on\nthree different real-world traffic datasets. Our proposed ensemble data-driven\nmethod can handle dynamic temporal and spatial dependencies and make long-term\nforecasts in an efficient manner.\n', ""  Robust prediction of citywide traffic flows at different time periods plays a\ncrucial role in intelligent transportation systems. While previous work has\nmade great efforts to model spatio-temporal correlations, existing methods\nstill suffer from two key limitations: i) Most models collectively predict all\nregions' flows without accounting for spatial heterogeneity, i.e., different\nregions may have skewed traffic flow distributions. ii) These models fail to\ncapture the temporal heterogeneity induced by time-varying traffic patterns, as\nthey typically model temporal correlations with a shared parameterized space\nfor all time periods. To tackle these challenges, we propose a novel\nSpatio-Temporal Self-Supervised Learning (ST-SSL) traffic prediction framework\nwhich enhances the traffic pattern representations to be reflective of both\nspatial and temporal heterogeneity, with auxiliary self-supervised learning\nparadigms. Specifically, our ST-SSL is built over an integrated module with\ntemporal and spatial convolutions for encoding the information across space and\ntime. To achieve the adaptive spatio-temporal self-supervised learning, our\nST-SSL first performs the adaptive augmentation over the traffic flow graph\ndata at both attribute- and structure-levels. On top of the augmented traffic\ngraph, two SSL auxiliary tasks are constructed to supplement the main traffic\nprediction task with spatial and temporal heterogeneity-aware augmentation.\nExperiments on four benchmark datasets demonstrate that ST-SSL consistently\noutperforms various state-of-the-art baselines. Since spatio-temporal\nheterogeneity widely exists in practical datasets, the proposed framework may\nalso cast light on other spatial-temporal applications. Model implementation is\navailable at https://github.com/Echo-Ji/ST-SSL.\n""]",Traffic Flow Prediction with Spatiotemporal Networks,Spatiotemporal Forecasting and Prediction in Transportation Systems,Transportation Systems and Environmental Analytics,Transportation Systems and Environmental Analytics
62,132,62_communities_hypergraph_hypergraphs_subgraph,"['communities', 'hypergraph', 'hypergraphs', 'subgraph', 'graphs', 'cluster', 'nodes', 'adjacency', 'clusters', 'clustering']","['community', 'communities', 'modularity', 'clustering', 'vertices', 'graphs', 'networks', 'hypergraphs', 'centrality', 'nodes']","['  The study of complex networks has significantly advanced our understanding of\ncommunity structures which serves as a crucial feature of real-world graphs.\nDetecting communities in graphs is a challenging problem with applications in\nsociology, biology, and computer science. Despite the efforts of an\ninterdisciplinary community of scientists, a satisfactory solution to this\nproblem has not yet been achieved. This review article delves into the topic of\ncommunity detection in graphs, which serves as a thorough exposition of various\ncommunity detection methods from perspectives of modularity-based method,\nspectral clustering, probabilistic modelling, and deep learning. Along with the\nmethods, a new community detection method designed by us is also presented.\nAdditionally, the performance of these methods on the datasets with and without\nground truth is compared. In conclusion, this comprehensive review provides a\ndeep understanding of community detection in graphs.\n', '  Graph clustering is an important unsupervised learning technique for\npartitioning graphs with attributes and detecting communities. However, current\nmethods struggle to accurately capture true community structures and\nintra-cluster relations, be computationally efficient, and identify smaller\ncommunities. We address these challenges by integrating coarsening and\nmodularity maximization, effectively leveraging both adjacency and node\nfeatures to enhance clustering accuracy. We propose a loss function\nincorporating log-determinant, smoothness, and modularity components using a\nblock majorization-minimization technique, resulting in superior clustering\noutcomes. The method is theoretically consistent under the Degree-Corrected\nStochastic Block Model (DC-SBM), ensuring asymptotic error-free performance and\ncomplete label recovery. Our provably convergent and time-efficient algorithm\nseamlessly integrates with graph neural networks (GNNs) and variational graph\nautoencoders (VGAEs) to learn enhanced node features and deliver exceptional\nclustering performance. Extensive experiments on benchmark datasets demonstrate\nits superiority over existing state-of-the-art methods for both attributed and\nnon-attributed graphs.\n', '  Hypergraphs are a representation of complex systems involving interactions\namong more than two entities and allow to investigation of higher-order\nstructure and dynamics in real-world complex systems. Community structure is a\ncommon property observed in empirical networks in various domains. Stochastic\nblock models have been employed to investigate community structure in networks.\nNode attribute data, often accompanying network data, has been found to\npotentially enhance the learning of community structure in dyadic networks. In\nthis study, we develop a statistical framework that incorporates node attribute\ndata into the learning of community structure in a hypergraph, employing a\nstochastic block model. We demonstrate that our model, which we refer to as\nHyperNEO, enhances the learning of community structure in synthetic and\nempirical hypergraphs when node attributes are sufficiently associated with the\ncommunities. Furthermore, we found that applying a dimensionality reduction\nmethod, UMAP, to the learned representations obtained using stochastic block\nmodels, including our model, maps nodes into a two-dimensional vector space\nwhile largely preserving community structure in empirical hypergraphs. We\nexpect that our framework will broaden the investigation and understanding of\nhigher-order community structure in real-world complex systems.\n']",Community Detection in Graphs and Hypergraphs,Graph Analysis and Processing Techniques,Data Analysis and Pattern Discovery,Data Analysis and Pattern Discovery
63,131,63_radiology_radiological_radiologists_radiologist,"['radiology', 'radiological', 'radiologists', 'radiologist', 'radiologic', 'multimodal', 'textual', 'reporting', 'medical', 'diagnostic']","['radiology', 'reports', 'medical', 'report', 'chest', 'ray', 'radiologists', 'clinical', 'multimodal', 'diagnostic']","[""  GPT-4V's purported strong multimodal abilities raise interests in using it to\nautomate radiology report writing, but there lacks thorough evaluations. In\nthis work, we perform a systematic evaluation of GPT-4V in generating radiology\nreports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt\nto directly generate reports using GPT-4V through different prompting\nstrategies and find that it fails terribly in both lexical metrics and clinical\nefficacy metrics. To understand the low performance, we decompose the task into\ntwo steps: 1) the medical image reasoning step of predicting medical condition\nlabels from images; and 2) the report synthesis step of generating reports from\n(groundtruth) conditions. We show that GPT-4V's performance in image reasoning\nis consistently low across different prompts. In fact, the distributions of\nmodel-predicted labels remain constant regardless of which groundtruth\nconditions are present on the image, suggesting that the model is not\ninterpreting chest X-rays meaningfully. Even when given groundtruth conditions\nin report synthesis, its generated reports are less correct and less\nnatural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt\non the viability of using GPT-4V in a radiology workflow.\n"", ""  Evaluating generated radiology reports is crucial for the development of\nradiology AI, but existing metrics fail to reflect the task's clinical\nrequirements. This study proposes a novel evaluation framework using large\nlanguage models (LLMs) to compare radiology reports for assessment. We compare\nthe performance of various LLMs and demonstrate that, when using GPT-4, our\nproposed metric achieves evaluation consistency close to that of radiologists.\nFurthermore, to reduce costs and improve accessibility, making this method\npractical, we construct a dataset using LLM evaluation results and perform\nknowledge distillation to train a smaller model. The distilled model achieves\nevaluation capabilities comparable to GPT-4. Our framework and distilled model\noffer an accessible and efficient evaluation method for radiology report\ngeneration, facilitating the development of more clinically relevant models.\nThe model will be further open-sourced and accessible.\n"", '  The impression section of a radiology report summarizes important radiology\nfindings and plays a critical role in communicating these findings to\nphysicians. However, the preparation of these summaries is time-consuming and\nerror-prone for radiologists. Recently, numerous models for radiology report\nsummarization have been developed. Nevertheless, there is currently no model\nthat can summarize these reports in multiple languages. Such a model could\ngreatly improve future research and the development of Deep Learning models\nthat incorporate data from patients with different ethnic backgrounds. In this\nstudy, the generation of radiology impressions in different languages was\nautomated by fine-tuning a model, publicly available, based on a multilingual\ntext-to-text Transformer to summarize findings available in English,\nPortuguese, and German radiology reports. In a blind test, two board-certified\nradiologists indicated that for at least 70% of the system-generated summaries,\nthe quality matched or exceeded the corresponding human-written summaries,\nsuggesting substantial clinical reliability. Furthermore, this study showed\nthat the multilingual model outperformed other models that specialized in\nsummarizing radiology reports in only one language, as well as models that were\nnot specifically designed for summarizing radiology reports, such as ChatGPT.\n']",Radiology Report Generation and Evaluation,Radiology Report Generation and Evaluation,Medical Imaging and Reporting,Medical Imaging and Reporting
64,130,64_predicting_sepsis_mortality_hospital,"['predicting', 'sepsis', 'mortality', 'hospital', 'predict', 'prediction', 'cohorts', 'icu', 'biomarkers', 'healthcare']","['patients', 'patient', 'glucose', 'mortality', 'clinical', 'diabetes', 'care', 'sepsis', 'health', 'cohort']","[""  Quantifying a patient's health status provides clinicians with insight into\npatient risk, and the ability to better triage and manage resources. Early\nWarning Scores (EWS) are widely deployed to measure overall health status, and\nrisk of adverse outcomes, in hospital patients. However, current EWS are\nlimited both by their lack of personalisation and use of static observations.\nWe propose a pipeline that groups intensive care unit patients by the\ntrajectories of observations data throughout their stay as a basis for the\ndevelopment of personalised risk predictions. Feature importance is considered\nto provide model explainability. Using the MIMIC-IV dataset, six clusters were\nidentified, capturing differences in disease codes, observations, lengths of\nadmissions and outcomes. Applying the pipeline to data from just the first four\nhours of each ICU stay assigns the majority of patients to the same cluster as\nwhen the entire stay duration is considered. In-hospital mortality prediction\nmodels trained on individual clusters had higher F1 score performance in five\nof the six clusters when compared against the unclustered patient cohort. The\npipeline could form the basis of a clinical decision support tool, working to\nimprove the clinical characterisation of risk groups and the early detection of\npatient deterioration.\n"", ""  Sepsis poses a major global health threat, accounting for millions of deaths\nannually and significant economic costs. Accurate predictions of mortality risk\nin sepsis patients facilitate the efficient allocation of medical resources,\nthereby enhancing patient survival and quality of life. Through precise risk\nassessments, healthcare facilities can effectively distribute intensive care\nbeds, medical equipment, and staff, ensuring high-risk patients receive timely\nand appropriate care. Early identification and intervention significantly\ndecrease mortality rates and improve patient outcomes. Current methods\ntypically utilize only one type of data--either constant, temporal, or ICD\ncodes. This study introduces the Time-Constant KAN Integrated Network(TCKIN),\nan innovative model that enhances the accuracy of sepsis mortality risk\npredictions by integrating both temporal and constant data from electronic\nhealth records and ICD codes. Validated against the MIMIC-III and MIMIC-IV\ndatasets, TCKIN surpasses existing machine learning and deep learning methods\nin accuracy, sensitivity, and specificity. Notably, TCKIN achieved AUCs of\n87.76% and 88.07%, demonstrating superior capability in identifying high-risk\npatients. Additionally, TCKIN effectively combats the prevalent issue of data\nimbalance in clinical settings, improving the detection of patients at elevated\nrisk of mortality and facilitating timely interventions. These results confirm\nthe model's effectiveness and its potential to transform patient management and\ntreatment optimization in clinical practice. With this advanced risk assessment\ntool, healthcare providers can devise more tailored treatment plans, optimize\nresource utilization, and ultimately enhance survival rates and quality of life\nfor sepsis patients.\n"", ""  Background: Sepsis is a severe condition responsible for many deaths\nworldwide. Accurate prediction of sepsis outcomes is crucial for timely and\neffective treatment. Although previous studies have used ML to forecast\noutcomes, they faced limitations in feature selection and model\ncomprehensibility, resulting in less effective predictions. Thus, this research\naims to develop an interpretable and accurate ML model to help clinical\nprofessionals predict in-hospital mortality.\n  Methods: We analyzed ICU patient records from the MIMIC-III database based on\nspecific criteria and extracted relevant data. Our feature selection process\nincluded a literature review, clinical input refinement, and using Random\nForest to select the top 35 features. We performed data preprocessing,\nincluding cleaning, imputation, standardization, and applied SMOTE for\noversampling to address imbalance, resulting in 4,683 patients, with admission\ncounts of 17,429. We compared the performance of Random Forest, Gradient\nBoosting, Logistic Regression, SVM, and KNN models.\n  Results: The Random Forest model was the most effective in predicting\nsepsis-related in-hospital mortality. It outperformed other models, achieving\nan accuracy of 0.90 and an AUROC of 0.97, significantly better than the\nexisting literature. Our meticulous feature selection contributed to the\nmodel's precision and identified critical determinants of sepsis mortality.\nThese results underscore the pivotal role of data-driven ML in healthcare,\nespecially for predicting in-hospital mortality due to sepsis.\n  Conclusion: This study represents a significant advancement in predicting\nin-hospital sepsis mortality, highlighting the potential of ML in healthcare.\nThe implications are profound, offering a data-driven approach that enhances\ndecision-making in patient care and reduces in-hospital mortality.\n""]",Sepsis Mortality Prediction in ICU Patients,Sepsis Management and Treatment Optimization,Sepsis Management and Treatment Optimization,Sepsis Management and Treatment Optimization
65,128,65_optimality_optimal_reinforcement_mdps,"['optimality', 'optimal', 'reinforcement', 'mdps', 'reward', 'mdp', 'complexity', 'markov', 'minimax', 'discount']","['policy', 'horizon', 'regret', 'reward', 'gradient', 'bound', 'risk', 'sample', 'convergence', 'decision']","['  We study Markov potential games under the infinite horizon average reward\ncriterion. Most previous studies have been for discounted rewards. We prove\nthat both algorithms based on independent policy gradient and independent\nnatural policy gradient converge globally to a Nash equilibrium for the average\nreward criterion. To set the stage for gradient-based methods, we first\nestablish that the average reward is a smooth function of policies and provide\nsensitivity bounds for the differential value functions, under certain\nconditions on ergodicity and the second largest eigenvalue of the underlying\nMarkov decision process (MDP). We prove that three algorithms, policy gradient,\nproximal-Q, and natural policy gradient (NPG), converge to an $\\epsilon$-Nash\nequilibrium with time complexity $O(\\frac{1}{\\epsilon^2})$, given a\ngradient/differential Q function oracle. When policy gradients have to be\nestimated, we propose an algorithm with\n$\\tilde{O}(\\frac{1}{\\min_{s,a}\\pi(a|s)\\delta})$ sample complexity to achieve\n$\\delta$ approximation error w.r.t~the $\\ell_2$ norm. Equipped with the\nestimator, we derive the first sample complexity analysis for a policy gradient\nascent algorithm, featuring a sample complexity of $\\tilde{O}(1/\\epsilon^5)$.\nSimulation studies are presented.\n', '  We study the sample complexity of learning an $\\varepsilon$-optimal policy in\nan average-reward Markov decision process (MDP) under a generative model. We\nestablish the complexity bound $\\widetilde{O}\\left(SA\\frac{H}{\\varepsilon^2}\n\\right)$, where $H$ is the span of the bias function of the optimal policy and\n$SA$ is the cardinality of the state-action space. Our result is the first that\nis minimax optimal (up to log factors) in all parameters $S,A,H$ and\n$\\varepsilon$, improving on existing work that either assumes uniformly bounded\nmixing times for all policies or has suboptimal dependence on the parameters.\n  Our result is based on reducing the average-reward MDP to a discounted MDP.\nTo establish the optimality of this reduction, we develop improved bounds for\n$\\gamma$-discounted MDPs, showing that\n$\\widetilde{O}\\left(SA\\frac{H}{(1-\\gamma)^2\\varepsilon^2} \\right)$ samples\nsuffice to learn a $\\varepsilon$-optimal policy in weakly communicating MDPs\nunder the regime that $\\gamma \\geq 1 - \\frac{1}{H}$, circumventing the\nwell-known lower bound of\n$\\widetilde{\\Omega}\\left(SA\\frac{1}{(1-\\gamma)^3\\varepsilon^2} \\right)$ for\ngeneral $\\gamma$-discounted MDPs. Our analysis develops upper bounds on certain\ninstance-dependent variance parameters in terms of the span parameter. These\nbounds are tighter than those based on the mixing time or diameter of the MDP\nand may be of broader use.\n', '  We study the sample complexity of learning an $\\varepsilon$-optimal policy in\nan average-reward Markov decision process (MDP) under a generative model. For\nweakly communicating MDPs, we establish the complexity bound\n$\\widetilde{O}(SA\\frac{H}{\\varepsilon^2} )$, where $H$ is the span of the bias\nfunction of the optimal policy and $SA$ is the cardinality of the state-action\nspace. Our result is the first that is minimax optimal (up to log factors) in\nall parameters $S,A,H$, and $\\varepsilon$, improving on existing work that\neither assumes uniformly bounded mixing times for all policies or has\nsuboptimal dependence on the parameters. We also initiate the study of sample\ncomplexity in general (multichain) average-reward MDPs. We argue a new\ntransient time parameter $B$ is necessary, establish an\n$\\widetilde{O}(SA\\frac{B + H}{\\varepsilon^2})$ complexity bound, and prove a\nmatching (up to log factors) minimax lower bound. Both results are based on\nreducing the average-reward MDP to a discounted MDP, which requires new ideas\nin the general setting. To optimally analyze this reduction, we develop\nimproved bounds for $\\gamma$-discounted MDPs, showing that\n$\\widetilde{O}(SA\\frac{H}{(1-\\gamma)^2\\varepsilon^2} )$ and\n$\\widetilde{O}(SA\\frac{B + H}{(1-\\gamma)^2\\varepsilon^2} )$ samples suffice to\nlearn $\\varepsilon$-optimal policies in weakly communicating and in general\nMDPs, respectively. Both these results circumvent the well-known minimax lower\nbound of $\\widetilde{\\Omega}(SA\\frac{1}{(1-\\gamma)^3\\varepsilon^2} )$ for\n$\\gamma$-discounted MDPs, and establish a quadratic rather than cubic horizon\ndependence for a fixed MDP instance.\n']",Optimality and Complexity in Markov Decision Processes,Decision Making under Uncertainty in Markov Decision Processes,Decision Making and Optimization under Uncertainty,Decision Making and Optimization under Uncertainty
66,128,66_quantization_quantizing_quantize_quantized,"['quantization', 'quantizing', 'quantize', 'quantized', 'memory', 'compression', 'compressed', 'weights', 'compressing', 'bits']","['quantization', 'bit', 'weight', 'weights', 'precision', 'bits', 'memory', 'activation', 'outliers', 'low']","[""  The growing demand for Large Language Models (LLMs) in applications such as\ncontent generation, intelligent chatbots, and sentiment analysis poses\nconsiderable challenges for LLM service providers. To efficiently use GPU\nresources and boost throughput, batching multiple requests has emerged as a\npopular paradigm; to further speed up batching, LLM quantization techniques\nreduce memory consumption and increase computing capacity. However, prevalent\nquantization schemes (e.g., 8-bit weight-activation quantization) cannot fully\nleverage the capabilities of modern GPUs, such as 4-bit integer operators,\nresulting in sub-optimal performance.\n  To maximize LLMs' serving throughput, we introduce Atom, a low-bit\nquantization method that achieves high throughput improvements with negligible\naccuracy loss. Atom significantly boosts serving throughput by using low-bit\noperators and considerably reduces memory consumption via low-bit quantization.\nIt attains high accuracy by applying a novel mixed-precision and fine-grained\nquantization process. We evaluate Atom on 4-bit weight-activation quantization\nin the serving context. Atom improves end-to-end throughput (token/s) by up to\n$7.7\\times$ compared to the FP16 and by $2.5\\times$ compared to INT8\nquantization, while maintaining the same latency target.\n"", '  Quantization has emerged as a promising technique for improving the memory\nand computational efficiency of large language models (LLMs). Though the\ntrade-off between performance and efficiency is well-known, there is still much\nto be learned about the relationship between quantization and LLM performance.\nTo shed light on this relationship, we propose a new perspective on\nquantization, viewing it as perturbations added to the weights and activations\nof LLMs. We call this approach ""the lens of perturbation"". Using this lens, we\nconduct experiments with various artificial perturbations to explore their\nimpact on LLM performance. Our findings reveal several connections between the\nproperties of perturbations and LLM performance, providing insights into the\nfailure cases of uniform quantization and suggesting potential solutions to\nimprove the robustness of LLM quantization. To demonstrate the significance of\nour findings, we implement a simple non-uniform quantization approach based on\nour insights. Our experiments show that this approach achieves minimal\nperformance degradation on both 4-bit weight quantization and 8-bit\nquantization for weights and activations. These results validate the\ncorrectness of our approach and highlight its potential to improve the\nefficiency of LLMs without sacrificing performance.\n', ""  Due to the high memory and computational costs associated with Large Language\nModels, model compression via quantization and parameter-efficient fine-tuning\n(PEFT) methods, such as low-rank adaptation (LoRA), are gaining popularity.\nThis has led to active research on quantization-aware PEFT techniques, which\naim to create models with high accuracy and low memory overhead. Among\nquantization methods, post-training quantization (PTQ) is more commonly used in\nprevious works than quantization-aware training (QAT), despite QAT's potential\nfor higher accuracy. This preference is due to PTQ's low training overhead.\nHowever, PTQ-based PEFT methods often utilize high-precision parameters, making\nit difficult to fully exploit the efficiency of quantization. Additionally,\nthey have limited adaptation ability due to a reduced and constrained LoRA\nparameter structure. To overcome these challenges, we propose L4Q, which\nleverages joint quantization and fine-tuning to reduce QAT's memory overhead\nand produce models that consist entirely of quantized weights while achieving\neffective adaptation to downstream tasks. By design, L4Q allows quantization\nparameters to reflect weight updates, while weight updates reduce quantization\nerrors. Our experiments demonstrate that this coupled quantization and\nfine-tuning approach yields superior accuracy compared to decoupled fine-tuning\nschemes in sub-4-bit quantization. Using the LLaMA model families and\ninstructional datasets, we showcase L4Q's capabilities in language tasks and\nfew-shot in-context learning.\n""]",Quantization Techniques for Large Language Models,Optimization and Efficiency of Large Language Models,Large Language Models,Large Language Models
66,128,66_quantization_quantizing_quantize_quantized,"['quantization', 'quantizing', 'quantize', 'quantized', 'memory', 'compression', 'compressed', 'weights', 'compressing', 'bits']","['quantization', 'bit', 'weight', 'weights', 'precision', 'bits', 'memory', 'activation', 'outliers', 'low']","[""  The growing demand for Large Language Models (LLMs) in applications such as\ncontent generation, intelligent chatbots, and sentiment analysis poses\nconsiderable challenges for LLM service providers. To efficiently use GPU\nresources and boost throughput, batching multiple requests has emerged as a\npopular paradigm; to further speed up batching, LLM quantization techniques\nreduce memory consumption and increase computing capacity. However, prevalent\nquantization schemes (e.g., 8-bit weight-activation quantization) cannot fully\nleverage the capabilities of modern GPUs, such as 4-bit integer operators,\nresulting in sub-optimal performance.\n  To maximize LLMs' serving throughput, we introduce Atom, a low-bit\nquantization method that achieves high throughput improvements with negligible\naccuracy loss. Atom significantly boosts serving throughput by using low-bit\noperators and considerably reduces memory consumption via low-bit quantization.\nIt attains high accuracy by applying a novel mixed-precision and fine-grained\nquantization process. We evaluate Atom on 4-bit weight-activation quantization\nin the serving context. Atom improves end-to-end throughput (token/s) by up to\n$7.7\\times$ compared to the FP16 and by $2.5\\times$ compared to INT8\nquantization, while maintaining the same latency target.\n"", '  Quantization has emerged as a promising technique for improving the memory\nand computational efficiency of large language models (LLMs). Though the\ntrade-off between performance and efficiency is well-known, there is still much\nto be learned about the relationship between quantization and LLM performance.\nTo shed light on this relationship, we propose a new perspective on\nquantization, viewing it as perturbations added to the weights and activations\nof LLMs. We call this approach ""the lens of perturbation"". Using this lens, we\nconduct experiments with various artificial perturbations to explore their\nimpact on LLM performance. Our findings reveal several connections between the\nproperties of perturbations and LLM performance, providing insights into the\nfailure cases of uniform quantization and suggesting potential solutions to\nimprove the robustness of LLM quantization. To demonstrate the significance of\nour findings, we implement a simple non-uniform quantization approach based on\nour insights. Our experiments show that this approach achieves minimal\nperformance degradation on both 4-bit weight quantization and 8-bit\nquantization for weights and activations. These results validate the\ncorrectness of our approach and highlight its potential to improve the\nefficiency of LLMs without sacrificing performance.\n', ""  Due to the high memory and computational costs associated with Large Language\nModels, model compression via quantization and parameter-efficient fine-tuning\n(PEFT) methods, such as low-rank adaptation (LoRA), are gaining popularity.\nThis has led to active research on quantization-aware PEFT techniques, which\naim to create models with high accuracy and low memory overhead. Among\nquantization methods, post-training quantization (PTQ) is more commonly used in\nprevious works than quantization-aware training (QAT), despite QAT's potential\nfor higher accuracy. This preference is due to PTQ's low training overhead.\nHowever, PTQ-based PEFT methods often utilize high-precision parameters, making\nit difficult to fully exploit the efficiency of quantization. Additionally,\nthey have limited adaptation ability due to a reduced and constrained LoRA\nparameter structure. To overcome these challenges, we propose L4Q, which\nleverages joint quantization and fine-tuning to reduce QAT's memory overhead\nand produce models that consist entirely of quantized weights while achieving\neffective adaptation to downstream tasks. By design, L4Q allows quantization\nparameters to reflect weight updates, while weight updates reduce quantization\nerrors. Our experiments demonstrate that this coupled quantization and\nfine-tuning approach yields superior accuracy compared to decoupled fine-tuning\nschemes in sub-4-bit quantization. Using the LLaMA model families and\ninstructional datasets, we showcase L4Q's capabilities in language tasks and\nfew-shot in-context learning.\n""]",Quantization Techniques for Large Language Models,Optimization and Efficiency of Large Language Models,Large Language Models,Large Language Models
67,127,67_programming_tutoring_tutor_tutors,"['programming', 'tutoring', 'tutor', 'tutors', 'chatbots', 'learners', 'students', 'pedagogical', 'educational', 'student']","['students', 'education', 'student', 'programming', 'educational', 'tutoring', 'course', 'tutors', 'teaching', 'tutor']","[""  The application of Artificial intelligence for teaching and learning in the\nacademic sphere is a trending subject of interest in the computing education.\nChatGPT, as an AI-based tool, provides various advantages, such as heightened\nstudent involvement, cooperation, accessibility and availability. This paper\naddresses the prospects and obstacles associated with utilizing ChatGPT as a\ntool for learning and assessment in undergraduate Computer Science curriculum\nin particular to teaching and learning fundamental programming courses.\nStudents having completed the course work for a Data Structures and Algorithms\n(a sophomore level course) participated in this study. Two groups of students\nwere given programming challenges to solve within a short period of time. The\ncontrol group (group A) had access to text books and notes of programming\ncourses, however no Internet access was provided. Group B students were given\naccess to ChatGPT and were encouraged to use it to help solve the programming\nchallenges. The challenge was conducted in a computer lab environment using PC2\nenvironment. Each team of students address the problem by writing executable\ncode that satisfies certain number of test cases. Student teams were scored\nbased on their performance in terms of number of successful passed testcases.\nResults show that students using ChatGPT had an advantage in terms of earned\nscores, however there were inconsistencies and inaccuracies in the submitted\ncode consequently affecting the overall performance. After a thorough analysis,\nthe paper's findings indicate that incorporating AI in higher education brings\nabout various opportunities and challenges.\n"", ""  This research paper contributes to the computing education research\ncommunity's understanding of Generative AI (GenAI) in the context of\nintroductory programming, and specifically, how students utilize related tools,\nsuch as ChatGPT. An increased understanding of students' use is mandatory for\neducators and higher education institutions, as GenAI is here to stay, and its\nperformance is likely to improve rapidly in the near future. Learning about\nstudents' use patterns is not only crucial to support their learning, but to\ndevelop adequate forms of instruction and assessment. With the rapid\nadvancement of AI, its broad availability, and ubiquitous presence in\neducational environments, elaborating how AI can enhance learning experiences,\nespecially in courses such as introductory programming is important. To date,\nmost studies have focused on the educator's perspective on GenAI, its\nperformance, characteristics, and limitations. However, the student\nperspective, and how they actually use GenAI tools in course contexts, has not\nbeen subject to a great number of studies. Therefore, this study is guided by\nthe following research questions: (1) What do students report on their use\npattern of ChatGPT in the context of introductory programming exercises? and\n(2) How do students perceive ChatGPT in the context of introductory programming\nexercises? To address these questions, computing students at a large German\nuniversity were asked to solve programming tasks with the assistance of ChatGPT\nas part of their introductory programming course. Students (n=298) provided\ninformation regarding the use of ChatGPT, and their evaluation of the tool via\nan online survey. This research provides a comprehensive evaluation of\nChatGPT-3.5's application by novice programmers in a higher education\ncontext...\n"", ""  The integration of ChatGPT as a supportive tool in education, notably in\nprogramming courses, addresses the unique challenges of programming education\nby providing assistance with debugging, code generation, and explanations.\nDespite existing research validating ChatGPT's effectiveness, its application\nin university-level programming education and a detailed understanding of\nstudent interactions and perspectives remain limited. This paper explores\nChatGPT's impact on learning in a Python programming course tailored for\nfirst-year students over eight weeks. By analyzing responses from surveys,\nopen-ended questions, and student-ChatGPT dialog data, we aim to provide a\ncomprehensive view of ChatGPT's utility and identify both its advantages and\nlimitations as perceived by students. Our study uncovers a generally positive\nreception toward ChatGPT and offers insights into its role in enhancing the\nprogramming education experience. These findings contribute to the broader\ndiscourse on AI's potential in education, suggesting paths for future research\nand application.\n""]",ChatGPT in Programming Education,Artificial Intelligence in Education,Artificial Intelligence in Education,Artificial Intelligence in Education
68,125,68_hallucination_hallucinations_hallucinating_hallucinate,"['hallucination', 'hallucinations', 'hallucinating', 'hallucinate', 'hallucinatory', 'hallucinated', 'decoding', 'annotator', 'text', 'halludial']","['hallucination', 'hallucinations', 'factual', 'truthfulness', 'factuality', 'detection', 'generations', 'hallucinatory', 'facts', 'outputs']","['  Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval.\n', '  Large Language Models (LLMs) have shown propensity to generate hallucinated\noutputs, i.e., texts that are factually incorrect or unsupported. Existing\nmethods for alleviating hallucinations typically require costly human\nannotations to identify and correct hallucinations in LLM outputs. Moreover,\nmost of these methods focus on a specific type of hallucination, e.g., entity\nor token errors, which limits their effectiveness in addressing various types\nof hallucinations exhibited in LLM outputs. To our best knowledge, in this\npaper we propose the first active learning framework to alleviate LLM\nhallucinations, reducing costly human annotations of hallucination needed. By\nmeasuring fine-grained hallucinations from errors in semantic frame, discourse\nand content verifiability in text summarization, we propose HAllucination\nDiversity-Aware Sampling (HADAS) to select diverse hallucinations for\nannotations in active learning for LLM finetuning. Extensive experiments on\nthree datasets and different backbone models demonstrate advantages of our\nmethod in effectively and efficiently mitigating LLM hallucinations.\n', '  In the era of large language models (LLMs), hallucination (i.e., the tendency\nto generate factually incorrect content) poses great challenge to trustworthy\nand reliable deployment of LLMs in real-world applications. To tackle the LLM\nhallucination, three key questions should be well studied: how to detect\nhallucinations (detection), why do LLMs hallucinate (source), and what can be\ndone to mitigate them (mitigation). To address these challenges, this work\npresents a systematic empirical study on LLM hallucination, focused on the the\nthree aspects of hallucination detection, source and mitigation. Specially, we\nconstruct a new hallucination benchmark HaluEval 2.0, and designs a simple yet\neffective detection method for LLM hallucination. Furthermore, we zoom into the\ndifferent training or utilization stages of LLMs and extensively analyze the\npotential factors that lead to the LLM hallucination. Finally, we implement and\nexamine a series of widely used techniques to mitigate the hallucinations in\nLLMs. Our work has led to several important findings to understand the\nhallucination origin and mitigate the hallucinations in LLMs. Our code and data\ncan be accessed at https://github.com/RUCAIBox/HaluEval-2.0.\n']",Large Language Model Hallucination Detection and Mitigation,Mitigating Hallucinations in Large Language and Vision-Language Models,Large Language Models,Large Language Models
68,125,68_hallucination_hallucinations_hallucinating_hallucinate,"['hallucination', 'hallucinations', 'hallucinating', 'hallucinate', 'hallucinatory', 'hallucinated', 'decoding', 'annotator', 'text', 'halludial']","['hallucination', 'hallucinations', 'factual', 'truthfulness', 'factuality', 'detection', 'generations', 'hallucinatory', 'facts', 'outputs']","['  Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval.\n', '  Large Language Models (LLMs) have shown propensity to generate hallucinated\noutputs, i.e., texts that are factually incorrect or unsupported. Existing\nmethods for alleviating hallucinations typically require costly human\nannotations to identify and correct hallucinations in LLM outputs. Moreover,\nmost of these methods focus on a specific type of hallucination, e.g., entity\nor token errors, which limits their effectiveness in addressing various types\nof hallucinations exhibited in LLM outputs. To our best knowledge, in this\npaper we propose the first active learning framework to alleviate LLM\nhallucinations, reducing costly human annotations of hallucination needed. By\nmeasuring fine-grained hallucinations from errors in semantic frame, discourse\nand content verifiability in text summarization, we propose HAllucination\nDiversity-Aware Sampling (HADAS) to select diverse hallucinations for\nannotations in active learning for LLM finetuning. Extensive experiments on\nthree datasets and different backbone models demonstrate advantages of our\nmethod in effectively and efficiently mitigating LLM hallucinations.\n', '  In the era of large language models (LLMs), hallucination (i.e., the tendency\nto generate factually incorrect content) poses great challenge to trustworthy\nand reliable deployment of LLMs in real-world applications. To tackle the LLM\nhallucination, three key questions should be well studied: how to detect\nhallucinations (detection), why do LLMs hallucinate (source), and what can be\ndone to mitigate them (mitigation). To address these challenges, this work\npresents a systematic empirical study on LLM hallucination, focused on the the\nthree aspects of hallucination detection, source and mitigation. Specially, we\nconstruct a new hallucination benchmark HaluEval 2.0, and designs a simple yet\neffective detection method for LLM hallucination. Furthermore, we zoom into the\ndifferent training or utilization stages of LLMs and extensively analyze the\npotential factors that lead to the LLM hallucination. Finally, we implement and\nexamine a series of widely used techniques to mitigate the hallucinations in\nLLMs. Our work has led to several important findings to understand the\nhallucination origin and mitigate the hallucinations in LLMs. Our code and data\ncan be accessed at https://github.com/RUCAIBox/HaluEval-2.0.\n']",Large Language Model Hallucination Detection and Mitigation,Mitigating Hallucinations in Large Language and Vision-Language Models,Large Language Models,Large Language Models
69,124,69_convnets_cnn_imagenet_cnns,"['convnets', 'cnn', 'imagenet', 'cnns', 'convnet', 'convolutions', 'attention', 'convolutional', 'vision', 'visual']","['vision', 'transformer', 'attention', 'transformers', 'token', 'channel', 'segmentation', 'convolution', 'architectures', 'computer']","['  Hierarchical vision transformers (ViTs) have two advantages over conventional\nViTs. First, hierarchical ViTs achieve linear computational complexity with\nrespect to image size by local self-attention. Second, hierarchical ViTs create\nhierarchical feature maps by merging image patches in deeper layers for dense\nprediction. However, existing pruning methods ignore the unique properties of\nhierarchical ViTs and use the magnitude value as the weight importance. This\napproach leads to two main drawbacks. First, the ""local"" attention weights are\ncompared at a ""global"" level, which may cause some ""locally"" important weights\nto be pruned due to their relatively small magnitude ""globally"". The second\nissue with magnitude pruning is that it fails to consider the distinct weight\ndistributions of the network, which are essential for extracting coarse to\nfine-grained features at various hierarchical levels.\n  To solve the aforementioned issues, we have developed a Data-independent\nModule-Aware Pruning method (DIMAP) to compress hierarchical ViTs. To ensure\nthat ""local"" attention weights at different hierarchical levels are compared\nfairly in terms of their contribution, we treat them as a module and examine\ntheir contribution by analyzing their information distortion. Furthermore, we\nintroduce a novel weight metric that is solely based on weights and does not\nrequire input images, thereby eliminating the dependence on the patch merging\nprocess. Our method validates its usefulness and strengths on Swin Transformers\nof different sizes on ImageNet-1k classification. Notably, the top-5 accuracy\ndrop is only 0.07% when we remove 52.5% FLOPs and 52.7% parameters of Swin-B.\nWhen we reduce 33.2% FLOPs and 33.2% parameters of Swin-S, we can even achieve\na 0.8% higher relative top-5 accuracy than the original model. Code is\navailable at: https://github.com/he-y/Data-independent-Module-Aware-Pruning\n', '  Vision Transformer (ViT) has emerged as a prominent architecture for various\ncomputer vision tasks. In ViT, we divide the input image into patch tokens and\nprocess them through a stack of self attention blocks. However, unlike\nConvolutional Neural Networks (CNN), ViTs simple architecture has no\ninformative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a\nlarge amount of data for pre-training. Various data efficient approaches (DeiT)\nhave been proposed to train ViT on balanced datasets effectively. However,\nlimited literature discusses the use of ViT for datasets with long-tailed\nimbalances. In this work, we introduce DeiT-LT to tackle the problem of\ntraining ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an\nefficient and effective way of distillation from CNN via distillation DIST\ntoken by using out-of-distribution images and re-weighting the distillation\nloss to enhance focus on tail classes. This leads to the learning of local\nCNN-like features in early ViT blocks, improving generalization for tail\nclasses. Further, to mitigate overfitting, we propose distilling from a flat\nCNN teacher, which leads to learning low-rank generalizable features for DIST\ntokens across all ViT blocks. With the proposed DeiT-LT scheme, the\ndistillation DIST token becomes an expert on the tail classes, and the\nclassifier CLS token becomes an expert on the head classes. The experts help to\neffectively learn features corresponding to both the majority and minority\nclasses using a distinct set of tokens within the same ViT architecture. We\nshow the effectiveness of DeiT-LT for training ViT from scratch on datasets\nranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018.\n', '  Attention-based vision models, such as Vision Transformer (ViT) and its\nvariants, have shown promising performance in various computer vision tasks.\nHowever, these emerging architectures suffer from large model sizes and high\ncomputational costs, calling for efficient model compression solutions. To\ndate, pruning ViTs has been well studied, while other compression strategies\nthat have been widely applied in CNN compression, e.g., model factorization, is\nlittle explored in the context of ViT compression. This paper explores an\nefficient method for compressing vision transformers to enrich the toolset for\nobtaining compact attention-based vision models. Based on the new insight on\nthe multi-head attention layer, we develop a highly efficient ViT compression\nsolution, which outperforms the state-of-the-art pruning methods. For\ncompressing DeiT-small and DeiT-base models on ImageNet, our proposed approach\ncan achieve 0.45% and 0.76% higher top-1 accuracy even with fewer parameters.\nOur finding can also be applied to improve the customization efficiency of\ntext-to-image diffusion models, with much faster training (up to $2.6\\times$\nspeedup) and lower extra storage cost (up to $1927.5\\times$ reduction) than the\nexisting works.\n']",Vision Transformers and Efficient Compression Methods,Image and Video Compression Techniques,Image and Video Processing,Image and Video Processing
70,122,70_optimizing_optimize_optimization_optimisation,"['optimizing', 'optimize', 'optimization', 'optimisation', 'optimal', 'hyperparameters', 'optimum', 'hyperparameter', 'algorithms', 'prior']","['optimization', 'acquisition', 'black', 'box', 'function', 'functions', 'surrogate', 'design', 'optimisation', 'objective']","[""  There has been a long-standing and widespread belief that Bayesian\nOptimization (BO) with standard Gaussian process (GP), referred to as standard\nBO, is ineffective in high-dimensional optimization problems. While this belief\nsounds reasonable, strong empirical evidence is lacking. In this paper, we\nsystematically investigated BO with standard GP regression across a variety of\nsynthetic and real-world benchmark problems for high-dimensional optimization.\nWe found that, surprisingly, when using Mat\\'ern kernels and Upper Confidence\nBound (UCB), standard BO consistently achieves top-tier performance, often\noutperforming other BO methods specifically designed for high-dimensional\noptimization. Contrary to the stereotype, we found that standard GP equipped\nwith Mat\\'ern kernels can serve as a capable surrogate for learning\nhigh-dimensional functions. Without strong structural assumptions, BO with\nstandard GP not only excels in high-dimensional optimization but also is robust\nin accommodating various structures within target functions. Furthermore, with\nstandard GP, achieving promising optimization performance is possible via\nmaximum a posterior (MAP) estimation with diffuse priors or merely maximum\nlikelihood estimation, eliminating the need for expensive Markov-Chain Monte\nCarlo (MCMC) sampling that might be required by more complex surrogate models.\nIn parallel, we also investigated and analyzed alternative popular settings in\nrunning standard BO, which, however, often fail in high-dimensional\noptimization. This might link to the a few failure cases reported in\nliterature. We thus advocate for a re-evaluation and in-depth study of the\npotential of standard BO in addressing high-dimensional problems.\n"", '  Bayesian Optimization (BO) is a method for globally optimizing black-box\nfunctions. While BO has been successfully applied to many scenarios, developing\neffective BO algorithms that scale to functions with high-dimensional domains\nis still a challenge. Optimizing such functions by vanilla BO is extremely\ntime-consuming. Alternative strategies for high-dimensional BO that are based\non the idea of embedding the high-dimensional space to the one with low\ndimension are sensitive to the choice of the embedding dimension, which needs\nto be pre-specified. We develop a new computationally efficient\nhigh-dimensional BO method that exploits variable selection. Our method is able\nto automatically learn axis-aligned sub-spaces, i.e. spaces containing selected\nvariables, without the demand of any pre-specified hyperparameters. We\ntheoretically analyze the computational complexity of our algorithm and derive\nthe regret bound. We empirically show the efficacy of our method on several\nsynthetic and real problems.\n', ""  Gaussian process (GP) based Bayesian optimization (BO) is a powerful method\nfor optimizing black-box functions efficiently. The practical performance and\ntheoretical guarantees of this approach depend on having the correct GP\nhyperparameter values, which are usually unknown in advance and need to be\nestimated from the observed data. However, in practice, these estimations could\nbe incorrect due to biased data sampling strategies used in BO. This can lead\nto degraded performance and break the sub-linear global convergence guarantee\nof BO. To address this issue, we propose a new BO method that can sub-linearly\nconverge to the objective function's global optimum even when the true GP\nhyperparameters are unknown in advance and need to be estimated from the\nobserved data. Our method uses a multi-armed bandit technique (EXP3) to add\nrandom data points to the BO process, and employs a novel training loss\nfunction for the GP hyperparameter estimation process that ensures consistent\nestimation. We further provide theoretical analysis of our proposed method.\nFinally, we demonstrate empirically that our method outperforms existing\napproaches on various synthetic and real-world problems.\n""]",Bayesian Optimization for High-Dimensional Problems,Optimization Methods and Algorithms,Optimization and Design,Optimization and Design
71,122,71_context_learning_icl_learn,"['context', 'learning', 'icl', 'learn', 'retrieval', 'examples', 'example', 'ica', 'nlp', 'language']","['demonstrations', 'demonstration', 'context', 'examples', 'shot', 'label', 'selection', 'task', 'tasks', 'learning']","['  It has long been assumed that the sheer number of parameters in large\nlanguage models (LLMs) drives in-context learning (ICL) capabilities, enabling\nremarkable performance improvements by leveraging task-specific demonstrations.\nChallenging this hypothesis, we introduce DEEP-ICL, a novel task Definition\nEnriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts\ntask definitions from given demonstrations and generates responses through\nlearning task-specific examples. We argue that improvement from ICL does not\ndirectly rely on model size, but essentially stems from understanding task\ndefinitions and task-guided learning. Inspired by this, DEEP-ICL combines two\n3B models with distinct roles (one for concluding task definitions and the\nother for learning task demonstrations) and achieves comparable performance to\nLLaMA2-13B. Furthermore, our framework outperforms conventional ICL by\novercoming pretraining sequence length limitations, by supporting unlimited\ndemonstrations. We contend that DEEP-ICL presents a novel alternative for\nachieving efficient few-shot learning, extending beyond the conventional ICL.\n', '  With the increasing capabilities of large language models (LLMs), in-context\nlearning (ICL) has emerged as a new paradigm for natural language processing\n(NLP), where LLMs make predictions based on contexts augmented with a few\nexamples. It has been a significant trend to explore ICL to evaluate and\nextrapolate the ability of LLMs. In this paper, we aim to survey and summarize\nthe progress and challenges of ICL. We first present a formal definition of ICL\nand clarify its correlation to related studies. Then, we organize and discuss\nadvanced techniques, including training strategies, prompt designing\nstrategies, and related analysis. Additionally, we explore various ICL\napplication scenarios, such as data engineering and knowledge updating.\nFinally, we address the challenges of ICL and suggest potential directions for\nfurther research. We hope that our work can encourage more research on\nuncovering how ICL works and improving ICL.\n', '  The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.\n']",In-Context Learning in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
71,122,71_context_learning_icl_learn,"['context', 'learning', 'icl', 'learn', 'retrieval', 'examples', 'example', 'ica', 'nlp', 'language']","['demonstrations', 'demonstration', 'context', 'examples', 'shot', 'label', 'selection', 'task', 'tasks', 'learning']","['  It has long been assumed that the sheer number of parameters in large\nlanguage models (LLMs) drives in-context learning (ICL) capabilities, enabling\nremarkable performance improvements by leveraging task-specific demonstrations.\nChallenging this hypothesis, we introduce DEEP-ICL, a novel task Definition\nEnriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts\ntask definitions from given demonstrations and generates responses through\nlearning task-specific examples. We argue that improvement from ICL does not\ndirectly rely on model size, but essentially stems from understanding task\ndefinitions and task-guided learning. Inspired by this, DEEP-ICL combines two\n3B models with distinct roles (one for concluding task definitions and the\nother for learning task demonstrations) and achieves comparable performance to\nLLaMA2-13B. Furthermore, our framework outperforms conventional ICL by\novercoming pretraining sequence length limitations, by supporting unlimited\ndemonstrations. We contend that DEEP-ICL presents a novel alternative for\nachieving efficient few-shot learning, extending beyond the conventional ICL.\n', '  With the increasing capabilities of large language models (LLMs), in-context\nlearning (ICL) has emerged as a new paradigm for natural language processing\n(NLP), where LLMs make predictions based on contexts augmented with a few\nexamples. It has been a significant trend to explore ICL to evaluate and\nextrapolate the ability of LLMs. In this paper, we aim to survey and summarize\nthe progress and challenges of ICL. We first present a formal definition of ICL\nand clarify its correlation to related studies. Then, we organize and discuss\nadvanced techniques, including training strategies, prompt designing\nstrategies, and related analysis. Additionally, we explore various ICL\napplication scenarios, such as data engineering and knowledge updating.\nFinally, we address the challenges of ICL and suggest potential directions for\nfurther research. We hope that our work can encourage more research on\nuncovering how ICL works and improving ICL.\n', '  The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.\n']",In-Context Learning in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
71,122,71_context_learning_icl_learn,"['context', 'learning', 'icl', 'learn', 'retrieval', 'examples', 'example', 'ica', 'nlp', 'language']","['demonstrations', 'demonstration', 'context', 'examples', 'shot', 'label', 'selection', 'task', 'tasks', 'learning']","['  It has long been assumed that the sheer number of parameters in large\nlanguage models (LLMs) drives in-context learning (ICL) capabilities, enabling\nremarkable performance improvements by leveraging task-specific demonstrations.\nChallenging this hypothesis, we introduce DEEP-ICL, a novel task Definition\nEnriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts\ntask definitions from given demonstrations and generates responses through\nlearning task-specific examples. We argue that improvement from ICL does not\ndirectly rely on model size, but essentially stems from understanding task\ndefinitions and task-guided learning. Inspired by this, DEEP-ICL combines two\n3B models with distinct roles (one for concluding task definitions and the\nother for learning task demonstrations) and achieves comparable performance to\nLLaMA2-13B. Furthermore, our framework outperforms conventional ICL by\novercoming pretraining sequence length limitations, by supporting unlimited\ndemonstrations. We contend that DEEP-ICL presents a novel alternative for\nachieving efficient few-shot learning, extending beyond the conventional ICL.\n', '  With the increasing capabilities of large language models (LLMs), in-context\nlearning (ICL) has emerged as a new paradigm for natural language processing\n(NLP), where LLMs make predictions based on contexts augmented with a few\nexamples. It has been a significant trend to explore ICL to evaluate and\nextrapolate the ability of LLMs. In this paper, we aim to survey and summarize\nthe progress and challenges of ICL. We first present a formal definition of ICL\nand clarify its correlation to related studies. Then, we organize and discuss\nadvanced techniques, including training strategies, prompt designing\nstrategies, and related analysis. Additionally, we explore various ICL\napplication scenarios, such as data engineering and knowledge updating.\nFinally, we address the challenges of ICL and suggest potential directions for\nfurther research. We hope that our work can encourage more research on\nuncovering how ICL works and improving ICL.\n', '  The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.\n']",In-Context Learning in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
71,122,71_context_learning_icl_learn,"['context', 'learning', 'icl', 'learn', 'retrieval', 'examples', 'example', 'ica', 'nlp', 'language']","['demonstrations', 'demonstration', 'context', 'examples', 'shot', 'label', 'selection', 'task', 'tasks', 'learning']","['  It has long been assumed that the sheer number of parameters in large\nlanguage models (LLMs) drives in-context learning (ICL) capabilities, enabling\nremarkable performance improvements by leveraging task-specific demonstrations.\nChallenging this hypothesis, we introduce DEEP-ICL, a novel task Definition\nEnriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts\ntask definitions from given demonstrations and generates responses through\nlearning task-specific examples. We argue that improvement from ICL does not\ndirectly rely on model size, but essentially stems from understanding task\ndefinitions and task-guided learning. Inspired by this, DEEP-ICL combines two\n3B models with distinct roles (one for concluding task definitions and the\nother for learning task demonstrations) and achieves comparable performance to\nLLaMA2-13B. Furthermore, our framework outperforms conventional ICL by\novercoming pretraining sequence length limitations, by supporting unlimited\ndemonstrations. We contend that DEEP-ICL presents a novel alternative for\nachieving efficient few-shot learning, extending beyond the conventional ICL.\n', '  With the increasing capabilities of large language models (LLMs), in-context\nlearning (ICL) has emerged as a new paradigm for natural language processing\n(NLP), where LLMs make predictions based on contexts augmented with a few\nexamples. It has been a significant trend to explore ICL to evaluate and\nextrapolate the ability of LLMs. In this paper, we aim to survey and summarize\nthe progress and challenges of ICL. We first present a formal definition of ICL\nand clarify its correlation to related studies. Then, we organize and discuss\nadvanced techniques, including training strategies, prompt designing\nstrategies, and related analysis. Additionally, we explore various ICL\napplication scenarios, such as data engineering and knowledge updating.\nFinally, we address the challenges of ICL and suggest potential directions for\nfurther research. We hope that our work can encourage more research on\nuncovering how ICL works and improving ICL.\n', '  The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.\n']",In-Context Learning in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
71,122,71_context_learning_icl_learn,"['context', 'learning', 'icl', 'learn', 'retrieval', 'examples', 'example', 'ica', 'nlp', 'language']","['demonstrations', 'demonstration', 'context', 'examples', 'shot', 'label', 'selection', 'task', 'tasks', 'learning']","['  It has long been assumed that the sheer number of parameters in large\nlanguage models (LLMs) drives in-context learning (ICL) capabilities, enabling\nremarkable performance improvements by leveraging task-specific demonstrations.\nChallenging this hypothesis, we introduce DEEP-ICL, a novel task Definition\nEnriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts\ntask definitions from given demonstrations and generates responses through\nlearning task-specific examples. We argue that improvement from ICL does not\ndirectly rely on model size, but essentially stems from understanding task\ndefinitions and task-guided learning. Inspired by this, DEEP-ICL combines two\n3B models with distinct roles (one for concluding task definitions and the\nother for learning task demonstrations) and achieves comparable performance to\nLLaMA2-13B. Furthermore, our framework outperforms conventional ICL by\novercoming pretraining sequence length limitations, by supporting unlimited\ndemonstrations. We contend that DEEP-ICL presents a novel alternative for\nachieving efficient few-shot learning, extending beyond the conventional ICL.\n', '  With the increasing capabilities of large language models (LLMs), in-context\nlearning (ICL) has emerged as a new paradigm for natural language processing\n(NLP), where LLMs make predictions based on contexts augmented with a few\nexamples. It has been a significant trend to explore ICL to evaluate and\nextrapolate the ability of LLMs. In this paper, we aim to survey and summarize\nthe progress and challenges of ICL. We first present a formal definition of ICL\nand clarify its correlation to related studies. Then, we organize and discuss\nadvanced techniques, including training strategies, prompt designing\nstrategies, and related analysis. Additionally, we explore various ICL\napplication scenarios, such as data engineering and knowledge updating.\nFinally, we address the challenges of ICL and suggest potential directions for\nfurther research. We hope that our work can encourage more research on\nuncovering how ICL works and improving ICL.\n', '  The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.\n']",In-Context Learning in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
71,122,71_context_learning_icl_learn,"['context', 'learning', 'icl', 'learn', 'retrieval', 'examples', 'example', 'ica', 'nlp', 'language']","['demonstrations', 'demonstration', 'context', 'examples', 'shot', 'label', 'selection', 'task', 'tasks', 'learning']","['  It has long been assumed that the sheer number of parameters in large\nlanguage models (LLMs) drives in-context learning (ICL) capabilities, enabling\nremarkable performance improvements by leveraging task-specific demonstrations.\nChallenging this hypothesis, we introduce DEEP-ICL, a novel task Definition\nEnriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts\ntask definitions from given demonstrations and generates responses through\nlearning task-specific examples. We argue that improvement from ICL does not\ndirectly rely on model size, but essentially stems from understanding task\ndefinitions and task-guided learning. Inspired by this, DEEP-ICL combines two\n3B models with distinct roles (one for concluding task definitions and the\nother for learning task demonstrations) and achieves comparable performance to\nLLaMA2-13B. Furthermore, our framework outperforms conventional ICL by\novercoming pretraining sequence length limitations, by supporting unlimited\ndemonstrations. We contend that DEEP-ICL presents a novel alternative for\nachieving efficient few-shot learning, extending beyond the conventional ICL.\n', '  With the increasing capabilities of large language models (LLMs), in-context\nlearning (ICL) has emerged as a new paradigm for natural language processing\n(NLP), where LLMs make predictions based on contexts augmented with a few\nexamples. It has been a significant trend to explore ICL to evaluate and\nextrapolate the ability of LLMs. In this paper, we aim to survey and summarize\nthe progress and challenges of ICL. We first present a formal definition of ICL\nand clarify its correlation to related studies. Then, we organize and discuss\nadvanced techniques, including training strategies, prompt designing\nstrategies, and related analysis. Additionally, we explore various ICL\napplication scenarios, such as data engineering and knowledge updating.\nFinally, we address the challenges of ICL and suggest potential directions for\nfurther research. We hope that our work can encourage more research on\nuncovering how ICL works and improving ICL.\n', '  The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.\n']",In-Context Learning in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
72,121,72_intrusions_intrusion_cybersecurity_iot,"['intrusions', 'intrusion', 'cybersecurity', 'iot', 'botnet', 'attacks', 'ddos', 'ids2018', 'security', 'attack']","['intrusion', 'detection', 'attacks', 'security', 'attack', 'cyber', 'traffic', 'threats', 'network', 'cybersecurity']","['  Network Intrusion Detection Systems (IDS) aim to detect the presence of an\nintruder by analyzing network packets arriving at an internet connected device.\nData-driven deep learning systems, popular due to their superior performance\ncompared to traditional IDS, depend on availability of high quality training\ndata for diverse intrusion classes. A way to overcome this limitation is\nthrough transferable learning, where training for one intrusion class can lead\nto detection of unseen intrusion classes after deployment. In this paper, we\nprovide a detailed study on the transferability of intrusion detection. We\ninvestigate practical federated learning configurations to enhance the\ntransferability of intrusion detection. We propose two techniques to\nsignificantly improve the transferability of a federated intrusion detection\nsystem. The code for this work can be found at\nhttps://github.com/ghosh64/transferability.\n', '  Intrusion Detection Systems (IDS) play a crucial role in ensuring the\nsecurity of computer networks. Machine learning has emerged as a popular\napproach for intrusion detection due to its ability to analyze and detect\npatterns in large volumes of data. However, current ML-based IDS solutions\noften struggle to keep pace with the ever-changing nature of attack patterns\nand the emergence of new attack types. Additionally, these solutions face\nchallenges related to class imbalance, where the number of instances belonging\nto different classes (normal and intrusions) is significantly imbalanced, which\nhinders their ability to effectively detect minor classes. In this paper, we\npropose a novel multi-agent reinforcement learning (RL) architecture, enabling\nautomatic, efficient, and robust network intrusion detection. To enhance the\ncapabilities of the proposed model, we have improved the DQN algorithm by\nimplementing the weighted mean square loss function and employing\ncost-sensitive learning techniques. Our solution introduces a resilient\narchitecture designed to accommodate the addition of new attacks and\neffectively adapt to changes in existing attack patterns. Experimental results\nrealized using CIC-IDS-2017 dataset, demonstrate that our approach can\neffectively handle the class imbalance problem and provide a fine grained\nclassification of attacks with a very low false positive rate. In comparison to\nthe current state-of-the-art works, our solution demonstrates a significant\nsuperiority in both detection rate and false positive rate.\n', '  The integration of Internet of Things (IoT) applications in our daily lives\nhas led to a surge in data traffic, posing significant security challenges. IoT\napplications using cloud and edge computing are at higher risk of cyberattacks\nbecause of the expanded attack surface from distributed edge and cloud\nservices, the vulnerability of IoT devices, and challenges in managing security\nacross interconnected systems leading to oversights. This led to the rise of\nML-based solutions for intrusion detection systems (IDSs), which have proven\neffective in enhancing network security and defending against diverse threats.\nHowever, ML-based IDS in IoT systems encounters challenges, particularly from\nnoisy, redundant, and irrelevant features in varied IoT datasets, potentially\nimpacting its performance. Therefore, reducing such features becomes crucial to\nenhance system performance and minimize computational costs. This paper focuses\non improving the effectiveness of ML-based IDS at the edge level by introducing\na novel method to find a balanced trade-off between cost and accuracy through\nthe creation of informative features in a two-tier edge-user IoT environment. A\nhybrid Binary Quantum-inspired Artificial Bee Colony and Genetic Programming\nalgorithm is utilized for this purpose. Three IoT intrusion detection datasets,\nnamely NSL-KDD, UNSW-NB15, and BoT-IoT, are used for the evaluation of the\nproposed approach.\n']",Intrusion Detection Systems for Cybersecurity,Intrusion Detection and Defense Strategies for Cybersecurity,Cybersecurity and Artificial Intelligence,Cybersecurity and Artificial Intelligence
73,121,73_routes_planning_metaheuristics_salesman,"['routes', 'planning', 'metaheuristics', 'salesman', 'heuristics', 'optimization', 'metaheuristic', 'heuristic', 'learning', 'routing']","['routing', 'salesman', 'vehicle', 'scheduling', 'inventory', 'instances', 'combinatorial', 'solutions', 'problems', 'problem']","['  Machine learning has been adapted to help solve NP-hard combinatorial\noptimization problems. One prevalent way is learning to construct solutions by\ndeep neural networks, which has been receiving more and more attention due to\nthe high efficiency and less requirement for expert knowledge. However, many\nneural construction methods for Vehicle Routing Problems~(VRPs) focus on\nsynthetic problem instances with specified node distributions and limited\nscales, leading to poor performance on real-world problems which usually\ninvolve complex and unknown node distributions together with large scales. To\nmake neural VRP solvers more practical, we design an auxiliary policy that\nlearns from the local transferable topological features, named local policy,\nand integrate it with a typical construction policy (which learns from the\nglobal information of VRP instances) to form an ensemble policy. With joint\ntraining, the aggregated policies perform cooperatively and complementarily to\nboost generalization. The experimental results on two well-known benchmarks,\nTSPLIB and CVRPLIB, of travelling salesman problem and capacitated VRP show\nthat the ensemble policy significantly improves both cross-distribution and\ncross-scale generalization performance, and even performs well on real-world\nproblems with several thousand nodes.\n', '  The neural combinatorial optimization (NCO) approach has shown great\npotential for solving routing problems without the requirement of expert\nknowledge. However, existing constructive NCO methods cannot directly solve\nlarge-scale instances, which significantly limits their application prospects.\nTo address these crucial shortcomings, this work proposes a novel\nInstance-Conditioned Adaptation Model (ICAM) for better large-scale\ngeneralization of neural combinatorial optimization. In particular, we design a\npowerful yet lightweight instance-conditioned adaptation module for the NCO\nmodel to generate better solutions for instances across different scales. In\naddition, we develop an efficient three-stage reinforcement learning-based\ntraining scheme that enables the model to learn cross-scale features without\nany labeled optimal solution. Experimental results show that our proposed\nmethod is capable of obtaining excellent results with a very fast inference\ntime in solving Traveling Salesman Problems (TSPs) and Capacitated Vehicle\nRouting Problems (CVRPs) across different scales. To the best of our knowledge,\nour model achieves state-of-the-art performance among all RL-based constructive\nmethods for TSP and CVRP with up to 1,000 nodes.\n', '  The end-to-end neural combinatorial optimization (NCO) method shows promising\nperformance in solving complex combinatorial optimization problems without the\nneed for expert design. However, existing methods struggle with large-scale\nproblems, hindering their practical applicability. To overcome this limitation,\nthis work proposes a novel Self-Improved Learning (SIL) method for better\nscalability of neural combinatorial optimization. Specifically, we develop an\nefficient self-improved mechanism that enables direct model training on\nlarge-scale problem instances without any labeled data. Powered by an\ninnovative local reconstruction approach, this method can iteratively generate\nbetter solutions by itself as pseudo-labels to guide efficient model training.\nIn addition, we design a linear complexity attention mechanism for the model to\nefficiently handle large-scale combinatorial problem instances with low\ncomputation overhead. Comprehensive experiments on the Travelling Salesman\nProblem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to\n100K nodes in both uniform and real-world distributions demonstrate the\nsuperior scalability of our method.\n']",Neural Combinatorial Optimization for Routing Problems,Neural Network Optimization Techniques,Deep Learning Optimization and Training,Deep Learning Optimization and Security
74,117,74_galaxies_galactic_galaxy_astronomical,"['galaxies', 'galactic', 'galaxy', 'astronomical', 'astronomy', 'astrophysical', 'cosmology', 'cosmic', 'cosmological', 'quasars']","['galaxy', 'cosmological', 'galaxies', 'gravitational', 'spectra', 'dark', 'astrophysical', 'redshift', 'matter', 'mass']","[""  It has been recently shown that a powerful way to constrain cosmological\nparameters from galaxy redshift surveys is to train graph neural networks to\nperform field-level likelihood-free inference without imposing cuts on scale.\nIn particular, de Santi et al. (2023) developed models that could accurately\ninfer the value of $\\Omega_{\\rm m}$ from catalogs that only contain the\npositions and radial velocities of galaxies that are robust to uncertainties in\nastrophysics and subgrid models. However, observations are affected by many\neffects, including 1) masking, 2) uncertainties in peculiar velocities and\nradial distances, and 3) different galaxy selections. Moreover, observations\nonly allow us to measure redshift, intertwining galaxies' radial positions and\nvelocities. In this paper we train and test our models on galaxy catalogs,\ncreated from thousands of state-of-the-art hydrodynamic simulations run with\ndifferent codes from the CAMELS project, that incorporate these observational\neffects. We find that, although the presence of these effects degrades the\nprecision and accuracy of the models, and increases the fraction of catalogs\nwhere the model breaks down, the fraction of galaxy catalogs where the model\nperforms well is over 90 %, demonstrating the potential of these models to\nconstrain cosmological parameters even when applied to real data.\n"", '  Modern spectroscopic surveys can only target a small fraction of the vast\namount of photometrically cataloged sources in wide-field surveys. Here, we\nreport the development of a generative AI method capable of predicting optical\ngalaxy spectra from photometric broad-band images alone. This method draws from\nthe latest advances in diffusion models in combination with contrastive\nnetworks. We pass multi-band galaxy images into the architecture to obtain\noptical spectra. From these, robust values for galaxy properties can be derived\nwith any methods in the spectroscopic toolbox, such as standard population\nsynthesis techniques and Lick indices. When trained and tested on 64x64-pixel\nimages from the Sloan Digital Sky Survey, the global bimodality of star-forming\nand quiescent galaxies in photometric space is recovered, as well as a\nmass-metallicity relation of star-forming galaxies. The comparison between the\nobserved and the artificially created spectra shows good agreement in overall\nmetallicity, age, Dn4000, stellar velocity dispersion, and E(B-V) values.\nPhotometric redshift estimates of our generative algorithm can compete with\nother current, specialized deep-learning techniques. Moreover, this work is the\nfirst attempt in the literature to infer velocity dispersion from photometric\nimages. Additionally, we can predict the presence of an active galactic nucleus\nup to an accuracy of 82%. With our method, scientifically interesting galaxy\nproperties, normally requiring spectroscopic inputs, can be obtained in future\ndata sets from large-scale photometric surveys alone. The spectra prediction\nvia AI can further assist in creating realistic mock catalogs.\n', '  We present AstroCLIP, a single, versatile model that can embed both galaxy\nimages and spectra into a shared, physically meaningful latent space. These\nembeddings can then be used - without any model fine-tuning - for a variety of\ndownstream tasks including (1) accurate in-modality and cross-modality semantic\nsimilarity search, (2) photometric redshift estimation, (3) galaxy property\nestimation from both images and spectra, and (4) morphology classification. Our\napproach to implementing AstroCLIP consists of two parts. First, we embed\ngalaxy images and spectra separately by pretraining separate transformer-based\nimage and spectrum encoders in self-supervised settings. We then align the\nencoders using a contrastive loss. We apply our method to spectra from the Dark\nEnergy Spectroscopic Instrument and images from its corresponding Legacy\nImaging Survey. Overall, we find remarkable performance on all downstream\ntasks, even relative to supervised baselines. For example, for a task like\nphotometric redshift prediction, we find similar performance to a\nspecifically-trained ResNet18, and for additional tasks like physical property\nestimation (stellar mass, age, metallicity, and sSFR), we beat this supervised\nbaseline by 19\\% in terms of $R^2$. We also compare our results to a\nstate-of-the-art self-supervised single-modal model for galaxy images, and find\nthat our approach outperforms this benchmark by roughly a factor of two on\nphotometric redshift estimation and physical property prediction in terms of\n$R^2$, while remaining roughly in-line in terms of morphology classification.\nUltimately, our approach represents the first cross-modal self-supervised model\nfor galaxies, and the first self-supervised transformer-based architectures for\ngalaxy images and spectra.\n']",Galaxy Properties and Cosmology with AI and Spectroscopy,Astroinformatics and Cosmology,Astrophysics and Astronomy Informatics,Astrophysics and Astronomy Informatics
75,115,75_backdoors_adversarial_backdoor_attacks,"['backdoors', 'adversarial', 'backdoor', 'attacks', 'vulnerabilities', 'malicious', 'security', 'vulnerability', 'backdoored', 'anydoor']","['backdoor', 'attacks', 'attack', 'injection', 'prompt', 'trigger', 'triggers', 'security', 'malicious', 'poisoning']","['  Large Language Models (LLMs) have shown significant promise in\ndecision-making tasks when fine-tuned on specific applications, leveraging\ntheir inherent common sense and reasoning abilities learned from vast amounts\nof data. However, these systems are exposed to substantial safety and security\nrisks during the fine-tuning phase. In this work, we propose the first\ncomprehensive framework for Backdoor Attacks against LLM-enabled\nDecision-making systems (BALD), systematically exploring how such attacks can\nbe introduced during the fine-tuning phase across various channels.\nSpecifically, we propose three attack mechanisms and corresponding backdoor\noptimization methods to attack different components in the LLM-based\ndecision-making pipeline: word injection, scenario manipulation, and knowledge\ninjection. Word injection embeds trigger words directly into the query prompt.\nScenario manipulation occurs in the physical environment, where a high-level\nbackdoor semantic scenario triggers the attack. Knowledge injection conducts\nbackdoor attacks on retrieval augmented generation (RAG)-based LLM systems,\nstrategically injecting word triggers into poisoned knowledge while ensuring\nthe information remains factually accurate for stealthiness. We conduct\nextensive experiments with three popular LLMs (GPT-3.5, LLaMA2, PaLM2), using\ntwo datasets (HighwayEnv, nuScenes), and demonstrate the effectiveness and\nstealthiness of our backdoor triggers and mechanisms. Finally, we critically\nassess the strengths and weaknesses of our proposed approaches, highlight the\ninherent vulnerabilities of LLMs in decision-making tasks, and evaluate\npotential defenses to safeguard LLM-based decision making systems.\n', '  Leveraging the rapid development of Large Language Models LLMs, LLM-based\nagents have been developed to handle various real-world applications, including\nfinance, healthcare, and shopping, etc. It is crucial to ensure the reliability\nand security of LLM-based agents during applications. However, the safety\nissues of LLM-based agents are currently under-explored. In this work, we take\nthe first step to investigate one of the typical safety threats, backdoor\nattack, to LLM-based agents. We first formulate a general framework of agent\nbackdoor attacks, then we present a thorough analysis on the different forms of\nagent backdoor attacks. Specifically, from the perspective of the final\nattacking outcomes, the attacker can either choose to manipulate the final\noutput distribution, or only introduce malicious behavior in the intermediate\nreasoning process, while keeping the final output correct. Furthermore, the\nformer category can be divided into two subcategories based on trigger\nlocations: the backdoor trigger can be hidden either in the user query or in an\nintermediate observation returned by the external environment. We propose the\ncorresponding data poisoning mechanisms to implement the above variations of\nagent backdoor attacks on two typical agent tasks, web shopping and tool\nutilization. Extensive experiments show that LLM-based agents suffer severely\nfrom backdoor attacks, indicating an urgent need for further research on the\ndevelopment of defenses against backdoor attacks on LLM-based agents. Warning:\nThis paper may contain biased content.\n', '  The large language models (LLMs), which bridge the gap between human language\nunderstanding and complex problem-solving, achieve state-of-the-art performance\non several NLP tasks, particularly in few-shot and zero-shot settings. Despite\nthe demonstrable efficacy of LMMs, due to constraints on computational\nresources, users have to engage with open-source language models or outsource\nthe entire training process to third-party platforms. However, research has\ndemonstrated that language models are susceptible to potential security\nvulnerabilities, particularly in backdoor attacks. Backdoor attacks are\ndesigned to introduce targeted vulnerabilities into language models by\npoisoning training samples or model weights, allowing attackers to manipulate\nmodel responses through malicious triggers. While existing surveys on backdoor\nattacks provide a comprehensive overview, they lack an in-depth examination of\nbackdoor attacks specifically targeting LLMs. To bridge this gap and grasp the\nlatest trends in the field, this paper presents a novel perspective on backdoor\nattacks for LLMs by focusing on fine-tuning methods. Specifically, we\nsystematically classify backdoor attacks into three categories: full-parameter\nfine-tuning, parameter-efficient fine-tuning, and attacks without fine-tuning.\nBased on insights from a substantial review, we also discuss crucial issues for\nfuture research on backdoor attacks, such as further exploring attack\nalgorithms that do not require fine-tuning, or developing more covert attack\nalgorithms.\n']",Backdoor Attacks on Large Language Models,Adversarial Attacks and Defenses in AI and Machine Learning Models,Adversarial Attacks and Vulnerabilities in AI and Machine Learning,Adversarial Attacks and Vulnerabilities in AI and Machine Learning
76,109,76_semantic_knowledge_relational_entities,"['semantic', 'knowledge', 'relational', 'entities', 'embeddings', 'subgraph', 'entity', 'completion', 'relations', 'kgexplainer']","['entities', 'link', 'knowledge', 'entity', 'relations', 'completion', 'graphs', 'relation', 'graph', 'relational']","['  Knowledge graph completion (KGC) is a widely used method to tackle\nincompleteness in knowledge graphs (KGs) by making predictions for missing\nlinks. Description-based KGC leverages pre-trained language models to learn\nentity and relation representations with their names or descriptions, which\nshows promising results. However, the performance of description-based KGC is\nstill limited by the quality of text and the incomplete structure, as it lacks\nsufficient entity descriptions and relies solely on relation names, leading to\nsub-optimal results. To address this issue, we propose MPIKGC, a general\nframework to compensate for the deficiency of contextualized knowledge and\nimprove KGC by querying large language models (LLMs) from various perspectives,\nwhich involves leveraging the reasoning, explanation, and summarization\ncapabilities of LLMs to expand entity descriptions, understand relations, and\nextract structures, respectively. We conducted extensive evaluation of the\neffectiveness and improvement of our framework based on four description-based\nKGC models and four datasets, for both link prediction and triplet\nclassification tasks.\n', '  Knowledge graphs (KGs) consist of links that describe relationships between\nentities. Due to the difficulty of manually enumerating all relationships\nbetween entities, automatically completing them is essential for KGs. Knowledge\nGraph Completion (KGC) is a task that infers unseen relationships between\nentities in a KG. Traditional embedding-based KGC methods, such as RESCAL,\nTransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using\nonly the knowledge from training data. In contrast, the recent Pre-trained\nLanguage Model (PLM)-based KGC utilizes knowledge obtained during pre-training.\nTherefore, PLM-based KGC can estimate missing links between entities by reusing\nmemorized knowledge from pre-training without inference. This approach is\nproblematic because building KGC models aims to infer unseen links between\nentities. However, conventional evaluations in KGC do not consider inference\nand memorization abilities separately. Thus, a PLM-based KGC method, which\nachieves high performance in current KGC evaluations, may be ineffective in\npractical applications. To address this issue, we analyze whether PLM-based KGC\nmethods make inferences or merely access memorized knowledge. For this purpose,\nwe propose a method for constructing synthetic datasets specified in this\nanalysis and conclude that PLMs acquire the inference abilities required for\nKGC through pre-training, even though the performance improvements mostly come\nfrom textual information of entities and relations.\n', '  Inductive knowledge graph completion (KGC) aims to infer the missing relation\nfor a set of newly-coming entities that never appeared in the training set.\nSuch a setting is more in line with reality, as real-world KGs are constantly\nevolving and introducing new knowledge. Recent studies have shown promising\nresults using message passing over subgraphs to embed newly-coming entities for\ninductive KGC. However, the inductive capability of these methods is usually\nlimited by two key issues. (i) KGC always suffers from data sparsity, and the\nsituation is even exacerbated in inductive KGC where new entities often have\nfew or no connections to the original KG. (ii) Cold-start problem. It is over\ncoarse-grained for accurate KG reasoning to generate representations for new\nentities by gathering the local information from few neighbors. To this end, we\npropose a novel iNfOmax RelAtion Network, namely NORAN, for inductive KG\ncompletion. It aims to mine latent relation patterns for inductive KG\ncompletion. Specifically, by centering on relations, NORAN provides a hyper\nview towards KG modeling, where the correlations between relations can be\nnaturally captured as entity-independent logical evidence to conduct inductive\nKGC. Extensive experiment results on five benchmarks show that our framework\nsubstantially outperforms the state-of-the-art KGC methods.\n']",Knowledge Graph Completion Methods,Knowledge Representation and Matrix Completion Methods,Tensor and Matrix Methods for Data Representation and Completion,Tensor and Matrix Methods for Data Representation and Completion
77,107,77_prediction_conformity_conformalized_conformal,"['prediction', 'conformity', 'conformalized', 'conformal', 'predictions', 'predictors', 'predictive', 'predictor', 'classification', 'regression']","['conformal', 'prediction', 'coverage', 'intervals', 'sets', 'quantile', 'uncertainty', 'conditional', 'calibration', 'guarantees']","['  Conformal prediction is a non-parametric technique for constructing\nprediction intervals or sets from arbitrary predictive models under the\nassumption that the data is exchangeable. It is popular as it comes with\ntheoretical guarantees on the marginal coverage of the prediction sets and the\nsplit conformal prediction variant has a very low computational cost compared\nto model training. We study the robustness of split conformal prediction in a\ndata contamination setting, where we assume a small fraction of the calibration\nscores are drawn from a different distribution than the bulk. We quantify the\nimpact of the corrupted data on the coverage and efficiency of the constructed\nsets when evaluated on ""clean"" test points, and verify our results with\nnumerical experiments. Moreover, we propose an adjustment in the classification\nsetting which we call Contamination Robust Conformal Prediction, and verify the\nefficacy of our approach using both synthetic and real datasets.\n', ""  Given the growing significance of reliable, trustworthy, and explainable\nmachine learning, the requirement of uncertainty quantification for anomaly\ndetection systems has become increasingly important. In this context,\neffectively controlling Type I error rates ($\\alpha$) without compromising the\nstatistical power ($1-\\beta$) of these systems can build trust and reduce costs\nrelated to false discoveries, particularly when follow-up procedures are\nexpensive. Leveraging the principles of conformal prediction emerges as a\npromising approach for providing respective statistical guarantees by\ncalibrating a model's uncertainty. This work introduces a novel framework for\nanomaly detection, termed cross-conformal anomaly detection, building upon\nwell-known cross-conformal methods designed for prediction tasks. With that, it\naddresses a natural research gap by extending previous works in the context of\ninductive conformal anomaly detection, relying on the split-conformal approach\nfor model calibration. Drawing on insights from conformal prediction, we\ndemonstrate that the derived methods for calculating cross-conformal $p$-values\nstrike a practical compromise between statistical efficiency (full-conformal)\nand computational efficiency (split-conformal) for uncertainty-quantified\nanomaly detection on benchmark datasets.\n"", '  Conformal Prediction (CP) is a distribution-free uncertainty estimation\nframework that constructs prediction sets guaranteed to contain the true answer\nwith a user-specified probability. Intuitively, the size of the prediction set\nencodes a general notion of uncertainty, with larger sets associated with\nhigher degrees of uncertainty. In this work, we leverage information theory to\nconnect conformal prediction to other notions of uncertainty. More precisely,\nwe prove three different ways to upper bound the intrinsic uncertainty, as\ndescribed by the conditional entropy of the target variable given the inputs,\nby combining CP with information theoretical inequalities. Moreover, we\ndemonstrate two direct and useful applications of such connection between\nconformal prediction and information theory: (i) more principled and effective\nconformal training objectives that generalize previous approaches and enable\nend-to-end training of machine learning models from scratch, and (ii) a natural\nmechanism to incorporate side information into conformal prediction. We\nempirically validate both applications in centralized and federated learning\nsettings, showing our theoretical results translate to lower inefficiency\n(average prediction set size) for popular CP methods.\n']",Conformal Prediction and Uncertainty Quantification,Uncertainty Estimation and Quantification in Machine Learning,Machine Learning Reliability and Uncertainty,Machine Learning Reliability and Uncertainty
78,107,78_quantization_quantizing_quantized_quantize,"['quantization', 'quantizing', 'quantized', 'quantize', 'quanvolutional', 'imagenet', 'cnns', 'bits', 'fpgas', 'compression']","['quantization', 'precision', 'bit', 'quantized', 'hardware', 'operations', 'bits', 'arithmetic', 'point', 'networks']","['  Quantization has become a mainstream compression technique for reducing model\nsize, computational requirements, and energy consumption for modern deep neural\nnetworks (DNNs). With improved numerical support in recent hardware, including\nmultiple variants of integer and floating point, mixed-precision quantization\nhas become necessary to achieve high-quality results with low model cost. Prior\nmixed-precision methods have performed either a post-training quantization\nsearch, which compromises on accuracy, or a differentiable quantization search,\nwhich leads to high memory usage from branching. Therefore, we propose the\nfirst one-shot mixed-precision quantization search that eliminates the need for\nretraining in both integer and low-precision floating point models. We evaluate\nour search (FLIQS) on multiple convolutional and vision transformer networks to\ndiscover Pareto-optimal models. Our approach improves upon uniform precision,\nmanual mixed-precision, and recent integer quantization search methods. With\ninteger models, we increase the accuracy of ResNet-18 on ImageNet by 1.31% and\nResNet-50 by 0.90% with equivalent model cost over previous methods.\nAdditionally, for the first time, we explore a novel mixed-precision\nfloating-point search and improve MobileNetV2 by up to 0.98% compared to prior\nstate-of-the-art FP8 models. Finally, we extend FLIQS to simultaneously search\na joint quantization and neural architecture space and improve the ImageNet\naccuracy by 2.69% with similar model cost on a MobileNetV2 search space.\n', '  Quantization of the weights and activations is one of the main methods to\nreduce the computational footprint of Deep Neural Networks (DNNs) training.\nCurrent methods enable 4-bit quantization of the forward phase. However, this\nconstitutes only a third of the training process. Reducing the computational\nfootprint of the entire training process requires the quantization of the\nneural gradients, i.e., the loss gradients with respect to the outputs of\nintermediate neural layers.\n  Previous works separately showed that accurate 4-bit quantization of the\nneural gradients needs to (1) be unbiased and (2) have a log scale. However, no\nprevious work aimed to combine both ideas, as we do in this work. Specifically,\nwe examine the importance of having unbiased quantization in quantized neural\nnetwork training, where to maintain it, and how to combine it with logarithmic\nquantization. Based on this, we suggest a $\\textit{logarithmic unbiased\nquantization}$ (LUQ) method to quantize both the forward and backward phases to\n4-bit, achieving state-of-the-art results in 4-bit training without the\noverhead. For example, in ResNet50 on ImageNet, we achieved a degradation of\n1.1%. We further improve this to a degradation of only 0.32% after three epochs\nof high precision fine-tuning, combined with a variance reduction method --\nwhere both these methods add overhead comparable to previously suggested\nmethods.\n', '  Low-bit quantization emerges as one of the most promising compression\napproaches for deploying deep neural networks on edge devices. Mixed-precision\nquantization leverages a mixture of bit-widths to unleash the accuracy and\nefficiency potential of quantized models. However, existing mixed-precision\nquantization methods rely on simulations in high-performance devices to achieve\naccuracy and efficiency trade-offs in immense search spaces. This leads to a\nnon-negligible gap between the estimated efficiency metrics and the actual\nhardware that makes quantized models far away from the optimal accuracy and\nefficiency, and also causes the quantization process to rely on additional\nhigh-performance devices. In this paper, we propose an On-Chip Hardware-Aware\nQuantization (OHQ) framework, performing hardware-aware mixed-precision\nquantization on deployed edge devices to achieve accurate and efficient\ncomputing. Specifically, for efficiency metrics, we built an On-Chip\nQuantization Aware pipeline, which allows the quantization process to perceive\nthe actual hardware efficiency of the quantization operator and avoid\noptimization errors caused by inaccurate simulation. For accuracy metrics, we\npropose Mask-Guided Quantization Estimation technology to effectively estimate\nthe accuracy impact of operators in the on-chip scenario, getting rid of the\ndependence of the quantization process on high computing power. By synthesizing\ninsights from quantized models and hardware through linear optimization, we can\nobtain optimized bit-width configurations to achieve outstanding performance on\naccuracy and efficiency. We evaluate inference accuracy and acceleration with\nquantization for various architectures and compression ratios on hardware. OHQ\nachieves 70% and 73% accuracy for ResNet-18 and MobileNetV3, respectively, and\ncan reduce latency by 15~30% compared to INT8 on real deployment.\n']",Quantization Techniques for Deep Neural Networks,Quantization and Compression Techniques for Deep Learning Models,Deep Learning Optimization and Security,Deep Learning Methodologies
79,106,79_pruning_cnn_prune_pruned,"['pruning', 'cnn', 'prune', 'pruned', 'cnns', 'imagenet', 'rnns', 'networks', 'neural', 'deep']","['pruning', 'sparsity', 'compression', 'networks', 'network', 'neural', 'accuracy', 'mask', 'computational', 'structured']","['  Modern deep neural networks (DNNs) consist of millions of parameters,\nnecessitating high-performance computing during training and inference. Pruning\nis one solution that significantly reduces the space and time complexities of\nDNNs. Traditional pruning methods that are applied post-training focus on\nstreamlining inference, but there are recent efforts to leverage sparsity early\non by pruning before training. Pruning methods, such as iterative\nmagnitude-based pruning (IMP) achieve up to a 90% parameter reduction while\nretaining accuracy comparable to the original model. However, this leads to\nimpractical runtime as it relies on multiple train-prune-reset cycles to\nidentify and eliminate redundant parameters. In contrast, training agnostic\nearly pruning methods, such as SNIP and SynFlow offer fast pruning but fall\nshort of the accuracy achieved by IMP at high sparsities. To bridge this gap,\nwe present Dual Gradient-Based Rapid Iterative Pruning (DRIVE), which leverages\ndense training for initial epochs to counteract the randomness inherent at the\ninitialization. Subsequently, it employs a unique dual gradient-based metric\nfor parameter ranking. It has been experimentally demonstrated for VGG and\nResNet architectures on CIFAR-10/100 and Tiny ImageNet, and ResNet on ImageNet\nthat DRIVE consistently has superior performance over other training-agnostic\nearly pruning methods in accuracy. Notably, DRIVE is 43$\\times$ to 869$\\times$\nfaster than IMP for pruning.\n', ""  With the increasing size of datasets used for training neural networks, data\npruning becomes an attractive field of research. However, most current data\npruning algorithms are limited in their ability to preserve accuracy compared\nto models trained on the full data, especially in high pruning regimes. In this\npaper we explore the application of data pruning while incorporating knowledge\ndistillation (KD) when training on a pruned subset. That is, rather than\nrelying solely on ground-truth labels, we also use the soft predictions from a\nteacher network pre-trained on the complete data. By integrating KD into\ntraining, we demonstrate significant improvement across datasets, pruning\nmethods, and on all pruning fractions. We first establish a theoretical\nmotivation for employing self-distillation to improve training on pruned data.\nThen, we empirically make a compelling and highly practical observation: using\nKD, simple random pruning is comparable or superior to sophisticated pruning\nmethods across all pruning regimes. On ImageNet for example, we achieve\nsuperior accuracy despite training on a random subset of only 50% of the data.\nAdditionally, we demonstrate a crucial connection between the pruning factor\nand the optimal knowledge distillation weight. This helps mitigate the impact\nof samples with noisy labels and low-quality images retained by typical pruning\nalgorithms. Finally, we make an intriguing observation: when using lower\npruning fractions, larger teachers lead to accuracy degradation, while\nsurprisingly, employing teachers with a smaller capacity than the student's may\nimprove results. Our code will be made available.\n"", '  Deep neural networks (DNNs) have demonstrated remarkable success in various\nfields. However, the large number of floating-point operations (FLOPs) in DNNs\nposes challenges for their deployment in resource-constrained applications,\ne.g., edge devices. To address the problem, pruning has been introduced to\nreduce the computational cost in executing DNNs. Previous pruning strategies\nare based on weight values, gradient values and activation outputs. Different\nfrom previous pruning solutions, in this paper, we propose a class-aware\npruning technique to compress DNNs, which provides a novel perspective to\nreduce the computational cost of DNNs. In each iteration, the neural network\ntraining is modified to facilitate the class-aware pruning. Afterwards, the\nimportance of filters with respect to the number of classes is evaluated. The\nfilters that are only important for a few number of classes are removed. The\nneural network is then retrained to compensate for the incurred accuracy loss.\nThe pruning iterations end until no filter can be removed anymore, indicating\nthat the remaining filters are very important for many classes. This pruning\ntechnique outperforms previous pruning solutions in terms of accuracy, pruning\nratio and the reduction of FLOPs. Experimental results confirm that this\nclass-aware pruning technique can significantly reduce the number of weights\nand FLOPs, while maintaining a high inference accuracy.\n']",Neural Network Pruning Techniques,Neural Network Optimization Techniques,Deep Learning Optimization and Training,Deep Learning Optimization and Security
80,106,80_datasets_dataset_tables_tabular,"['datasets', 'dataset', 'tables', 'tabular', 'table', 'learning', 'tabpfn', 'neural', 'features', 'data']","['tabular', 'tables', 'tree', 'feature', 'columns', 'data', 'datasets', 'deep', 'features', 'categorical']","['  In the domain of data science, the predictive tasks of classification,\nregression, and imputation of missing values are commonly encountered\nchallenges associated with tabular data. This research endeavors to apply Large\nLanguage Models (LLMs) towards addressing these predictive tasks. Despite their\nproficiency in comprehending natural language, LLMs fall short in dealing with\nstructured tabular data. This limitation stems from their lacking exposure to\nthe intricacies of tabular data during their foundational training. Our\nresearch aims to mitigate this gap by compiling a comprehensive corpus of\ntables annotated with instructions and executing large-scale training of\nLlama-2 on this enriched dataset. Furthermore, we investigate the practical\napplication of applying the trained model to zero-shot prediction, few-shot\nprediction, and in-context learning scenarios. Through extensive experiments,\nour methodology has shown significant improvements over existing benchmarks.\nThese advancements highlight the efficacy of tailoring LLM training to solve\ntable-related problems in data science, thereby establishing a new benchmark in\nthe utilization of LLMs for enhancing tabular intelligence.\n', '  Tabular data from different tables exhibit significant diversity due to\nvaried definitions and types of features, as well as complex inter-feature and\nfeature-target relationships. Cross-dataset pretraining, which learns reusable\npatterns from upstream data to support downstream tasks, have shown notable\nsuccess in various fields. Yet, when applied to tabular data prediction, this\nparadigm faces challenges due to the limited reusable patterns among diverse\ntabular datasets (tables) and the general scarcity of tabular data available\nfor fine-tuning. In this study, we fill this gap by introducing a cross-table\npretrained Transformer, XTFormer, for versatile downstream tabular prediction\ntasks. Our methodology insight is pretraining XTFormer to establish a\n""meta-function"" space that encompasses all potential feature-target mappings.\nIn pre-training, a variety of potential mappings are extracted from\npre-training tabular datasets and are embedded into the ""meta-function"" space,\nand suited mappings are extracted from the ""meta-function"" space for downstream\ntasks by a specified coordinate positioning approach. Experiments show that, in\n190 downstream tabular prediction tasks, our cross-table pretrained XTFormer\nwins both XGBoost and Catboost on 137 (72%) tasks, and surpasses representative\ndeep learning models FT-Transformer and the tabular pre-training approach XTab\non 144 (76%) and 162 (85%) tasks.\n', ""  Tabular data is prevalent across various domains in machine learning.\nAlthough Deep Neural Network (DNN)-based methods have shown promising\nperformance comparable to tree-based ones, in-depth evaluation of these methods\nis challenging due to varying performance ranks across diverse datasets. In\nthis paper, we propose a comprehensive benchmark comprising 300 tabular\ndatasets, covering a wide range of task types, size distributions, and domains.\nWe perform an extensive comparison between state-of-the-art deep tabular\nmethods and tree-based methods, revealing the average rank of all methods and\nhighlighting the key factors that influence the success of deep tabular\nmethods. Next, we analyze deep tabular methods based on their training\ndynamics, including changes in validation metrics and other statistics. For\neach dataset-method pair, we learn a mapping from both the meta-features of\ndatasets and the first part of the validation curve to the final validation set\nperformance and even the evolution of validation curves. This mapping extracts\nessential meta-features that influence prediction accuracy, helping the\nanalysis of tabular methods from novel aspects. Based on the performance of all\nmethods on this large benchmark, we identify two subsets of 45 datasets each.\nThe first subset contains datasets that favor either tree-based methods or\nDNN-based methods, serving as effective analysis tools to evaluate strategies\n(e.g., attribute encoding strategies) for improving deep tabular models. The\nsecond subset contains datasets where the ranks of methods are consistent with\nthe overall benchmark, acting as a probe for tabular analysis. These ``tiny\ntabular benchmarks'' will facilitate further studies on tabular data.\n""]",Tabular Data Prediction,Machine Learning for Data Analysis and Modeling,Machine Learning and Artificial Intelligence,Machine Learning and Intelligent Systems
81,104,81_vulnerabilities_vulnerability_vulnerable_developers,"['vulnerabilities', 'vulnerability', 'vulnerable', 'developers', 'security', 'fuzzing', 'bugs', 'code', 'programming', 'malicious']","['vulnerability', 'vulnerabilities', 'security', 'software', 'code', 'fuzzing', 'vulnerable', 'repair', 'developers', 'detection']","['  The security guarantee of AI-enabled software systems (particularly using\ndeep learning techniques as a functional core) is pivotal against the\nadversarial attacks exploiting software vulnerabilities. However, little\nattention has been paid to a systematic investigation of vulnerabilities in\nsuch systems. A common situation learned from the open source software\ncommunity is that deep learning engineers frequently integrate off-the-shelf or\nopen-source learning frameworks into their ecosystems. In this work, we\nspecifically look into deep learning (DL) framework and perform the first\nsystematic study of vulnerabilities in DL systems through a comprehensive\nanalysis of identified vulnerabilities from Common Vulnerabilities and\nExposures (CVE) and open-source DL tools, including TensorFlow, Caffe, OpenCV,\nKeras, and PyTorch. We propose a two-stream data analysis framework to explore\nvulnerability patterns from various databases. We investigate the unique DL\nframeworks and libraries development ecosystems that appear to be decentralized\nand fragmented. By revisiting the Common Weakness Enumeration (CWE) List, which\nprovides the traditional software vulnerability related practices, we observed\nthat it is more challenging to detect and fix the vulnerabilities throughout\nthe DL systems lifecycle. Moreover, we conducted a large-scale empirical study\nof 3,049 DL vulnerabilities to better understand the patterns of vulnerability\nand the challenges in fixing them. We have released the full replication\npackage at https://github.com/codelzz/Vulnerabilities4DLSystem. We anticipate\nthat our study can advance the development of secure DL systems.\n', '  Despite various approaches being employed to detect vulnerabilities, the\nnumber of reported vulnerabilities shows an upward trend over the years. This\nsuggests the problems are not caught before the code is released, which could\nbe caused by many factors, like lack of awareness, limited efficacy of the\nexisting vulnerability detection tools or the tools not being user-friendly. To\nhelp combat some issues with traditional vulnerability detection tools, we\npropose using large language models (LLMs) to assist in finding vulnerabilities\nin source code. LLMs have shown a remarkable ability to understand and generate\ncode, underlining their potential in code-related tasks. The aim is to test\nmultiple state-of-the-art LLMs and identify the best prompting strategies,\nallowing extraction of the best value from the LLMs. We provide an overview of\nthe strengths and weaknesses of the LLM-based approach and compare the results\nto those of traditional static analysis tools. We find that LLMs can pinpoint\nmany more issues than traditional static analysis tools, outperforming\ntraditional tools in terms of recall and F1 scores. The results should benefit\nsoftware developers and security analysts responsible for ensuring that the\ncode is free of vulnerabilities.\n', '  Software, while beneficial, poses potential cybersecurity risks due to\ninherent vulnerabilities. Detecting these vulnerabilities is crucial, and deep\nlearning has shown promise as an effective tool for this task due to its\nability to perform well without extensive feature engineering. However, a\nchallenge in deploying deep learning for vulnerability detection is the limited\navailability of training data. Recent research highlights the deep learning\nefficacy in diverse tasks. This success is attributed to instruction\nfine-tuning, a technique that remains under-explored in the context of\nvulnerability detection. This paper investigates the capability of models,\nspecifically a recent language model, to generalize beyond the programming\nlanguages used in their training data. It also examines the role of natural\nlanguage instructions in enhancing this generalization. Our study evaluates the\nmodel performance on a real-world dataset to predict vulnerable code. We\npresent key insights and lessons learned, contributing to understanding the\ndeep learning application in software vulnerability detection.\n']",Vulnerability Detection in Software Systems,Cybersecurity Threat Detection and Prevention,Cybersecurity and Artificial Intelligence,Cybersecurity and Artificial Intelligence
82,103,82_consciousness_conscious_cognition_ai,"['consciousness', 'conscious', 'cognition', 'ai', 'brain', 'cognitive', 'philosophical', 'artificial', 'neuroscience', 'intelligence']","['consciousness', 'intelligence', 'artificial', 'human', 'conscious', 'humans', 'social', 'cognitive', 'theories', 'cognition']","['  We here analyse the question of developing artificial consciousness from an\nevolutionary perspective, taking the evolution of the human brain and its\nrelation with consciousness as a reference model. This kind of analysis reveals\nseveral structural and functional features of the human brain that appear to be\nkey for reaching human-like complex conscious experience and that current\nresearch on Artificial Intelligence (AI) should take into account in its\nattempt to develop systems capable of conscious processing. We argue that, even\nif AI is limited in its ability to emulate human consciousness for both\nintrinsic (structural and architectural) and extrinsic (related to the current\nstage of scientific and technological knowledge) reasons, taking inspiration\nfrom those characteristics of the brain that make conscious processing possible\nand/or modulate it, is a potentially promising strategy towards developing\nconscious AI. Also, it is theoretically possible that AI research can develop\npartial or potentially alternative forms of consciousness that is qualitatively\ndifferent from the human, and that may be either more or less sophisticated\ndepending on the perspectives. Therefore, we recommend neuroscience-inspired\ncaution in talking about artificial consciousness: since the use of the same\nword consciousness for humans and AI becomes ambiguous and potentially\nmisleading, we propose to clearly specify what is common and what differs in AI\nconscious processing from full human conscious experience.\n', '  Is artificial consciousness theoretically possible? Is it plausible? If so,\nis it technically feasible? To make progress on these questions, it is\nnecessary to lay some groundwork clarifying the logical and empirical\nconditions for artificial consciousness to arise and the meaning of relevant\nterms involved. Consciousness is a polysemic word: researchers from different\nfields, including neuroscience, Artificial Intelligence, robotics, and\nphilosophy, among others, sometimes use different terms in order to refer to\nthe same phenomena or the same terms to refer to different phenomena. In fact,\nif we want to pursue artificial consciousness, a proper definition of the key\nconcepts is required. Here, after some logical and conceptual preliminaries, we\nargue for the necessity of using dimensions and profiles of consciousness for a\nbalanced discussion about their possible instantiation or realisation in\nartificial systems. Our primary goal in this paper is to review the main\ntheoretical questions that arise in the domain of artificial consciousness. On\nthe basis of this review, we propose to assess the issue of artificial\nconsciousness within a multidimensional account. The theoretical possibility of\nartificial consciousness is already presumed within some theoretical\nframeworks; however, empirical possibility cannot simply be deduced from these\nframeworks but needs independent empirical validation. We break down the\ncomplexity of consciousness by identifying constituents, components, and\ndimensions, and reflect pragmatically about the general challenges confronting\nthe creation of artificial consciousness. Despite these challenges, we outline\na research strategy for showing how ""awareness"" as we propose to understand it\ncould plausibly be realised in artificial systems.\n', '  Consciousness is notoriously hard to define with objective terms. An\nobjective definition of consciousness is critically needed so that we might\naccurately understand how consciousness and resultant choice behaviour may\narise in biological or artificial systems. Many theories have integrated\nneurobiological and psychological research to explain how consciousness might\narise, but few, if any, outline what is fundamentally required to generate\nconsciousness. To identify such requirements, I examine current theories of\nconsciousness and corresponding scientific research to generate a new\ndefinition of consciousness from first principles. Critically, consciousness is\nthe apparatus that provides the ability to make decisions, but it is not\ndefined by the decision itself. As such, a definition of consciousness does not\nrequire choice behaviour or an explicit awareness of temporality despite both\nbeing well-characterised outcomes of conscious thought. Rather, requirements\nfor consciousness include: at least some capability for perception, a memory\nfor the storage of such perceptual information which in turn provides a\nframework for an imagination with which a sense of self can be capable of\nmaking decisions based on possible and desired futures. Thought experiments and\nobservable neurological phenomena demonstrate that these components are\nfundamentally required of consciousness, whereby the loss of any one component\nremoves the capability for conscious thought. Identifying these requirements\nprovides a new definition for consciousness by which we can objectively\ndetermine consciousness in any conceivable agent, such as non-human animals and\nartificially intelligent systems.\n']",Artificial Consciousness and Cognitive AI,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
82,103,82_consciousness_conscious_cognition_ai,"['consciousness', 'conscious', 'cognition', 'ai', 'brain', 'cognitive', 'philosophical', 'artificial', 'neuroscience', 'intelligence']","['consciousness', 'intelligence', 'artificial', 'human', 'conscious', 'humans', 'social', 'cognitive', 'theories', 'cognition']","['  We here analyse the question of developing artificial consciousness from an\nevolutionary perspective, taking the evolution of the human brain and its\nrelation with consciousness as a reference model. This kind of analysis reveals\nseveral structural and functional features of the human brain that appear to be\nkey for reaching human-like complex conscious experience and that current\nresearch on Artificial Intelligence (AI) should take into account in its\nattempt to develop systems capable of conscious processing. We argue that, even\nif AI is limited in its ability to emulate human consciousness for both\nintrinsic (structural and architectural) and extrinsic (related to the current\nstage of scientific and technological knowledge) reasons, taking inspiration\nfrom those characteristics of the brain that make conscious processing possible\nand/or modulate it, is a potentially promising strategy towards developing\nconscious AI. Also, it is theoretically possible that AI research can develop\npartial or potentially alternative forms of consciousness that is qualitatively\ndifferent from the human, and that may be either more or less sophisticated\ndepending on the perspectives. Therefore, we recommend neuroscience-inspired\ncaution in talking about artificial consciousness: since the use of the same\nword consciousness for humans and AI becomes ambiguous and potentially\nmisleading, we propose to clearly specify what is common and what differs in AI\nconscious processing from full human conscious experience.\n', '  Is artificial consciousness theoretically possible? Is it plausible? If so,\nis it technically feasible? To make progress on these questions, it is\nnecessary to lay some groundwork clarifying the logical and empirical\nconditions for artificial consciousness to arise and the meaning of relevant\nterms involved. Consciousness is a polysemic word: researchers from different\nfields, including neuroscience, Artificial Intelligence, robotics, and\nphilosophy, among others, sometimes use different terms in order to refer to\nthe same phenomena or the same terms to refer to different phenomena. In fact,\nif we want to pursue artificial consciousness, a proper definition of the key\nconcepts is required. Here, after some logical and conceptual preliminaries, we\nargue for the necessity of using dimensions and profiles of consciousness for a\nbalanced discussion about their possible instantiation or realisation in\nartificial systems. Our primary goal in this paper is to review the main\ntheoretical questions that arise in the domain of artificial consciousness. On\nthe basis of this review, we propose to assess the issue of artificial\nconsciousness within a multidimensional account. The theoretical possibility of\nartificial consciousness is already presumed within some theoretical\nframeworks; however, empirical possibility cannot simply be deduced from these\nframeworks but needs independent empirical validation. We break down the\ncomplexity of consciousness by identifying constituents, components, and\ndimensions, and reflect pragmatically about the general challenges confronting\nthe creation of artificial consciousness. Despite these challenges, we outline\na research strategy for showing how ""awareness"" as we propose to understand it\ncould plausibly be realised in artificial systems.\n', '  Consciousness is notoriously hard to define with objective terms. An\nobjective definition of consciousness is critically needed so that we might\naccurately understand how consciousness and resultant choice behaviour may\narise in biological or artificial systems. Many theories have integrated\nneurobiological and psychological research to explain how consciousness might\narise, but few, if any, outline what is fundamentally required to generate\nconsciousness. To identify such requirements, I examine current theories of\nconsciousness and corresponding scientific research to generate a new\ndefinition of consciousness from first principles. Critically, consciousness is\nthe apparatus that provides the ability to make decisions, but it is not\ndefined by the decision itself. As such, a definition of consciousness does not\nrequire choice behaviour or an explicit awareness of temporality despite both\nbeing well-characterised outcomes of conscious thought. Rather, requirements\nfor consciousness include: at least some capability for perception, a memory\nfor the storage of such perceptual information which in turn provides a\nframework for an imagination with which a sense of self can be capable of\nmaking decisions based on possible and desired futures. Thought experiments and\nobservable neurological phenomena demonstrate that these components are\nfundamentally required of consciousness, whereby the loss of any one component\nremoves the capability for conscious thought. Identifying these requirements\nprovides a new definition for consciousness by which we can objectively\ndetermine consciousness in any conceivable agent, such as non-human animals and\nartificially intelligent systems.\n']",Artificial Consciousness and Cognitive AI,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
83,102,83_adversarial_adaptation_supervised_domainnet,"['adversarial', 'adaptation', 'supervised', 'domainnet', 'learn', 'features', 'feature', 'classification', 'training', 'regularization']","['domain', 'adaptation', 'target', 'source', 'domains', 'unlabeled', 'unsupervised', 'invariant', 'shifts', 'feature']","['  Prior Unsupervised Domain Adaptation (UDA) methods often aim to train a\ndomain-invariant feature extractor, which may hinder the model from learning\nsufficiently discriminative features. To tackle this, a line of works based on\nprompt learning leverages the power of large-scale pre-trained vision-language\nmodels to learn both domain-invariant and specific features through a set of\ndomain-agnostic and domain-specific learnable prompts. Those studies typically\nenforce invariant constraints on representation, output, or prompt space to\nlearn such prompts. Differently, we cast UDA as a multiple-objective\noptimization problem in which each objective is represented by a domain loss.\nUnder this new framework, we propose aligning per-objective gradients to foster\nconsensus between them. Additionally, to prevent potential overfitting when\nfine-tuning this deep learning architecture, we penalize the norm of these\ngradients. To achieve these goals, we devise a practical gradient update\nprocedure that can work under both single-source and multi-source UDA.\nEmpirically, our method consistently surpasses other prompt-based baselines by\na large margin on different UDA benchmarks\n', '  Unsupervised domain adaptation (UDA) aims to transfer knowledge from a\nlabeled source domain to an unlabeled target domain. The most recent UDA\nmethods always resort to adversarial training to yield state-of-the-art results\nand a dominant number of existing UDA methods employ convolutional neural\nnetworks (CNNs) as feature extractors to learn domain invariant features.\nVision transformer (ViT) has attracted tremendous attention since its emergence\nand has been widely used in various computer vision tasks, such as image\nclassification, object detection, and semantic segmentation, yet its potential\nin adversarial domain adaptation has never been investigated. In this paper, we\nfill this gap by employing the ViT as the feature extractor in adversarial\ndomain adaptation. Moreover, we empirically demonstrate that ViT can be a\nplug-and-play component in adversarial domain adaptation, which means directly\nreplacing the CNN-based feature extractor in existing UDA methods with the\nViT-based feature extractor can easily obtain performance improvement. The code\nis available at https://github.com/LluckyYH/VT-ADA.\n', ""  Unsupervised domain adaptation (UDA) has achieved remarkable success in fault\ndiagnosis, bringing significant benefits to diverse industrial applications.\nWhile most UDA methods focus on cross-working condition scenarios where the\nsource and target domains are notably similar, real-world applications often\ngrapple with severe domain shifts. We coin the term `distant domain adaptation\nproblem' to describe the challenge of adapting from a labeled source domain to\na significantly disparate unlabeled target domain. This problem exhibits the\nrisk of negative transfer, where extraneous knowledge from the source domain\nadversely affects the target domain performance. Unfortunately, conventional\nUDA methods often falter in mitigating this negative transfer, leading to\nsuboptimal performance. In response to this challenge, we propose a novel\nOnline Selective Adversarial Alignment (OSAA) approach. Central to OSAA is its\nability to dynamically identify and exclude distant source samples via an\nonline gradient masking approach, focusing primarily on source samples that\nclosely resemble the target samples. Furthermore, recognizing the inherent\ncomplexities in bridging the source and target domains, we construct an\nintermediate domain to act as a transitional domain and ease the adaptation\nprocess. Lastly, we develop a class-conditional adversarial adaptation to\naddress the label distribution disparities while learning domain invariant\nrepresentation to account for potential label distribution disparities between\nthe domains. Through detailed experiments and ablation studies on two\nreal-world datasets, we validate the superior performance of the OSAA method\nover state-of-the-art methods, underscoring its significant utility in\npractical scenarios with severe domain shifts.\n""]",Unsupervised Domain Adaptation Methods,Efficient Model Adaptation Techniques,Advanced Statistical and Machine Learning Methods,Advanced Statistical and Machine Learning Methods
84,100,84_flow_flows_generative_modeling,"['flow', 'flows', 'generative', 'modeling', 'perflow', 'diffusion', 'sde', 'denoising', 'stochastic', 'unnormalized']","['flow', 'diffusion', 'matching', 'flows', 'distributions', 'stochastic', 'sampler', 'sampling', 'score', 'normalizing']","['  Continuous normalizing flows (CNFs) learn an ordinary differential equation\nto transform prior samples into data. Flow matching (FM) has recently emerged\nas a simulation-free approach for training CNFs by regressing a velocity model\ntowards the conditional velocity field. However, on constrained domains, the\nlearned velocity model may lead to undesirable flows that result in highly\nunnatural samples, e.g., oversaturated images, due to both flow matching error\nand simulation error. To address this, we add a boundary constraint term to\nCNFs, which leads to reflected CNFs that keep trajectories within the\nconstrained domains. We propose reflected flow matching (RFM) to train the\nvelocity model in reflected CNFs by matching the conditional velocity fields in\na simulation-free manner, similar to the vanilla FM. Moreover, the analytical\nform of conditional velocity fields in RFM avoids potentially biased\napproximations, making it superior to existing score-based generative models on\nconstrained domains. We demonstrate that RFM achieves comparable or better\nresults on standard image benchmarks and produces high-quality\nclass-conditioned samples under high guidance weight.\n', '  Score-based generative models are a popular class of generative modelling\ntechniques relying on stochastic differential equations (SDE). From their\ninception, it was realized that it was also possible to perform generation\nusing ordinary differential equations (ODE) rather than SDE. This led to the\nintroduction of the probability flow ODE approach and denoising diffusion\nimplicit models. Flow matching methods have recently further extended these\nODE-based approaches and approximate a flow between two arbitrary probability\ndistributions. Previous work derived bounds on the approximation error of\ndiffusion models under the stochastic sampling regime, given assumptions on the\n$L^2$ loss. We present error bounds for the flow matching procedure using fully\ndeterministic sampling, assuming an $L^2$ bound on the approximation error and\na certain regularity condition on the data distributions.\n', '  Gaussian denoising has emerged as a powerful method for constructing\nsimulation-free continuous normalizing flows for generative modeling. Despite\ntheir empirical successes, theoretical properties of these flows and the\nregularizing effect of Gaussian denoising have remained largely unexplored. In\nthis work, we aim to address this gap by investigating the well-posedness of\nsimulation-free continuous normalizing flows built on Gaussian denoising.\nThrough a unified framework termed Gaussian interpolation flow, we establish\nthe Lipschitz regularity of the flow velocity field, the existence and\nuniqueness of the flow, and the Lipschitz continuity of the flow map and the\ntime-reversed flow map for several rich classes of target distributions. This\nanalysis also sheds light on the auto-encoding and cycle consistency properties\nof Gaussian interpolation flows. Additionally, we study the stability of these\nflows in source distributions and perturbations of the velocity field, using\nthe quadratic Wasserstein distance as a metric. Our findings offer valuable\ninsights into the learning techniques employed in Gaussian interpolation flows\nfor generative modeling, providing a solid theoretical foundation for\nend-to-end error analyses of learning Gaussian interpolation flows with\nempirical observations.\n']",Flow-based Generative Modeling Techniques,Generative Modeling Techniques,Generative Modeling and Artificial Intelligence,Generative Modeling and Artificial Intelligence
85,99,85_activelab_supervised_activellm_classification,"['activelab', 'supervised', 'activellm', 'classification', 'annotators', 'annotations', 'annotation', 'annotate', 'labeled', 'learning']","['active', 'annotation', 'labeling', 'informative', 'unlabeled', 'uncertainty', 'budget', 'samples', 'label', 'sampling']","[""  We conduct a comprehensive evaluation of state-of-the-art deep active\nlearning methods. Surprisingly, under general settings, no single-model method\ndecisively outperforms entropy-based active learning, and some even fall short\nof random sampling. We delve into overlooked aspects like starting budget,\nbudget step, and pretraining's impact, revealing their significance in\nachieving superior results. Additionally, we extend our evaluation to other\ntasks, exploring the active learning effectiveness in combination with\nsemi-supervised learning, and object detection. Our experiments provide\nvaluable insights and concrete recommendations for future active learning\nstudies. By uncovering the limitations of current methods and understanding the\nimpact of different experimental settings, we aim to inspire more efficient\ntraining of deep learning models in real-world scenarios with limited\nannotation budgets. This work contributes to advancing active learning's\nefficacy in deep learning and empowers researchers to make informed decisions\nwhen applying active learning to their tasks.\n"", '  This paper explores the integration of active machine learning (ML) for 6G\nnetworks, an area that remains under-explored yet holds potential. Unlike\npassive ML systems, active ML can be made to interact with the network\nenvironment. It actively selects informative and representative data points for\ntraining, thereby reducing the volume of data needed while accelerating the\nlearning process. While active learning research mainly focuses on data\nannotation, we call for a network-centric active learning framework that\nconsiders both annotation (i.e., what is the label) and data acquisition (i.e.,\nwhich and how many samples to collect). Moreover, we explore the synergy\nbetween generative artificial intelligence (AI) and active learning to overcome\nexisting limitations in both active learning and generative AI. This paper also\nfeatures a case study on a mmWave throughput prediction problem to demonstrate\nthe practical benefits and improved performance of active learning for 6G\nnetworks. Furthermore, we discuss how the implications of active learning\nextend to numerous 6G network use cases. We highlight the potential of active\nlearning based 6G networks to enhance computational efficiency, data annotation\nand acquisition efficiency, adaptability, and overall network intelligence. We\nconclude with a discussion on challenges and future research directions for\nactive learning in 6G networks, including development of novel query\nstrategies, distributed learning integration, and inclusion of human- and\nmachine-in-the-loop learning.\n', '  Class imbalance is a prevalent issue in real world machine learning\napplications, often leading to poor performance in rare and minority classes.\nWith an abundance of wild unlabeled data, active learning is perhaps the most\neffective technique in solving the problem at its root -- collecting a more\nbalanced and informative set of labeled examples during annotation. Label noise\nis another common issue in data annotation jobs, which is especially\nchallenging for active learning methods. In this work, we conduct the first\nstudy of active learning under both class imbalance and label noise. We propose\na novel algorithm that robustly identifies the class separation threshold and\nannotates the most uncertain examples that are closest from it. Through a novel\nreduction to one-dimensional active learning, our algorithm DIRECT is able to\nleverage the classic active learning literature to address issues such as batch\nlabeling and tolerance towards label noise. We present extensive experiments on\nimbalanced datasets with and without label noise. Our results demonstrate that\nDIRECT can save more than 60% of the annotation budget compared to state-of-art\nactive learning algorithms and more than 80% of annotation budget compared to\nrandom sampling.\n']",Active Learning for Efficient Annotation,Active Learning and Crowdsourcing for Efficient Data Annotation,Machine Learning Methodologies,Machine Learning Methodologies
86,96,86_videos_captioning_captions_vid,"['videos', 'captioning', 'captions', 'vid', 'textual', 'videoqa', 'videotree', 'video', 'clips', 'multimodal']","['video', 'videos', 'frames', 'frame', 'retrieval', 'clips', 'captioning', 'visual', 'answering', 'query']","['  Video-text Large Language Models (video-text LLMs) have shown remarkable\nperformance in answering questions and holding conversations on simple videos.\nHowever, they perform almost the same as random on grounding text queries in\nlong and complicated videos, having little ability to understand and reason\nabout temporal information, which is the most fundamental difference between\nvideos and images. In this paper, we propose HawkEye, one of the first\nvideo-text LLMs that can perform temporal video grounding in a fully\ntext-to-text manner. To collect training data that is applicable for temporal\nvideo grounding, we construct InternVid-G, a large-scale video-text corpus with\nsegment-level captions and negative spans, with which we introduce two new\ntime-aware training objectives to video-text LLMs. We also propose a\ncoarse-grained method of representing segments in videos, which is more robust\nand easier for LLMs to learn and follow than other alternatives. Extensive\nexperiments show that HawkEye is better at temporal video grounding and\ncomparable on other video-text tasks with existing video-text LLMs, which\nverifies its superior video-text multi-modal understanding abilities.\n', ""  A more robust and holistic language-video representation is the key to\npushing video understanding forward. Despite the improvement in training\nstrategies, the quality of the language-video dataset is less attention to. The\ncurrent plain and simple text descriptions and the visual-only focus for the\nlanguage-video tasks result in a limited capacity in real-world natural\nlanguage video retrieval tasks where queries are much more complex. This paper\nintroduces a method to automatically enhance video-language datasets, making\nthem more modality and context-aware for more sophisticated representation\nlearning needs, hence helping all downstream tasks. Our multifaceted video\ncaptioning method captures entities, actions, speech transcripts, aesthetics,\nand emotional cues, providing detailed and correlating information from the\ntext side to the video side for training. We also develop an agent-like\nstrategy using language models to generate high-quality, factual textual\ndescriptions, reducing human intervention and enabling scalability. The\nmethod's effectiveness in improving language-video representation is evaluated\nthrough text-video retrieval using the MSR-VTT dataset and several multi-modal\nretrieval models.\n"", '  Recent advancements in image understanding have benefited from the extensive\nuse of web image-text pairs. However, video understanding remains a challenge\ndespite the availability of substantial web video-text data. This difficulty\nprimarily arises from the inherent complexity of videos and the inefficient\nlanguage supervision in recent web-collected video-text datasets. In this\npaper, we introduce Text-Only Pre-Alignment (TOPA), a novel approach to extend\nlarge language models (LLMs) for video understanding, without the need for\npre-training on real video data. Specifically, we first employ an advanced LLM\nto automatically generate Textual Videos comprising continuous textual frames,\nalong with corresponding annotations to simulate real video-text data. Then,\nthese annotated textual videos are used to pre-align a language-only LLM with\nthe video modality. To bridge the gap between textual and real videos, we\nemploy the CLIP model as the feature extractor to align image and text\nmodalities. During text-only pre-alignment, the continuous textual frames,\nencoded as a sequence of CLIP text features, are analogous to continuous CLIP\nimage features, thus aligning the LLM with real video representation. Extensive\nexperiments, including zero-shot evaluation and finetuning on various video\nunderstanding tasks, demonstrate that TOPA is an effective and efficient\nframework for aligning video content with LLMs. In particular, without training\non any video data, the TOPA-Llama2-13B model achieves a Top-1 accuracy of 51.0%\non the challenging long-form video understanding benchmark, Egoschema. This\nperformance surpasses previous video-text pre-training approaches and proves\ncompetitive with recent GPT-3.5-based video agents.\n']",Video Understanding and Multimodal Representation,Multimodal Video and Image Understanding,Multimodal Learning and Applications,Multimodal Learning and Vision-Language Models
87,96,87_relations_relation_entities_entity,"['relations', 'relation', 'entities', 'entity', 'relational', 'semantic', 'textual', 'supervised', 'annotated', 'tagging']","['relation', 'extraction', 'relations', 'entity', 'entities', 'document', 'sentence', 'triples', 'relational', 'shot']","['  We introduce a novel graph-based framework for alleviating key challenges in\ndistantly-supervised relation extraction and demonstrate its effectiveness in\nthe challenging and important domain of biomedical data. Specifically, we\npropose a graph view of sentence bags referring to an entity pair, which\nenables message-passing based aggregation of information related to the entity\npair over the sentence bag. The proposed framework alleviates the common\nproblem of noisy labeling in distantly supervised relation extraction and also\neffectively incorporates inter-dependencies between sentences within a bag.\nExtensive experiments on two large-scale biomedical relation datasets and the\nwidely utilized NYT dataset demonstrate that our proposed framework\nsignificantly outperforms the state-of-the-art methods for biomedical distant\nsupervision relation extraction while also providing excellent performance for\nrelation extraction in the general text mining domain.\n', '  Recent years have seen rapid development in Information Extraction, as well\nas its subtask, Relation Extraction. Relation Extraction is able to detect\nsemantic relations between entities in sentences. Currently, many efficient\napproaches have been applied to relation extraction tasks. Supervised learning\napproaches especially have good performance. However, there are still many\ndifficult challenges. One of the most serious problems is that manually labeled\ndata is difficult to acquire. In most cases, limited data for supervised\napproaches equals lousy performance. Thus here, under the situation with only\nlimited training data, we focus on how to improve the performance of our\nsupervised baseline system with unsupervised pre-training. Feature is one of\nthe key components in improving the supervised approaches. Traditional\napproaches usually apply hand-crafted features, which require expert knowledge\nand expensive human labor. However, this type of feature might suffer from data\nsparsity: when the training set size is small, the model parameters might be\npoorly estimated. In this thesis, we present several novel unsupervised\npre-training models to learn the distributed text representation features,\nwhich are encoded with rich syntactic-semantic patterns of relation\nexpressions. The experiments have demonstrated that this type of feature,\ncombine with the traditional hand-crafted features, could improve the\nperformance of the logistic classification model for relation extraction,\nespecially on the classification of relations with only minor training\ninstances.\n', '  Document-level relation extraction aims at inferring structured human\nknowledge from textual documents. State-of-the-art methods for this task use\npre-trained language models (LMs) via fine-tuning, yet fine-tuning is\ncomputationally expensive and cannot adapt to new relation types or new LMs. As\na remedy, we leverage the generalization capabilities of pre-trained LMs and\npresent a novel framework for document-level in-context few-shot relation\nextraction. Our framework has three strengths: it eliminates the need (1) for\nnamed entity recognition and (2) for human annotations of documents, and (3) it\ncan be updated to new LMs without re-training. We evaluate our framework using\nDocRED, the largest publicly available dataset for document-level relation\nextraction, and demonstrate that our framework achieves state-of-the-art\nperformance. We further show that our framework actually performs much better\nthan the original labels from the development set of DocRED. Finally, we\ndemonstrate that our complete framework yields consistent performance gains\nacross diverse datasets and across different pre-trained LMs. To the best of\nour knowledge, we are the first to reformulate the document-level relation\nextraction task as a tailored in-context few-shot learning paradigm.\n']",Relation Extraction from Textual Data,Information Extraction from Text,Information Extraction,Information Extraction
88,91,88_crop_agriculture_agricultural_crops,"['crop', 'agriculture', 'agricultural', 'crops', 'cnn', 'farming', 'images', 'livestock', 'vision', 'image']","['agricultural', 'agriculture', 'species', 'weed', 'crop', 'monitoring', 'bee', 'food', 'phytoplankton', 'animal']","[""  Vision is a major component in several digital technologies and tools used in\nagriculture. The object detector, You Look Only Once (YOLO), has gained\npopularity in agriculture in a relatively short span due to its\nstate-of-the-art performance. YOLO offers real-time detection with good\naccuracy and is implemented in various agricultural tasks, including\nmonitoring, surveillance, sensing, automation, and robotics. The research and\napplication of YOLO in agriculture are accelerating rapidly but are fragmented\nand multidisciplinary. Moreover, the performance characteristics (i.e.,\naccuracy, speed, computation) of the object detector influence the rate of\ntechnology implementation and adoption in agriculture. Thus, the study aims to\ncollect extensive literature to document and critically evaluate the advances\nand application of YOLO for agricultural object recognition. First, we\nconducted a bibliometric review of 257 articles to understand the scholarly\nlandscape of YOLO in agricultural domain. Secondly, we conducted a systematic\nreview of 30 articles to identify current knowledge, gaps, and modifications in\nYOLO for specific agricultural tasks. The study critically assesses and\nsummarizes the information on YOLO's end-to-end learning approach, including\ndata acquisition, processing, network modification, integration, and\ndeployment. We also discussed task-specific YOLO algorithm modification and\nintegration to meet the agricultural object or environment-specific challenges.\nIn general, YOLO-integrated digital tools and technologies show the potential\nfor real-time, automated monitoring, surveillance, and object handling to\nreduce labor, production cost, and environmental impact while maximizing\nresource efficiency. The study provides detailed documentation and\nsignificantly advances the existing knowledge on applying YOLO in agriculture,\nwhich can greatly benefit the scientific community.\n"", '  Large models can play important roles in many domains. Agriculture is another\nkey factor affecting the lives of people around the world. It provides food,\nfabric, and coal for humanity. However, facing many challenges such as pests\nand diseases, soil degradation, global warming, and food security, how to\nsteadily increase the yield in the agricultural sector is a problem that humans\nstill need to solve. Large models can help farmers improve production\nefficiency and harvest by detecting a series of agricultural production tasks\nsuch as pests and diseases, soil quality, and seed quality. It can also help\nfarmers make wise decisions through a variety of information, such as images,\ntext, etc. Herein, we delve into the potential applications of large models in\nagriculture, from large language model (LLM) and large vision model (LVM) to\nlarge vision-language models (LVLM). After gaining a deeper understanding of\nmultimodal large language models (MLLM), it can be recognized that problems\nsuch as agricultural image processing, agricultural question answering systems,\nand agricultural machine automation can all be solved by large models. Large\nmodels have great potential in the field of agriculture. We outline the current\napplications of agricultural large models, and aims to emphasize the importance\nof large models in the domain of agriculture. In the end, we envisage a future\nin which famers use MLLM to accomplish many tasks in agriculture, which can\ngreatly improve agricultural production efficiency and yield.\n', ""  We present a specialized procedural model for generating synthetic\nagricultural scenes, focusing on soybean crops, along with various weeds. This\nmodel is capable of simulating distinct growth stages of these plants, diverse\nsoil conditions, and randomized field arrangements under varying lighting\nconditions. The integration of real-world textures and environmental factors\ninto the procedural generation process enhances the photorealism and\napplicability of the synthetic data. Our dataset includes 12,000 images with\nsemantic labels, offering a comprehensive resource for computer vision tasks in\nprecision agriculture, such as semantic segmentation for autonomous weed\ncontrol. We validate our model's effectiveness by comparing the synthetic data\nagainst real agricultural images, demonstrating its potential to significantly\naugment training data for machine learning models in agriculture. This approach\nnot only provides a cost-effective solution for generating high-quality,\ndiverse data but also addresses specific needs in agricultural vision tasks\nthat are not fully covered by general-purpose models.\n""]",Agricultural Object Recognition and Monitoring,Computer Vision Applications in Agriculture and Aerial Imagery,Aerial Robotics and Agricultural Computer Vision,Aerial Robotics and Agricultural Computer Vision
89,91,89_editing_editor_editors_edits,"['editing', 'editor', 'editors', 'edits', 'modifying', 'modify', 'edit', 'retraining', 'edited', 'forgetting']","['editing', 'knowledge', 'edited', 'edits', 'edit', 'outdated', 'facts', 'factual', 'lingual', 'update']","[""  Model editing is a technique that edits the large language models (LLMs) with\nupdated knowledge to alleviate hallucinations without resource-intensive\nretraining. While current model editing methods can effectively modify a\nmodel's behavior within a specific area of interest, they often overlook the\npotential unintended side effects on the general abilities of LLMs such as\nreasoning, natural language inference, and question answering. In this paper,\nwe raise concerns that model editing's improvements on factuality may come at\nthe cost of a significant degradation of the model's general abilities. We\nsystematically analyze the side effects by evaluating four popular editing\nmethods on three LLMs across eight representative tasks. Our extensive\nempirical experiments show that it is challenging for current editing methods\nto simultaneously improve factuality of LLMs and maintain their general\nabilities. Our analysis reveals that the side effects are caused by model\nediting altering the original model weights excessively, leading to overfitting\nto the edited facts. To mitigate this, a method named RECT (RElative Change in\nweighT) is proposed to regularize the edit update weights. Evaluation results\nshow that RECT can significantly mitigate the side effects of editing while\nstill maintaining over 94% editing performance.\n"", '  Knowledge editing aims to rectify inaccuracies in large language models\n(LLMs) without costly retraining for outdated or erroneous knowledge. However,\ncurrent knowledge editing methods primarily focus on single editing, failing to\nmeet the requirements for lifelong editing. This study reveals a performance\ndegradation encountered by knowledge editing in lifelong editing, characterized\nby toxicity buildup and toxicity flash, with the primary cause identified as\npattern unmatch. We introduce a knowledge editing approach named Wise-Layer\nKnowledge Editor (WilKE), which selects editing layer based on the pattern\nmatching degree of editing knowledge across different layers in language\nmodels. Experimental results demonstrate that, in lifelong editing, WilKE\nexhibits an average improvement of 46.2% and 67.8% on editing GPT2-XL and GPT-J\nrelative to state-of-the-art knowledge editing methods.\n', '  This study presents a targeted model editing analysis focused on the latest\nlarge language model, Llama-3. We explore the efficacy of popular model editing\ntechniques - ROME, MEMIT, and EMMET, which are designed for precise layer\ninterventions. We identify the most effective layers for targeted edits through\nan evaluation that encompasses up to 4096 edits across three distinct\nstrategies: sequential editing, batch editing, and a hybrid approach we call as\nsequential-batch editing. Our findings indicate that increasing edit\nbatch-sizes may degrade model performance more significantly than using smaller\nedit batches sequentially for equal number of edits. With this, we argue that\nsequential model editing is an important component for scaling model editing\nmethods and future research should focus on methods that combine both batched\nand sequential editing. This observation suggests a potential limitation in\ncurrent model editing methods which push towards bigger edit batch sizes, and\nwe hope it paves way for future investigations into optimizing batch sizes and\nmodel editing performance.\n']",Model Editing in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
89,91,89_editing_editor_editors_edits,"['editing', 'editor', 'editors', 'edits', 'modifying', 'modify', 'edit', 'retraining', 'edited', 'forgetting']","['editing', 'knowledge', 'edited', 'edits', 'edit', 'outdated', 'facts', 'factual', 'lingual', 'update']","[""  Model editing is a technique that edits the large language models (LLMs) with\nupdated knowledge to alleviate hallucinations without resource-intensive\nretraining. While current model editing methods can effectively modify a\nmodel's behavior within a specific area of interest, they often overlook the\npotential unintended side effects on the general abilities of LLMs such as\nreasoning, natural language inference, and question answering. In this paper,\nwe raise concerns that model editing's improvements on factuality may come at\nthe cost of a significant degradation of the model's general abilities. We\nsystematically analyze the side effects by evaluating four popular editing\nmethods on three LLMs across eight representative tasks. Our extensive\nempirical experiments show that it is challenging for current editing methods\nto simultaneously improve factuality of LLMs and maintain their general\nabilities. Our analysis reveals that the side effects are caused by model\nediting altering the original model weights excessively, leading to overfitting\nto the edited facts. To mitigate this, a method named RECT (RElative Change in\nweighT) is proposed to regularize the edit update weights. Evaluation results\nshow that RECT can significantly mitigate the side effects of editing while\nstill maintaining over 94% editing performance.\n"", '  Knowledge editing aims to rectify inaccuracies in large language models\n(LLMs) without costly retraining for outdated or erroneous knowledge. However,\ncurrent knowledge editing methods primarily focus on single editing, failing to\nmeet the requirements for lifelong editing. This study reveals a performance\ndegradation encountered by knowledge editing in lifelong editing, characterized\nby toxicity buildup and toxicity flash, with the primary cause identified as\npattern unmatch. We introduce a knowledge editing approach named Wise-Layer\nKnowledge Editor (WilKE), which selects editing layer based on the pattern\nmatching degree of editing knowledge across different layers in language\nmodels. Experimental results demonstrate that, in lifelong editing, WilKE\nexhibits an average improvement of 46.2% and 67.8% on editing GPT2-XL and GPT-J\nrelative to state-of-the-art knowledge editing methods.\n', '  This study presents a targeted model editing analysis focused on the latest\nlarge language model, Llama-3. We explore the efficacy of popular model editing\ntechniques - ROME, MEMIT, and EMMET, which are designed for precise layer\ninterventions. We identify the most effective layers for targeted edits through\nan evaluation that encompasses up to 4096 edits across three distinct\nstrategies: sequential editing, batch editing, and a hybrid approach we call as\nsequential-batch editing. Our findings indicate that increasing edit\nbatch-sizes may degrade model performance more significantly than using smaller\nedit batches sequentially for equal number of edits. With this, we argue that\nsequential model editing is an important component for scaling model editing\nmethods and future research should focus on methods that combine both batched\nand sequential editing. This observation suggests a potential limitation in\ncurrent model editing methods which push towards bigger edit batch sizes, and\nwe hope it paves way for future investigations into optimizing batch sizes and\nmodel editing performance.\n']",Model Editing in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
89,91,89_editing_editor_editors_edits,"['editing', 'editor', 'editors', 'edits', 'modifying', 'modify', 'edit', 'retraining', 'edited', 'forgetting']","['editing', 'knowledge', 'edited', 'edits', 'edit', 'outdated', 'facts', 'factual', 'lingual', 'update']","[""  Model editing is a technique that edits the large language models (LLMs) with\nupdated knowledge to alleviate hallucinations without resource-intensive\nretraining. While current model editing methods can effectively modify a\nmodel's behavior within a specific area of interest, they often overlook the\npotential unintended side effects on the general abilities of LLMs such as\nreasoning, natural language inference, and question answering. In this paper,\nwe raise concerns that model editing's improvements on factuality may come at\nthe cost of a significant degradation of the model's general abilities. We\nsystematically analyze the side effects by evaluating four popular editing\nmethods on three LLMs across eight representative tasks. Our extensive\nempirical experiments show that it is challenging for current editing methods\nto simultaneously improve factuality of LLMs and maintain their general\nabilities. Our analysis reveals that the side effects are caused by model\nediting altering the original model weights excessively, leading to overfitting\nto the edited facts. To mitigate this, a method named RECT (RElative Change in\nweighT) is proposed to regularize the edit update weights. Evaluation results\nshow that RECT can significantly mitigate the side effects of editing while\nstill maintaining over 94% editing performance.\n"", '  Knowledge editing aims to rectify inaccuracies in large language models\n(LLMs) without costly retraining for outdated or erroneous knowledge. However,\ncurrent knowledge editing methods primarily focus on single editing, failing to\nmeet the requirements for lifelong editing. This study reveals a performance\ndegradation encountered by knowledge editing in lifelong editing, characterized\nby toxicity buildup and toxicity flash, with the primary cause identified as\npattern unmatch. We introduce a knowledge editing approach named Wise-Layer\nKnowledge Editor (WilKE), which selects editing layer based on the pattern\nmatching degree of editing knowledge across different layers in language\nmodels. Experimental results demonstrate that, in lifelong editing, WilKE\nexhibits an average improvement of 46.2% and 67.8% on editing GPT2-XL and GPT-J\nrelative to state-of-the-art knowledge editing methods.\n', '  This study presents a targeted model editing analysis focused on the latest\nlarge language model, Llama-3. We explore the efficacy of popular model editing\ntechniques - ROME, MEMIT, and EMMET, which are designed for precise layer\ninterventions. We identify the most effective layers for targeted edits through\nan evaluation that encompasses up to 4096 edits across three distinct\nstrategies: sequential editing, batch editing, and a hybrid approach we call as\nsequential-batch editing. Our findings indicate that increasing edit\nbatch-sizes may degrade model performance more significantly than using smaller\nedit batches sequentially for equal number of edits. With this, we argue that\nsequential model editing is an important component for scaling model editing\nmethods and future research should focus on methods that combine both batched\nand sequential editing. This observation suggests a potential limitation in\ncurrent model editing methods which push towards bigger edit batch sizes, and\nwe hope it paves way for future investigations into optimizing batch sizes and\nmodel editing performance.\n']",Model Editing in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
89,91,89_editing_editor_editors_edits,"['editing', 'editor', 'editors', 'edits', 'modifying', 'modify', 'edit', 'retraining', 'edited', 'forgetting']","['editing', 'knowledge', 'edited', 'edits', 'edit', 'outdated', 'facts', 'factual', 'lingual', 'update']","[""  Model editing is a technique that edits the large language models (LLMs) with\nupdated knowledge to alleviate hallucinations without resource-intensive\nretraining. While current model editing methods can effectively modify a\nmodel's behavior within a specific area of interest, they often overlook the\npotential unintended side effects on the general abilities of LLMs such as\nreasoning, natural language inference, and question answering. In this paper,\nwe raise concerns that model editing's improvements on factuality may come at\nthe cost of a significant degradation of the model's general abilities. We\nsystematically analyze the side effects by evaluating four popular editing\nmethods on three LLMs across eight representative tasks. Our extensive\nempirical experiments show that it is challenging for current editing methods\nto simultaneously improve factuality of LLMs and maintain their general\nabilities. Our analysis reveals that the side effects are caused by model\nediting altering the original model weights excessively, leading to overfitting\nto the edited facts. To mitigate this, a method named RECT (RElative Change in\nweighT) is proposed to regularize the edit update weights. Evaluation results\nshow that RECT can significantly mitigate the side effects of editing while\nstill maintaining over 94% editing performance.\n"", '  Knowledge editing aims to rectify inaccuracies in large language models\n(LLMs) without costly retraining for outdated or erroneous knowledge. However,\ncurrent knowledge editing methods primarily focus on single editing, failing to\nmeet the requirements for lifelong editing. This study reveals a performance\ndegradation encountered by knowledge editing in lifelong editing, characterized\nby toxicity buildup and toxicity flash, with the primary cause identified as\npattern unmatch. We introduce a knowledge editing approach named Wise-Layer\nKnowledge Editor (WilKE), which selects editing layer based on the pattern\nmatching degree of editing knowledge across different layers in language\nmodels. Experimental results demonstrate that, in lifelong editing, WilKE\nexhibits an average improvement of 46.2% and 67.8% on editing GPT2-XL and GPT-J\nrelative to state-of-the-art knowledge editing methods.\n', '  This study presents a targeted model editing analysis focused on the latest\nlarge language model, Llama-3. We explore the efficacy of popular model editing\ntechniques - ROME, MEMIT, and EMMET, which are designed for precise layer\ninterventions. We identify the most effective layers for targeted edits through\nan evaluation that encompasses up to 4096 edits across three distinct\nstrategies: sequential editing, batch editing, and a hybrid approach we call as\nsequential-batch editing. Our findings indicate that increasing edit\nbatch-sizes may degrade model performance more significantly than using smaller\nedit batches sequentially for equal number of edits. With this, we argue that\nsequential model editing is an important component for scaling model editing\nmethods and future research should focus on methods that combine both batched\nand sequential editing. This observation suggests a potential limitation in\ncurrent model editing methods which push towards bigger edit batch sizes, and\nwe hope it paves way for future investigations into optimizing batch sizes and\nmodel editing performance.\n']",Model Editing in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
89,91,89_editing_editor_editors_edits,"['editing', 'editor', 'editors', 'edits', 'modifying', 'modify', 'edit', 'retraining', 'edited', 'forgetting']","['editing', 'knowledge', 'edited', 'edits', 'edit', 'outdated', 'facts', 'factual', 'lingual', 'update']","[""  Model editing is a technique that edits the large language models (LLMs) with\nupdated knowledge to alleviate hallucinations without resource-intensive\nretraining. While current model editing methods can effectively modify a\nmodel's behavior within a specific area of interest, they often overlook the\npotential unintended side effects on the general abilities of LLMs such as\nreasoning, natural language inference, and question answering. In this paper,\nwe raise concerns that model editing's improvements on factuality may come at\nthe cost of a significant degradation of the model's general abilities. We\nsystematically analyze the side effects by evaluating four popular editing\nmethods on three LLMs across eight representative tasks. Our extensive\nempirical experiments show that it is challenging for current editing methods\nto simultaneously improve factuality of LLMs and maintain their general\nabilities. Our analysis reveals that the side effects are caused by model\nediting altering the original model weights excessively, leading to overfitting\nto the edited facts. To mitigate this, a method named RECT (RElative Change in\nweighT) is proposed to regularize the edit update weights. Evaluation results\nshow that RECT can significantly mitigate the side effects of editing while\nstill maintaining over 94% editing performance.\n"", '  Knowledge editing aims to rectify inaccuracies in large language models\n(LLMs) without costly retraining for outdated or erroneous knowledge. However,\ncurrent knowledge editing methods primarily focus on single editing, failing to\nmeet the requirements for lifelong editing. This study reveals a performance\ndegradation encountered by knowledge editing in lifelong editing, characterized\nby toxicity buildup and toxicity flash, with the primary cause identified as\npattern unmatch. We introduce a knowledge editing approach named Wise-Layer\nKnowledge Editor (WilKE), which selects editing layer based on the pattern\nmatching degree of editing knowledge across different layers in language\nmodels. Experimental results demonstrate that, in lifelong editing, WilKE\nexhibits an average improvement of 46.2% and 67.8% on editing GPT2-XL and GPT-J\nrelative to state-of-the-art knowledge editing methods.\n', '  This study presents a targeted model editing analysis focused on the latest\nlarge language model, Llama-3. We explore the efficacy of popular model editing\ntechniques - ROME, MEMIT, and EMMET, which are designed for precise layer\ninterventions. We identify the most effective layers for targeted edits through\nan evaluation that encompasses up to 4096 edits across three distinct\nstrategies: sequential editing, batch editing, and a hybrid approach we call as\nsequential-batch editing. Our findings indicate that increasing edit\nbatch-sizes may degrade model performance more significantly than using smaller\nedit batches sequentially for equal number of edits. With this, we argue that\nsequential model editing is an important component for scaling model editing\nmethods and future research should focus on methods that combine both batched\nand sequential editing. This observation suggests a potential limitation in\ncurrent model editing methods which push towards bigger edit batch sizes, and\nwe hope it paves way for future investigations into optimizing batch sizes and\nmodel editing performance.\n']",Model Editing in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
89,91,89_editing_editor_editors_edits,"['editing', 'editor', 'editors', 'edits', 'modifying', 'modify', 'edit', 'retraining', 'edited', 'forgetting']","['editing', 'knowledge', 'edited', 'edits', 'edit', 'outdated', 'facts', 'factual', 'lingual', 'update']","[""  Model editing is a technique that edits the large language models (LLMs) with\nupdated knowledge to alleviate hallucinations without resource-intensive\nretraining. While current model editing methods can effectively modify a\nmodel's behavior within a specific area of interest, they often overlook the\npotential unintended side effects on the general abilities of LLMs such as\nreasoning, natural language inference, and question answering. In this paper,\nwe raise concerns that model editing's improvements on factuality may come at\nthe cost of a significant degradation of the model's general abilities. We\nsystematically analyze the side effects by evaluating four popular editing\nmethods on three LLMs across eight representative tasks. Our extensive\nempirical experiments show that it is challenging for current editing methods\nto simultaneously improve factuality of LLMs and maintain their general\nabilities. Our analysis reveals that the side effects are caused by model\nediting altering the original model weights excessively, leading to overfitting\nto the edited facts. To mitigate this, a method named RECT (RElative Change in\nweighT) is proposed to regularize the edit update weights. Evaluation results\nshow that RECT can significantly mitigate the side effects of editing while\nstill maintaining over 94% editing performance.\n"", '  Knowledge editing aims to rectify inaccuracies in large language models\n(LLMs) without costly retraining for outdated or erroneous knowledge. However,\ncurrent knowledge editing methods primarily focus on single editing, failing to\nmeet the requirements for lifelong editing. This study reveals a performance\ndegradation encountered by knowledge editing in lifelong editing, characterized\nby toxicity buildup and toxicity flash, with the primary cause identified as\npattern unmatch. We introduce a knowledge editing approach named Wise-Layer\nKnowledge Editor (WilKE), which selects editing layer based on the pattern\nmatching degree of editing knowledge across different layers in language\nmodels. Experimental results demonstrate that, in lifelong editing, WilKE\nexhibits an average improvement of 46.2% and 67.8% on editing GPT2-XL and GPT-J\nrelative to state-of-the-art knowledge editing methods.\n', '  This study presents a targeted model editing analysis focused on the latest\nlarge language model, Llama-3. We explore the efficacy of popular model editing\ntechniques - ROME, MEMIT, and EMMET, which are designed for precise layer\ninterventions. We identify the most effective layers for targeted edits through\nan evaluation that encompasses up to 4096 edits across three distinct\nstrategies: sequential editing, batch editing, and a hybrid approach we call as\nsequential-batch editing. Our findings indicate that increasing edit\nbatch-sizes may degrade model performance more significantly than using smaller\nedit batches sequentially for equal number of edits. With this, we argue that\nsequential model editing is an important component for scaling model editing\nmethods and future research should focus on methods that combine both batched\nand sequential editing. This observation suggests a potential limitation in\ncurrent model editing methods which push towards bigger edit batch sizes, and\nwe hope it paves way for future investigations into optimizing batch sizes and\nmodel editing performance.\n']",Model Editing in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
90,91,90_credibility_factagent_news_disinformation,"['credibility', 'factagent', 'news', 'disinformation', 'tweets', 'veracity', 'journalists', 'hoax', 'content', 'debunking']","['news', 'fake', 'misinformation', 'media', 'detection', 'credibility', 'checking', 'articles', 'fact', 'content']","['  The prevalence of fake news across various online sources has had a\nsignificant influence on the public. Existing Chinese fake news detection\ndatasets are limited to news sourced solely from Weibo. However, fake news\noriginating from multiple sources exhibits diversity in various aspects,\nincluding its content and social context. Methods trained on purely one single\nnews source can hardly be applicable to real-world scenarios. Our pilot\nexperiment demonstrates that the F1 score of the state-of-the-art method that\nlearns from a large Chinese fake news detection dataset, Weibo-21, drops\nsignificantly from 0.943 to 0.470 when the test data is changed to multi-source\nnews data, failing to identify more than one-third of the multi-source fake\nnews. To address this limitation, we constructed the first multi-source\nbenchmark dataset for Chinese fake news detection, termed MCFEND, which is\ncomposed of news we collected from diverse sources such as social platforms,\nmessaging apps, and traditional online news outlets. Notably, such news has\nbeen fact-checked by 14 authoritative fact-checking agencies worldwide. In\naddition, various existing Chinese fake news detection methods are thoroughly\nevaluated on our proposed dataset in cross-source, multi-source, and unseen\nsource ways. MCFEND, as a benchmark dataset, aims to advance Chinese fake news\ndetection approaches in real-world scenarios.\n', '  In the age of large language models (LLMs) and the widespread adoption of\nAI-driven content creation, the landscape of information dissemination has\nwitnessed a paradigm shift. With the proliferation of both human-written and\nmachine-generated real and fake news, robustly and effectively discerning the\nveracity of news articles has become an intricate challenge. While substantial\nresearch has been dedicated to fake news detection, this either assumes that\nall news articles are human-written or abruptly assumes that all\nmachine-generated news are fake. Thus, a significant gap exists in\nunderstanding the interplay between machine-(paraphrased) real news,\nmachine-generated fake news, human-written fake news, and human-written real\nnews. In this paper, we study this gap by conducting a comprehensive evaluation\nof fake news detectors trained in various scenarios. Our primary objectives\nrevolve around the following pivotal question: How to adapt fake news detectors\nto the era of LLMs? Our experiments reveal an interesting pattern that\ndetectors trained exclusively on human-written articles can indeed perform well\nat detecting machine-generated fake news, but not vice versa. Moreover, due to\nthe bias of detectors against machine-generated texts \\cite{su2023fake}, they\nshould be trained on datasets with a lower machine-generated news ratio than\nthe test set. Building on our findings, we provide a practical strategy for the\ndevelopment of robust fake news detectors.\n', '  Fake news significantly influence our society. They impact consumers, voters,\nand many other societal groups. While Fake News exist for a centuries,\nGenerative AI brings fake news on a new level. It is now possible to automate\nthe creation of masses of high-quality individually targeted Fake News. On the\nother end, Generative AI can also help detecting Fake News. Both fields are\nyoung but developing fast.\n  This survey provides a comprehensive examination of the research and\npractical use of Generative AI for Fake News detection and creation in 2024.\nFollowing the Structured Literature Survey approach, the paper synthesizes\ncurrent results in the following topic clusters 1) enabling technologies, 2)\ncreation of Fake News, 3) case study social media as most relevant distribution\nchannel, 4) detection of Fake News, and 5) deepfakes as upcoming technology.\n  The article also identifies current challenges and open issues.\n']",Fake News Detection and Analysis,Misinformation and Disinformation Detection,Information Verification and Validation,Information Verification and Validation
91,89,91_auction_bidding_auctions_bids,"['auction', 'bidding', 'auctions', 'bids', 'bid', 'bidders', 'bandit', 'optimal', 'pricing', 'bidder']","['auctions', 'auction', 'price', 'pricing', 'seller', 'revenue', 'regret', 'valuations', 'prices', 'advertising']","['  Learning to bid in repeated first-price auctions is a fundamental problem at\nthe interface of game theory and machine learning, which has seen a recent\nsurge in interest due to the transition of display advertising to first-price\nauctions. In this work, we propose a novel concave formulation for\npure-strategy bidding in first-price auctions, and use it to analyze natural\nGradient-Ascent-based algorithms for this problem. Importantly, our analysis\ngoes beyond regret, which was the typical focus of past work, and also accounts\nfor the strategic backdrop of online-advertising markets where bidding\nalgorithms are deployed -- we provide the first guarantees of\nstrategic-robustness and incentive-compatibility for Gradient Ascent.\n  Concretely, we show that our algorithms achieve $O(\\sqrt{T})$ regret when the\nhighest competing bids are generated adversarially, and show that no online\nalgorithm can do better. We further prove that the regret reduces to $O(\\log\nT)$ when the competition is stationary and stochastic, which drastically\nimproves upon the previous best of $O(\\sqrt{T})$. Moving beyond regret, we show\nthat a strategic seller cannot exploit our algorithms to extract more revenue\non average than is possible under the optimal mechanism. Finally, we prove that\nour algorithm is also incentive compatible -- it is a (nearly) dominant\nstrategy for the buyer to report her values truthfully to the algorithm as a\nwhole. Altogether, these guarantees make our algorithms the first to\nsimultaneously achieve both optimal regret and strategic-robustness.\n', ""  Advertisers increasingly use automated bidding to optimize their ad campaigns\non online advertising platforms. Autobidding optimizes an advertiser's\nobjective subject to various constraints, e.g. average ROI and budget\nconstraints. In this paper, we study the problem of designing online\nautobidding algorithms to optimize value subject to ROI and budget constraints\nwhen the platform is running any mixture of first and second price auction.\n  We consider the following stochastic setting: There is an item for sale in\neach of $T$ rounds. In each round, buyers submit bids and an auction is run to\nsell the item. We focus on one buyer, possibly with budget and ROI constraints.\nWe assume that the buyer's value and the highest competing bid are drawn i.i.d.\nfrom some unknown (joint) distribution in each round. We design a low-regret\nbidding algorithm that satisfies the buyer's constraints. Our benchmark is the\nobjective value achievable by the best possible Lipschitz function that maps\nvalues to bids, which is rich enough to best respond to many different\ncorrelation structures between value and highest competing bid. Our main result\nis an algorithm with full information feedback that guarantees a near-optimal\n$\\tilde O(\\sqrt T)$ regret with respect to the best Lipschitz function. Our\nresult applies to a wide range of auctions, most notably any mixture of first\nand second price auctions (price is a convex combination of the first and\nsecond price). In addition, our result holds for both value-maximizing buyers\nand quasi-linear utility-maximizing buyers.\n  We also study the bandit setting, where we show an $\\Omega(T^{2/3})$ lower\nbound on the regret for first-price auctions, showing a large disparity between\nthe full information and bandit settings. We also design an algorithm with\n$\\tilde O(T^{3/4})$ regret, when the value distribution is known and is\nindependent of the highest competing bid.\n"", ""  We consider repeated multi-unit auctions with uniform pricing, which are\nwidely used in practice for allocating goods such as carbon licenses. In each\nround, $K$ identical units of a good are sold to a group of buyers that have\nvaluations with diminishing marginal returns. The buyers submit bids for the\nunits, and then a price $p$ is set per unit so that all the units are sold. We\nconsider two variants of the auction, where the price is set to the $K$-th\nhighest bid and $(K+1)$-st highest bid, respectively.\n  We analyze the properties of this auction in both the offline and online\nsettings. In the offline setting, we consider the problem that one player $i$\nis facing: given access to a data set that contains the bids submitted by\ncompetitors in past auctions, find a bid vector that maximizes player $i$'s\ncumulative utility on the data set. We design a polynomial time algorithm for\nthis problem, by showing it is equivalent to finding a maximum-weight path on a\ncarefully constructed directed acyclic graph.\n  In the online setting, the players run learning algorithms to update their\nbids as they participate in the auction over time. Based on our offline\nalgorithm, we design efficient online learning algorithms for bidding. The\nalgorithms have sublinear regret, under both full information and bandit\nfeedback structures. We complement our online learning algorithms with regret\nlower bounds.\n  Finally, we analyze the quality of the equilibria in the worst case through\nthe lens of the core solution concept in the game among the bidders. We show\nthat the $(K+1)$-st price format is susceptible to collusion among the bidders;\nmeanwhile, the $K$-th price format does not have this issue.\n""]",Auction Bidding Strategies and Mechanisms,Mechanisms and Strategies in Auctions and Voting Systems,Optimization and Decision Making in Complex Systems,Complex System Optimization and Management
92,88,92_models_knowledge_trained_training,"['models', 'knowledge', 'trained', 'training', 'teacher', 'students', 'distillation', 'teachers', 'distilling', 'model']","['teacher', 'student', 'distillation', 'knowledge', 'teachers', 'logits', 'transfer', 'smaller', 'compact', 'model']","[""  Knowledge Distillation (KD) has proven effective for compressing large\nteacher models into smaller student models. While it is well known that student\nmodels can achieve similar accuracies as the teachers, it has also been shown\nthat they nonetheless often do not learn the same function. It is, however,\noften highly desirable that the student's and teacher's functions share similar\nproperties such as basing the prediction on the same input features, as this\nensures that students learn the 'right features' from the teachers. In this\nwork, we explore whether this can be achieved by not only optimizing the\nclassic KD loss but also the similarity of the explanations generated by the\nteacher and the student. Despite the idea being simple and intuitive, we find\nthat our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides\nlarge gains in terms of accuracy and student-teacher agreement, (2) ensures\nthat the student learns from the teacher to be right for the right reasons and\nto give similar explanations, and (3) is robust with respect to the model\narchitectures, the amount of training data, and even works with 'approximate',\npre-computed explanations.\n"", ""  Knowledge distillation (KD) involves transferring the knowledge from one\nneural network to another, often from a larger, well-trained model (teacher) to\na smaller, more efficient model (student). Traditional KD methods minimize the\nKullback-Leibler (KL) divergence between the probabilistic outputs of the\nteacher and student networks. However, this approach often overlooks crucial\nstructural knowledge embedded within the teacher's network. In this paper, we\nintroduce Invariant Consistency Distillation (ICD), a novel methodology\ndesigned to enhance KD by ensuring that the student model's representations are\nconsistent with those of the teacher. Our approach combines contrastive\nlearning with an explicit invariance penalty, capturing significantly more\ninformation from the teacher's representation of the data. Our results on\nCIFAR-100 demonstrate that ICD outperforms traditional KD techniques and\nsurpasses 13 state-of-the-art methods. In some cases, the student model even\nexceeds the teacher model in terms of accuracy. Furthermore, we successfully\ntransfer our method to other datasets, including Tiny ImageNet and STL-10. The\ncode will be made public soon.\n"", '  Deep cascaded architectures for magnetic resonance imaging (MRI) acceleration\nhave shown remarkable success in providing high-quality reconstruction.\nHowever, as the number of cascades increases, the improvements in\nreconstruction tend to become marginal, indicating possible excess model\ncapacity. Knowledge distillation (KD) is an emerging technique to compress\nthese models, in which a trained deep teacher network is used to distill\nknowledge to a smaller student network such that the student learns to mimic\nthe behavior of the teacher. Most KD methods focus on effectively training the\nstudent with a pre-trained teacher unaware of the student model. We propose\nSFT-KD-Recon, a student-friendly teacher training approach along with the\nstudent as a prior step to KD to make the teacher aware of the structure and\ncapacity of the student and enable aligning the representations of the teacher\nwith the student. In SFT, the teacher is jointly trained with the unfolded\nbranch configurations of the student blocks using three loss terms -\nteacher-reconstruction loss, student-reconstruction loss, and teacher-student\nimitation loss, followed by KD of the student. We perform extensive experiments\nfor MRI acceleration in 4x and 5x under-sampling on the brain and cardiac\ndatasets on five KD methods using the proposed approach as a prior step. We\nconsider the DC-CNN architecture and setup teacher as D5C5 (141765 parameters),\nand student as D3C5 (49285 parameters), denoting a compression of 2.87:1.\nResults show that (i) our approach consistently improves the KD methods with\nimproved reconstruction performance and image quality, and (ii) the student\ndistilled using our approach is competitive with the teacher, with the\nperformance gap reduced from 0.53 dB to 0.03 dB.\n']",Knowledge Distillation for Model Compression,Model and Data Distillation Techniques,Model and Data Distillation Techniques,Model and Data Distillation Techniques
93,88,93_clusterings_clustering_cluster_clusters,"['clusterings', 'clustering', 'cluster', 'clusters', 'clustered', 'dbscan', 'algorithms', 'algorithm', 'hpclust', 'unsupervised']","['clustering', 'clusters', 'cluster', 'means', 'index', 'points', 'algorithm', 'algorithms', 'density', 'similarity']","[""  Clustering algorithms aim to organize data into groups or clusters based on\nthe inherent patterns and similarities within the data. They play an important\nrole in today's life, such as in marketing and e-commerce, healthcare, data\norganization and analysis, and social media. Numerous clustering algorithms\nexist, with ongoing developments introducing new ones. Each algorithm possesses\nits own set of strengths and weaknesses, and as of now, there is no universally\napplicable algorithm for all tasks. In this work, we analyzed existing\nclustering algorithms and classify mainstream algorithms across five different\ndimensions: underlying principles and characteristics, data point assignment to\nclusters, dataset capacity, predefined cluster numbers and application area.\nThis classification facilitates researchers in understanding clustering\nalgorithms from various perspectives and helps them identify algorithms\nsuitable for solving specific tasks. Finally, we discussed the current trends\nand potential future directions in clustering algorithms. We also identified\nand discussed open challenges and unresolved issues in the field.\n"", '  Data clustering involves identifying latent similarities within a dataset and\norganizing them into clusters or groups. The outcomes of various clustering\nalgorithms differ as they are susceptible to the intrinsic characteristics of\nthe original dataset, including noise and dimensionality. The effectiveness of\nsuch clustering procedures directly impacts the homogeneity of clusters,\nunderscoring the significance of evaluating algorithmic outcomes. Consequently,\nthe assessment of clustering quality presents a significant and complex\nendeavor. A pivotal aspect affecting clustering validation is the cluster\nvalidity metric, which aids in determining the optimal number of clusters. The\nmain goal of this study is to comprehensively review and explain the\nmathematical operation of internal and external cluster validity indices, but\nnot all, to categorize these indices and to brainstorm suggestions for future\nadvancement of clustering validation research. In addition, we review and\nevaluate the performance of internal and external clustering validation indices\non the most common clustering algorithms, such as the evolutionary clustering\nalgorithm star (ECA*). Finally, we suggest a classification framework for\nexamining the functionality of both internal and external clustering validation\nmeasures regarding their ideal values, user-friendliness, responsiveness to\ninput data, and appropriateness across various fields. This classification aids\nresearchers in selecting the appropriate clustering validation measure to suit\ntheir specific requirements.\n', '  This paper focuses on density-based clustering, particularly the Density Peak\n(DP) algorithm and the one based on density-connectivity DBSCAN; and proposes a\nnew method which takes advantage of the individual strengths of these two\nmethods to yield a density-based hierarchical clustering algorithm. Our\ninvestigation begins with formally defining the types of clusters DP and DBSCAN\nare designed to detect; and then identifies the kinds of distributions that DP\nand DBSCAN individually fail to detect all clusters in a dataset. These\nidentified weaknesses inspire us to formally define a new kind of clusters and\npropose a new method called DC-HDP to overcome these weaknesses to identify\nclusters with arbitrary shapes and varied densities. In addition, the new\nmethod produces a richer clustering result in terms of hierarchy or dendrogram\nfor better cluster structures understanding. Our empirical evaluation results\nshow that DC-HDP produces the best clustering results on 14 datasets in\ncomparison with 7 state-of-the-art clustering algorithms.\n']",Clustering Algorithms and Techniques,Clustering Techniques and Algorithms,Data Analysis and Pattern Discovery,Data Analysis and Pattern Discovery
94,88,94_mri_imaging_denoising_tomography,"['mri', 'imaging', 'denoising', 'tomography', 'reconstructed', 'undersampled', 'undersampling', 'deep', 'reconstruction', 'reconstructions']","['reconstruction', 'imaging', 'resonance', 'magnetic', 'undersampled', 'dose', 'motion', 'image', 'resolution', 'tomography']","[""  Image generative AI has garnered significant attention in recent years. In\nparticular, the diffusion model, a core component of recent generative AI,\nproduces high-quality images with rich diversity. In this study, we propose a\nnovel CT reconstruction method by combining the denoising diffusion\nprobabilistic model with iterative CT reconstruction. In sharp contrast to\nprevious studies, we optimize the fidelity loss of CT reconstruction with\nrespect to the latent variable of the diffusion model, instead of the image and\nmodel parameters. To suppress anatomical structure changes produced by the\ndiffusion model, we shallow the diffusion and reverse processes, and fix a set\nof added noises in the reverse process to make it deterministic during\ninference. We demonstrate the effectiveness of the proposed method through\nsparse view CT reconstruction of 1/10 view projection data. Despite the\nsimplicity of the implementation, the proposed method shows the capability of\nreconstructing high-quality images while preserving the patient's anatomical\nstructure, and outperforms existing methods including iterative reconstruction,\niterative reconstruction with total variation, and the diffusion model alone in\nterms of quantitative indices such as SSIM and PSNR. We also explore further\nsparse view CT using 1/20 view projection data with the same trained diffusion\nmodel. As the number of iterations increases, image quality improvement\ncomparable to that of 1/10 sparse view CT reconstruction is achieved. In\nprinciple, the proposed method can be widely applied not only to CT but also to\nother imaging modalities such as MRI, PET, and SPECT.\n"", ""  Motion artifacts in Magnetic Resonance Imaging (MRI) are one of the\nfrequently occurring artifacts due to patient movements during scanning. Motion\nis estimated to be present in approximately 30% of clinical MRI scans; however,\nmotion has not been explicitly modeled within deep learning image\nreconstruction models. Deep learning (DL) algorithms have been demonstrated to\nbe effective for both the image reconstruction task and the motion correction\ntask, but the two tasks are considered separately. The image reconstruction\ntask involves removing undersampling artifacts such as noise and aliasing\nartifacts, whereas motion correction involves removing artifacts including\nblurring, ghosting, and ringing. In this work, we propose a novel method to\nsimultaneously accelerate imaging and correct motion. This is achieved by\nintegrating a motion module into the deep learning-based MRI reconstruction\nprocess, enabling real-time detection and correction of motion. We model motion\nas a tightly integrated auxiliary layer in the deep learning model during\ntraining, making the deep learning model 'motion-informed'. During inference,\nimage reconstruction is performed from undersampled raw k-space data using a\ntrained motion-informed DL model. Experimental results demonstrate that the\nproposed motion-informed deep learning image reconstruction network\noutperformed the conventional image reconstruction network for motion-degraded\nMRI datasets.\n"", ""  In Magnetic Resonance Imaging (MRI), image acquisitions are often\nundersampled in the measurement domain to accelerate the scanning process, at\nthe expense of image quality. However, image quality is a crucial factor that\ninfluences the accuracy of clinical diagnosis; hence, high-quality image\nreconstruction from undersampled measurements has been a key area of research.\nRecently, deep learning (DL) methods have emerged as the state-of-the-art for\nMRI reconstruction, typically involving deep neural networks to transform\nundersampled MRI images into high-quality MRI images through data-driven\nprocesses. Nevertheless, there is clear and significant room for improvement in\nundersampled DL MRI reconstruction to meet the high standards required for\nclinical diagnosis, in terms of eliminating aliasing artifacts and reducing\nimage noise. In this paper, we introduce a self-supervised pretraining\nprocedure using contrastive learning to improve the accuracy of undersampled DL\nMRI reconstruction. We use contrastive learning to transform the MRI image\nrepresentations into a latent space that maximizes mutual information among\ndifferent undersampled representations and optimizes the information content at\nthe input of the downstream DL reconstruction models. Our experiments\ndemonstrate improved reconstruction accuracy across a range of acceleration\nfactors and datasets, both quantitatively and qualitatively. Furthermore, our\nextended experiments validate the proposed framework's robustness under\nadversarial conditions, such as measurement noise, different k-space sampling\npatterns, and pathological abnormalities, and also prove the transfer learning\ncapabilities on MRI datasets with completely different anatomy. Additionally,\nwe conducted experiments to visualize and analyze the properties of the\nproposed MRI contrastive learning latent space.\n""]",Deep Learning for MRI Reconstruction and Imaging,Deep Learning for Medical Imaging and Analysis,Deep Learning for Medical Imaging and Analysis,Deep Learning for Medical Imaging and Analysis
95,88,95_unlearning_privacy_adversary_obfuscation,"['unlearning', 'privacy', 'adversary', 'obfuscation', 'security', 'attacks', 'attacker', 'defenses', 'adversaries', 'trained']","['unlearning', 'membership', 'privacy', 'attacks', 'attack', 'inference', 'machine', 'adversary', 'vulnerability', 'leakage']","['  With the continued advancement and widespread adoption of machine learning\n(ML) models across various domains, ensuring user privacy and data security has\nbecome a paramount concern. In compliance with data privacy regulations, such\nas GDPR, a secure machine learning framework should not only grant users the\nright to request the removal of their contributed data used for model training\nbut also facilitates the elimination of sensitive data fingerprints within\nmachine learning models to mitigate potential attack - a process referred to as\nmachine unlearning. In this study, we present a novel unlearning mechanism\ndesigned to effectively remove the impact of specific data samples from a\nneural network while considering the performance of the unlearned model on the\nprimary task. In achieving this goal, we crafted a novel loss function tailored\nto eliminate privacy-sensitive information from weights and activation values\nof the target model by combining target classification loss and membership\ninference loss. Our adaptable framework can easily incorporate various privacy\nleakage approximation mechanisms to guide the unlearning process. We provide\nempirical evidence of the effectiveness of our unlearning approach with a\ntheoretical upper-bound analysis through a membership inference mechanism as a\nproof of concept. Our results showcase the superior performance of our approach\nin terms of unlearning efficacy and latency as well as the fidelity of the\nprimary task, across four datasets and four deep learning architectures.\n', ""  This paper focuses on the challenge of machine unlearning, aiming to remove\nthe influence of specific training data on machine learning models.\nTraditionally, the development of unlearning algorithms runs parallel with that\nof membership inference attacks (MIA), a type of privacy threat to determine\nwhether a data instance was used for training. However, the two strands are\nintimately connected: one can view machine unlearning through the lens of MIA\nsuccess with respect to removed data. Recognizing this connection, we propose a\ngame-theoretic framework that integrates MIAs into the design of unlearning\nalgorithms. Specifically, we model the unlearning problem as a Stackelberg game\nin which an unlearner strives to unlearn specific training data from a model,\nwhile an auditor employs MIAs to detect the traces of the ostensibly removed\ndata. Adopting this adversarial perspective allows the utilization of new\nattack advancements, facilitating the design of unlearning algorithms. Our\nframework stands out in two ways. First, it takes an adversarial approach and\nproactively incorporates the attacks into the design of unlearning algorithms.\nSecondly, it uses implicit differentiation to obtain the gradients that limit\nthe attacker's success, thus benefiting the process of unlearning. We present\nempirical results to demonstrate the effectiveness of the proposed approach for\nmachine unlearning.\n"", '  The high cost of model training makes it increasingly desirable to develop\ntechniques for unlearning. These techniques seek to remove the influence of a\ntraining example without having to retrain the model from scratch. Intuitively,\nonce a model has unlearned, an adversary that interacts with the model should\nno longer be able to tell whether the unlearned example was included in the\nmodel\'s training set or not. In the privacy literature, this is known as\nmembership inference. In this work, we discuss adaptations of Membership\nInference Attacks (MIAs) to the setting of unlearning (leading to their ""U-MIA""\ncounterparts). We propose a categorization of existing U-MIAs into ""population\nU-MIAs"", where the same attacker is instantiated for all examples, and\n""per-example U-MIAs"", where a dedicated attacker is instantiated for each\nexample. We show that the latter category, wherein the attacker tailors its\nmembership prediction to each example under attack, is significantly stronger.\nIndeed, our results show that the commonly used U-MIAs in the unlearning\nliterature overestimate the privacy protection afforded by existing unlearning\ntechniques on both vision and language models. Our investigation reveals a\nlarge variance in the vulnerability of different examples to per-example\nU-MIAs. In fact, several unlearning algorithms lead to a reduced vulnerability\nfor some, but not all, examples that we wish to unlearn, at the expense of\nincreasing it for other examples. Notably, we find that the privacy protection\nfor the remaining training examples may worsen as a consequence of unlearning.\nWe also discuss the fundamental difficulty of equally protecting all examples\nusing existing unlearning schemes, due to the different rates at which examples\nare unlearned. We demonstrate that naive attempts at tailoring unlearning\nstopping criteria to different examples fail to alleviate these issues.\n']",Machine Unlearning for Data Privacy,Machine Learning for Data Privacy and Security,Machine Learning and Data Privacy,Machine Learning and Data Privacy
96,85,96_forecasting_lstm_stocks_predicting,"['forecasting', 'lstm', 'stocks', 'predicting', 'stock', 'investing', 'prediction', 'stockformer', 'predict', 'stockgpt']","['stock', 'market', 'price', 'cryptocurrency', 'financial', 'prices', 'volatility', 'stocks', 'investment', 'trading']","[""  Navigating the intricate landscape of financial markets requires adept\nforecasting of stock price movements. This paper delves into the potential of\nLong Short-Term Memory (LSTM) networks for predicting stock dynamics, with a\nfocus on discerning nuanced rise and fall patterns. Leveraging a dataset from\nthe New York Stock Exchange (NYSE), the study incorporates multiple features to\nenhance LSTM's capacity in capturing complex patterns. Visualization of key\nattributes, such as opening, closing, low, and high prices, aids in unraveling\nsubtle distinctions crucial for comprehensive market understanding. The\nmeticulously crafted LSTM input structure, inspired by established guidelines,\nincorporates both price and volume attributes over a 25-day time step, enabling\nthe model to capture temporal intricacies. A comprehensive methodology,\nincluding hyperparameter tuning with Grid Search, Early Stopping, and Callback\nmechanisms, leads to a remarkable 53% improvement in predictive accuracy. The\nstudy concludes with insights into model robustness, contributions to financial\nforecasting literature, and a roadmap for real-time stock market prediction.\nThe amalgamation of LSTM networks, strategic hyperparameter tuning, and\ninformed feature selection presents a potent framework for advancing the\naccuracy of stock price predictions, contributing substantively to financial\ntime series forecasting discourse.\n"", '  Predicting a fast and accurate model for stock price forecasting is been a\nchallenging task and this is an active area of research where it is yet to be\nfound which is the best way to forecast the stock price. Machine learning, deep\nlearning and statistical analysis techniques are used here to get the accurate\nresult so the investors can see the future trend and maximize the return of\ninvestment in stock trading. This paper will review many deep learning\nalgorithms for stock price forecasting. We use a record of s&p 500 index data\nfor training and testing. The survey motive is to check various deep learning\nand statistical model techniques for stock price forecasting that are Moving\nAverages, ARIMA which are statistical techniques and LSTM, RNN, CNN, and FULL\nCNN which are deep learning models. It will discuss various models, including\nthe Auto regression integration moving average model, the Recurrent neural\nnetwork model, the long short-term model which is the type of RNN used for long\ndependency for data, the convolutional neural network model, and the full\nconvolutional neural network model, in terms of error calculation or percentage\nof accuracy that how much it is accurate which measures by the function like\nRoot mean square error, mean absolute error, mean squared error. The model can\nbe used to predict the stock price by checking the low MAE value as lower the\nMAE value the difference between the predicting and the actual value will be\nless and this model will predict the price more accurately than other models.\n', '  One of the most enticing research areas is the stock market, and projecting\nstock prices may help investors profit by making the best decisions at the\ncorrect time. Deep learning strategies have emerged as a critical technique in\nthe field of the financial market. The stock market is impacted due to two\naspects, one is the geo-political, social and global events on the bases of\nwhich the price trends could be affected. Meanwhile, the second aspect purely\nfocuses on historical price trends and seasonality, allowing us to forecast\nstock prices. In this paper, our aim is to focus on the second aspect and build\na model that predicts future prices with minimal errors. In order to provide\nbetter prediction results of stock price, we propose a new model named Long\nShort-Term Memory (LSTM) with Sequential Self-Attention Mechanism (LSTM-SSAM).\nFinally, we conduct extensive experiments on the three stock datasets: SBIN,\nHDFCBANK, and BANKBARODA. The experimental results prove the effectiveness and\nfeasibility of the proposed model compared to existing models. The experimental\nfindings demonstrate that the root-mean-squared error (RMSE), and R-square (R2)\nevaluation indicators are giving the best results.\n']",Stock Price Forecasting with LSTM Networks,Time Series Analysis and Prediction,Predictive Modeling and Forecasting,Predictive Modeling and Forecasting
97,85,97_ensembles_treeshap_ensemble_boosting,"['ensembles', 'treeshap', 'ensemble', 'boosting', 'forests', 'forest', 'trees', 'classification', 'tree', 'metatree']","['trees', 'tree', 'forests', 'decision', 'forest', 'random', 'rules', 'ensembles', 'ensemble', 'interpretable']","['  Decision trees are a popular tool in machine learning and yield\neasy-to-understand models. Several techniques have been proposed in the\nliterature for learning a decision tree classifier, with different techniques\nworking well for data from different domains. In this work, we develop\napproaches to design decision tree learning algorithms given repeated access to\ndata from the same domain. We propose novel parameterized classes of node\nsplitting criteria in top-down algorithms, which interpolate between popularly\nused entropy and Gini impurity based criteria, and provide theoretical bounds\non the number of samples needed to learn the splitting function appropriate for\nthe data at hand. We also study the sample complexity of tuning prior\nparameters in Bayesian decision tree learning, and extend our results to\ndecision tree regression. We further consider the problem of tuning\nhyperparameters in pruning the decision tree for classical pruning algorithms\nincluding min-cost complexity pruning. We also study the interpretability of\nthe learned decision trees and introduce a data-driven approach for optimizing\nthe explainability versus accuracy trade-off using decision trees. Finally, we\ndemonstrate the significance of our approach on real world datasets by learning\ndata-specific decision trees which are simultaneously more accurate and\ninterpretable.\n', '  A decision tree is one of the most popular approaches in machine learning\nfields. However, it suffers from the problem of overfitting caused by overly\ndeepened trees. Then, a meta-tree is recently proposed. It solves the problem\nof overfitting caused by overly deepened trees. Moreover, the meta-tree\nguarantees statistical optimality based on Bayes decision theory. Therefore,\nthe meta-tree is expected to perform better than the decision tree. In contrast\nto a single decision tree, it is known that ensembles of decision trees, which\nare typically constructed boosting algorithms, are more effective in improving\npredictive performance. Thus, it is expected that ensembles of meta-trees are\nmore effective in improving predictive performance than a single meta-tree, and\nthere are no previous studies that construct multiple meta-trees in boosting.\nTherefore, in this study, we propose a method to construct multiple meta-trees\nusing a boosting approach. Through experiments with synthetic and benchmark\ndatasets, we conduct a performance comparison between the proposed methods and\nthe conventional methods using ensembles of decision trees. Furthermore, while\nensembles of decision trees can cause overfitting as well as a single decision\ntree, experiments confirmed that ensembles of meta-trees can prevent\noverfitting due to the tree depth.\n', '  Decades after their inception, random forests continue to provide\nstate-of-the-art accuracy in a variety of learning problems, outperforming in\nthis respect alternative machine learning algorithms such as decision trees or\neven neural networks. However, being an ensemble method, the one aspect where\nrandom forests tend to severely underperform decision trees is\ninterpretability. In the present work, we propose a post-hoc approach that aims\nto have the best of both worlds: the accuracy of random forests and the\ninterpretability of decision trees. To this end, we present two forest-pruning\nmethods to find an optimal sub-forest within a given random forest, and then,\nwhen applicable, combine the selected trees into one. Our first method relies\non constrained exhaustive search, while our second method is based on an\nadaptation of the LASSO methodology. Extensive experiments over synthetic and\nreal world datasets show that, in the majority of scenarios, at least one of\nthe two methods proposed is more accurate than the original random forest,\nwhile just using a small fraction of the trees, aiding result interpretability.\nCompared to current state-of-the-art forest pruning methods, namely sequential\nforward selection and (a variation of) sequential backward selection, our\nmethods tend to outperform both of them, whether in terms of accuracy, number\nof trees employed, or both.\n']",Decision Trees and Ensemble Methods,Ensemble Methods and Classification Techniques,Machine Learning Ensembles and Multi-View Methods,Machine Learning Ensembles and Multi-View Methods
98,84,98_microgrid_renewable_scheduling_reinforcement,"['microgrid', 'renewable', 'scheduling', 'reinforcement', 'electricity', 'ev', 'energy', 'discharging', 'hvac', 'agent']","['energy', 'charging', 'renewable', 'grid', 'power', 'reinforcement', 'residential', 'control', 'demand', 'electric']","['  Dairy farming consumes a significant amount of energy, making it an\nenergy-intensive sector within agriculture. Integrating renewable energy\ngeneration into dairy farming could help address this challenge. Effective\nbattery management is important for integrating renewable energy generation.\nManaging battery charging and discharging poses significant challenges because\nof fluctuations in electrical consumption, the intermittent nature of renewable\nenergy generation, and fluctuations in energy prices. Artificial Intelligence\n(AI) has the potential to significantly improve the use of renewable energy in\ndairy farming, however, there is limited research conducted in this particular\ndomain. This research considers Ireland as a case study as it works towards\nattaining its 2030 energy strategy centered on the utilization of renewable\nsources. This study proposes a Q-learning-based algorithm for scheduling\nbattery charging and discharging in a dairy farm setting. This research also\nexplores the effect of the proposed algorithm by adding wind generation data\nand considering additional case studies. The proposed algorithm reduces the\ncost of imported electricity from the grid by 13.41%, peak demand by 2%, and\n24.49% when utilizing wind generation. These results underline how\nreinforcement learning is highly effective in managing batteries in the dairy\nfarming sector.\n', ""  The increasing integration of electric vehicles (EVs) into the grid can pose\na significant risk to the distribution system operation in the absence of\ncoordination. In response to the need for effective coordination of EVs within\nthe distribution network, this paper presents a safety-aware reinforcement\nlearning (RL) algorithm designed to manage EV charging stations while ensuring\nthe satisfaction of system constraints. Unlike existing methods, our proposed\nalgorithm does not rely on explicit penalties for constraint violations,\neliminating the need for penalty coefficient tuning. Furthermore, managing EV\ncharging stations is further complicated by multiple uncertainties, notably the\nvariability in solar energy generation and energy prices. To address this\nchallenge, we develop an off-policy RL algorithm to efficiently utilize data to\nlearn patterns in such uncertain environments. Our algorithm also incorporates\na maximum entropy framework to enhance the RL algorithm's exploratory process,\npreventing convergence to local optimal solutions. Simulation results\ndemonstrate that our algorithm outperforms traditional RL algorithms in\nmanaging EV charging in the distribution network.\n"", '  The widespread adoption of electric vehicles (EVs) poses several challenges\nto power distribution networks and smart grid infrastructure due to the\npossibility of significantly increasing electricity demands, especially during\npeak hours. Furthermore, when EVs participate in demand-side management\nprograms, charging expenses can be reduced by using optimal charging control\npolicies that fully utilize real-time pricing schemes. However, devising\noptimal charging methods and control strategies for EVs is challenging due to\nvarious stochastic and uncertain environmental factors. Currently, most EV\ncharging controllers operate based on a centralized model. In this paper, we\nintroduce a novel approach for distributed and cooperative charging strategy\nusing a Multi-Agent Reinforcement Learning (MARL) framework. Our method is\nbuilt upon the Deep Deterministic Policy Gradient (DDPG) algorithm for a group\nof EVs in a residential community, where all EVs are connected to a shared\ntransformer. This method, referred to as CTDE-DDPG, adopts a Centralized\nTraining Decentralized Execution (CTDE) approach to establish cooperation\nbetween agents during the training phase, while ensuring a distributed and\nprivacy-preserving operation during execution. We theoretically examine the\nperformance of centralized and decentralized critics for the DDPG-based MARL\nimplementation and demonstrate their trade-offs. Furthermore, we numerically\nexplore the efficiency, scalability, and performance of centralized and\ndecentralized critics. Our theoretical and numerical results indicate that,\ndespite higher policy gradient variances and training complexity, the CTDE-DDPG\nframework significantly improves charging efficiency by reducing total\nvariation by approximately %36 and charging cost by around %9.1 on average...\n']",Renewable Energy Management with Reinforcement Learning,Reinforcement Learning Applications and Methodologies,Reinforcement Learning,Reinforcement Learning
99,80,99_faults_fault_detect_feature,"['faults', 'fault', 'detect', 'feature', 'detection', 'features', 'monitoring', 'machinery', 'wavelet', 'machines']","['fault', 'faults', 'maintenance', 'industrial', 'diagnosis', 'vibration', 'failure', 'monitoring', 'manufacturing', 'signal']","['  Domain generalization achieves fault diagnosis on unseen modes. In process\nindustrial systems, fault samples are limited, and only single-mode fault data\ncan be obtained. Extracting domain-invariant fault features from single-mode\ndata for unseen mode fault diagnosis poses challenges. Existing methods utilize\na generator module to simulate samples of unseen modes. However, multi-mode\nsamples contain complex spatiotemporal information, which brings significant\ndifficulties to accurate sample generation. Therefore, double gradient reversal\nnetwork (DGRN) is proposed. First, the model is pre-trained to acquire fault\nknowledge from the single seen mode. Then, pseudo-fault feature generation\nstrategy is designed by Adaptive instance normalization, to simulate fault\nfeatures of unseen mode. The dual adversarial training strategy is created to\nenhance the diversity of pseudo-fault features, which models unseen modes with\nsignificant distribution differences. Subsequently, domain-invariant feature\nextraction strategy is constructed by contrastive learning and adversarial\nlearning. This strategy extracts common features of faults and helps multi-mode\nfault diagnosis. Finally, the experiments were conducted on Tennessee Eastman\nprocess and continuous stirred-tank reactor. The experiments demonstrate that\nDGRN achieves high classification accuracy on unseen modes while maintaining a\nsmall model size.\n', '  Bearings are one of the vital components of rotating machines that are prone\nto unexpected faults. Therefore, bearing fault diagnosis and condition\nmonitoring is essential for reducing operational costs and downtime in numerous\nindustries. In various production conditions, bearings can be operated under a\nrange of loads and speeds, which causes different vibration patterns associated\nwith each fault type. Normal data is ample as systems usually work in desired\nconditions. On the other hand, fault data is rare, and in many conditions,\nthere is no data recorded for the fault classes. Accessing fault data is\ncrucial for developing data-driven fault diagnosis tools that can improve both\nthe performance and safety of operations. To this end, a novel algorithm based\non Conditional Generative Adversarial Networks (CGANs) is introduced. Trained\non the normal and fault data on any actual fault conditions, this algorithm\ngenerates fault data from normal data of target conditions. The proposed method\nis validated on a real-world bearing dataset, and fault data are generated for\ndifferent conditions. Several state-of-the-art classifiers and visualization\nmodels are implemented to evaluate the quality of the synthesized data. The\nresults demonstrate the efficacy of the proposed algorithm.\n', '  One important characteristic of modern fault classification systems is the\nability to flag the system when faced with previously unseen fault types. This\nwork considers the unknown fault detection capabilities of deep neural\nnetwork-based fault classifiers. Specifically, we propose a methodology on how,\nwhen available, labels regarding the fault taxonomy can be used to increase\nunknown fault detection performance without sacrificing model performance. To\nachieve this, we propose to utilize soft label techniques to improve the\nstate-of-the-art deep novel fault detection techniques during the training\nprocess and novel hierarchically consistent detection statistics for online\nnovel fault detection. Finally, we demonstrated increased detection performance\non novel fault detection in inspection images from the hot steel rolling\nprocess, with results well replicated across multiple scenarios and baseline\ndetection methods.\n']",Fault Detection and Diagnosis in Industrial Systems,Predictive Maintenance and Fault Detection in Industrial Systems,Industrial Automation and Control Systems,Industrial Automation and Control Systems
100,79,100_soccernet_esports_sports_soccer,"['soccernet', 'esports', 'sports', 'soccer', 'sport', 'athletes', 'soccerrag', 'players', 'athlete', 'matches']","['sports', 'soccer', 'players', 'player', 'game', 'tennis', 'badminton', 'esports', 'football', 'match']","['  In this paper, we present a novel sequential team selection model in soccer.\nSpecifically, we model the stochastic process of player injury and\nunavailability using player-specific information learned from real-world soccer\ndata. Monte-Carlo Tree Search is used to select teams for games that optimise\nlong-term team performance across a soccer season by reasoning over player\ninjury probability. We validate our approach compared to benchmark solutions\nfor the 2018/19 English Premier League season. Our model achieves similar\nseason expected points to the benchmark whilst reducing first-team injuries by\n~13% and the money inefficiently spent on injured players by ~11% -\ndemonstrating the potential to reduce costs and improve player welfare in\nreal-world soccer teams.\n', '  This paper represents an analysis on the momentum of tennis match. And due to\nGeneralization performance of it, it can be helpful in constructing a system to\npredict the result of sports game and analyze the performance of player based\non the Technical statistics. We First use hidden markov models to predict the\nmomentum which is defined as the performance of players. Then we use Xgboost to\nprove the significance of momentum. Finally we use LightGBM to evaluate the\nperformance of our model and use SHAP feature importance ranking and weight\nanalysis to find the key points that affect the performance of players.\n', '  Understanding sports is crucial for the advancement of Natural Language\nProcessing (NLP) due to its intricate and dynamic nature. Reasoning over\ncomplex sports scenarios has posed significant challenges to current NLP\ntechnologies which require advanced cognitive capabilities. Toward addressing\nthe limitations of existing benchmarks on sports understanding in the NLP\nfield, we extensively evaluated mainstream large language models for various\nsports tasks. Our evaluation spans from simple queries on basic rules and\nhistorical facts to complex, context-specific reasoning, leveraging strategies\nfrom zero-shot to few-shot learning, and chain-of-thought techniques. In\naddition to unimodal analysis, we further assessed the sports reasoning\ncapabilities of mainstream video language models to bridge the gap in\nmultimodal sports understanding benchmarking. Our findings highlighted the\ncritical challenges of sports understanding for NLP. We proposed a new\nbenchmark based on a comprehensive overview of existing sports datasets and\nprovided extensive error analysis which we hope can help identify future\nresearch priorities in this field.\n']",Sports Analytics and Performance Optimization,Artificial Intelligence in Sports and Games,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
100,79,100_soccernet_esports_sports_soccer,"['soccernet', 'esports', 'sports', 'soccer', 'sport', 'athletes', 'soccerrag', 'players', 'athlete', 'matches']","['sports', 'soccer', 'players', 'player', 'game', 'tennis', 'badminton', 'esports', 'football', 'match']","['  In this paper, we present a novel sequential team selection model in soccer.\nSpecifically, we model the stochastic process of player injury and\nunavailability using player-specific information learned from real-world soccer\ndata. Monte-Carlo Tree Search is used to select teams for games that optimise\nlong-term team performance across a soccer season by reasoning over player\ninjury probability. We validate our approach compared to benchmark solutions\nfor the 2018/19 English Premier League season. Our model achieves similar\nseason expected points to the benchmark whilst reducing first-team injuries by\n~13% and the money inefficiently spent on injured players by ~11% -\ndemonstrating the potential to reduce costs and improve player welfare in\nreal-world soccer teams.\n', '  This paper represents an analysis on the momentum of tennis match. And due to\nGeneralization performance of it, it can be helpful in constructing a system to\npredict the result of sports game and analyze the performance of player based\non the Technical statistics. We First use hidden markov models to predict the\nmomentum which is defined as the performance of players. Then we use Xgboost to\nprove the significance of momentum. Finally we use LightGBM to evaluate the\nperformance of our model and use SHAP feature importance ranking and weight\nanalysis to find the key points that affect the performance of players.\n', '  Understanding sports is crucial for the advancement of Natural Language\nProcessing (NLP) due to its intricate and dynamic nature. Reasoning over\ncomplex sports scenarios has posed significant challenges to current NLP\ntechnologies which require advanced cognitive capabilities. Toward addressing\nthe limitations of existing benchmarks on sports understanding in the NLP\nfield, we extensively evaluated mainstream large language models for various\nsports tasks. Our evaluation spans from simple queries on basic rules and\nhistorical facts to complex, context-specific reasoning, leveraging strategies\nfrom zero-shot to few-shot learning, and chain-of-thought techniques. In\naddition to unimodal analysis, we further assessed the sports reasoning\ncapabilities of mainstream video language models to bridge the gap in\nmultimodal sports understanding benchmarking. Our findings highlighted the\ncritical challenges of sports understanding for NLP. We proposed a new\nbenchmark based on a comprehensive overview of existing sports datasets and\nprovided extensive error analysis which we hope can help identify future\nresearch priorities in this field.\n']",Sports Analytics and Performance Optimization,Artificial Intelligence in Sports and Games,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
101,78,101_topological_topologically_topology_homeomorphic,"['topological', 'topologically', 'topology', 'homeomorphic', 'networks', 'manifolds', 'homology', 'classification', 'graphcodes', 'neural']","['topological', 'homology', 'persistent', 'persistence', 'topology', 'simplicial', 'geometric', 'complexes', 'invariant', 'graph']","['  Persistent homology, a technique from computational topology, has recently\nshown strong empirical performance in the context of graph classification.\nBeing able to capture long range graph properties via higher-order topological\nfeatures, such as cycles of arbitrary length, in combination with multi-scale\ntopological descriptors, has improved predictive performance for data sets with\nprominent topological structures, such as molecules. At the same time, the\ntheoretical properties of persistent homology have not been formally assessed\nin this context. This paper intends to bridge the gap between computational\ntopology and graph machine learning by providing a brief introduction to\npersistent homology in the context of graphs, as well as a theoretical\ndiscussion and empirical analysis of its expressivity for graph learning tasks.\n', '  Topological Data Analysis (TDA) allows us to extract powerful topological and\nhigher-order information on the global shape of a data set or point cloud.\nTools like Persistent Homology or the Euler Transform give a single complex\ndescription of the global structure of the point cloud. However, common machine\nlearning applications like classification require point-level information and\nfeatures to be available. In this paper, we bridge this gap and propose a novel\nmethod to extract node-level topological features from complex point clouds\nusing discrete variants of concepts from algebraic topology and differential\ngeometry. We verify the effectiveness of these topological point features\n(TOPF) on both synthetic and real-world data and study their robustness under\nnoise.\n', ""  We specialize techniques from topological data analysis to the problem of\ncharacterizing the topological complexity (as defined in the body of the paper)\nof a multi-class data set. As a by-product, a topological classifier is defined\nthat uses an open sub-covering of the data set. This sub-covering can be used\nto construct a simplicial complex whose topological features (e.g., Betti\nnumbers) provide information about the classification problem. We use these\ntopological constructs to study the impact of topological complexity on\nlearning in feedforward deep neural networks (DNNs). We hypothesize that\ntopological complexity is negatively correlated with the ability of a fully\nconnected feedforward deep neural network to learn to classify data correctly.\nWe evaluate our topological classification algorithm on multiple constructed\nand open source data sets. We also validate our hypothesis regarding the\nrelationship between topological complexity and learning in DNN's on multiple\ndata sets.\n""]",Topological Data Analysis for Graph Classification,Graph Analysis and Processing Techniques,Data Analysis and Pattern Discovery,Data Analysis and Pattern Discovery
102,78,102_gans_gan_generative_adversarial,"['gans', 'gan', 'generative', 'adversarial', 'flowgan', 'codegan', 'cgan', 'cyclegan', 'stylegan', 'inception']","['generative', 'generator', 'adversarial', 'distance', 'distribution', 'distributions', 'collapse', 'inception', 'samples', 'training']","['  The empirical success of Generative Adversarial Networks (GANs) caused an\nincreasing interest in theoretical research. The statistical literature is\nmainly focused on Wasserstein GANs and generalizations thereof, which\nespecially allow for good dimension reduction properties. Statistical results\nfor Vanilla GANs, the original optimization problem, are still rather limited\nand require assumptions such as smooth activation functions and equal\ndimensions of the latent space and the ambient space. To bridge this gap, we\ndraw a connection from Vanilla GANs to the Wasserstein distance. By doing so,\nexisting results for Wasserstein GANs can be extended to Vanilla GANs. In\nparticular, we obtain an oracle inequality for Vanilla GANs in Wasserstein\ndistance. The assumptions of this oracle inequality are designed to be\nsatisfied by network architectures commonly used in practice, such as\nfeedforward ReLU networks. By providing a quantitative result for the\napproximation of a Lipschitz function by a feedforward ReLU network with\nbounded H\\""older norm, we conclude a rate of convergence for Vanilla GANs as\nwell as Wasserstein GANs as estimators of the unknown probability distribution.\n', ""  Modern GANs achieve remarkable performance in terms of generating realistic\nand diverse samples. This has led many to believe that ``GANs capture the\ntraining data manifold''. In this work we show that this interpretation is\nwrong. We empirically show that the manifold learned by modern GANs does not\nfit the training distribution: specifically the manifold does not pass through\nthe training examples and passes closer to out-of-distribution images than to\nin-distribution images. We also investigate the distribution over images\nimplied by the prior over the latent codes and study whether modern GANs learn\na density that approximates the training distribution. Surprisingly, we find\nthat the learned density is very far from the data distribution and that GANs\ntend to assign higher density to out-of-distribution images. Finally, we\ndemonstrate that the set of images used to train modern GANs are often not part\nof the typical set described by the GANs' distribution.\n"", '  We investigate the impact of the input dimension on the generalization error\nin generative adversarial networks (GANs). In particular, we first provide both\ntheoretical and practical evidence to validate the existence of an optimal\ninput dimension (OID) that minimizes the generalization error. Then, to\nidentify the OID, we introduce a novel framework called generalized GANs\n(G-GANs), which includes existing GANs as a special case. By incorporating the\ngroup penalty and the architecture penalty developed in the paper, G-GANs have\nseveral intriguing features. First, our framework offers adaptive\ndimensionality reduction from the initial dimension to a dimension necessary\nfor generating the target distribution. Second, this reduction in\ndimensionality also shrinks the required size of the generator network\narchitecture, which is automatically identified by the proposed architecture\npenalty. Both reductions in dimensionality and the generator network\nsignificantly improve the stability and the accuracy of the estimation and\nprediction. Theoretical support for the consistent selection of the input\ndimension and the generator network is provided. Third, the proposed algorithm\ninvolves an end-to-end training process, and the algorithm allows for dynamic\nadjustments between the input dimension and the generator network during\ntraining, further enhancing the overall performance of G-GANs. Extensive\nexperiments conducted with simulated and benchmark data demonstrate the\nsuperior performance of G-GANs. In particular, compared to that of\noff-the-shelf methods, G-GANs achieves an average improvement of 45.68% in the\nCT slice dataset, 43.22% in the MNIST dataset and 46.94% in the FashionMNIST\ndataset in terms of the maximum mean discrepancy or Frechet inception distance.\nMoreover, the features generated based on the input dimensions identified by\nG-GANs align with visually significant features.\n']",Generative Adversarial Networks (GANs) Theory and Applications,Generative Adversarial Networks (GANs) and Their Applications,Generative Modeling and Artificial Intelligence,Generative Modeling and Artificial Intelligence
103,78,103_instruction_language_instructmining_texttuning,"['instruction', 'language', 'instructmining', 'texttuning', 'tuning', 'training', 'tuned', 'curriculum', 'benchmarks', 'tasks']","['instruction', 'instructions', 'tuning', 'quality', 'responses', 'selection', 'diversity', 'format', 'finetuning', 'data']","['  Instruction tuning -- fine-tuning a large language model (LLM) on pairs of\ninstructions and desired outcomes -- is an approach that enables pre-trained\nlanguage models to perform real-world tasks and follow human instructions. Its\npractical success depends on the model learning a broader set of instructions\nthan those it was trained on. Yet the factors that determine model\ngeneralization to such \\emph{unseen tasks} are not well understood. %To\nunderstand the driving factors of generalization, In this paper, we experiment\nwith string rewrites, a symbolic task that serves as a building block for\nTuring complete Markov algorithms while allowing experimental control of\n""inputs"" and ""instructions"". We investigate the trade-off between the number of\ninstructions the model is trained on and the number of training samples\nprovided for each instruction and observe that the diversity of the instruction\nset determines generalization. Generalization emerges once a diverse enough set\nof tasks is provided, even though very few examples are provided for each task.\nInstruction diversity also ensures robustness with respect to non-uniform\ndistributions of instructions in the training set.\n', '  Instruction tuning is a vital step of training large language models (LLM),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLM. Therefore, recently\na lot of studies focus on exploring the methods of selecting high-quality\nsubset from instruction datasets, aiming to reduce training costs and enhance\nthe instruction-following capabilities of LLMs. This paper presents a\ncomprehensive survey on data selection for LLM instruction tuning. Firstly, we\nintroduce the wildly used instruction datasets. Then, we propose a new taxonomy\nof the data selection methods and provide a detailed introduction of recent\nadvances,and the evaluation strategies and results of data selection methods\nare also elaborated in detail. Finally, we emphasize the open challenges and\npresent new frontiers of this task.\n', ""  Instruction tuning -- tuning large language models on instruction-output\npairs -- is a promising technique for making models better adapted to the real\nworld. Yet, the key factors driving the model's capability to understand and\nfollow instructions not seen during training remain under-explored. Our\ninvestigation begins with a series of synthetic experiments within the\ntheoretical framework of a Turing-complete algorithm called Markov algorithm,\nwhich allows fine-grained control over the instruction-tuning data.\nGeneralization and robustness with respect to the training distribution emerge\nonce a diverse enough set of tasks is provided, even though very few examples\nare provided for each task. We extend these initial results to a real-world\napplication scenario of code generation and find that a more diverse\ninstruction set, extending beyond code-related tasks, improves the performance\nof code generation. Our observations suggest that a more diverse semantic space\nfor instruction-tuning sets greatly improves the model's ability to follow\ninstructions and perform tasks.\n""]",Instruction Tuning for Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
103,78,103_instruction_language_instructmining_texttuning,"['instruction', 'language', 'instructmining', 'texttuning', 'tuning', 'training', 'tuned', 'curriculum', 'benchmarks', 'tasks']","['instruction', 'instructions', 'tuning', 'quality', 'responses', 'selection', 'diversity', 'format', 'finetuning', 'data']","['  Instruction tuning -- fine-tuning a large language model (LLM) on pairs of\ninstructions and desired outcomes -- is an approach that enables pre-trained\nlanguage models to perform real-world tasks and follow human instructions. Its\npractical success depends on the model learning a broader set of instructions\nthan those it was trained on. Yet the factors that determine model\ngeneralization to such \\emph{unseen tasks} are not well understood. %To\nunderstand the driving factors of generalization, In this paper, we experiment\nwith string rewrites, a symbolic task that serves as a building block for\nTuring complete Markov algorithms while allowing experimental control of\n""inputs"" and ""instructions"". We investigate the trade-off between the number of\ninstructions the model is trained on and the number of training samples\nprovided for each instruction and observe that the diversity of the instruction\nset determines generalization. Generalization emerges once a diverse enough set\nof tasks is provided, even though very few examples are provided for each task.\nInstruction diversity also ensures robustness with respect to non-uniform\ndistributions of instructions in the training set.\n', '  Instruction tuning is a vital step of training large language models (LLM),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLM. Therefore, recently\na lot of studies focus on exploring the methods of selecting high-quality\nsubset from instruction datasets, aiming to reduce training costs and enhance\nthe instruction-following capabilities of LLMs. This paper presents a\ncomprehensive survey on data selection for LLM instruction tuning. Firstly, we\nintroduce the wildly used instruction datasets. Then, we propose a new taxonomy\nof the data selection methods and provide a detailed introduction of recent\nadvances,and the evaluation strategies and results of data selection methods\nare also elaborated in detail. Finally, we emphasize the open challenges and\npresent new frontiers of this task.\n', ""  Instruction tuning -- tuning large language models on instruction-output\npairs -- is a promising technique for making models better adapted to the real\nworld. Yet, the key factors driving the model's capability to understand and\nfollow instructions not seen during training remain under-explored. Our\ninvestigation begins with a series of synthetic experiments within the\ntheoretical framework of a Turing-complete algorithm called Markov algorithm,\nwhich allows fine-grained control over the instruction-tuning data.\nGeneralization and robustness with respect to the training distribution emerge\nonce a diverse enough set of tasks is provided, even though very few examples\nare provided for each task. We extend these initial results to a real-world\napplication scenario of code generation and find that a more diverse\ninstruction set, extending beyond code-related tasks, improves the performance\nof code generation. Our observations suggest that a more diverse semantic space\nfor instruction-tuning sets greatly improves the model's ability to follow\ninstructions and perform tasks.\n""]",Instruction Tuning for Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
103,78,103_instruction_language_instructmining_texttuning,"['instruction', 'language', 'instructmining', 'texttuning', 'tuning', 'training', 'tuned', 'curriculum', 'benchmarks', 'tasks']","['instruction', 'instructions', 'tuning', 'quality', 'responses', 'selection', 'diversity', 'format', 'finetuning', 'data']","['  Instruction tuning -- fine-tuning a large language model (LLM) on pairs of\ninstructions and desired outcomes -- is an approach that enables pre-trained\nlanguage models to perform real-world tasks and follow human instructions. Its\npractical success depends on the model learning a broader set of instructions\nthan those it was trained on. Yet the factors that determine model\ngeneralization to such \\emph{unseen tasks} are not well understood. %To\nunderstand the driving factors of generalization, In this paper, we experiment\nwith string rewrites, a symbolic task that serves as a building block for\nTuring complete Markov algorithms while allowing experimental control of\n""inputs"" and ""instructions"". We investigate the trade-off between the number of\ninstructions the model is trained on and the number of training samples\nprovided for each instruction and observe that the diversity of the instruction\nset determines generalization. Generalization emerges once a diverse enough set\nof tasks is provided, even though very few examples are provided for each task.\nInstruction diversity also ensures robustness with respect to non-uniform\ndistributions of instructions in the training set.\n', '  Instruction tuning is a vital step of training large language models (LLM),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLM. Therefore, recently\na lot of studies focus on exploring the methods of selecting high-quality\nsubset from instruction datasets, aiming to reduce training costs and enhance\nthe instruction-following capabilities of LLMs. This paper presents a\ncomprehensive survey on data selection for LLM instruction tuning. Firstly, we\nintroduce the wildly used instruction datasets. Then, we propose a new taxonomy\nof the data selection methods and provide a detailed introduction of recent\nadvances,and the evaluation strategies and results of data selection methods\nare also elaborated in detail. Finally, we emphasize the open challenges and\npresent new frontiers of this task.\n', ""  Instruction tuning -- tuning large language models on instruction-output\npairs -- is a promising technique for making models better adapted to the real\nworld. Yet, the key factors driving the model's capability to understand and\nfollow instructions not seen during training remain under-explored. Our\ninvestigation begins with a series of synthetic experiments within the\ntheoretical framework of a Turing-complete algorithm called Markov algorithm,\nwhich allows fine-grained control over the instruction-tuning data.\nGeneralization and robustness with respect to the training distribution emerge\nonce a diverse enough set of tasks is provided, even though very few examples\nare provided for each task. We extend these initial results to a real-world\napplication scenario of code generation and find that a more diverse\ninstruction set, extending beyond code-related tasks, improves the performance\nof code generation. Our observations suggest that a more diverse semantic space\nfor instruction-tuning sets greatly improves the model's ability to follow\ninstructions and perform tasks.\n""]",Instruction Tuning for Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
103,78,103_instruction_language_instructmining_texttuning,"['instruction', 'language', 'instructmining', 'texttuning', 'tuning', 'training', 'tuned', 'curriculum', 'benchmarks', 'tasks']","['instruction', 'instructions', 'tuning', 'quality', 'responses', 'selection', 'diversity', 'format', 'finetuning', 'data']","['  Instruction tuning -- fine-tuning a large language model (LLM) on pairs of\ninstructions and desired outcomes -- is an approach that enables pre-trained\nlanguage models to perform real-world tasks and follow human instructions. Its\npractical success depends on the model learning a broader set of instructions\nthan those it was trained on. Yet the factors that determine model\ngeneralization to such \\emph{unseen tasks} are not well understood. %To\nunderstand the driving factors of generalization, In this paper, we experiment\nwith string rewrites, a symbolic task that serves as a building block for\nTuring complete Markov algorithms while allowing experimental control of\n""inputs"" and ""instructions"". We investigate the trade-off between the number of\ninstructions the model is trained on and the number of training samples\nprovided for each instruction and observe that the diversity of the instruction\nset determines generalization. Generalization emerges once a diverse enough set\nof tasks is provided, even though very few examples are provided for each task.\nInstruction diversity also ensures robustness with respect to non-uniform\ndistributions of instructions in the training set.\n', '  Instruction tuning is a vital step of training large language models (LLM),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLM. Therefore, recently\na lot of studies focus on exploring the methods of selecting high-quality\nsubset from instruction datasets, aiming to reduce training costs and enhance\nthe instruction-following capabilities of LLMs. This paper presents a\ncomprehensive survey on data selection for LLM instruction tuning. Firstly, we\nintroduce the wildly used instruction datasets. Then, we propose a new taxonomy\nof the data selection methods and provide a detailed introduction of recent\nadvances,and the evaluation strategies and results of data selection methods\nare also elaborated in detail. Finally, we emphasize the open challenges and\npresent new frontiers of this task.\n', ""  Instruction tuning -- tuning large language models on instruction-output\npairs -- is a promising technique for making models better adapted to the real\nworld. Yet, the key factors driving the model's capability to understand and\nfollow instructions not seen during training remain under-explored. Our\ninvestigation begins with a series of synthetic experiments within the\ntheoretical framework of a Turing-complete algorithm called Markov algorithm,\nwhich allows fine-grained control over the instruction-tuning data.\nGeneralization and robustness with respect to the training distribution emerge\nonce a diverse enough set of tasks is provided, even though very few examples\nare provided for each task. We extend these initial results to a real-world\napplication scenario of code generation and find that a more diverse\ninstruction set, extending beyond code-related tasks, improves the performance\nof code generation. Our observations suggest that a more diverse semantic space\nfor instruction-tuning sets greatly improves the model's ability to follow\ninstructions and perform tasks.\n""]",Instruction Tuning for Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
103,78,103_instruction_language_instructmining_texttuning,"['instruction', 'language', 'instructmining', 'texttuning', 'tuning', 'training', 'tuned', 'curriculum', 'benchmarks', 'tasks']","['instruction', 'instructions', 'tuning', 'quality', 'responses', 'selection', 'diversity', 'format', 'finetuning', 'data']","['  Instruction tuning -- fine-tuning a large language model (LLM) on pairs of\ninstructions and desired outcomes -- is an approach that enables pre-trained\nlanguage models to perform real-world tasks and follow human instructions. Its\npractical success depends on the model learning a broader set of instructions\nthan those it was trained on. Yet the factors that determine model\ngeneralization to such \\emph{unseen tasks} are not well understood. %To\nunderstand the driving factors of generalization, In this paper, we experiment\nwith string rewrites, a symbolic task that serves as a building block for\nTuring complete Markov algorithms while allowing experimental control of\n""inputs"" and ""instructions"". We investigate the trade-off between the number of\ninstructions the model is trained on and the number of training samples\nprovided for each instruction and observe that the diversity of the instruction\nset determines generalization. Generalization emerges once a diverse enough set\nof tasks is provided, even though very few examples are provided for each task.\nInstruction diversity also ensures robustness with respect to non-uniform\ndistributions of instructions in the training set.\n', '  Instruction tuning is a vital step of training large language models (LLM),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLM. Therefore, recently\na lot of studies focus on exploring the methods of selecting high-quality\nsubset from instruction datasets, aiming to reduce training costs and enhance\nthe instruction-following capabilities of LLMs. This paper presents a\ncomprehensive survey on data selection for LLM instruction tuning. Firstly, we\nintroduce the wildly used instruction datasets. Then, we propose a new taxonomy\nof the data selection methods and provide a detailed introduction of recent\nadvances,and the evaluation strategies and results of data selection methods\nare also elaborated in detail. Finally, we emphasize the open challenges and\npresent new frontiers of this task.\n', ""  Instruction tuning -- tuning large language models on instruction-output\npairs -- is a promising technique for making models better adapted to the real\nworld. Yet, the key factors driving the model's capability to understand and\nfollow instructions not seen during training remain under-explored. Our\ninvestigation begins with a series of synthetic experiments within the\ntheoretical framework of a Turing-complete algorithm called Markov algorithm,\nwhich allows fine-grained control over the instruction-tuning data.\nGeneralization and robustness with respect to the training distribution emerge\nonce a diverse enough set of tasks is provided, even though very few examples\nare provided for each task. We extend these initial results to a real-world\napplication scenario of code generation and find that a more diverse\ninstruction set, extending beyond code-related tasks, improves the performance\nof code generation. Our observations suggest that a more diverse semantic space\nfor instruction-tuning sets greatly improves the model's ability to follow\ninstructions and perform tasks.\n""]",Instruction Tuning for Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
103,78,103_instruction_language_instructmining_texttuning,"['instruction', 'language', 'instructmining', 'texttuning', 'tuning', 'training', 'tuned', 'curriculum', 'benchmarks', 'tasks']","['instruction', 'instructions', 'tuning', 'quality', 'responses', 'selection', 'diversity', 'format', 'finetuning', 'data']","['  Instruction tuning -- fine-tuning a large language model (LLM) on pairs of\ninstructions and desired outcomes -- is an approach that enables pre-trained\nlanguage models to perform real-world tasks and follow human instructions. Its\npractical success depends on the model learning a broader set of instructions\nthan those it was trained on. Yet the factors that determine model\ngeneralization to such \\emph{unseen tasks} are not well understood. %To\nunderstand the driving factors of generalization, In this paper, we experiment\nwith string rewrites, a symbolic task that serves as a building block for\nTuring complete Markov algorithms while allowing experimental control of\n""inputs"" and ""instructions"". We investigate the trade-off between the number of\ninstructions the model is trained on and the number of training samples\nprovided for each instruction and observe that the diversity of the instruction\nset determines generalization. Generalization emerges once a diverse enough set\nof tasks is provided, even though very few examples are provided for each task.\nInstruction diversity also ensures robustness with respect to non-uniform\ndistributions of instructions in the training set.\n', '  Instruction tuning is a vital step of training large language models (LLM),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLM. Therefore, recently\na lot of studies focus on exploring the methods of selecting high-quality\nsubset from instruction datasets, aiming to reduce training costs and enhance\nthe instruction-following capabilities of LLMs. This paper presents a\ncomprehensive survey on data selection for LLM instruction tuning. Firstly, we\nintroduce the wildly used instruction datasets. Then, we propose a new taxonomy\nof the data selection methods and provide a detailed introduction of recent\nadvances,and the evaluation strategies and results of data selection methods\nare also elaborated in detail. Finally, we emphasize the open challenges and\npresent new frontiers of this task.\n', ""  Instruction tuning -- tuning large language models on instruction-output\npairs -- is a promising technique for making models better adapted to the real\nworld. Yet, the key factors driving the model's capability to understand and\nfollow instructions not seen during training remain under-explored. Our\ninvestigation begins with a series of synthetic experiments within the\ntheoretical framework of a Turing-complete algorithm called Markov algorithm,\nwhich allows fine-grained control over the instruction-tuning data.\nGeneralization and robustness with respect to the training distribution emerge\nonce a diverse enough set of tasks is provided, even though very few examples\nare provided for each task. We extend these initial results to a real-world\napplication scenario of code generation and find that a more diverse\ninstruction set, extending beyond code-related tasks, improves the performance\nof code generation. Our observations suggest that a more diverse semantic space\nfor instruction-tuning sets greatly improves the model's ability to follow\ninstructions and perform tasks.\n""]",Instruction Tuning for Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
104,77,104_graphllm_graphprompter_graphs_embeddings,"['graphllm', 'graphprompter', 'graphs', 'embeddings', 'textual', 'nodes', 'graph', 'networks', 'supervised', 'node']","['graph', 'graphs', 'node', 'textual', 'text', 'nodes', 'shot', 'link', 'structures', 'structure']","['  We present Simplified Text-Attributed Graph Embeddings (STAGE), a\nstraightforward yet effective method for enhancing node features in Graph\nNeural Network (GNN) models that encode Text-Attributed Graphs (TAGs). Our\napproach leverages Large-Language Models (LLMs) to generate embeddings for\ntextual attributes. STAGE achieves competitive results on various node\nclassification benchmarks while also maintaining a simplicity in implementation\nrelative to current state-of-the-art (SoTA) techniques. We show that utilizing\npre-trained LLMs as embedding generators provides robust features for ensemble\nGNN training, enabling pipelines that are simpler than current SoTA approaches\nwhich require multiple expensive training and prompting stages. We also\nimplement diffusion-pattern GNNs in an effort to make this pipeline scalable to\ngraphs beyond academic benchmarks.\n', '  The text-attributed graph (TAG) is one kind of important real-world\ngraph-structured data with each node associated with raw texts. For TAGs,\ntraditional few-shot node classification methods directly conduct training on\nthe pre-processed node features and do not consider the raw texts. The\nperformance is highly dependent on the choice of the feature pre-processing\nmethod. In this paper, we propose P2TAG, a framework designed for few-shot node\nclassification on TAGs with graph pre-training and prompting. P2TAG first\npre-trains the language model (LM) and graph neural network (GNN) on TAGs with\nself-supervised loss. To fully utilize the ability of language models, we adapt\nthe masked language modeling objective for our framework. The pre-trained model\nis then used for the few-shot node classification with a mixed prompt method,\nwhich simultaneously considers both text and graph information. We conduct\nexperiments on six real-world TAGs, including paper citation networks and\nproduct co-purchasing networks. Experimental results demonstrate that our\nproposed framework outperforms existing graph few-shot learning methods on\nthese datasets with +18.98% ~ +35.98% improvements.\n', ""  Foundation models like ChatGPT and GPT-4 have revolutionized artificial\nintelligence, exhibiting remarkable abilities to generalize across a wide array\nof tasks and applications beyond their initial training objectives. However,\nwhen this concept is applied to graph learning, a stark contrast emerges. Graph\nlearning has predominantly focused on single-graph models, tailored to specific\ntasks or datasets, lacking the ability to transfer learned knowledge to\ndifferent domains. This limitation stems from the inherent complexity and\ndiversity of graph structures, along with the different feature and label\nspaces specific to graph data. In this paper, we present our UniGraph\nframework, designed to train a graph foundation model capable of generalizing\nto unseen graphs and tasks across diverse domains. Unlike single-graph models\nthat use pre-computed node features of varying dimensions as input, our\napproach leverages Text-Attributed Graphs (TAGs) for unifying node\nrepresentations. We propose a cascaded architecture of Language Models (LMs)\nand Graph Neural Networks (GNNs) as backbone networks with a self-supervised\ntraining objective based on Masked Graph Modeling (MGM). We introduce graph\ninstruction tuning using Large Language Models (LLMs) to enable zero-shot\nprediction ability. Our comprehensive experiments across various graph learning\ntasks and domains demonstrate the model's effectiveness in self-supervised\nrepresentation learning on unseen graphs, few-shot in-context transfer, and\nzero-shot transfer, even surpassing or matching the performance of GNNs that\nhave undergone supervised training on target datasets.\n""]",Graph Embeddings with Text Attributes,Graph and Text Embeddings for Representation Learning,Graph Representation Learning and Neural Networks,Graph Representation Learning and Neural Networks
105,77,105_confidence_confident_reliable_language,"['confidence', 'confident', 'reliable', 'language', 'verbalized', 'reliability', 'models', 'uncertainties', 'prediction', 'semantic']","['confidence', 'uncertainty', 'calibration', 'conformal', 'quantification', 'answers', 'responses', 'answer', 'answering', 'response']","['  Empowering large language models to accurately express confidence in their\nanswers is essential for trustworthy decision-making. Previous confidence\nelicitation methods, which primarily rely on white-box access to internal model\ninformation or model fine-tuning, have become less suitable for LLMs,\nespecially closed-source commercial APIs. This leads to a growing need to\nexplore the untapped area of black-box approaches for LLM uncertainty\nestimation. To better break down the problem, we define a systematic framework\nwith three components: prompting strategies for eliciting verbalized\nconfidence, sampling methods for generating multiple responses, and aggregation\ntechniques for computing consistency. We then benchmark these methods on two\nkey tasks-confidence calibration and failure prediction-across five types of\ndatasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs\nincluding GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights:\n1) LLMs, when verbalizing their confidence, tend to be overconfident,\npotentially imitating human patterns of expressing confidence. 2) As model\ncapability scales up, both calibration and failure prediction performance\nimprove. 3) Employing our proposed strategies, such as human-inspired prompts,\nconsistency among multiple responses, and better aggregation strategies can\nhelp mitigate this overconfidence from various perspectives. 4) Comparisons\nwith white-box methods indicate that while white-box methods perform better,\nthe gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements,\nnone of these techniques consistently outperform others, and all investigated\nmethods struggle in challenging tasks, such as those requiring professional\nknowledge, indicating significant scope for improvement. We believe this study\ncan serve as a strong baseline and provide insights for eliciting confidence in\nblack-box LLMs.\n', '  Although large language models (LLMs) are capable of performing various\ntasks, they still suffer from producing plausible but incorrect responses. To\nimprove the reliability of LLMs, recent research has focused on uncertainty\nquantification to predict whether a response is correct or not. However, most\nuncertainty quantification methods have been evaluated on questions requiring a\nsingle clear answer, ignoring the existence of data uncertainty that arises\nfrom irreducible randomness. Instead, these methods only consider model\nuncertainty, which arises from a lack of knowledge. In this paper, we\ninvestigate previous uncertainty quantification methods under the presence of\ndata uncertainty. Our contributions are two-fold: 1) proposing a new\nMulti-Answer Question Answering dataset, MAQA, consisting of world knowledge,\nmathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty\nquantification regarding data uncertainty, and 2) assessing 5 uncertainty\nquantification methods of diverse white- and black-box LLMs. Our findings show\nthat entropy and consistency-based methods estimate the model uncertainty well\neven under data uncertainty, while other methods for white- and black-box LLMs\nstruggle depending on the tasks. Additionally, methods designed for white-box\nLLMs suffer from overconfidence in reasoning tasks compared to simple knowledge\nqueries. We believe our observations will pave the way for future work on\nuncertainty quantification in realistic setting.\n', ""  To enhance Large Language Models' (LLMs) reliability, calibration is\nessential -- the model's assessed confidence scores should align with the\nactual likelihood of its responses being correct. However, current confidence\nelicitation methods and calibration metrics typically rely on a binary\ntrue/false assessment of response correctness. This approach does not apply to\nlong-form generation, where an answer can be partially correct. Addressing this\ngap, we introduce a unified calibration framework, in which both the\ncorrectness of the LLMs' responses and their associated confidence levels are\ntreated as distributions across a range of scores. Within this framework, we\ndevelop three metrics to precisely evaluate LLM calibration and further propose\ntwo confidence elicitation methods based on self-consistency and\nself-evaluation. Our experiments, which include long-form QA and summarization\ntasks, demonstrate that larger models don't necessarily guarantee better\ncalibration, that calibration performance is found to be metric-dependent, and\nthat self-consistency methods excel in factoid datasets. We also find that\ncalibration can be enhanced through techniques such as fine-tuning, integrating\nrelevant source documents, scaling the temperature, and combining\nself-consistency with self-evaluation. Lastly, we showcase a practical\napplication of our system: selecting and cascading open-source models and\nChatGPT to optimize correctness given a limited API budget. This research not\nonly challenges existing notions of LLM calibration but also offers practical\nmethodologies for improving trustworthiness in long-form generation.\n""]",Confidence Estimation in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
105,77,105_confidence_confident_reliable_language,"['confidence', 'confident', 'reliable', 'language', 'verbalized', 'reliability', 'models', 'uncertainties', 'prediction', 'semantic']","['confidence', 'uncertainty', 'calibration', 'conformal', 'quantification', 'answers', 'responses', 'answer', 'answering', 'response']","['  Empowering large language models to accurately express confidence in their\nanswers is essential for trustworthy decision-making. Previous confidence\nelicitation methods, which primarily rely on white-box access to internal model\ninformation or model fine-tuning, have become less suitable for LLMs,\nespecially closed-source commercial APIs. This leads to a growing need to\nexplore the untapped area of black-box approaches for LLM uncertainty\nestimation. To better break down the problem, we define a systematic framework\nwith three components: prompting strategies for eliciting verbalized\nconfidence, sampling methods for generating multiple responses, and aggregation\ntechniques for computing consistency. We then benchmark these methods on two\nkey tasks-confidence calibration and failure prediction-across five types of\ndatasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs\nincluding GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights:\n1) LLMs, when verbalizing their confidence, tend to be overconfident,\npotentially imitating human patterns of expressing confidence. 2) As model\ncapability scales up, both calibration and failure prediction performance\nimprove. 3) Employing our proposed strategies, such as human-inspired prompts,\nconsistency among multiple responses, and better aggregation strategies can\nhelp mitigate this overconfidence from various perspectives. 4) Comparisons\nwith white-box methods indicate that while white-box methods perform better,\nthe gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements,\nnone of these techniques consistently outperform others, and all investigated\nmethods struggle in challenging tasks, such as those requiring professional\nknowledge, indicating significant scope for improvement. We believe this study\ncan serve as a strong baseline and provide insights for eliciting confidence in\nblack-box LLMs.\n', '  Although large language models (LLMs) are capable of performing various\ntasks, they still suffer from producing plausible but incorrect responses. To\nimprove the reliability of LLMs, recent research has focused on uncertainty\nquantification to predict whether a response is correct or not. However, most\nuncertainty quantification methods have been evaluated on questions requiring a\nsingle clear answer, ignoring the existence of data uncertainty that arises\nfrom irreducible randomness. Instead, these methods only consider model\nuncertainty, which arises from a lack of knowledge. In this paper, we\ninvestigate previous uncertainty quantification methods under the presence of\ndata uncertainty. Our contributions are two-fold: 1) proposing a new\nMulti-Answer Question Answering dataset, MAQA, consisting of world knowledge,\nmathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty\nquantification regarding data uncertainty, and 2) assessing 5 uncertainty\nquantification methods of diverse white- and black-box LLMs. Our findings show\nthat entropy and consistency-based methods estimate the model uncertainty well\neven under data uncertainty, while other methods for white- and black-box LLMs\nstruggle depending on the tasks. Additionally, methods designed for white-box\nLLMs suffer from overconfidence in reasoning tasks compared to simple knowledge\nqueries. We believe our observations will pave the way for future work on\nuncertainty quantification in realistic setting.\n', ""  To enhance Large Language Models' (LLMs) reliability, calibration is\nessential -- the model's assessed confidence scores should align with the\nactual likelihood of its responses being correct. However, current confidence\nelicitation methods and calibration metrics typically rely on a binary\ntrue/false assessment of response correctness. This approach does not apply to\nlong-form generation, where an answer can be partially correct. Addressing this\ngap, we introduce a unified calibration framework, in which both the\ncorrectness of the LLMs' responses and their associated confidence levels are\ntreated as distributions across a range of scores. Within this framework, we\ndevelop three metrics to precisely evaluate LLM calibration and further propose\ntwo confidence elicitation methods based on self-consistency and\nself-evaluation. Our experiments, which include long-form QA and summarization\ntasks, demonstrate that larger models don't necessarily guarantee better\ncalibration, that calibration performance is found to be metric-dependent, and\nthat self-consistency methods excel in factoid datasets. We also find that\ncalibration can be enhanced through techniques such as fine-tuning, integrating\nrelevant source documents, scaling the temperature, and combining\nself-consistency with self-evaluation. Lastly, we showcase a practical\napplication of our system: selecting and cascading open-source models and\nChatGPT to optimize correctness given a limited API budget. This research not\nonly challenges existing notions of LLM calibration but also offers practical\nmethodologies for improving trustworthiness in long-form generation.\n""]",Confidence Estimation in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
105,77,105_confidence_confident_reliable_language,"['confidence', 'confident', 'reliable', 'language', 'verbalized', 'reliability', 'models', 'uncertainties', 'prediction', 'semantic']","['confidence', 'uncertainty', 'calibration', 'conformal', 'quantification', 'answers', 'responses', 'answer', 'answering', 'response']","['  Empowering large language models to accurately express confidence in their\nanswers is essential for trustworthy decision-making. Previous confidence\nelicitation methods, which primarily rely on white-box access to internal model\ninformation or model fine-tuning, have become less suitable for LLMs,\nespecially closed-source commercial APIs. This leads to a growing need to\nexplore the untapped area of black-box approaches for LLM uncertainty\nestimation. To better break down the problem, we define a systematic framework\nwith three components: prompting strategies for eliciting verbalized\nconfidence, sampling methods for generating multiple responses, and aggregation\ntechniques for computing consistency. We then benchmark these methods on two\nkey tasks-confidence calibration and failure prediction-across five types of\ndatasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs\nincluding GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights:\n1) LLMs, when verbalizing their confidence, tend to be overconfident,\npotentially imitating human patterns of expressing confidence. 2) As model\ncapability scales up, both calibration and failure prediction performance\nimprove. 3) Employing our proposed strategies, such as human-inspired prompts,\nconsistency among multiple responses, and better aggregation strategies can\nhelp mitigate this overconfidence from various perspectives. 4) Comparisons\nwith white-box methods indicate that while white-box methods perform better,\nthe gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements,\nnone of these techniques consistently outperform others, and all investigated\nmethods struggle in challenging tasks, such as those requiring professional\nknowledge, indicating significant scope for improvement. We believe this study\ncan serve as a strong baseline and provide insights for eliciting confidence in\nblack-box LLMs.\n', '  Although large language models (LLMs) are capable of performing various\ntasks, they still suffer from producing plausible but incorrect responses. To\nimprove the reliability of LLMs, recent research has focused on uncertainty\nquantification to predict whether a response is correct or not. However, most\nuncertainty quantification methods have been evaluated on questions requiring a\nsingle clear answer, ignoring the existence of data uncertainty that arises\nfrom irreducible randomness. Instead, these methods only consider model\nuncertainty, which arises from a lack of knowledge. In this paper, we\ninvestigate previous uncertainty quantification methods under the presence of\ndata uncertainty. Our contributions are two-fold: 1) proposing a new\nMulti-Answer Question Answering dataset, MAQA, consisting of world knowledge,\nmathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty\nquantification regarding data uncertainty, and 2) assessing 5 uncertainty\nquantification methods of diverse white- and black-box LLMs. Our findings show\nthat entropy and consistency-based methods estimate the model uncertainty well\neven under data uncertainty, while other methods for white- and black-box LLMs\nstruggle depending on the tasks. Additionally, methods designed for white-box\nLLMs suffer from overconfidence in reasoning tasks compared to simple knowledge\nqueries. We believe our observations will pave the way for future work on\nuncertainty quantification in realistic setting.\n', ""  To enhance Large Language Models' (LLMs) reliability, calibration is\nessential -- the model's assessed confidence scores should align with the\nactual likelihood of its responses being correct. However, current confidence\nelicitation methods and calibration metrics typically rely on a binary\ntrue/false assessment of response correctness. This approach does not apply to\nlong-form generation, where an answer can be partially correct. Addressing this\ngap, we introduce a unified calibration framework, in which both the\ncorrectness of the LLMs' responses and their associated confidence levels are\ntreated as distributions across a range of scores. Within this framework, we\ndevelop three metrics to precisely evaluate LLM calibration and further propose\ntwo confidence elicitation methods based on self-consistency and\nself-evaluation. Our experiments, which include long-form QA and summarization\ntasks, demonstrate that larger models don't necessarily guarantee better\ncalibration, that calibration performance is found to be metric-dependent, and\nthat self-consistency methods excel in factoid datasets. We also find that\ncalibration can be enhanced through techniques such as fine-tuning, integrating\nrelevant source documents, scaling the temperature, and combining\nself-consistency with self-evaluation. Lastly, we showcase a practical\napplication of our system: selecting and cascading open-source models and\nChatGPT to optimize correctness given a limited API budget. This research not\nonly challenges existing notions of LLM calibration but also offers practical\nmethodologies for improving trustworthiness in long-form generation.\n""]",Confidence Estimation in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
105,77,105_confidence_confident_reliable_language,"['confidence', 'confident', 'reliable', 'language', 'verbalized', 'reliability', 'models', 'uncertainties', 'prediction', 'semantic']","['confidence', 'uncertainty', 'calibration', 'conformal', 'quantification', 'answers', 'responses', 'answer', 'answering', 'response']","['  Empowering large language models to accurately express confidence in their\nanswers is essential for trustworthy decision-making. Previous confidence\nelicitation methods, which primarily rely on white-box access to internal model\ninformation or model fine-tuning, have become less suitable for LLMs,\nespecially closed-source commercial APIs. This leads to a growing need to\nexplore the untapped area of black-box approaches for LLM uncertainty\nestimation. To better break down the problem, we define a systematic framework\nwith three components: prompting strategies for eliciting verbalized\nconfidence, sampling methods for generating multiple responses, and aggregation\ntechniques for computing consistency. We then benchmark these methods on two\nkey tasks-confidence calibration and failure prediction-across five types of\ndatasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs\nincluding GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights:\n1) LLMs, when verbalizing their confidence, tend to be overconfident,\npotentially imitating human patterns of expressing confidence. 2) As model\ncapability scales up, both calibration and failure prediction performance\nimprove. 3) Employing our proposed strategies, such as human-inspired prompts,\nconsistency among multiple responses, and better aggregation strategies can\nhelp mitigate this overconfidence from various perspectives. 4) Comparisons\nwith white-box methods indicate that while white-box methods perform better,\nthe gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements,\nnone of these techniques consistently outperform others, and all investigated\nmethods struggle in challenging tasks, such as those requiring professional\nknowledge, indicating significant scope for improvement. We believe this study\ncan serve as a strong baseline and provide insights for eliciting confidence in\nblack-box LLMs.\n', '  Although large language models (LLMs) are capable of performing various\ntasks, they still suffer from producing plausible but incorrect responses. To\nimprove the reliability of LLMs, recent research has focused on uncertainty\nquantification to predict whether a response is correct or not. However, most\nuncertainty quantification methods have been evaluated on questions requiring a\nsingle clear answer, ignoring the existence of data uncertainty that arises\nfrom irreducible randomness. Instead, these methods only consider model\nuncertainty, which arises from a lack of knowledge. In this paper, we\ninvestigate previous uncertainty quantification methods under the presence of\ndata uncertainty. Our contributions are two-fold: 1) proposing a new\nMulti-Answer Question Answering dataset, MAQA, consisting of world knowledge,\nmathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty\nquantification regarding data uncertainty, and 2) assessing 5 uncertainty\nquantification methods of diverse white- and black-box LLMs. Our findings show\nthat entropy and consistency-based methods estimate the model uncertainty well\neven under data uncertainty, while other methods for white- and black-box LLMs\nstruggle depending on the tasks. Additionally, methods designed for white-box\nLLMs suffer from overconfidence in reasoning tasks compared to simple knowledge\nqueries. We believe our observations will pave the way for future work on\nuncertainty quantification in realistic setting.\n', ""  To enhance Large Language Models' (LLMs) reliability, calibration is\nessential -- the model's assessed confidence scores should align with the\nactual likelihood of its responses being correct. However, current confidence\nelicitation methods and calibration metrics typically rely on a binary\ntrue/false assessment of response correctness. This approach does not apply to\nlong-form generation, where an answer can be partially correct. Addressing this\ngap, we introduce a unified calibration framework, in which both the\ncorrectness of the LLMs' responses and their associated confidence levels are\ntreated as distributions across a range of scores. Within this framework, we\ndevelop three metrics to precisely evaluate LLM calibration and further propose\ntwo confidence elicitation methods based on self-consistency and\nself-evaluation. Our experiments, which include long-form QA and summarization\ntasks, demonstrate that larger models don't necessarily guarantee better\ncalibration, that calibration performance is found to be metric-dependent, and\nthat self-consistency methods excel in factoid datasets. We also find that\ncalibration can be enhanced through techniques such as fine-tuning, integrating\nrelevant source documents, scaling the temperature, and combining\nself-consistency with self-evaluation. Lastly, we showcase a practical\napplication of our system: selecting and cascading open-source models and\nChatGPT to optimize correctness given a limited API budget. This research not\nonly challenges existing notions of LLM calibration but also offers practical\nmethodologies for improving trustworthiness in long-form generation.\n""]",Confidence Estimation in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
105,77,105_confidence_confident_reliable_language,"['confidence', 'confident', 'reliable', 'language', 'verbalized', 'reliability', 'models', 'uncertainties', 'prediction', 'semantic']","['confidence', 'uncertainty', 'calibration', 'conformal', 'quantification', 'answers', 'responses', 'answer', 'answering', 'response']","['  Empowering large language models to accurately express confidence in their\nanswers is essential for trustworthy decision-making. Previous confidence\nelicitation methods, which primarily rely on white-box access to internal model\ninformation or model fine-tuning, have become less suitable for LLMs,\nespecially closed-source commercial APIs. This leads to a growing need to\nexplore the untapped area of black-box approaches for LLM uncertainty\nestimation. To better break down the problem, we define a systematic framework\nwith three components: prompting strategies for eliciting verbalized\nconfidence, sampling methods for generating multiple responses, and aggregation\ntechniques for computing consistency. We then benchmark these methods on two\nkey tasks-confidence calibration and failure prediction-across five types of\ndatasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs\nincluding GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights:\n1) LLMs, when verbalizing their confidence, tend to be overconfident,\npotentially imitating human patterns of expressing confidence. 2) As model\ncapability scales up, both calibration and failure prediction performance\nimprove. 3) Employing our proposed strategies, such as human-inspired prompts,\nconsistency among multiple responses, and better aggregation strategies can\nhelp mitigate this overconfidence from various perspectives. 4) Comparisons\nwith white-box methods indicate that while white-box methods perform better,\nthe gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements,\nnone of these techniques consistently outperform others, and all investigated\nmethods struggle in challenging tasks, such as those requiring professional\nknowledge, indicating significant scope for improvement. We believe this study\ncan serve as a strong baseline and provide insights for eliciting confidence in\nblack-box LLMs.\n', '  Although large language models (LLMs) are capable of performing various\ntasks, they still suffer from producing plausible but incorrect responses. To\nimprove the reliability of LLMs, recent research has focused on uncertainty\nquantification to predict whether a response is correct or not. However, most\nuncertainty quantification methods have been evaluated on questions requiring a\nsingle clear answer, ignoring the existence of data uncertainty that arises\nfrom irreducible randomness. Instead, these methods only consider model\nuncertainty, which arises from a lack of knowledge. In this paper, we\ninvestigate previous uncertainty quantification methods under the presence of\ndata uncertainty. Our contributions are two-fold: 1) proposing a new\nMulti-Answer Question Answering dataset, MAQA, consisting of world knowledge,\nmathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty\nquantification regarding data uncertainty, and 2) assessing 5 uncertainty\nquantification methods of diverse white- and black-box LLMs. Our findings show\nthat entropy and consistency-based methods estimate the model uncertainty well\neven under data uncertainty, while other methods for white- and black-box LLMs\nstruggle depending on the tasks. Additionally, methods designed for white-box\nLLMs suffer from overconfidence in reasoning tasks compared to simple knowledge\nqueries. We believe our observations will pave the way for future work on\nuncertainty quantification in realistic setting.\n', ""  To enhance Large Language Models' (LLMs) reliability, calibration is\nessential -- the model's assessed confidence scores should align with the\nactual likelihood of its responses being correct. However, current confidence\nelicitation methods and calibration metrics typically rely on a binary\ntrue/false assessment of response correctness. This approach does not apply to\nlong-form generation, where an answer can be partially correct. Addressing this\ngap, we introduce a unified calibration framework, in which both the\ncorrectness of the LLMs' responses and their associated confidence levels are\ntreated as distributions across a range of scores. Within this framework, we\ndevelop three metrics to precisely evaluate LLM calibration and further propose\ntwo confidence elicitation methods based on self-consistency and\nself-evaluation. Our experiments, which include long-form QA and summarization\ntasks, demonstrate that larger models don't necessarily guarantee better\ncalibration, that calibration performance is found to be metric-dependent, and\nthat self-consistency methods excel in factoid datasets. We also find that\ncalibration can be enhanced through techniques such as fine-tuning, integrating\nrelevant source documents, scaling the temperature, and combining\nself-consistency with self-evaluation. Lastly, we showcase a practical\napplication of our system: selecting and cascading open-source models and\nChatGPT to optimize correctness given a limited API budget. This research not\nonly challenges existing notions of LLM calibration but also offers practical\nmethodologies for improving trustworthiness in long-form generation.\n""]",Confidence Estimation in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
105,77,105_confidence_confident_reliable_language,"['confidence', 'confident', 'reliable', 'language', 'verbalized', 'reliability', 'models', 'uncertainties', 'prediction', 'semantic']","['confidence', 'uncertainty', 'calibration', 'conformal', 'quantification', 'answers', 'responses', 'answer', 'answering', 'response']","['  Empowering large language models to accurately express confidence in their\nanswers is essential for trustworthy decision-making. Previous confidence\nelicitation methods, which primarily rely on white-box access to internal model\ninformation or model fine-tuning, have become less suitable for LLMs,\nespecially closed-source commercial APIs. This leads to a growing need to\nexplore the untapped area of black-box approaches for LLM uncertainty\nestimation. To better break down the problem, we define a systematic framework\nwith three components: prompting strategies for eliciting verbalized\nconfidence, sampling methods for generating multiple responses, and aggregation\ntechniques for computing consistency. We then benchmark these methods on two\nkey tasks-confidence calibration and failure prediction-across five types of\ndatasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs\nincluding GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights:\n1) LLMs, when verbalizing their confidence, tend to be overconfident,\npotentially imitating human patterns of expressing confidence. 2) As model\ncapability scales up, both calibration and failure prediction performance\nimprove. 3) Employing our proposed strategies, such as human-inspired prompts,\nconsistency among multiple responses, and better aggregation strategies can\nhelp mitigate this overconfidence from various perspectives. 4) Comparisons\nwith white-box methods indicate that while white-box methods perform better,\nthe gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements,\nnone of these techniques consistently outperform others, and all investigated\nmethods struggle in challenging tasks, such as those requiring professional\nknowledge, indicating significant scope for improvement. We believe this study\ncan serve as a strong baseline and provide insights for eliciting confidence in\nblack-box LLMs.\n', '  Although large language models (LLMs) are capable of performing various\ntasks, they still suffer from producing plausible but incorrect responses. To\nimprove the reliability of LLMs, recent research has focused on uncertainty\nquantification to predict whether a response is correct or not. However, most\nuncertainty quantification methods have been evaluated on questions requiring a\nsingle clear answer, ignoring the existence of data uncertainty that arises\nfrom irreducible randomness. Instead, these methods only consider model\nuncertainty, which arises from a lack of knowledge. In this paper, we\ninvestigate previous uncertainty quantification methods under the presence of\ndata uncertainty. Our contributions are two-fold: 1) proposing a new\nMulti-Answer Question Answering dataset, MAQA, consisting of world knowledge,\nmathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty\nquantification regarding data uncertainty, and 2) assessing 5 uncertainty\nquantification methods of diverse white- and black-box LLMs. Our findings show\nthat entropy and consistency-based methods estimate the model uncertainty well\neven under data uncertainty, while other methods for white- and black-box LLMs\nstruggle depending on the tasks. Additionally, methods designed for white-box\nLLMs suffer from overconfidence in reasoning tasks compared to simple knowledge\nqueries. We believe our observations will pave the way for future work on\nuncertainty quantification in realistic setting.\n', ""  To enhance Large Language Models' (LLMs) reliability, calibration is\nessential -- the model's assessed confidence scores should align with the\nactual likelihood of its responses being correct. However, current confidence\nelicitation methods and calibration metrics typically rely on a binary\ntrue/false assessment of response correctness. This approach does not apply to\nlong-form generation, where an answer can be partially correct. Addressing this\ngap, we introduce a unified calibration framework, in which both the\ncorrectness of the LLMs' responses and their associated confidence levels are\ntreated as distributions across a range of scores. Within this framework, we\ndevelop three metrics to precisely evaluate LLM calibration and further propose\ntwo confidence elicitation methods based on self-consistency and\nself-evaluation. Our experiments, which include long-form QA and summarization\ntasks, demonstrate that larger models don't necessarily guarantee better\ncalibration, that calibration performance is found to be metric-dependent, and\nthat self-consistency methods excel in factoid datasets. We also find that\ncalibration can be enhanced through techniques such as fine-tuning, integrating\nrelevant source documents, scaling the temperature, and combining\nself-consistency with self-evaluation. Lastly, we showcase a practical\napplication of our system: selecting and cascading open-source models and\nChatGPT to optimize correctness given a limited API budget. This research not\nonly challenges existing notions of LLM calibration but also offers practical\nmethodologies for improving trustworthiness in long-form generation.\n""]",Confidence Estimation in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
106,77,106_channels_channel_decoding_decoders,"['channels', 'channel', 'decoding', 'decoders', 'mimo', 'decoder', 'multiplexing', 'communications', 'wireless', 'receiver']","['channel', 'codes', 'interference', 'wireless', 'signal', 'channels', 'coding', 'transmission', 'communication', 'communications']","['  Though achieving marvelous progress in various scenarios, existing semantic\ncommunication frameworks mainly consider single-input single-output Gaussian\nchannels or Rayleigh fading channels, neglecting the widely-used multiple-input\nmultiple-output (MIMO) channels, which hinders the application into practical\nsystems. One common solution to combat MIMO fading is to utilize feedback MIMO\nchannel state information (CSI). In this paper, we incorporate MIMO CSI into\nsystem designs from a new perspective and propose the learnable CSI fusion\nsemantic communication (LCFSC) framework, where CSI is treated as side\ninformation by the semantic extractor to enhance the semantic coding. To avoid\nfeature fusion due to abrupt combination of CSI with features, we present a\nnon-invasive CSI fusion multi-head attention module inside the Swin\nTransformer. With the learned attention masking map determined by both source\nand channel states, more robust attention distribution could be generated.\nFurthermore, the percentage of mask elements could be flexibly adjusted by the\nlearnable mask ratio, which is produced based on the conditional variational\ninterference in an unsupervised manner. In this way, CSI-aware semantic coding\nis achieved through learnable CSI fusion masking. Experiment results testify\nthe superiority of LCFSC over traditional schemes and state-of-the-art Swin\nTransformer-based semantic communication frameworks in MIMO fading channels.\n', '  Recently, deep learning (DL) has been emerging as a promising approach for\nchannel estimation and signal detection in wireless communications. The\nmajority of the existing studies investigating the use of DL techniques in this\ndomain focus on analysing channel impulse responses that are generated from\nonly one channel distribution such as additive white Gaussian channel noise and\nRayleigh channels. In practice, to cope with the dynamic nature of the wireless\nchannel, DL methods must be re-trained on newly non-aged collected data which\nis costly, inefficient, and impractical. To tackle this challenge, this paper\nproposes a novel universal deep neural network (Uni-DNN) that can achieve high\ndetection performance in various wireless environments without retraining the\nmodel. In particular, our proposed Uni-DNN model consists of a wireless channel\nclassifier and a signal detector which are constructed by using DNNs. The\nwireless channel classifier enables the signal detector to generalise and\nperform optimally for multiple wireless channel distributions. In addition, to\nfurther improve the signal detection performance of the proposed model,\nconvolutional neural network is employed. Extensive simulations using the\northogonal frequency division multiplexing scheme demonstrate that the bit\nerror rate performance of our proposed solution can outperform conventional\nDL-based approaches as well as least square and minimum mean square error\nchannel estimators in practical low pilot density scenarios.\n', '  In general, reliable communication via multiple-input multiple-output (MIMO)\northogonal frequency division multiplexing (OFDM) requires accurate channel\nestimation at the receiver. The existing literature largely focuses on\ndenoising methods for channel estimation that depend on either (i) channel\nanalysis in the time-domain with prior channel knowledge or (ii) supervised\nlearning techniques which require large pre-labeled datasets for training. To\naddress these limitations, we present a frequency-domain denoising method based\non a reinforcement learning framework that does not need a priori channel\nknowledge and pre-labeled data. Our methodology includes a new successive\nchannel denoising process based on channel curvature computation, for which we\nobtain a channel curvature magnitude threshold to identify unreliable channel\nestimates. Based on this process, we formulate the denoising mechanism as a\nMarkov decision process, where we define the actions through a geometry-based\nchannel estimation update, and the reward function based on a policy that\nreduces mean squared error (MSE). We then resort to Q-learning to update the\nchannel estimates. Numerical results verify that our denoising algorithm can\nsuccessfully mitigate noise in channel estimates. In particular, our algorithm\nprovides a significant improvement over the practical least squares (LS)\nestimation method and provides performance that approaches that of the ideal\nlinear minimum mean square error (LMMSE) estimation with perfect knowledge of\nchannel statistics.\n']",MIMO Channel Estimation and Decoding in Wireless Communications,Advanced Wireless Communication Technologies,Wireless Technologies and Sensing Systems,Wireless Technologies and Sensing Systems
107,76,107_symbolic_ai_neural_neuro,"['symbolic', 'ai', 'neural', 'neuro', 'symbols', 'neurosymbolic', 'semantic', 'semantics', 'knowledge', 'deductive']","['symbolic', 'neurosymbolic', 'logic', 'logical', 'rules', 'symbol', 'reasoning', 'probabilistic', 'neural', 'rule']","['  Neuro-Symbolic (NeSy) integration combines symbolic reasoning with Neural\nNetworks (NNs) for tasks requiring perception and reasoning. Most NeSy systems\nrely on continuous relaxation of logical knowledge, and no discrete decisions\nare made within the model pipeline. Furthermore, these methods assume that the\nsymbolic rules are given. In this paper, we propose Deep Symbolic Learning\n(DSL), a NeSy system that learns NeSy-functions, i.e., the composition of a\n(set of) perception functions which map continuous data to discrete symbols,\nand a symbolic function over the set of symbols. DSL learns simultaneously the\nperception and symbolic functions while being trained only on their composition\n(NeSy-function). The key novelty of DSL is that it can create internal\n(interpretable) symbolic representations and map them to perception inputs\nwithin a differentiable NN learning pipeline. The created symbols are\nautomatically selected to generate symbolic functions that best explain the\ndata. We provide experimental analysis to substantiate the efficacy of DSL in\nsimultaneously learning perception and symbolic functions.\n', '  The field of neuro-symbolic AI aims to benefit from the combination of neural\nnetworks and symbolic systems. A cornerstone of the field is the translation or\nencoding of symbolic knowledge into neural networks. Although many\nneuro-symbolic methods and approaches have been proposed throughout the years,\nand with an large increase in recent years, no common definition of encoding\nexists that can enable a precise, theoretical comparison of neuro-symbolic\nmethods. This paper addresses this problem by introducing a semantic framework\nfor neuro-symbolic AI. We start by providing a formal definition of semantic\nencoding, specifying the components and conditions under which a knowledge-base\ncan be encoded correctly by a neural network. We then show that many\nneuro-symbolic approaches are accounted for by this definition. We provide a\nnumber of examples and correspondence proofs of the application of the proposed\nframework to the neural encoding of various forms of knowledge representation.\nMany, at first sight disparate, neuro-symbolic methods, are shown to fall\nwithin the proposed formalization. This is expected to provide a guidance to\nfuture neuro-symbolic encodings by placing them in the broader context of the\nsemantic encoding of entire families of existing neuro-symbolic systems. The\npaper is hoped to help initiate a discussion around the provision of a theory\nfor neuro-symbolic AI and a semantics for deep learning.\n', '  Neurosymbolic artificial intelligence (AI) is an emerging branch of AI that\ncombines the strengths of symbolic AI and sub-symbolic AI. A major drawback of\nsub-symbolic AI is that it acts as a ""black box"", meaning that predictions are\ndifficult to explain, making the testing & evaluation (T&E) and validation &\nverification (V&V) processes of a system that uses sub-symbolic AI a challenge.\nSince neurosymbolic AI combines the advantages of both symbolic and\nsub-symbolic AI, this survey explores how neurosymbolic applications can ease\nthe V&V process. This survey considers two taxonomies of neurosymbolic AI,\nevaluates them, and analyzes which algorithms are commonly used as the symbolic\nand sub-symbolic components in current applications. Additionally, an overview\nof current techniques for the T&E and V&V processes of these components is\nprovided. Furthermore, it is investigated how the symbolic part is used for T&E\nand V&V purposes in current neurosymbolic applications. Our research shows that\nneurosymbolic AI as great potential to ease the T&E and V&V processes of\nsub-symbolic AI by leveraging the possibilities of symbolic AI. Additionally,\nthe applicability of current T&E and V&V methods to neurosymbolic AI is\nassessed, and how different neurosymbolic architectures can impact these\nmethods is explored. It is found that current T&E and V&V techniques are partly\nsufficient to test, evaluate, verify, or validate the symbolic and sub-symbolic\npart of neurosymbolic applications independently, while some of them use\napproaches where current T&E and V&V methods are not applicable by default, and\nadjustments or even new approaches are needed. Our research shows that there is\ngreat potential in using symbolic AI to test, evaluate, verify, or validate the\npredictions of a sub-symbolic model, making neurosymbolic AI an interesting\nresearch direction for safe, secure, and trustworthy AI.\n']",Neuro-Symbolic AI and Semantic Integration,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
107,76,107_symbolic_ai_neural_neuro,"['symbolic', 'ai', 'neural', 'neuro', 'symbols', 'neurosymbolic', 'semantic', 'semantics', 'knowledge', 'deductive']","['symbolic', 'neurosymbolic', 'logic', 'logical', 'rules', 'symbol', 'reasoning', 'probabilistic', 'neural', 'rule']","['  Neuro-Symbolic (NeSy) integration combines symbolic reasoning with Neural\nNetworks (NNs) for tasks requiring perception and reasoning. Most NeSy systems\nrely on continuous relaxation of logical knowledge, and no discrete decisions\nare made within the model pipeline. Furthermore, these methods assume that the\nsymbolic rules are given. In this paper, we propose Deep Symbolic Learning\n(DSL), a NeSy system that learns NeSy-functions, i.e., the composition of a\n(set of) perception functions which map continuous data to discrete symbols,\nand a symbolic function over the set of symbols. DSL learns simultaneously the\nperception and symbolic functions while being trained only on their composition\n(NeSy-function). The key novelty of DSL is that it can create internal\n(interpretable) symbolic representations and map them to perception inputs\nwithin a differentiable NN learning pipeline. The created symbols are\nautomatically selected to generate symbolic functions that best explain the\ndata. We provide experimental analysis to substantiate the efficacy of DSL in\nsimultaneously learning perception and symbolic functions.\n', '  The field of neuro-symbolic AI aims to benefit from the combination of neural\nnetworks and symbolic systems. A cornerstone of the field is the translation or\nencoding of symbolic knowledge into neural networks. Although many\nneuro-symbolic methods and approaches have been proposed throughout the years,\nand with an large increase in recent years, no common definition of encoding\nexists that can enable a precise, theoretical comparison of neuro-symbolic\nmethods. This paper addresses this problem by introducing a semantic framework\nfor neuro-symbolic AI. We start by providing a formal definition of semantic\nencoding, specifying the components and conditions under which a knowledge-base\ncan be encoded correctly by a neural network. We then show that many\nneuro-symbolic approaches are accounted for by this definition. We provide a\nnumber of examples and correspondence proofs of the application of the proposed\nframework to the neural encoding of various forms of knowledge representation.\nMany, at first sight disparate, neuro-symbolic methods, are shown to fall\nwithin the proposed formalization. This is expected to provide a guidance to\nfuture neuro-symbolic encodings by placing them in the broader context of the\nsemantic encoding of entire families of existing neuro-symbolic systems. The\npaper is hoped to help initiate a discussion around the provision of a theory\nfor neuro-symbolic AI and a semantics for deep learning.\n', '  Neurosymbolic artificial intelligence (AI) is an emerging branch of AI that\ncombines the strengths of symbolic AI and sub-symbolic AI. A major drawback of\nsub-symbolic AI is that it acts as a ""black box"", meaning that predictions are\ndifficult to explain, making the testing & evaluation (T&E) and validation &\nverification (V&V) processes of a system that uses sub-symbolic AI a challenge.\nSince neurosymbolic AI combines the advantages of both symbolic and\nsub-symbolic AI, this survey explores how neurosymbolic applications can ease\nthe V&V process. This survey considers two taxonomies of neurosymbolic AI,\nevaluates them, and analyzes which algorithms are commonly used as the symbolic\nand sub-symbolic components in current applications. Additionally, an overview\nof current techniques for the T&E and V&V processes of these components is\nprovided. Furthermore, it is investigated how the symbolic part is used for T&E\nand V&V purposes in current neurosymbolic applications. Our research shows that\nneurosymbolic AI as great potential to ease the T&E and V&V processes of\nsub-symbolic AI by leveraging the possibilities of symbolic AI. Additionally,\nthe applicability of current T&E and V&V methods to neurosymbolic AI is\nassessed, and how different neurosymbolic architectures can impact these\nmethods is explored. It is found that current T&E and V&V techniques are partly\nsufficient to test, evaluate, verify, or validate the symbolic and sub-symbolic\npart of neurosymbolic applications independently, while some of them use\napproaches where current T&E and V&V methods are not applicable by default, and\nadjustments or even new approaches are needed. Our research shows that there is\ngreat potential in using symbolic AI to test, evaluate, verify, or validate the\npredictions of a sub-symbolic model, making neurosymbolic AI an interesting\nresearch direction for safe, secure, and trustworthy AI.\n']",Neuro-Symbolic AI and Semantic Integration,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
108,75,108_cybersecurity_cyberattacks_cyberattack_cybercrime,"['cybersecurity', 'cyberattacks', 'cyberattack', 'cybercrime', 'llm4security', 'security', 'malware', 'cybermetric', 'threats', 'phishing']","['cybersecurity', 'cyber', 'threat', 'security', 'threats', 'vulnerabilities', 'attack', 'malware', 'attacks', 'cybercrime']","['  The rapid advancement of Large Language Models (LLMs) has opened up new\nopportunities for leveraging artificial intelligence in various domains,\nincluding cybersecurity. As the volume and sophistication of cyber threats\ncontinue to grow, there is an increasing need for intelligent systems that can\nautomatically detect vulnerabilities, analyze malware, and respond to attacks.\nIn this survey, we conduct a comprehensive review of the literature on the\napplication of LLMs in cybersecurity (LLM4Security). By comprehensively\ncollecting over 30K relevant papers and systematically analyzing 127 papers\nfrom top security and software engineering venues, we aim to provide a holistic\nview of how LLMs are being used to solve diverse problems across the\ncybersecurity domain. Through our analysis, we identify several key findings.\nFirst, we observe that LLMs are being applied to a wide range of cybersecurity\ntasks, including vulnerability detection, malware analysis, network intrusion\ndetection, and phishing detection. Second, we find that the datasets used for\ntraining and evaluating LLMs in these tasks are often limited in size and\ndiversity, highlighting the need for more comprehensive and representative\ndatasets. Third, we identify several promising techniques for adapting LLMs to\nspecific cybersecurity domains, such as fine-tuning, transfer learning, and\ndomain-specific pre-training. Finally, we discuss the main challenges and\nopportunities for future research in LLM4Security, including the need for more\ninterpretable and explainable models, the importance of addressing data privacy\nand security concerns, and the potential for leveraging LLMs for proactive\ndefense and threat hunting. Overall, our survey provides a comprehensive\noverview of the current state-of-the-art in LLM4Security and identifies several\npromising directions for future research.\n', '  Cybersecurity researchers have contributed to the automated extraction of CTI\nfrom textual sources, such as threat reports and online articles, where\ncyberattack strategies, procedures, and tools are described. The goal of this\narticle is to aid cybersecurity researchers understand the current techniques\nused for cyberthreat intelligence extraction from text through a survey of\nrelevant studies in the literature. We systematically collect ""CTI extraction\nfrom text""-related studies from the literature and categorize the CTI\nextraction purposes. We propose a CTI extraction pipeline abstracted from these\nstudies. We identify the data sources, techniques, and CTI sharing formats\nutilized in the context of the proposed pipeline. Our work finds ten types of\nextraction purposes, such as extraction indicators of compromise extraction,\nTTPs (tactics, techniques, procedures of attack), and cybersecurity keywords.\nWe also identify seven types of textual sources for CTI extraction, and textual\ndata obtained from hacker forums, threat reports, social media posts, and\nonline news articles have been used by almost 90% of the studies. Natural\nlanguage processing along with both supervised and unsupervised machine\nlearning techniques such as named entity recognition, topic modelling,\ndependency parsing, supervised classification, and clustering are used for CTI\nextraction. We observe the technical challenges associated with these studies\nrelated to obtaining available clean, labelled data which could assure\nreplication, validation, and further extension of the studies. As we find the\nstudies focusing on CTI information extraction from text, we advocate for\nbuilding upon the current CTI extraction work to help cybersecurity\npractitioners with proactive decision making such as threat prioritization,\nautomated threat modelling to utilize knowledge from past cybersecurity\nincidents.\n', '  This paper provides a comprehensive review of the future of cybersecurity\nthrough Generative AI and Large Language Models (LLMs). We explore LLM\napplications across various domains, including hardware design security,\nintrusion detection, software engineering, design verification, cyber threat\nintelligence, malware detection, and phishing detection. We present an overview\nof LLM evolution and its current state, focusing on advancements in models such\nas GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends\nto LLM vulnerabilities, such as prompt injection, insecure output handling,\ndata poisoning, DDoS attacks, and adversarial instructions. We delve into\nmitigation strategies to protect these models, providing a comprehensive look\nat potential attack scenarios and prevention techniques. Furthermore, we\nevaluate the performance of 42 LLM models in cybersecurity knowledge and\nhardware security, highlighting their strengths and weaknesses. We thoroughly\nevaluate cybersecurity datasets for LLM training and testing, covering the\nlifecycle from data creation to usage and identifying gaps for future research.\nIn addition, we review new strategies for leveraging LLMs, including techniques\nlike Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human\nFeedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank\nAdapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim\nto enhance real-time cybersecurity defenses and improve the sophistication of\nLLM applications in threat detection and response. Our paper provides a\nfoundational understanding and strategic direction for integrating LLMs into\nfuture cybersecurity frameworks, emphasizing innovation and robust model\ndeployment to safeguard against evolving cyber threats.\n']",Large Language Models for Cybersecurity,"Large Language Models: Safety, Security, and Ethics",Large Language Models,Large Language Models
108,75,108_cybersecurity_cyberattacks_cyberattack_cybercrime,"['cybersecurity', 'cyberattacks', 'cyberattack', 'cybercrime', 'llm4security', 'security', 'malware', 'cybermetric', 'threats', 'phishing']","['cybersecurity', 'cyber', 'threat', 'security', 'threats', 'vulnerabilities', 'attack', 'malware', 'attacks', 'cybercrime']","['  The rapid advancement of Large Language Models (LLMs) has opened up new\nopportunities for leveraging artificial intelligence in various domains,\nincluding cybersecurity. As the volume and sophistication of cyber threats\ncontinue to grow, there is an increasing need for intelligent systems that can\nautomatically detect vulnerabilities, analyze malware, and respond to attacks.\nIn this survey, we conduct a comprehensive review of the literature on the\napplication of LLMs in cybersecurity (LLM4Security). By comprehensively\ncollecting over 30K relevant papers and systematically analyzing 127 papers\nfrom top security and software engineering venues, we aim to provide a holistic\nview of how LLMs are being used to solve diverse problems across the\ncybersecurity domain. Through our analysis, we identify several key findings.\nFirst, we observe that LLMs are being applied to a wide range of cybersecurity\ntasks, including vulnerability detection, malware analysis, network intrusion\ndetection, and phishing detection. Second, we find that the datasets used for\ntraining and evaluating LLMs in these tasks are often limited in size and\ndiversity, highlighting the need for more comprehensive and representative\ndatasets. Third, we identify several promising techniques for adapting LLMs to\nspecific cybersecurity domains, such as fine-tuning, transfer learning, and\ndomain-specific pre-training. Finally, we discuss the main challenges and\nopportunities for future research in LLM4Security, including the need for more\ninterpretable and explainable models, the importance of addressing data privacy\nand security concerns, and the potential for leveraging LLMs for proactive\ndefense and threat hunting. Overall, our survey provides a comprehensive\noverview of the current state-of-the-art in LLM4Security and identifies several\npromising directions for future research.\n', '  Cybersecurity researchers have contributed to the automated extraction of CTI\nfrom textual sources, such as threat reports and online articles, where\ncyberattack strategies, procedures, and tools are described. The goal of this\narticle is to aid cybersecurity researchers understand the current techniques\nused for cyberthreat intelligence extraction from text through a survey of\nrelevant studies in the literature. We systematically collect ""CTI extraction\nfrom text""-related studies from the literature and categorize the CTI\nextraction purposes. We propose a CTI extraction pipeline abstracted from these\nstudies. We identify the data sources, techniques, and CTI sharing formats\nutilized in the context of the proposed pipeline. Our work finds ten types of\nextraction purposes, such as extraction indicators of compromise extraction,\nTTPs (tactics, techniques, procedures of attack), and cybersecurity keywords.\nWe also identify seven types of textual sources for CTI extraction, and textual\ndata obtained from hacker forums, threat reports, social media posts, and\nonline news articles have been used by almost 90% of the studies. Natural\nlanguage processing along with both supervised and unsupervised machine\nlearning techniques such as named entity recognition, topic modelling,\ndependency parsing, supervised classification, and clustering are used for CTI\nextraction. We observe the technical challenges associated with these studies\nrelated to obtaining available clean, labelled data which could assure\nreplication, validation, and further extension of the studies. As we find the\nstudies focusing on CTI information extraction from text, we advocate for\nbuilding upon the current CTI extraction work to help cybersecurity\npractitioners with proactive decision making such as threat prioritization,\nautomated threat modelling to utilize knowledge from past cybersecurity\nincidents.\n', '  This paper provides a comprehensive review of the future of cybersecurity\nthrough Generative AI and Large Language Models (LLMs). We explore LLM\napplications across various domains, including hardware design security,\nintrusion detection, software engineering, design verification, cyber threat\nintelligence, malware detection, and phishing detection. We present an overview\nof LLM evolution and its current state, focusing on advancements in models such\nas GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends\nto LLM vulnerabilities, such as prompt injection, insecure output handling,\ndata poisoning, DDoS attacks, and adversarial instructions. We delve into\nmitigation strategies to protect these models, providing a comprehensive look\nat potential attack scenarios and prevention techniques. Furthermore, we\nevaluate the performance of 42 LLM models in cybersecurity knowledge and\nhardware security, highlighting their strengths and weaknesses. We thoroughly\nevaluate cybersecurity datasets for LLM training and testing, covering the\nlifecycle from data creation to usage and identifying gaps for future research.\nIn addition, we review new strategies for leveraging LLMs, including techniques\nlike Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human\nFeedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank\nAdapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim\nto enhance real-time cybersecurity defenses and improve the sophistication of\nLLM applications in threat detection and response. Our paper provides a\nfoundational understanding and strategic direction for integrating LLMs into\nfuture cybersecurity frameworks, emphasizing innovation and robust model\ndeployment to safeguard against evolving cyber threats.\n']",Large Language Models for Cybersecurity,"Large Language Models: Safety, Security, and Ethics",Large Language Models,Large Language Models
109,75,109_creativity_creative_generative_ai,"['creativity', 'creative', 'generative', 'ai', 'ideation', 'storymaking', 'creation', 'creators', 'generated', 'writers']","['creativity', 'creative', 'writing', 'story', 'storytelling', 'stories', 'ideas', 'writers', 'writer', 'human']","['  What constitutes human creativity, and is it possible for computers to\nexhibit genuine creativity? We argue that achieving human-level intelligence in\ncomputers, or so-called Artificial General Intelligence, necessitates attaining\nalso human-level creativity. We contribute to this discussion by developing a\nstatistical representation of human creativity, incorporating prior insights\nfrom stochastic theory, psychology, philosophy, neuroscience, and chaos theory.\nThis highlights the stochastic nature of the human creative process, which\nincludes both a bias guided, random proposal step, and an evaluation step\ndepending on a flexible or transformable bias structure. The acquired\nrepresentation of human creativity is subsequently used to assess the\ncreativity levels of various contemporary AI systems. Our analysis includes\nmodern AI algorithms such as reinforcement learning, diffusion models, and\nlarge language models, addressing to what extent they measure up to human level\ncreativity. We conclude that these technologies currently lack the capability\nfor autonomous creative action at a human level.\n', ""  In the field of natural language processing, the rapid development of large\nlanguage model (LLM) has attracted more and more attention. LLMs have shown a\nhigh level of creativity in various tasks, but the methods for assessing such\ncreativity are inadequate. The assessment of LLM creativity needs to consider\ndifferences from humans, requiring multi-dimensional measurement while\nbalancing accuracy and efficiency. This paper aims to establish an efficient\nframework for assessing the level of creativity in LLMs. By adapting the\nmodified Torrance Tests of Creative Thinking, the research evaluates the\ncreative performance of various LLMs across 7 tasks, emphasizing 4 criteria\nincluding Fluency, Flexibility, Originality, and Elaboration. In this context,\nwe develop a comprehensive dataset of 700 questions for testing and an\nLLM-based evaluation method. In addition, this study presents a novel analysis\nof LLMs' responses to diverse prompts and role-play situations. We found that\nthe creativity of LLMs primarily falls short in originality, while excelling in\nelaboration. Besides, the use of prompts and the role-play settings of the\nmodel significantly influence creativity. Additionally, the experimental\nresults also indicate that collaboration among multiple LLMs can enhance\noriginality. Notably, our findings reveal a consensus between human evaluations\nand LLMs regarding the personality traits that influence creativity. The\nfindings underscore the significant impact of LLM design on creativity and\nbridges artificial intelligence and human creativity, offering insights into\nLLMs' creativity and potential applications.\n"", ""  Creativity serves as a cornerstone for societal progress and innovation. With\nthe rise of advanced generative AI models capable of tasks once reserved for\nhuman creativity, the study of AI's creative potential becomes imperative for\nits responsible development and application. In this paper, we prove in theory\nthat AI can be as creative as humans under the condition that it can properly\nfit the data generated by human creators. Therefore, the debate on AI's\ncreativity is reduced into the question of its ability to fit a sufficient\namount of data. To arrive at this conclusion, this paper first addresses the\ncomplexities in defining creativity by introducing a new concept called\nRelative Creativity. Rather than attempting to define creativity universally,\nwe shift the focus to whether AI can match the creative abilities of a\nhypothetical human. The methodological shift leads to a statistically\nquantifiable assessment of AI's creativity, term Statistical Creativity. This\nconcept, statistically comparing the creative abilities of AI with those of\nspecific human groups, facilitates theoretical exploration of AI's creative\npotential. Our analysis reveals that by fitting extensive conditional data\nwithout marginalizing out the generative conditions, AI can emerge as a\nhypothetical new creator. The creator possesses the same creative abilities on\npar with the human creators it was trained on. Building on theoretical\nfindings, we discuss the application in prompt-conditioned autoregressive\nmodels, providing a practical means for evaluating creative abilities of\ngenerative AI models, such as Large Language Models (LLMs). Additionally, this\nstudy provides an actionable training guideline, bridging the theoretical\nquantification of creativity with practical model training.\n""]",Assessing Creativity in Artificial Intelligence,Artificial Intelligence in Creative Industries,Artificial Intelligence in Creative Industries,Artificial Intelligence in Creative Industries
110,74,110_turbulent_flows_turbulence_flow,"['turbulent', 'flows', 'turbulence', 'flow', 'aerodynamic', 'reynolds', 'vortex', 'stokes', 'vorticity', 'fluids']","['fluid', 'mesh', 'turbulence', 'turbulent', 'flow', 'simulations', 'meshes', 'velocity', 'flows', 'dynamics']","['  Fluid data completion is a research problem with high potential benefit for\nboth experimental and computational fluid dynamics. An effective fluid data\ncompletion method reduces the required number of sensors in a fluid dynamics\nexperiment, and allows a coarser and more adaptive mesh for a Computational\nFluid Dynamics (CFD) simulation. However, the ill-posed nature of the fluid\ndata completion problem makes it prohibitively difficult to obtain a\ntheoretical solution and presents high numerical uncertainty and instability\nfor a data-driven approach (e.g., a neural network model). To address these\nchallenges, we leverage recent advancements in computer vision, employing the\nvector quantization technique to map both complete and incomplete fluid data\nspaces onto discrete-valued lower-dimensional representations via a two-stage\nlearning procedure. We demonstrated the effectiveness of our approach on\nKolmogorov flow data (Reynolds number: 1000) occluded by masks of different\nsize and arrangement. Experimental results show that our proposed model\nconsistently outperforms benchmark models under different occlusion settings in\nterms of point-wise reconstruction accuracy as well as turbulent energy\nspectrum and vorticity distribution.\n', ""  Computational Fluid Dynamics (CFD) serves as a powerful tool for simulating\nfluid flow across diverse industries. High-resolution CFD simulations offer\nvaluable insights into fluid behavior and flow patterns, aiding in optimizing\ndesign features or enhancing system performance. However, as resolution\nincreases, computational data requirements and time increase proportionately.\nThis presents a persistent challenge in CFD. Recently, efforts have been\ndirected towards accurately predicting fine-mesh simulations using coarse-mesh\nsimulations, with geometry and boundary conditions as input. Drawing\ninspiration from models designed for super-resolution, deep learning techniques\nlike UNets have been applied to address this challenge. However, these existing\nmethods are limited to structured data and fail if the mesh is unstructured due\nto its inability to convolute. Additionally, incorporating geometry/mesh\ninformation in the training process introduces drawbacks such as increased data\nrequirements, challenges in generalizing to unseen geometries for the same\nphysical phenomena, and issues with robustness to mesh distortions. To address\nthese concerns, we propose a novel framework, PointSAGE a mesh-independent\nnetwork that leverages the unordered, mesh-less nature of Pointcloud to learn\nthe complex fluid flow and directly predict fine simulations, completely\nneglecting mesh information. Utilizing an adaptable framework, the model\naccurately predicts the fine data across diverse point cloud sizes, regardless\nof the training dataset's dimension. We have evaluated the effectiveness of\nPointSAGE on diverse datasets in different scenarios, demonstrating notable\nresults and a significant acceleration in computational time in generating fine\nsimulations compared to standard CFD techniques.\n"", '  Simulations of turbulent flows in 3D are one of the most expensive\nsimulations in computational fluid dynamics (CFD). Many works have been written\non surrogate models to replace numerical solvers for fluid flows with faster,\nlearned, autoregressive models. However, the intricacies of turbulence in three\ndimensions necessitate training these models with very small time steps, while\ngenerating realistic flow states requires either long roll-outs with many steps\nand significant error accumulation or starting from a known, realistic flow\nstate - something we aimed to avoid in the first place. Instead, we propose to\napproach turbulent flow simulation as a generative task directly learning the\nmanifold of all possible turbulent flow states without relying on any initial\nflow state. For our experiments, we introduce a challenging 3D turbulence\ndataset of high-resolution flows and detailed vortex structures caused by\nvarious objects and derive two novel sample evaluation metrics for turbulent\nflows. On this dataset, we show that our generative model captures the\ndistribution of turbulent flows caused by unseen objects and generates\nhigh-quality, realistic samples amenable for downstream applications without\naccess to any initial state.\n']",Turbulent Flow Simulation and Modeling,Fluid Dynamics and Plasma Simulation,Fluid and Plasma Simulation and Modeling,Fluid and Plasma Simulation and Modeling
111,74,111_nl2sql_database_sql_databases,"['nl2sql', 'database', 'sql', 'databases', 'querying', 'sqlfuse', 'queries', 'schemas', 'metasql', 'schema']","['schema', 'database', 'databases', 'queries', 'query', 'text', 'columns', 'execution', 'tables', 'questions']","[""  Text-to-SQL, the process of translating natural language into Structured\nQuery Language (SQL), represents a transformative application of large language\nmodels (LLMs), potentially revolutionizing how humans interact with data. This\npaper introduces the SQL-PaLM framework, a comprehensive solution for\nunderstanding and enhancing Text-to-SQL using LLMs, using in the learning\nregimes of few-shot prompting and instruction fine-tuning. With few-shot\nprompting, we explore the effectiveness of consistency decoding with\nexecution-based error filtering. With instruction fine-tuning, we delve deep in\nunderstanding the critical paradigms that influence the performance of tuned\nLLMs. In particular, we investigate how performance can be improved through\nexpanded training data coverage and diversity, synthetic data augmentation, and\nintegrating query-specific database content. We propose a test-time selection\nmethod to further refine accuracy by integrating SQL outputs from multiple\nparadigms with execution feedback as guidance. Additionally, we tackle the\npractical challenge of navigating intricate databases with a significant number\nof tables and columns, proposing efficient techniques for accurately selecting\nrelevant database elements to enhance Text-to-SQL performance. Our holistic\napproach yields substantial advancements in Text-to-SQL, as demonstrated on two\nkey public benchmarks, Spider and BIRD. Through comprehensive ablations and\nerror analyses, we shed light on the strengths and weaknesses of our framework,\noffering valuable insights into Text-to-SQL's future work.\n"", '  Generating accurate SQL from natural language questions (text-to-SQL) is a\nlong-standing challenge due to the complexities in user question understanding,\ndatabase schema comprehension, and SQL generation. Conventional text-to-SQL\nsystems, comprising human engineering and deep neural networks, have made\nsubstantial progress. Subsequently, pre-trained language models (PLMs) have\nbeen developed and utilized for text-to-SQL tasks, achieving promising\nperformance. As modern databases become more complex, the corresponding user\nquestions also grow more challenging, causing PLMs with parameter constraints\nto produce incorrect SQL. This necessitates more sophisticated and tailored\noptimization methods, which, in turn, restricts the applications of PLM-based\nsystems. Recently, large language models (LLMs) have demonstrated significant\ncapabilities in natural language understanding as the model scale increases.\nTherefore, integrating LLM-based implementation can bring unique opportunities,\nimprovements, and solutions to text-to-SQL research. In this survey, we present\na comprehensive review of LLM-based text-to-SQL. Specifically, we propose a\nbrief overview of the technical challenges and the evolutionary process of\ntext-to-SQL. Then, we provide a detailed introduction to the datasets and\nmetrics designed to evaluate text-to-SQL systems. After that, we present a\nsystematic analysis of recent advances in LLM-based text-to-SQL. Finally, we\ndiscuss the remaining challenges in this field and propose expectations for\nfuture research directions.\n', ""  Generating accurate Structured Querying Language (SQL) is a long-standing\nproblem, especially in matching users' semantic queries with structured\ndatabases and then generating structured SQL. Existing models typically input\nqueries and database schemas into the LLM and rely on the LLM to perform\nsemantic-structure matching and generate structured SQL. However, such\nsolutions overlook the structural information within user queries and\ndatabases, which can be utilized to enhance the generation of structured SQL.\nThis oversight can lead to inaccurate or unexecutable SQL generation. To fully\nexploit the structure, we propose a structure-to-SQL framework, which leverages\nthe inherent structure information to improve the SQL generation of LLMs.\nSpecifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.\nSGU-SQL first links user queries and databases in a structure-enhanced manner.\nIt then decomposes complicated linked structures with grammar trees to guide\nthe LLM to generate the SQL step by step. Extensive experiments on two\nbenchmark datasets illustrate that SGU-SQL can outperform sixteen SQL\ngeneration baselines.\n""]",Natural Language to SQL Generation,Natural Language Processing for Text Generation and Evaluation,Natural Language Processing,Natural Language Processing
112,74,112_imagenet_recognition_clips_descriptors,"['imagenet', 'recognition', 'clips', 'descriptors', 'classification', 'visual', 'trained', 'features', 'learnable', 'datasets']","['shot', 'vision', 'prompt', 'classes', 'class', 'visual', 'tuning', 'classification', 'downstream', 'image']","['  Recent large vision-language models such as CLIP have shown remarkable\nout-of-distribution (OOD) detection and generalization performance. However,\ntheir zero-shot in-distribution (ID) accuracy is often limited for downstream\ndatasets. Recent CLIP-based fine-tuning methods such as prompt learning have\ndemonstrated significant improvements in ID classification and OOD\ngeneralization where OOD labels are available. Nonetheless, it remains unclear\nwhether the model is reliable to semantic shifts without OOD labels. In this\npaper, we aim to bridge the gap and present a comprehensive study to understand\nhow fine-tuning impact OOD detection for few-shot downstream tasks. By framing\nOOD detection as multi-modal concept matching, we establish a connection\nbetween fine-tuning methods and various OOD scores. Our results suggest that a\nproper choice of OOD scores is essential for CLIP-based fine-tuning. In\nparticular, the maximum concept matching (MCM) score provides a promising\nsolution consistently. We also show that prompt learning demonstrates the\nstate-of-the-art OOD detection performance over the zero-shot counterpart.\n', ""  Foundation models like CLIP are trained on hundreds of millions of samples\nand effortlessly generalize to new tasks and inputs. Out of the box, CLIP shows\nstellar zero-shot and few-shot capabilities on a wide range of\nout-of-distribution (OOD) benchmarks, which prior works attribute mainly to\ntoday's large and comprehensive training dataset (like LAION). However, it is\nquestionable how meaningful terms like out-of-distribution generalization are\nfor CLIP as it seems likely that web-scale datasets like LAION simply contain\nmany samples that are similar to common OOD benchmarks originally designed for\nImageNet. To test this hypothesis, we retrain CLIP on pruned LAION splits that\nreplicate ImageNet's train-test similarity with respect to common OOD\nbenchmarks. While we observe a performance drop on some benchmarks,\nsurprisingly, CLIP's overall performance remains high. This shows that high\ntrain-test similarity is insufficient to explain CLIP's OOD performance, and\nother properties of the training data must drive CLIP to learn more\ngeneralizable representations. Additionally, by pruning data points that are\ndissimilar to the OOD benchmarks, we uncover a 100M split of LAION\n($\\frac{1}{4}$th of its original size) on which CLIP can be trained to match\nits original OOD performance.\n"", ""  The fusion of vision and language has brought about a transformative shift in\ncomputer vision through the emergence of Vision-Language Models (VLMs).\nHowever, the resource-intensive nature of existing VLMs poses a significant\nchallenge. We need an accessible method for developing the next generation of\nVLMs. To address this issue, we propose Zoom-shot, a novel method for\ntransferring the zero-shot capabilities of CLIP to any pre-trained vision\nencoder. We do this by exploiting the multimodal information (i.e. text and\nimage) present in the CLIP latent space through the use of specifically\ndesigned multimodal loss functions. These loss functions are (1)\ncycle-consistency loss and (2) our novel prompt-guided knowledge distillation\nloss (PG-KD). PG-KD combines the concept of knowledge distillation with CLIP's\nzero-shot classification, to capture the interactions between text and image\nfeatures. With our multimodal losses, we train a $\\textbf{linear mapping}$\nbetween the CLIP latent space and the latent space of a pre-trained vision\nencoder, for only a $\\textbf{single epoch}$. Furthermore, Zoom-shot is entirely\nunsupervised and is trained using $\\textbf{unpaired}$ data. We test the\nzero-shot capabilities of a range of vision encoders augmented as new VLMs, on\ncoarse and fine-grained classification datasets, outperforming the previous\nstate-of-the-art in this problem domain. In our ablations, we find Zoom-shot\nallows for a trade-off between data and compute during training; and our\nstate-of-the-art results can be obtained by reducing training from 20% to 1% of\nthe ImageNet training data with 20 epochs. All code and models are available on\nGitHub.\n""]",Vision-Language Models for Image Classification,Vision-Language Models and Multimodal Learning,Multimodal Learning and Vision-Language Models,Multimodal Learning
113,72,113_safety_unsafe_safepatching_safeguards,"['safety', 'unsafe', 'safepatching', 'safeguards', 'safeguarding', 'safeguard', 'harmfulness', 'safe', 'malicious', 'risks']","['safety', 'unsafe', 'harmful', 'refusal', 'alignment', 'safe', 'helpfulness', 'prompts', 'toxic', 'content']","[""  Recent progress in large language models (LLMs) has led to their widespread\nadoption in various domains. However, these advancements have also introduced\nadditional safety risks and raised concerns regarding their detrimental impact\non already marginalized populations. Despite growing mitigation efforts to\ndevelop safety safeguards, such as supervised safety-oriented fine-tuning and\nleveraging safe reinforcement learning from human feedback, multiple concerns\nregarding the safety and ingrained biases in these models remain. Furthermore,\nprevious work has demonstrated that models optimized for safety often display\nexaggerated safety behaviors, such as a tendency to refrain from responding to\ncertain requests as a precautionary measure. As such, a clear trade-off between\nthe helpfulness and safety of these models has been documented in the\nliterature. In this paper, we further investigate the effectiveness of safety\nmeasures by evaluating models on already mitigated biases. Using the case of\nLlama 2 as an example, we illustrate how LLMs' safety responses can still\nencode harmful assumptions. To do so, we create a set of non-toxic prompts,\nwhich we then use to evaluate Llama models. Through our new taxonomy of LLMs\nresponses to users, we observe that the safety/helpfulness trade-offs are more\npronounced for certain demographic groups which can lead to quality-of-service\nharms for marginalized populations.\n"", '  Training large language models to follow instructions makes them perform\nbetter on a wide range of tasks and generally become more helpful. However, a\nperfectly helpful model will follow even the most malicious instructions and\nreadily generate harmful content. In this paper, we raise concerns over the\nsafety of models that only emphasize helpfulness, not harmlessness, in their\ninstruction-tuning. We show that several popular instruction-tuned models are\nhighly unsafe. Moreover, we show that adding just 3% safety examples (a few\nhundred demonstrations) when fine-tuning a model like LLaMA can substantially\nimprove its safety. Our safety-tuning does not make models significantly less\ncapable or helpful as measured by standard benchmarks. However, we do find\nexaggerated safety behaviours, where too much safety-tuning makes models refuse\nperfectly safe prompts if they superficially resemble unsafe ones. As a whole,\nour results illustrate trade-offs in training LLMs to be helpful and training\nthem to be safe.\n', '  Ensuring the safe alignment of large language models (LLMs) with human values\nis critical as they become integral to applications like translation and\nquestion answering. Current alignment methods struggle with dynamic user\nintentions and complex objectives, making models vulnerable to generating\nharmful content. We propose Safety Arithmetic, a training-free framework\nenhancing LLM safety across different scenarios: Base models, Supervised\nfine-tuned models (SFT), and Edited models. Safety Arithmetic involves Harm\nDirection Removal to avoid harmful content and Safety Alignment to promote safe\nresponses. Additionally, we present NoIntentEdit, a dataset highlighting edit\ninstances that could compromise model safety if used unintentionally. Our\nexperiments show that Safety Arithmetic significantly improves safety measures,\nreduces over-safety, and maintains model utility, outperforming existing\nmethods in ensuring safe content generation.\n']",Large Language Model Safety and Risk Mitigation,"Large Language Models: Safety, Security, and Ethics",Large Language Models,Large Language Models
113,72,113_safety_unsafe_safepatching_safeguards,"['safety', 'unsafe', 'safepatching', 'safeguards', 'safeguarding', 'safeguard', 'harmfulness', 'safe', 'malicious', 'risks']","['safety', 'unsafe', 'harmful', 'refusal', 'alignment', 'safe', 'helpfulness', 'prompts', 'toxic', 'content']","[""  Recent progress in large language models (LLMs) has led to their widespread\nadoption in various domains. However, these advancements have also introduced\nadditional safety risks and raised concerns regarding their detrimental impact\non already marginalized populations. Despite growing mitigation efforts to\ndevelop safety safeguards, such as supervised safety-oriented fine-tuning and\nleveraging safe reinforcement learning from human feedback, multiple concerns\nregarding the safety and ingrained biases in these models remain. Furthermore,\nprevious work has demonstrated that models optimized for safety often display\nexaggerated safety behaviors, such as a tendency to refrain from responding to\ncertain requests as a precautionary measure. As such, a clear trade-off between\nthe helpfulness and safety of these models has been documented in the\nliterature. In this paper, we further investigate the effectiveness of safety\nmeasures by evaluating models on already mitigated biases. Using the case of\nLlama 2 as an example, we illustrate how LLMs' safety responses can still\nencode harmful assumptions. To do so, we create a set of non-toxic prompts,\nwhich we then use to evaluate Llama models. Through our new taxonomy of LLMs\nresponses to users, we observe that the safety/helpfulness trade-offs are more\npronounced for certain demographic groups which can lead to quality-of-service\nharms for marginalized populations.\n"", '  Training large language models to follow instructions makes them perform\nbetter on a wide range of tasks and generally become more helpful. However, a\nperfectly helpful model will follow even the most malicious instructions and\nreadily generate harmful content. In this paper, we raise concerns over the\nsafety of models that only emphasize helpfulness, not harmlessness, in their\ninstruction-tuning. We show that several popular instruction-tuned models are\nhighly unsafe. Moreover, we show that adding just 3% safety examples (a few\nhundred demonstrations) when fine-tuning a model like LLaMA can substantially\nimprove its safety. Our safety-tuning does not make models significantly less\ncapable or helpful as measured by standard benchmarks. However, we do find\nexaggerated safety behaviours, where too much safety-tuning makes models refuse\nperfectly safe prompts if they superficially resemble unsafe ones. As a whole,\nour results illustrate trade-offs in training LLMs to be helpful and training\nthem to be safe.\n', '  Ensuring the safe alignment of large language models (LLMs) with human values\nis critical as they become integral to applications like translation and\nquestion answering. Current alignment methods struggle with dynamic user\nintentions and complex objectives, making models vulnerable to generating\nharmful content. We propose Safety Arithmetic, a training-free framework\nenhancing LLM safety across different scenarios: Base models, Supervised\nfine-tuned models (SFT), and Edited models. Safety Arithmetic involves Harm\nDirection Removal to avoid harmful content and Safety Alignment to promote safe\nresponses. Additionally, we present NoIntentEdit, a dataset highlighting edit\ninstances that could compromise model safety if used unintentionally. Our\nexperiments show that Safety Arithmetic significantly improves safety measures,\nreduces over-safety, and maintains model utility, outperforming existing\nmethods in ensuring safe content generation.\n']",Large Language Model Safety and Risk Mitigation,"Large Language Models: Safety, Security, and Ethics",Large Language Models,Large Language Models
114,72,114_drafts_draft_speculative_decoding,"['drafts', 'draft', 'speculative', 'decoding', 'memory', 'drafter', 'drafters', 'drafting', 'language', 'speedups']","['speculative', 'draft', 'decoding', 'tokens', 'autoregressive', 'drafting', 'token', 'speedup', 'parallel', 'inference']","['  Autoregressive sampling from large language models has led to\nstate-of-the-art results in several natural language tasks. However,\nautoregressive sampling generates tokens one at a time making it slow, and even\nprohibitive in certain tasks. One way to speed up sampling is\n$\\textit{speculative decoding}$: use a small model to sample a $\\textit{draft}$\n(block or sequence of tokens), and then score all tokens in the draft by the\nlarge language model in parallel. A subset of the tokens in the draft are\naccepted (and the rest rejected) based on a statistical method to guarantee\nthat the final output follows the distribution of the large model. In this\nwork, we provide a principled understanding of speculative decoding through the\nlens of optimal transport (OT) with $\\textit{membership cost}$. This framework\ncan be viewed as an extension of the well-known $\\textit{maximal-coupling}$\nproblem. This new formulation enables us to generalize the speculative decoding\nmethod to allow for a set of $k$ candidates at the token-level, which leads to\nan improved optimal membership cost. We show that the optimal draft selection\nalgorithm (transport plan) can be computed via linear programming, whose\nbest-known runtime is exponential in $k$. We then propose a valid draft\nselection algorithm whose acceptance probability is $(1-1/e)$-optimal\nmultiplicatively. Moreover, it can be computed in time almost linear with size\nof domain of a single token. Using this $new draft selection$ algorithm, we\ndevelop a new autoregressive sampling algorithm called $\\textit{SpecTr}$, which\nprovides speedup in decoding while ensuring that there is no quality\ndegradation in the decoded output. We experimentally demonstrate that for\nstate-of-the-art large language models, the proposed approach achieves a wall\nclock speedup of 2.13X, a further 1.37X speedup over speculative decoding on\nstandard benchmarks.\n', '  Speculative decoding is a prominent technique to speed up the inference of a\nlarge target language model based on predictions of an auxiliary draft model.\nWhile effective, in application-specific settings, it often involves\nfine-tuning both draft and target models to achieve high acceptance rates. As\nthe number of downstream tasks grows, these draft models add significant\ncomplexity to inference systems. We propose Speculative Streaming, a\nsingle-model speculative decoding method that fuses drafting into the target\nmodel by changing the fine-tuning objective from next token prediction to\nfuture n-gram prediction. Speculative Streaming speeds up decoding by 1.8 -\n3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and\nMeaning Representation, without sacrificing generation quality. Additionally,\nSpeculative Streaming is parameter-efficient. It achieves on-par/higher\nspeed-ups than Medusa-style architectures while using ~10000X fewer extra\nparameters, making it well-suited for resource-constrained devices.\n', ""  Speculative Decoding is a widely used technique to speed up inference for\nLarge Language Models (LLMs) without sacrificing quality. When performing\ninference, speculative decoding uses a smaller draft model to generate\nspeculative tokens and then uses the target LLM to verify those draft tokens.\nThe speedup provided by speculative decoding heavily depends on the choice of\nthe draft model. In this work, we perform a detailed study comprising over 350\nexperiments with LLaMA-65B and OPT-66B using speculative decoding and delineate\nthe factors that affect the performance gain provided by speculative decoding.\nOur experiments indicate that the performance of speculative decoding depends\nheavily on the latency of the draft model, and the draft model's capability in\nlanguage modeling does not correlate strongly with its performance in\nspeculative decoding. Based on these insights we explore a new design space for\ndraft models and design hardware-efficient draft models for speculative\ndecoding. Our newly designed draft model for LLaMA-65B can provide 111% higher\nthroughput than existing draft models and can generalize further to the LLaMA-2\nmodel family and supervised fine-tuned models.\n""]",Speculative Decoding for Efficient Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
114,72,114_drafts_draft_speculative_decoding,"['drafts', 'draft', 'speculative', 'decoding', 'memory', 'drafter', 'drafters', 'drafting', 'language', 'speedups']","['speculative', 'draft', 'decoding', 'tokens', 'autoregressive', 'drafting', 'token', 'speedup', 'parallel', 'inference']","['  Autoregressive sampling from large language models has led to\nstate-of-the-art results in several natural language tasks. However,\nautoregressive sampling generates tokens one at a time making it slow, and even\nprohibitive in certain tasks. One way to speed up sampling is\n$\\textit{speculative decoding}$: use a small model to sample a $\\textit{draft}$\n(block or sequence of tokens), and then score all tokens in the draft by the\nlarge language model in parallel. A subset of the tokens in the draft are\naccepted (and the rest rejected) based on a statistical method to guarantee\nthat the final output follows the distribution of the large model. In this\nwork, we provide a principled understanding of speculative decoding through the\nlens of optimal transport (OT) with $\\textit{membership cost}$. This framework\ncan be viewed as an extension of the well-known $\\textit{maximal-coupling}$\nproblem. This new formulation enables us to generalize the speculative decoding\nmethod to allow for a set of $k$ candidates at the token-level, which leads to\nan improved optimal membership cost. We show that the optimal draft selection\nalgorithm (transport plan) can be computed via linear programming, whose\nbest-known runtime is exponential in $k$. We then propose a valid draft\nselection algorithm whose acceptance probability is $(1-1/e)$-optimal\nmultiplicatively. Moreover, it can be computed in time almost linear with size\nof domain of a single token. Using this $new draft selection$ algorithm, we\ndevelop a new autoregressive sampling algorithm called $\\textit{SpecTr}$, which\nprovides speedup in decoding while ensuring that there is no quality\ndegradation in the decoded output. We experimentally demonstrate that for\nstate-of-the-art large language models, the proposed approach achieves a wall\nclock speedup of 2.13X, a further 1.37X speedup over speculative decoding on\nstandard benchmarks.\n', '  Speculative decoding is a prominent technique to speed up the inference of a\nlarge target language model based on predictions of an auxiliary draft model.\nWhile effective, in application-specific settings, it often involves\nfine-tuning both draft and target models to achieve high acceptance rates. As\nthe number of downstream tasks grows, these draft models add significant\ncomplexity to inference systems. We propose Speculative Streaming, a\nsingle-model speculative decoding method that fuses drafting into the target\nmodel by changing the fine-tuning objective from next token prediction to\nfuture n-gram prediction. Speculative Streaming speeds up decoding by 1.8 -\n3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and\nMeaning Representation, without sacrificing generation quality. Additionally,\nSpeculative Streaming is parameter-efficient. It achieves on-par/higher\nspeed-ups than Medusa-style architectures while using ~10000X fewer extra\nparameters, making it well-suited for resource-constrained devices.\n', ""  Speculative Decoding is a widely used technique to speed up inference for\nLarge Language Models (LLMs) without sacrificing quality. When performing\ninference, speculative decoding uses a smaller draft model to generate\nspeculative tokens and then uses the target LLM to verify those draft tokens.\nThe speedup provided by speculative decoding heavily depends on the choice of\nthe draft model. In this work, we perform a detailed study comprising over 350\nexperiments with LLaMA-65B and OPT-66B using speculative decoding and delineate\nthe factors that affect the performance gain provided by speculative decoding.\nOur experiments indicate that the performance of speculative decoding depends\nheavily on the latency of the draft model, and the draft model's capability in\nlanguage modeling does not correlate strongly with its performance in\nspeculative decoding. Based on these insights we explore a new design space for\ndraft models and design hardware-efficient draft models for speculative\ndecoding. Our newly designed draft model for LLaMA-65B can provide 111% higher\nthroughput than existing draft models and can generalize further to the LLaMA-2\nmodel family and supervised fine-tuned models.\n""]",Speculative Decoding for Efficient Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
114,72,114_drafts_draft_speculative_decoding,"['drafts', 'draft', 'speculative', 'decoding', 'memory', 'drafter', 'drafters', 'drafting', 'language', 'speedups']","['speculative', 'draft', 'decoding', 'tokens', 'autoregressive', 'drafting', 'token', 'speedup', 'parallel', 'inference']","['  Autoregressive sampling from large language models has led to\nstate-of-the-art results in several natural language tasks. However,\nautoregressive sampling generates tokens one at a time making it slow, and even\nprohibitive in certain tasks. One way to speed up sampling is\n$\\textit{speculative decoding}$: use a small model to sample a $\\textit{draft}$\n(block or sequence of tokens), and then score all tokens in the draft by the\nlarge language model in parallel. A subset of the tokens in the draft are\naccepted (and the rest rejected) based on a statistical method to guarantee\nthat the final output follows the distribution of the large model. In this\nwork, we provide a principled understanding of speculative decoding through the\nlens of optimal transport (OT) with $\\textit{membership cost}$. This framework\ncan be viewed as an extension of the well-known $\\textit{maximal-coupling}$\nproblem. This new formulation enables us to generalize the speculative decoding\nmethod to allow for a set of $k$ candidates at the token-level, which leads to\nan improved optimal membership cost. We show that the optimal draft selection\nalgorithm (transport plan) can be computed via linear programming, whose\nbest-known runtime is exponential in $k$. We then propose a valid draft\nselection algorithm whose acceptance probability is $(1-1/e)$-optimal\nmultiplicatively. Moreover, it can be computed in time almost linear with size\nof domain of a single token. Using this $new draft selection$ algorithm, we\ndevelop a new autoregressive sampling algorithm called $\\textit{SpecTr}$, which\nprovides speedup in decoding while ensuring that there is no quality\ndegradation in the decoded output. We experimentally demonstrate that for\nstate-of-the-art large language models, the proposed approach achieves a wall\nclock speedup of 2.13X, a further 1.37X speedup over speculative decoding on\nstandard benchmarks.\n', '  Speculative decoding is a prominent technique to speed up the inference of a\nlarge target language model based on predictions of an auxiliary draft model.\nWhile effective, in application-specific settings, it often involves\nfine-tuning both draft and target models to achieve high acceptance rates. As\nthe number of downstream tasks grows, these draft models add significant\ncomplexity to inference systems. We propose Speculative Streaming, a\nsingle-model speculative decoding method that fuses drafting into the target\nmodel by changing the fine-tuning objective from next token prediction to\nfuture n-gram prediction. Speculative Streaming speeds up decoding by 1.8 -\n3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and\nMeaning Representation, without sacrificing generation quality. Additionally,\nSpeculative Streaming is parameter-efficient. It achieves on-par/higher\nspeed-ups than Medusa-style architectures while using ~10000X fewer extra\nparameters, making it well-suited for resource-constrained devices.\n', ""  Speculative Decoding is a widely used technique to speed up inference for\nLarge Language Models (LLMs) without sacrificing quality. When performing\ninference, speculative decoding uses a smaller draft model to generate\nspeculative tokens and then uses the target LLM to verify those draft tokens.\nThe speedup provided by speculative decoding heavily depends on the choice of\nthe draft model. In this work, we perform a detailed study comprising over 350\nexperiments with LLaMA-65B and OPT-66B using speculative decoding and delineate\nthe factors that affect the performance gain provided by speculative decoding.\nOur experiments indicate that the performance of speculative decoding depends\nheavily on the latency of the draft model, and the draft model's capability in\nlanguage modeling does not correlate strongly with its performance in\nspeculative decoding. Based on these insights we explore a new design space for\ndraft models and design hardware-efficient draft models for speculative\ndecoding. Our newly designed draft model for LLaMA-65B can provide 111% higher\nthroughput than existing draft models and can generalize further to the LLaMA-2\nmodel family and supervised fine-tuned models.\n""]",Speculative Decoding for Efficient Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
114,72,114_drafts_draft_speculative_decoding,"['drafts', 'draft', 'speculative', 'decoding', 'memory', 'drafter', 'drafters', 'drafting', 'language', 'speedups']","['speculative', 'draft', 'decoding', 'tokens', 'autoregressive', 'drafting', 'token', 'speedup', 'parallel', 'inference']","['  Autoregressive sampling from large language models has led to\nstate-of-the-art results in several natural language tasks. However,\nautoregressive sampling generates tokens one at a time making it slow, and even\nprohibitive in certain tasks. One way to speed up sampling is\n$\\textit{speculative decoding}$: use a small model to sample a $\\textit{draft}$\n(block or sequence of tokens), and then score all tokens in the draft by the\nlarge language model in parallel. A subset of the tokens in the draft are\naccepted (and the rest rejected) based on a statistical method to guarantee\nthat the final output follows the distribution of the large model. In this\nwork, we provide a principled understanding of speculative decoding through the\nlens of optimal transport (OT) with $\\textit{membership cost}$. This framework\ncan be viewed as an extension of the well-known $\\textit{maximal-coupling}$\nproblem. This new formulation enables us to generalize the speculative decoding\nmethod to allow for a set of $k$ candidates at the token-level, which leads to\nan improved optimal membership cost. We show that the optimal draft selection\nalgorithm (transport plan) can be computed via linear programming, whose\nbest-known runtime is exponential in $k$. We then propose a valid draft\nselection algorithm whose acceptance probability is $(1-1/e)$-optimal\nmultiplicatively. Moreover, it can be computed in time almost linear with size\nof domain of a single token. Using this $new draft selection$ algorithm, we\ndevelop a new autoregressive sampling algorithm called $\\textit{SpecTr}$, which\nprovides speedup in decoding while ensuring that there is no quality\ndegradation in the decoded output. We experimentally demonstrate that for\nstate-of-the-art large language models, the proposed approach achieves a wall\nclock speedup of 2.13X, a further 1.37X speedup over speculative decoding on\nstandard benchmarks.\n', '  Speculative decoding is a prominent technique to speed up the inference of a\nlarge target language model based on predictions of an auxiliary draft model.\nWhile effective, in application-specific settings, it often involves\nfine-tuning both draft and target models to achieve high acceptance rates. As\nthe number of downstream tasks grows, these draft models add significant\ncomplexity to inference systems. We propose Speculative Streaming, a\nsingle-model speculative decoding method that fuses drafting into the target\nmodel by changing the fine-tuning objective from next token prediction to\nfuture n-gram prediction. Speculative Streaming speeds up decoding by 1.8 -\n3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and\nMeaning Representation, without sacrificing generation quality. Additionally,\nSpeculative Streaming is parameter-efficient. It achieves on-par/higher\nspeed-ups than Medusa-style architectures while using ~10000X fewer extra\nparameters, making it well-suited for resource-constrained devices.\n', ""  Speculative Decoding is a widely used technique to speed up inference for\nLarge Language Models (LLMs) without sacrificing quality. When performing\ninference, speculative decoding uses a smaller draft model to generate\nspeculative tokens and then uses the target LLM to verify those draft tokens.\nThe speedup provided by speculative decoding heavily depends on the choice of\nthe draft model. In this work, we perform a detailed study comprising over 350\nexperiments with LLaMA-65B and OPT-66B using speculative decoding and delineate\nthe factors that affect the performance gain provided by speculative decoding.\nOur experiments indicate that the performance of speculative decoding depends\nheavily on the latency of the draft model, and the draft model's capability in\nlanguage modeling does not correlate strongly with its performance in\nspeculative decoding. Based on these insights we explore a new design space for\ndraft models and design hardware-efficient draft models for speculative\ndecoding. Our newly designed draft model for LLaMA-65B can provide 111% higher\nthroughput than existing draft models and can generalize further to the LLaMA-2\nmodel family and supervised fine-tuned models.\n""]",Speculative Decoding for Efficient Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
114,72,114_drafts_draft_speculative_decoding,"['drafts', 'draft', 'speculative', 'decoding', 'memory', 'drafter', 'drafters', 'drafting', 'language', 'speedups']","['speculative', 'draft', 'decoding', 'tokens', 'autoregressive', 'drafting', 'token', 'speedup', 'parallel', 'inference']","['  Autoregressive sampling from large language models has led to\nstate-of-the-art results in several natural language tasks. However,\nautoregressive sampling generates tokens one at a time making it slow, and even\nprohibitive in certain tasks. One way to speed up sampling is\n$\\textit{speculative decoding}$: use a small model to sample a $\\textit{draft}$\n(block or sequence of tokens), and then score all tokens in the draft by the\nlarge language model in parallel. A subset of the tokens in the draft are\naccepted (and the rest rejected) based on a statistical method to guarantee\nthat the final output follows the distribution of the large model. In this\nwork, we provide a principled understanding of speculative decoding through the\nlens of optimal transport (OT) with $\\textit{membership cost}$. This framework\ncan be viewed as an extension of the well-known $\\textit{maximal-coupling}$\nproblem. This new formulation enables us to generalize the speculative decoding\nmethod to allow for a set of $k$ candidates at the token-level, which leads to\nan improved optimal membership cost. We show that the optimal draft selection\nalgorithm (transport plan) can be computed via linear programming, whose\nbest-known runtime is exponential in $k$. We then propose a valid draft\nselection algorithm whose acceptance probability is $(1-1/e)$-optimal\nmultiplicatively. Moreover, it can be computed in time almost linear with size\nof domain of a single token. Using this $new draft selection$ algorithm, we\ndevelop a new autoregressive sampling algorithm called $\\textit{SpecTr}$, which\nprovides speedup in decoding while ensuring that there is no quality\ndegradation in the decoded output. We experimentally demonstrate that for\nstate-of-the-art large language models, the proposed approach achieves a wall\nclock speedup of 2.13X, a further 1.37X speedup over speculative decoding on\nstandard benchmarks.\n', '  Speculative decoding is a prominent technique to speed up the inference of a\nlarge target language model based on predictions of an auxiliary draft model.\nWhile effective, in application-specific settings, it often involves\nfine-tuning both draft and target models to achieve high acceptance rates. As\nthe number of downstream tasks grows, these draft models add significant\ncomplexity to inference systems. We propose Speculative Streaming, a\nsingle-model speculative decoding method that fuses drafting into the target\nmodel by changing the fine-tuning objective from next token prediction to\nfuture n-gram prediction. Speculative Streaming speeds up decoding by 1.8 -\n3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and\nMeaning Representation, without sacrificing generation quality. Additionally,\nSpeculative Streaming is parameter-efficient. It achieves on-par/higher\nspeed-ups than Medusa-style architectures while using ~10000X fewer extra\nparameters, making it well-suited for resource-constrained devices.\n', ""  Speculative Decoding is a widely used technique to speed up inference for\nLarge Language Models (LLMs) without sacrificing quality. When performing\ninference, speculative decoding uses a smaller draft model to generate\nspeculative tokens and then uses the target LLM to verify those draft tokens.\nThe speedup provided by speculative decoding heavily depends on the choice of\nthe draft model. In this work, we perform a detailed study comprising over 350\nexperiments with LLaMA-65B and OPT-66B using speculative decoding and delineate\nthe factors that affect the performance gain provided by speculative decoding.\nOur experiments indicate that the performance of speculative decoding depends\nheavily on the latency of the draft model, and the draft model's capability in\nlanguage modeling does not correlate strongly with its performance in\nspeculative decoding. Based on these insights we explore a new design space for\ndraft models and design hardware-efficient draft models for speculative\ndecoding. Our newly designed draft model for LLaMA-65B can provide 111% higher\nthroughput than existing draft models and can generalize further to the LLaMA-2\nmodel family and supervised fine-tuned models.\n""]",Speculative Decoding for Efficient Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
114,72,114_drafts_draft_speculative_decoding,"['drafts', 'draft', 'speculative', 'decoding', 'memory', 'drafter', 'drafters', 'drafting', 'language', 'speedups']","['speculative', 'draft', 'decoding', 'tokens', 'autoregressive', 'drafting', 'token', 'speedup', 'parallel', 'inference']","['  Autoregressive sampling from large language models has led to\nstate-of-the-art results in several natural language tasks. However,\nautoregressive sampling generates tokens one at a time making it slow, and even\nprohibitive in certain tasks. One way to speed up sampling is\n$\\textit{speculative decoding}$: use a small model to sample a $\\textit{draft}$\n(block or sequence of tokens), and then score all tokens in the draft by the\nlarge language model in parallel. A subset of the tokens in the draft are\naccepted (and the rest rejected) based on a statistical method to guarantee\nthat the final output follows the distribution of the large model. In this\nwork, we provide a principled understanding of speculative decoding through the\nlens of optimal transport (OT) with $\\textit{membership cost}$. This framework\ncan be viewed as an extension of the well-known $\\textit{maximal-coupling}$\nproblem. This new formulation enables us to generalize the speculative decoding\nmethod to allow for a set of $k$ candidates at the token-level, which leads to\nan improved optimal membership cost. We show that the optimal draft selection\nalgorithm (transport plan) can be computed via linear programming, whose\nbest-known runtime is exponential in $k$. We then propose a valid draft\nselection algorithm whose acceptance probability is $(1-1/e)$-optimal\nmultiplicatively. Moreover, it can be computed in time almost linear with size\nof domain of a single token. Using this $new draft selection$ algorithm, we\ndevelop a new autoregressive sampling algorithm called $\\textit{SpecTr}$, which\nprovides speedup in decoding while ensuring that there is no quality\ndegradation in the decoded output. We experimentally demonstrate that for\nstate-of-the-art large language models, the proposed approach achieves a wall\nclock speedup of 2.13X, a further 1.37X speedup over speculative decoding on\nstandard benchmarks.\n', '  Speculative decoding is a prominent technique to speed up the inference of a\nlarge target language model based on predictions of an auxiliary draft model.\nWhile effective, in application-specific settings, it often involves\nfine-tuning both draft and target models to achieve high acceptance rates. As\nthe number of downstream tasks grows, these draft models add significant\ncomplexity to inference systems. We propose Speculative Streaming, a\nsingle-model speculative decoding method that fuses drafting into the target\nmodel by changing the fine-tuning objective from next token prediction to\nfuture n-gram prediction. Speculative Streaming speeds up decoding by 1.8 -\n3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and\nMeaning Representation, without sacrificing generation quality. Additionally,\nSpeculative Streaming is parameter-efficient. It achieves on-par/higher\nspeed-ups than Medusa-style architectures while using ~10000X fewer extra\nparameters, making it well-suited for resource-constrained devices.\n', ""  Speculative Decoding is a widely used technique to speed up inference for\nLarge Language Models (LLMs) without sacrificing quality. When performing\ninference, speculative decoding uses a smaller draft model to generate\nspeculative tokens and then uses the target LLM to verify those draft tokens.\nThe speedup provided by speculative decoding heavily depends on the choice of\nthe draft model. In this work, we perform a detailed study comprising over 350\nexperiments with LLaMA-65B and OPT-66B using speculative decoding and delineate\nthe factors that affect the performance gain provided by speculative decoding.\nOur experiments indicate that the performance of speculative decoding depends\nheavily on the latency of the draft model, and the draft model's capability in\nlanguage modeling does not correlate strongly with its performance in\nspeculative decoding. Based on these insights we explore a new design space for\ndraft models and design hardware-efficient draft models for speculative\ndecoding. Our newly designed draft model for LLaMA-65B can provide 111% higher\nthroughput than existing draft models and can generalize further to the LLaMA-2\nmodel family and supervised fine-tuned models.\n""]",Speculative Decoding for Efficient Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
115,71,115_embeddings_textual_nlp_similarity,"['embeddings', 'textual', 'nlp', 'similarity', 'embedding', 'semantic', 'word2vec', 'sentences', 'fasttext', 'similarities']","['sentence', 'embeddings', 'word', 'similarity', 'contrastive', 'semantic', 'sentences', 'text', 'words', 'cosine']","[""  Semantic textual similarity (STS) is a fundamental NLP task that measures the\nsemantic similarity between a pair of sentences. In order to reduce the\ninherent ambiguity posed from the sentences, a recent work called Conditional\nSTS (C-STS) has been proposed to measure the sentences' similarity conditioned\non a certain aspect. Despite the popularity of C-STS, we find that the current\nC-STS dataset suffers from various issues that could impede proper evaluation\non this task. In this paper, we reannotate the C-STS validation set and observe\nan annotator discrepancy on 55% of the instances resulting from the annotation\nerrors in the original label, ill-defined conditions, and the lack of clarity\nin the task definition. After a thorough dataset analysis, we improve the C-STS\ntask by leveraging the models' capability to understand the conditions under a\nQA task setting. With the generated answers, we present an automatic error\nidentification pipeline that is able to identify annotation errors from the\nC-STS data with over 80% F1 score. We also propose a new method that largely\nimproves the performance over baselines on the C-STS data by training the\nmodels with the answers. Finally we discuss the conditionality annotation based\non the typed-feature structure (TFS) of entity types. We show in examples that\nthe TFS is able to provide a linguistic foundation for constructing C-STS data\nwith new conditions.\n"", '  High-quality text embedding is pivotal in improving semantic textual\nsimilarity (STS) tasks, which are crucial components in Large Language Model\n(LLM) applications. However, a common challenge existing text embedding models\nface is the problem of vanishing gradients, primarily due to their reliance on\nthe cosine function in the optimization objective, which has saturation zones.\nTo address this issue, this paper proposes a novel angle-optimized text\nembedding model called AnglE. The core idea of AnglE is to introduce angle\noptimization in a complex space. This novel approach effectively mitigates the\nadverse effects of the saturation zone in the cosine function, which can impede\ngradient and hinder optimization processes. To set up a comprehensive STS\nevaluation, we experimented on existing short-text STS datasets and a newly\ncollected long-text STS dataset from GitHub Issues. Furthermore, we examine\ndomain-specific STS scenarios with limited labeled data and explore how AnglE\nworks with LLM-annotated data. Extensive experiments were conducted on various\ntasks including short-text STS, long-text STS, and domain-specific STS tasks.\nThe results show that AnglE outperforms the state-of-the-art (SOTA) STS models\nthat ignore the cosine saturation zone. These findings demonstrate the ability\nof AnglE to generate high-quality text embeddings and the usefulness of angle\noptimization in STS.\n', '  Since the introduction of BERT and RoBERTa, research on Semantic Textual\nSimilarity (STS) has made groundbreaking progress. Particularly, the adoption\nof contrastive learning has substantially elevated state-of-the-art performance\nacross various STS benchmarks. However, contrastive learning categorizes text\npairs as either semantically similar or dissimilar, failing to leverage\nfine-grained annotated information and necessitating large batch sizes to\nprevent model collapse. These constraints pose challenges for researchers\nengaged in STS tasks that require nuanced similarity levels or those with\nlimited computational resources, compelling them to explore alternatives like\nSentence-BERT. Nonetheless, Sentence-BERT tackles STS tasks from a\nclassification perspective, overlooking the progressive nature of semantic\nrelationships, which results in suboptimal performance. To bridge this gap,\nthis paper presents an innovative regression framework and proposes two simple\nyet effective loss functions: Translated ReLU and Smooth K2 Loss. Experimental\nanalyses demonstrate that our method achieves convincing performance across\nseven established STS benchmarks, especially when supplemented with\ntask-specific training data.\n']",Semantic Textual Similarity (STS),Natural Language Processing and Semantic Analysis,Natural Language Processing,Natural Language Processing
116,71,116_adversarial_backdoors_backdoor_attacks,"['adversarial', 'backdoors', 'backdoor', 'attacks', 'networks', 'vulnerabilities', 'graphs', 'graphmu', 'gnns', 'adversary']","['attacks', 'attack', 'backdoor', 'graph', 'adversarial', 'node', 'robustness', 'triggers', 'perturbations', 'nodes']","['  Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet\nthey are vulnerable to backdoor attacks that can compromise their performance\nand ethical application. The detection of these attacks is crucial for\nmaintaining the reliability and security of GNN classification tasks, but\neffective detection techniques are lacking. Following an initial investigation,\nwe observed that while graph-level explanations can offer limited insights,\ntheir effectiveness in detecting backdoor triggers is inconsistent and\nincomplete. To bridge this gap, we extract and transform secondary outputs of\nGNN explanation mechanisms, designing seven novel metrics that more effectively\ndetect backdoor attacks. Additionally, we develop an adaptive attack to\nrigorously evaluate our approach. We test our method on multiple benchmark\ndatasets and examine its efficacy against various attack models. Our results\nshow that our method can achieve high detection performance, marking a\nsignificant advancement in safeguarding GNNs against backdoor attacks.\n', '  Graph Neural Networks (GNNs) are a class of deep learning models capable of\nprocessing graph-structured data, and they have demonstrated significant\nperformance in a variety of real-world applications. Recent studies have found\nthat GNN models are vulnerable to backdoor attacks. When specific patterns\n(called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input\ndata, the backdoor embedded in the GNN models is activated, which misclassifies\nthe input data into the target class label specified by the attacker, whereas\nwhen there are no backdoor triggers in the input, the backdoor embedded in the\nGNN models is not activated, and the models work normally. Backdoor attacks are\nhighly stealthy and expose GNN models to serious security risks. Currently,\nresearch on backdoor attacks against GNNs mainly focus on tasks such as graph\nclassification and node classification, and backdoor attacks against link\nprediction tasks are rarely studied. In this paper, we propose a backdoor\nattack against the link prediction tasks based on GNNs and reveal the existence\nof such security vulnerability in GNN models, which make the backdoored GNN\nmodels to incorrectly predict unlinked two nodes as having a link relationship\nwhen a trigger appear. The method uses a single node as the trigger and poison\nselected node pairs in the training graph, and then the backdoor will be\nembedded in the GNN models through the training process. In the inference\nstage, the backdoor in the GNN models can be activated by simply linking the\ntrigger node to the two end nodes of the unlinked node pairs in the input data,\ncausing the GNN models to produce incorrect link prediction results for the\ntarget node pairs.\n', '  Graph Neural Networks (GNNs) are gaining popularity across various domains\ndue to their effectiveness in learning graph-structured data. Nevertheless,\nthey have been shown to be susceptible to backdoor poisoning attacks, which\npose serious threats to real-world applications. Meanwhile, graph reduction\ntechniques, including coarsening and sparsification, which have long been\nemployed to improve the scalability of large graph computational tasks, have\nrecently emerged as effective methods for accelerating GNN training on\nlarge-scale graphs. However, the current development and deployment of graph\nreduction techniques for large graphs overlook the potential risks of data\npoisoning attacks against GNNs. It is not yet clear how graph reduction\ninteracts with existing backdoor attacks. This paper conducts a thorough\nexamination of the robustness of graph reduction methods in scalable GNN\ntraining in the presence of state-of-the-art backdoor attacks. We performed a\ncomprehensive robustness analysis across six coarsening methods and six\nsparsification methods for graph reduction, under three GNN backdoor attacks\nagainst three GNN architectures. Our findings indicate that the effectiveness\nof graph reduction methods in mitigating attack success rates varies\nsignificantly, with some methods even exacerbating the attacks. Through\ndetailed analyses of triggers and poisoned nodes, we interpret our findings and\nenhance our understanding of how graph reduction influences robustness against\nbackdoor attacks. These results highlight the critical need for incorporating\nrobustness considerations in graph reduction for GNN training, ensuring that\nenhancements in computational efficiency do not compromise the security of GNN\nsystems.\n']",Backdoor Attacks on Graph Neural Networks,Backdoor Attacks on AI Models,Adversarial Attacks and Vulnerabilities in AI and Machine Learning,Adversarial Attacks and Vulnerabilities in AI and Machine Learning
117,70,117_proofnet_formalizations_provers_prover,"['proofnet', 'formalizations', 'provers', 'prover', 'deductive', 'formalization', 'formalized', 'verifiable', 'proofs', 'solvers']","['theorem', 'proving', 'proof', 'theorems', 'formal', 'proofs', 'geometry', 'mathematics', 'mathematical', 'prover']","['  Proof assistants like Lean have revolutionized mathematical proof\nverification, ensuring high accuracy and reliability. Although large language\nmodels (LLMs) show promise in mathematical reasoning, their advancement in\nformal theorem proving is hindered by a lack of training data. To address this\nissue, we introduce an approach to generate extensive Lean 4 proof data derived\nfrom high-school and undergraduate-level mathematical competition problems.\nThis approach involves translating natural language problems into formal\nstatements, filtering out low-quality statements, and generating proofs to\ncreate synthetic data. After fine-tuning the DeepSeekMath 7B model on this\nsynthetic dataset, which comprises 8 million formal statements with proofs, our\nmodel achieved whole-proof generation accuracies of 46.3% with 64 samples and\n52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at\n23.0% with 64 samples and a tree search reinforcement learning method at 41.0%.\nAdditionally, our model successfully proved 5 out of 148 problems in the Lean 4\nFormalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4\nfailed to prove any. These results demonstrate the potential of leveraging\nlarge-scale synthetic data to enhance theorem-proving capabilities in LLMs.\nBoth the synthetic dataset and the model will be made available to facilitate\nfurther research in this promising field.\n', '  Theorem proving is an important challenge for large language models (LLMs),\nas formal proofs can be checked rigorously by proof assistants such as Lean,\nleaving no room for hallucination. Existing LLM-based provers try to prove\ntheorems in a fully autonomous mode without human intervention. In this mode,\nthey struggle with novel and challenging theorems, for which human insights may\nbe critical. In this paper, we explore LLMs as copilots that assist humans in\nproving theorems. We introduce Lean Copilot, a framework for running LLM\ninference in Lean. It enables programmers to build various LLM-based proof\nautomation tools that integrate seamlessly into the workflow of Lean users.\nUsing Lean Copilot, we build tools for suggesting proof steps (tactic\nsuggestion), completing intermediate proof goals (proof search), and selecting\nrelevant premises (premise selection) using LLMs. Users can use our pretrained\nmodels or bring their own ones that run either locally (with or without GPUs)\nor on the cloud. Experimental results demonstrate the effectiveness of our\nmethod in assisting humans and automating theorem proving process compared to\nexisting rule-based proof automation in Lean. We open source all codes under a\npermissive MIT license to facilitate further research.\n', ""  Traditional language model-based theorem proving assumes that by training on\na sufficient amount of formal proof data, a model will learn to prove theorems.\nOur key observation is that a wealth of informal information that is not\npresent in formal proofs can be useful for learning to prove theorems. For\ninstance, humans think through steps of a proof, but this thought process is\nnot visible in the resulting code. We present Lean-STaR, a framework for\ntraining language models to produce informal thoughts prior to each step of a\nproof, thereby boosting the model's theorem-proving capabilities. Lean-STaR\nuses retrospective ground-truth tactics to generate synthetic thoughts for\ntraining the language model. At inference time, the trained model directly\ngenerates the thoughts prior to the prediction of the tactics in each proof\nstep. Building on the self-taught reasoner framework, we then apply expert\niteration to further fine-tune the model on the correct proofs it samples and\nverifies using the Lean solver. Lean-STaR achieves state-of-the-art results on\nthe miniF2F-test benchmark within the Lean theorem proving environment,\nsignificantly outperforming base models ($\\boldsymbol{43.4\\% \\rightarrow\n46.3\\%,}$ Pass@64). We also analyze the impact of the augmented thoughts on\nvarious aspects of the theorem proving process, providing insights into their\neffectiveness.\n""]",Mathematical Proof Verification with Large Language Models,"Reasoning and Problem-Solving with Logic, Language, and Graphs",Artificial Intelligence and Reasoning Systems,Intelligent Systems
118,70,118_solvers_symbolic_regression_expressions,"['solvers', 'symbolic', 'regression', 'expressions', 'formulas', 'formulagpt', 'variables', 'neural', 'discovering', 'mathematical']","['symbolic', 'expressions', 'regression', 'equations', 'mathematical', 'expression', 'equation', 'scientific', 'discovery', 'laws']","['  Formulas are the language of communication between humans and nature. It is\nan important research topic of artificial intelligence to find expressions from\nobserved data to reflect the relationship between each variable in the data,\nwhich is called a symbolic regression problem. The existing symbolic regression\nmethods directly generate expressions according to the given observation data,\nand we cannot require the algorithm to generate expressions that meet specific\nrequirements according to the known prior knowledge. For example, the\nexpression needs to contain $\\sin$ or be symmetric, and so on. Even if it can,\nit often requires very complex operations, which is very inconvenient. In this\npaper, based on multi-modal large language models, we propose MLLM-SR, a\nconversational symbolic regression method that can generate expressions that\nmeet the requirements simply by describing the requirements with natural\nlanguage instructions. By experimenting on the Nguyen dataset, we can\ndemonstrate that MLLM-SR leads the state-of-the-art baselines in fitting\nperformance. More notably, we experimentally demonstrate that MLLM-SR can well\nunderstand the prior knowledge we add to the natural language instructions.\nMoreover, the addition of prior knowledge can effectively guide MLLM-SR to\ngenerate correct expressions.\n', '  Symbolic regression (SR) is a powerful technique for discovering the\nanalytical mathematical expression from data, finding various applications in\nnatural sciences due to its good interpretability of results. However, existing\nmethods face scalability issues when dealing with complex equations involving\nmultiple variables. To address this challenge, we propose ScaleSR, a scalable\nsymbolic regression model that leverages control variables to enhance both\naccuracy and scalability. The core idea is to decompose multi-variable symbolic\nregression into a set of single-variable SR problems, which are then combined\nin a bottom-up manner. The proposed method involves a four-step process. First,\nwe learn a data generator from observed data using deep neural networks (DNNs).\nSecond, the data generator is used to generate samples for a certain variable\nby controlling the input variables. Thirdly, single-variable symbolic\nregression is applied to estimate the corresponding mathematical expression.\nLastly, we repeat steps 2 and 3 by gradually adding variables one by one until\ncompletion. We evaluate the performance of our method on multiple benchmark\ndatasets. Experimental results demonstrate that the proposed ScaleSR\nsignificantly outperforms state-of-the-art baselines in discovering\nmathematical expressions with multiple variables. Moreover, it can\nsubstantially reduce the search space for symbolic regression. The source code\nwill be made publicly available upon publication.\n', ""  Mathematical equations have been unreasonably effective in describing complex\nnatural phenomena across various scientific disciplines. However, discovering\nsuch insightful equations from data presents significant challenges due to the\nnecessity of navigating extremely high-dimensional combinatorial and nonlinear\nhypothesis spaces. Traditional methods of equation discovery, commonly known as\nsymbolic regression, largely focus on extracting equations from data alone,\noften neglecting the rich domain-specific prior knowledge that scientists\ntypically depend on. To bridge this gap, we introduce LLM-SR, a novel approach\nthat leverages the extensive scientific knowledge and robust code generation\ncapabilities of Large Language Models (LLMs) to discover scientific equations\nfrom data in an efficient manner. Specifically, LLM-SR treats equations as\nprograms with mathematical operators and combines LLMs' scientific priors with\nevolutionary search over equation programs. The LLM iteratively proposes new\nequation skeleton hypotheses, drawing from its physical understanding, which\nare then optimized against data to estimate skeleton parameters. We demonstrate\nLLM-SR's effectiveness across three diverse scientific domains, where it\ndiscovers physically accurate equations that provide significantly better fits\nto in-domain and out-of-domain data compared to the well-established symbolic\nregression baselines. Incorporating scientific prior knowledge also enables\nLLM-SR to search the equation space more efficiently than baselines. Code is\navailable at: https://github.com/deep-symbolic-mathematics/LLM-SR\n""]",Symbolic Regression Methods,Machine Learning for Data Analysis and Modeling,Machine Learning and Artificial Intelligence,Machine Learning and Intelligent Systems
119,70,119_bound_generalization_bounds_learning,"['bound', 'generalization', 'bounds', 'learning', 'optimal', 'learnability', 'learnable', 'bayes', 'complexity', 'compression']","['bounds', 'hypothesis', 'transductive', 'learner', 'agnostic', 'sample', 'realizable', 'dimension', 'generalization', 'boosting']","['  We introduce a new PAC-Bayes oracle bound for unbounded losses. This result\ncan be understood as a PAC-Bayesian version of the Cram\\\'er-Chernoff bound. The\nproof technique relies on controlling the tails of certain random variables\ninvolving the Cram\\\'er transform of the loss. We highlight several applications\nof the main theorem. First, we show that our result naturally allows exact\noptimization of the free parameter on many PAC-Bayes bounds. Second, we recover\nand generalize previous results. Finally, we show that our approach allows\nworking with richer assumptions that result in more informative and potentially\ntighter bounds. In this direction, we provide a general bound under a new\n``model-dependent bounded CGF"" assumption from which we obtain bounds based on\nparameter norms and log-Sobolev inequalities. All these bounds can be minimized\nto obtain novel posteriors.\n', '  This paper studies the truncation method from Alquier [1] to derive\nhigh-probability PAC-Bayes bounds for unbounded losses with heavy tails.\nAssuming that the $p$-th moment is bounded, the resulting bounds interpolate\nbetween a slow rate $1 / \\sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p\n\\to \\infty$ and the loss is essentially bounded. Moreover, the paper derives a\nhigh-probability PAC-Bayes bound for losses with a bounded variance. This bound\nhas an exponentially better dependence on the confidence parameter and the\ndependency measure than previous bounds in the literature. Finally, the paper\nextends all results to guarantees in expectation and single-draw PAC-Bayes. In\norder to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded\nlosses from [2] in these settings.\n', ""  We propose data-dependent uniform generalization bounds by approaching the\nproblem from a PAC-Bayesian perspective. We first apply the PAC-Bayesian\nframework on `random sets' in a rigorous way, where the training algorithm is\nassumed to output a data-dependent hypothesis set after observing the training\ndata. This approach allows us to prove data-dependent bounds, which can be\napplicable in numerous contexts. To highlight the power of our approach, we\nconsider two main applications. First, we propose a PAC-Bayesian formulation of\nthe recently developed fractal-dimension-based generalization bounds. The\nderived results are shown to be tighter and they unify the existing results\naround one simple proof technique. Second, we prove uniform bounds over the\ntrajectories of continuous Langevin dynamics and stochastic gradient Langevin\ndynamics. These results provide novel information about the generalization\nproperties of noisy algorithms.\n""]",PAC-Bayes Bounds for Learning,Machine Learning Theory and Methods,Machine Learning and Artificial Intelligence,Machine Learning and Intelligent Systems
120,70,120_explanations_networks_explainers_attention,"['explanations', 'networks', 'explainers', 'attention', 'subgraph', 'explainability', 'graphs', 'explaining', 'neural', 'explainer']","['explanations', 'explanation', 'graph', 'explainability', 'subgraph', 'subgraphs', 'explainer', 'graphs', 'explainers', 'causal']","['  Graph Neural Networks (GNNs) are effective for node classification in\ngraph-structured data, but they lack explainability, especially at the global\nlevel. Current research mainly utilizes subgraphs of the input as local\nexplanations or generates new graphs as global explanations. However, these\ngraph-based methods are limited in their ability to explain classes with\nmultiple sufficient explanations. To provide more expressive explanations, we\npropose utilizing class expressions (CEs) from the field of description logic\n(DL). Our approach explains heterogeneous graphs with different types of nodes\nusing CEs in the EL description logic. To identify the best explanation among\nmultiple candidate explanations, we employ and compare two different scoring\nfunctions: (1) For a given CE, we construct multiple graphs, have the GNN make\na prediction for each graph, and aggregate the predicted scores. (2) We score\nthe CE in terms of fidelity, i.e., we compare the predictions of the GNN to the\npredictions by the CE on a separate validation set. Instead of subgraph-based\nexplanations, we offer CE-based explanations.\n', '  Aside from graph neural networks (GNNs) attracting significant attention as a\npowerful framework revolutionizing graph representation learning, there has\nbeen an increasing demand for explaining GNN models. Although various\nexplanation methods for GNNs have been developed, most studies have focused on\ninstance-level explanations, which produce explanations tailored to a given\ngraph instance. In our study, we propose Prototype-bAsed GNN-Explainer (PAGE),\na novel model-level GNN explanation method that explains what the underlying\nGNN model has learned for graph classification by discovering\nhuman-interpretable prototype graphs. Our method produces explanations for a\ngiven class, thus being capable of offering more concise and comprehensive\nexplanations than those of instance-level explanations. First, PAGE selects\nembeddings of class-discriminative input graphs on the graph-level embedding\nspace after clustering them. Then, PAGE discovers a common subgraph pattern by\niteratively searching for high matching node tuples using node-level embeddings\nvia a prototype scoring function, thereby yielding a prototype graph as our\nexplanation. Using six graph classification datasets, we demonstrate that PAGE\nqualitatively and quantitatively outperforms the state-of-the-art model-level\nexplanation method. We also carry out systematic experimental studies by\ndemonstrating the relationship between PAGE and instance-level explanation\nmethods, the robustness of PAGE to input data scarce environments, and the\ncomputational efficiency of the proposed prototype scoring function in PAGE.\n', ""  Graph Neural Networks (GNNs) have gained considerable traction for their\ncapability to effectively process topological data, yet their interpretability\nremains a critical concern. Current interpretation methods are dominated by\npost-hoc explanations to provide a transparent and intuitive understanding of\nGNNs. However, they have limited performance in interpreting complicated\nsubgraphs and can't utilize the explanation to advance GNN predictions. On the\nother hand, transparent GNN models are proposed to capture critical subgraphs.\nWhile such methods could improve GNN predictions, they usually don't perform\nwell on explanations. Thus, it is desired for a new strategy to better couple\nGNN explanation and prediction. In this study, we have developed a novel\ninterpretable causal GNN framework that incorporates retrieval-based causal\nlearning with Graph Information Bottleneck (GIB) theory. The framework could\nsemi-parametrically retrieve crucial subgraphs detected by GIB and compress the\nexplanatory subgraphs via a causal module. The framework was demonstrated to\nconsistently outperform state-of-the-art methods, and to achieve 32.71\\% higher\nprecision on real-world explanation scenarios with diverse explanation types.\nMore importantly, the learned explanations were shown able to also improve GNN\nprediction performance.\n""]",Graph Neural Network Explainability,Explainability and Interpretability in Neural Networks,Explainable Artificial Intelligence,Artificial Intelligence and Machine Learning Interpretability and Explainability
121,70,121_denoising_inverse_inverses_diffusion,"['denoising', 'inverse', 'inverses', 'diffusion', 'regularization', 'inversion', 'denoiser', 'regularisation', 'iterative', 'priors']","['inverse', 'posterior', 'problems', 'diffusion', 'imaging', 'reconstruction', 'deblurring', 'sampling', 'inversion', 'priors']","['  Inverse problems have many applications in science and engineering. In\nComputer vision, several image restoration tasks such as inpainting,\ndeblurring, and super-resolution can be formally modeled as inverse problems.\nRecently, methods have been developed for solving inverse problems that only\nleverage a pre-trained unconditional diffusion model and do not require\nadditional task-specific training. In such methods, however, the inherent\nintractability of determining the conditional score function during the reverse\ndiffusion process poses a real challenge, leaving the methods to settle with an\napproximation instead, which affects their performance in practice. Here, we\npropose a MAP estimation framework to model the reverse conditional generation\nprocess of a continuous time diffusion model as an optimization process of the\nunderlying MAP objective, whose gradient term is tractable. In theory, the\nproposed framework can be applied to solve general inverse problems using\ngradient-based optimization methods. However, given the highly non-convex\nnature of the loss objective, finding a perfect gradient-based optimization\nalgorithm can be quite challenging, nevertheless, our framework offers several\npotential research directions. We use our proposed formulation and develop\nempirically effective algorithms for solving noiseless and noisy image\ninpainting tasks. We validate our proposed algorithms with extensive\nexperiments across diverse mask settings.\n', ""  Constructing fast samplers for unconditional diffusion and flow-matching\nmodels has received much attention recently; however, existing methods for\nsolving inverse problems, such as super-resolution, inpainting, or deblurring,\nstill require hundreds to thousands of iterative steps to obtain high-quality\nresults. We propose a plug-and-play framework for constructing efficient\nsamplers for inverse problems, requiring only pre-trained diffusion or\nflow-matching models. We present Conditional Conjugate Integrators, which\nleverage the specific form of the inverse problem to project the respective\nconditional diffusion/flow dynamics into a more amenable space for sampling.\nOur method complements popular posterior approximation methods for solving\ninverse problems using diffusion/flow models. We evaluate the proposed method's\nperformance on various linear image restoration tasks across multiple datasets,\nemploying diffusion and flow-matching models. Notably, on challenging inverse\nproblems like 4$\\times$ super-resolution on the ImageNet dataset, our method\ncan generate high-quality samples in as few as 5 conditional sampling steps and\noutperforms competing baselines requiring 20-1000 steps. Our code and models\nwill be publicly available at https://github.com/mandt-lab/CI2RM.\n"", '  Diffusion models have been recently studied as powerful generative inverse\nproblem solvers, owing to their high quality reconstructions and the ease of\ncombining existing iterative solvers. However, most works focus on solving\nsimple linear inverse problems in noiseless settings, which significantly\nunder-represents the complexity of real-world problems. In this work, we extend\ndiffusion solvers to efficiently handle general noisy (non)linear inverse\nproblems via approximation of the posterior sampling. Interestingly, the\nresulting posterior sampling scheme is a blended version of diffusion sampling\nwith the manifold constrained gradient without a strict measurement consistency\nprojection step, yielding a more desirable generative path in noisy settings\ncompared to the previous studies. Our method demonstrates that diffusion models\ncan incorporate various measurement noise statistics such as Gaussian and\nPoisson, and also efficiently handle noisy nonlinear inverse problems such as\nFourier phase retrieval and non-uniform deblurring. Code available at\nhttps://github.com/DPS2022/diffusion-posterior-sampling\n']",Inverse Problems with Diffusion Models,Diffusion Models for Generative Tasks,Diffusion Models for Generative Tasks,Diffusion Models for Generative Tasks
122,69,122_annotations_annotation_entities_entity,"['annotations', 'annotation', 'entities', 'entity', 'annotated', 'nlp', 'corpus', 'labeling', 'supervised', 'sentences']","['entity', 'entities', 'recognition', 'span', 'shot', 'annotated', 'annotation', 'extraction', 'corpus', 'types']","['  Named Entity Recognition (NER) serves as a fundamental task in natural\nlanguage understanding, bearing direct implications for web content analysis,\nsearch engines, and information retrieval systems. Fine-tuned NER models\nexhibit satisfactory performance on standard NER benchmarks. However, due to\nlimited fine-tuning data and lack of knowledge, it performs poorly on unseen\nentity recognition. As a result, the usability and reliability of NER models in\nweb-related applications are compromised. Instead, Large Language Models (LLMs)\nlike GPT-4 possess extensive external knowledge, but research indicates that\nthey lack specialty for NER tasks. Furthermore, non-public and large-scale\nweights make tuning LLMs difficult. To address these challenges, we propose a\nframework that combines small fine-tuned models with LLMs (LinkNER) and an\nuncertainty-based linking strategy called RDC that enables fine-tuned models to\ncomplement black-box LLMs, achieving better performance. We experiment with\nboth standard NER test sets and noisy social media datasets. LinkNER enhances\nNER task performance, notably surpassing SOTA models in robustness tests. We\nalso quantitatively analyze the influence of key components like uncertainty\nestimation methods, LLMs, and in-context learning on diverse NER tasks,\noffering specific web-related recommendations.\n', '  Lately, instruction-based techniques have made significant strides in\nimproving performance in few-shot learning scenarios. They achieve this by\nbridging the gap between pre-trained language models and fine-tuning for\nspecific downstream tasks. Despite these advancements, the performance of Large\nLanguage Models (LLMs) in information extraction tasks like Named Entity\nRecognition (NER), using prompts or instructions, still falls short of\nsupervised baselines. The reason for this performance gap can be attributed to\nthe fundamental disparity between NER and LLMs. NER is inherently a sequence\nlabeling task, where the model must assign entity-type labels to individual\ntokens within a sentence. In contrast, LLMs are designed as a text generation\ntask. This distinction between semantic labeling and text generation leads to\nsubpar performance. In this paper, we transform the NER task into a\ntext-generation task that can be readily adapted by LLMs. This involves\nenhancing source sentences with task-specific instructions and answer choices,\nallowing for the identification of entities and their types within natural\nlanguage. We harness the strength of LLMs by integrating supervised learning\nwithin them. The goal of this combined strategy is to boost the performance of\nLLMs in extraction tasks like NER while simultaneously addressing hallucination\nissues often observed in LLM-generated content. A novel corpus Contract NER\ncomprising seven frequently observed contract categories, encompassing named\nentities associated with 18 distinct legal entity types is released along with\nour baseline models. Our models and dataset are available to the community for\nfuture research * .\n', '  Named Entity Recognition (NER) is a well-studied problem in NLP. However,\nthere is much less focus on studying NER datasets, compared to developing new\nNER models. In this paper, we employed three simple techniques to detect\nannotation errors in the OntoNotes 5.0 corpus for English NER, which is the\nlargest available NER corpus for English. Our techniques corrected ~10% of the\nsentences in train/dev/test data. In terms of entity mentions, we corrected the\nspan and/or type of ~8% of mentions in the dataset, while\nadding/deleting/splitting/merging a few more. These are large numbers of\nchanges, considering the size of OntoNotes. We used three NER libraries to\ntrain, evaluate and compare the models trained with the original and the\nre-annotated datasets, which showed an average improvement of 1.23% in overall\nF-scores, with large (>10%) improvements for some of the entity types. While\nour annotation error detection methods are not exhaustive and there is some\nmanual annotation effort involved, they are largely language agnostic and can\nbe employed with other NER datasets, and other sequence labelling tasks.\n']",Named Entity Recognition in NLP,Natural Language Processing (NLP) Techniques and Applications,Natural Language Processing,Natural Language Processing
123,69,123_lidar_lidars_laser_radar,"['lidar', 'lidars', 'laser', 'radar', 'lasermix', 'fusion', '3d', 'deepipcv2', 'detection', 'clouds']","['lidar', 'radar', 'object', 'driving', 'camera', 'autonomous', 'fusion', 'clouds', 'perception', 'point']","['  Efficient data utilization is crucial for advancing 3D scene understanding in\nautonomous driving, where reliance on heavily human-annotated LiDAR point\nclouds challenges fully supervised methods. Addressing this, our study extends\ninto semi-supervised learning for LiDAR semantic segmentation, leveraging the\nintrinsic spatial priors of driving scenes and multi-sensor complements to\naugment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved\nframework that integrates laser beam manipulations from disparate LiDAR scans\nand incorporates LiDAR-camera correspondences to further assist data-efficient\nlearning. Our framework is tailored to enhance 3D scene consistency\nregularization by incorporating multi-modality, including 1) multi-modal\nLaserMix operation for fine-grained cross-sensor interactions; 2)\ncamera-to-LiDAR feature distillation that enhances LiDAR feature learning; and\n3) language-driven knowledge guidance generating auxiliary supervisions using\nopen-vocabulary models. The versatility of LaserMix++ enables applications\nacross LiDAR representations, establishing it as a universally applicable\nsolution. Our framework is rigorously validated through theoretical analysis\nand extensive experiments on popular driving perception datasets. Results\ndemonstrate that LaserMix++ markedly outperforms fully supervised alternatives,\nachieving comparable accuracy with five times fewer annotations and\nsignificantly improving the supervised-only baselines. This substantial\nadvancement underscores the potential of semi-supervised approaches in reducing\nthe reliance on extensive labeled data in LiDAR-based 3D scene understanding\nsystems.\n', '  LiDAR point clouds have become the most common data source in autonomous\ndriving. However, due to the sparsity of point clouds, accurate and reliable\ndetection cannot be achieved in specific scenarios. Because of their\ncomplementarity with point clouds, images are getting increasing attention.\nAlthough with some success, existing fusion methods either perform hard fusion\nor do not fuse in a direct manner. In this paper, we propose a generic 3D\ndetection framework called MMFusion, using multi-modal features. The framework\naims to achieve accurate fusion between LiDAR and images to improve 3D\ndetection in complex scenes. Our framework consists of two separate streams:\nthe LiDAR stream and the camera stream, which can be compatible with any\nsingle-modal feature extraction network. The Voxel Local Perception Module in\nthe LiDAR stream enhances local feature representation, and then the\nMulti-modal Feature Fusion Module selectively combines feature output from\ndifferent streams to achieve better fusion. Extensive experiments have shown\nthat our framework not only outperforms existing benchmarks but also improves\ntheir detection, especially for detecting cyclists and pedestrians on KITTI\nbenchmarks, with strong robustness and generalization capabilities. Hopefully,\nour work will stimulate more research into multi-modal fusion for autonomous\ndriving tasks.\n', ""  With the widespread application of Light Detection and Ranging (LiDAR)\ntechnology in fields such as autonomous driving, robot navigation, and terrain\nmapping, the importance of edge detection in LiDAR images has become\nincreasingly prominent. Traditional edge detection methods often face\nchallenges in accuracy and computational complexity when processing LiDAR\nimages. To address these issues, this study proposes an edge detection method\nfor LiDAR images based on artificial intelligence technology. This paper first\nreviews the current state of research on LiDAR technology and image edge\ndetection, introducing common edge detection algorithms and their applications\nin LiDAR image processing. Subsequently, a deep learning-based edge detection\nmodel is designed and implemented, optimizing the model training process\nthrough preprocessing and enhancement of the LiDAR image dataset. Experimental\nresults indicate that the proposed method outperforms traditional methods in\nterms of detection accuracy and computational efficiency, showing significant\npractical application value. Finally, improvement strategies are proposed for\nthe current method's shortcomings, and the improvements are validated through\nexperiments.\n""]",LiDAR-based 3D Scene Understanding and Perception,Sensor Fusion and Perception for Autonomous Driving,Autonomous Systems and Safety Assessment,Autonomous Systems and Safety Assessment
124,68,124_wifi_fingerprinting_gps_wi,"['wifi', 'fingerprinting', 'gps', 'wi', 'wireless', 'fingerprint', 'localization', 'ranging', 'sensing', 'indoor']","['localization', 'indoor', 'radio', 'channel', 'wireless', 'sensing', 'sight', 'signal', 'measurements', 'antenna']","[""  The rise of the Internet of Things (IoT) and mobile internet applications has\nspurred interest in location-based services (LBS) for commercial, military, and\nsocial applications. While the global positioning system (GPS) dominates\noutdoor localization, its efficacy wanes indoors due to signal challenges.\nIndoor localization systems leverage wireless technologies like Wi-Fi, ZigBee,\nBluetooth, UWB, selecting based on context. Received signal strength indicator\n(RSSI) technology, known for its accuracy and simplicity, is widely adopted.\nThis study employs machine learning algorithms in three phases: supervised\nregressors, supervised classifiers, and ensemble methods for RSSI-based indoor\nlocalization. Additionally, it introduces a weighted least squares technique\nand pseudo-linear solution approach to address non-linear RSSI measurement\nequations by approximating them with linear equations. An experimental testbed,\nutilizing diverse wireless technologies and anchor nodes, is designed for data\ncollection, employing IoT cloud architectures. Pre-processing involves\ninvestigating filters for data refinement before algorithm training. The study\nemploys machine learning models like linear regression, polynomial regression,\nsupport vector regression, random forest regression, and decision tree\nregressor across various wireless technologies. These models estimate the\ngeographical coordinates of a moving target node, and their performance is\nevaluated using metrics such as accuracy, root mean square errors, precision,\nrecall, sensitivity, coefficient of determinant, and the f1-score. The\nexperiment's outcomes provide insights into the effectiveness of different\nsupervised machine learning techniques in terms of localization accuracy and\nrobustness in indoor environments.\n"", '  Wi-Fi fingerprinting has emerged as the most popular approach to indoor\nlocalization. The use of ML algorithms has greatly improved the localization\nperformance of Wi-Fi fingerprinting, but its success depends on the\navailability of fingerprint databases composed of a large number of RSSIs, the\nMAC addresses of access points, and the other measurement information. However,\nmost fingerprint databases do not reflect well the time varying nature of\nelectromagnetic interferences in complicated modern indoor environment. This\ncould result in significant changes in statistical characteristics of\ntraining/validation and testing datasets, which are often constructed at\ndifferent times, and even the characteristics of the testing datasets could be\ndifferent from those of the data submitted by users during the operation of\nlocalization systems after their deployment. In this paper, we consider the\nimplications of time-varying Wi-Fi fingerprints on indoor localization from a\ndata-centric point of view and discuss the differences between static and\ndynamic databases. As a case study, we have constructed a dynamic database\ncovering three floors of the IR building of XJTLU based on RSSI measurements,\nover 44 days, and investigated the differences between static and dynamic\ndatabases in terms of statistical characteristics and localization performance.\nThe analyses based on variance calculations and Isolation Forest show the\ntemporal shifts in RSSIs, which result in a noticeable trend of the increase in\nthe localization error of a Gaussian process regression model with the maximum\nerror of 6.65 m after 14 days of training without model adjustments. The\nresults of the case study with the XJTLU dynamic database clearly demonstrate\nthe limitations of static databases and the importance of the creation and\nadoption of dynamic databases for future indoor localization research and\nreal-world deployment.\n', '  The recognition of human activities based on WiFi Channel State Information\n(CSI) enables contactless and visual privacy-preserving sensing in indoor\nenvironments. However, poor model generalization, due to varying environmental\nconditions and sensing hardware, is a well-known problem in this space. To\naddress this issue, in this work, data augmentation techniques commonly used in\nimage-based learning are applied to WiFi CSI to investigate their effects on\nmodel generalization performance in cross-scenario and cross-system settings.\nIn particular, we focus on the generalization between line-of-sight (LOS) and\nnon-line-of-sight (NLOS) through-wall scenarios, as well as on the\ngeneralization between different antenna systems, which remains under-explored.\nWe collect and make publicly available a dataset of CSI amplitude spectrograms\nof human activities. Utilizing this data, an ablation study is conducted in\nwhich activity recognition models based on the EfficientNetV2 architecture are\ntrained, allowing us to assess the effects of each augmentation on model\ngeneralization performance. The gathered results show that specific\ncombinations of simple data augmentation techniques applied to CSI amplitude\ndata can significantly improve cross-scenario and cross-system generalization.\n']",Indoor Localization using WiFi Fingerprinting,Indoor Navigation and Tracking Technologies,Wireless Technologies and Sensing Systems,Wireless Technologies and Sensing Systems
125,68,125_assessment_assessments_educational_education,"['assessment', 'assessments', 'educational', 'education', 'academic', 'ai', 'academia', 'pedagogical', 'students', 'educators']","['education', 'students', 'educators', 'educational', 'teachers', 'universities', 'academic', 'ethical', 'teaching', 'intelligence']","[""  The rise of Artificial Intelligence (AI) and Generative Artificial\nIntelligence (GenAI) in higher education necessitates assessment reform. This\nstudy addresses a critical gap by exploring student and academic staff\nexperiences with AI and GenAI tools, focusing on their familiarity and comfort\nwith current and potential future applications in learning and assessment. An\nonline survey collected data from 35 academic staff and 282 students across two\nuniversities in Vietnam and one in Singapore, examining GenAI familiarity,\nperceptions of its use in assessment marking and feedback, knowledge checking\nand participation, and experiences of GenAI text detection.\n  Descriptive statistics and reflexive thematic analysis revealed a generally\nlow familiarity with GenAI among both groups. GenAI feedback was viewed\nnegatively; however, it was viewed more positively when combined with\ninstructor feedback. Academic staff were more accepting of GenAI text detection\ntools and grade adjustments based on detection results compared to students.\nQualitative analysis identified three themes: unclear understanding of text\ndetection tools, variability in experiences with GenAI detectors, and mixed\nfeelings about GenAI's future impact on educational assessment. These findings\nhave major implications regarding the development of policies and practices for\nGenAI-enabled assessment and feedback in higher education.\n"", ""  The advancements in Generative Artificial Intelligence (GenAI) provide\nopportunities to enrich educational experiences, but also raise concerns about\nacademic integrity. Many educators have expressed anxiety and hesitation in\nintegrating GenAI in their teaching practices, and are in needs of\nrecommendations and guidance from their institutions that can support them to\nincorporate GenAI in their classrooms effectively. In order to respond to\nhigher educators' needs, this study aims to explore how universities and\neducators respond and adapt to the development of GenAI in their academic\ncontexts by analyzing academic policies and guidelines established by\ntop-ranked U.S. universities regarding the use of GenAI, especially ChatGPT.\nData sources include academic policies, statements, guidelines, and relevant\nresources provided by the top 100 universities in the U.S. Results show that\nthe majority of these universities adopt an open but cautious approach towards\nGenAI. Primary concerns lie in ethical usage, accuracy, and data privacy. Most\nuniversities actively respond and provide diverse types of resources, such as\nsyllabus templates, workshops, shared articles, and one-on-one consultations\nfocusing on a range of topics: general technical introduction, ethical\nconcerns, pedagogical applications, preventive strategies, data privacy,\nlimitations, and detective tools. The findings provide four practical\npedagogical implications for educators in teaching practices: accept its\npresence, align its use with learning objectives, evolve curriculum to prevent\nmisuse, and adopt multifaceted evaluation strategies rather than relying on AI\ndetectors. Two recommendations are suggested for educators in policy making:\nestablish discipline-specific policies and guidelines, and manage sensitive\ninformation carefully.\n"", '  Recent developments in Generative Artificial Intelligence (GenAI) have\ncreated a paradigm shift in multiple areas of society, and the use of these\ntechnologies is likely to become a defining feature of education in coming\ndecades. GenAI offers transformative pedagogical opportunities, while\nsimultaneously posing ethical and academic challenges. Against this backdrop,\nwe outline a practical, simple, and sufficiently comprehensive tool to allow\nfor the integration of GenAI tools into educational assessment: the AI\nAssessment Scale (AIAS).\n  The AIAS empowers educators to select the appropriate level of GenAI usage in\nassessments based on the learning outcomes they seek to address. The AIAS\noffers greater clarity and transparency for students and educators, provides a\nfair and equitable policy tool for institutions to work with, and offers a\nnuanced approach which embraces the opportunities of GenAI while recognising\nthat there are instances where such tools may not be pedagogically appropriate\nor necessary.\n  By adopting a practical, flexible approach that can be implemented quickly,\nthe AIAS can form a much-needed starting point to address the current\nuncertainty and anxiety regarding GenAI in education. As a secondary objective,\nwe engage with the current literature and advocate for a refocused discourse on\nGenAI tools in education, one which foregrounds how technologies can help\nsupport and enhance teaching and learning, which contrasts with the current\nfocus on GenAI as a facilitator of academic misconduct.\n']",Generative AI in Education and Assessment,Generative AI Applications and Implications,Generative Modeling and Artificial Intelligence,Generative Modeling and Artificial Intelligence
126,67,126_veracity_factuality_factdetect_credibility,"['veracity', 'factuality', 'factdetect', 'credibility', 'evidence', 'factscore', 'corpus', 'semantic', 'claims', 'factual']","['claim', 'fact', 'checking', 'claims', 'evidence', 'verification', 'veracity', 'checkers', 'factuality', 'factual']","[""  Evidence retrieval is a core part of automatic fact-checking. Prior work\nmakes simplifying assumptions in retrieval that depart from real-world use\ncases: either no access to evidence, access to evidence curated by a human\nfact-checker, or access to evidence available long after the claim has been\nmade. In this work, we present the first fully automated pipeline to check\nreal-world claims by retrieving raw evidence from the web. We restrict our\nretriever to only search documents available prior to the claim's making,\nmodeling the realistic scenario where an emerging claim needs to be checked.\nOur pipeline includes five components: claim decomposition, raw document\nretrieval, fine-grained evidence retrieval, claim-focused summarization, and\nveracity judgment. We conduct experiments on complex political claims in the\nClaimDecomp dataset and show that the aggregated evidence produced by our\npipeline improves veracity judgments. Human evaluation finds the evidence\nsummary produced by our system is reliable (it does not hallucinate\ninformation) and relevant to answering key questions about a claim, suggesting\nthat it can assist fact-checkers even when it cannot surface a complete\nevidence set.\n"", ""  Fact-checking real-world claims often requires reviewing multiple multimodal\ndocuments to assess a claim's truthfulness, which is a highly laborious and\ntime-consuming task. In this paper, we present a summarization model designed\nto generate claim-specific summaries useful for fact-checking from multimodal,\nmulti-document datasets. The model takes inputs in the form of documents,\nimages, and a claim, with the objective of assisting in fact-checking tasks. We\nintroduce a dynamic perceiver-based model that can handle inputs from multiple\nmodalities of arbitrary lengths. To train our model, we leverage a novel\nreinforcement learning-based entailment objective to generate summaries that\nprovide evidence distinguishing between different truthfulness labels. To\nassess the efficacy of our approach, we conduct experiments on both an existing\nbenchmark and a new dataset of multi-document claims that we contribute. Our\napproach outperforms the SOTA approach by 4.6% in the claim verification task\non the MOCHEG dataset and demonstrates strong performance on our new\nMulti-News-Fact-Checking dataset.\n"", '  Fact-checking is the task of verifying the factuality of a given claim by\nexamining the available evidence. High-quality evidence plays a vital role in\nenhancing fact-checking systems and facilitating the generation of explanations\nthat are understandable to humans. However, the provision of both sufficient\nand relevant evidence for explainable fact-checking systems poses a challenge.\nTo tackle this challenge, we propose a method based on a Large Language Model\nto automatically retrieve and summarize evidence from the Web. Furthermore, we\nconstruct RU22Fact, a novel multilingual explainable fact-checking dataset on\nthe Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world\nclaims, optimized evidence, and referenced explanation. To establish a baseline\nfor our dataset, we also develop an end-to-end explainable fact-checking system\nto verify claims and generate explanations. Experimental results demonstrate\nthe prospect of optimized evidence in increasing fact-checking performance and\nalso indicate the possibility of further progress in the end-to-end claim\nverification and explanation generation tasks.\n']",Automated Fact-Checking and Evidence Retrieval,Automated Information Verification and Analysis,Information Verification and Validation,Information Verification and Validation
127,67,127_hallucinations_hallucination_hallucinate_hallucinatory,"['hallucinations', 'hallucination', 'hallucinate', 'hallucinatory', 'hallucinated', 'multimodal', 'hallusionbench', 'captioning', 'lvlm', 'visual']","['hallucinations', 'hallucination', 'visual', 'object', 'hallucinatory', 'vision', 'objects', 'multimodal', 'image', 'descriptions']","['  The rapidly developing Large Vision Language Models (LVLMs) have shown\nnotable capabilities on a range of multi-modal tasks, but still face the\nhallucination phenomena where the generated texts do not align with the given\ncontexts, significantly restricting the usages of LVLMs. Most previous work\ndetects and mitigates hallucination at the coarse-grained level or requires\nexpensive annotation (e.g., labeling by proprietary models or human experts).\nTo address these issues, we propose detecting and mitigating hallucinations in\nLVLMs via fine-grained AI feedback. The basic idea is that we generate a\nsmall-size sentence-level hallucination annotation dataset by proprietary\nmodels, whereby we train a hallucination detection model which can perform\nsentence-level hallucination detection, covering primary hallucination types\n(i.e., object, attribute, and relationship). Then, we propose a\ndetect-then-rewrite pipeline to automatically construct preference dataset for\ntraining hallucination mitigating model. Furthermore, we propose\ndifferentiating the severity of hallucinations, and introducing a Hallucination\nSeverity-Aware Direct Preference Optimization (HSA-DPO) for mitigating\nhallucination in LVLMs by incorporating the severity of hallucinations into\npreference learning. Extensive experiments demonstrate the effectiveness of our\nmethod.\n', ""  Though advanced in understanding visual information with human languages,\nLarge Vision-Language Models (LVLMs) still suffer from multimodal\nhallucinations. A natural concern is that during multimodal interaction, the\ngenerated hallucinations could influence the LVLMs' subsequent generation.\nThus, we raise a question: When presented with a query relevant to the\npreviously generated hallucination, will LVLMs be misled and respond\nincorrectly, even though the ground visual information exists? To answer this,\nwe propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when\nencountering generated hallucinations, where LVLMs are required to answer\nspecific visual questions within a curated hallucinatory conversation.\nCrucially, our experiment shows that the performance of open-source LVLMs drops\nby at least $31\\%$, indicating that LVLMs are prone to accept the generated\nhallucinations and make false claims that they would not have supported without\ndistractions. We term this phenomenon Multimodal Hallucination Snowballing. To\nmitigate this, we further propose a training-free method called Residual Visual\nDecoding, where we revise the output distribution of LVLMs with the one derived\nfrom the residual visual input, providing models with direct access to the\nvisual information. Experiments show that our method can mitigate more than\n$24\\%$ of the snowballed multimodal hallucination while maintaining\ncapabilities.\n"", ""  Recent development of Large Vision-Language Models (LVLMs) has attracted\ngrowing attention within the AI landscape for its practical implementation\npotential. However, ``hallucination'', or more specifically, the misalignment\nbetween factual visual content and corresponding textual generation, poses a\nsignificant challenge of utilizing LVLMs. In this comprehensive survey, we\ndissect LVLM-related hallucinations in an attempt to establish an overview and\nfacilitate future mitigation. Our scrutiny starts with a clarification of the\nconcept of hallucinations in LVLMs, presenting a variety of hallucination\nsymptoms and highlighting the unique challenges inherent in LVLM\nhallucinations. Subsequently, we outline the benchmarks and methodologies\ntailored specifically for evaluating hallucinations unique to LVLMs.\nAdditionally, we delve into an investigation of the root causes of these\nhallucinations, encompassing insights from the training data and model\ncomponents. We also critically review existing methods for mitigating\nhallucinations. The open questions and future directions pertaining to\nhallucinations within LVLMs are discussed to conclude this survey.\n""]",Mitigating Hallucinations in Large Vision-Language Models,Mitigating Hallucinations in Large Language and Vision-Language Models,Large Language Models,Large Language Models
127,67,127_hallucinations_hallucination_hallucinate_hallucinatory,"['hallucinations', 'hallucination', 'hallucinate', 'hallucinatory', 'hallucinated', 'multimodal', 'hallusionbench', 'captioning', 'lvlm', 'visual']","['hallucinations', 'hallucination', 'visual', 'object', 'hallucinatory', 'vision', 'objects', 'multimodal', 'image', 'descriptions']","['  The rapidly developing Large Vision Language Models (LVLMs) have shown\nnotable capabilities on a range of multi-modal tasks, but still face the\nhallucination phenomena where the generated texts do not align with the given\ncontexts, significantly restricting the usages of LVLMs. Most previous work\ndetects and mitigates hallucination at the coarse-grained level or requires\nexpensive annotation (e.g., labeling by proprietary models or human experts).\nTo address these issues, we propose detecting and mitigating hallucinations in\nLVLMs via fine-grained AI feedback. The basic idea is that we generate a\nsmall-size sentence-level hallucination annotation dataset by proprietary\nmodels, whereby we train a hallucination detection model which can perform\nsentence-level hallucination detection, covering primary hallucination types\n(i.e., object, attribute, and relationship). Then, we propose a\ndetect-then-rewrite pipeline to automatically construct preference dataset for\ntraining hallucination mitigating model. Furthermore, we propose\ndifferentiating the severity of hallucinations, and introducing a Hallucination\nSeverity-Aware Direct Preference Optimization (HSA-DPO) for mitigating\nhallucination in LVLMs by incorporating the severity of hallucinations into\npreference learning. Extensive experiments demonstrate the effectiveness of our\nmethod.\n', ""  Though advanced in understanding visual information with human languages,\nLarge Vision-Language Models (LVLMs) still suffer from multimodal\nhallucinations. A natural concern is that during multimodal interaction, the\ngenerated hallucinations could influence the LVLMs' subsequent generation.\nThus, we raise a question: When presented with a query relevant to the\npreviously generated hallucination, will LVLMs be misled and respond\nincorrectly, even though the ground visual information exists? To answer this,\nwe propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when\nencountering generated hallucinations, where LVLMs are required to answer\nspecific visual questions within a curated hallucinatory conversation.\nCrucially, our experiment shows that the performance of open-source LVLMs drops\nby at least $31\\%$, indicating that LVLMs are prone to accept the generated\nhallucinations and make false claims that they would not have supported without\ndistractions. We term this phenomenon Multimodal Hallucination Snowballing. To\nmitigate this, we further propose a training-free method called Residual Visual\nDecoding, where we revise the output distribution of LVLMs with the one derived\nfrom the residual visual input, providing models with direct access to the\nvisual information. Experiments show that our method can mitigate more than\n$24\\%$ of the snowballed multimodal hallucination while maintaining\ncapabilities.\n"", ""  Recent development of Large Vision-Language Models (LVLMs) has attracted\ngrowing attention within the AI landscape for its practical implementation\npotential. However, ``hallucination'', or more specifically, the misalignment\nbetween factual visual content and corresponding textual generation, poses a\nsignificant challenge of utilizing LVLMs. In this comprehensive survey, we\ndissect LVLM-related hallucinations in an attempt to establish an overview and\nfacilitate future mitigation. Our scrutiny starts with a clarification of the\nconcept of hallucinations in LVLMs, presenting a variety of hallucination\nsymptoms and highlighting the unique challenges inherent in LVLM\nhallucinations. Subsequently, we outline the benchmarks and methodologies\ntailored specifically for evaluating hallucinations unique to LVLMs.\nAdditionally, we delve into an investigation of the root causes of these\nhallucinations, encompassing insights from the training data and model\ncomponents. We also critically review existing methods for mitigating\nhallucinations. The open questions and future directions pertaining to\nhallucinations within LVLMs are discussed to conclude this survey.\n""]",Mitigating Hallucinations in Large Vision-Language Models,Mitigating Hallucinations in Large Language and Vision-Language Models,Large Language Models,Large Language Models
128,67,128_alzheimer_dementia_neurodegenerative_neuroimaging,"['alzheimer', 'dementia', 'neurodegenerative', 'neuroimaging', 'neurodegeneration', 'mri', 'amyloid', 'predicting', 'fmri', 'mris']","['disease', 'brain', 'neuroimaging', 'progression', 'dementia', 'age', 'imaging', 'cognitive', 'amyloid', 'impairment']","[""  Neurodegeneration as measured through magnetic resonance imaging (MRI) is\nrecognized as a potential biomarker for diagnosing Alzheimer's disease (AD),\nbut is generally considered less specific than amyloid or tau based biomarkers.\nDue to a large amount of variability in brain anatomy between different\nindividuals, we hypothesize that leveraging MRI time series can help improve\nspecificity, by treating each patient as their own baseline. Here we turn to\nconditional variational autoencoders to generate individualized MRI predictions\ngiven the subject's age, disease status and one previous scan. Using serial\nimaging data from the Alzheimer's Disease Neuroimaging Initiative, we train a\nnovel architecture to build a latent space distribution which can be sampled\nfrom to generate future predictions of changing anatomy. This enables us to\nextrapolate beyond the dataset and predict MRIs up to 10 years. We evaluated\nthe model on a held-out set from ADNI and an independent dataset (from Open\nAccess Series of Imaging Studies). By comparing to several alternatives, we\nshow that our model produces more individualized images with higher resolution.\nFurther, if an individual already has a follow-up MRI, we demonstrate a usage\nof our model to compute a likelihood ratio classifier for disease status. In\npractice, the model may be able to assist in early diagnosis of AD and provide\na counterfactual baseline trajectory for treatment effect estimation.\nFurthermore, it generates a synthetic dataset that can potentially be used for\ndownstream tasks such as anomaly detection and classification.\n"", ""  Objectives: The objectives of this narrative review are to summarize the\ncurrent state of AI applications in neuroimaging for early Alzheimer's disease\n(AD) prediction and to highlight the potential of AI techniques in improving\nearly AD diagnosis, prognosis, and management.\n  Methods: We conducted a narrative review of studies using AI techniques\napplied to neuroimaging data for early AD prediction. We examined\nsingle-modality studies using structural MRI and PET imaging, as well as\nmulti-modality studies integrating multiple neuroimaging techniques and\nbiomarkers. Furthermore, they reviewed longitudinal studies that model AD\nprogression and identify individuals at risk of rapid decline.\n  Results: Single-modality studies using structural MRI and PET imaging have\ndemonstrated high accuracy in classifying AD and predicting progression from\nmild cognitive impairment (MCI) to AD. Multi-modality studies, integrating\nmultiple neuroimaging techniques and biomarkers, have shown improved\nperformance and robustness compared to single-modality approaches. Longitudinal\nstudies have highlighted the value of AI in modeling AD progression and\nidentifying individuals at risk of rapid decline. However, challenges remain in\ndata standardization, model interpretability, generalizability, clinical\nintegration, and ethical considerations.\n  Conclusion: AI techniques applied to neuroimaging data have the potential to\nimprove early AD diagnosis, prognosis, and management. Addressing challenges\nrelated to data standardization, model interpretability, generalizability,\nclinical integration, and ethical considerations is crucial for realizing the\nfull potential of AI in AD research and clinical practice. Collaborative\nefforts among researchers, clinicians, and regulatory agencies are needed to\ndevelop reliable, robust, and ethical AI tools that can benefit AD patients and\nsociety.\n"", ""  Alzheimer's is a brain disease that gets worse over time and affects memory,\nthinking, and behavior. Alzheimer's disease (AD) can be treated and managed if\nit is diagnosed early, which can slow the progression of symptoms and improve\nquality of life. In this study, we suggested using the Visual Transformer (ViT)\nand bi-LSTM to process MRI images for diagnosing Alzheimer's disease. We used\nViT to extract features from the MRI and then map them to a feature sequence.\nThen, we used Bi-LSTM sequence modeling to keep the interdependencies between\nrelated features. In addition, we evaluated the performance of the proposed\nmodel for the binary classification of AD patients using data from the\nAlzheimer's Disease Neuroimaging Initiative (ADNI). Finally, we evaluated our\nmethod against other deep learning models in the literature. The proposed\nmethod performs well in terms of accuracy, precision, F-score, and recall for\nthe diagnosis of AD.\n""]",Alzheimer's Disease Diagnosis using Neuroimaging,Neuroimaging and Deep Learning for Brain Disorder Diagnosis and Analysis,Deep Learning for Medical Imaging and Analysis,Deep Learning for Medical Imaging and Analysis
129,67,129_aspects_aspect_sentiment_sentiments,"['aspects', 'aspect', 'sentiment', 'sentiments', 'annotated', 'sentences', 'parsing', 'syntactic', 'attention', 'insightnet']","['sentiment', 'aspect', 'sentiments', 'polarity', 'syntactic', 'aspects', 'reviews', 'sentence', 'opinion', 'category']","['  Aspect-based Sentiment Analysis (ABSA) is crucial for understanding sentiment\nnuances in text, especially across diverse languages and cultures. This paper\nintroduces a novel Deep Convolutional Neural Network (CNN)-based model tailored\nfor aspect and polarity classification in Hausa movie reviews, an\nunderrepresented language in sentiment analysis research. A comprehensive Hausa\nABSA dataset is created, filling a significant gap in resource availability.\nThe dataset, preprocessed using sci-kit-learn for TF-IDF transformation,\nincludes manually annotated aspect-level feature ontology words and sentiment\npolarity assignments. The proposed model combines CNNs with attention\nmechanisms for aspect-word prediction, leveraging contextual information and\nsentiment polarities. With 91% accuracy on aspect term extraction and 92% on\nsentiment polarity classification, the model outperforms traditional machine\nmodels, offering insights into specific aspects and sentiments. This study\nadvances ABSA research, particularly in underrepresented languages, with\nimplications for cross-cultural linguistic research.\n', '  Aspect-Based Sentiment Analysis (ABSA) is a fine-grained linguistics problem\nthat entails the extraction of multifaceted aspects, opinions, and sentiments\nfrom the given text. Both standalone and compound ABSA tasks have been\nextensively used in the literature to examine the nuanced information present\nin online reviews and social media posts. Current ABSA methods often rely on\nstatic hyperparameters for attention-masking mechanisms, which can struggle\nwith context adaptation and may overlook the unique relevance of words in\nvaried situations. This leads to challenges in accurately analyzing complex\nsentences containing multiple aspects with differing sentiments. In this work,\nwe present adaptive masking methods that remove irrelevant tokens based on\ncontext to assist in Aspect Term Extraction and Aspect Sentiment Classification\nsubtasks of ABSA. We show with our experiments that the proposed methods\noutperform the baseline methods in terms of accuracy and F1 scores on four\nbenchmark online review datasets. Further, we show that the proposed methods\ncan be extended with multiple adaptations and demonstrate a qualitative\nanalysis of the proposed approach using sample text for aspect term extraction.\n', '  Aspect-Based Sentiment Analysis (ABSA) aims to identify terms or multiword\nexpressions (MWEs) on which sentiments are expressed and the sentiment\npolarities associated with them. The development of supervised models has been\nat the forefront of research in this area. However, training these models\nrequires the availability of manually annotated datasets which is both\nexpensive and time-consuming. Furthermore, the available annotated datasets are\ntailored to a specific domain, language, and text type. In this work, we\naddress this notable challenge in current state-of-the-art ABSA research. We\npropose a hybrid approach for Aspect Based Sentiment Analysis using transfer\nlearning. The approach focuses on generating weakly-supervised annotations by\nexploiting the strengths of both large language models (LLM) and traditional\nsyntactic dependencies. We utilise syntactic dependency structures of sentences\nto complement the annotations generated by LLMs, as they may overlook\ndomain-specific aspect terms. Extensive experimentation on multiple datasets is\nperformed to demonstrate the efficacy of our hybrid method for the tasks of\naspect term extraction and aspect sentiment classification.\n  Keywords: Aspect Based Sentiment Analysis, Syntactic Parsing, large language\nmodel (LLM)\n']",Aspect-Based Sentiment Analysis (ABSA),Sentiment Analysis,Business and Marketing Analytics,Business and Marketing Analytics
130,67,130_adversarial_fedsecurity_adversary_datadefense,"['adversarial', 'fedsecurity', 'adversary', 'datadefense', 'feddefender', 'malicious', 'attacks', 'federated', 'aggregators', 'adversaries']","['poisoning', 'clients', 'attacks', 'malicious', 'backdoor', 'attack', 'federated', 'defense', 'updates', 'defenses']","[""  Federated Learning (FL) is a decentralized machine learning method that\nenables participants to collaboratively train a model without sharing their\nprivate data. Despite its privacy and scalability benefits, FL is susceptible\nto backdoor attacks, where adversaries poison the local training data of a\nsubset of clients using a backdoor trigger, aiming to make the aggregated model\nproduce malicious results when the same backdoor condition is met by an\ninference-time input. Existing backdoor attacks in FL suffer from common\ndeficiencies: fixed trigger patterns and reliance on the assistance of model\npoisoning. State-of-the-art defenses based on Byzantine-robust aggregation\nexhibit a good defense performance on these attacks because of the significant\ndivergence between malicious and benign model updates. To effectively conceal\nmalicious model updates among benign ones, we propose DPOT, a backdoor attack\nstrategy in FL that dynamically constructs backdoor objectives by optimizing a\nbackdoor trigger, making backdoor data have minimal effect on model updates. We\nprovide theoretical justifications for DPOT's attacking principle and display\nexperimental results showing that DPOT, via only a data-poisoning attack,\neffectively undermines state-of-the-art defenses and outperforms existing\nbackdoor attack techniques on various datasets.\n"", ""  Federated Learning (FL) is susceptible to poisoning attacks, wherein\ncompromised clients manipulate the global model by modifying local datasets or\nsending manipulated model updates. Experienced defenders can readily detect and\nmitigate the poisoning effects of malicious behaviors using Byzantine-robust\naggregation rules. However, the exploration of poisoning attacks in scenarios\nwhere such behaviors are absent remains largely unexplored for Byzantine-robust\nFL. This paper addresses the challenging problem of poisoning Byzantine-robust\nFL by introducing catastrophic forgetting. To fill this gap, we first formally\ndefine generalization error and establish its connection to catastrophic\nforgetting, paving the way for the development of a clean-label data poisoning\nattack named BadSampler. This attack leverages only clean-label data (i.e.,\nwithout poisoned data) to poison Byzantine-robust FL and requires the adversary\nto selectively sample training data with high loss to feed model training and\nmaximize the model's generalization error. We formulate the attack as an\noptimization problem and present two elegant adversarial sampling strategies,\nTop-$\\kappa$ sampling, and meta-sampling, to approximately solve it.\nAdditionally, our formal error upper bound and time complexity analysis\ndemonstrate that our design can preserve attack utility with high efficiency.\nExtensive evaluations on two real-world datasets illustrate the effectiveness\nand performance of our proposed attacks.\n"", '  Federated Learning (FL) is an emerging distributed machine learning paradigm\nthat allows multiple clients to collaboratively train a global model without\nsharing private local data. However, FL systems are vulnerable to attacks from\nmalicious clients, who can degrade the global model performance through data\npoisoning and model poisoning. Existing defense methods typically focus on a\nsingle type of attack, such as Byzantine attacks or backdoor attacks, and are\noften ineffective against potential data poisoning attacks like label flipping\nand label shuffling. Additionally, these methods often lack accuracy and\nrobustness in detecting and handling malicious updates. To address these\nissues, we propose a novel method based on model confidence scores, which\nevaluates the uncertainty of client model updates to detect and defend against\nmalicious clients. Our approach is comprehensively effective for both model\npoisoning and data poisoning attacks and is capable of accurately identifying\nand mitigating potential malicious updates from being aggregated. Experimental\nresults demonstrate that our method significantly improves the robustness of FL\nsystems against various types of attacks, also achieving higher model accuracy\nand stability across various scenarios.\n']",Federated Learning Security and Adversarial Attacks,Federated Learning for Privacy-Preserving Machine Learning,Machine Learning and Data Privacy,Machine Learning and Data Privacy
131,67,131_parsers_treebanks_treebank_parsing,"['parsers', 'treebanks', 'treebank', 'parsing', 'parser', 'parses', 'parse', 'syntactic', 'corpus', 'grammars']","['dependency', 'syntactic', 'parsing', 'morphological', 'treebanks', 'parsers', 'constituency', 'word', 'treebank', 'languages']","[""  Many studies have shown that human languages tend to optimize for lower\ncomplexity and increased communication efficiency. Syntactic dependency\ndistance, which measures the linear distance between dependent words, is often\nconsidered a key indicator of language processing difficulty and working memory\nload. The current paper looks at diachronic trends in syntactic language change\nin both English and German, using corpora of parliamentary debates from the\nlast c. 160 years. We base our observations on five dependency parsers,\nincluding the widely used Stanford CoreNLP as well as 4 newer alternatives. Our\nanalysis of syntactic language change goes beyond linear dependency distance\nand explores 15 metrics relevant to dependency distance minimization (DDM)\nand/or based on tree graph properties, such as the tree height and degree\nvariance. Even though we have evidence that recent parsers trained on modern\ntreebanks are not heavily affected by data 'noise' such as spelling changes and\nOCR errors in our historic data, we find that results of syntactic language\nchange are sensitive to the parsers involved, which is a caution against using\na single parser for evaluating syntactic language change as done in previous\nwork. We also show that syntactic language change over the time period\ninvestigated is largely similar between English and German for the different\nmetrics explored: only 4% of cases we examine yield opposite conclusions\nregarding upwards and downtrends of syntactic metrics across German and\nEnglish. We also show that changes in syntactic measures seem to be more\nfrequent at the tails of sentence length distributions. To our best knowledge,\nours is the most comprehensive analysis of syntactic language change using\nmodern NLP technology in recent corpora of English and German.\n"", '  Parsing is the process of breaking a sentence into its grammatical components\nand identifying the syntactic structure of the sentence. The syntactically\ncorrect sentence structure is achieved by assigning grammatical labels to its\nconstituents using lexicon and syntactic rules. In linguistics, parser is\nextremely useful due to the number of different applications like name entity\nrecognition, QA systems and information extraction, etc. The two most common\ntechniques used for parsing are phrase structure and dependency Structure.\nBecause Urdu is a low-resource language, there has been little progress in\nbuilding an Urdu parser. A comparison of several parsers revealed that the\ndependency parsing approach is better suited for order-free languages such as\nUrdu. We have made significant progress in parsing Urdu, a South Asian language\nwith a complex morphology. For Urdu dependency parsing, a basic feature model\nconsisting of word location, word head, and dependency relation is employed as\na starting point, followed by more complex feature models. The dependency\ntagset is designed after careful consideration of the complex morphological\nstructure of the Urdu language, word order variation, and lexical ambiguity and\nit contains 22 tags. Our dataset comprises of sentences from news articles, and\nwe tried to include sentences of different complexity (which is quite\nchallenging), to get reliable results. All experiments are performed using\nMaltParser, exploring all 9 algorithms and classifiers. We have achieved a 70\npercent overall best-labeled accuracy (LA), as well as an 84 percent overall\nbest-unlabeled attachment score (UAS) using the Nivreeager algorithm. The\ncomparison of output data with treebank test data that has been manually parsed\nis then used to carry out error assessment and to identify the errors produced\nby the parser.\n', '  Contemporary multilingual dependency parsers can parse a diverse set of\nlanguages, but for Morphologically Rich Languages (MRLs), performance is\nattested to be lower than other languages. The key challenge is that, due to\nhigh morphological complexity and ambiguity of the space-delimited input\ntokens, the linguistic units that act as nodes in the tree are not known in\nadvance. Pre-neural dependency parsers for MRLs subscribed to the joint\nmorpho-syntactic hypothesis, stating that morphological segmentation and\nsyntactic parsing should be solved jointly, rather than as a pipeline where\nsegmentation precedes parsing. However, neural state-of-the-art parsers to date\nuse a strict pipeline. In this paper we introduce a joint neural architecture\nwhere a lattice-based representation preserving all morphological ambiguity of\nthe input is provided to an arc-factored model, which then solves the\nmorphological segmentation and syntactic parsing tasks at once. Our experiments\non Hebrew, a rich and highly ambiguous MRL, demonstrate state-of-the-art\nperformance on parsing, tagging and segmentation of the Hebrew section of UD,\nusing a single model. This proposed architecture is LLM-based and language\nagnostic, providing a solid foundation for MRLs to obtain further performance\nimprovements and bridge the gap with other languages.\n']",Syntactic Parsing and Language Change Analysis,Natural Language Processing and Linguistics,Natural Language Processing,Natural Language Processing
132,66,132_circuits_circuitsynth_analogcoder_circuitvae,"['circuits', 'circuitsynth', 'analogcoder', 'circuitvae', 'hardware', 'circuit', 'analog', 'chip', 'transistor', 'engineers']","['circuit', 'design', 'analog', 'circuits', 'chip', 'sizing', 'designs', 'chiplet', 'wirelength', 'automation']","['  Analog circuit design is a significant task in modern chip technology,\nfocusing on the selection of component types, connectivity, and parameters to\nensure proper circuit functionality. Despite advances made by Large Language\nModels (LLMs) in digital circuit design, the complexity and scarcity of data in\nanalog circuitry pose significant challenges. To mitigate these issues, we\nintroduce AnalogCoder, the first training-free LLM agent for designing analog\ncircuits through Python code generation. Firstly, AnalogCoder incorporates a\nfeedback-enhanced flow with tailored domain-specific prompts, enabling the\nautomated and self-correcting design of analog circuits with a high success\nrate. Secondly, it proposes a circuit tool library to archive successful\ndesigns as reusable modular sub-circuits, simplifying composite circuit\ncreation. Thirdly, extensive experiments on a benchmark designed to cover a\nwide range of analog circuit tasks show that AnalogCoder outperforms other\nLLM-based methods. It has successfully designed 20 circuits, 5 more than\nstandard GPT-4o. We believe AnalogCoder can significantly improve the\nlabor-intensive chip design process, enabling non-experts to design analog\ncircuits efficiently.\n', ""  The electronic design automation of analog circuits has been a longstanding\nchallenge in the integrated circuit field due to the huge design space and\ncomplex design trade-offs among circuit specifications. In the past decades,\nintensive research efforts have mostly been paid to automate the transistor\nsizing with a given circuit topology. By recognizing the graph nature of\ncircuits, this paper presents a Circuit Graph Neural Network (CktGNN) that\nsimultaneously automates the circuit topology generation and device sizing\nbased on the encoder-dependent optimization subroutines. Particularly, CktGNN\nencodes circuit graphs using a two-level GNN framework (of nested GNN) where\ncircuits are represented as combinations of subgraphs in a known subgraph\nbasis. In this way, it significantly improves design efficiency by reducing the\nnumber of subgraphs to perform message passing. Nonetheless, another critical\nroadblock to advancing learning-assisted circuit design automation is a lack of\npublic benchmarks to perform canonical assessment and reproducible research. To\ntackle the challenge, we introduce Open Circuit Benchmark (OCB), an\nopen-sourced dataset that contains $10$K distinct operational amplifiers with\ncarefully-extracted circuit specifications. OCB is also equipped with\ncommunicative circuit generation and evaluation capabilities such that it can\nhelp to generalize CktGNN to design various analog circuits by producing\ncorresponding datasets. Experiments on OCB show the extraordinary advantages of\nCktGNN through representation-based optimization frameworks over other recent\npowerful GNN baselines and human experts' manual designs. Our work paves the\nway toward a learning-based open-sourced design automation for analog circuits.\nOur source code is available at \\url{https://github.com/zehao-dong/CktGNN}.\n"", '  Analog and radio-frequency circuit design requires extensive exploration of\nboth circuit topology and parameters to meet specific design criteria like\npower consumption and bandwidth. Designers must review state-of-the-art\ntopology configurations in the literature and sweep various circuit parameters\nwithin each configuration. This design process is highly specialized and\ntime-intensive, particularly as the number of circuit parameters increases and\nthe circuit becomes more complex. Prior research has explored the potential of\nmachine learning to enhance circuit design procedures. However, these studies\nprimarily focus on simple circuits, overlooking the more practical and complex\nanalog and radio-frequency systems. A major obstacle for bearing the power of\nmachine learning in circuit design is the availability of a generic and diverse\ndataset, along with robust metrics, which are essential for thoroughly\nevaluating and improving machine learning algorithms in the analog and\nradio-frequency circuit domain. We present AICircuit, a comprehensive\nmulti-level dataset and benchmark for developing and evaluating ML algorithms\nin analog and radio-frequency circuit design. AICircuit comprises seven\ncommonly used basic circuits and two complex wireless transceiver systems\ncomposed of multiple circuit blocks, encompassing a wide array of design\nscenarios encountered in real-world applications. We extensively evaluate\nvarious ML algorithms on the dataset, revealing the potential of ML algorithms\nin learning the mapping from the design specifications to the desired circuit\nparameters.\n']",Analog Circuit Design and Automation,Design Automation and Experimentation in Engineering,Design and Experimentation in Engineering,Design and Experimentation in Engineering
133,66,133_privacy_federated_private_distributed,"['privacy', 'federated', 'private', 'distributed', 'decentralized', 'sharing', 'security', 'secret', 'cryptographic', 'secure']","['privacy', 'federated', 'protection', 'server', 'secure', 'aggregation', 'leakage', 'clients', 'private', 'updates']","['  Federated learning (FL) enables multiple clients to collaboratively learn a\nshared model without sharing their individual data. Concerns about utility,\nprivacy, and training efficiency in FL have garnered significant research\nattention. Differential privacy has emerged as a prevalent technique in FL,\nsafeguarding the privacy of individual user data while impacting utility and\ntraining efficiency. Within Differential Privacy Federated Learning (DPFL),\nprevious studies have primarily focused on the utility-privacy trade-off,\nneglecting training efficiency, which is crucial for timely completion.\nMoreover, differential privacy achieves privacy by introducing controlled\nrandomness (noise) on selected clients in each communication round. Previous\nwork has mainly examined the impact of noise level ($\\sigma$) and communication\nrounds ($T$) on the privacy-utility dynamic, overlooking other influential\nfactors like the sample ratio ($q$, the proportion of selected clients). This\npaper systematically formulates an efficiency-constrained utility-privacy\nbi-objective optimization problem in DPFL, focusing on $\\sigma$, $T$, and $q$.\nWe provide a comprehensive theoretical analysis, yielding analytical solutions\nfor the Pareto front. Extensive empirical experiments verify the validity and\nefficacy of our analysis, offering valuable guidance for low-cost parameter\ndesign in DPFL.\n', '  In recent years, privacy and security concerns in machine learning have\npromoted trusted federated learning to the forefront of research. Differential\nprivacy has emerged as the de facto standard for privacy protection in\nfederated learning due to its rigorous mathematical foundation and provable\nguarantee. Despite extensive research on algorithms that incorporate\ndifferential privacy within federated learning, there remains an evident\ndeficiency in systematic reviews that categorize and synthesize these studies.\n  Our work presents a systematic overview of the differentially private\nfederated learning. Existing taxonomies have not adequately considered objects\nand level of privacy protection provided by various differential privacy models\nin federated learning. To rectify this gap, we propose a new taxonomy of\ndifferentially private federated learning based on definition and guarantee of\nvarious differential privacy models and federated scenarios. Our classification\nallows for a clear delineation of the protected objects across various\ndifferential privacy models and their respective neighborhood levels within\nfederated learning environments. Furthermore, we explore the applications of\ndifferential privacy in federated learning scenarios. Our work provide valuable\ninsights into privacy-preserving federated learning and suggest practical\ndirections for future research.\n', ""  Federated Learning (FL) is an emerging paradigm that holds great promise for\nprivacy-preserving machine learning using distributed data. To enhance privacy,\nFL can be combined with Differential Privacy (DP), which involves adding\nGaussian noise to the model weights. However, FL faces a significant challenge\nin terms of large communication overhead when transmitting these model weights.\nTo address this issue, quantization is commonly employed. Nevertheless, the\npresence of quantized Gaussian noise introduces complexities in understanding\nprivacy protection. This research paper investigates the impact of quantization\non privacy in FL systems. We examine the privacy guarantees of quantized\nGaussian mechanisms using R\\'enyi Differential Privacy (RDP). By deriving the\nprivacy budget of quantized Gaussian mechanisms, we demonstrate that lower\nquantization bit levels provide improved privacy protection. To validate our\ntheoretical findings, we employ Membership Inference Attacks (MIA), which gauge\nthe accuracy of privacy leakage. The numerical results align with our\ntheoretical analysis, confirming that quantization can indeed enhance privacy\nprotection. This study not only enhances our understanding of the correlation\nbetween privacy and communication in FL but also underscores the advantages of\nquantization in preserving privacy.\n""]",Differential Privacy in Federated Learning,Federated Learning for Privacy-Preserving Machine Learning,Machine Learning and Data Privacy,Machine Learning and Data Privacy
134,66,134_memory_attention_cachegen_caches,"['memory', 'attention', 'cachegen', 'caches', 'cache', 'caching', 'bottlenecks', 'cacheblend', 'bottleneck', 'decoding']","['cache', 'attention', 'memory', 'tokens', 'eviction', 'heads', 'caches', 'inference', 'key', 'value']","['  The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy.\n', '  In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusin on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques achieving up to a 20.5 absolute accuracy improvement on\nTREC.\n', '  How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.\n']",Optimizing Cache Memory for Large Language Models,Optimization and Efficiency of Large Language Models,Large Language Models,Large Language Models
134,66,134_memory_attention_cachegen_caches,"['memory', 'attention', 'cachegen', 'caches', 'cache', 'caching', 'bottlenecks', 'cacheblend', 'bottleneck', 'decoding']","['cache', 'attention', 'memory', 'tokens', 'eviction', 'heads', 'caches', 'inference', 'key', 'value']","['  The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy.\n', '  In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusin on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques achieving up to a 20.5 absolute accuracy improvement on\nTREC.\n', '  How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.\n']",Optimizing Cache Memory for Large Language Models,Optimization and Efficiency of Large Language Models,Large Language Models,Large Language Models
135,65,135_tutoring_students_learners_tracing,"['tutoring', 'students', 'learners', 'tracing', 'learning', 'student', 'knowledge', 'assessment', 'classroom', 'learner']","['students', 'tracing', 'student', 'educational', 'education', 'course', 'cognitive', 'mastery', 'diagnosis', 'courses']","[""  Modern online education has the capacity to provide intelligent educational\nservices by automatically analyzing substantial amounts of student behavioral\ndata. Knowledge Tracing (KT) is one of the fundamental tasks for student\nbehavioral data analysis, aiming to monitor students' evolving knowledge state\nduring their problem-solving process. In recent years, a substantial number of\nstudies have concentrated on this rapidly growing field, significantly\ncontributing to its advancements. In this survey, we will conduct a thorough\ninvestigation of these progressions. Firstly, we present three types of\nfundamental KT models with distinct technical routes. Subsequently, we review\nextensive variants of the fundamental KT models that consider more stringent\nlearning assumptions. Moreover, the development of KT cannot be separated from\nits applications, thereby we present typical KT applications in various\nscenarios. To facilitate the work of researchers and practitioners in this\nfield, we have developed two open-source algorithm libraries: EduData that\nenables the download and preprocessing of KT-related datasets, and EduKTM that\nprovides an extensible and unified implementation of existing mainstream KT\nmodels. Finally, we discuss potential directions for future research in this\nrapidly growing field. We hope that the current survey will assist both\nresearchers and practitioners in fostering the development of KT, thereby\nbenefiting a broader range of students.\n"", ""  Knowledge Tracing (KT) is a critical task in online education systems, aiming\nto monitor students' knowledge states throughout a learning period. Common KT\napproaches involve predicting the probability of a student correctly answering\nthe next question based on their exercise history. However, these methods often\nsuffer from performance degradation when faced with the scarcity of student\ninteractions in new education systems. To address this, we leverage student\ninteractions from existing education systems to mitigate performance\ndegradation caused by limited training data. Nevertheless, these interactions\nexhibit significant differences since they are derived from different education\nsystems. To address this issue, we propose a domain generalization approach for\nknowledge tracing, where existing education systems are considered source\ndomains, and new education systems with limited data are considered target\ndomains. Additionally, we design a domain-generalizable knowledge tracing\nframework (DGKT) that can be applied to any KT model. Specifically, we present\na concept aggregation approach designed to reduce conceptual disparities within\nsequences of student interactions from diverse domains. To further mitigate\ndomain discrepancies, we introduce a novel normalization module called Sequence\nInstance Normalization (SeqIN). Moreover, to fully leverage exercise\ninformation, we propose a new knowledge tracing model tailored for the domain\ngeneralization KT task, named Domain-Generalizable Relation-based Knowledge\nTracing (DGRKT). Extensive experiments across five benchmark datasets\ndemonstrate that the proposed method performs well despite limited training\ndata.\n"", ""  Knowledge tracing (KT), aiming to mine students' mastery of knowledge by\ntheir exercise records and predict their performance on future test questions,\nis a critical task in educational assessment. While researchers achieved\ntremendous success with the rapid development of deep learning techniques,\ncurrent knowledge tracing tasks fall into the cracks from real-world teaching\nscenarios. Relying heavily on extensive student data and solely predicting\nnumerical performances differs from the settings where teachers assess\nstudents' knowledge state from limited practices and provide explanatory\nfeedback. To fill this gap, we explore a new task formulation: Explainable\nFew-shot Knowledge Tracing. By leveraging the powerful reasoning and generation\nabilities of large language models (LLMs), we then propose a cognition-guided\nframework that can track the student knowledge from a few student records while\nproviding natural language explanations. Experimental results from three widely\nused datasets show that LLMs can perform comparable or superior to competitive\ndeep knowledge tracing methods. We also discuss potential directions and call\nfor future improvements in relevant topics.\n""]",Knowledge Tracing in Online Education,Artificial Intelligence in Education,Artificial Intelligence in Education,Artificial Intelligence in Education
136,64,136_ontologies_ontology_ontological_semantic,"['ontologies', 'ontology', 'ontological', 'semantic', 'ontologically', 'owl', 'rdf', 'owl2vec', 'entities', 'semantically']","['ontology', 'ontologies', 'ontological', 'axioms', 'matching', 'logic', 'experts', 'knowledge', 'interoperability', 'conceptual']","['  Ontology alignment, a critical process in the Semantic Web for detecting\nrelationships between different ontologies, has traditionally focused on\nidentifying so-called ""simple"" 1-to-1 relationships through class labels and\nproperties comparison. The more practically useful exploration of more complex\nalignments remains a hard problem to automate, and as such is largely\nunderexplored, i.e. in application practice it is usually done manually by\nontology and domain experts. Recently, the surge in Natural Language Processing\n(NLP) capabilities, driven by advancements in Large Language Models (LLMs),\npresents new opportunities for enhancing ontology engineering practices,\nincluding ontology alignment tasks. This paper investigates the application of\nLLM technologies to tackle the complex ontology alignment challenge. Leveraging\na prompt-based approach and integrating rich ontology content so-called modules\nour work constitutes a significant advance towards automating the complex\nalignment task.\n', '  Ontologies are widely used for representing domain knowledge and meta data,\nplaying an increasingly important role in Information Systems, the Semantic\nWeb, Bioinformatics and many other domains. However, logical reasoning that\nontologies can directly support are quite limited in learning, approximation\nand prediction. One straightforward solution is to integrate statistical\nanalysis and machine learning. To this end, automatically learning vector\nrepresentation for knowledge of an ontology i.e., ontology embedding has been\nwidely investigated in recent years. Numerous papers have been published on\nontology embedding, but a lack of systematic reviews hinders researchers from\ngaining a comprehensive understanding of this field. To bridge this gap, we\nwrite this survey paper, which first introduces different kinds of semantics of\nontologies, and formally defines ontology embedding from the perspectives of\nboth mathematics and machine learning, as well as its property of faithfulness.\nBased on this, it systematically categorises and analyses a relatively complete\nset of over 80 papers, according to the ontologies and semantics that they aim\nat, and their technical solutions including geometric modeling, sequence\nmodeling and graph propagation. This survey also introduces the applications of\nontology embedding in ontology engineering, machine learning augmentation and\nlife sciences, presents a new library mOWL, and discusses the challenges and\nfuture directions.\n', '  Ontologies provide formal representation of knowledge shared within Semantic\nWeb applications. Ontology learning involves the construction of ontologies\nfrom a given corpus. In the past years, ontology learning has traversed through\nshallow learning and deep learning methodologies, each offering distinct\nadvantages and limitations in the quest for knowledge extraction and\nrepresentation. A new trend of these approaches is relying on large language\nmodels (LLMs) to enhance ontology learning. This paper gives a review in\napproaches and challenges of ontology learning. It analyzes the methodologies\nand limitations of shallow-learning-based and deep-learning-based techniques\nfor ontology learning, and provides comprehensive knowledge for the frontier\nwork of using LLMs to enhance ontology learning. In addition, it proposes\nseveral noteworthy future directions for further exploration into the\nintegration of LLMs with ontology learning tasks.\n']",Ontology Alignment and Learning with Semantic Web Technologies,Entity Alignment and Ontology Learning with Semantic Technologies,Entity Understanding and Semantic Knowledge Integration,Entity Understanding and Semantic Knowledge Integration
137,63,137_topics_topicgpt_topic_topical,"['topics', 'topicgpt', 'topic', 'topical', 'contextualized', 'embeddings', 'corpus', 'semantic', 'keywords', 'unsupervised']","['topic', 'topics', 'modeling', 'keywords', 'documents', 'words', 'document', 'word', 'corpus', 'coherence']","['  Topic modelling is fundamentally a soft clustering problem (of known objects\n-- documents, over unknown clusters -- topics). That is, the task is\nincorrectly posed. In particular, the topic models are unstable and incomplete.\nAll this leads to the fact that the process of finding a good topic model\n(repeated hyperparameter selection, model training, and topic quality\nassessment) can be particularly long and labor-intensive. We aim to simplify\nthe process, to make it more deterministic and provable. To this end, we\npresent a method for iterative training of a topic model. The essence of the\nmethod is that a series of related topic models are trained so that each\nsubsequent model is at least as good as the previous one, i.e., that it retains\nall the good topics found earlier. The connection between the models is\nachieved by additive regularization. The result of this iterative training is\nthe last topic model in the series, which we call the iteratively updated\nadditively regularized topic model (ITAR). Experiments conducted on several\ncollections of natural language texts show that the proposed ITAR model\nperforms better than other popular topic models (LDA, ARTM, BERTopic), its\ntopics are diverse, and its perplexity (ability to ""explain"" the underlying\ndata) is moderate.\n', ""  Large language models (LLMs) with their strong zero-shot topic extraction\ncapabilities offer an alternative to probabilistic topic modelling and\nclosed-set topic classification approaches. As zero-shot topic extractors, LLMs\nare expected to understand human instructions to generate relevant and\nnon-hallucinated topics based on the given documents. However, LLM-based topic\nmodelling approaches often face difficulties in generating topics with\nadherence to granularity as specified in human instructions, often resulting in\nmany near-duplicate topics. Furthermore, methods for addressing hallucinated\ntopics generated by LLMs have not yet been investigated. In this paper, we\nfocus on addressing the issues of topic granularity and hallucinations for\nbetter LLM-based topic modelling. To this end, we introduce a novel approach\nthat leverages Direct Preference Optimisation (DPO) to fine-tune open-source\nLLMs, such as Mistral-7B. Our approach does not rely on traditional human\nannotation to rank preferred answers but employs a reconstruction pipeline to\nmodify raw topics generated by LLMs, thus enabling a fast and efficient\ntraining and inference framework. Comparative experiments show that our\nfine-tuning approach not only significantly improves the LLM's capability to\nproduce more coherent, relevant, and precise topics, but also reduces the\nnumber of hallucinated topics.\n"", ""  Topic models are valuable for understanding extensive document collections,\nbut they don't always identify the most relevant topics. Classical\nprobabilistic and anchor-based topic models offer interactive versions that\nallow users to guide the models towards more pertinent topics. However, such\ninteractive features have been lacking in neural topic models. To correct this\nlacuna, we introduce a user-friendly interaction for neural topic models. This\ninteraction permits users to assign a word label to a topic, leading to an\nupdate in the topic model where the words in the topic become closely aligned\nwith the given label. Our approach encompasses two distinct kinds of neural\ntopic models. The first includes models where topic embeddings are trainable\nand evolve during the training process. The second kind involves models where\ntopic embeddings are integrated post-training, offering a different approach to\ntopic refinement. To facilitate user interaction with these neural topic\nmodels, we have developed an interactive interface. This interface enables\nusers to engage with and re-label topics as desired. We evaluate our method\nthrough a human study, where users can relabel topics to find relevant\ndocuments. Using our method, user labeling improves document rank scores,\nhelping to find more relevant documents to a given query when compared to no\nuser labeling.\n""]",Topic Modelling and Extraction Methods,Information Extraction from Text,Information Extraction,Information Extraction
138,63,138_reservoir_reservoirs_neural_rnn,"['reservoir', 'reservoirs', 'neural', 'rnn', 'lstm', 'chaotic', 'forecasting', 'predicting', 'computing', 'memory']","['reservoir', 'chaotic', 'dynamical', 'computing', 'reservoirs', 'dynamics', 'attractor', 'series', 'chaos', 'recurrent']","['  A reservoir computer is a type of dynamical system arranged to do\ncomputation. Typically, a reservoir computer is constructed by connecting a\nlarge number of nonlinear nodes in a network that includes recurrent\nconnections. In order to achieve accurate results, the reservoir usually\ncontains hundreds to thousands of nodes. This high dimensionality makes it\ndifficult to analyze the reservoir computer using tools from dynamical systems\ntheory. Additionally, the need to create and connect large numbers of nonlinear\nnodes makes it difficult to design and build analog reservoir computers that\ncan be faster and consume less power than digital reservoir computers. We\ndemonstrate here that a reservoir computer may be divided into two parts; a\nsmall set of nonlinear nodes (the reservoir), and a separate set of\ntime-shifted reservoir output signals. The time-shifted output signals serve to\nincrease the rank and memory of the reservoir computer, and the set of\nnonlinear nodes may create an embedding of the input dynamical system. We use\nthis time-shifting technique to obtain excellent performance from an\nopto-electronic delay-based reservoir computer with only a small number of\nvirtual nodes. Because only a few nonlinear nodes are required, construction of\na reservoir computer becomes much easier, and delay-based reservoir computers\ncan operate at much higher speeds.\n', '  Photonic reservoir computing has been successfully utilized in time-series\nprediction as the need for hardware implementations has increased. Prediction\nof chaotic time series remains a significant challenge, an area where the\nconventional reservoir computing framework encounters limitations of prediction\naccuracy. We introduce an attention mechanism to the reservoir computing model\nin the output stage. This attention layer is designed to prioritize distinct\nfeatures and temporal sequences, thereby substantially enhancing the prediction\naccuracy. Our results show that a photonic reservoir computer enhanced with the\nattention mechanism exhibits improved prediction capabilities for smaller\nreservoirs. These advancements highlight the transformative possibilities of\nreservoir computing for practical applications where accurate prediction of\nchaotic time series is crucial.\n', '  Reservoir computing is a form of machine learning that utilizes nonlinear\ndynamical systems to perform complex tasks in a cost-effective manner when\ncompared to typical neural networks. Many recent advancements in reservoir\ncomputing, in particular quantum reservoir computing, make use of reservoirs\nthat are inherently stochastic. However, the theoretical justification for\nusing these systems has not yet been well established. In this paper, we\ninvestigate the universality of stochastic reservoir computers, in which we use\na stochastic system for reservoir computing using the probabilities of each\nreservoir state as the readout instead of the states themselves. In stochastic\nreservoir computing, the number of distinct states of the entire reservoir\ncomputer can potentially scale exponentially with the size of the reservoir\nhardware, offering the advantage of compact device size. We prove that classes\nof stochastic echo state networks, and therefore the class of all stochastic\nreservoir computers, are universal approximating classes. We also investigate\nthe performance of two practical examples of stochastic reservoir computers in\nclassification and chaotic time series prediction. While shot noise is a\nlimiting factor in the performance of stochastic reservoir computing, we show\nsignificantly improved performance compared to a deterministic reservoir\ncomputer with similar hardware in cases where the effects of noise are small.\n']",Reservoir Computing and Chaotic Time Series Prediction,Advanced Computing Methods for Environmental Prediction and Modeling,Environmental Modeling and Prediction using Advanced Computing and AI,Environmental Modeling and Prediction using Advanced Computing and AI
139,62,139_classifiers_classifier_classification_classifications,"['classifiers', 'classifier', 'classification', 'classifications', 'imbalance', 'imbalanced', 'svm', 'roc', 'accuracy', 'classes']","['imbalanced', 'class', 'classifiers', 'imbalance', 'classification', 'classifier', 'binary', 'multiclass', 'nearest', 'neighbors']","['  Class imbalance remains a significant challenge in machine learning,\nparticularly for tabular data classification tasks. While Gradient Boosting\nDecision Trees (GBDT) models have proven highly effective for such tasks, their\nperformance can be compromised when dealing with imbalanced datasets. This\npaper presents the first comprehensive study on adapting class-balanced loss\nfunctions to three GBDT algorithms across various tabular classification tasks,\nincluding binary, multi-class, and multi-label classification. We conduct\nextensive experiments on multiple datasets to evaluate the impact of\nclass-balanced losses on different GBDT models, establishing a valuable\nbenchmark. Our results demonstrate the potential of class-balanced loss\nfunctions to enhance GBDT performance on imbalanced datasets, offering a robust\napproach for practitioners facing class imbalance challenges in real-world\napplications. Additionally, we introduce a Python package that facilitates the\nintegration of class-balanced loss functions into GBDT workflows, making these\nadvanced techniques accessible to a wider audience.\n', '  Although binary classification is a well-studied problem in computer vision,\ntraining reliable classifiers under severe class imbalance remains a\nchallenging problem. Recent work has proposed techniques that mitigate the\neffects of training under imbalance by modifying the loss functions or\noptimization methods. While this work has led to significant improvements in\nthe overall accuracy in the multi-class case, we observe that slight changes in\nhyperparameter values of these methods can result in highly variable\nperformance in terms of Receiver Operating Characteristic (ROC) curves on\nbinary problems with severe imbalance. To reduce the sensitivity to\nhyperparameter choices and train more general models, we propose training over\na family of loss functions, instead of a single loss function. We develop a\nmethod for applying Loss Conditional Training (LCT) to an imbalanced\nclassification problem. Extensive experiment results, on both CIFAR and Kaggle\ncompetition datasets, show that our method improves model performance and is\nmore robust to hyperparameter choices. Code is available at\nhttps://github.com/klieberman/roc_lct.\n', ""  We show that established performance metrics in binary classification, such\nas the F-score, the Jaccard similarity coefficient or Matthews' correlation\ncoefficient (MCC), are not robust to class imbalance in the sense that if the\nproportion of the minority class tends to $0$, the true positive rate (TPR) of\nthe Bayes classifier under these metrics tends to $0$ as well. Thus, in\nimbalanced classification problems, these metrics favour classifiers which\nignore the minority class. To alleviate this issue we introduce robust\nmodifications of the F-score and the MCC for which, even in strongly imbalanced\nsettings, the TPR is bounded away from $0$. We numerically illustrate the\nbehaviour of the various performance metrics in simulations as well as on a\ncredit default data set. We also discuss connections to the ROC and\nprecision-recall curves and give recommendations on how to combine their usage\nwith performance metrics.\n""]",Class Imbalance in Classification Tasks,Machine Learning for Imbalanced Classification and Fraud Detection,Machine Learning and Blockchain for Security and Fraud Detection,Machine Learning and Blockchain for Security and Fraud Detection
140,61,140_prompts_prompt_prompting_promptwizard,"['prompts', 'prompt', 'prompting', 'promptwizard', 'language', 'automatic', 'feedback', 'instruction', 'tasks', 'examples']","['prompt', 'prompts', 'engineering', 'prompting', 'optimization', 'instructions', 'feedback', 'examples', 'automatic', 'instruction']","['  Prompt-based models have gathered a lot of attention from researchers due to\ntheir remarkable advancements in the fields of zero-shot and few-shot learning.\nDeveloping an effective prompt template plays a critical role. However, prior\nstudies have mainly focused on prompt vocabulary searching or embedding\ninitialization within a predefined template with the prompt position fixed. In\nthis empirical study, we conduct the most comprehensive analysis to date of\nprompt position for diverse Natural Language Processing (NLP) tasks. Our\nfindings quantify the substantial impact prompt position has on model\nperformance. We observe that the prompt positions used in prior studies are\noften sub-optimal, and this observation is consistent even in widely used\ninstruction-tuned models. These findings suggest prompt position optimisation\nas a valuable research direction to augment prompt engineering methodologies\nand prompt position-aware instruction tuning as a potential way to build more\nrobust models in the future.\n', ""  Prompt engineering is a challenging and important task due to the high\nsensitivity of Large Language Models (LLMs) to the given prompt and the\ninherent ambiguity of a textual task instruction. Automatic prompt engineering\nis essential to achieve optimized performance from LLMs. Recent studies have\ndemonstrated the capabilities of LLMs to automatically conduct prompt\nengineering by employing a meta-prompt that incorporates the outcomes of the\nlast trials and proposes an improved prompt. However, this requires a\nhigh-quality benchmark to compare different prompts, which is difficult and\nexpensive to acquire in many real-world use cases. In this work, we introduce a\nnew method for automatic prompt engineering, using a calibration process that\niteratively refines the prompt to the user intent. During the optimization\nprocess, the system jointly generates synthetic data of boundary use cases and\noptimizes the prompt according to the generated dataset. We demonstrate the\neffectiveness of our method with respect to strong proprietary models on\nreal-world tasks such as moderation and generation. Our method outperforms\nstate-of-the-art methods with a limited number of annotated samples.\nFurthermore, we validate the advantages of each one of the system's key\ncomponents. Our system is built in a modular way, facilitating easy adaptation\nto other tasks. The code is available\n$\\href{https://github.com/Eladlev/AutoPrompt}{here}$.\n"", '  Prompt engineering is a critical technique in the field of natural language\nprocessing that involves designing and optimizing the prompts used to input\ninformation into models, aiming to enhance their performance on specific tasks.\nWith the recent advancements in large language models, prompt engineering has\nshown significant superiority across various domains and has become\nincreasingly important in the healthcare domain. However, there is a lack of\ncomprehensive reviews specifically focusing on prompt engineering in the\nmedical field. This review will introduce the latest advances in prompt\nengineering in the field of natural language processing for the medical field.\nFirst, we will provide the development of prompt engineering and emphasize its\nsignificant contributions to healthcare natural language processing\napplications such as question-answering systems, text summarization, and\nmachine translation. With the continuous improvement of general large language\nmodels, the importance of prompt engineering in the healthcare domain is\nbecoming increasingly prominent. The aim of this article is to provide useful\nresources and bridges for healthcare natural language processing researchers to\nbetter explore the application of prompt engineering in this field. We hope\nthat this review can provide new ideas and inspire for research and application\nin medical natural language processing.\n']",Prompt Engineering for NLP Tasks,Conversational AI and Language Models,Conversational AI and Human-Computer Interaction,Conversational AI and Human-Computer Interaction
141,60,141_pruning_pruningbench_pruner_prune,"['pruning', 'pruningbench', 'pruner', 'prune', 'pruned', 'compression', 'sparse', 'sparsegpt', 'compressing', 'efficient']","['pruning', 'sparsity', 'compression', 'retraining', 'weight', 'pruner', 'weights', 'structured', 'sparse', 'layer']","['  To remove redundant components of large language models (LLMs) without\nincurring significant computational costs, this work focuses on single-shot\npruning without a retraining phase. We simplify the pruning process for\nTransformer-based LLMs by identifying a depth-2 pruning structure that\nfunctions independently. Additionally, we propose two inference-aware pruning\ncriteria derived from the optimization perspective of output approximation,\nwhich outperforms traditional training-aware metrics such as gradient and\nHessian. We also introduce a two-step reconstruction technique to mitigate\npruning errors without model retraining. Experimental results demonstrate that\nour approach significantly reduces computational costs and hardware\nrequirements while maintaining superior performance across various datasets and\nmodels.\n', '  Large Vision-Language Models (LVLMs) can understand the world comprehensively\nby integrating rich information from different modalities, achieving remarkable\nadvancements on various multimodal downstream tasks. However, deploying LVLMs\nis often problematic due to their massive computational/energy costs and carbon\nconsumption. Such issues make it infeasible to adopt conventional iterative\nglobal pruning, which is costly due to computing the Hessian matrix of the\nentire large model for sparsification. Alternatively, several studies have\nrecently proposed layer-wise pruning approaches to avoid the expensive\ncomputation of global pruning and efficiently compress model weights according\nto their importance within a layer. However, they often suffer from suboptimal\nmodel compression due to their lack of a global perspective. To address this\nlimitation in recent efficient pruning methods for large models, we propose\nEfficient Coarse-to-Fine LayerWise Pruning (ECoFLaP), a two-stage\ncoarse-to-fine weight pruning approach for LVLMs. We first determine the\nsparsity ratios of different layers or blocks by leveraging the global\nimportance score, which is efficiently computed based on the zeroth-order\napproximation of the global model gradients. Then, the model performs local\nlayer-wise unstructured weight pruning based on globally-informed sparsity\nratios. We validate our proposed method across various multimodal and unimodal\nmodels and datasets, demonstrating significant performance improvements over\nprevalent pruning techniques in the high-sparsity regime.\n', '  Despite the remarkable capabilities, Large Language Models (LLMs) face\ndeployment challenges due to their extensive size. Pruning methods drop a\nsubset of weights to accelerate, but many of them require retraining, which is\nprohibitively expensive and computationally demanding. Recently, post-training\npruning approaches introduced novel metrics, enabling the pruning of LLMs\nwithout retraining. However, these metrics require the involvement of human\nexperts and tedious trial and error. To efficiently identify superior pruning\nmetrics, we develop an automatic framework for searching symbolic pruning\nmetrics using genetic programming. In particular, we devise an elaborate search\nspace encompassing the existing pruning metrics to discover the potential\nsymbolic pruning metric. We propose an opposing operation simplification\nstrategy to increase the diversity of the population. In this way, Pruner-Zero\nallows auto-generation of symbolic pruning metrics. Based on the searched\nresults, we explore the correlation between pruning metrics and performance\nafter pruning and summarize some principles. Extensive experiments on LLaMA and\nLLaMA-2 on language modeling and zero-shot tasks demonstrate that our\nPruner-Zero obtains superior performance than SOTA post-training pruning\nmethods. Code at: \\url{https://github.com/pprp/Pruner-Zero}.\n']",Efficient Pruning Methods for Large Language Models,Optimization and Efficiency of Large Language Models,Large Language Models,Large Language Models
141,60,141_pruning_pruningbench_pruner_prune,"['pruning', 'pruningbench', 'pruner', 'prune', 'pruned', 'compression', 'sparse', 'sparsegpt', 'compressing', 'efficient']","['pruning', 'sparsity', 'compression', 'retraining', 'weight', 'pruner', 'weights', 'structured', 'sparse', 'layer']","['  To remove redundant components of large language models (LLMs) without\nincurring significant computational costs, this work focuses on single-shot\npruning without a retraining phase. We simplify the pruning process for\nTransformer-based LLMs by identifying a depth-2 pruning structure that\nfunctions independently. Additionally, we propose two inference-aware pruning\ncriteria derived from the optimization perspective of output approximation,\nwhich outperforms traditional training-aware metrics such as gradient and\nHessian. We also introduce a two-step reconstruction technique to mitigate\npruning errors without model retraining. Experimental results demonstrate that\nour approach significantly reduces computational costs and hardware\nrequirements while maintaining superior performance across various datasets and\nmodels.\n', '  Large Vision-Language Models (LVLMs) can understand the world comprehensively\nby integrating rich information from different modalities, achieving remarkable\nadvancements on various multimodal downstream tasks. However, deploying LVLMs\nis often problematic due to their massive computational/energy costs and carbon\nconsumption. Such issues make it infeasible to adopt conventional iterative\nglobal pruning, which is costly due to computing the Hessian matrix of the\nentire large model for sparsification. Alternatively, several studies have\nrecently proposed layer-wise pruning approaches to avoid the expensive\ncomputation of global pruning and efficiently compress model weights according\nto their importance within a layer. However, they often suffer from suboptimal\nmodel compression due to their lack of a global perspective. To address this\nlimitation in recent efficient pruning methods for large models, we propose\nEfficient Coarse-to-Fine LayerWise Pruning (ECoFLaP), a two-stage\ncoarse-to-fine weight pruning approach for LVLMs. We first determine the\nsparsity ratios of different layers or blocks by leveraging the global\nimportance score, which is efficiently computed based on the zeroth-order\napproximation of the global model gradients. Then, the model performs local\nlayer-wise unstructured weight pruning based on globally-informed sparsity\nratios. We validate our proposed method across various multimodal and unimodal\nmodels and datasets, demonstrating significant performance improvements over\nprevalent pruning techniques in the high-sparsity regime.\n', '  Despite the remarkable capabilities, Large Language Models (LLMs) face\ndeployment challenges due to their extensive size. Pruning methods drop a\nsubset of weights to accelerate, but many of them require retraining, which is\nprohibitively expensive and computationally demanding. Recently, post-training\npruning approaches introduced novel metrics, enabling the pruning of LLMs\nwithout retraining. However, these metrics require the involvement of human\nexperts and tedious trial and error. To efficiently identify superior pruning\nmetrics, we develop an automatic framework for searching symbolic pruning\nmetrics using genetic programming. In particular, we devise an elaborate search\nspace encompassing the existing pruning metrics to discover the potential\nsymbolic pruning metric. We propose an opposing operation simplification\nstrategy to increase the diversity of the population. In this way, Pruner-Zero\nallows auto-generation of symbolic pruning metrics. Based on the searched\nresults, we explore the correlation between pruning metrics and performance\nafter pruning and summarize some principles. Extensive experiments on LLaMA and\nLLaMA-2 on language modeling and zero-shot tasks demonstrate that our\nPruner-Zero obtains superior performance than SOTA post-training pruning\nmethods. Code at: \\url{https://github.com/pprp/Pruner-Zero}.\n']",Efficient Pruning Methods for Large Language Models,Optimization and Efficiency of Large Language Models,Large Language Models,Large Language Models
142,60,142_events_event_annotations_annotation,"['events', 'event', 'annotations', 'annotation', 'corpus', 'annotated', 'entity', 'extracting', 'triggers', 'coreference']","['event', 'extraction', 'events', 'coreference', 'argument', 'document', 'arguments', 'mentions', 'relations', 'causality']","['  Events describe the state changes of entities. In a document, multiple events\nare connected by various relations (e.g., Coreference, Temporal, Causal, and\nSubevent). Therefore, obtaining the connections between events through\nEvent-Event Relation Extraction (ERE) is critical to understand natural\nlanguage. There are two main problems in the current ERE works: a. Only\nembeddings of the event triggers are used for event feature representation,\nignoring event arguments (e.g., time, place, person, etc.) and their structure\nwithin the event. b. The interconnection between relations (e.g., temporal and\ncausal relations usually interact with each other ) is ignored. To solve the\nabove problems, this paper proposes a jointly multiple ERE framework called\nGraphERE based on Graph-enhanced Event Embeddings. First, we enrich the event\nembeddings with event argument and structure features by using static AMR\ngraphs and IE graphs; Then, to jointly extract multiple event relations, we use\nNode Transformer and construct Task-specific Dynamic Event Graphs for each type\nof relation. Finally, we used a multi-task learning strategy to train the whole\nframework. Experimental results on the latest MAVEN-ERE dataset validate that\nGraphERE significantly outperforms existing methods. Further analyses indicate\nthe effectiveness of the graph-enhanced event embeddings and the joint\nextraction strategy.\n', '  Existing approaches on zero-shot event detection usually train models on\ndatasets annotated with known event types, and prompt them with unseen event\ndefinitions. These approaches yield sporadic successes, yet generally fall\nshort of expectations. In this work, we aim to improve zero-shot event\ndetection by training models to better follow event definitions. We hypothesize\nthat a diverse set of event types and definitions are the key for models to\nlearn to follow event definitions while existing event extraction datasets\nfocus on annotating many high-quality examples for a few event types. To verify\nour hypothesis, we construct an automatically generated Diverse Event\nDefinition (DivED) dataset and conduct comparative studies. Our experiments\nreveal that a large number of event types (200) and diverse event definitions\ncan significantly boost event extraction performance; on the other hand, the\nperformance does not scale with over ten examples per event type. Beyond\nscaling, we incorporate event ontology information and hard-negative samples\nduring training, further boosting the performance. Based on these findings, we\nfine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that\nsurpasses SOTA large language models like GPT-3.5 across three open benchmarks\non zero-shot event detection.\n', '  Event sequence models have been found to be highly effective in the analysis\nand prediction of events. Building such models requires availability of\nabundant high-quality event sequence data. In certain applications, however,\nclean structured event sequences are not available, and automated sequence\nextraction results in data that is too noisy and incomplete. In this work, we\nexplore the use of Large Language Models (LLMs) to generate event sequences\nthat can effectively be used for probabilistic event model construction. This\ncan be viewed as a mechanism of distilling event sequence knowledge from LLMs.\nOur approach relies on a Knowledge Graph (KG) of event concepts with partial\ncausal relations to guide the generative language model for causal event\nsequence generation. We show that our approach can generate high-quality event\nsequences, filling a knowledge gap in the input KG. Furthermore, we explore how\nthe generated sequences can be leveraged to discover useful and more complex\nstructured knowledge from pattern mining and probabilistic event models. We\nrelease our sequence generation code and evaluation framework, as well as\ncorpus of event sequence data.\n']",Event Extraction and Relation Analysis,Information Extraction from Text,Information Extraction,Information Extraction
143,60,143_equivariance_symmetries_equivariant_invariance,"['equivariance', 'symmetries', 'equivariant', 'invariance', 'invariants', 'symmetry', 'representations', 'cnns', 'convolutions', 'neural']","['equivariant', 'symmetry', 'equivariance', 'group', 'symmetries', 'invariant', 'symmetric', 'groups', 'breaking', 'transformations']","['  We present a novel framework to overcome the limitations of equivariant\narchitectures in learning functions with group symmetries. In contrary to\nequivariant architectures, we use an arbitrary base model such as an MLP or a\ntransformer and symmetrize it to be equivariant to the given group by employing\na small equivariant network that parameterizes the probabilistic distribution\nunderlying the symmetrization. The distribution is end-to-end trained with the\nbase model which can maximize performance while reducing sample complexity of\nsymmetrization. We show that this approach ensures not only equivariance to\ngiven group but also universal approximation capability in expectation. We\nimplement our method on various base models, including patch-based transformers\nthat can be initialized from pretrained vision transformers, and test them for\na wide range of symmetry groups including permutation and Euclidean groups and\ntheir combinations. Empirical tests show competitive results against tailored\nequivariant architectures, suggesting the potential for learning equivariant\nfunctions for diverse groups using a non-equivariant universal base\narchitecture. We further show evidence of enhanced learning in symmetric\nmodalities, like graphs, when pretrained from non-symmetric modalities, like\nvision. Code is available at https://github.com/jw9730/lps.\n', '  Equivariant deep learning architectures exploit symmetries in learning\nproblems to improve the sample efficiency of neural-network-based models and\ntheir ability to generalise. However, when modelling real-world data, learning\nproblems are often not exactly equivariant, but only approximately. For\nexample, when estimating the global temperature field from weather station\nobservations, local topographical features like mountains break translation\nequivariance. In these scenarios, it is desirable to construct architectures\nthat can flexibly depart from exact equivariance in a data-driven way. In this\npaper, we develop a general approach to achieving this using existing\nequivariant architectures. Our approach is agnostic to both the choice of\nsymmetry group and model architecture, making it widely applicable. We consider\nthe use of approximately equivariant architectures in neural processes (NPs), a\npopular family of meta-learning models. We demonstrate the effectiveness of our\napproach on a number of synthetic and real-world regression experiments,\ndemonstrating that approximately equivariant NP models can outperform both\ntheir non-equivariant and strictly equivariant counterparts.\n', '  In this paper we develop a manifestly geometric framework for equivariant\nmanifold neural ordinary differential equations (NODEs), and use it to analyse\ntheir modelling capabilities for symmetric data. First, we consider the action\nof a Lie group $G$ on a smooth manifold $M$ and establish the equivalence\nbetween equivariance of vector fields, symmetries of the corresponding Cauchy\nproblems, and equivariance of the associated NODEs. We also propose a novel\nformulation of the equivariant NODEs in terms of the differential invariants of\nthe action of $G$ on $M$, based on Lie theory for symmetries of differential\nequations, which provides an efficient parameterisation of the space of\nequivariant vector fields in a way that is agnostic to both the manifold $M$\nand the symmetry group $G$. Second, we construct augmented manifold NODEs,\nthrough embeddings into equivariant flows, and show that they are universal\napproximators of equivariant diffeomorphisms on any path-connected $M$.\nFurthermore, we show that the augmented NODEs can be incorporated in the\ngeometric framework and parameterised using higher order differential\ninvariants. Finally, we consider the induced action of $G$ on different fields\non $M$ and show how it can be used to generalise previous work, on, e.g.,\ncontinuous normalizing flows, to equivariant models in any geometry.\n']",Equivariant Neural Networks and Symmetries,Equivariant Neural Networks and Symmetry in Deep Learning,Geometric and Equivariant Deep Learning,Geometric and Equivariant Deep Learning
144,60,144_contexts_attention_lengthy_context,"['contexts', 'attention', 'lengthy', 'context', 'embedding', 'memory', 'positional', 'position', 'positions', 'longer']","['window', 'position', 'length', 'context', 'long', 'positional', 'extrapolation', 'attention', 'longer', 'memory']","['  Position embedding is a core component of current Large Language Models\n(LLMs). Rotary position embedding (RoPE), a technique that encodes the position\ninformation with a rotation matrix, has been the de facto choice for position\nembedding in many LLMs, such as the Llama series. RoPE has been further\nutilized to extend long context capability, which is roughly based on adjusting\nthe \\textit{base} parameter of RoPE to mitigate out-of-distribution (OOD)\nproblems in position embedding. However, in this paper, we find that LLMs may\nobtain a superficial long-context ability based on the OOD theory. We revisit\nthe role of RoPE in LLMs and propose a novel property of long-term decay, we\nderive that the \\textit{base of RoPE bounds context length}: there is an\nabsolute lower bound for the base value to obtain certain context length\ncapability. Our work reveals the relationship between context length and RoPE\nbase both theoretically and empirically, which may shed light on future long\ncontext training.\n', ""  Large Language Models (LLMs) are known to have limited extrapolation ability\nbeyond their pre-trained context window, constraining their application in\ndownstream tasks with lengthy inputs. Recent studies have sought to extend\nLLMs' context window by modifying rotary position embedding (RoPE), a popular\nposition encoding method adopted by well-known LLMs such as LLaMA, PaLM, and\nGPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are\nresource-intensive and lack comparative experiments to assess their\napplicability. In this work, we identify the inherent need for LLMs' attention\nentropy (i.e. the information entropy of attention scores) to maintain\nstability and introduce a novel extension to RoPE which combines adjusting\nRoPE's base frequency and scaling the attention logits to help LLMs efficiently\nadapt to a larger context window. We validate the superiority of our method in\nboth fine-tuning performance and robustness across different context window\nsizes on various context-demanding tasks. Notably, our method extends the\ncontext window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6\ntraining steps, showcasing extraordinary efficiency. Finally, we also explore\nhow data compositions and training curricula affect context window extension\nfor specific downstream tasks, suggesting fine-tuning LLMs with lengthy\nconversations as a good starting point. We release our code and SFT data at\nhttps://github.com/GAIR-NLP/Entropy-ABF.\n"", '  Large Language Models (LLMs) are trained with a pre-defined context length,\nrestricting their use in scenarios requiring long inputs. Previous efforts for\nadapting LLMs to a longer length usually requires fine-tuning with this target\nlength (Full-length fine-tuning), suffering intensive training cost. To\ndecouple train length from target length for efficient context window\nextension, we propose Positional Skip-wisE (PoSE) training that smartly\nsimulates long inputs using a fixed context window. This is achieved by first\ndividing the original context window into several chunks, then designing\ndistinct skipping bias terms to manipulate the position indices of each chunk.\nThese bias terms and the lengths of each chunk are altered for every training\nexample, allowing the model to adapt to all positions within target length.\nExperimental results show that PoSE greatly reduces memory and time overhead\ncompared with Full-length fine-tuning, with minimal impact on performance.\nLeveraging this advantage, we have successfully extended the LLaMA model to\n128k tokens using a 2k training context window. Furthermore, we empirically\nconfirm that PoSE is compatible with all RoPE-based LLMs and position\ninterpolation strategies. Notably, our method can potentially support infinite\nlength, limited only by memory usage in inference. With ongoing progress for\nefficient inference, we believe PoSE can further scale the context window\nbeyond 128k.\n']",Extending Context Length in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
144,60,144_contexts_attention_lengthy_context,"['contexts', 'attention', 'lengthy', 'context', 'embedding', 'memory', 'positional', 'position', 'positions', 'longer']","['window', 'position', 'length', 'context', 'long', 'positional', 'extrapolation', 'attention', 'longer', 'memory']","['  Position embedding is a core component of current Large Language Models\n(LLMs). Rotary position embedding (RoPE), a technique that encodes the position\ninformation with a rotation matrix, has been the de facto choice for position\nembedding in many LLMs, such as the Llama series. RoPE has been further\nutilized to extend long context capability, which is roughly based on adjusting\nthe \\textit{base} parameter of RoPE to mitigate out-of-distribution (OOD)\nproblems in position embedding. However, in this paper, we find that LLMs may\nobtain a superficial long-context ability based on the OOD theory. We revisit\nthe role of RoPE in LLMs and propose a novel property of long-term decay, we\nderive that the \\textit{base of RoPE bounds context length}: there is an\nabsolute lower bound for the base value to obtain certain context length\ncapability. Our work reveals the relationship between context length and RoPE\nbase both theoretically and empirically, which may shed light on future long\ncontext training.\n', ""  Large Language Models (LLMs) are known to have limited extrapolation ability\nbeyond their pre-trained context window, constraining their application in\ndownstream tasks with lengthy inputs. Recent studies have sought to extend\nLLMs' context window by modifying rotary position embedding (RoPE), a popular\nposition encoding method adopted by well-known LLMs such as LLaMA, PaLM, and\nGPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are\nresource-intensive and lack comparative experiments to assess their\napplicability. In this work, we identify the inherent need for LLMs' attention\nentropy (i.e. the information entropy of attention scores) to maintain\nstability and introduce a novel extension to RoPE which combines adjusting\nRoPE's base frequency and scaling the attention logits to help LLMs efficiently\nadapt to a larger context window. We validate the superiority of our method in\nboth fine-tuning performance and robustness across different context window\nsizes on various context-demanding tasks. Notably, our method extends the\ncontext window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6\ntraining steps, showcasing extraordinary efficiency. Finally, we also explore\nhow data compositions and training curricula affect context window extension\nfor specific downstream tasks, suggesting fine-tuning LLMs with lengthy\nconversations as a good starting point. We release our code and SFT data at\nhttps://github.com/GAIR-NLP/Entropy-ABF.\n"", '  Large Language Models (LLMs) are trained with a pre-defined context length,\nrestricting their use in scenarios requiring long inputs. Previous efforts for\nadapting LLMs to a longer length usually requires fine-tuning with this target\nlength (Full-length fine-tuning), suffering intensive training cost. To\ndecouple train length from target length for efficient context window\nextension, we propose Positional Skip-wisE (PoSE) training that smartly\nsimulates long inputs using a fixed context window. This is achieved by first\ndividing the original context window into several chunks, then designing\ndistinct skipping bias terms to manipulate the position indices of each chunk.\nThese bias terms and the lengths of each chunk are altered for every training\nexample, allowing the model to adapt to all positions within target length.\nExperimental results show that PoSE greatly reduces memory and time overhead\ncompared with Full-length fine-tuning, with minimal impact on performance.\nLeveraging this advantage, we have successfully extended the LLaMA model to\n128k tokens using a 2k training context window. Furthermore, we empirically\nconfirm that PoSE is compatible with all RoPE-based LLMs and position\ninterpolation strategies. Notably, our method can potentially support infinite\nlength, limited only by memory usage in inference. With ongoing progress for\nefficient inference, we believe PoSE can further scale the context window\nbeyond 128k.\n']",Extending Context Length in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
144,60,144_contexts_attention_lengthy_context,"['contexts', 'attention', 'lengthy', 'context', 'embedding', 'memory', 'positional', 'position', 'positions', 'longer']","['window', 'position', 'length', 'context', 'long', 'positional', 'extrapolation', 'attention', 'longer', 'memory']","['  Position embedding is a core component of current Large Language Models\n(LLMs). Rotary position embedding (RoPE), a technique that encodes the position\ninformation with a rotation matrix, has been the de facto choice for position\nembedding in many LLMs, such as the Llama series. RoPE has been further\nutilized to extend long context capability, which is roughly based on adjusting\nthe \\textit{base} parameter of RoPE to mitigate out-of-distribution (OOD)\nproblems in position embedding. However, in this paper, we find that LLMs may\nobtain a superficial long-context ability based on the OOD theory. We revisit\nthe role of RoPE in LLMs and propose a novel property of long-term decay, we\nderive that the \\textit{base of RoPE bounds context length}: there is an\nabsolute lower bound for the base value to obtain certain context length\ncapability. Our work reveals the relationship between context length and RoPE\nbase both theoretically and empirically, which may shed light on future long\ncontext training.\n', ""  Large Language Models (LLMs) are known to have limited extrapolation ability\nbeyond their pre-trained context window, constraining their application in\ndownstream tasks with lengthy inputs. Recent studies have sought to extend\nLLMs' context window by modifying rotary position embedding (RoPE), a popular\nposition encoding method adopted by well-known LLMs such as LLaMA, PaLM, and\nGPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are\nresource-intensive and lack comparative experiments to assess their\napplicability. In this work, we identify the inherent need for LLMs' attention\nentropy (i.e. the information entropy of attention scores) to maintain\nstability and introduce a novel extension to RoPE which combines adjusting\nRoPE's base frequency and scaling the attention logits to help LLMs efficiently\nadapt to a larger context window. We validate the superiority of our method in\nboth fine-tuning performance and robustness across different context window\nsizes on various context-demanding tasks. Notably, our method extends the\ncontext window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6\ntraining steps, showcasing extraordinary efficiency. Finally, we also explore\nhow data compositions and training curricula affect context window extension\nfor specific downstream tasks, suggesting fine-tuning LLMs with lengthy\nconversations as a good starting point. We release our code and SFT data at\nhttps://github.com/GAIR-NLP/Entropy-ABF.\n"", '  Large Language Models (LLMs) are trained with a pre-defined context length,\nrestricting their use in scenarios requiring long inputs. Previous efforts for\nadapting LLMs to a longer length usually requires fine-tuning with this target\nlength (Full-length fine-tuning), suffering intensive training cost. To\ndecouple train length from target length for efficient context window\nextension, we propose Positional Skip-wisE (PoSE) training that smartly\nsimulates long inputs using a fixed context window. This is achieved by first\ndividing the original context window into several chunks, then designing\ndistinct skipping bias terms to manipulate the position indices of each chunk.\nThese bias terms and the lengths of each chunk are altered for every training\nexample, allowing the model to adapt to all positions within target length.\nExperimental results show that PoSE greatly reduces memory and time overhead\ncompared with Full-length fine-tuning, with minimal impact on performance.\nLeveraging this advantage, we have successfully extended the LLaMA model to\n128k tokens using a 2k training context window. Furthermore, we empirically\nconfirm that PoSE is compatible with all RoPE-based LLMs and position\ninterpolation strategies. Notably, our method can potentially support infinite\nlength, limited only by memory usage in inference. With ongoing progress for\nefficient inference, we believe PoSE can further scale the context window\nbeyond 128k.\n']",Extending Context Length in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
144,60,144_contexts_attention_lengthy_context,"['contexts', 'attention', 'lengthy', 'context', 'embedding', 'memory', 'positional', 'position', 'positions', 'longer']","['window', 'position', 'length', 'context', 'long', 'positional', 'extrapolation', 'attention', 'longer', 'memory']","['  Position embedding is a core component of current Large Language Models\n(LLMs). Rotary position embedding (RoPE), a technique that encodes the position\ninformation with a rotation matrix, has been the de facto choice for position\nembedding in many LLMs, such as the Llama series. RoPE has been further\nutilized to extend long context capability, which is roughly based on adjusting\nthe \\textit{base} parameter of RoPE to mitigate out-of-distribution (OOD)\nproblems in position embedding. However, in this paper, we find that LLMs may\nobtain a superficial long-context ability based on the OOD theory. We revisit\nthe role of RoPE in LLMs and propose a novel property of long-term decay, we\nderive that the \\textit{base of RoPE bounds context length}: there is an\nabsolute lower bound for the base value to obtain certain context length\ncapability. Our work reveals the relationship between context length and RoPE\nbase both theoretically and empirically, which may shed light on future long\ncontext training.\n', ""  Large Language Models (LLMs) are known to have limited extrapolation ability\nbeyond their pre-trained context window, constraining their application in\ndownstream tasks with lengthy inputs. Recent studies have sought to extend\nLLMs' context window by modifying rotary position embedding (RoPE), a popular\nposition encoding method adopted by well-known LLMs such as LLaMA, PaLM, and\nGPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are\nresource-intensive and lack comparative experiments to assess their\napplicability. In this work, we identify the inherent need for LLMs' attention\nentropy (i.e. the information entropy of attention scores) to maintain\nstability and introduce a novel extension to RoPE which combines adjusting\nRoPE's base frequency and scaling the attention logits to help LLMs efficiently\nadapt to a larger context window. We validate the superiority of our method in\nboth fine-tuning performance and robustness across different context window\nsizes on various context-demanding tasks. Notably, our method extends the\ncontext window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6\ntraining steps, showcasing extraordinary efficiency. Finally, we also explore\nhow data compositions and training curricula affect context window extension\nfor specific downstream tasks, suggesting fine-tuning LLMs with lengthy\nconversations as a good starting point. We release our code and SFT data at\nhttps://github.com/GAIR-NLP/Entropy-ABF.\n"", '  Large Language Models (LLMs) are trained with a pre-defined context length,\nrestricting their use in scenarios requiring long inputs. Previous efforts for\nadapting LLMs to a longer length usually requires fine-tuning with this target\nlength (Full-length fine-tuning), suffering intensive training cost. To\ndecouple train length from target length for efficient context window\nextension, we propose Positional Skip-wisE (PoSE) training that smartly\nsimulates long inputs using a fixed context window. This is achieved by first\ndividing the original context window into several chunks, then designing\ndistinct skipping bias terms to manipulate the position indices of each chunk.\nThese bias terms and the lengths of each chunk are altered for every training\nexample, allowing the model to adapt to all positions within target length.\nExperimental results show that PoSE greatly reduces memory and time overhead\ncompared with Full-length fine-tuning, with minimal impact on performance.\nLeveraging this advantage, we have successfully extended the LLaMA model to\n128k tokens using a 2k training context window. Furthermore, we empirically\nconfirm that PoSE is compatible with all RoPE-based LLMs and position\ninterpolation strategies. Notably, our method can potentially support infinite\nlength, limited only by memory usage in inference. With ongoing progress for\nefficient inference, we believe PoSE can further scale the context window\nbeyond 128k.\n']",Extending Context Length in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
144,60,144_contexts_attention_lengthy_context,"['contexts', 'attention', 'lengthy', 'context', 'embedding', 'memory', 'positional', 'position', 'positions', 'longer']","['window', 'position', 'length', 'context', 'long', 'positional', 'extrapolation', 'attention', 'longer', 'memory']","['  Position embedding is a core component of current Large Language Models\n(LLMs). Rotary position embedding (RoPE), a technique that encodes the position\ninformation with a rotation matrix, has been the de facto choice for position\nembedding in many LLMs, such as the Llama series. RoPE has been further\nutilized to extend long context capability, which is roughly based on adjusting\nthe \\textit{base} parameter of RoPE to mitigate out-of-distribution (OOD)\nproblems in position embedding. However, in this paper, we find that LLMs may\nobtain a superficial long-context ability based on the OOD theory. We revisit\nthe role of RoPE in LLMs and propose a novel property of long-term decay, we\nderive that the \\textit{base of RoPE bounds context length}: there is an\nabsolute lower bound for the base value to obtain certain context length\ncapability. Our work reveals the relationship between context length and RoPE\nbase both theoretically and empirically, which may shed light on future long\ncontext training.\n', ""  Large Language Models (LLMs) are known to have limited extrapolation ability\nbeyond their pre-trained context window, constraining their application in\ndownstream tasks with lengthy inputs. Recent studies have sought to extend\nLLMs' context window by modifying rotary position embedding (RoPE), a popular\nposition encoding method adopted by well-known LLMs such as LLaMA, PaLM, and\nGPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are\nresource-intensive and lack comparative experiments to assess their\napplicability. In this work, we identify the inherent need for LLMs' attention\nentropy (i.e. the information entropy of attention scores) to maintain\nstability and introduce a novel extension to RoPE which combines adjusting\nRoPE's base frequency and scaling the attention logits to help LLMs efficiently\nadapt to a larger context window. We validate the superiority of our method in\nboth fine-tuning performance and robustness across different context window\nsizes on various context-demanding tasks. Notably, our method extends the\ncontext window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6\ntraining steps, showcasing extraordinary efficiency. Finally, we also explore\nhow data compositions and training curricula affect context window extension\nfor specific downstream tasks, suggesting fine-tuning LLMs with lengthy\nconversations as a good starting point. We release our code and SFT data at\nhttps://github.com/GAIR-NLP/Entropy-ABF.\n"", '  Large Language Models (LLMs) are trained with a pre-defined context length,\nrestricting their use in scenarios requiring long inputs. Previous efforts for\nadapting LLMs to a longer length usually requires fine-tuning with this target\nlength (Full-length fine-tuning), suffering intensive training cost. To\ndecouple train length from target length for efficient context window\nextension, we propose Positional Skip-wisE (PoSE) training that smartly\nsimulates long inputs using a fixed context window. This is achieved by first\ndividing the original context window into several chunks, then designing\ndistinct skipping bias terms to manipulate the position indices of each chunk.\nThese bias terms and the lengths of each chunk are altered for every training\nexample, allowing the model to adapt to all positions within target length.\nExperimental results show that PoSE greatly reduces memory and time overhead\ncompared with Full-length fine-tuning, with minimal impact on performance.\nLeveraging this advantage, we have successfully extended the LLaMA model to\n128k tokens using a 2k training context window. Furthermore, we empirically\nconfirm that PoSE is compatible with all RoPE-based LLMs and position\ninterpolation strategies. Notably, our method can potentially support infinite\nlength, limited only by memory usage in inference. With ongoing progress for\nefficient inference, we believe PoSE can further scale the context window\nbeyond 128k.\n']",Extending Context Length in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
144,60,144_contexts_attention_lengthy_context,"['contexts', 'attention', 'lengthy', 'context', 'embedding', 'memory', 'positional', 'position', 'positions', 'longer']","['window', 'position', 'length', 'context', 'long', 'positional', 'extrapolation', 'attention', 'longer', 'memory']","['  Position embedding is a core component of current Large Language Models\n(LLMs). Rotary position embedding (RoPE), a technique that encodes the position\ninformation with a rotation matrix, has been the de facto choice for position\nembedding in many LLMs, such as the Llama series. RoPE has been further\nutilized to extend long context capability, which is roughly based on adjusting\nthe \\textit{base} parameter of RoPE to mitigate out-of-distribution (OOD)\nproblems in position embedding. However, in this paper, we find that LLMs may\nobtain a superficial long-context ability based on the OOD theory. We revisit\nthe role of RoPE in LLMs and propose a novel property of long-term decay, we\nderive that the \\textit{base of RoPE bounds context length}: there is an\nabsolute lower bound for the base value to obtain certain context length\ncapability. Our work reveals the relationship between context length and RoPE\nbase both theoretically and empirically, which may shed light on future long\ncontext training.\n', ""  Large Language Models (LLMs) are known to have limited extrapolation ability\nbeyond their pre-trained context window, constraining their application in\ndownstream tasks with lengthy inputs. Recent studies have sought to extend\nLLMs' context window by modifying rotary position embedding (RoPE), a popular\nposition encoding method adopted by well-known LLMs such as LLaMA, PaLM, and\nGPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are\nresource-intensive and lack comparative experiments to assess their\napplicability. In this work, we identify the inherent need for LLMs' attention\nentropy (i.e. the information entropy of attention scores) to maintain\nstability and introduce a novel extension to RoPE which combines adjusting\nRoPE's base frequency and scaling the attention logits to help LLMs efficiently\nadapt to a larger context window. We validate the superiority of our method in\nboth fine-tuning performance and robustness across different context window\nsizes on various context-demanding tasks. Notably, our method extends the\ncontext window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6\ntraining steps, showcasing extraordinary efficiency. Finally, we also explore\nhow data compositions and training curricula affect context window extension\nfor specific downstream tasks, suggesting fine-tuning LLMs with lengthy\nconversations as a good starting point. We release our code and SFT data at\nhttps://github.com/GAIR-NLP/Entropy-ABF.\n"", '  Large Language Models (LLMs) are trained with a pre-defined context length,\nrestricting their use in scenarios requiring long inputs. Previous efforts for\nadapting LLMs to a longer length usually requires fine-tuning with this target\nlength (Full-length fine-tuning), suffering intensive training cost. To\ndecouple train length from target length for efficient context window\nextension, we propose Positional Skip-wisE (PoSE) training that smartly\nsimulates long inputs using a fixed context window. This is achieved by first\ndividing the original context window into several chunks, then designing\ndistinct skipping bias terms to manipulate the position indices of each chunk.\nThese bias terms and the lengths of each chunk are altered for every training\nexample, allowing the model to adapt to all positions within target length.\nExperimental results show that PoSE greatly reduces memory and time overhead\ncompared with Full-length fine-tuning, with minimal impact on performance.\nLeveraging this advantage, we have successfully extended the LLaMA model to\n128k tokens using a 2k training context window. Furthermore, we empirically\nconfirm that PoSE is compatible with all RoPE-based LLMs and position\ninterpolation strategies. Notably, our method can potentially support infinite\nlength, limited only by memory usage in inference. With ongoing progress for\nefficient inference, we believe PoSE can further scale the context window\nbeyond 128k.\n']",Extending Context Length in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
145,59,145_priors_bayesian_posteriors_bayes,"['priors', 'bayesian', 'posteriors', 'bayes', 'neural', 'posterior', 'deep', 'prior', 'likelihood', 'inference']","['posterior', 'priors', 'variational', 'uncertainty', 'inference', 'approximation', 'approximate', 'posteriors', 'neural', 'likelihood']","['  Uncertainty quantification is an important task in machine learning - a task\nin which standardneural networks (NNs) have traditionally not excelled. This\ncan be a limitation for safety-critical applications, where uncertainty-aware\nmethods like Gaussian processes or Bayesian linear regression are often\npreferred. Bayesian neural networks are an approach to address this limitation.\nThey assume probability distributions for all parameters and yield distributed\npredictions. However, training and inference are typically intractable and\napproximations must be employed. A promising approximation is NNs with Bayesian\nlast layer (BLL). They assume distributed weights only in the linear output\nlayer and yield a normally distributed prediction. To approximate the\nintractable Bayesian neural network, point estimates of the distributed weights\nin all but the last layer should be obtained by maximizing the marginal\nlikelihood. This has previously been challenging, as the marginal likelihood is\nexpensive to evaluate in this setting. We present a reformulation of the\nlog-marginal likelihood of a NN with BLL which allows for efficient training\nusing backpropagation. Furthermore, we address the challenge of uncertainty\nquantification for extrapolation points. We provide a metric to quantify the\ndegree of extrapolation and derive a method to improve the uncertainty\nquantification for these points. Our methods are derived for the multivariate\ncase and demonstrated in a simulation study. In comparison to Bayesian linear\nregression with fixed features, and a Bayesian neural network trained with\nvariational inference, our proposed method achieves the highest log-predictive\ndensity on test data.\n', '  Bayesian neural networks (BNNs) have recently gained popularity due to their\nability to quantify model uncertainty. However, specifying a prior for BNNs\nthat captures relevant domain knowledge is often extremely challenging. In this\nwork, we propose a framework for integrating general forms of domain knowledge\n(i.e., any knowledge that can be represented by a loss function) into a BNN\nprior through variational inference, while enabling computationally efficient\nposterior inference and sampling. Specifically, our approach results in a prior\nover neural network weights that assigns high probability mass to models that\nbetter align with our domain knowledge, leading to posterior samples that also\nexhibit this behavior. We show that BNNs using our proposed domain knowledge\npriors outperform those with standard priors (e.g., isotropic Gaussian,\nGaussian process), successfully incorporating diverse types of prior\ninformation such as fairness, physics rules, and healthcare knowledge and\nachieving better predictive performance. We also present techniques for\ntransferring the learned priors across different model architectures,\ndemonstrating their broad utility across various settings.\n', '  Deep learning has revolutionized the last decade, being at the forefront of\nextraordinary advances in a wide range of tasks including computer vision,\nnatural language processing, and reinforcement learning, to name but a few.\nHowever, it is well-known that deep models trained via maximum likelihood\nestimation tend to be overconfident and give poorly-calibrated predictions.\nBayesian deep learning attempts to address this by placing priors on the model\nparameters, which are then combined with a likelihood to perform posterior\ninference. Unfortunately, for deep models, the true posterior is intractable,\nforcing the user to resort to approximations. In this thesis, we explore the\nuse of variational inference (VI) as an approximation, as it is unique in\nsimultaneously approximating the posterior and providing a lower bound to the\nmarginal likelihood. If tight enough, this lower bound can be used to optimize\nhyperparameters and to facilitate model selection. However, this capacity has\nrarely been used to its full extent for Bayesian neural networks, likely\nbecause the approximate posteriors typically used in practice can lack the\nflexibility to effectively bound the marginal likelihood. We therefore explore\nthree aspects of Bayesian learning for deep models: 1) we ask whether it is\nnecessary to perform inference over as many parameters as possible, or whether\nit is reasonable to treat many of them as optimizable hyperparameters; 2) we\npropose a variational posterior that provides a unified view of inference in\nBayesian neural networks and deep Gaussian processes; 3) we demonstrate how VI\ncan be improved in certain deep Gaussian process models by analytically\nremoving symmetries from the posterior, and performing inference on Gram\nmatrices instead of features. We hope that our contributions will provide a\nstepping stone to fully realize the promises of VI in the future.\n']",Bayesian Neural Networks and Deep Learning,Bayesian Neural Networks,Probabilistic Machine Learning,Probabilistic Machine Learning
146,59,146_anonymization_spoofing_hiddenspeaker_anonymized,"['anonymization', 'spoofing', 'hiddenspeaker', 'anonymized', 'adversarial', 'voice', 'spoof', 'spoofed', 'speaker', 'audioseal']","['audio', 'anonymization', 'speaker', 'deepfake', 'speech', 'fake', 'spoofing', 'voice', 'spoofed', 'attacks']","['  It is now well-known that automatic speaker verification (ASV) systems can be\nspoofed using various types of adversaries. The usual approach to counteract\nASV systems against such attacks is to develop a separate spoofing\ncountermeasure (CM) module to classify speech input either as a bonafide, or a\nspoofed utterance. Nevertheless, such a design requires additional computation\nand utilization efforts at the authentication stage. An alternative strategy\ninvolves a single monolithic ASV system designed to handle both zero-effort\nimposter (non-targets) and spoofing attacks. Such spoof-aware ASV systems have\nthe potential to provide stronger protections and more economic computations.\nTo this end, we propose to generalize the standalone ASV (G-SASV) against\nspoofing attacks, where we leverage limited training data from CM to enhance a\nsimple backend in the embedding space, without the involvement of a separate CM\nmodule during the test (authentication) phase. We propose a novel yet simple\nbackend classifier based on deep neural networks and conduct the study via\ndomain adaptation and multi-task integration of spoof embeddings at the\ntraining stage. Experiments are conducted on the ASVspoof 2019 logical access\ndataset, where we improve the performance of statistical ASV backends on the\njoint (bonafide and spoofed) and spoofed conditions by a maximum of 36.2% and\n49.8% in terms of equal error rates, respectively.\n', ""  The growing use of voice user interfaces has led to a surge in the collection\nand storage of speech data. While data collection allows for the development of\nefficient tools powering most speech services, it also poses serious privacy\nissues for users as centralized storage makes private personal speech data\nvulnerable to cyber threats. With the increasing use of voice-based digital\nassistants like Amazon's Alexa, Google's Home, and Apple's Siri, and with the\nincreasing ease with which personal speech data can be collected, the risk of\nmalicious use of voice-cloning and speaker/gender/pathological/etc. recognition\nhas increased.\n  This thesis proposes solutions for anonymizing speech and evaluating the\ndegree of the anonymization. In this work, anonymization refers to making\npersonal speech data unlinkable to an identity while maintaining the usefulness\n(utility) of the speech signal (e.g., access to linguistic content). We start\nby identifying several challenges that evaluation protocols need to consider to\nevaluate the degree of privacy protection properly. We clarify how\nanonymization systems must be configured for evaluation purposes and highlight\nthat many practical deployment configurations do not permit privacy evaluation.\nFurthermore, we study and examine the most common voice conversion-based\nanonymization system and identify its weak points before suggesting new methods\nto overcome some limitations. We isolate all components of the anonymization\nsystem to evaluate the degree of speaker PPI associated with each of them.\nThen, we propose several transformation methods for each component to reduce as\nmuch as possible speaker PPI while maintaining utility. We promote\nanonymization algorithms based on quantization-based transformation as an\nalternative to the most-used and well-known noise-based approach. Finally, we\nendeavor a new attack method to invert anonymization.\n"", '  Privacy-preserving voice protection approaches primarily suppress\nprivacy-related information derived from paralinguistic attributes while\npreserving the linguistic content. Existing solutions focus on single-speaker\nscenarios. However, they lack practicality for real-world applications, i.e.,\nmulti-speaker scenarios. In this paper, we present an initial attempt to\nprovide a multi-speaker anonymization benchmark by defining the task and\nevaluation protocol, proposing benchmarking solutions, and discussing the\nprivacy leakage of overlapping conversations. Specifically, ideal multi-speaker\nanonymization should preserve the number of speakers and the turn-taking\nstructure of the conversation, ensuring accurate context conveyance while\nmaintaining privacy. To achieve that, a cascaded system uses speaker\ndiarization to aggregate the speech of each speaker and speaker anonymization\nto conceal speaker privacy and preserve speech content. Additionally, we\npropose two conversation-level speaker vector anonymization methods to improve\nthe utility further. Both methods aim to make the original and corresponding\npseudo-speaker identities of each speaker unlinkable while preserving or even\nimproving the distinguishability among pseudo-speakers in a conversation. The\nfirst method minimizes the differential similarity across speaker pairs in the\noriginal and anonymized conversations to maintain original speaker\nrelationships in the anonymized version. The other method minimizes the\naggregated similarity across anonymized speakers to achieve better\ndifferentiation between speakers. Experiments conducted on both non-overlap\nsimulated and real-world datasets demonstrate the effectiveness of the\nmulti-speaker anonymization system with the proposed speaker anonymizers.\nAdditionally, we analyzed overlapping speech regarding privacy leakage and\nprovide potential solutions.\n']",Speaker Anonymization and Spoofing Countermeasures,Speaker Privacy and Security in Voice Systems,Information Protection and Concealment,Information Protection and Concealment
147,58,147_diffusionmtl_denoising_diffusion_generative,"['diffusionmtl', 'denoising', 'diffusion', 'generative', 'models', 'noisy', 'training', 'imagenet', 'noise', 'dtr']","['diffusion', 'denoising', 'sampling', 'noise', 'guidance', 'conditional', 'image', 'unconditional', 'consistency', 'schedule']","[""  Diffusion models generate highly realistic images by learning a multi-step\ndenoising process, naturally embodying the principles of multi-task learning\n(MTL). Despite the inherent connection between diffusion models and MTL, there\nremains an unexplored area in designing neural architectures that explicitly\nincorporate MTL into the framework of diffusion models. In this paper, we\npresent Denoising Task Routing (DTR), a simple add-on strategy for existing\ndiffusion model architectures to establish distinct information pathways for\nindividual tasks within a single architecture by selectively activating subsets\nof channels in the model. What makes DTR particularly compelling is its\nseamless integration of prior knowledge of denoising tasks into the framework:\n(1) Task Affinity: DTR activates similar channels for tasks at adjacent\ntimesteps and shifts activated channels as sliding windows through timesteps,\ncapitalizing on the inherent strong affinity between tasks at adjacent\ntimesteps. (2) Task Weights: During the early stages (higher timesteps) of the\ndenoising process, DTR assigns a greater number of task-specific channels,\nleveraging the insight that diffusion models prioritize reconstructing global\nstructure and perceptually rich contents in earlier stages, and focus on simple\nnoise removal in later stages. Our experiments reveal that DTR not only\nconsistently boosts diffusion models' performance across different evaluation\nprotocols without adding extra parameters but also accelerates training\nconvergence. Finally, we show the complementarity between our architectural\napproach and existing MTL optimization techniques, providing a more complete\nview of MTL in the context of diffusion training. Significantly, by leveraging\nthis complementarity, we attain matched performance of DiT-XL using the smaller\nDiT-L with a reduction in training iterations from 7M to 2M.\n"", '  Diffusion-based generative models have emerged as powerful tools in the realm\nof generative modeling. Despite extensive research on denoising across various\ntimesteps and noise levels, a conflict persists regarding the relative\ndifficulties of the denoising tasks. While various studies argue that lower\ntimesteps present more challenging tasks, others contend that higher timesteps\nare more difficult. To address this conflict, our study undertakes a\ncomprehensive examination of task difficulties, focusing on convergence\nbehavior and changes in relative entropy between consecutive probability\ndistributions across timesteps. Our observational study reveals that denoising\nat earlier timesteps poses challenges characterized by slower convergence and\nhigher relative entropy, indicating increased task difficulty at these lower\ntimesteps. Building on these observations, we introduce an easy-to-hard\nlearning scheme, drawing from curriculum learning, to enhance the training\nprocess of diffusion models. By organizing timesteps or noise levels into\nclusters and training models with ascending orders of difficulty, we facilitate\nan order-aware training regime, progressing from easier to harder denoising\ntasks, thereby deviating from the conventional approach of training diffusion\nmodels simultaneously across all timesteps. Our approach leads to improved\nperformance and faster convergence by leveraging benefits of curriculum\nlearning, while maintaining orthogonality with existing improvements in\ndiffusion training techniques. We validate these advantages through\ncomprehensive experiments in image generation tasks, including unconditional,\nclass-conditional, and text-to-image generation.\n', '  Diffusion models have demonstrated remarkable efficacy in various generative\ntasks with the predictive prowess of denoising model. Currently, these models\nemploy a uniform denoising approach across all timesteps. However, the inherent\nvariations in noisy latents at each timestep lead to conflicts during training,\nconstraining the potential of diffusion models. To address this challenge, we\npropose a novel two-stage training strategy termed Step-Adaptive Training. In\nthe initial stage, a base denoising model is trained to encompass all\ntimesteps. Subsequently, we partition the timesteps into distinct groups,\nfine-tuning the model within each group to achieve specialized denoising\ncapabilities. Recognizing that the difficulties of predicting noise at\ndifferent timesteps vary, we introduce a diverse model size requirement. We\ndynamically adjust the model size for each timestep by estimating task\ndifficulty based on its signal-to-noise ratio before fine-tuning. This\nadjustment is facilitated by a proxy-based structural importance assessment\nmechanism, enabling precise and efficient pruning of the base denoising model.\nOur experiments validate the effectiveness of the proposed training strategy,\ndemonstrating an improvement in the FID score on CIFAR10 by over 0.3 while\nutilizing only 80\\% of the computational resources. This innovative approach\nnot only enhances model performance but also significantly reduces\ncomputational costs, opening new avenues for the development and application of\ndiffusion models.\n']",Diffusion Models for Generative Denoising Tasks,Diffusion Models for Generative Tasks,Diffusion Models for Generative Tasks,Diffusion Models for Generative Tasks
148,58,148_distributed_compression_optimization_sgd,"['distributed', 'compression', 'optimization', 'sgd', 'minimizer', 'compressed', 'gradient', 'bottleneck', 'gradients', 'optimal']","['communication', 'compression', 'decentralized', 'convex', 'convergence', 'gradient', 'local', 'stochastic', 'compressors', 'optimization']","['  In the last few years, various communication compression techniques have\nemerged as an indispensable tool helping to alleviate the communication\nbottleneck in distributed learning. However, despite the fact biased\ncompressors often show superior performance in practice when compared to the\nmuch more studied and understood unbiased compressors, very little is known\nabout them. In this work we study three classes of biased compression\noperators, two of which are new, and their performance when applied to\n(stochastic) gradient descent and distributed (stochastic) gradient descent. We\nshow for the first time that biased compressors can lead to linear convergence\nrates both in the single node and distributed settings. We prove that\ndistributed compressed SGD method, employed with error feedback mechanism,\nenjoys the ergodic rate $O\\left( \\delta L \\exp \\left[-\\frac{\\mu K}{\\delta\nL}\\right] + \\frac{(C + \\delta D)}{K\\mu}\\right)$, where $\\delta\\ge 1$ is a\ncompression parameter which grows when more compression is applied, $L$ and\n$\\mu$ are the smoothness and strong convexity constants, $C$ captures\nstochastic gradient noise ($C=0$ if full gradients are computed on each node)\nand $D$ captures the variance of the gradients at the optimum ($D=0$ for\nover-parameterized models). Further, via a theoretical study of several\nsynthetic and empirical distributions of communicated gradients, we shed light\non why and by how much biased compressors outperform their unbiased variants.\nFinally, we propose several new biased compressors with promising theoretical\nguarantees and practical performance.\n', '  Communication compression is a common technique in distributed optimization\nthat can alleviate communication overhead by transmitting compressed gradients\nand model parameters. However, compression can introduce information\ndistortion, which slows down convergence and incurs more communication rounds\nto achieve desired solutions. Given the trade-off between lower per-round\ncommunication costs and additional rounds of communication, it is unclear\nwhether communication compression reduces the total communication cost.\n  This paper explores the conditions under which unbiased compression, a widely\nused form of compression, can reduce the total communication cost, as well as\nthe extent to which it can do so. To this end, we present the first theoretical\nformulation for characterizing the total communication cost in distributed\noptimization with communication compression. We demonstrate that unbiased\ncompression alone does not necessarily save the total communication cost, but\nthis outcome can be achieved if the compressors used by all workers are further\nassumed independent. We establish lower bounds on the communication rounds\nrequired by algorithms using independent unbiased compressors to minimize\nsmooth convex functions and show that these lower bounds are tight by refining\nthe analysis for ADIANA. Our results reveal that using independent unbiased\ncompression can reduce the total communication cost by a factor of up to\n$\\Theta(\\sqrt{\\min\\{n, \\kappa\\}})$ when all local smoothness constants are\nconstrained by a common upper bound, where $n$ is the number of workers and\n$\\kappa$ is the condition number of the functions being minimized. These\ntheoretical findings are supported by experimental results.\n', '  We propose a novel algorithm for distributed stochastic gradient descent\n(SGD) with compressed gradient communication in the parameter-server framework.\nOur gradient compression technique, named flattened one-bit stochastic gradient\ndescent (FO-SGD), relies on two simple algorithmic ideas: (i) a one-bit\nquantization procedure leveraging the technique of dithering, and (ii) a\nrandomized fast Walsh-Hadamard transform to flatten the stochastic gradient\nbefore quantization. As a result, the approximation of the true gradient in\nthis scheme is biased, but it prevents commonly encountered algorithmic\nproblems, such as exploding variance in the one-bit compression regime,\ndeterioration of performance in the case of sparse gradients, and restrictive\nassumptions on the distribution of the stochastic gradients. In fact, we show\nSGD-like convergence guarantees under mild conditions. The compression\ntechnique can be used in both directions of worker-server communication,\ntherefore admitting distributed optimization with full communication\ncompression.\n']",Distributed Optimization with Gradient Compression,Optimization Methods for Distributed Deep Learning,Deep Learning Optimization and Training,Deep Learning Optimization and Security
149,58,149_imitation_reinforcement_mimic_demonstrations,"['imitation', 'reinforcement', 'mimic', 'demonstrations', 'imitate', 'learning', 'adversarial', 'cloning', 'reward', 'learned']","['imitation', 'expert', 'demonstrations', 'reward', 'policy', 'cloning', 'inverse', 'discriminator', 'policies', 'reinforcement']","[""  We focus on offline imitation learning (IL), which aims to mimic an expert's\nbehavior using demonstrations without any interaction with the environment. One\nof the main challenges in offline IL is the limited support of expert\ndemonstrations, which typically cover only a small fraction of the state-action\nspace. While it may not be feasible to obtain numerous expert demonstrations,\nit is often possible to gather a larger set of sub-optimal demonstrations. For\nexample, in treatment optimization problems, there are varying levels of doctor\ntreatments available for different chronic conditions. These range from\ntreatment specialists and experienced general practitioners to less experienced\ngeneral practitioners. Similarly, when robots are trained to imitate humans in\nroutine tasks, they might learn from individuals with different levels of\nexpertise and efficiency.\n  In this paper, we propose an offline IL approach that leverages the larger\nset of sub-optimal demonstrations while effectively mimicking expert\ntrajectories. Existing offline IL methods based on behavior cloning or\ndistribution matching often face issues such as overfitting to the limited set\nof expert demonstrations or inadvertently imitating sub-optimal trajectories\nfrom the larger dataset. Our approach, which is based on inverse soft-Q\nlearning, learns from both expert and sub-optimal demonstrations. It assigns\nhigher importance (through learned weights) to aligning with expert\ndemonstrations and lower importance to aligning with sub-optimal ones. A key\ncontribution of our approach, called SPRINQL, is transforming the offline IL\nproblem into a convex optimization over the space of Q functions. Through\ncomprehensive experimental evaluations, we demonstrate that the SPRINQL\nalgorithm achieves state-of-the-art (SOTA) performance on offline IL\nbenchmarks. Code is available at https://github.com/hmhuy2000/SPRINQL.\n"", '  Imitation learning is often used in addition to reinforcement learning in\nenvironments where reward design is difficult or where the reward is sparse,\nbut it is difficult to be able to imitate well in unknown states from a small\namount of expert data and sampling data. Supervised learning methods such as\nBehavioral Cloning do not require sampling data, but usually suffer from\ndistribution shift. The methods based on reinforcement learning, such as\ninverse reinforcement learning and Generative Adversarial imitation learning\n(GAIL), can learn from only a few expert data. However, they often need to\ninteract with the environment. Soft Q imitation learning (SQIL) addressed the\nproblems, and it was shown that it could learn efficiently by combining\nBehavioral Cloning and soft Q-learning with constant rewards. In order to make\nthis algorithm more robust to distribution shift, we propose more efficient and\nrobust algorithm by adding to this method a reward function based on\nadversarial inverse reinforcement learning that rewards the agent for\nperforming actions in status similar to the demo. We call this algorithm\nDiscriminator Soft Q Imitation Learning (DSQIL). We evaluated it on MuJoCo\nenvironments.\n', ""  In many real-world settings, an agent must learn to act in environments where\nno reward signal can be specified, but a set of expert demonstrations is\navailable. Imitation learning (IL) is a popular framework for learning policies\nfrom such demonstrations. However, in some cases, differences in observability\nbetween the expert and the agent can give rise to an imitation gap such that\nthe expert's policy is not optimal for the agent and a naive application of IL\ncan fail catastrophically. In particular, if the expert observes the Markov\nstate and the agent does not, then the expert will not demonstrate the\ninformation-gathering behavior needed by the agent but not the expert. In this\npaper, we propose a Bayesian solution to the Imitation Gap (BIG), first using\nthe expert demonstrations, together with a prior specifying the cost of\nexploratory behavior that is not demonstrated, to infer a posterior over\nrewards with Bayesian inverse reinforcement learning (IRL). BIG then uses the\nreward posterior to learn a Bayes-optimal policy. Our experiments show that\nBIG, unlike IL, allows the agent to explore at test time when presented with an\nimitation gap, whilst still learning to behave optimally using expert\ndemonstrations when no such gap exists.\n""]",Imitation Learning from Demonstrations,Imitation Learning and Robotics,Robotics and Artificial Intelligence,Robotics and Artificial Intelligence
150,57,150_games_gameplay_chess_ai,"['games', 'gameplay', 'chess', 'ai', 'game', 'strategy', 'gaming', 'poker', 'hearthstone', 'play']","['games', 'game', 'chess', 'poker', 'players', 'imperfect', 'card', 'board', 'player', 'perfect']","['  Games have been the perfect test-beds for artificial intelligence research\nfor the characteristics that widely exist in real-world scenarios. Learning and\noptimisation, decision making in dynamic and uncertain environments, game\ntheory, planning and scheduling, design and education are common research areas\nshared between games and real-world problems. Numerous open-source games or\ngame-based environments have been implemented for studying artificial\nintelligence. In addition to single- or multi-player, collaborative or\nadversarial games, there has also been growing interest in implementing\nplatforms for creative design in recent years. Those platforms provide ideal\nbenchmarks for exploring and comparing artificial intelligence ideas and\ntechniques. This paper reviews the games and game-based platforms for\nartificial intelligence research, provides guidance on matching particular\ntypes of artificial intelligence with suitable games for testing and matching\nparticular needs in games with suitable artificial intelligence techniques,\ndiscusses the research trend induced by the evolution of those games and\nplatforms, and gives an outlook.\n', '  Creating and evaluating games manually is an arduous and laborious task.\nProcedural content generation can aid by creating game artifacts, but usually\nnot an entire game. Evolutionary game design, which combines evolutionary\nalgorithms with automated playtesting, has been used to create novel board\ngames with simple equipment; however, the original approach does not include\ncomplex tabletop games with dice, cards, and maps. This work proposes an\nextension of the approach for tabletop games, evaluating the process by\ngenerating variants of Risk, a military strategy game where players must\nconquer map territories to win. We achieved this using a genetic algorithm to\nevolve the chosen parameters, as well as a rules-based agent to test the games\nand a variety of quality criteria to evaluate the new variations generated. Our\nresults show the creation of new variations of the original game with smaller\nmaps, resulting in shorter matches. Also, the variants produce more balanced\nmatches, maintaining the usual drama. We also identified limitations in the\nprocess, where, in many cases, where the objective function was correctly\npursued, but the generated games were nearly trivial. This work paves the way\ntowards promising research regarding the use of evolutionary game design beyond\nclassic board games.\n', '  Poker is in the family of imperfect information games unlike other games such\nas chess, connect four, etc which are perfect information game instead. While\nmany perfect information games have been solved, no non-trivial imperfect\ninformation game has been solved to date. This makes poker a great test bed for\nArtificial Intelligence research. In this paper we firstly compare Game theory\noptimal poker to Exploitative poker. Secondly, we discuss the intricacies of\nabstraction techniques, betting models, and specific strategies employed by\nsuccessful poker bots like Tartanian[1] and Pluribus[6]. Thirdly, we also\nexplore 2-player vs multi-player games and the limitations that come when\nplaying with more players. Finally, this paper discusses the role of machine\nlearning and theoretical approaches in developing winning strategies and\nsuggests future directions for this rapidly evolving field.\n']",Artificial Intelligence in Games and Gaming,Artificial Intelligence in Sports and Games,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
150,57,150_games_gameplay_chess_ai,"['games', 'gameplay', 'chess', 'ai', 'game', 'strategy', 'gaming', 'poker', 'hearthstone', 'play']","['games', 'game', 'chess', 'poker', 'players', 'imperfect', 'card', 'board', 'player', 'perfect']","['  Games have been the perfect test-beds for artificial intelligence research\nfor the characteristics that widely exist in real-world scenarios. Learning and\noptimisation, decision making in dynamic and uncertain environments, game\ntheory, planning and scheduling, design and education are common research areas\nshared between games and real-world problems. Numerous open-source games or\ngame-based environments have been implemented for studying artificial\nintelligence. In addition to single- or multi-player, collaborative or\nadversarial games, there has also been growing interest in implementing\nplatforms for creative design in recent years. Those platforms provide ideal\nbenchmarks for exploring and comparing artificial intelligence ideas and\ntechniques. This paper reviews the games and game-based platforms for\nartificial intelligence research, provides guidance on matching particular\ntypes of artificial intelligence with suitable games for testing and matching\nparticular needs in games with suitable artificial intelligence techniques,\ndiscusses the research trend induced by the evolution of those games and\nplatforms, and gives an outlook.\n', '  Creating and evaluating games manually is an arduous and laborious task.\nProcedural content generation can aid by creating game artifacts, but usually\nnot an entire game. Evolutionary game design, which combines evolutionary\nalgorithms with automated playtesting, has been used to create novel board\ngames with simple equipment; however, the original approach does not include\ncomplex tabletop games with dice, cards, and maps. This work proposes an\nextension of the approach for tabletop games, evaluating the process by\ngenerating variants of Risk, a military strategy game where players must\nconquer map territories to win. We achieved this using a genetic algorithm to\nevolve the chosen parameters, as well as a rules-based agent to test the games\nand a variety of quality criteria to evaluate the new variations generated. Our\nresults show the creation of new variations of the original game with smaller\nmaps, resulting in shorter matches. Also, the variants produce more balanced\nmatches, maintaining the usual drama. We also identified limitations in the\nprocess, where, in many cases, where the objective function was correctly\npursued, but the generated games were nearly trivial. This work paves the way\ntowards promising research regarding the use of evolutionary game design beyond\nclassic board games.\n', '  Poker is in the family of imperfect information games unlike other games such\nas chess, connect four, etc which are perfect information game instead. While\nmany perfect information games have been solved, no non-trivial imperfect\ninformation game has been solved to date. This makes poker a great test bed for\nArtificial Intelligence research. In this paper we firstly compare Game theory\noptimal poker to Exploitative poker. Secondly, we discuss the intricacies of\nabstraction techniques, betting models, and specific strategies employed by\nsuccessful poker bots like Tartanian[1] and Pluribus[6]. Thirdly, we also\nexplore 2-player vs multi-player games and the limitations that come when\nplaying with more players. Finally, this paper discusses the role of machine\nlearning and theoretical approaches in developing winning strategies and\nsuggests future directions for this rapidly evolving field.\n']",Artificial Intelligence in Games and Gaming,Artificial Intelligence in Sports and Games,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
151,57,151_adversarial_adversary_adversaries_malicious,"['adversarial', 'adversary', 'adversaries', 'malicious', 'attacks', 'vulnerabilities', 'attacking', 'threat', 'attack', 'vulnerability']","['adversarial', 'attacks', 'attack', 'examples', 'robustness', 'perturbations', 'word', 'defense', 'perturbation', 'vulnerable']","['  Recent studies have revealed the vulnerability of pre-trained language models\nto adversarial attacks. Existing adversarial defense techniques attempt to\nreconstruct adversarial examples within feature or text spaces. However, these\nmethods struggle to effectively repair the semantics in adversarial examples,\nresulting in unsatisfactory performance and limiting their practical utility.\nTo repair the semantics in adversarial examples, we introduce a novel approach\nnamed Reactive Perturbation Defocusing (Rapid). Rapid employs an adversarial\ndetector to identify fake labels of adversarial examples and leverage\nadversarial attackers to repair the semantics in adversarial examples. Our\nextensive experimental results conducted on four public datasets, convincingly\ndemonstrate the effectiveness of Rapid in various adversarial attack scenarios.\nTo address the problem of defense performance validation in previous works, we\nprovide a demonstration of adversarial detection and repair based on our work,\nwhich can be easily evaluated at https://tinyurl.com/22ercuf8.\n', '  Text classification systems have been proven vulnerable to adversarial text\nexamples, modified versions of the original text examples that are often\nunnoticed by human eyes, yet can force text classification models to alter\ntheir classification. Often, research works quantifying the impact of\nadversarial text attacks have been applied only to models trained in English.\nIn this paper, we introduce the first word-level study of adversarial attacks\nin Arabic. Specifically, we use a synonym (word-level) attack using a Masked\nLanguage Modeling (MLM) task with a BERT model in a black-box setting to assess\nthe robustness of the state-of-the-art text classification models to\nadversarial attacks in Arabic. To evaluate the grammatical and semantic\nsimilarities of the newly produced adversarial examples using our synonym\nBERT-based attack, we invite four human evaluators to assess and compare the\nproduced adversarial examples with their original examples. We also study the\ntransferability of these newly produced Arabic adversarial examples to various\nmodels and investigate the effectiveness of defense mechanisms against these\nadversarial examples on the BERT models. We find that fine-tuned BERT models\nwere more susceptible to our synonym attacks than the other Deep Neural\nNetworks (DNN) models like WordCNN and WordLSTM we trained. We also find that\nfine-tuned BERT models were more susceptible to transferred attacks. We,\nlastly, find that fine-tuned BERT models successfully regain at least 2% in\naccuracy after applying adversarial training as an initial defense mechanism.\n', '  Deep neural networks have been proven to be vulnerable to adversarial\nexamples and various methods have been proposed to defend against adversarial\nattacks for natural language processing tasks. However, previous defense\nmethods have limitations in maintaining effective defense while ensuring the\nperformance of the original task. In this paper, we propose a malicious\nperturbation based adversarial training method (MPAT) for building robust deep\nneural networks against textual adversarial attacks. Specifically, we construct\na multi-level malicious example generation strategy to generate adversarial\nexamples with malicious perturbations, which are used instead of original\ninputs for model training. Additionally, we employ a novel training objective\nfunction to ensure achieving the defense goal without compromising the\nperformance on the original task. We conduct comprehensive experiments to\nevaluate our defense method by attacking five victim models on three benchmark\ndatasets. The result demonstrates that our method is more effective against\nmalicious adversarial attacks compared with previous defense methods while\nmaintaining or further improving the performance on the original task.\n']",Adversarial Attacks on NLP Models,Adversarial Attacks and Defenses in AI and Machine Learning Models,Adversarial Attacks and Vulnerabilities in AI and Machine Learning,Adversarial Attacks and Vulnerabilities in AI and Machine Learning
152,56,152_signwriting_gestures_gesture_signbank,"['signwriting', 'gestures', 'gesture', 'signbank', 'sign', 'signllm', 'signcl', 'signclip', 'signers', 'signing']","['sign', 'deaf', 'translation', 'hearing', 'signs', 'videos', 'signers', 'language', 'signer', 'languages']","['  Even for better-studied sign languages like American Sign Language (ASL),\ndata is the bottleneck for machine learning research. The situation is worse\nyet for the many other sign languages used by Deaf/Hard of Hearing communities\naround the world. In this paper, we present YouTube-SL-25, a large-scale,\nopen-domain multilingual corpus of sign language videos with seemingly\nwell-aligned captions drawn from YouTube. With >3000 hours of videos across >25\nsign languages, YouTube-SL-25 is a) >3x the size of YouTube-ASL, b) the largest\nparallel sign language dataset to date, and c) the first or largest parallel\ndataset for many of its component languages. We provide baselines for\nsign-to-text tasks using a unified multilingual multitask model based on T5 and\nreport scores on benchmarks across 4 sign languages. The results demonstrate\nthat multilingual transfer benefits both higher- and lower-resource sign\nlanguages within YouTube-SL-25.\n', '  Sign language understanding has made significant strides; however, there is\nstill no viable solution for generating sign sequences directly from entire\nspoken content, e.g., text or speech. In this paper, we propose a unified\nframework for continuous sign language production, easing communication between\nsign and non-sign language users. In particular, a sequence diffusion model,\nutilizing embeddings extracted from text or speech, is crafted to generate sign\npredictions step by step. Moreover, by creating a joint embedding space for\ntext, audio, and sign, we bind these modalities and leverage the semantic\nconsistency among them to provide informative feedback for the model training.\nThis embedding-consistency learning strategy minimizes the reliance on sign\ntriplets and ensures continuous model refinement, even with a missing audio\nmodality. Experiments on How2Sign and PHOENIX14T datasets demonstrate that our\nmodel achieves competitive performance in sign language production.\n', '  Sign Language Translation (SLT) is a challenging task that aims to translate\nsign videos into spoken language. Inspired by the strong translation\ncapabilities of large language models (LLMs) that are trained on extensive\nmultilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT.\nIn this paper, we regularize the sign videos to embody linguistic\ncharacteristics of spoken language, and propose a novel SignLLM framework to\ntransform sign videos into a language-like representation for improved\nreadability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The\nVector-Quantized Visual Sign module converts sign videos into a sequence of\ndiscrete character-level sign tokens, and (2) the Codebook Reconstruction and\nAlignment module converts these character-level tokens into word-level sign\nrepresentations using an optimal transport formulation. A sign-text alignment\nloss further bridges the gap between sign and text tokens, enhancing semantic\ncompatibility. We achieve state-of-the-art gloss-free results on two\nwidely-used SLT benchmarks.\n']",Sign Language Processing and Translation,Multimodal Human-Computer Interaction and Communication,Multimodal Learning and Applications,Multimodal Learning and Vision-Language Models
153,56,153_watermarking_watermark_watermarked_watermarks,"['watermarking', 'watermark', 'watermarked', 'watermarks', 'postmark', 'text', 'steganographic', 'tokens', 'steganography', 'embedding']","['watermarking', 'watermark', 'watermarks', 'watermarked', 'text', 'detectable', 'robustness', 'tokens', 'green', 'secret']","['  Recent advancements of large language models (LLMs) have resulted in\nindistinguishable text outputs comparable to human-generated text. Watermarking\nalgorithms are potential tools that offer a way to differentiate between LLM-\nand human-generated text by embedding detectable signatures within\nLLM-generated output. However, current watermarking schemes lack robustness\nagainst known attacks against watermarking algorithms. In addition, they are\nimpractical considering an LLM generates tens of thousands of text outputs per\nday and the watermarking algorithm needs to memorize each output it generates\nfor the detection to work. In this work, focusing on the limitations of current\nwatermarking schemes, we propose the concept of a ""topic-based watermarking\nalgorithm"" for LLMs. The proposed algorithm determines how to generate tokens\nfor the watermarked LLM output based on extracted topics of an input prompt or\nthe output of a non-watermarked LLM. Inspired from previous work, we propose\nusing a pair of lists (that are generated based on the specified extracted\ntopic(s)) that specify certain tokens to be included or excluded while\ngenerating the watermarked output of the LLM. Using the proposed watermarking\nalgorithm, we show the practicality of a watermark detection algorithm.\nFurthermore, we discuss a wide range of attacks that can emerge against\nwatermarking algorithms for LLMs and the benefit of the proposed watermarking\nscheme for the feasibility of modeling a potential attacker considering its\nbenefit vs. loss.\n', '  With the widespread adoption of Large Language Models (LLMs), concerns about\npotential misuse have emerged. To this end, watermarking has been adapted to\nLLM, enabling a simple and effective way to detect and monitor generated text.\nHowever, while the existing methods can differentiate between watermarked and\nunwatermarked text with high accuracy, they often face a trade-off between the\nquality of the generated text and the effectiveness of the watermarking\nprocess. In this work, we present a novel type of LLM watermark, Sparse\nWatermark, which aims to mitigate this trade-off by applying watermarks to a\nsmall subset of generated tokens distributed across the text. The key strategy\ninvolves anchoring watermarked tokens to words that have specific\nPart-of-Speech (POS) tags. Our experimental results demonstrate that the\nproposed watermarking scheme achieves high detectability while generating text\nthat outperforms previous LLM watermarking methods in quality across various\ntasks\n', '  Watermarking of language model outputs enables statistical detection of\nmodel-generated text, which can mitigate harms and misuses of language models.\nExisting watermarking strategies operate by altering the decoder of an existing\nlanguage model. In this paper, we ask whether language models can directly\nlearn to generate watermarked text, which would have significant implications\nfor the real-world deployment of watermarks. First, learned watermarks could be\nused to build open models that naturally generate watermarked text, enabling\nwatermarking for open models, where users can control the decoding procedure.\nSecond, if watermarking is used to determine the provenance of generated text,\nan adversary can hurt the reputation of a victim model by spoofing its\nwatermark and generating damaging watermarked text. To investigate the\nlearnability of watermarks, we propose watermark distillation, which trains a\nstudent model to behave like a teacher model that uses decoding-based\nwatermarking. We test our approach on three decoding-based watermarking\nstrategies and various hyperparameter settings, finding that models can learn\nto generate watermarked text with high detectability. We also find limitations\nto learnability, including the loss of watermarking capabilities under\nfine-tuning on normal text and high sample complexity when learning\nlow-distortion watermarks.\n']",Watermarking Techniques for Large Language Models,"Large Language Models: Safety, Security, and Ethics",Large Language Models,Large Language Models
153,56,153_watermarking_watermark_watermarked_watermarks,"['watermarking', 'watermark', 'watermarked', 'watermarks', 'postmark', 'text', 'steganographic', 'tokens', 'steganography', 'embedding']","['watermarking', 'watermark', 'watermarks', 'watermarked', 'text', 'detectable', 'robustness', 'tokens', 'green', 'secret']","['  Recent advancements of large language models (LLMs) have resulted in\nindistinguishable text outputs comparable to human-generated text. Watermarking\nalgorithms are potential tools that offer a way to differentiate between LLM-\nand human-generated text by embedding detectable signatures within\nLLM-generated output. However, current watermarking schemes lack robustness\nagainst known attacks against watermarking algorithms. In addition, they are\nimpractical considering an LLM generates tens of thousands of text outputs per\nday and the watermarking algorithm needs to memorize each output it generates\nfor the detection to work. In this work, focusing on the limitations of current\nwatermarking schemes, we propose the concept of a ""topic-based watermarking\nalgorithm"" for LLMs. The proposed algorithm determines how to generate tokens\nfor the watermarked LLM output based on extracted topics of an input prompt or\nthe output of a non-watermarked LLM. Inspired from previous work, we propose\nusing a pair of lists (that are generated based on the specified extracted\ntopic(s)) that specify certain tokens to be included or excluded while\ngenerating the watermarked output of the LLM. Using the proposed watermarking\nalgorithm, we show the practicality of a watermark detection algorithm.\nFurthermore, we discuss a wide range of attacks that can emerge against\nwatermarking algorithms for LLMs and the benefit of the proposed watermarking\nscheme for the feasibility of modeling a potential attacker considering its\nbenefit vs. loss.\n', '  With the widespread adoption of Large Language Models (LLMs), concerns about\npotential misuse have emerged. To this end, watermarking has been adapted to\nLLM, enabling a simple and effective way to detect and monitor generated text.\nHowever, while the existing methods can differentiate between watermarked and\nunwatermarked text with high accuracy, they often face a trade-off between the\nquality of the generated text and the effectiveness of the watermarking\nprocess. In this work, we present a novel type of LLM watermark, Sparse\nWatermark, which aims to mitigate this trade-off by applying watermarks to a\nsmall subset of generated tokens distributed across the text. The key strategy\ninvolves anchoring watermarked tokens to words that have specific\nPart-of-Speech (POS) tags. Our experimental results demonstrate that the\nproposed watermarking scheme achieves high detectability while generating text\nthat outperforms previous LLM watermarking methods in quality across various\ntasks\n', '  Watermarking of language model outputs enables statistical detection of\nmodel-generated text, which can mitigate harms and misuses of language models.\nExisting watermarking strategies operate by altering the decoder of an existing\nlanguage model. In this paper, we ask whether language models can directly\nlearn to generate watermarked text, which would have significant implications\nfor the real-world deployment of watermarks. First, learned watermarks could be\nused to build open models that naturally generate watermarked text, enabling\nwatermarking for open models, where users can control the decoding procedure.\nSecond, if watermarking is used to determine the provenance of generated text,\nan adversary can hurt the reputation of a victim model by spoofing its\nwatermark and generating damaging watermarked text. To investigate the\nlearnability of watermarks, we propose watermark distillation, which trains a\nstudent model to behave like a teacher model that uses decoding-based\nwatermarking. We test our approach on three decoding-based watermarking\nstrategies and various hyperparameter settings, finding that models can learn\nto generate watermarked text with high detectability. We also find limitations\nto learnability, including the loss of watermarking capabilities under\nfine-tuning on normal text and high sample complexity when learning\nlow-distortion watermarks.\n']",Watermarking Techniques for Large Language Models,"Large Language Models: Safety, Security, and Ethics",Large Language Models,Large Language Models
154,55,154_wasserstein_metrics_distances_metric,"['wasserstein', 'metrics', 'distances', 'metric', 'distance', 'measures', 'gaussian', 'distributions', 'optimal', 'embedding']","['distance', 'sliced', 'distances', 'manifold', 'metric', 'measures', 'transport', 'probability', 'distributions', 'spaces']","[""  The $2$-Wasserstein distance is sensitive to minor geometric differences\nbetween distributions, making it a very powerful dissimilarity metric. However,\ndue to this sensitivity, a small outlier mass can also cause a significant\nincrease in the $2$-Wasserstein distance between two similar distributions.\nSimilarly, sampling discrepancy can cause the empirical $2$-Wasserstein\ndistance on $n$ samples in $\\mathbb{R}^2$ to converge to the true distance at a\nrate of $n^{-1/4}$, which is significantly slower than the rate of $n^{-1/2}$\nfor $1$-Wasserstein distance. We introduce a new family of distances\nparameterized by $k \\ge 0$, called $k$-RPW that is based on computing the\npartial $2$-Wasserstein distance. We show that (1) $k$-RPW satisfies the metric\nproperties, (2) $k$-RPW is robust to small outlier mass while retaining the\nsensitivity of $2$-Wasserstein distance to minor geometric differences, and (3)\nwhen $k$ is a constant, $k$-RPW distance between empirical distributions on $n$\nsamples in $\\mathbb{R}^2$ converges to the true distance at a rate of\n$n^{-1/3}$, which is faster than the convergence rate of $n^{-1/4}$ for the\n$2$-Wasserstein distance. Using the partial $p$-Wasserstein distance, we extend\nour distance to any $p \\in [1,\\infty]$. By setting parameters $k$ or $p$\nappropriately, we can reduce our distance to the total variation,\n$p$-Wasserstein, and the L\\'evy-Prokhorov distances. Experiments show that our\ndistance function achieves higher accuracy in comparison to the\n$1$-Wasserstein, $2$-Wasserstein, and TV distances for image retrieval tasks on\nnoisy real-world data sets.\n"", '  The sliced Wasserstein (SW) distances between two probability measures are\ndefined as the expectation of the Wasserstein distance between two\none-dimensional projections of the two measures. The randomness comes from a\nprojecting direction that is used to project the two input measures to one\ndimension. Due to the intractability of the expectation, Monte Carlo\nintegration is performed to estimate the value of the SW distance. Despite\nhaving various variants, there has been no prior work that improves the Monte\nCarlo estimation scheme for the SW distance in terms of controlling its\nvariance. To bridge the literature on variance reduction and the literature on\nthe SW distance, we propose computationally efficient control variates to\nreduce the variance of the empirical estimation of the SW distance. The key\nidea is to first find Gaussian approximations of projected one-dimensional\nmeasures, then we utilize the closed-form of the Wasserstein-2 distance between\ntwo Gaussian distributions to design the control variates. In particular, we\npropose using a lower bound and an upper bound of the Wasserstein-2 distance\nbetween two fitted Gaussians as two computationally efficient control variates.\nWe empirically show that the proposed control variate estimators can help to\nreduce the variance considerably when comparing measures over images and\npoint-clouds. Finally, we demonstrate the favorable performance of the proposed\ncontrol variate estimators in gradient flows to interpolate between two\npoint-clouds and in deep generative modeling on standard image datasets, such\nas CIFAR10 and CelebA.\n', '  While many Machine Learning methods were developed or transposed on\nRiemannian manifolds to tackle data with known non Euclidean geometry, Optimal\nTransport (OT) methods on such spaces have not received much attention. The\nmain OT tool on these spaces is the Wasserstein distance which suffers from a\nheavy computational burden. On Euclidean spaces, a popular alternative is the\nSliced-Wasserstein distance, which leverages a closed-form solution of the\nWasserstein distance in one dimension, but which is not readily available on\nmanifolds. In this work, we derive general constructions of Sliced-Wasserstein\ndistances on Cartan-Hadamard manifolds, Riemannian manifolds with non-positive\ncurvature, which include among others Hyperbolic spaces or the space of\nSymmetric Positive Definite matrices. Then, we propose different applications.\nAdditionally, we derive non-parametric schemes to minimize these new distances\nby approximating their Wasserstein gradient flows.\n']",Wasserstein Distance Metrics,Optimal Transport and Wasserstein Metrics for Machine Learning and Optimization,Machine Learning and Optimization,Machine Learning and Artificial Intelligence
155,55,155_optimal_optimization_bandit_minimize,"['optimal', 'optimization', 'bandit', 'minimize', 'convex', 'minimization', 'adversary', 'convexity', 'regret', 'guarantees']","['regret', 'online', 'convex', 'bounds', 'learner', 'constraint', 'cumulative', 'smoothed', 'algorithms', 'functions']","['  In this paper, we analyze the problem of online convex optimization in\ndifferent settings. We show that any algorithm for online linear optimization\nwith fully adaptive adversaries is an algorithm for online convex optimization.\nWe also show that any such algorithm that requires full-information feedback\nmay be transformed to an algorithm with semi-bandit feedback with comparable\nregret bound. We further show that algorithms that are designed for fully\nadaptive adversaries using deterministic semi-bandit feedback can obtain\nsimilar bounds using only stochastic semi-bandit feedback when facing oblivious\nadversaries. We use this to describe general meta-algorithms to convert first\norder algorithms to zeroth order algorithms with comparable regret bounds. Our\nframework allows us to analyze online optimization in various settings, such\nfull-information feedback, bandit feedback, stochastic regret, adversarial\nregret and various forms of non-stationary regret.\n', '  Online convex optimization (OCO) is a widely used framework in online\nlearning. In each round, the learner chooses a decision in a convex set and an\nadversary chooses a convex loss function, and then the learner suffers the loss\nassociated with their current decision. However, in many applications the\nlearner\'s loss depends not only on the current decision but on the entire\nhistory of decisions until that point. The OCO framework and its existing\ngeneralizations do not capture this, and they can only be applied to many\nsettings of interest after a long series of approximation arguments. They also\nleave open the question of whether the dependence on memory is tight because\nthere are no non-trivial lower bounds. In this work we introduce a\ngeneralization of the OCO framework, ""Online Convex Optimization with Unbounded\nMemory"", that captures long-term dependence on past decisions. We introduce the\nnotion of $p$-effective memory capacity, $H_p$, that quantifies the maximum\ninfluence of past decisions on present losses. We prove an $O(\\sqrt{H_p T})$\nupper bound on the policy regret and a matching (worst-case) lower bound. As a\nspecial case, we prove the first non-trivial lower bound for OCO with finite\nmemory \\citep{anavaHM2015online}, which could be of independent interest, and\nalso improve existing upper bounds. We demonstrate the broad applicability of\nour framework by using it to derive regret bounds, and to improve and simplify\nexisting regret bound derivations, for a variety of online learning problems\nincluding online linear control and an online variant of performative\nprediction.\n', '  In this paper, we explore online convex optimization (OCO) and introduce a\nnew analysis that provides fast rates by exploiting the curvature of feasible\nsets. In online linear optimization, it is known that if the average gradient\nof loss functions is larger than a certain value, the curvature of feasible\nsets can be exploited by the follow-the-leader (FTL) algorithm to achieve a\nlogarithmic regret. This paper reveals that algorithms adaptive to the\ncurvature of loss functions can also leverage the curvature of feasible sets.\nWe first prove that if an optimal decision is on the boundary of a feasible set\nand the gradient of an underlying loss function is non-zero, then the algorithm\nachieves a regret upper bound of $O(\\rho \\log T)$ in stochastic environments.\nHere, $\\rho > 0$ is the radius of the smallest sphere that includes the optimal\ndecision and encloses the feasible set. Our approach, unlike existing ones, can\nwork directly with convex loss functions, exploiting the curvature of loss\nfunctions simultaneously, and can achieve the logarithmic regret only with a\nlocal property of feasible sets. Additionally, it achieves an $O(\\sqrt{T})$\nregret even in adversarial environments where FTL suffers an $\\Omega(T)$\nregret, and attains an $O(\\rho \\log T + \\sqrt{C \\rho \\log T})$ regret bound in\ncorrupted stochastic environments with corruption level $C$. Furthermore, by\nextending our analysis, we establish a regret upper bound of\n$O\\Big(T^{\\frac{q-2}{2(q-1)}} (\\log T)^{\\frac{q}{2(q-1)}}\\Big)$ for\n$q$-uniformly convex feasible sets, where uniformly convex sets include\nstrongly convex sets and $\\ell_p$-balls for $p \\in [1,\\infty)$. This bound\nbridges the gap between the $O(\\log T)$ regret bound for strongly convex sets\n($q=2$) and the $O(\\sqrt{T})$ regret bound for non-curved sets ($q\\to\\infty$).\n']",Online Convex Optimization,Optimization Methods and Algorithms,Optimization and Design,Optimization and Design
156,55,156_counterfactuals_counterfactual_explanations_classifier,"['counterfactuals', 'counterfactual', 'explanations', 'classifier', 'robustness', 'explaining', 'interpretable', 'predictions', 'robust', 'algorithmic']","['counterfactual', 'explanations', 'counterfactuals', 'actionable', 'recourse', 'outcome', 'changes', 'explanation', 'change', 'plausibility']","['  Counterfactual explanations describe how to modify a feature vector in order\nto flip the outcome of a trained classifier. Obtaining robust counterfactual\nexplanations is essential to provide valid algorithmic recourse and meaningful\nexplanations. We study the robustness of explanations of randomized ensembles,\nwhich are always subject to algorithmic uncertainty even when the training data\nis fixed. We formalize the generation of robust counterfactual explanations as\na probabilistic problem and show the link between the robustness of ensemble\nmodels and the robustness of base learners. We develop a practical method with\ngood empirical performance and support it with theoretical guarantees for\nensembles of convex base learners. Our results show that existing methods give\nsurprisingly low robustness: the validity of naive counterfactuals is below\n$50\\%$ on most data sets and can fall to $20\\%$ on problems with many features.\nIn contrast, our method achieves high robustness with only a small increase in\nthe distance from counterfactual explanations to their initial observations.\n', '  The accuracy and understandability of bank failure prediction models are\ncrucial. While interpretable models like logistic regression are favored for\ntheir explainability, complex models such as random forest, support vector\nmachines, and deep learning offer higher predictive performance but lower\nexplainability. These models, known as black boxes, make it difficult to derive\nactionable insights. To address this challenge, using counterfactual\nexplanations is suggested. These explanations demonstrate how changes in input\nvariables can alter the model output and suggest ways to mitigate bank failure\nrisk. The key challenge lies in selecting the most effective method for\ngenerating useful counterfactuals, which should demonstrate validity,\nproximity, sparsity, and plausibility. The paper evaluates several\ncounterfactual generation methods: WhatIf, Multi Objective, and Nearest\nInstance Counterfactual Explanation, and also explores resampling methods like\nundersampling, oversampling, SMOTE, and the cost sensitive approach to address\ndata imbalance in bank failure prediction in the US. The results indicate that\nthe Nearest Instance Counterfactual Explanation method yields higher quality\ncounterfactual explanations, mainly using the cost sensitive approach. Overall,\nthe Multi Objective Counterfactual and Nearest Instance Counterfactual\nExplanation methods outperform others regarding validity, proximity, and\nsparsity metrics, with the cost sensitive approach providing the most desirable\ncounterfactual explanations. These findings highlight the variability in the\nperformance of counterfactual generation methods across different balancing\nstrategies and machine learning models, offering valuable strategies to enhance\nthe utility of black box bank failure prediction models.\n', ""  In the past decade, we have experienced a massive boom in the usage of\ndigital solutions in higher education. Due to this boom, large amounts of data\nhave enabled advanced data analysis methods to support learners and examine\nlearning processes. One of the dominant research directions in learning\nanalytics is predictive modeling of learners' success using various machine\nlearning methods. To build learners' and teachers' trust in such methods and\nsystems, exploring the methods and methodologies that enable relevant\nstakeholders to deeply understand the underlying machine-learning models is\nnecessary. In this context, counterfactual explanations from explainable\nmachine learning tools are promising. Several counterfactual generation methods\nhold much promise, but the features must be actionable and causal to be\neffective. Thus, obtaining which counterfactual generation method suits the\nstudent success prediction models in terms of desiderata, stability, and\nrobustness is essential. Although a few studies have been published in recent\nyears on the use of counterfactual explanations in educational sciences, they\nhave yet to discuss which counterfactual generation method is more suitable for\nthis problem. This paper analyzed the effectiveness of commonly used\ncounterfactual generation methods, such as WhatIf Counterfactual Explanations,\nMulti-Objective Counterfactual Explanations, and Nearest Instance\nCounterfactual Explanations after balancing. This contribution presents a case\nstudy using the Open University Learning Analytics dataset to demonstrate the\npractical usefulness of counterfactual explanations. The results illustrate the\nmethod's effectiveness and describe concrete steps that could be taken to alter\nthe model's prediction.\n""]",Counterfactual Explanations for Machine Learning Models,Causal Analysis and Counterfactual Reasoning,Causal Analysis and Reasoning,Causal Analysis and Reasoning
157,55,157_raman_spectroscopy_microscopy_nanoscale,"['raman', 'spectroscopy', 'microscopy', 'nanoscale', 'microscope', 'spectroscopic', 'nanostructures', 'optical', 'optics', 'imaging']","['optical', 'imaging', 'phase', 'diffraction', 'spectroscopy', 'microscopy', 'electron', 'ray', 'spectra', 'cryo']","['  Phase retrieval, the problem of recovering lost phase information from\nmeasured intensity alone, is an inverse problem that is widely faced in various\nimaging modalities ranging from astronomy to nanoscale imaging. The current\nprocess of phase recovery is iterative in nature. As a result, the image\nformation is time-consuming and computationally expensive, precluding real-time\nimaging. Here, we use 3D nanoscale X-ray imaging as a representative example to\ndevelop a deep learning model to address this phase retrieval problem. We\nintroduce 3D-CDI-NN, a deep convolutional neural network and differential\nprogramming framework trained to predict 3D structure and strain solely from\ninput 3D X-ray coherent scattering data. Our networks are designed to be\n""physics-aware"" in multiple aspects; in that the physics of x-ray scattering\nprocess is explicitly enforced in the training of the network, and the training\ndata are drawn from atomistic simulations that are representative of the\nphysics of the material. We further refine the neural network prediction\nthrough a physics-based optimization procedure to enable maximum accuracy at\nlowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction\npattern to real-space structure and strain hundreds of times faster than\ntraditional iterative phase retrieval methods, with negligible loss in\naccuracy. Our integrated machine learning and differential programming solution\nto the phase retrieval problem is broadly applicable across inverse problems in\nother application areas.\n', '  Raman spectroscopy, a photonic modality based on the inelastic backscattering\nof coherent light, is a valuable asset to the intraoperative sensing space,\noffering non-ionizing potential and highly-specific molecular fingerprint-like\nspectroscopic signatures that can be used for diagnosis of pathological tissue\nin the dynamic surgical field. Though Raman suffers from weakness in intensity,\nSurface-Enhanced Raman Spectroscopy (SERS), which uses metal nanostructures to\namplify Raman signals, can achieve detection sensitivities that rival\ntraditional photonic modalities. In this study, we outline a robotic Raman\nsystem that can reliably pinpoint the location and boundaries of a tumor\nembedded in healthy tissue, modeled here as a tissue-mimicking phantom with\nselectively infused Gold Nanostar regions. Further, due to the relative dearth\nof collected biological SERS or Raman data, we implement transfer learning to\nachieve 100% validation classification accuracy for Gold Nanostars compared to\nControl Agarose, thus providing a proof-of-concept for Raman-based deep\nlearning training pipelines. We reconstruct a surgical field of 30x60mm in 10.2\nminutes, and achieve 98.2% accuracy, preserving relative measurements between\nfeatures in the phantom. We also achieve an 84.3% Intersection-over-Union\nscore, which is the extent of overlap between the ground truth and predicted\nreconstructions. Lastly, we also demonstrate that the Raman system and\nclassification algorithm do not discern based on sample color, but instead on\npresence of SERS agents. This study provides a crucial step in the translation\nof intelligent Raman systems in intraoperative oncological spaces.\n', '  Accurate detection and analysis of traces of persistent organic pollutants in\nwater is important in many areas, including environmental monitoring and food\nquality control, due to their long environmental stability and potential\nbioaccumulation. While conventional analysis of organic pollutants requires\nexpensive equipment, surface enhanced Raman spectroscopy (SERS) has\ndemonstrated great potential for accurate detection of these contaminants.\nHowever, SERS analytical difficulties, such as spectral preprocessing,\ndenoising, and substrate-based spectral variation, have hindered widespread use\nof the technique. Here, we demonstrate an approach for predicting the\nconcentration of sample pollutants from messy, unprocessed Raman data using\nmachine learning. Frequency domain transform methods, including the Fourier and\nWalsh Hadamard transforms, are applied to sets of Raman spectra of three model\nmicropollutants in water (rhodamine 6G, chlorpyrifos, and triclosan), which are\nthen used to train machine learning algorithms. Using standard machine learning\nmodels, the concentration of sample pollutants are predicted with more than 80\npercent cross-validation accuracy from raw Raman data. cross-validation\naccuracy of 85 percent was achieved using deep learning for a moderately sized\ndataset (100 spectra), and 70 to 80 percent cross-validation accuracy was\nachieved even for very small datasets (50 spectra). Additionally, standard\nmodels were shown to accurately identify characteristic peaks via analysis of\ntheir importance scores. The approach shown here has the potential to be\napplied to facilitate accurate detection and analysis of persistent organic\npollutants by surface-enhanced Raman spectroscopy.\n']",Optical Imaging and Spectroscopy for Nanoscale Analysis,"Optical and Photonic Technologies for Imaging, Sensing, and Computing","Advanced Technologies for Simulation, Sensing, and Fabrication","Advanced Technologies for Simulation, Sensing, and Fabrication"
157,55,157_raman_spectroscopy_microscopy_nanoscale,"['raman', 'spectroscopy', 'microscopy', 'nanoscale', 'microscope', 'spectroscopic', 'nanostructures', 'optical', 'optics', 'imaging']","['optical', 'imaging', 'phase', 'diffraction', 'spectroscopy', 'microscopy', 'electron', 'ray', 'spectra', 'cryo']","['  Phase retrieval, the problem of recovering lost phase information from\nmeasured intensity alone, is an inverse problem that is widely faced in various\nimaging modalities ranging from astronomy to nanoscale imaging. The current\nprocess of phase recovery is iterative in nature. As a result, the image\nformation is time-consuming and computationally expensive, precluding real-time\nimaging. Here, we use 3D nanoscale X-ray imaging as a representative example to\ndevelop a deep learning model to address this phase retrieval problem. We\nintroduce 3D-CDI-NN, a deep convolutional neural network and differential\nprogramming framework trained to predict 3D structure and strain solely from\ninput 3D X-ray coherent scattering data. Our networks are designed to be\n""physics-aware"" in multiple aspects; in that the physics of x-ray scattering\nprocess is explicitly enforced in the training of the network, and the training\ndata are drawn from atomistic simulations that are representative of the\nphysics of the material. We further refine the neural network prediction\nthrough a physics-based optimization procedure to enable maximum accuracy at\nlowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction\npattern to real-space structure and strain hundreds of times faster than\ntraditional iterative phase retrieval methods, with negligible loss in\naccuracy. Our integrated machine learning and differential programming solution\nto the phase retrieval problem is broadly applicable across inverse problems in\nother application areas.\n', '  Raman spectroscopy, a photonic modality based on the inelastic backscattering\nof coherent light, is a valuable asset to the intraoperative sensing space,\noffering non-ionizing potential and highly-specific molecular fingerprint-like\nspectroscopic signatures that can be used for diagnosis of pathological tissue\nin the dynamic surgical field. Though Raman suffers from weakness in intensity,\nSurface-Enhanced Raman Spectroscopy (SERS), which uses metal nanostructures to\namplify Raman signals, can achieve detection sensitivities that rival\ntraditional photonic modalities. In this study, we outline a robotic Raman\nsystem that can reliably pinpoint the location and boundaries of a tumor\nembedded in healthy tissue, modeled here as a tissue-mimicking phantom with\nselectively infused Gold Nanostar regions. Further, due to the relative dearth\nof collected biological SERS or Raman data, we implement transfer learning to\nachieve 100% validation classification accuracy for Gold Nanostars compared to\nControl Agarose, thus providing a proof-of-concept for Raman-based deep\nlearning training pipelines. We reconstruct a surgical field of 30x60mm in 10.2\nminutes, and achieve 98.2% accuracy, preserving relative measurements between\nfeatures in the phantom. We also achieve an 84.3% Intersection-over-Union\nscore, which is the extent of overlap between the ground truth and predicted\nreconstructions. Lastly, we also demonstrate that the Raman system and\nclassification algorithm do not discern based on sample color, but instead on\npresence of SERS agents. This study provides a crucial step in the translation\nof intelligent Raman systems in intraoperative oncological spaces.\n', '  Accurate detection and analysis of traces of persistent organic pollutants in\nwater is important in many areas, including environmental monitoring and food\nquality control, due to their long environmental stability and potential\nbioaccumulation. While conventional analysis of organic pollutants requires\nexpensive equipment, surface enhanced Raman spectroscopy (SERS) has\ndemonstrated great potential for accurate detection of these contaminants.\nHowever, SERS analytical difficulties, such as spectral preprocessing,\ndenoising, and substrate-based spectral variation, have hindered widespread use\nof the technique. Here, we demonstrate an approach for predicting the\nconcentration of sample pollutants from messy, unprocessed Raman data using\nmachine learning. Frequency domain transform methods, including the Fourier and\nWalsh Hadamard transforms, are applied to sets of Raman spectra of three model\nmicropollutants in water (rhodamine 6G, chlorpyrifos, and triclosan), which are\nthen used to train machine learning algorithms. Using standard machine learning\nmodels, the concentration of sample pollutants are predicted with more than 80\npercent cross-validation accuracy from raw Raman data. cross-validation\naccuracy of 85 percent was achieved using deep learning for a moderately sized\ndataset (100 spectra), and 70 to 80 percent cross-validation accuracy was\nachieved even for very small datasets (50 spectra). Additionally, standard\nmodels were shown to accurately identify characteristic peaks via analysis of\ntheir importance scores. The approach shown here has the potential to be\napplied to facilitate accurate detection and analysis of persistent organic\npollutants by surface-enhanced Raman spectroscopy.\n']",Optical Imaging and Spectroscopy for Nanoscale Analysis,"Optical and Photonic Technologies for Imaging, Sensing, and Computing","Advanced Technologies for Simulation, Sensing, and Fabrication","Advanced Technologies for Simulation, Sensing, and Fabrication"
157,55,157_raman_spectroscopy_microscopy_nanoscale,"['raman', 'spectroscopy', 'microscopy', 'nanoscale', 'microscope', 'spectroscopic', 'nanostructures', 'optical', 'optics', 'imaging']","['optical', 'imaging', 'phase', 'diffraction', 'spectroscopy', 'microscopy', 'electron', 'ray', 'spectra', 'cryo']","['  Phase retrieval, the problem of recovering lost phase information from\nmeasured intensity alone, is an inverse problem that is widely faced in various\nimaging modalities ranging from astronomy to nanoscale imaging. The current\nprocess of phase recovery is iterative in nature. As a result, the image\nformation is time-consuming and computationally expensive, precluding real-time\nimaging. Here, we use 3D nanoscale X-ray imaging as a representative example to\ndevelop a deep learning model to address this phase retrieval problem. We\nintroduce 3D-CDI-NN, a deep convolutional neural network and differential\nprogramming framework trained to predict 3D structure and strain solely from\ninput 3D X-ray coherent scattering data. Our networks are designed to be\n""physics-aware"" in multiple aspects; in that the physics of x-ray scattering\nprocess is explicitly enforced in the training of the network, and the training\ndata are drawn from atomistic simulations that are representative of the\nphysics of the material. We further refine the neural network prediction\nthrough a physics-based optimization procedure to enable maximum accuracy at\nlowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction\npattern to real-space structure and strain hundreds of times faster than\ntraditional iterative phase retrieval methods, with negligible loss in\naccuracy. Our integrated machine learning and differential programming solution\nto the phase retrieval problem is broadly applicable across inverse problems in\nother application areas.\n', '  Raman spectroscopy, a photonic modality based on the inelastic backscattering\nof coherent light, is a valuable asset to the intraoperative sensing space,\noffering non-ionizing potential and highly-specific molecular fingerprint-like\nspectroscopic signatures that can be used for diagnosis of pathological tissue\nin the dynamic surgical field. Though Raman suffers from weakness in intensity,\nSurface-Enhanced Raman Spectroscopy (SERS), which uses metal nanostructures to\namplify Raman signals, can achieve detection sensitivities that rival\ntraditional photonic modalities. In this study, we outline a robotic Raman\nsystem that can reliably pinpoint the location and boundaries of a tumor\nembedded in healthy tissue, modeled here as a tissue-mimicking phantom with\nselectively infused Gold Nanostar regions. Further, due to the relative dearth\nof collected biological SERS or Raman data, we implement transfer learning to\nachieve 100% validation classification accuracy for Gold Nanostars compared to\nControl Agarose, thus providing a proof-of-concept for Raman-based deep\nlearning training pipelines. We reconstruct a surgical field of 30x60mm in 10.2\nminutes, and achieve 98.2% accuracy, preserving relative measurements between\nfeatures in the phantom. We also achieve an 84.3% Intersection-over-Union\nscore, which is the extent of overlap between the ground truth and predicted\nreconstructions. Lastly, we also demonstrate that the Raman system and\nclassification algorithm do not discern based on sample color, but instead on\npresence of SERS agents. This study provides a crucial step in the translation\nof intelligent Raman systems in intraoperative oncological spaces.\n', '  Accurate detection and analysis of traces of persistent organic pollutants in\nwater is important in many areas, including environmental monitoring and food\nquality control, due to their long environmental stability and potential\nbioaccumulation. While conventional analysis of organic pollutants requires\nexpensive equipment, surface enhanced Raman spectroscopy (SERS) has\ndemonstrated great potential for accurate detection of these contaminants.\nHowever, SERS analytical difficulties, such as spectral preprocessing,\ndenoising, and substrate-based spectral variation, have hindered widespread use\nof the technique. Here, we demonstrate an approach for predicting the\nconcentration of sample pollutants from messy, unprocessed Raman data using\nmachine learning. Frequency domain transform methods, including the Fourier and\nWalsh Hadamard transforms, are applied to sets of Raman spectra of three model\nmicropollutants in water (rhodamine 6G, chlorpyrifos, and triclosan), which are\nthen used to train machine learning algorithms. Using standard machine learning\nmodels, the concentration of sample pollutants are predicted with more than 80\npercent cross-validation accuracy from raw Raman data. cross-validation\naccuracy of 85 percent was achieved using deep learning for a moderately sized\ndataset (100 spectra), and 70 to 80 percent cross-validation accuracy was\nachieved even for very small datasets (50 spectra). Additionally, standard\nmodels were shown to accurately identify characteristic peaks via analysis of\ntheir importance scores. The approach shown here has the potential to be\napplied to facilitate accurate detection and analysis of persistent organic\npollutants by surface-enhanced Raman spectroscopy.\n']",Optical Imaging and Spectroscopy for Nanoscale Analysis,"Optical and Photonic Technologies for Imaging, Sensing, and Computing","Advanced Technologies for Simulation, Sensing, and Fabrication","Advanced Technologies for Simulation, Sensing, and Fabrication"
158,54,158_privacy_anonymization_privacyrestore_anonymized,"['privacy', 'anonymization', 'privacyrestore', 'anonymized', 'anonymisation', 'disclosures', 'security', 'obfuscation', 'disclosure', 'incognitext']","['privacy', 'personal', 'anonymization', 'protection', 'sensitive', 'utility', 'private', 'anonymisation', 'risks', 'policies']","['  Large language models (LLMs), renowned for their impressive capabilities in\nvarious tasks, have significantly advanced artificial intelligence. Yet, these\nadvancements have raised growing concerns about privacy and security\nimplications. To address these issues and explain the risks inherent in these\nmodels, we have devised a three-tiered progressive framework tailored for\nevaluating privacy in language systems. This framework consists of\nprogressively complex and in-depth privacy test tasks at each tier. Our primary\nobjective is to comprehensively evaluate the sensitivity of large language\nmodels to private information, examining how effectively they discern, manage,\nand safeguard sensitive data in diverse scenarios. This systematic evaluation\nhelps us understand the degree to which these models comply with privacy\nprotection guidelines and the effectiveness of their inherent safeguards\nagainst privacy breaches. Our observations indicate that existing Chinese large\nlanguage models universally show privacy protection shortcomings. It seems that\nat the moment this widespread issue is unavoidable and may pose corresponding\nprivacy risks in applications based on these models.\n', ""  Large language models (LLMs) have emerged as powerful tools for tackling\ncomplex tasks across diverse domains, but they also raise privacy concerns when\nfine-tuned on sensitive data due to potential memorization. While differential\nprivacy (DP) offers a promising solution by ensuring models are 'almost\nindistinguishable' with or without any particular privacy unit, current\nevaluations on LLMs mostly treat each example (text record) as the privacy\nunit. This leads to uneven user privacy guarantees when contributions per user\nvary. We therefore study user-level DP motivated by applications where it\nnecessary to ensure uniform privacy protection across users. We present a\nsystematic evaluation of user-level DP for LLM fine-tuning on natural language\ngeneration tasks. Focusing on two mechanisms for achieving user-level DP\nguarantees, Group Privacy and User-wise DP-SGD, we investigate design choices\nlike data selection strategies and parameter tuning for the best\nprivacy-utility tradeoff.\n"", ""  The emergence of large language models (LLMs), and their increased use in\nuser-facing systems, has led to substantial privacy concerns. To date, research\non these privacy concerns has been model-centered: exploring how LLMs lead to\nprivacy risks like memorization, or can be used to infer personal\ncharacteristics about people from their content. We argue that there is a need\nfor more research focusing on the human aspect of these privacy issues: e.g.,\nresearch on how design paradigms for LLMs affect users' disclosure behaviors,\nusers' mental models and preferences for privacy controls, and the design of\ntools, systems, and artifacts that empower end-users to reclaim ownership over\ntheir personal data. To build usable, efficient, and privacy-friendly systems\npowered by these models with imperfect privacy properties, our goal is to\ninitiate discussions to outline an agenda for conducting human-centered\nresearch on privacy issues in LLM-powered systems. This Special Interest Group\n(SIG) aims to bring together researchers with backgrounds in usable security\nand privacy, human-AI collaboration, NLP, or any other related domains to share\ntheir perspectives and experiences on this problem, to help our community\nestablish a collective understanding of the challenges, research opportunities,\nresearch methods, and strategies to collaborate with researchers outside of\nHCI.\n""]",Large Language Models and Privacy Concerns,"Large Language Models: Safety, Security, and Ethics",Large Language Models,Large Language Models
158,54,158_privacy_anonymization_privacyrestore_anonymized,"['privacy', 'anonymization', 'privacyrestore', 'anonymized', 'anonymisation', 'disclosures', 'security', 'obfuscation', 'disclosure', 'incognitext']","['privacy', 'personal', 'anonymization', 'protection', 'sensitive', 'utility', 'private', 'anonymisation', 'risks', 'policies']","['  Large language models (LLMs), renowned for their impressive capabilities in\nvarious tasks, have significantly advanced artificial intelligence. Yet, these\nadvancements have raised growing concerns about privacy and security\nimplications. To address these issues and explain the risks inherent in these\nmodels, we have devised a three-tiered progressive framework tailored for\nevaluating privacy in language systems. This framework consists of\nprogressively complex and in-depth privacy test tasks at each tier. Our primary\nobjective is to comprehensively evaluate the sensitivity of large language\nmodels to private information, examining how effectively they discern, manage,\nand safeguard sensitive data in diverse scenarios. This systematic evaluation\nhelps us understand the degree to which these models comply with privacy\nprotection guidelines and the effectiveness of their inherent safeguards\nagainst privacy breaches. Our observations indicate that existing Chinese large\nlanguage models universally show privacy protection shortcomings. It seems that\nat the moment this widespread issue is unavoidable and may pose corresponding\nprivacy risks in applications based on these models.\n', ""  Large language models (LLMs) have emerged as powerful tools for tackling\ncomplex tasks across diverse domains, but they also raise privacy concerns when\nfine-tuned on sensitive data due to potential memorization. While differential\nprivacy (DP) offers a promising solution by ensuring models are 'almost\nindistinguishable' with or without any particular privacy unit, current\nevaluations on LLMs mostly treat each example (text record) as the privacy\nunit. This leads to uneven user privacy guarantees when contributions per user\nvary. We therefore study user-level DP motivated by applications where it\nnecessary to ensure uniform privacy protection across users. We present a\nsystematic evaluation of user-level DP for LLM fine-tuning on natural language\ngeneration tasks. Focusing on two mechanisms for achieving user-level DP\nguarantees, Group Privacy and User-wise DP-SGD, we investigate design choices\nlike data selection strategies and parameter tuning for the best\nprivacy-utility tradeoff.\n"", ""  The emergence of large language models (LLMs), and their increased use in\nuser-facing systems, has led to substantial privacy concerns. To date, research\non these privacy concerns has been model-centered: exploring how LLMs lead to\nprivacy risks like memorization, or can be used to infer personal\ncharacteristics about people from their content. We argue that there is a need\nfor more research focusing on the human aspect of these privacy issues: e.g.,\nresearch on how design paradigms for LLMs affect users' disclosure behaviors,\nusers' mental models and preferences for privacy controls, and the design of\ntools, systems, and artifacts that empower end-users to reclaim ownership over\ntheir personal data. To build usable, efficient, and privacy-friendly systems\npowered by these models with imperfect privacy properties, our goal is to\ninitiate discussions to outline an agenda for conducting human-centered\nresearch on privacy issues in LLM-powered systems. This Special Interest Group\n(SIG) aims to bring together researchers with backgrounds in usable security\nand privacy, human-AI collaboration, NLP, or any other related domains to share\ntheir perspectives and experiences on this problem, to help our community\nestablish a collective understanding of the challenges, research opportunities,\nresearch methods, and strategies to collaborate with researchers outside of\nHCI.\n""]",Large Language Models and Privacy Concerns,"Large Language Models: Safety, Security, and Ethics",Large Language Models,Large Language Models
159,54,159_multimodal_modality_modal_supervised,"['multimodal', 'modality', 'modal', 'supervised', 'unlearning', 'embeddings', 'unimodal', 'modalities', 'learning', 'learnt']","['multimodal', 'modality', 'modalities', 'unimodal', 'modal', 'fusion', 'missing', 'uni', 'cross', 'information']","['  Multimodal fusion focuses on integrating information from multiple modalities\nwith the goal of more accurate prediction, which has achieved remarkable\nprogress in a wide range of scenarios, including autonomous driving and medical\ndiagnosis. However, the reliability of multimodal fusion remains largely\nunexplored especially under low-quality data settings. This paper surveys the\ncommon challenges and recent advances of multimodal fusion in the wild and\npresents them in a comprehensive taxonomy. From a data-centric view, we\nidentify four main challenges that are faced by multimodal fusion on\nlow-quality data, namely (1) noisy multimodal data that are contaminated with\nheterogeneous noises, (2) incomplete multimodal data that some modalities are\nmissing, (3) imbalanced multimodal data that the qualities or properties of\ndifferent modalities are significantly different and (4) quality-varying\nmultimodal data that the quality of each modality dynamically changes with\nrespect to different samples. This new taxonomy will enable researchers to\nunderstand the state of the field and identify several potential directions. We\nalso provide discussion for the open problems in this field together with\ninteresting future research directions.\n', '  Multimodal learning seeks to utilize data from multiple sources to improve\nthe overall performance of downstream tasks. It is desirable for redundancies\nin the data to make multimodal systems robust to missing or corrupted\nobservations in some correlated modalities. However, we observe that the\nperformance of several existing multimodal networks significantly deteriorates\nif one or multiple modalities are absent at test time. To enable robustness to\nmissing modalities, we propose a simple and parameter-efficient adaptation\nprocedure for pretrained multimodal networks. In particular, we exploit\nmodulation of intermediate features to compensate for the missing modalities.\nWe demonstrate that such adaptation can partially bridge performance drop due\nto missing modalities and outperform independent, dedicated networks trained\nfor the available modality combinations in some cases. The proposed adaptation\nrequires extremely small number of parameters (e.g., fewer than 1% of the total\nparameters) and applicable to a wide range of modality combinations and tasks.\nWe conduct a series of experiments to highlight the missing modality robustness\nof our proposed method on five different multimodal tasks across seven\ndatasets. Our proposed method demonstrates versatility across various tasks and\ndatasets, and outperforms existing methods for robust multimodal learning with\nmissing modalities.\n', '  Multimodal learning typically relies on the assumption that all modalities\nare fully available during both the training and inference phases. However, in\nreal-world scenarios, consistently acquiring complete multimodal data presents\nsignificant challenges due to various factors. This often leads to the issue of\nmissing modalities, where data for certain modalities are absent, posing\nconsiderable obstacles not only for the availability of multimodal pretrained\nmodels but also for their fine-tuning and the preservation of robustness in\ndownstream tasks. To address these challenges, we propose a novel framework\nintegrating parameter-efficient fine-tuning of unimodal pretrained models with\na self-supervised joint-embedding learning method. This framework enables the\nmodel to predict the embedding of a missing modality in the representation\nspace during inference. Our method effectively predicts the missing embedding\nthrough prompt tuning, leveraging information from available modalities. We\nevaluate our approach on several multimodal benchmark datasets and demonstrate\nits effectiveness and robustness across various scenarios of missing\nmodalities.\n']",Multimodal Learning and Fusion,Multimodal Learning and Fusion,Multimodal Learning and Applications,Multimodal Learning and Vision-Language Models
159,54,159_multimodal_modality_modal_supervised,"['multimodal', 'modality', 'modal', 'supervised', 'unlearning', 'embeddings', 'unimodal', 'modalities', 'learning', 'learnt']","['multimodal', 'modality', 'modalities', 'unimodal', 'modal', 'fusion', 'missing', 'uni', 'cross', 'information']","['  Multimodal fusion focuses on integrating information from multiple modalities\nwith the goal of more accurate prediction, which has achieved remarkable\nprogress in a wide range of scenarios, including autonomous driving and medical\ndiagnosis. However, the reliability of multimodal fusion remains largely\nunexplored especially under low-quality data settings. This paper surveys the\ncommon challenges and recent advances of multimodal fusion in the wild and\npresents them in a comprehensive taxonomy. From a data-centric view, we\nidentify four main challenges that are faced by multimodal fusion on\nlow-quality data, namely (1) noisy multimodal data that are contaminated with\nheterogeneous noises, (2) incomplete multimodal data that some modalities are\nmissing, (3) imbalanced multimodal data that the qualities or properties of\ndifferent modalities are significantly different and (4) quality-varying\nmultimodal data that the quality of each modality dynamically changes with\nrespect to different samples. This new taxonomy will enable researchers to\nunderstand the state of the field and identify several potential directions. We\nalso provide discussion for the open problems in this field together with\ninteresting future research directions.\n', '  Multimodal learning seeks to utilize data from multiple sources to improve\nthe overall performance of downstream tasks. It is desirable for redundancies\nin the data to make multimodal systems robust to missing or corrupted\nobservations in some correlated modalities. However, we observe that the\nperformance of several existing multimodal networks significantly deteriorates\nif one or multiple modalities are absent at test time. To enable robustness to\nmissing modalities, we propose a simple and parameter-efficient adaptation\nprocedure for pretrained multimodal networks. In particular, we exploit\nmodulation of intermediate features to compensate for the missing modalities.\nWe demonstrate that such adaptation can partially bridge performance drop due\nto missing modalities and outperform independent, dedicated networks trained\nfor the available modality combinations in some cases. The proposed adaptation\nrequires extremely small number of parameters (e.g., fewer than 1% of the total\nparameters) and applicable to a wide range of modality combinations and tasks.\nWe conduct a series of experiments to highlight the missing modality robustness\nof our proposed method on five different multimodal tasks across seven\ndatasets. Our proposed method demonstrates versatility across various tasks and\ndatasets, and outperforms existing methods for robust multimodal learning with\nmissing modalities.\n', '  Multimodal learning typically relies on the assumption that all modalities\nare fully available during both the training and inference phases. However, in\nreal-world scenarios, consistently acquiring complete multimodal data presents\nsignificant challenges due to various factors. This often leads to the issue of\nmissing modalities, where data for certain modalities are absent, posing\nconsiderable obstacles not only for the availability of multimodal pretrained\nmodels but also for their fine-tuning and the preservation of robustness in\ndownstream tasks. To address these challenges, we propose a novel framework\nintegrating parameter-efficient fine-tuning of unimodal pretrained models with\na self-supervised joint-embedding learning method. This framework enables the\nmodel to predict the embedding of a missing modality in the representation\nspace during inference. Our method effectively predicts the missing embedding\nthrough prompt tuning, leveraging information from available modalities. We\nevaluate our approach on several multimodal benchmark datasets and demonstrate\nits effectiveness and robustness across various scenarios of missing\nmodalities.\n']",Multimodal Learning and Fusion,Multimodal Learning and Fusion,Multimodal Learning and Applications,Multimodal Learning and Vision-Language Models
160,53,160_faces_facial_face_recognition,"['faces', 'facial', 'face', 'recognition', 'classifiers', 'biometrics', 'biometric', 'generative', 'siamese', 'classifier']","['face', 'facial', 'iris', 'biometric', 'age', 'recognition', 'faces', 'images', 'biometrics', 'identity']","['  Computer vision systems have been deployed in various applications involving\nbiometrics like human faces. These systems can identify social media users,\nsearch for missing persons, and verify identity of individuals. While computer\nvision models are often evaluated for accuracy on available benchmarks, more\nannotated data is necessary to learn about their robustness and fairness\nagainst semantic distributional shifts in input data, especially in face data.\nAmong annotated data, counterfactual examples grant strong explainability\ncharacteristics. Because collecting natural face data is prohibitively\nexpensive, we put forth a generative AI-based framework to construct targeted,\ncounterfactual, high-quality synthetic face data. Our synthetic data pipeline\nhas many use cases, including face recognition systems sensitivity evaluations\nand image understanding system probes. The pipeline is validated with multiple\nuser studies. We showcase the efficacy of our face generation pipeline on a\nleading commercial vision model. We identify facial attributes that cause\nvision systems to fail.\n', '  In this work we focus on learning facial representations that can be adapted\nto train effective face recognition models, particularly in the absence of\nlabels. Firstly, compared with existing labelled face datasets, a vastly larger\nmagnitude of unlabeled faces exists in the real world. We explore the learning\nstrategy of these unlabeled facial images through self-supervised pretraining\nto transfer generalized face recognition performance. Moreover, motivated by\none recent finding, that is, the face saliency area is critical for face\nrecognition, in contrast to utilizing random cropped blocks of images for\nconstructing augmentations in pretraining, we utilize patches localized by\nextracted facial landmarks. This enables our method - namely LAndmark-based\nFacial Self-supervised learning LAFS), to learn key representation that is more\ncritical for face recognition. We also incorporate two landmark-specific\naugmentations which introduce more diversity of landmark information to further\nregularize the learning. With learned landmark-based facial representations, we\nfurther adapt the representation for face recognition with regularization\nmitigating variations in landmark positions. Our method achieves significant\nimprovement over the state-of-the-art on multiple face recognition benchmarks,\nespecially on more challenging few-shot scenarios.\n', '  As a significant step for human face modeling, editing, and generation, face\nlandmarking aims at extracting facial keypoints from images. A generalizable\nface landmarker is required in practice because real-world facial images, e.g.,\nthe avatars in animations and games, are often stylized in various ways.\nHowever, achieving generalizable face landmarking is challenging due to the\ndiversity of facial styles and the scarcity of labeled stylized faces. In this\nstudy, we propose a simple but effective paradigm to learn a generalizable face\nlandmarker based on labeled real human faces and unlabeled stylized faces. Our\nmethod learns the face landmarker as the key module of a conditional face\nwarper. Given a pair of real and stylized facial images, the conditional face\nwarper predicts a warping field from the real face to the stylized one, in\nwhich the face landmarker predicts the ending points of the warping field and\nprovides us with high-quality pseudo landmarks for the corresponding stylized\nfacial images. Applying an alternating optimization strategy, we learn the face\nlandmarker to minimize $i)$ the discrepancy between the stylized faces and the\nwarped real ones and $ii)$ the prediction errors of both real and pseudo\nlandmarks. Experiments on various datasets show that our method outperforms\nexisting state-of-the-art domain adaptation methods in face landmarking tasks,\nleading to a face landmarker with better generalizability. Code is available at\nhttps://plustwo0.github.io/project-face-landmarker.\n']",Face Recognition and Biometrics,Computer Vision and Pattern Recognition,Computer Vision,Computer Vision
161,53,161_causalbench_causal_causality_causalnlp,"['causalbench', 'causal', 'causality', 'causalnlp', 'reasoning', 'inference', 'knowledge', 'relational', 'counterfactual', 'entailment']","['causal', 'causality', 'discovery', 'reasoning', 'graphs', 'effect', 'knowledge', 'facts', 'relations', 'relational']","['  This paper explores the causal reasoning of large language models (LLMs) to\nenhance their interpretability and reliability in advancing artificial\nintelligence. Despite the proficiency of LLMs in a range of tasks, their\npotential for understanding causality requires further exploration. We propose\na novel causal attribution model that utilizes ``do-operators"" for constructing\ncounterfactual scenarios, allowing us to systematically quantify the influence\nof input numerical data and LLMs\' pre-existing knowledge on their causal\nreasoning processes. Our newly developed experimental setup assesses LLMs\'\nreliance on contextual information and inherent knowledge across various\ndomains. Our evaluation reveals that LLMs\' causal reasoning ability mainly\ndepends on the context and domain-specific knowledge provided. In the absence\nof such knowledge, LLMs can still maintain a degree of causal reasoning using\nthe available numerical data, albeit with limitations in the calculations. This\nmotivates the proposed fine-tuned LLM for pairwise causal discovery,\neffectively leveraging both knowledge and numerical information.\n', ""  Recent advances in artificial intelligence have seen Large Language Models\n(LLMs) demonstrate notable proficiency in causal discovery tasks. This study\nexplores the factors influencing the performance of LLMs in causal discovery\ntasks. Utilizing open-source LLMs, we examine how the frequency of causal\nrelations within their pre-training corpora affects their ability to accurately\nrespond to causal discovery queries. Our findings reveal that a higher\nfrequency of causal mentions correlates with better model performance,\nsuggesting that extensive exposure to causal information during training\nenhances the models' causal discovery capabilities. Additionally, we\ninvestigate the impact of context on the validity of causal relations. Our\nresults indicate that LLMs might exhibit divergent predictions for identical\ncausal relations when presented in different contexts. This paper provides the\nfirst comprehensive analysis of how different factors contribute to LLM\nperformance in causal discovery tasks.\n"", ""  Large language models (LLMs) have achieved significant success across various\ndomains. However, the inherent complexity of causal problems and causal theory\nposes challenges in accurately describing them in natural language, making it\ndifficult for LLMs to comprehend and use them effectively. Causal methods are\nnot easily conveyed through natural language, which hinders LLMs' ability to\napply them accurately. Additionally, causal datasets are typically tabular,\nwhile LLMs excel in handling natural language data, creating a structural\nmismatch that impedes effective reasoning with tabular data. This lack of\ncausal reasoning capability limits the development of LLMs. To address these\nchallenges, we have equipped the LLM with causal tools within an agent\nframework, named the Causal Agent, enabling it to tackle causal problems. The\ncausal agent comprises tools, memory, and reasoning modules. In the tools\nmodule, the causal agent applies causal methods to align tabular data with\nnatural language. In the reasoning module, the causal agent employs the ReAct\nframework to perform reasoning through multiple iterations with the tools. In\nthe memory module, the causal agent maintains a dictionary instance where the\nkeys are unique names and the values are causal graphs. To verify the causal\nability of the causal agent, we established a benchmark consisting of four\nlevels of causal problems: variable level, edge level, causal graph level, and\ncausal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for\nthese four levels of issues and tested the causal agent on the datasets. Our\nmethodology demonstrates remarkable efficacy on the four-level causal problems,\nwith accuracy rates all above 80%. For further insights and implementation\ndetails, our code is accessible via the GitHub repository\nhttps://github.com/Kairong-Han/Causal_Agent.\n""]",Causal Reasoning in Large Language Models,Large Language Models and Cognitive Abilities,Large Language Models,Large Language Models
161,53,161_causalbench_causal_causality_causalnlp,"['causalbench', 'causal', 'causality', 'causalnlp', 'reasoning', 'inference', 'knowledge', 'relational', 'counterfactual', 'entailment']","['causal', 'causality', 'discovery', 'reasoning', 'graphs', 'effect', 'knowledge', 'facts', 'relations', 'relational']","['  This paper explores the causal reasoning of large language models (LLMs) to\nenhance their interpretability and reliability in advancing artificial\nintelligence. Despite the proficiency of LLMs in a range of tasks, their\npotential for understanding causality requires further exploration. We propose\na novel causal attribution model that utilizes ``do-operators"" for constructing\ncounterfactual scenarios, allowing us to systematically quantify the influence\nof input numerical data and LLMs\' pre-existing knowledge on their causal\nreasoning processes. Our newly developed experimental setup assesses LLMs\'\nreliance on contextual information and inherent knowledge across various\ndomains. Our evaluation reveals that LLMs\' causal reasoning ability mainly\ndepends on the context and domain-specific knowledge provided. In the absence\nof such knowledge, LLMs can still maintain a degree of causal reasoning using\nthe available numerical data, albeit with limitations in the calculations. This\nmotivates the proposed fine-tuned LLM for pairwise causal discovery,\neffectively leveraging both knowledge and numerical information.\n', ""  Recent advances in artificial intelligence have seen Large Language Models\n(LLMs) demonstrate notable proficiency in causal discovery tasks. This study\nexplores the factors influencing the performance of LLMs in causal discovery\ntasks. Utilizing open-source LLMs, we examine how the frequency of causal\nrelations within their pre-training corpora affects their ability to accurately\nrespond to causal discovery queries. Our findings reveal that a higher\nfrequency of causal mentions correlates with better model performance,\nsuggesting that extensive exposure to causal information during training\nenhances the models' causal discovery capabilities. Additionally, we\ninvestigate the impact of context on the validity of causal relations. Our\nresults indicate that LLMs might exhibit divergent predictions for identical\ncausal relations when presented in different contexts. This paper provides the\nfirst comprehensive analysis of how different factors contribute to LLM\nperformance in causal discovery tasks.\n"", ""  Large language models (LLMs) have achieved significant success across various\ndomains. However, the inherent complexity of causal problems and causal theory\nposes challenges in accurately describing them in natural language, making it\ndifficult for LLMs to comprehend and use them effectively. Causal methods are\nnot easily conveyed through natural language, which hinders LLMs' ability to\napply them accurately. Additionally, causal datasets are typically tabular,\nwhile LLMs excel in handling natural language data, creating a structural\nmismatch that impedes effective reasoning with tabular data. This lack of\ncausal reasoning capability limits the development of LLMs. To address these\nchallenges, we have equipped the LLM with causal tools within an agent\nframework, named the Causal Agent, enabling it to tackle causal problems. The\ncausal agent comprises tools, memory, and reasoning modules. In the tools\nmodule, the causal agent applies causal methods to align tabular data with\nnatural language. In the reasoning module, the causal agent employs the ReAct\nframework to perform reasoning through multiple iterations with the tools. In\nthe memory module, the causal agent maintains a dictionary instance where the\nkeys are unique names and the values are causal graphs. To verify the causal\nability of the causal agent, we established a benchmark consisting of four\nlevels of causal problems: variable level, edge level, causal graph level, and\ncausal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for\nthese four levels of issues and tested the causal agent on the datasets. Our\nmethodology demonstrates remarkable efficacy on the four-level causal problems,\nwith accuracy rates all above 80%. For further insights and implementation\ndetails, our code is accessible via the GitHub repository\nhttps://github.com/Kairong-Han/Causal_Agent.\n""]",Causal Reasoning in Large Language Models,Large Language Models and Cognitive Abilities,Large Language Models,Large Language Models
162,52,162_networks_graphs_nodes_rnns,"['networks', 'graphs', 'nodes', 'rnns', 'graph', 'temporal', 'rnn', 'node', 'prediction', 'embeddings']","['temporal', 'dynamic', 'link', 'graphs', 'graph', 'node', 'nodes', 'neighbor', 'evolving', 'networks']","[""  Graphs are a powerful representation tool in machine learning applications,\nwith link prediction being a key task in graph learning. Temporal link\nprediction in dynamic networks is of particular interest due to its potential\nfor solving complex scientific and real-world problems. Traditional approaches\nto temporal link prediction have focused on finding the aggregation of dynamics\nof the network as a unified output. In this study, we propose a novel\nperspective on temporal link prediction by defining nodes as Newtonian objects\nand incorporating the concept of velocity to predict network dynamics. By\ncomputing more specific dynamics of each node, rather than overall dynamics, we\nimprove both accuracy and explainability in predicting future connections. We\ndemonstrate the effectiveness of our approach using two datasets, including 17\nyears of co-authorship data from PubMed. Experimental results show that our\ntemporal graph embedding dynamics approach improves downstream classification\nmodels' ability to predict future collaboration efficacy in co-authorship\nnetworks by 17.34% (AUROC improvement relative to the baseline model).\nFurthermore, our approach offers an interpretable layer over traditional\napproaches to address the temporal link prediction problem.\n"", '  Inductive representation learning on temporal heterogeneous graphs is crucial\nfor scalable deep learning on heterogeneous information networks (HINs) which\nare time-varying, such as citation networks. However, most existing approaches\nare not inductive and thus cannot handle new nodes or edges. Moreover, previous\ntemporal graph embedding methods are often trained with the temporal link\nprediction task to simulate the link formation process of temporal graphs,\nwhile ignoring the evolution of high-order topological structures on temporal\ngraphs. To fill these gaps, we propose a Continuous-Time Representation\nLearning (CTRL) model on temporal HINs. To preserve heterogeneous node features\nand temporal structures, CTRL integrates three parts in a single layer, they\nare 1) a \\emph{heterogeneous attention} unit that measures the semantic\ncorrelation between nodes, 2) a \\emph{edge-based Hawkes process} to capture\ntemporal influence between heterogeneous nodes, and 3) \\emph{dynamic\ncentrality} that indicates the dynamic importance of a node. We train the CTRL\nmodel with a future event (a subgraph) prediction task to capture the evolution\nof the high-order network structure. Extensive experiments have been conducted\non three benchmark datasets. The results demonstrate that our model\nsignificantly boosts performance and outperforms various state-of-the-art\napproaches. Ablation studies are conducted to demonstrate the effectiveness of\nthe model design.\n', ""  Dynamic link prediction is an important problem considered by many recent\nworks proposing various approaches for learning temporal edge patterns. To\nassess their efficacy, models are evaluated on publicly available benchmark\ndatasets involving continuous-time and discrete-time temporal graphs. However,\nas we show in this work, the suitability of common batch-oriented evaluation\ndepends on the datasets' characteristics, which can cause two issues: First,\nfor continuous-time temporal graphs, fixed-size batches create time windows\nwith different durations, resulting in an inconsistent dynamic link prediction\ntask. Second, for discrete-time temporal graphs, the sequence of batches can\nadditionally introduce temporal dependencies that are not present in the data.\nIn this work, we empirically show that this common evaluation approach leads to\nskewed model performance and hinders the fair comparison of methods. We\nmitigate this problem by reformulating dynamic link prediction as a link\nforecasting task that better accounts for temporal information present in the\ndata. We provide implementations of our new evaluation method for commonly used\ngraph learning frameworks.\n""]",Temporal Graph Embeddings and Link Prediction,Graph and Text Embeddings for Representation Learning,Graph Representation Learning and Neural Networks,Graph Representation Learning and Neural Networks
163,52,163_empathy_empatheticdialogues_empathicstories_empathetic,"['empathy', 'empatheticdialogues', 'empathicstories', 'empathetic', 'empathic', 'conversations', 'empathically', 'dialogue', 'conversation', 'emotionbench']","['empathy', 'empathetic', 'emotional', 'empathic', 'emotion', 'emotions', 'dialogue', 'responses', 'conversations', 'affective']","[""  This paper investigates the empathetic responding capabilities of ChatGPT,\nparticularly its latest iteration, GPT-4, in comparison to human-generated\nresponses to a wide range of emotional scenarios, both positive and negative.\nWe employ a rigorous evaluation methodology, involving a between-groups study\nwith 600 participants, to evaluate the level of empathy in responses generated\nby humans and ChatGPT. ChatGPT is prompted in two distinct ways: a standard\napproach and one explicitly detailing empathy's cognitive, affective, and\ncompassionate counterparts. Our findings indicate that the average empathy\nrating of responses generated by ChatGPT exceeds those crafted by humans by\napproximately 10%. Additionally, instructing ChatGPT to incorporate a clear\nunderstanding of empathy in its responses makes the responses align\napproximately 5 times more closely with the expectations of individuals\npossessing a high degree of empathy, compared to human responses. The proposed\nevaluation framework serves as a scalable and adaptable framework to assess the\nempathetic capabilities of newer and updated versions of large language models,\neliminating the need to replicate the current study's results in future\nresearch.\n"", ""  Empathetic response generation, aiming at understanding the user's situation\nand feelings and respond empathically, is crucial in building human-like\ndialogue systems. Previous methods mainly focus on using maximum likelihood\nestimation as the optimization objective for training response generation\nmodels, without taking into account the empathy level alignment between\ngenerated responses and target responses. To this end, we propose an empathetic\nresponse generation using reinforcement learning (EmpRL) framework. The\nframework designs an effective empathy reward function and generates empathetic\nresponses by maximizing the expected reward through reinforcement learning.\nGiven the powerful text generation capability of pre-trained language models,\nEmpRL utilizes the pre-trained T5 model as the generator and conducts further\ntraining to initialize the policy. To align the empathy level between generated\nresponses and target responses in the context, an empathy reward function\ncontaining three empathy communication mechanisms, i.e., emotional reaction,\ninterpretation, and exploration, is constructed using pre-designed and\npre-trained empathy identifiers. Finally, the proximal policy optimization\nalgorithm is used to further train the policy to produce empathetic responses.\nBoth automatic and manual evaluations demonstrate that the proposed EmpRL\nframework can improve the quality of generated responses, enhance the empathy\nlevel similarity between generated and target responses, and produce empathetic\nresponses covering both affective and cognitive aspects.\n"", ""  Empathetic response generation is designed to comprehend the emotions of\nothers and select the most appropriate strategies to assist them in resolving\nemotional challenges. Empathy can be categorized into cognitive empathy and\naffective empathy. The former pertains to the ability to understand and discern\nthe emotional issues and situations of others, while the latter involves the\ncapacity to provide comfort. To enhance one's empathetic abilities, it is\nessential to develop both these aspects. Therefore, we develop an innovative\nframework that combines retrieval augmentation and emotional support strategy\nintegration. Our framework starts with the introduction of a comprehensive\nemotional palette for empathy. We then apply appraisal theory to decompose this\npalette and create a database of empathetic responses. This database serves as\nan external resource and enhances the LLM's empathy by integrating semantic\nretrieval mechanisms. Moreover, our framework places a strong emphasis on the\nproper articulation of response strategies. By incorporating emotional support\nstrategies, we aim to enrich the model's capabilities in both cognitive and\naffective empathy, leading to a more nuanced and comprehensive empathetic\nresponse. Finally, we extract datasets ED and ET from the empathetic dialogue\ndataset \\textsc{EmpatheticDialogues} and ExTES based on dialogue length.\nExperiments demonstrate that our framework can enhance the empathy ability of\nLLMs from both cognitive and affective empathy perspectives. Our code is\nreleased at https://github.com/CAS-SIAT-XinHai/APTNESS.\n""]",Empathetic Response Generation,Conversational AI and Empathy in Human-Computer Interaction,Conversational AI and Human-Computer Interaction,Conversational AI and Human-Computer Interaction
164,52,164_sparse_expert_experts_moe,"['sparse', 'expert', 'experts', 'moe', 'models', 'specialization', 'sparsely', 'moerging', 'training', 'moes']","['experts', 'routing', 'expert', 'mixture', 'sparse', 'routers', 'tokens', 'dense', 'token', 'specialization']","['  Mixture-of-Expert (MoE) based large language models (LLMs), such as the\nrecent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size\nwithout suffering from the quadratic growth of training cost of dense\ntransformers. Like dense models, training MoEs requires answering the same\nquestion: given a training budget, what is the optimal allocation on the model\nsize and number of tokens? We study the scaling law of MoE-based LLMs regarding\nthe relations between the model performance, model size, dataset size, and the\nexpert degree. Echoing previous research studying MoE in different contexts, we\nobserve the diminishing return of increasing the number of experts, but this\nseems to suggest we should scale the number of experts until saturation, as the\ntraining cost would remain constant, which is problematic during inference\ntime. We propose to amend the scaling law of MoE by introducing inference\nefficiency as another metric besides the validation loss. We find that MoEs\nwith a few (4/8) experts are the most serving efficient solution under the same\nperformance, but costs 2.5-3.5x more in training. On the other hand, training a\n(16/32) expert MoE much smaller (70-85%) than the loss-optimal solution, but\nwith a larger training dataset is a promising setup under a training budget.\n', ""  Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates\npredictions from several specialized sub-models (referred to as experts). This\nfusion is accomplished through a router mechanism, dynamically assigning\nweights to each expert's contribution based on the input data. Conventional MoE\nmechanisms select all available experts, incurring substantial computational\ncosts. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages\nonly a limited number, or even just one expert, significantly reducing\ncomputation overhead while empirically preserving, and sometimes even\nenhancing, performance. Despite its wide-ranging applications and these\nadvantageous characteristics, MoE's theoretical underpinnings have remained\nelusive. In this paper, we embark on an exploration of Sparse MoE's\ngeneralization error concerning various critical factors. Specifically, we\ninvestigate the impact of the number of data samples, the total number of\nexperts, the sparsity in expert selection, the complexity of the routing\nmechanism, and the complexity of individual experts. Our analysis sheds light\non \\textit{how \\textbf{sparsity} contributes to the MoE's generalization},\noffering insights from the perspective of classical learning theory.\n"", '  Mixture-of-experts (MoE) is gaining increasing attention due to its unique\nproperties and remarkable performance, especially for language tasks. By\nsparsely activating a subset of parameters for each token, MoE architecture\ncould increase the model size without sacrificing computational efficiency,\nachieving a better trade-off between performance and training costs. However,\nthe underlying mechanism of MoE still lacks further exploration, and its\nmodularization degree remains questionable. In this paper, we make an initial\nattempt to understand the inner workings of MoE-based large language models.\nConcretely, we comprehensively study the parametric and behavioral features of\nthree recent MoE-based models and reveal some intriguing observations,\nincluding (1) Neurons act like fine-grained experts. (2) The router of MoE\nusually selects experts with larger output norms. (3) The expert diversity\nincreases as the layer increases, while the last layer is an outlier. Based on\nthe observations, we also provide suggestions for a broad spectrum of MoE\npractitioners, such as router design and expert allocation. We hope this work\ncould shed light on future research on the MoE framework and other modular\narchitectures. Code is available at\nhttps://github.com/kamanphoebe/Look-into-MoEs.\n']",Mixture of Experts (MoE) Models and Training,Mixture of Experts (MoE) Models and Their Optimizations,Machine Learning and Optimization,Machine Learning and Artificial Intelligence
165,51,165_scaling_models_smaller_scale,"['scaling', 'models', 'smaller', 'scale', 'autoscale', 'larger', 'predicting', 'performance', 'predictable', 'language']","['scaling', 'law', 'laws', 'compute', 'size', 'training', 'pre', 'larger', 'predictable', 'sizes']","[""  The scientific scale-up of large language models (LLMs) necessitates a\ncomprehensive understanding of their scaling properties. However, the existing\nliterature on the scaling properties only yields an incomplete answer:\noptimization loss decreases predictably as the model size increases, in line\nwith established scaling law; yet no scaling law for task has been established\nand the task performances are far from predictable during scaling. Task\nperformances typically show minor gains on small models until they improve\ndramatically once models exceed a size threshold, exemplifying the ``emergent\nabilities''. In this study, we discover that small models, although they\nexhibit minor performance, demonstrate critical and consistent task performance\nimprovements that are not captured by conventional evaluation strategies due to\ninsufficient measurement resolution. To measure such improvements, we introduce\nPassUntil, an evaluation strategy with theoretically infinite resolution,\nthrough massive sampling in the decoding phase. With PassUntil, we conduct a\nquantitative investigation into the scaling law of task performance. The\ninvestigation contains two parts. Firstly, a strict task scaling law that is\nnot conventionally known to exist, is identified, enhancing the predictability\nof task performances. Remarkably, we are able to predict the performance of the\n2.4B model on code generation with merely 0.05\\% deviation before training\nstarts, which is the first systematic attempt to verify predictable scaling\nproposed by GPT-4's report. Secondly, we are able to study emergent abilities\nquantitatively. We identify a kind of accelerated emergence whose scaling curve\ncannot be fitted by standard scaling law function and has a increasing speed.\nWe then examine two hypothesis and imply that the ``multiple circuits\nhypothesis'' might be responsible for the accelerated emergence.\n"", '  Large language model pre-training has become increasingly expensive, with\nmost practitioners relying on scaling laws to allocate compute budgets for\nmodel size and training tokens, commonly referred to as Compute-Optimal or\nChinchilla Optimal. In this paper, we hypothesize a new scaling law that\nsuggests model performance depends mostly on the amount of compute spent for\ntransformer-based models, independent of the specific allocation to model size\nand dataset size. Using this unified scaling law, we predict that (a) for\ninference efficiency, training should prioritize smaller model sizes and larger\ntraining datasets, and (b) assuming the exhaustion of available web datasets,\nscaling the model size might be the only way to further improve model\nperformance.\n', '  Recently, Large Language Models (LLMs) have been widely adopted in a wide\nrange of tasks, leading to increasing attention towards the research on how\nscaling LLMs affects their performance. Existing works, termed Scaling Laws,\nhave discovered that the final test loss of LLMs scales as power-laws with\nmodel size, computational budget, and dataset size. However, the temporal\nchange of the test loss of an LLM throughout its pre-training process remains\nunexplored, though it is valuable in many aspects, such as selecting better\nhyperparameters \\textit{directly} on the target LLM. In this paper, we propose\nthe novel concept of Temporal Scaling Law, studying how the test loss of an LLM\nevolves as the training steps scale up. In contrast to modeling the test loss\nas a whole in a coarse-grained manner, we break it down and dive into the\nfine-grained test loss of each token position, and further develop a dynamic\nhyperbolic-law. Afterwards, we derive the much more precise temporal scaling\nlaw by studying the temporal patterns of the parameters in the dynamic\nhyperbolic-law. Results on both in-distribution (ID) and out-of-distribution\n(OOD) validation datasets demonstrate that our temporal scaling law accurately\npredicts the test loss of LLMs across training steps. Our temporal scaling law\nhas broad practical applications. First, it enables direct and efficient\nhyperparameter selection on the target LLM, such as data mixture proportions.\nSecondly, viewing the LLM pre-training dynamics from the token position\ngranularity provides some insights to enhance the understanding of LLM\npre-training.\n']",Scaling Laws for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
165,51,165_scaling_models_smaller_scale,"['scaling', 'models', 'smaller', 'scale', 'autoscale', 'larger', 'predicting', 'performance', 'predictable', 'language']","['scaling', 'law', 'laws', 'compute', 'size', 'training', 'pre', 'larger', 'predictable', 'sizes']","[""  The scientific scale-up of large language models (LLMs) necessitates a\ncomprehensive understanding of their scaling properties. However, the existing\nliterature on the scaling properties only yields an incomplete answer:\noptimization loss decreases predictably as the model size increases, in line\nwith established scaling law; yet no scaling law for task has been established\nand the task performances are far from predictable during scaling. Task\nperformances typically show minor gains on small models until they improve\ndramatically once models exceed a size threshold, exemplifying the ``emergent\nabilities''. In this study, we discover that small models, although they\nexhibit minor performance, demonstrate critical and consistent task performance\nimprovements that are not captured by conventional evaluation strategies due to\ninsufficient measurement resolution. To measure such improvements, we introduce\nPassUntil, an evaluation strategy with theoretically infinite resolution,\nthrough massive sampling in the decoding phase. With PassUntil, we conduct a\nquantitative investigation into the scaling law of task performance. The\ninvestigation contains two parts. Firstly, a strict task scaling law that is\nnot conventionally known to exist, is identified, enhancing the predictability\nof task performances. Remarkably, we are able to predict the performance of the\n2.4B model on code generation with merely 0.05\\% deviation before training\nstarts, which is the first systematic attempt to verify predictable scaling\nproposed by GPT-4's report. Secondly, we are able to study emergent abilities\nquantitatively. We identify a kind of accelerated emergence whose scaling curve\ncannot be fitted by standard scaling law function and has a increasing speed.\nWe then examine two hypothesis and imply that the ``multiple circuits\nhypothesis'' might be responsible for the accelerated emergence.\n"", '  Large language model pre-training has become increasingly expensive, with\nmost practitioners relying on scaling laws to allocate compute budgets for\nmodel size and training tokens, commonly referred to as Compute-Optimal or\nChinchilla Optimal. In this paper, we hypothesize a new scaling law that\nsuggests model performance depends mostly on the amount of compute spent for\ntransformer-based models, independent of the specific allocation to model size\nand dataset size. Using this unified scaling law, we predict that (a) for\ninference efficiency, training should prioritize smaller model sizes and larger\ntraining datasets, and (b) assuming the exhaustion of available web datasets,\nscaling the model size might be the only way to further improve model\nperformance.\n', '  Recently, Large Language Models (LLMs) have been widely adopted in a wide\nrange of tasks, leading to increasing attention towards the research on how\nscaling LLMs affects their performance. Existing works, termed Scaling Laws,\nhave discovered that the final test loss of LLMs scales as power-laws with\nmodel size, computational budget, and dataset size. However, the temporal\nchange of the test loss of an LLM throughout its pre-training process remains\nunexplored, though it is valuable in many aspects, such as selecting better\nhyperparameters \\textit{directly} on the target LLM. In this paper, we propose\nthe novel concept of Temporal Scaling Law, studying how the test loss of an LLM\nevolves as the training steps scale up. In contrast to modeling the test loss\nas a whole in a coarse-grained manner, we break it down and dive into the\nfine-grained test loss of each token position, and further develop a dynamic\nhyperbolic-law. Afterwards, we derive the much more precise temporal scaling\nlaw by studying the temporal patterns of the parameters in the dynamic\nhyperbolic-law. Results on both in-distribution (ID) and out-of-distribution\n(OOD) validation datasets demonstrate that our temporal scaling law accurately\npredicts the test loss of LLMs across training steps. Our temporal scaling law\nhas broad practical applications. First, it enables direct and efficient\nhyperparameter selection on the target LLM, such as data mixture proportions.\nSecondly, viewing the LLM pre-training dynamics from the token position\ngranularity provides some insights to enhance the understanding of LLM\npre-training.\n']",Scaling Laws for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
165,51,165_scaling_models_smaller_scale,"['scaling', 'models', 'smaller', 'scale', 'autoscale', 'larger', 'predicting', 'performance', 'predictable', 'language']","['scaling', 'law', 'laws', 'compute', 'size', 'training', 'pre', 'larger', 'predictable', 'sizes']","[""  The scientific scale-up of large language models (LLMs) necessitates a\ncomprehensive understanding of their scaling properties. However, the existing\nliterature on the scaling properties only yields an incomplete answer:\noptimization loss decreases predictably as the model size increases, in line\nwith established scaling law; yet no scaling law for task has been established\nand the task performances are far from predictable during scaling. Task\nperformances typically show minor gains on small models until they improve\ndramatically once models exceed a size threshold, exemplifying the ``emergent\nabilities''. In this study, we discover that small models, although they\nexhibit minor performance, demonstrate critical and consistent task performance\nimprovements that are not captured by conventional evaluation strategies due to\ninsufficient measurement resolution. To measure such improvements, we introduce\nPassUntil, an evaluation strategy with theoretically infinite resolution,\nthrough massive sampling in the decoding phase. With PassUntil, we conduct a\nquantitative investigation into the scaling law of task performance. The\ninvestigation contains two parts. Firstly, a strict task scaling law that is\nnot conventionally known to exist, is identified, enhancing the predictability\nof task performances. Remarkably, we are able to predict the performance of the\n2.4B model on code generation with merely 0.05\\% deviation before training\nstarts, which is the first systematic attempt to verify predictable scaling\nproposed by GPT-4's report. Secondly, we are able to study emergent abilities\nquantitatively. We identify a kind of accelerated emergence whose scaling curve\ncannot be fitted by standard scaling law function and has a increasing speed.\nWe then examine two hypothesis and imply that the ``multiple circuits\nhypothesis'' might be responsible for the accelerated emergence.\n"", '  Large language model pre-training has become increasingly expensive, with\nmost practitioners relying on scaling laws to allocate compute budgets for\nmodel size and training tokens, commonly referred to as Compute-Optimal or\nChinchilla Optimal. In this paper, we hypothesize a new scaling law that\nsuggests model performance depends mostly on the amount of compute spent for\ntransformer-based models, independent of the specific allocation to model size\nand dataset size. Using this unified scaling law, we predict that (a) for\ninference efficiency, training should prioritize smaller model sizes and larger\ntraining datasets, and (b) assuming the exhaustion of available web datasets,\nscaling the model size might be the only way to further improve model\nperformance.\n', '  Recently, Large Language Models (LLMs) have been widely adopted in a wide\nrange of tasks, leading to increasing attention towards the research on how\nscaling LLMs affects their performance. Existing works, termed Scaling Laws,\nhave discovered that the final test loss of LLMs scales as power-laws with\nmodel size, computational budget, and dataset size. However, the temporal\nchange of the test loss of an LLM throughout its pre-training process remains\nunexplored, though it is valuable in many aspects, such as selecting better\nhyperparameters \\textit{directly} on the target LLM. In this paper, we propose\nthe novel concept of Temporal Scaling Law, studying how the test loss of an LLM\nevolves as the training steps scale up. In contrast to modeling the test loss\nas a whole in a coarse-grained manner, we break it down and dive into the\nfine-grained test loss of each token position, and further develop a dynamic\nhyperbolic-law. Afterwards, we derive the much more precise temporal scaling\nlaw by studying the temporal patterns of the parameters in the dynamic\nhyperbolic-law. Results on both in-distribution (ID) and out-of-distribution\n(OOD) validation datasets demonstrate that our temporal scaling law accurately\npredicts the test loss of LLMs across training steps. Our temporal scaling law\nhas broad practical applications. First, it enables direct and efficient\nhyperparameter selection on the target LLM, such as data mixture proportions.\nSecondly, viewing the LLM pre-training dynamics from the token position\ngranularity provides some insights to enhance the understanding of LLM\npre-training.\n']",Scaling Laws for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
165,51,165_scaling_models_smaller_scale,"['scaling', 'models', 'smaller', 'scale', 'autoscale', 'larger', 'predicting', 'performance', 'predictable', 'language']","['scaling', 'law', 'laws', 'compute', 'size', 'training', 'pre', 'larger', 'predictable', 'sizes']","[""  The scientific scale-up of large language models (LLMs) necessitates a\ncomprehensive understanding of their scaling properties. However, the existing\nliterature on the scaling properties only yields an incomplete answer:\noptimization loss decreases predictably as the model size increases, in line\nwith established scaling law; yet no scaling law for task has been established\nand the task performances are far from predictable during scaling. Task\nperformances typically show minor gains on small models until they improve\ndramatically once models exceed a size threshold, exemplifying the ``emergent\nabilities''. In this study, we discover that small models, although they\nexhibit minor performance, demonstrate critical and consistent task performance\nimprovements that are not captured by conventional evaluation strategies due to\ninsufficient measurement resolution. To measure such improvements, we introduce\nPassUntil, an evaluation strategy with theoretically infinite resolution,\nthrough massive sampling in the decoding phase. With PassUntil, we conduct a\nquantitative investigation into the scaling law of task performance. The\ninvestigation contains two parts. Firstly, a strict task scaling law that is\nnot conventionally known to exist, is identified, enhancing the predictability\nof task performances. Remarkably, we are able to predict the performance of the\n2.4B model on code generation with merely 0.05\\% deviation before training\nstarts, which is the first systematic attempt to verify predictable scaling\nproposed by GPT-4's report. Secondly, we are able to study emergent abilities\nquantitatively. We identify a kind of accelerated emergence whose scaling curve\ncannot be fitted by standard scaling law function and has a increasing speed.\nWe then examine two hypothesis and imply that the ``multiple circuits\nhypothesis'' might be responsible for the accelerated emergence.\n"", '  Large language model pre-training has become increasingly expensive, with\nmost practitioners relying on scaling laws to allocate compute budgets for\nmodel size and training tokens, commonly referred to as Compute-Optimal or\nChinchilla Optimal. In this paper, we hypothesize a new scaling law that\nsuggests model performance depends mostly on the amount of compute spent for\ntransformer-based models, independent of the specific allocation to model size\nand dataset size. Using this unified scaling law, we predict that (a) for\ninference efficiency, training should prioritize smaller model sizes and larger\ntraining datasets, and (b) assuming the exhaustion of available web datasets,\nscaling the model size might be the only way to further improve model\nperformance.\n', '  Recently, Large Language Models (LLMs) have been widely adopted in a wide\nrange of tasks, leading to increasing attention towards the research on how\nscaling LLMs affects their performance. Existing works, termed Scaling Laws,\nhave discovered that the final test loss of LLMs scales as power-laws with\nmodel size, computational budget, and dataset size. However, the temporal\nchange of the test loss of an LLM throughout its pre-training process remains\nunexplored, though it is valuable in many aspects, such as selecting better\nhyperparameters \\textit{directly} on the target LLM. In this paper, we propose\nthe novel concept of Temporal Scaling Law, studying how the test loss of an LLM\nevolves as the training steps scale up. In contrast to modeling the test loss\nas a whole in a coarse-grained manner, we break it down and dive into the\nfine-grained test loss of each token position, and further develop a dynamic\nhyperbolic-law. Afterwards, we derive the much more precise temporal scaling\nlaw by studying the temporal patterns of the parameters in the dynamic\nhyperbolic-law. Results on both in-distribution (ID) and out-of-distribution\n(OOD) validation datasets demonstrate that our temporal scaling law accurately\npredicts the test loss of LLMs across training steps. Our temporal scaling law\nhas broad practical applications. First, it enables direct and efficient\nhyperparameter selection on the target LLM, such as data mixture proportions.\nSecondly, viewing the LLM pre-training dynamics from the token position\ngranularity provides some insights to enhance the understanding of LLM\npre-training.\n']",Scaling Laws for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
165,51,165_scaling_models_smaller_scale,"['scaling', 'models', 'smaller', 'scale', 'autoscale', 'larger', 'predicting', 'performance', 'predictable', 'language']","['scaling', 'law', 'laws', 'compute', 'size', 'training', 'pre', 'larger', 'predictable', 'sizes']","[""  The scientific scale-up of large language models (LLMs) necessitates a\ncomprehensive understanding of their scaling properties. However, the existing\nliterature on the scaling properties only yields an incomplete answer:\noptimization loss decreases predictably as the model size increases, in line\nwith established scaling law; yet no scaling law for task has been established\nand the task performances are far from predictable during scaling. Task\nperformances typically show minor gains on small models until they improve\ndramatically once models exceed a size threshold, exemplifying the ``emergent\nabilities''. In this study, we discover that small models, although they\nexhibit minor performance, demonstrate critical and consistent task performance\nimprovements that are not captured by conventional evaluation strategies due to\ninsufficient measurement resolution. To measure such improvements, we introduce\nPassUntil, an evaluation strategy with theoretically infinite resolution,\nthrough massive sampling in the decoding phase. With PassUntil, we conduct a\nquantitative investigation into the scaling law of task performance. The\ninvestigation contains two parts. Firstly, a strict task scaling law that is\nnot conventionally known to exist, is identified, enhancing the predictability\nof task performances. Remarkably, we are able to predict the performance of the\n2.4B model on code generation with merely 0.05\\% deviation before training\nstarts, which is the first systematic attempt to verify predictable scaling\nproposed by GPT-4's report. Secondly, we are able to study emergent abilities\nquantitatively. We identify a kind of accelerated emergence whose scaling curve\ncannot be fitted by standard scaling law function and has a increasing speed.\nWe then examine two hypothesis and imply that the ``multiple circuits\nhypothesis'' might be responsible for the accelerated emergence.\n"", '  Large language model pre-training has become increasingly expensive, with\nmost practitioners relying on scaling laws to allocate compute budgets for\nmodel size and training tokens, commonly referred to as Compute-Optimal or\nChinchilla Optimal. In this paper, we hypothesize a new scaling law that\nsuggests model performance depends mostly on the amount of compute spent for\ntransformer-based models, independent of the specific allocation to model size\nand dataset size. Using this unified scaling law, we predict that (a) for\ninference efficiency, training should prioritize smaller model sizes and larger\ntraining datasets, and (b) assuming the exhaustion of available web datasets,\nscaling the model size might be the only way to further improve model\nperformance.\n', '  Recently, Large Language Models (LLMs) have been widely adopted in a wide\nrange of tasks, leading to increasing attention towards the research on how\nscaling LLMs affects their performance. Existing works, termed Scaling Laws,\nhave discovered that the final test loss of LLMs scales as power-laws with\nmodel size, computational budget, and dataset size. However, the temporal\nchange of the test loss of an LLM throughout its pre-training process remains\nunexplored, though it is valuable in many aspects, such as selecting better\nhyperparameters \\textit{directly} on the target LLM. In this paper, we propose\nthe novel concept of Temporal Scaling Law, studying how the test loss of an LLM\nevolves as the training steps scale up. In contrast to modeling the test loss\nas a whole in a coarse-grained manner, we break it down and dive into the\nfine-grained test loss of each token position, and further develop a dynamic\nhyperbolic-law. Afterwards, we derive the much more precise temporal scaling\nlaw by studying the temporal patterns of the parameters in the dynamic\nhyperbolic-law. Results on both in-distribution (ID) and out-of-distribution\n(OOD) validation datasets demonstrate that our temporal scaling law accurately\npredicts the test loss of LLMs across training steps. Our temporal scaling law\nhas broad practical applications. First, it enables direct and efficient\nhyperparameter selection on the target LLM, such as data mixture proportions.\nSecondly, viewing the LLM pre-training dynamics from the token position\ngranularity provides some insights to enhance the understanding of LLM\npre-training.\n']",Scaling Laws for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
165,51,165_scaling_models_smaller_scale,"['scaling', 'models', 'smaller', 'scale', 'autoscale', 'larger', 'predicting', 'performance', 'predictable', 'language']","['scaling', 'law', 'laws', 'compute', 'size', 'training', 'pre', 'larger', 'predictable', 'sizes']","[""  The scientific scale-up of large language models (LLMs) necessitates a\ncomprehensive understanding of their scaling properties. However, the existing\nliterature on the scaling properties only yields an incomplete answer:\noptimization loss decreases predictably as the model size increases, in line\nwith established scaling law; yet no scaling law for task has been established\nand the task performances are far from predictable during scaling. Task\nperformances typically show minor gains on small models until they improve\ndramatically once models exceed a size threshold, exemplifying the ``emergent\nabilities''. In this study, we discover that small models, although they\nexhibit minor performance, demonstrate critical and consistent task performance\nimprovements that are not captured by conventional evaluation strategies due to\ninsufficient measurement resolution. To measure such improvements, we introduce\nPassUntil, an evaluation strategy with theoretically infinite resolution,\nthrough massive sampling in the decoding phase. With PassUntil, we conduct a\nquantitative investigation into the scaling law of task performance. The\ninvestigation contains two parts. Firstly, a strict task scaling law that is\nnot conventionally known to exist, is identified, enhancing the predictability\nof task performances. Remarkably, we are able to predict the performance of the\n2.4B model on code generation with merely 0.05\\% deviation before training\nstarts, which is the first systematic attempt to verify predictable scaling\nproposed by GPT-4's report. Secondly, we are able to study emergent abilities\nquantitatively. We identify a kind of accelerated emergence whose scaling curve\ncannot be fitted by standard scaling law function and has a increasing speed.\nWe then examine two hypothesis and imply that the ``multiple circuits\nhypothesis'' might be responsible for the accelerated emergence.\n"", '  Large language model pre-training has become increasingly expensive, with\nmost practitioners relying on scaling laws to allocate compute budgets for\nmodel size and training tokens, commonly referred to as Compute-Optimal or\nChinchilla Optimal. In this paper, we hypothesize a new scaling law that\nsuggests model performance depends mostly on the amount of compute spent for\ntransformer-based models, independent of the specific allocation to model size\nand dataset size. Using this unified scaling law, we predict that (a) for\ninference efficiency, training should prioritize smaller model sizes and larger\ntraining datasets, and (b) assuming the exhaustion of available web datasets,\nscaling the model size might be the only way to further improve model\nperformance.\n', '  Recently, Large Language Models (LLMs) have been widely adopted in a wide\nrange of tasks, leading to increasing attention towards the research on how\nscaling LLMs affects their performance. Existing works, termed Scaling Laws,\nhave discovered that the final test loss of LLMs scales as power-laws with\nmodel size, computational budget, and dataset size. However, the temporal\nchange of the test loss of an LLM throughout its pre-training process remains\nunexplored, though it is valuable in many aspects, such as selecting better\nhyperparameters \\textit{directly} on the target LLM. In this paper, we propose\nthe novel concept of Temporal Scaling Law, studying how the test loss of an LLM\nevolves as the training steps scale up. In contrast to modeling the test loss\nas a whole in a coarse-grained manner, we break it down and dive into the\nfine-grained test loss of each token position, and further develop a dynamic\nhyperbolic-law. Afterwards, we derive the much more precise temporal scaling\nlaw by studying the temporal patterns of the parameters in the dynamic\nhyperbolic-law. Results on both in-distribution (ID) and out-of-distribution\n(OOD) validation datasets demonstrate that our temporal scaling law accurately\npredicts the test loss of LLMs across training steps. Our temporal scaling law\nhas broad practical applications. First, it enables direct and efficient\nhyperparameter selection on the target LLM, such as data mixture proportions.\nSecondly, viewing the LLM pre-training dynamics from the token position\ngranularity provides some insights to enhance the understanding of LLM\npre-training.\n']",Scaling Laws for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
166,51,166_imagenet_neural_rnn_supernet,"['imagenet', 'neural', 'rnn', 'supernet', 'architecture', 'architectures', 'nas', 'searching', 'networks', 'architectural']","['search', 'architecture', 'architectures', 'proxies', 'hardware', 'neural', 'evolutionary', 'supernet', 'cost', 'network']","['  Efficient evaluation of a network architecture drawn from a large search\nspace remains a key challenge in Neural Architecture Search (NAS). Vanilla NAS\nevaluates each architecture by training from scratch, which gives the true\nperformance but is extremely time-consuming. Recently, one-shot NAS\nsubstantially reduces the computation cost by training only one supernetwork,\na.k.a. supernet, to approximate the performance of every architecture in the\nsearch space via weight-sharing. However, the performance estimation can be\nvery inaccurate due to the co-adaption among operations. In this paper, we\npropose few-shot NAS that uses multiple supernetworks, called sub-supernet,\neach covering different regions of the search space to alleviate the undesired\nco-adaption. Compared to one-shot NAS, few-shot NAS improves the accuracy of\narchitecture evaluation with a small increase of evaluation cost. With only up\nto 7 sub-supernets, few-shot NAS establishes new SoTAs: on ImageNet, it finds\nmodels that reach 80.5% top-1 accuracy at 600 MB FLOPS and 77.5% top-1 accuracy\nat 238 MFLOPS; on CIFAR10, it reaches 98.72% top-1 accuracy without using extra\ndata or transfer learning. In Auto-GAN, few-shot NAS outperforms the previously\npublished results by up to 20%. Extensive experiments show that few-shot NAS\nsignificantly improves various one-shot methods, including 4 gradient-based and\n6 search-based methods on 3 different tasks in NasBench-201 and\nNasBench1-shot-1.\n', ""  The paper provides a comprehensive overview of Neural Architecture Search\n(NAS), emphasizing its evolution from manual design to automated,\ncomputationally-driven approaches. It covers the inception and growth of NAS,\nhighlighting its application across various domains, including medical imaging\nand natural language processing. The document details the shift from\nexpert-driven design to algorithm-driven processes, exploring initial\nmethodologies like reinforcement learning and evolutionary algorithms. It also\ndiscusses the challenges of computational demands and the emergence of\nefficient NAS methodologies, such as Differentiable Architecture Search and\nhardware-aware NAS. The paper further elaborates on NAS's application in\ncomputer vision, NLP, and beyond, demonstrating its versatility and potential\nfor optimizing neural network architectures across different tasks. Future\ndirections and challenges, including computational efficiency and the\nintegration with emerging AI domains, are addressed, showcasing NAS's dynamic\nnature and its continued evolution towards more sophisticated and efficient\narchitecture search methods.\n"", '  Neural architecture search (NAS) has become a key component of AutoML and a\nstandard tool to automate the design of deep neural networks. Recently,\ntraining-free NAS as an emerging paradigm has successfully reduced the search\ncosts of standard training-based NAS by estimating the true architecture\nperformance with only training-free metrics. Nevertheless, the estimation\nability of these metrics typically varies across different tasks, making it\nchallenging to achieve robust and consistently good search performance on\ndiverse tasks with only a single training-free metric. Meanwhile, the\nestimation gap between training-free metrics and the true architecture\nperformances limits training-free NAS to achieve superior performance. To\naddress these challenges, we propose the robustifying and boosting\ntraining-free NAS (RoBoT) algorithm which (a) employs the optimized combination\nof existing training-free metrics explored from Bayesian optimization to\ndevelop a robust and consistently better-performing metric on diverse tasks,\nand (b) applies greedy search, i.e., the exploitation, on the newly developed\nmetric to bridge the aforementioned gap and consequently to boost the search\nperformance of standard training-free NAS further. Remarkably, the expected\nperformance of our RoBoT can be theoretically guaranteed, which improves over\nthe existing training-free NAS under mild conditions with additional\ninteresting insights. Our extensive experiments on various NAS benchmark tasks\nyield substantial empirical evidence to support our theoretical results.\n']",Neural Architecture Search (NAS) Methods and Applications,Neural Network Optimization Techniques,Deep Learning Optimization and Training,Deep Learning Optimization and Security
167,50,167_subgraph_hypergraph_graphs_graphmae,"['subgraph', 'hypergraph', 'graphs', 'graphmae', 'graphacl', 'graphlearner', 'hypergraphs', 'embeddings', 'graph', 'supervised']","['contrastive', 'graph', 'hypergraph', 'negative', 'node', 'augmentation', 'views', 'positive', 'nodes', 'hypergraphs']","[""  Graph contrastive learning (GCL) is a popular method for leaning graph\nrepresentations by maximizing the consistency of features across augmented\nviews. Traditional GCL methods utilize single-perspective i.e. data or\nmodel-perspective) augmentation to generate positive samples, restraining the\ndiversity of positive samples. In addition, these positive samples may be\nunreliable due to uncontrollable augmentation strategies that potentially alter\nthe semantic information. To address these challenges, this paper proposed a\ninnovative framework termed dual-perspective cross graph contrastive learning\n(DC-GCL), which incorporates three modifications designed to enhance positive\nsample diversity and reliability: 1) We propose dual-perspective augmentation\nstrategy that provide the model with more diverse training data, enabling the\nmodel effective learning of feature consistency across different views. 2) From\nthe data perspective, we slightly perturb the original graphs using\ncontrollable data augmentation, effectively preserving their semantic\ninformation. 3) From the model perspective, we enhance the encoder by utilizing\nmore powerful graph transformers instead of graph neural networks. Based on the\nmodel's architecture, we propose three pruning-based strategies to slightly\nperturb the encoder, providing more reliable positive samples. These\nmodifications collectively form the DC-GCL's foundation and provide more\ndiverse and reliable training inputs, offering significant improvements over\ntraditional GCL methods. Extensive experiments on various benchmarks\ndemonstrate that DC-GCL consistently outperforms different baselines on various\ndatasets and tasks.\n"", '  Graph contrastive learning (GCL) is an effective paradigm for node\nrepresentation learning in graphs. The key components hidden behind GCL are\ndata augmentation and positive-negative pair selection. Typical data\naugmentations in GCL, such as uniform deletion of edges, are generally blind\nand resort to local perturbation, which is prone to producing under-diversity\nviews. Additionally, there is a risk of making the augmented data traverse to\nother classes. Moreover, most methods always treat all other samples as\nnegatives. Such a negative pairing naturally results in sampling bias and\nlikewise may make the learned representation suffer from semantic drift.\nTherefore, to increase the diversity of the contrastive view, we propose two\nsimple and effective global topological augmentations to compensate current\nGCL. One is to mine the semantic correlation between nodes in the feature\nspace. The other is to utilize the algebraic properties of the adjacency matrix\nto characterize the topology by eigen-decomposition. With the help of both, we\ncan retain important edges to build a better view. To reduce the risk of\nsemantic drift, a prototype-based negative pair selection is further designed\nwhich can filter false negative samples. Extensive experiments on various tasks\ndemonstrate the advantages of the model compared to the state-of-the-art\nmethods.\n', '  Graph Contrastive Learning (GCL) has emerged as a popular training approach\nfor learning node embeddings from augmented graphs without labels. Despite the\nkey principle that maximizing the similarity between positive node pairs while\nminimizing it between negative node pairs is well established, some fundamental\nproblems are still unclear. Considering the complex graph structure, are some\nnodes consistently well-trained and following this principle even with\ndifferent graph augmentations? Or are there some nodes more likely to be\nuntrained across graph augmentations and violate the principle? How to\ndistinguish these nodes and further guide the training of GCL? To answer these\nquestions, we first present experimental evidence showing that the training of\nGCL is indeed imbalanced across all nodes. To address this problem, we propose\nthe metric ""node compactness"", which is the lower bound of how a node follows\nthe GCL principle related to the range of augmentations. We further derive the\nform of node compactness theoretically through bound propagation, which can be\nintegrated into binary cross-entropy as a regularization. To this end, we\npropose the PrOvable Training (POT) for GCL, which regularizes the training of\nGCL to encode node embeddings that follows the GCL principle better. Through\nextensive experiments on various benchmarks, POT consistently improves the\nexisting GCL approaches, serving as a friendly plugin.\n']",Graph Contrastive Learning (GCL) Methods,Graph Neural Networks and Deep Learning on Graph-Structured Data,Graph Representation Learning and Neural Networks,Graph Representation Learning and Neural Networks
168,50,168_personality_personalities_traits_trait,"['personality', 'personalities', 'traits', 'trait', 'profiles', 'conversational', 'personas', 'profile', 'extraversion', 'psychometrics']","['personality', 'traits', 'personalities', 'psychological', 'trait', 'psychology', 'assessment', 'tests', 'extraversion', 'psychometrics']","[""  Personality psychologists have analyzed the relationship between personality\nand safety behaviors in human society. Although Large Language Models (LLMs)\ndemonstrate personality traits, the relationship between personality traits and\nsafety abilities in LLMs still remains a mystery. In this paper, we discover\nthat LLMs' personality traits are closely related to their safety abilities,\ni.e., toxicity, privacy, and fairness, based on the reliable MBTI-M scale.\nMeanwhile, the safety alignment generally increases various LLMs' Extraversion,\nSensing, and Judging traits. According to such findings, we can edit LLMs'\npersonality traits and improve their safety performance, e.g., inducing\npersonality from ISTJ to ISTP resulted in a relative improvement of\napproximately 43% and 10% in privacy and fairness performance, respectively.\nAdditionally, we find that LLMs with different personality traits are\ndifferentially susceptible to jailbreak. This study pioneers the investigation\nof LLM safety from a personality perspective, providing new insights into LLM\nsafety enhancement.\n"", ""  Personality detection aims to detect one's personality traits underlying in\nsocial media posts. One challenge of this task is the scarcity of ground-truth\npersonality traits which are collected from self-report questionnaires. Most\nexisting methods learn post features directly by fine-tuning the pre-trained\nlanguage models under the supervision of limited personality labels. This leads\nto inferior quality of post features and consequently affects the performance.\nIn addition, they treat personality traits as one-hot classification labels,\noverlooking the semantic information within them. In this paper, we propose a\nlarge language model (LLM) based text augmentation enhanced personality\ndetection model, which distills the LLM's knowledge to enhance the small model\nfor personality detection, even when the LLM fails in this task. Specifically,\nwe enable LLM to generate post analyses (augmentations) from the aspects of\nsemantic, sentiment, and linguistic, which are critical for personality\ndetection. By using contrastive learning to pull them together in the embedding\nspace, the post encoder can better capture the psycho-linguistic information\nwithin the post representations, thus improving personality detection.\nFurthermore, we utilize the LLM to enrich the information of personality labels\nfor enhancing the detection performance. Experimental results on the benchmark\ndatasets demonstrate that our model outperforms the state-of-the-art methods on\npersonality detection.\n"", ""  Textual personality detection aims to identify personality characteristics by\nanalyzing user-generated content toward social media platforms. Numerous\npsychological literature highlighted that personality encompasses both\nlong-term stable traits and short-term dynamic states. However, existing\nstudies often concentrate only on either long-term or short-term personality\nrepresentations, without effectively combining both aspects. This limitation\nhinders a comprehensive understanding of individuals' personalities, as both\nstable traits and dynamic states are vital. To bridge this gap, we propose a\nDual Enhanced Network(DEN) to jointly model users' long-term and short-term\npersonality for textual personality detection. In DEN, a Long-term Personality\nEncoding is devised to effectively model long-term stable personality traits.\nShort-term Personality Encoding is presented to capture short-term dynamic\npersonality states. The Bi-directional Interaction component facilitates the\nintegration of both personality aspects, allowing for a comprehensive\nrepresentation of the user's personality. Experimental results on two\npersonality detection datasets demonstrate the effectiveness of the DEN model\nand the benefits of considering both the dynamic and stable nature of\npersonality characteristics for textual personality detection.\n""]",Personality Detection and Analysis in Language Models,Language Analysis for Human Behavior and Intent,Human Behavior and Emotion Analysis through Language and Interaction,Human Behavior and Emotion Analysis through Language and Interaction
169,50,169_photonics_photonic_optical_nanophotonic,"['photonics', 'photonic', 'optical', 'nanophotonic', 'optics', 'multiplexing', 'laser', 'wavelength', 'waveguide', 'throughput']","['photonic', 'optical', 'photonics', 'accelerators', 'computing', 'wavelength', 'analog', 'energy', 'microring', 'crosstalk']","['  Among the promising advantages of photonic computing over conventional\ncomputing architectures is the potential to increase computing efficiency\nthrough massive parallelism by using the many degrees of freedom provided by\nphotonics. Here, we numerically demonstrate the simultaneous use of time and\nfrequency (equivalently wavelength) multiplexing to solve three independent\ntasks at the same time on the same photonic circuit. In particular, we consider\na microring-based time-delay reservoir computing (TDRC) scheme that\nsimultaneously solves three tasks: Time-series prediction, classification, and\nwireless channel equalization. The scheme relies on time-division multiplexing\nto avoid the necessity of multiple physical nonlinear nodes, while the tasks\nare parallelized using wavelength division multiplexing (WDM). The input data\nmodulated on each optical channel is mapped to a higher dimensional space by\nthe nonlinear dynamics of the silicon microring cavity. The carrier wavelength\nand input power assigned to each optical channel have a high influence on the\nperformance of its respective task. When all tasks operate under the same\nwavelength/power conditions, our results show that the computing nature of each\ntask is the deciding factor of the level of performance achievable. However, it\nis possible to achieve good performance for all tasks simultaneously by\noptimizing the parameters of each optical channel. The variety of applications\ncovered by the tasks shows the versatility of the proposed photonic TDRC\nscheme. Overall, this work provides insight into the potential of WDM-based\nschemes for improving the computing capabilities of reservoir computing\nschemes.\n', '  Solving partial differential equations (PDEs) numerically often requires huge\ncomputing time, energy cost, and hardware resources in practical applications.\nThis has limited their applications in many scenarios (e.g., autonomous\nsystems, supersonic flows) that have a limited energy budget and require near\nreal-time response. Leveraging optical computing, this paper develops an\non-chip training framework for physics-informed neural networks (PINNs), aiming\nto solve high-dimensional PDEs with fJ/MAC photonic power consumption and\nultra-low latency. Despite the ultra-high speed of optical neural networks,\ntraining a PINN on an optical chip is hard due to (1) the large size of\nphotonic devices, and (2) the lack of scalable optical memory devices to store\nthe intermediate results of back-propagation (BP). To enable realistic optical\nPINN training, this paper presents a scalable method to avoid the BP process.\nWe also employ a tensor-compressed approach to improve the convergence and\nscalability of our optical PINN training. This training framework is designed\nwith tensorized optical neural networks (TONN) for scalable inference\nacceleration and MZI phase-domain tuning for \\textit{in-situ} optimization. Our\nsimulation results of a 20-dim HJB PDE show that our photonic accelerator can\nreduce the number of MZIs by a factor of $1.17\\times 10^3$, with only $1.36$ J\nand $1.15$ s to solve this equation. This is the first real-size optical PINN\ntraining framework that can be applied to solve high-dimensional PDEs.\n', '  Subwavelength photonic structures and metamaterials provide revolutionary\napproaches for controlling light. The inverse design methods proposed for these\nsubwavelength structures are vital to the development of new photonic devices.\nHowever, most of the existing inverse design methods cannot realize direct\nmapping from optical properties to photonic structures but instead rely on\nforward simulation methods to perform iterative optimization. In this work, we\nexploit the powerful generative abilities of artificial intelligence (AI) and\npropose a practical inverse design method based on latent diffusion models. Our\nmethod maps directly the optical properties to structures without the\nrequirement of forward simulation and iterative optimization. Here, the given\noptical properties can work as ""prompts"" and guide the constructed model to\ncorrectly ""draw"" the required photonic structures. Experiments show that our\ndirect mapping-based inverse design method can generate subwavelength photonic\nstructures at high fidelity while following the given optical properties. This\nmay change the method used for optical design and greatly accelerate the\nresearch on new photonic devices.\n']",Photonic Computing and Optical Information Processing,"Optical and Photonic Technologies for Imaging, Sensing, and Computing","Advanced Technologies for Simulation, Sensing, and Fabrication","Advanced Technologies for Simulation, Sensing, and Fabrication"
169,50,169_photonics_photonic_optical_nanophotonic,"['photonics', 'photonic', 'optical', 'nanophotonic', 'optics', 'multiplexing', 'laser', 'wavelength', 'waveguide', 'throughput']","['photonic', 'optical', 'photonics', 'accelerators', 'computing', 'wavelength', 'analog', 'energy', 'microring', 'crosstalk']","['  Among the promising advantages of photonic computing over conventional\ncomputing architectures is the potential to increase computing efficiency\nthrough massive parallelism by using the many degrees of freedom provided by\nphotonics. Here, we numerically demonstrate the simultaneous use of time and\nfrequency (equivalently wavelength) multiplexing to solve three independent\ntasks at the same time on the same photonic circuit. In particular, we consider\na microring-based time-delay reservoir computing (TDRC) scheme that\nsimultaneously solves three tasks: Time-series prediction, classification, and\nwireless channel equalization. The scheme relies on time-division multiplexing\nto avoid the necessity of multiple physical nonlinear nodes, while the tasks\nare parallelized using wavelength division multiplexing (WDM). The input data\nmodulated on each optical channel is mapped to a higher dimensional space by\nthe nonlinear dynamics of the silicon microring cavity. The carrier wavelength\nand input power assigned to each optical channel have a high influence on the\nperformance of its respective task. When all tasks operate under the same\nwavelength/power conditions, our results show that the computing nature of each\ntask is the deciding factor of the level of performance achievable. However, it\nis possible to achieve good performance for all tasks simultaneously by\noptimizing the parameters of each optical channel. The variety of applications\ncovered by the tasks shows the versatility of the proposed photonic TDRC\nscheme. Overall, this work provides insight into the potential of WDM-based\nschemes for improving the computing capabilities of reservoir computing\nschemes.\n', '  Solving partial differential equations (PDEs) numerically often requires huge\ncomputing time, energy cost, and hardware resources in practical applications.\nThis has limited their applications in many scenarios (e.g., autonomous\nsystems, supersonic flows) that have a limited energy budget and require near\nreal-time response. Leveraging optical computing, this paper develops an\non-chip training framework for physics-informed neural networks (PINNs), aiming\nto solve high-dimensional PDEs with fJ/MAC photonic power consumption and\nultra-low latency. Despite the ultra-high speed of optical neural networks,\ntraining a PINN on an optical chip is hard due to (1) the large size of\nphotonic devices, and (2) the lack of scalable optical memory devices to store\nthe intermediate results of back-propagation (BP). To enable realistic optical\nPINN training, this paper presents a scalable method to avoid the BP process.\nWe also employ a tensor-compressed approach to improve the convergence and\nscalability of our optical PINN training. This training framework is designed\nwith tensorized optical neural networks (TONN) for scalable inference\nacceleration and MZI phase-domain tuning for \\textit{in-situ} optimization. Our\nsimulation results of a 20-dim HJB PDE show that our photonic accelerator can\nreduce the number of MZIs by a factor of $1.17\\times 10^3$, with only $1.36$ J\nand $1.15$ s to solve this equation. This is the first real-size optical PINN\ntraining framework that can be applied to solve high-dimensional PDEs.\n', '  Subwavelength photonic structures and metamaterials provide revolutionary\napproaches for controlling light. The inverse design methods proposed for these\nsubwavelength structures are vital to the development of new photonic devices.\nHowever, most of the existing inverse design methods cannot realize direct\nmapping from optical properties to photonic structures but instead rely on\nforward simulation methods to perform iterative optimization. In this work, we\nexploit the powerful generative abilities of artificial intelligence (AI) and\npropose a practical inverse design method based on latent diffusion models. Our\nmethod maps directly the optical properties to structures without the\nrequirement of forward simulation and iterative optimization. Here, the given\noptical properties can work as ""prompts"" and guide the constructed model to\ncorrectly ""draw"" the required photonic structures. Experiments show that our\ndirect mapping-based inverse design method can generate subwavelength photonic\nstructures at high fidelity while following the given optical properties. This\nmay change the method used for optical design and greatly accelerate the\nresearch on new photonic devices.\n']",Photonic Computing and Optical Information Processing,"Optical and Photonic Technologies for Imaging, Sensing, and Computing","Advanced Technologies for Simulation, Sensing, and Fabrication","Advanced Technologies for Simulation, Sensing, and Fabrication"
169,50,169_photonics_photonic_optical_nanophotonic,"['photonics', 'photonic', 'optical', 'nanophotonic', 'optics', 'multiplexing', 'laser', 'wavelength', 'waveguide', 'throughput']","['photonic', 'optical', 'photonics', 'accelerators', 'computing', 'wavelength', 'analog', 'energy', 'microring', 'crosstalk']","['  Among the promising advantages of photonic computing over conventional\ncomputing architectures is the potential to increase computing efficiency\nthrough massive parallelism by using the many degrees of freedom provided by\nphotonics. Here, we numerically demonstrate the simultaneous use of time and\nfrequency (equivalently wavelength) multiplexing to solve three independent\ntasks at the same time on the same photonic circuit. In particular, we consider\na microring-based time-delay reservoir computing (TDRC) scheme that\nsimultaneously solves three tasks: Time-series prediction, classification, and\nwireless channel equalization. The scheme relies on time-division multiplexing\nto avoid the necessity of multiple physical nonlinear nodes, while the tasks\nare parallelized using wavelength division multiplexing (WDM). The input data\nmodulated on each optical channel is mapped to a higher dimensional space by\nthe nonlinear dynamics of the silicon microring cavity. The carrier wavelength\nand input power assigned to each optical channel have a high influence on the\nperformance of its respective task. When all tasks operate under the same\nwavelength/power conditions, our results show that the computing nature of each\ntask is the deciding factor of the level of performance achievable. However, it\nis possible to achieve good performance for all tasks simultaneously by\noptimizing the parameters of each optical channel. The variety of applications\ncovered by the tasks shows the versatility of the proposed photonic TDRC\nscheme. Overall, this work provides insight into the potential of WDM-based\nschemes for improving the computing capabilities of reservoir computing\nschemes.\n', '  Solving partial differential equations (PDEs) numerically often requires huge\ncomputing time, energy cost, and hardware resources in practical applications.\nThis has limited their applications in many scenarios (e.g., autonomous\nsystems, supersonic flows) that have a limited energy budget and require near\nreal-time response. Leveraging optical computing, this paper develops an\non-chip training framework for physics-informed neural networks (PINNs), aiming\nto solve high-dimensional PDEs with fJ/MAC photonic power consumption and\nultra-low latency. Despite the ultra-high speed of optical neural networks,\ntraining a PINN on an optical chip is hard due to (1) the large size of\nphotonic devices, and (2) the lack of scalable optical memory devices to store\nthe intermediate results of back-propagation (BP). To enable realistic optical\nPINN training, this paper presents a scalable method to avoid the BP process.\nWe also employ a tensor-compressed approach to improve the convergence and\nscalability of our optical PINN training. This training framework is designed\nwith tensorized optical neural networks (TONN) for scalable inference\nacceleration and MZI phase-domain tuning for \\textit{in-situ} optimization. Our\nsimulation results of a 20-dim HJB PDE show that our photonic accelerator can\nreduce the number of MZIs by a factor of $1.17\\times 10^3$, with only $1.36$ J\nand $1.15$ s to solve this equation. This is the first real-size optical PINN\ntraining framework that can be applied to solve high-dimensional PDEs.\n', '  Subwavelength photonic structures and metamaterials provide revolutionary\napproaches for controlling light. The inverse design methods proposed for these\nsubwavelength structures are vital to the development of new photonic devices.\nHowever, most of the existing inverse design methods cannot realize direct\nmapping from optical properties to photonic structures but instead rely on\nforward simulation methods to perform iterative optimization. In this work, we\nexploit the powerful generative abilities of artificial intelligence (AI) and\npropose a practical inverse design method based on latent diffusion models. Our\nmethod maps directly the optical properties to structures without the\nrequirement of forward simulation and iterative optimization. Here, the given\noptical properties can work as ""prompts"" and guide the constructed model to\ncorrectly ""draw"" the required photonic structures. Experiments show that our\ndirect mapping-based inverse design method can generate subwavelength photonic\nstructures at high fidelity while following the given optical properties. This\nmay change the method used for optical design and greatly accelerate the\nresearch on new photonic devices.\n']",Photonic Computing and Optical Information Processing,"Optical and Photonic Technologies for Imaging, Sensing, and Computing","Advanced Technologies for Simulation, Sensing, and Fabrication","Advanced Technologies for Simulation, Sensing, and Fabrication"
170,50,170_sentiment_nlp_sentiments_sentiment140,"['sentiment', 'nlp', 'sentiments', 'sentiment140', 'sentimentality', 'subjectivity', 'tweets', 'texts', 'microblog', 'text']","['sentiment', 'analysis', 'sentiments', 'comments', 'languages', 'classification', 'airline', 'tweet', 'macro', 'resource']","[""  In sentiment analysis of longer texts, there may be a variety of topics\ndiscussed, of entities mentioned, and of sentiments expressed regarding each\nentity. We find a lack of studies exploring how such texts express their\nsentiment towards each entity of interest, and how these sentiments can be\nmodelled. In order to better understand how sentiment regarding persons and\norganizations (each entity in our scope) is expressed in longer texts, we have\ncollected a dataset of expert annotations where the overall sentiment regarding\neach entity is identified, together with the sentence-level sentiment for these\nentities separately. We show that the reader's perceived sentiment regarding an\nentity often differs from an arithmetic aggregation of sentiments at the\nsentence level. Only 70\\% of the positive and 55\\% of the negative entities\nreceive a correct overall sentiment label when we aggregate the\n(human-annotated) sentiment labels for the sentences where the entity is\nmentioned. Our dataset reveals the complexity of entity-specific sentiment in\nlonger texts, and allows for more precise modelling and evaluation of such\nsentiment expressions.\n"", '  Sentiment analysis plays a pivotal role in understanding public opinion,\nparticularly in the political domain where the portrayal of entities in news\narticles influences public perception. In this paper, we investigate the\neffectiveness of Large Language Models (LLMs) in predicting entity-specific\nsentiment from political news articles. Leveraging zero-shot and few-shot\nstrategies, we explore the capability of LLMs to discern sentiment towards\npolitical entities in news content. Employing a chain-of-thought (COT) approach\naugmented with rationale in few-shot in-context learning, we assess whether\nthis method enhances sentiment prediction accuracy. Our evaluation on\nsentiment-labeled datasets demonstrates that LLMs, outperform fine-tuned BERT\nmodels in capturing entity-specific sentiment. We find that learning in-context\nsignificantly improves model performance, while the self-consistency mechanism\nenhances consistency in sentiment prediction. Despite the promising results, we\nobserve inconsistencies in the effectiveness of the COT prompting method.\nOverall, our findings underscore the potential of LLMs in entity-centric\nsentiment analysis within the political news domain and highlight the\nimportance of suitable prompting strategies and model architectures.\n', '  With the rapid development of natural language processing (NLP) technology,\nlarge-scale pre-trained language models such as GPT-3 have become a popular\nresearch object in NLP field. This paper aims to explore sentiment analysis\noptimization techniques based on large pre-trained language models such as\nGPT-3 to improve model performance and effect and further promote the\ndevelopment of natural language processing (NLP). By introducing the importance\nof sentiment analysis and the limitations of traditional methods, GPT-3 and\nFine-tuning techniques are introduced in this paper, and their applications in\nsentiment analysis are explained in detail. The experimental results show that\nthe Fine-tuning technique can optimize GPT-3 model and obtain good performance\nin sentiment analysis task. This study provides an important reference for\nfuture sentiment analysis using large-scale language models.\n']",Sentiment Analysis in Texts,Sentiment Analysis,Business and Marketing Analytics,Business and Marketing Analytics
171,50,171_verilog_programmable_programming_hardware,"['verilog', 'programmable', 'programming', 'hardware', 'verilogeval', 'generate', 'tools', 'automation', 'vhdl', 'veribug']","['verilog', 'hardware', 'design', 'designs', 'verification', 'chip', 'assertions', 'automation', 'code', 'description']","['  Large Language Models (LLMs) have recently shown promise in streamlining\nhardware design processes by encapsulating vast amounts of domain-specific\ndata. In addition, they allow users to interact with the design processes\nthrough natural language instructions, thus making hardware design more\naccessible to developers. However, effectively leveraging LLMs in hardware\ndesign necessitates providing domain-specific data during inference (e.g.,\nthrough in-context learning), fine-tuning, or pre-training. Unfortunately,\nexisting publicly available hardware datasets are often limited in size,\ncomplexity, or detail, which hinders the effectiveness of LLMs in hardware\ndesign tasks. To address this issue, we first propose a set of criteria for\ncreating high-quality hardware datasets that can effectively enhance\nLLM-assisted hardware design. Based on these criteria, we propose a\nMulti-Grained-Verilog (MG-Verilog) dataset, which encompasses descriptions at\nvarious levels of detail and corresponding code samples. To benefit the broader\nhardware design community, we have developed an open-source infrastructure that\nfacilitates easy access, integration, and extension of the dataset to meet\nspecific project needs. Furthermore, to fully exploit the potential of the\nMG-Verilog dataset, which varies in complexity and detail, we introduce a\nbalanced fine-tuning scheme. This scheme serves as a unique use case to\nleverage the diverse levels of detail provided by the dataset. Extensive\nexperiments demonstrate that the proposed dataset and fine-tuning scheme\nconsistently improve the performance of LLMs in hardware design tasks.\n', '  Natural language interfaces have exhibited considerable potential in the\nautomation of Verilog generation derived from high-level specifications through\nthe utilization of large language models, garnering significant attention.\nNevertheless, this paper elucidates that visual representations contribute\nessential contextual information critical to design intent for hardware\narchitectures possessing spatial complexity, potentially surpassing the\nefficacy of natural-language-only inputs. Expanding upon this premise, our\npaper introduces an open-source benchmark for multi-modal generative models\ntailored for Verilog synthesis from visual-linguistic inputs, addressing both\nsingular and complex modules. Additionally, we introduce an open-source visual\nand natural language Verilog query language framework to facilitate efficient\nand user-friendly multi-modal queries. To evaluate the performance of the\nproposed multi-modal hardware generative AI in Verilog generation tasks, we\ncompare it with a popular method that relies solely on natural language. Our\nresults demonstrate a significant accuracy improvement in the multi-modal\ngenerated Verilog compared to queries based solely on natural language. We hope\nto reveal a new approach to hardware design in the large-hardware-design-model\nera, thereby fostering a more diversified and productive approach to hardware\ndesign.\n', '  Recent advances in large language models have demonstrated their potential\nfor automated generation of hardware description language (HDL) code from\nhigh-level prompts. Researchers have utilized fine-tuning to enhance the\nability of these large language models (LLMs) in the field of Chip Design.\nHowever, the lack of Verilog data hinders further improvement in the quality of\nVerilog generation by LLMs. Additionally, the absence of a Verilog and\nElectronic Design Automation (EDA) script data augmentation framework\nsignificantly increases the time required to prepare the training dataset for\nLLM trainers. This paper proposes an automated design-data augmentation\nframework, which generates high-volume and high-quality natural language\naligned with Verilog and EDA scripts. For Verilog generation, it translates\nVerilog files to an abstract syntax tree and then maps nodes to natural\nlanguage with a predefined template. For Verilog repair, it uses predefined\nrules to generate the wrong verilog file and then pairs EDA Tool feedback with\nthe right and wrong verilog file. For EDA Script generation, it uses existing\nLLM(GPT-3.5) to obtain the description of the Script. To evaluate the\neffectiveness of our data augmentation method, we finetune Llama2-13B and\nLlama2-7B models using the dataset generated by our augmentation framework. The\nresults demonstrate a significant improvement in the Verilog generation tasks\nwith LLMs. Moreover, the accuracy of Verilog generation surpasses that of the\ncurrent state-of-the-art open-source Verilog generation model, increasing from\n58.8% to 70.6% with the same benchmark. Our 13B model (ChipGPT-FT) has a pass\nrate improvement compared with GPT-3.5 in Verilog generation and outperforms in\nEDA script (i.e., SiliconCompiler) generation with only 200 EDA script data.\n']",Verilog Generation and Hardware Design Automation,Hardware Design and Acceleration for AI and ML Applications,Artificial Intelligence and Machine Learning Hardware Acceleration and Optimization,Artificial Intelligence and Machine Learning Hardware Acceleration and Optimization
172,49,172_actions_action_multimodal_activities,"['actions', 'action', 'multimodal', 'activities', 'videos', 'activity', 'recognition', 'features', 'cnn', 'motion']","['action', 'skeleton', 'egocentric', 'recognition', 'video', 'exocentric', 'videos', 'motion', 'temporal', 'frames']","['  The fine-grained action analysis of the existing action datasets is\nchallenged by insufficient action categories, low fine granularities, limited\nmodalities, and tasks. In this paper, we propose a Multi-modality and\nMulti-task dataset of Figure Skating (MMFS) which was collected from the World\nFigure Skating Championships. MMFS, which possesses action recognition and\naction quality assessment, captures RGB, skeleton, and is collected the score\nof actions from 11671 clips with 256 categories including spatial and temporal\nlabels. The key contributions of our dataset fall into three aspects as\nfollows. (1) Independently spatial and temporal categories are first proposed\nto further explore fine-grained action recognition and quality assessment. (2)\nMMFS first introduces the skeleton modality for complex fine-grained action\nquality assessment. (3) Our multi-modality and multi-task dataset encourage\nmore action analysis models. To benchmark our dataset, we adopt RGB-based and\nskeleton-based baseline methods for action recognition and action quality\nassessment.\n', '  Supervised and self-supervised learning are two main training paradigms for\nskeleton-based human action recognition. However, the former one-hot\nclassification requires labor-intensive predefined action categories\nannotations, while the latter involves skeleton transformations (e.g.,\ncropping) in the pretext tasks that may impair the skeleton structure. To\naddress these challenges, we introduce a novel skeleton-based training\nframework (C$^2$VL) based on Cross-modal Contrastive learning that uses the\nprogressive distillation to learn task-agnostic human skeleton action\nrepresentation from the Vision-Language knowledge prompts. Specifically, we\nestablish the vision-language action concept space through vision-language\nknowledge prompts generated by pre-trained large multimodal models (LMMs),\nwhich enrich the fine-grained details that the skeleton action space lacks.\nMoreover, we propose the intra-modal self-similarity and inter-modal\ncross-consistency softened targets in the cross-modal contrastive process to\nprogressively control and guide the degree of pulling vision-language knowledge\nprompts and corresponding skeletons closer. These soft instance discrimination\nand self-knowledge distillation strategies contribute to the learning of better\nskeleton-based action representations from the noisy skeleton-vision-language\npairs. During the inference phase, our method requires only the skeleton data\nas the input for action recognition and no longer for vision-language prompts.\nExtensive experiments show that our method achieves state-of-the-art results on\nNTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets. The code will be available\nin the future.\n', '  One-shot skeleton action recognition, which aims to learn a skeleton action\nrecognition model with a single training sample, has attracted increasing\ninterest due to the challenge of collecting and annotating large-scale skeleton\naction data. However, most existing studies match skeleton sequences by\ncomparing their feature vectors directly which neglects spatial structures and\ntemporal orders of skeleton data. This paper presents a novel one-shot skeleton\naction recognition technique that handles skeleton action recognition via\nmulti-scale spatial-temporal feature matching. We represent skeleton data at\nmultiple spatial and temporal scales and achieve optimal feature matching from\ntwo perspectives. The first is multi-scale matching which captures the\nscale-wise semantic relevance of skeleton data at multiple spatial and temporal\nscales simultaneously. The second is cross-scale matching which handles\ndifferent motion magnitudes and speeds by capturing sample-wise relevance\nacross multiple scales. Extensive experiments over three large-scale datasets\n(NTU RGB+D, NTU RGB+D 120, and PKU-MMD) show that our method achieves superior\none-shot skeleton action recognition, and it outperforms the state-of-the-art\nconsistently by large margins.\n']",Skeleton-based Action Recognition,Human Activity and Stance Analysis with Multimodal Sensing and AI,Human Behavior Analysis with AI and Multimodal Data,Human Behavior Analysis with AI and Multimodal Data
173,49,173_counseling_psychotherapeutic_conversations_counselor,"['counseling', 'psychotherapeutic', 'conversations', 'counselor', 'psychotherapists', 'dialogue', 'counselors', 'dialogues', 'conversational', 'interventions']","['mental', 'counseling', 'health', 'psychological', 'therapists', 'psychotherapy', 'therapy', 'psychiatric', 'counselors', 'dialogue']","['  Recently, the demand for psychological counseling has significantly increased\nas more individuals express concerns about their mental health. This surge has\naccelerated efforts to improve the accessibility of counseling by using large\nlanguage models (LLMs) as counselors. To ensure client privacy, training\nopen-source LLMs faces a key challenge: the absence of realistic counseling\ndatasets. To address this, we introduce Cactus, a multi-turn dialogue dataset\nthat emulates real-life interactions using the goal-oriented and structured\napproach of Cognitive Behavioral Therapy (CBT). We create a diverse and\nrealistic dataset by designing clients with varied, specific personas, and\nhaving counselors systematically apply CBT techniques in their interactions. To\nassess the quality of our data, we benchmark against established psychological\ncriteria used to evaluate real counseling sessions, ensuring alignment with\nexpert evaluations. Experimental results demonstrate that Camel, a model\ntrained with Cactus, outperforms other models in counseling skills,\nhighlighting its effectiveness and potential as a counseling agent. We make our\ndata, model, and code publicly available.\n', '  The advent of large language models (LLMs) has significantly advanced various\nfields, including natural language processing and automated dialogue systems.\nThis paper explores the application of LLMs in psychological counseling,\naddressing the increasing demand for mental health services. We present a\nmethod for instruction tuning LLMs with specialized prompts to enhance their\nperformance in providing empathetic, relevant, and supportive responses. Our\napproach involves developing a comprehensive dataset of counseling-specific\nprompts, refining them through feedback from professional counselors, and\nconducting rigorous evaluations using both automatic metrics and human\nassessments. The results demonstrate that our instruction-tuned model\noutperforms several baseline LLMs, highlighting its potential as a scalable and\naccessible tool for mental health support.\n', ""  Global rates of mental health concerns are rising, and there is increasing\nrealization that existing models of mental health care will not adequately\nexpand to meet the demand. With the emergence of large language models (LLMs)\nhas come great optimism regarding their promise to create novel, large-scale\nsolutions to support mental health. Despite their nascence, LLMs have already\nbeen applied to mental health related tasks. In this paper, we summarize the\nextant literature on efforts to use LLMs to provide mental health education,\nassessment, and intervention and highlight key opportunities for positive\nimpact in each area. We then highlight risks associated with LLMs' application\nto mental health and encourage the adoption of strategies to mitigate these\nrisks. The urgent need for mental health support must be balanced with\nresponsible development, testing, and deployment of mental health LLMs. It is\nespecially critical to ensure that mental health LLMs are fine-tuned for mental\nhealth, enhance mental health equity, and adhere to ethical standards and that\npeople, including those with lived experience with mental health concerns, are\ninvolved in all stages from development through deployment. Prioritizing these\nefforts will minimize potential harms to mental health and maximize the\nlikelihood that LLMs will positively impact mental health globally.\n""]",Large Language Models for Mental Health Counseling,Applications of Large Language Models in Healthcare,Large Language Models,Large Language Models
173,49,173_counseling_psychotherapeutic_conversations_counselor,"['counseling', 'psychotherapeutic', 'conversations', 'counselor', 'psychotherapists', 'dialogue', 'counselors', 'dialogues', 'conversational', 'interventions']","['mental', 'counseling', 'health', 'psychological', 'therapists', 'psychotherapy', 'therapy', 'psychiatric', 'counselors', 'dialogue']","['  Recently, the demand for psychological counseling has significantly increased\nas more individuals express concerns about their mental health. This surge has\naccelerated efforts to improve the accessibility of counseling by using large\nlanguage models (LLMs) as counselors. To ensure client privacy, training\nopen-source LLMs faces a key challenge: the absence of realistic counseling\ndatasets. To address this, we introduce Cactus, a multi-turn dialogue dataset\nthat emulates real-life interactions using the goal-oriented and structured\napproach of Cognitive Behavioral Therapy (CBT). We create a diverse and\nrealistic dataset by designing clients with varied, specific personas, and\nhaving counselors systematically apply CBT techniques in their interactions. To\nassess the quality of our data, we benchmark against established psychological\ncriteria used to evaluate real counseling sessions, ensuring alignment with\nexpert evaluations. Experimental results demonstrate that Camel, a model\ntrained with Cactus, outperforms other models in counseling skills,\nhighlighting its effectiveness and potential as a counseling agent. We make our\ndata, model, and code publicly available.\n', '  The advent of large language models (LLMs) has significantly advanced various\nfields, including natural language processing and automated dialogue systems.\nThis paper explores the application of LLMs in psychological counseling,\naddressing the increasing demand for mental health services. We present a\nmethod for instruction tuning LLMs with specialized prompts to enhance their\nperformance in providing empathetic, relevant, and supportive responses. Our\napproach involves developing a comprehensive dataset of counseling-specific\nprompts, refining them through feedback from professional counselors, and\nconducting rigorous evaluations using both automatic metrics and human\nassessments. The results demonstrate that our instruction-tuned model\noutperforms several baseline LLMs, highlighting its potential as a scalable and\naccessible tool for mental health support.\n', ""  Global rates of mental health concerns are rising, and there is increasing\nrealization that existing models of mental health care will not adequately\nexpand to meet the demand. With the emergence of large language models (LLMs)\nhas come great optimism regarding their promise to create novel, large-scale\nsolutions to support mental health. Despite their nascence, LLMs have already\nbeen applied to mental health related tasks. In this paper, we summarize the\nextant literature on efforts to use LLMs to provide mental health education,\nassessment, and intervention and highlight key opportunities for positive\nimpact in each area. We then highlight risks associated with LLMs' application\nto mental health and encourage the adoption of strategies to mitigate these\nrisks. The urgent need for mental health support must be balanced with\nresponsible development, testing, and deployment of mental health LLMs. It is\nespecially critical to ensure that mental health LLMs are fine-tuned for mental\nhealth, enhance mental health equity, and adhere to ethical standards and that\npeople, including those with lived experience with mental health concerns, are\ninvolved in all stages from development through deployment. Prioritizing these\nefforts will minimize potential harms to mental health and maximize the\nlikelihood that LLMs will positively impact mental health globally.\n""]",Large Language Models for Mental Health Counseling,Applications of Large Language Models in Healthcare,Large Language Models,Large Language Models
174,49,174_privacy_federated_datasets_learning,"['privacy', 'federated', 'datasets', 'learning', 'personalized', 'distributed', 'sharing', 'data', 'collaborative', 'healthcare']","['federated', 'medical', 'healthcare', 'privacy', 'patient', 'heterogeneity', 'clients', 'clinical', 'centralized', 'data']","[""  Machine learning (ML) and Artificial Intelligence (AI) have fueled remarkable\nadvancements, particularly in healthcare. Within medical imaging, ML models\nhold the promise of improving disease diagnoses, treatment planning, and\npost-treatment monitoring. Various computer vision tasks like image\nclassification, object detection, and image segmentation are poised to become\nroutine in clinical analysis. However, privacy concerns surrounding patient\ndata hinder the assembly of large training datasets needed for developing and\ntraining accurate, robust, and generalizable models. Federated Learning (FL)\nemerges as a compelling solution, enabling organizations to collaborate on ML\nmodel training by sharing model training information (gradients) rather than\ndata (e.g., medical images). FL's distributed learning framework facilitates\ninter-institutional collaboration while preserving patient privacy. However,\nFL, while robust in privacy preservation, faces several challenges. Sensitive\ninformation can still be gleaned from shared gradients that are passed on\nbetween organizations during model training. Additionally, in medical imaging,\nquantifying model confidence\\uncertainty accurately is crucial due to the noise\nand artifacts present in the data. Uncertainty estimation in FL encounters\nunique hurdles due to data heterogeneity across organizations. This paper\noffers a comprehensive review of FL, privacy preservation, and uncertainty\nestimation, with a focus on medical imaging. Alongside a survey of current\nresearch, we identify gaps in the field and suggest future directions for FL\nresearch to enhance privacy and address noisy medical imaging data challenges.\n"", ""  Data privacy has become a major concern in healthcare due to the increasing\ndigitization of medical records and data-driven medical research. Protecting\nsensitive patient information from breaches and unauthorized access is\ncritical, as such incidents can have severe legal and ethical complications.\nFederated Learning (FL) addresses this concern by enabling multiple healthcare\ninstitutions to collaboratively learn from decentralized data without sharing\nit. FL's scope in healthcare covers areas such as disease prediction, treatment\ncustomization, and clinical trial research. However, implementing FL poses\nchallenges, including model convergence in non-IID (independent and identically\ndistributed) data environments, communication overhead, and managing\nmulti-institutional collaborations. A systematic review of FL in healthcare is\nnecessary to evaluate how effectively FL can provide privacy while maintaining\nthe integrity and usability of medical data analysis. In this study, we analyze\nexisting literature on FL applications in healthcare. We explore the current\nstate of model security practices, identify prevalent challenges, and discuss\npractical applications and their implications. Additionally, the review\nhighlights promising future research directions to refine FL implementations,\nenhance data security protocols, and expand FL's use to broader healthcare\napplications, which will benefit future researchers and practitioners.\n"", '  Distributed training can facilitate the processing of large medical image\ndatasets, and improve the accuracy and efficiency of disease diagnosis while\nprotecting patient privacy, which is crucial for achieving efficient medical\nimage analysis and accelerating medical research progress. This paper presents\nan innovative approach to medical image classification, leveraging Federated\nLearning (FL) to address the dual challenges of data privacy and efficient\ndisease diagnosis. Traditional Centralized Machine Learning models, despite\ntheir widespread use in medical imaging for tasks such as disease diagnosis,\nraise significant privacy concerns due to the sensitive nature of patient data.\nAs an alternative, FL emerges as a promising solution by allowing the training\nof a collective global model across local clients without centralizing the\ndata, thus preserving privacy. Focusing on the application of FL in Magnetic\nResonance Imaging (MRI) brain tumor detection, this study demonstrates the\neffectiveness of the Federated Learning framework coupled with EfficientNet-B0\nand the FedAvg algorithm in enhancing both privacy and diagnostic accuracy.\nThrough a meticulous selection of preprocessing methods, algorithms, and\nhyperparameters, and a comparative analysis of various Convolutional Neural\nNetwork (CNN) architectures, the research uncovers optimal strategies for image\nclassification. The experimental results reveal that EfficientNet-B0\noutperforms other models like ResNet in handling data heterogeneity and\nachieving higher accuracy and lower loss, highlighting the potential of FL in\novercoming the limitations of traditional models. The study underscores the\nsignificance of addressing data heterogeneity and proposes further research\ndirections for broadening the applicability of FL in medical image analysis.\n']",Federated Learning for Healthcare Data Privacy,Federated Learning for Privacy-Preserving Machine Learning,Machine Learning and Data Privacy,Machine Learning and Data Privacy
175,49,175_retina_retinal_macular_retinopathy,"['retina', 'retinal', 'macular', 'retinopathy', 'ophthalmologists', 'ophthalmology', 'ocular', 'blindness', 'glaucoma', 'supervised']","['retinal', 'fundus', 'glaucoma', 'retinopathy', 'diabetic', 'disease', 'optic', 'eye', 'diseases', 'images']","['  Retinal fundus images play a crucial role in the early detection of eye\ndiseases and, using deep learning approaches, recent studies have even\ndemonstrated their potential for detecting cardiovascular risk factors and\nneurological disorders. However, the impact of technical factors on these\nimages can pose challenges for reliable AI applications in ophthalmology. For\nexample, large fundus cohorts are often confounded by factors like camera type,\nimage quality or illumination level, bearing the risk of learning shortcuts\nrather than the causal relationships behind the image generation process. Here,\nwe introduce a novel population model for retinal fundus images that\neffectively disentangles patient attributes from camera effects, thus enabling\ncontrollable and highly realistic image generation. To achieve this, we propose\na novel disentanglement loss based on distance correlation. Through qualitative\nand quantitative analyses, we demonstrate the effectiveness of this novel loss\nfunction in disentangling the learned subspaces. Our results show that our\nmodel provides a new perspective on the complex relationship between patient\nattributes and technical confounders in retinal fundus image generation.\n', '  Optical coherence tomography (OCT) is a non-invasive imaging technique with\nextensive clinical applications in ophthalmology. OCT enables the visualization\nof the retinal layers, playing a vital role in the early detection and\nmonitoring of retinal diseases. OCT uses the principle of light wave\ninterference to create detailed images of the retinal microstructures, making\nit a valuable tool for diagnosing ocular conditions. This work presents an\nopen-access OCT dataset (OCTDL) comprising over 2000 OCT images labeled\naccording to disease group and retinal pathology. The dataset consists of OCT\nrecords of patients with Age-related Macular Degeneration (AMD), Diabetic\nMacular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery Occlusion (RAO),\nRetinal Vein Occlusion (RVO), and Vitreomacular Interface Disease (VID). The\nimages were acquired with an Optovue Avanti RTVue XR using raster scanning\nprotocols with dynamic scan length and image resolution. Each retinal b-scan\nwas acquired by centering on the fovea and interpreted and cataloged by an\nexperienced retinal specialist. In this work, we applied Deep Learning\nclassification techniques to this new open-access dataset.\n', '  Retinal optical coherence tomography (OCT) images provide crucial insights\ninto the health of the posterior ocular segment. Therefore, the advancement of\nautomated image analysis methods is imperative to equip clinicians and\nresearchers with quantitative data, thereby facilitating informed\ndecision-making. The application of deep learning (DL)-based approaches has\ngained extensive traction for executing these analysis tasks, demonstrating\nremarkable performance compared to labor-intensive manual analyses. However,\nthe acquisition of Retinal OCT images often presents challenges stemming from\nprivacy concerns and the resource-intensive labeling procedures, which\ncontradicts the prevailing notion that DL models necessitate substantial data\nvolumes for achieving superior performance. Moreover, limitations in available\ncomputational resources constrain the progress of high-performance medical\nartificial intelligence, particularly in less developed regions and countries.\nThis paper introduces a novel ensemble learning mechanism designed for\nrecognizing retinal diseases under limited resources (e.g., data, computation).\nThe mechanism leverages insights from multiple pre-trained models, facilitating\nthe transfer and adaptation of their knowledge to Retinal OCT images. This\napproach establishes a robust model even when confronted with limited labeled\ndata, eliminating the need for an extensive array of parameters, as required in\nlearning from scratch. Comprehensive experimentation on real-world datasets\ndemonstrates that the proposed approach can achieve superior performance in\nrecognizing Retinal OCT images, even when dealing with exceedingly restricted\nlabeled datasets. Furthermore, this method obviates the necessity of learning\nextensive-scale parameters, making it well-suited for deployment in\nlow-resource scenarios.\n']",Retinal Imaging and Ophthalmology Analysis,Retinal Imaging and Analysis in Ophthalmology,Eye and Vision Research,Eye and Vision Research
176,49,176_reinforcement_agents_agent_learning,"['reinforcement', 'agents', 'agent', 'learning', 'rewards', 'multiagent', 'reward', 'starcraft', 'cooperation', 'exploration']","['agent', 'agents', 'team', 'reinforcement', 'cooperative', 'multi', 'policies', 'heterogeneous', 'cooperation', 'coordination']","['  Recently, deep multi-agent reinforcement learning (MARL) has gained\nsignificant popularity due to its success in various cooperative multi-agent\ntasks. However, exploration still remains a challenging problem in MARL due to\nthe partial observability of the agents and the exploration space that can grow\nexponentially as the number of agents increases. Firstly, in order to address\nthe scalability issue of the exploration space, we define a formation-based\nequivalence relation on the exploration space and aim to reduce the search\nspace by exploring only meaningful states in different formations. Then, we\npropose a novel formation-aware exploration (FoX) framework that encourages\npartially observable agents to visit the states in diverse formations by\nguiding them to be well aware of their current formation solely based on their\nown observations. Numerical results show that the proposed FoX framework\nsignificantly outperforms the state-of-the-art MARL algorithms on Google\nResearch Football (GRF) and sparse Starcraft II multi-agent challenge (SMAC)\ntasks.\n', '  Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in\ntackling complex tasks that require collaboration and competition among agents\nin dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch\nis arduous and may not always be feasible, particularly for MASs with a large\nnumber of interactive agents due to the extensive sample complexity. Therefore,\nreusing knowledge gained from past experiences or other agents could\nefficiently accelerate the learning process and upscale MARL algorithms. In\nthis study, we introduce a novel framework that enables transfer learning for\nMARL through unifying various state spaces into fixed-size inputs that allow\none unified deep-learning policy viable in different scenarios within a MAS. We\nevaluated our approach in a range of scenarios within the StarCraft Multi-Agent\nChallenge (SMAC) environment, and the findings show significant enhancements in\nmulti-agent learning performance using maneuvering skills learned from other\nscenarios compared to agents learning from scratch. Furthermore, we adopted\nCurriculum Transfer Learning (CTL), enabling our deep learning policy to\nprogressively acquire knowledge and skills across pre-designed homogeneous\nlearning scenarios organized by difficulty levels. This process promotes inter-\nand intra-agent knowledge transfer, leading to high multi-agent learning\nperformance in more complicated heterogeneous scenarios.\n', ""  The rise of multi-agent systems, especially the success of multi-agent\nreinforcement learning (MARL), is reshaping our future across diverse domains\nlike autonomous vehicle networks. However, MARL still faces significant\nchallenges, particularly in achieving zero-shot scalability, which allows\ntrained MARL models to be directly applied to unseen tasks with varying numbers\nof agents. In addition, real-world multi-agent systems usually contain agents\nwith different functions and strategies, while the existing scalable MARL\nmethods only have limited heterogeneity. To address this, we propose a novel\nMARL framework named Scalable and Heterogeneous Proximal Policy Optimization\n(SHPPO), integrating heterogeneity into parameter-shared PPO-based MARL\nnetworks. we first leverage a latent network to adaptively learn strategy\npatterns for each agent. Second, we introduce a heterogeneous layer for\ndecision-making, whose parameters are specifically generated by the learned\nlatent variables. Our approach is scalable as all the parameters are shared\nexcept for the heterogeneous layer, and gains both inter-individual and\ntemporal heterogeneity at the same time. We implement our approach based on the\nstate-of-the-art backbone PPO-based algorithm as SHPPO, while our approach is\nagnostic to the backbone and can be seamlessly plugged into any\nparameter-shared MARL method. SHPPO exhibits superior performance over the\nbaselines such as MAPPO and HAPPO in classic MARL environments like Starcraft\nMulti-Agent Challenge (SMAC) and Google Research Football (GRF), showcasing\nenhanced zero-shot scalability and offering insights into the learned latent\nrepresentation's impact on team performance by visualization.\n""]",Multi-Agent Reinforcement Learning (MARL) Exploration,Multi-Agent Decision Making and Reinforcement Learning,Multi-Agent Systems and Artificial Intelligence,Multi-Agent Systems and Artificial Intelligence
177,49,177_captioning_captions_caption_multimodal,"['captioning', 'captions', 'caption', 'multimodal', 'alttexts', 'visual', 'coco', 'descriptions', 'images', 'nlg']","['captions', 'captioning', 'image', 'caption', 'descriptions', 'text', 'images', 'visual', 'news', 'metrics']","['  The objective of image captioning models is to bridge the gap between the\nvisual and linguistic modalities by generating natural language descriptions\nthat accurately reflect the content of input images. In recent years,\nresearchers have leveraged deep learning-based models and made advances in the\nextraction of visual features and the design of multimodal connections to\ntackle this task. This work presents a novel approach towards developing image\ncaptioning models that utilize an external kNN memory to improve the generation\nprocess. Specifically, we propose two model variants that incorporate a\nknowledge retriever component that is based on visual similarities, a\ndifferentiable encoder to represent input images, and a kNN-augmented language\nmodel to predict tokens based on contextual cues and text retrieved from the\nexternal memory. We experimentally validate our approach on COCO and nocaps\ndatasets and demonstrate that incorporating an explicit external memory can\nsignificantly enhance the quality of captions, especially with a larger\nretrieval corpus. This work provides valuable insights into retrieval-augmented\ncaptioning models and opens up new avenues for improving image captioning at a\nlarger scale.\n', '  Effectively aligning with human judgment when evaluating machine-generated\nimage captions represents a complex yet intriguing challenge. Existing\nevaluation metrics like CIDEr or CLIP-Score fall short in this regard as they\ndo not take into account the corresponding image or lack the capability of\nencoding fine-grained details and penalizing hallucinations. To overcome these\nissues, in this paper, we propose BRIDGE, a new learnable and reference-free\nimage captioning metric that employs a novel module to map visual features into\ndense vectors and integrates them into multi-modal pseudo-captions which are\nbuilt during the evaluation process. This approach results in a multimodal\nmetric that properly incorporates information from the input image without\nrelying on reference captions, bridging the gap between human judgment and\nmachine-generated image captions. Experiments spanning several datasets\ndemonstrate that our proposal achieves state-of-the-art results compared to\nexisting reference-free evaluation scores. Our source code and trained models\nare publicly available at: https://github.com/aimagelab/bridge-score.\n', '  Training image captioning models using teacher forcing results in very\ngeneric samples, whereas more distinctive captions can be very useful in\nretrieval applications or to produce alternative texts describing images for\naccessibility. Reinforcement Learning (RL) allows to use cross-modal retrieval\nsimilarity score between the generated caption and the input image as reward to\nguide the training, leading to more distinctive captions. Recent studies show\nthat pre-trained cross-modal retrieval models can be used to provide this\nreward, completely eliminating the need for reference captions. However, we\nargue in this paper that Ground Truth (GT) captions can still be useful in this\nRL framework. We propose a new image captioning model training strategy that\nmakes use of GT captions in different ways. Firstly, they can be used to train\na simple MLP discriminator that serves as a regularization to prevent reward\nhacking and ensures the fluency of generated captions, resulting in a textual\nGAN setup extended for multimodal inputs. Secondly, they can serve as\nadditional trajectories in the RL strategy, resulting in a teacher forcing loss\nweighted by the similarity of the GT to the image. This objective acts as an\nadditional learning signal grounded to the distribution of the GT captions.\nThirdly, they can serve as strong baselines when added to the pool of captions\nused to compute the proposed contrastive reward to reduce the variance of\ngradient estimate. Experiments on MS-COCO demonstrate the interest of the\nproposed training strategy to produce highly distinctive captions while\nmaintaining high writing quality.\n']",Image Captioning Models,Multimodal Vision-Language Understanding and Generation,Multimodal Learning and Vision-Language Models,Multimodal Learning
178,49,178_malware_adversarial_malicious_ransomware,"['malware', 'adversarial', 'malicious', 'ransomware', 'antivirus', 'classifiers', 'classifier', 'cybersecurity', 'executable', 'obfuscation']","['malware', 'ransomware', 'evasion', 'adversarial', 'files', 'detection', 'detectors', 'attacks', 'file', 'antivirus']","['  Malware has been one of the most damaging threats to computers that span\nacross multiple operating systems and various file formats. To defend against\never-increasing and ever-evolving malware, tremendous efforts have been made to\npropose a variety of malware detection that attempt to effectively and\nefficiently detect malware so as to mitigate possible damages as early as\npossible. Recent studies have shown that, on the one hand, existing ML and DL\ntechniques enable superior solutions in detecting newly emerging and previously\nunseen malware. However, on the other hand, ML and DL models are inherently\nvulnerable to adversarial attacks in the form of adversarial examples. In this\npaper, we focus on malware with the file format of portable executable (PE) in\nthe family of Windows operating systems, namely Windows PE malware, as a\nrepresentative case to study the adversarial attack methods in such adversarial\nsettings. To be specific, we start by first outlining the general learning\nframework of Windows PE malware detection based on ML/DL and subsequently\nhighlighting three unique challenges of performing adversarial attacks in the\ncontext of Windows PE malware. Then, we conduct a comprehensive and systematic\nreview to categorize the state-of-the-art adversarial attacks against PE\nmalware detection, as well as corresponding defenses to increase the robustness\nof Windows PE malware detection. Finally, we conclude the paper by first\npresenting other related attacks against Windows PE malware detection beyond\nthe adversarial attacks and then shedding light on future research directions\nand opportunities. In addition, a curated resource list of adversarial attacks\nand defenses for Windows PE malware detection is also available at\nhttps://github.com/ryderling/adversarial-attacks-and-defenses-for-windows-pe-malware-detection.\n', '  Machine learning has proven to be a useful tool for automated malware\ndetection, but machine learning models have also been shown to be vulnerable to\nadversarial attacks. This article addresses the problem of generating\nadversarial malware samples, specifically malicious Windows Portable Executable\nfiles. We summarize and compare work that has focused on adversarial machine\nlearning for malware detection. We use gradient-based, evolutionary\nalgorithm-based, and reinforcement-based methods to generate adversarial\nsamples, and then test the generated samples against selected antivirus\nproducts. We compare the selected methods in terms of accuracy and practical\napplicability. The results show that applying optimized modifications to\npreviously detected malware can lead to incorrect classification of the file as\nbenign. It is also known that generated malware samples can be successfully\nused against detection models other than those used to generate them and that\nusing combinations of generators can create new samples that evade detection.\nExperiments show that the Gym-malware generator, which uses a reinforcement\nlearning approach, has the greatest practical potential. This generator\nachieved an average sample generation time of 5.73 seconds and the highest\naverage evasion rate of 44.11%. Using the Gym-malware generator in combination\nwith itself improved the evasion rate to 58.35%.\n', '  Malware attacks have become significantly more frequent and sophisticated in\nrecent years. Therefore, malware detection and classification are critical\ncomponents of information security. Due to the large amount of malware samples\navailable, it is essential to categorize malware samples according to their\nmalicious characteristics. Clustering algorithms are thus becoming more widely\nused in computer security to analyze the behavior of malware variants and\ndiscover new malware families. Online clustering algorithms help us to\nunderstand malware behavior and produce a quicker response to new threats. This\npaper introduces a novel machine learning-based model for the online clustering\nof malicious samples into malware families. Streaming data is divided according\nto the clustering decision rule into samples from known and new emerging\nmalware families. The streaming data is classified using the weighted k-nearest\nneighbor classifier into known families, and the online k-means algorithm\nclusters the remaining streaming data and achieves a purity of clusters from\n90.20% for four clusters to 93.34% for ten clusters. This work is based on\nstatic analysis of portable executable files for the Windows operating system.\nExperimental results indicate that the proposed online clustering model can\ncreate high-purity clusters corresponding to malware families. This allows\nmalware analysts to receive similar malware samples, speeding up their\nanalysis.\n']",Adversarial Attacks on Malware Detection in Windows PE Files,Cybersecurity Threat Detection and Prevention,Cybersecurity and Artificial Intelligence,Cybersecurity and Artificial Intelligence
179,49,179_unlearning_ununlearning_unlearn_memorization,"['unlearning', 'ununlearning', 'unlearn', 'memorization', 'forgetting', 'memorize', 'memorized', 'unlearned', 'corpus', 'forget']","['unlearning', 'forget', 'unlearned', 'memorization', 'harmful', 'knowledge', 'copyright', 'sensitive', 'ascent', 'undesirable']","['  Large Language Models (LLMs) have highlighted the necessity of effective\nunlearning mechanisms to comply with data regulations and ethical AI practices.\nLLM unlearning aims at removing undesired data influences and associated model\ncapabilities without compromising utility beyond the scope of unlearning. While\ninterest in studying LLM unlearning is growing, the impact of the optimizer\nchoice for LLM unlearning remains unexplored. In this work, we shed light on\nthe significance of optimizer selection in LLM unlearning for the first time,\nestablishing a clear connection between second-order optimization and influence\nunlearning (a classical approach using influence functions to update the model\nfor data influence removal). This insight propels us to develop a second-order\noptimization-based LLM unlearning framework, termed Second-Order UnLearning\n(SOUL), which extends the static, one-shot model update using influence\nunlearning to a dynamic, iterative unlearning process. Our extensive\nexperiments show that SOUL consistently outperforms conventional first-order\nmethods across various unlearning tasks, models, and metrics, indicating that\nsecond-order optimization offers an effective and broadly applicable solution\nfor LLM unlearning. Codes are available at https://github.com/OPTML-Group/SOUL.\n', '  Despite the strong capabilities of Large Language Models (LLMs) to acquire\nknowledge from their training corpora, the memorization of sensitive\ninformation in the corpora such as copyrighted, harmful, and private content\nhas led to ethical and legal concerns. In response to these challenges,\nunlearning has emerged as a potential remedy for LLMs affected by problematic\ntraining data. However, previous unlearning techniques are either not\napplicable to black-box LLMs due to required access to model internal weights,\nor violate data protection principles by retaining sensitive data for\ninference-time correction. We propose $\\delta$-unlearning, an offset unlearning\nframework for black-box LLMs. Instead of tuning the black-box LLM itself,\n$\\delta$-unlearning learns the logit offset needed for unlearning by\ncontrasting the logits from a pair of smaller models. Experiments demonstrate\nthat $\\delta$-unlearning can effectively unlearn target data while maintaining\nsimilar or even stronger performance on general out-of-forget-scope tasks.\n$\\delta$-unlearning also effectively incorporates different unlearning\nalgorithms, making our approach a versatile solution to adapting various\nexisting unlearning algorithms to black-box LLMs.\n', '  Large language models trained on massive corpora of data from the web can\nmemorize and reproduce sensitive or private data raising both legal and ethical\nconcerns. Unlearning, or tuning models to forget information present in their\ntraining data, provides us with a way to protect private data after training.\nAlthough several methods exist for such unlearning, it is unclear to what\nextent they result in models equivalent to those where the data to be forgotten\nwas never learned in the first place. To address this challenge, we present\nTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen\nour understanding of unlearning. We offer a dataset of 200 diverse synthetic\nauthor profiles, each consisting of 20 question-answer pairs, and a subset of\nthese profiles called the forget set that serves as the target for unlearning.\nWe compile a suite of metrics that work together to provide a holistic picture\nof unlearning efficacy. Finally, we provide a set of baseline results from\nexisting unlearning algorithms. Importantly, none of the baselines we consider\nshow effective unlearning motivating continued efforts to develop approaches\nfor unlearning that effectively tune models so that they truly behave as if\nthey were never trained on the forget data at all.\n']",Unlearning in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
179,49,179_unlearning_ununlearning_unlearn_memorization,"['unlearning', 'ununlearning', 'unlearn', 'memorization', 'forgetting', 'memorize', 'memorized', 'unlearned', 'corpus', 'forget']","['unlearning', 'forget', 'unlearned', 'memorization', 'harmful', 'knowledge', 'copyright', 'sensitive', 'ascent', 'undesirable']","['  Large Language Models (LLMs) have highlighted the necessity of effective\nunlearning mechanisms to comply with data regulations and ethical AI practices.\nLLM unlearning aims at removing undesired data influences and associated model\ncapabilities without compromising utility beyond the scope of unlearning. While\ninterest in studying LLM unlearning is growing, the impact of the optimizer\nchoice for LLM unlearning remains unexplored. In this work, we shed light on\nthe significance of optimizer selection in LLM unlearning for the first time,\nestablishing a clear connection between second-order optimization and influence\nunlearning (a classical approach using influence functions to update the model\nfor data influence removal). This insight propels us to develop a second-order\noptimization-based LLM unlearning framework, termed Second-Order UnLearning\n(SOUL), which extends the static, one-shot model update using influence\nunlearning to a dynamic, iterative unlearning process. Our extensive\nexperiments show that SOUL consistently outperforms conventional first-order\nmethods across various unlearning tasks, models, and metrics, indicating that\nsecond-order optimization offers an effective and broadly applicable solution\nfor LLM unlearning. Codes are available at https://github.com/OPTML-Group/SOUL.\n', '  Despite the strong capabilities of Large Language Models (LLMs) to acquire\nknowledge from their training corpora, the memorization of sensitive\ninformation in the corpora such as copyrighted, harmful, and private content\nhas led to ethical and legal concerns. In response to these challenges,\nunlearning has emerged as a potential remedy for LLMs affected by problematic\ntraining data. However, previous unlearning techniques are either not\napplicable to black-box LLMs due to required access to model internal weights,\nor violate data protection principles by retaining sensitive data for\ninference-time correction. We propose $\\delta$-unlearning, an offset unlearning\nframework for black-box LLMs. Instead of tuning the black-box LLM itself,\n$\\delta$-unlearning learns the logit offset needed for unlearning by\ncontrasting the logits from a pair of smaller models. Experiments demonstrate\nthat $\\delta$-unlearning can effectively unlearn target data while maintaining\nsimilar or even stronger performance on general out-of-forget-scope tasks.\n$\\delta$-unlearning also effectively incorporates different unlearning\nalgorithms, making our approach a versatile solution to adapting various\nexisting unlearning algorithms to black-box LLMs.\n', '  Large language models trained on massive corpora of data from the web can\nmemorize and reproduce sensitive or private data raising both legal and ethical\nconcerns. Unlearning, or tuning models to forget information present in their\ntraining data, provides us with a way to protect private data after training.\nAlthough several methods exist for such unlearning, it is unclear to what\nextent they result in models equivalent to those where the data to be forgotten\nwas never learned in the first place. To address this challenge, we present\nTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen\nour understanding of unlearning. We offer a dataset of 200 diverse synthetic\nauthor profiles, each consisting of 20 question-answer pairs, and a subset of\nthese profiles called the forget set that serves as the target for unlearning.\nWe compile a suite of metrics that work together to provide a holistic picture\nof unlearning efficacy. Finally, we provide a set of baseline results from\nexisting unlearning algorithms. Importantly, none of the baselines we consider\nshow effective unlearning motivating continued efforts to develop approaches\nfor unlearning that effectively tune models so that they truly behave as if\nthey were never trained on the forget data at all.\n']",Unlearning in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
179,49,179_unlearning_ununlearning_unlearn_memorization,"['unlearning', 'ununlearning', 'unlearn', 'memorization', 'forgetting', 'memorize', 'memorized', 'unlearned', 'corpus', 'forget']","['unlearning', 'forget', 'unlearned', 'memorization', 'harmful', 'knowledge', 'copyright', 'sensitive', 'ascent', 'undesirable']","['  Large Language Models (LLMs) have highlighted the necessity of effective\nunlearning mechanisms to comply with data regulations and ethical AI practices.\nLLM unlearning aims at removing undesired data influences and associated model\ncapabilities without compromising utility beyond the scope of unlearning. While\ninterest in studying LLM unlearning is growing, the impact of the optimizer\nchoice for LLM unlearning remains unexplored. In this work, we shed light on\nthe significance of optimizer selection in LLM unlearning for the first time,\nestablishing a clear connection between second-order optimization and influence\nunlearning (a classical approach using influence functions to update the model\nfor data influence removal). This insight propels us to develop a second-order\noptimization-based LLM unlearning framework, termed Second-Order UnLearning\n(SOUL), which extends the static, one-shot model update using influence\nunlearning to a dynamic, iterative unlearning process. Our extensive\nexperiments show that SOUL consistently outperforms conventional first-order\nmethods across various unlearning tasks, models, and metrics, indicating that\nsecond-order optimization offers an effective and broadly applicable solution\nfor LLM unlearning. Codes are available at https://github.com/OPTML-Group/SOUL.\n', '  Despite the strong capabilities of Large Language Models (LLMs) to acquire\nknowledge from their training corpora, the memorization of sensitive\ninformation in the corpora such as copyrighted, harmful, and private content\nhas led to ethical and legal concerns. In response to these challenges,\nunlearning has emerged as a potential remedy for LLMs affected by problematic\ntraining data. However, previous unlearning techniques are either not\napplicable to black-box LLMs due to required access to model internal weights,\nor violate data protection principles by retaining sensitive data for\ninference-time correction. We propose $\\delta$-unlearning, an offset unlearning\nframework for black-box LLMs. Instead of tuning the black-box LLM itself,\n$\\delta$-unlearning learns the logit offset needed for unlearning by\ncontrasting the logits from a pair of smaller models. Experiments demonstrate\nthat $\\delta$-unlearning can effectively unlearn target data while maintaining\nsimilar or even stronger performance on general out-of-forget-scope tasks.\n$\\delta$-unlearning also effectively incorporates different unlearning\nalgorithms, making our approach a versatile solution to adapting various\nexisting unlearning algorithms to black-box LLMs.\n', '  Large language models trained on massive corpora of data from the web can\nmemorize and reproduce sensitive or private data raising both legal and ethical\nconcerns. Unlearning, or tuning models to forget information present in their\ntraining data, provides us with a way to protect private data after training.\nAlthough several methods exist for such unlearning, it is unclear to what\nextent they result in models equivalent to those where the data to be forgotten\nwas never learned in the first place. To address this challenge, we present\nTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen\nour understanding of unlearning. We offer a dataset of 200 diverse synthetic\nauthor profiles, each consisting of 20 question-answer pairs, and a subset of\nthese profiles called the forget set that serves as the target for unlearning.\nWe compile a suite of metrics that work together to provide a holistic picture\nof unlearning efficacy. Finally, we provide a set of baseline results from\nexisting unlearning algorithms. Importantly, none of the baselines we consider\nshow effective unlearning motivating continued efforts to develop approaches\nfor unlearning that effectively tune models so that they truly behave as if\nthey were never trained on the forget data at all.\n']",Unlearning in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
179,49,179_unlearning_ununlearning_unlearn_memorization,"['unlearning', 'ununlearning', 'unlearn', 'memorization', 'forgetting', 'memorize', 'memorized', 'unlearned', 'corpus', 'forget']","['unlearning', 'forget', 'unlearned', 'memorization', 'harmful', 'knowledge', 'copyright', 'sensitive', 'ascent', 'undesirable']","['  Large Language Models (LLMs) have highlighted the necessity of effective\nunlearning mechanisms to comply with data regulations and ethical AI practices.\nLLM unlearning aims at removing undesired data influences and associated model\ncapabilities without compromising utility beyond the scope of unlearning. While\ninterest in studying LLM unlearning is growing, the impact of the optimizer\nchoice for LLM unlearning remains unexplored. In this work, we shed light on\nthe significance of optimizer selection in LLM unlearning for the first time,\nestablishing a clear connection between second-order optimization and influence\nunlearning (a classical approach using influence functions to update the model\nfor data influence removal). This insight propels us to develop a second-order\noptimization-based LLM unlearning framework, termed Second-Order UnLearning\n(SOUL), which extends the static, one-shot model update using influence\nunlearning to a dynamic, iterative unlearning process. Our extensive\nexperiments show that SOUL consistently outperforms conventional first-order\nmethods across various unlearning tasks, models, and metrics, indicating that\nsecond-order optimization offers an effective and broadly applicable solution\nfor LLM unlearning. Codes are available at https://github.com/OPTML-Group/SOUL.\n', '  Despite the strong capabilities of Large Language Models (LLMs) to acquire\nknowledge from their training corpora, the memorization of sensitive\ninformation in the corpora such as copyrighted, harmful, and private content\nhas led to ethical and legal concerns. In response to these challenges,\nunlearning has emerged as a potential remedy for LLMs affected by problematic\ntraining data. However, previous unlearning techniques are either not\napplicable to black-box LLMs due to required access to model internal weights,\nor violate data protection principles by retaining sensitive data for\ninference-time correction. We propose $\\delta$-unlearning, an offset unlearning\nframework for black-box LLMs. Instead of tuning the black-box LLM itself,\n$\\delta$-unlearning learns the logit offset needed for unlearning by\ncontrasting the logits from a pair of smaller models. Experiments demonstrate\nthat $\\delta$-unlearning can effectively unlearn target data while maintaining\nsimilar or even stronger performance on general out-of-forget-scope tasks.\n$\\delta$-unlearning also effectively incorporates different unlearning\nalgorithms, making our approach a versatile solution to adapting various\nexisting unlearning algorithms to black-box LLMs.\n', '  Large language models trained on massive corpora of data from the web can\nmemorize and reproduce sensitive or private data raising both legal and ethical\nconcerns. Unlearning, or tuning models to forget information present in their\ntraining data, provides us with a way to protect private data after training.\nAlthough several methods exist for such unlearning, it is unclear to what\nextent they result in models equivalent to those where the data to be forgotten\nwas never learned in the first place. To address this challenge, we present\nTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen\nour understanding of unlearning. We offer a dataset of 200 diverse synthetic\nauthor profiles, each consisting of 20 question-answer pairs, and a subset of\nthese profiles called the forget set that serves as the target for unlearning.\nWe compile a suite of metrics that work together to provide a holistic picture\nof unlearning efficacy. Finally, we provide a set of baseline results from\nexisting unlearning algorithms. Importantly, none of the baselines we consider\nshow effective unlearning motivating continued efforts to develop approaches\nfor unlearning that effectively tune models so that they truly behave as if\nthey were never trained on the forget data at all.\n']",Unlearning in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
179,49,179_unlearning_ununlearning_unlearn_memorization,"['unlearning', 'ununlearning', 'unlearn', 'memorization', 'forgetting', 'memorize', 'memorized', 'unlearned', 'corpus', 'forget']","['unlearning', 'forget', 'unlearned', 'memorization', 'harmful', 'knowledge', 'copyright', 'sensitive', 'ascent', 'undesirable']","['  Large Language Models (LLMs) have highlighted the necessity of effective\nunlearning mechanisms to comply with data regulations and ethical AI practices.\nLLM unlearning aims at removing undesired data influences and associated model\ncapabilities without compromising utility beyond the scope of unlearning. While\ninterest in studying LLM unlearning is growing, the impact of the optimizer\nchoice for LLM unlearning remains unexplored. In this work, we shed light on\nthe significance of optimizer selection in LLM unlearning for the first time,\nestablishing a clear connection between second-order optimization and influence\nunlearning (a classical approach using influence functions to update the model\nfor data influence removal). This insight propels us to develop a second-order\noptimization-based LLM unlearning framework, termed Second-Order UnLearning\n(SOUL), which extends the static, one-shot model update using influence\nunlearning to a dynamic, iterative unlearning process. Our extensive\nexperiments show that SOUL consistently outperforms conventional first-order\nmethods across various unlearning tasks, models, and metrics, indicating that\nsecond-order optimization offers an effective and broadly applicable solution\nfor LLM unlearning. Codes are available at https://github.com/OPTML-Group/SOUL.\n', '  Despite the strong capabilities of Large Language Models (LLMs) to acquire\nknowledge from their training corpora, the memorization of sensitive\ninformation in the corpora such as copyrighted, harmful, and private content\nhas led to ethical and legal concerns. In response to these challenges,\nunlearning has emerged as a potential remedy for LLMs affected by problematic\ntraining data. However, previous unlearning techniques are either not\napplicable to black-box LLMs due to required access to model internal weights,\nor violate data protection principles by retaining sensitive data for\ninference-time correction. We propose $\\delta$-unlearning, an offset unlearning\nframework for black-box LLMs. Instead of tuning the black-box LLM itself,\n$\\delta$-unlearning learns the logit offset needed for unlearning by\ncontrasting the logits from a pair of smaller models. Experiments demonstrate\nthat $\\delta$-unlearning can effectively unlearn target data while maintaining\nsimilar or even stronger performance on general out-of-forget-scope tasks.\n$\\delta$-unlearning also effectively incorporates different unlearning\nalgorithms, making our approach a versatile solution to adapting various\nexisting unlearning algorithms to black-box LLMs.\n', '  Large language models trained on massive corpora of data from the web can\nmemorize and reproduce sensitive or private data raising both legal and ethical\nconcerns. Unlearning, or tuning models to forget information present in their\ntraining data, provides us with a way to protect private data after training.\nAlthough several methods exist for such unlearning, it is unclear to what\nextent they result in models equivalent to those where the data to be forgotten\nwas never learned in the first place. To address this challenge, we present\nTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen\nour understanding of unlearning. We offer a dataset of 200 diverse synthetic\nauthor profiles, each consisting of 20 question-answer pairs, and a subset of\nthese profiles called the forget set that serves as the target for unlearning.\nWe compile a suite of metrics that work together to provide a holistic picture\nof unlearning efficacy. Finally, we provide a set of baseline results from\nexisting unlearning algorithms. Importantly, none of the baselines we consider\nshow effective unlearning motivating continued efforts to develop approaches\nfor unlearning that effectively tune models so that they truly behave as if\nthey were never trained on the forget data at all.\n']",Unlearning in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
179,49,179_unlearning_ununlearning_unlearn_memorization,"['unlearning', 'ununlearning', 'unlearn', 'memorization', 'forgetting', 'memorize', 'memorized', 'unlearned', 'corpus', 'forget']","['unlearning', 'forget', 'unlearned', 'memorization', 'harmful', 'knowledge', 'copyright', 'sensitive', 'ascent', 'undesirable']","['  Large Language Models (LLMs) have highlighted the necessity of effective\nunlearning mechanisms to comply with data regulations and ethical AI practices.\nLLM unlearning aims at removing undesired data influences and associated model\ncapabilities without compromising utility beyond the scope of unlearning. While\ninterest in studying LLM unlearning is growing, the impact of the optimizer\nchoice for LLM unlearning remains unexplored. In this work, we shed light on\nthe significance of optimizer selection in LLM unlearning for the first time,\nestablishing a clear connection between second-order optimization and influence\nunlearning (a classical approach using influence functions to update the model\nfor data influence removal). This insight propels us to develop a second-order\noptimization-based LLM unlearning framework, termed Second-Order UnLearning\n(SOUL), which extends the static, one-shot model update using influence\nunlearning to a dynamic, iterative unlearning process. Our extensive\nexperiments show that SOUL consistently outperforms conventional first-order\nmethods across various unlearning tasks, models, and metrics, indicating that\nsecond-order optimization offers an effective and broadly applicable solution\nfor LLM unlearning. Codes are available at https://github.com/OPTML-Group/SOUL.\n', '  Despite the strong capabilities of Large Language Models (LLMs) to acquire\nknowledge from their training corpora, the memorization of sensitive\ninformation in the corpora such as copyrighted, harmful, and private content\nhas led to ethical and legal concerns. In response to these challenges,\nunlearning has emerged as a potential remedy for LLMs affected by problematic\ntraining data. However, previous unlearning techniques are either not\napplicable to black-box LLMs due to required access to model internal weights,\nor violate data protection principles by retaining sensitive data for\ninference-time correction. We propose $\\delta$-unlearning, an offset unlearning\nframework for black-box LLMs. Instead of tuning the black-box LLM itself,\n$\\delta$-unlearning learns the logit offset needed for unlearning by\ncontrasting the logits from a pair of smaller models. Experiments demonstrate\nthat $\\delta$-unlearning can effectively unlearn target data while maintaining\nsimilar or even stronger performance on general out-of-forget-scope tasks.\n$\\delta$-unlearning also effectively incorporates different unlearning\nalgorithms, making our approach a versatile solution to adapting various\nexisting unlearning algorithms to black-box LLMs.\n', '  Large language models trained on massive corpora of data from the web can\nmemorize and reproduce sensitive or private data raising both legal and ethical\nconcerns. Unlearning, or tuning models to forget information present in their\ntraining data, provides us with a way to protect private data after training.\nAlthough several methods exist for such unlearning, it is unclear to what\nextent they result in models equivalent to those where the data to be forgotten\nwas never learned in the first place. To address this challenge, we present\nTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen\nour understanding of unlearning. We offer a dataset of 200 diverse synthetic\nauthor profiles, each consisting of 20 question-answer pairs, and a subset of\nthese profiles called the forget set that serves as the target for unlearning.\nWe compile a suite of metrics that work together to provide a holistic picture\nof unlearning efficacy. Finally, we provide a set of baseline results from\nexisting unlearning algorithms. Importantly, none of the baselines we consider\nshow effective unlearning motivating continued efforts to develop approaches\nfor unlearning that effectively tune models so that they truly behave as if\nthey were never trained on the forget data at all.\n']",Unlearning in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
180,48,180_ocr_handwriting_handwritten_recognition,"['ocr', 'handwriting', 'handwritten', 'recognition', 'text', 'inscriptions', 'locr', 'manuscripts', 'marksheet', 'documents']","['handwriting', 'handwritten', 'character', 'recognition', 'manuscripts', 'characters', 'script', 'ancient', 'document', 'optical']","[""  This project undertakes the training and analysis of optical character\nrecognition OCR methods applied to 10th century ancient Tamil inscriptions\ndiscovered on the walls of the Brihadeeswarar Temple.The chosen OCR methods\ninclude Tesseract,a widely used OCR engine,using modern ICR techniques to pre\nprocess the raw data and a box editing software to finetune our model.The\nanalysis with Tesseract aims to evaluate their effectiveness in accurately\ndeciphering the nuances of the ancient Tamil characters.The performance of our\nmodel for the dataset are determined by their accuracy rate where the evaluated\ndataset divided into training set and testing set.By addressing the unique\nchallenges posed by the script's historical context,this study seeks to\ncontribute valuable insights to the broader field of OCR,facilitating improved\npreservation and interpretation of ancient inscriptions\n"", '  Teaching Computer Science (CS) by having students write programs by hand on\npaper has key pedagogical advantages: It allows focused learning and requires\ncareful thinking compared to the use of Integrated Development Environments\n(IDEs) with intelligent support tools or ""just trying things out"". The familiar\nenvironment of pens and paper also lessens the cognitive load of students with\nno prior experience with computers, for whom the mere basic usage of computers\ncan be intimidating. Finally, this teaching approach opens learning\nopportunities to students with limited access to computers.\n  However, a key obstacle is the current lack of teaching methods and support\nsoftware for working with and running handwritten programs. Optical character\nrecognition (OCR) of handwritten code is challenging: Minor OCR errors, perhaps\ndue to varied handwriting styles, easily make code not run, and recognizing\nindentation is crucial for languages like Python but is difficult to do due to\ninconsistent horizontal spacing in handwriting. Our approach integrates two\ninnovative methods. The first combines OCR with an indentation recognition\nmodule and a language model designed for post-OCR error correction without\nintroducing hallucinations. This method, to our knowledge, surpasses all\nexisting systems in handwritten code recognition. It reduces error from 30\\% in\nthe state of the art to 5\\% with minimal hallucination of logical fixes to\nstudent programs. The second method leverages a multimodal language model to\nrecognize handwritten programs in an end-to-end fashion. We hope this\ncontribution can stimulate further pedagogical research and contribute to the\ngoal of making CS education universally accessible. We release a dataset of\nhandwritten programs and code to support future research at\nhttps://github.com/mdoumbouya/codeocr\n', '  The adoption of tablets with touchscreens and styluses is increasing, and a\nkey feature is converting handwriting to text, enabling search, indexing, and\nAI assistance. Meanwhile, vision-language models (VLMs) are now the go-to\nsolution for image understanding, thanks to both their state-of-the-art\nperformance across a variety of tasks and the simplicity of a unified approach\nto training, fine-tuning, and inference. While VLMs obtain high performance on\nimage-based tasks, they perform poorly on handwriting recognition when applied\nnaively, i.e., by rendering handwriting as an image and performing optical\ncharacter recognition (OCR). In this paper, we study online handwriting\nrecognition with VLMs, going beyond naive OCR. We propose a novel tokenized\nrepresentation of digital ink (online handwriting) that includes both a\ntime-ordered sequence of strokes as text, and as image. We show that this\nrepresentation yields results comparable to or better than state-of-the-art\nonline handwriting recognizers. Wide applicability is shown through results\nwith two different VLM families, on multiple public datasets. Our approach can\nbe applied to off-the-shelf VLMs, does not require any changes in their\narchitecture, and can be used in both fine-tuning and parameter-efficient\ntuning. We perform a detailed ablation study to identify the key elements of\nthe proposed representation.\n']",Optical Character Recognition of Handwritten Text,Computer Vision and Pattern Recognition,Computer Vision,Computer Vision
181,48,181_audioset_captioning_captions_audiocaps,"['audioset', 'captioning', 'captions', 'audiocaps', 'caption', 'audio', 'voice', 'audios', 'speech', 'clips']","['audio', 'speech', 'captions', 'visual', 'captioning', 'caption', 'clips', 'video', 'modality', 'acoustic']","['  Multi-modal learning in the audio-language domain has seen significant\nadvancements in recent years. However, audio-language learning faces challenges\ndue to limited and lower-quality data compared to image-language tasks.\nExisting audio-language datasets are notably smaller, and manual labeling is\nhindered by the need to listen to entire audio clips for accurate labeling.\n  Our method systematically generates audio-caption pairs by augmenting audio\nclips with natural language labels and corresponding audio signal processing\noperations. Leveraging a Large Language Model, we generate descriptions of\naugmented audio clips with a prompt template. This scalable method produces\nAudioSetMix, a high-quality training dataset for text-and-audio related models.\n  Integration of our dataset improves models performance on benchmarks by\nproviding diversified and better-aligned examples. Notably, our dataset\naddresses the absence of modifiers (adjectives and adverbs) in existing\ndatasets. By enabling models to learn these concepts, and generating hard\nnegative examples during training, we achieve state-of-the-art performance on\nmultiple benchmarks.\n', '  Humans are adept at leveraging visual cues from lip movements for recognizing\nspeech in adverse listening conditions. Audio-Visual Speech Recognition (AVSR)\nmodels follow similar approach to achieve robust speech recognition in noisy\nconditions. In this work, we present a multilingual AVSR model incorporating\nseveral enhancements to improve performance and audio noise robustness.\nNotably, we adapt the recently proposed Fast Conformer model to process both\naudio and visual modalities using a novel hybrid CTC/RNN-T architecture. We\nincrease the amount of audio-visual training data for six distinct languages,\ngenerating automatic transcriptions of unlabelled multilingual datasets\n(VoxCeleb2 and AVSpeech). Our proposed model achieves new state-of-the-art\nperformance on the LRS3 dataset, reaching WER of 0.8%. On the recently\nintroduced MuAViC benchmark, our model yields an absolute average-WER reduction\nof 11.9% in comparison to the original baseline. Finally, we demonstrate the\nability of the proposed model to perform audio-only, visual-only, and\naudio-visual speech recognition at test time.\n', '  Automated audio captioning is a cross-modal translation task for describing\nthe content of audio clips with natural language sentences. This task has\nattracted increasing attention and substantial progress has been made in recent\nyears. Captions generated by existing models are generally faithful to the\ncontent of audio clips, however, these machine-generated captions are often\ndeterministic (e.g., generating a fixed caption for a given audio clip), simple\n(e.g., using common words and simple grammar), and generic (e.g., generating\nthe same caption for similar audio clips). When people are asked to describe\nthe content of an audio clip, different people tend to focus on different sound\nevents and describe an audio clip diversely from various aspects using distinct\nwords and grammar. We believe that an audio captioning system should have the\nability to generate diverse captions, either for a fixed audio clip, or across\nsimilar audio clips. To this end, we propose an adversarial training framework\nbased on a conditional generative adversarial network (C-GAN) to improve\ndiversity of audio captioning systems. A caption generator and two hybrid\ndiscriminators compete and are learned jointly, where the caption generator can\nbe any standard encoder-decoder captioning model used to generate captions, and\nthe hybrid discriminators assess the generated captions from different\ncriteria, such as their naturalness and semantics. We conduct experiments on\nthe Clotho dataset. The results show that our proposed model can generate\ncaptions with better diversity as compared to state-of-the-art methods.\n']",Audio Captioning and Speech Recognition,Audio and Speech Processing,Speech and Audio Processing,Speech and Audio Processing
182,48,182_temporal_timelines_answering_timebench,"['temporal', 'timelines', 'answering', 'timebench', 'timeline', 'reasoning', 'timeml', 'knowledge', 'questions', 'answers']","['temporal', 'events', 'event', 'questions', 'reasoning', 'answering', 'question', 'timelines', 'timeline', 'facts']","[""  Temporal knowledge graph question answering (TKGQA) poses a significant\nchallenge task, due to the temporal constraints hidden in questions and the\nanswers sought from dynamic structured knowledge. Although large language\nmodels (LLMs) have made considerable progress in their reasoning ability over\nstructured data, their application to the TKGQA task is a relatively unexplored\narea. This paper first proposes a novel generative temporal knowledge graph\nquestion answering framework, GenTKGQA, which guides LLMs to answer temporal\nquestions through two phases: Subgraph Retrieval and Answer Generation. First,\nwe exploit LLM's intrinsic knowledge to mine temporal constraints and\nstructural links in the questions without extra training, thus narrowing down\nthe subgraph search space in both temporal and structural dimensions. Next, we\ndesign virtual knowledge indicators to fuse the graph neural network signals of\nthe subgraph and the text representations of the LLM in a non-shallow way,\nwhich helps the open-source LLM deeply understand the temporal order and\nstructural dependencies among the retrieved facts through instruction tuning.\nExperimental results on two widely used datasets demonstrate the superiority of\nour model.\n"", '  Recently, Large Language Models (LLMs) have demonstrated great potential in\nvarious data mining tasks, such as knowledge question answering, mathematical\nreasoning, and commonsense reasoning. However, the reasoning capability of LLMs\non temporal event forecasting has been under-explored. To systematically\ninvestigate their abilities in temporal event forecasting, we conduct a\ncomprehensive evaluation of LLM-based methods for temporal event forecasting.\nDue to the lack of a high-quality dataset that involves both graph and textual\ndata, we first construct a benchmark dataset, named MidEast-TE-mini. Based on\nthis dataset, we design a series of baseline methods, characterized by various\ninput formats and retrieval augmented generation(RAG) modules. From extensive\nexperiments, we find that directly integrating raw texts into the input of LLMs\ndoes not enhance zero-shot extrapolation performance. In contrast,\nincorporating raw texts in specific complex events and fine-tuning LLMs\nsignificantly improves performance. Moreover, enhanced with retrieval modules,\nLLM can effectively capture temporal relational patterns hidden in historical\nevents. Meanwhile, issues such as popularity bias and the long-tail problem\nstill persist in LLMs, particularly in the RAG-based method. These findings not\nonly deepen our understanding of LLM-based event forecasting methods but also\nhighlight several promising research directions.We consider that this\ncomprehensive evaluation, along with the identified research opportunities,\nwill significantly contribute to future research on temporal event forecasting\nthrough LLMs.\n', ""  Knowledge in the real world is being updated constantly. However, it is\ncostly to frequently update large language models (LLMs). Therefore, it is\ncrucial for LLMs to understand the concept of temporal knowledge. However,\nprior works on temporal question answering (TQA) did not emphasize multi-answer\nand multi-hop types of temporal reasoning. In this paper, we propose a complex\ntemporal question-answering dataset Complex-TR that focuses on multi-answer and\nmulti-hop temporal reasoning. Besides, we also propose a novel data\naugmentation strategy to improve the complex temporal reasoning capability and\nrobustness of LLMs. We conducted experiments on multiple temporal QA datasets.\nExperimental results show that our method is able to improve LLMs' performance\non temporal QA benchmarks by significant margins. Our code and data are\nreleased at: https://github.com/nusnlp/complex-tr.\n""]",Temporal Knowledge Graph Question Answering,Temporal Knowledge Graph Reasoning and Question Answering,Artificial Intelligence and Reasoning Systems,Intelligent Systems
183,48,183_forecasting_forecast_forecasts_prediction,"['forecasting', 'forecast', 'forecasts', 'prediction', 'electricity', 'microgrids', 'microgrid', 'demand', 'appliance', 'renewable']","['electricity', 'forecasting', 'load', 'price', 'forecasts', 'energy', 'ahead', 'day', 'renewable', 'market']","[""  Trading on the day-ahead electricity markets requires accurate information\nabout the realization of electricity prices and the uncertainty attached to the\npredictions. Deriving accurate forecasting models presents a difficult task due\nto the day-ahead price's non-stationarity resulting from changing market\nconditions, e.g., due to changes resulting from the energy crisis in 2021. We\npresent a probabilistic forecasting approach for day-ahead electricity prices\nusing the fully data-driven deep generative model called normalizing flow. Our\nmodeling approach generates full-day scenarios of day-ahead electricity prices\nbased on conditional features such as residual load forecasts. Furthermore, we\npropose extended feature sets of prior realizations and a periodic retraining\nscheme that allows the normalizing flow to adapt to the changing conditions of\nmodern electricity markets. Our results highlight that the normalizing flow\ngenerates high-quality scenarios that reproduce the true price distribution and\nyield accurate forecasts. Additionally, our analysis highlights how our\nimprovements towards adaptations in changing regimes allow the normalizing flow\nto adapt to changing market conditions and enable continued sampling of\nhigh-quality day-ahead price scenarios.\n"", '  The precise forecasting of electricity demand also referred to as load\nforecasting, is essential for both planning and managing a power system. It is\ncrucial for many tasks, including choosing which power units to commit to,\nmaking plans for future power generation capacity, enhancing the power network,\nand controlling electricity consumption. As Bangladesh is a developing country,\nthe electricity infrastructure is critical for economic growth and employment\nin this country. Accurate forecasting of electricity demand is crucial for\nensuring that this country has a reliable and sustainable electricity supply to\nmeet the needs of its growing population and economy. The complex and nonlinear\nbehavior of such energy systems inhibits the creation of precise algorithms.\nWithin this context, this paper aims to propose a hybrid model of Convolutional\nNeural Network (CNN) and stacked Bidirectional Long-short Term Memory (BiLSTM)\narchitecture to perform an accurate short-term forecast of the electricity\ndemand of Dhaka city. Short-term forecasting is ordinarily done to anticipate\nload for the following few hours to a few weeks. Normalization techniques have\nbeen also investigated because of the sensitivity of these models towards the\ninput range. The proposed approach produced the best prediction results in\ncomparison to the other benchmark models (LSTM, CNN- BiLSTM and CNN-LSTM) used\nin the study, with MAPE 1.64%, MSE 0.015, RMSE 0.122 and MAE 0.092. The result\nof the proposed model also outperformed some of the existing works on\nload-forecasting.\n', ""  The flexibility in electricity consumption and production in communities of\nresidential buildings, including those with renewable energy sources and energy\nstorage (a.k.a., prosumers), can effectively be utilized through the\nadvancement of short-term demand response mechanisms. It is known that\nflexibility can further be increased if demand response is performed at the\nlevel of communities of prosumers, since aggregated groups can better\ncoordinate electricity consumption. However, the effectiveness of such\nshort-term optimization is highly dependent on the accuracy of electricity load\nforecasts both for each building as well as for the whole community. Structural\nvariations in the electricity load profile can be associated with different\nexogenous factors, such as weather conditions, calendar information and day of\nthe week, as well as user behavior. In this paper, we review a wide range of\nelectricity load forecasting techniques, that can provide significant\nassistance in optimizing load consumption in prosumer communities. We present\nand test artificial intelligence (AI) powered short-term load forecasting\nmethodologies that operate with black-box time series models, such as\nFacebook's Prophet and Long Short-term Memory (LSTM) models; season-based\nSARIMA and smoothing Holt-Winters models; and empirical regression-based models\nthat utilize domain knowledge. The integration of weather forecasts into\ndata-driven time series forecasts is also tested. Results show that the\ncombination of persistent and regression terms (adapted to the load forecasting\ntask) achieves the best forecast accuracy.\n""]",Electricity Demand Forecasting,Energy Forecasting and Management,Predictive Modeling and Forecasting,Predictive Modeling and Forecasting
184,48,184_tables_tablellm_table_tableinstruct,"['tables', 'tablellm', 'table', 'tableinstruct', 'tabular', 'texttableqa', 'sql', 'spreadsheets', 'queries', 'spreadsheetbench']","['table', 'tables', 'spreadsheet', 'spreadsheets', 'tabular', 'reasoning', 'question', 'structured', 'formulas', 'questions']","[""  Table question answering is a popular task that assesses a model's ability to\nunderstand and interact with structured data. However, the given table often\ndoes not contain sufficient information for answering the question,\nnecessitating the integration of external knowledge. Existing methods either\nconvert both the table and external knowledge into text, which neglects the\nstructured nature of the table; or they embed queries for external sources in\nthe interaction with the table, which complicates the process. In this paper,\nwe propose a simple yet effective method to integrate external information in a\ngiven table. Our method first constructs an augmenting table containing the\nmissing information and then generates a SQL query over the two tables to\nanswer the question. Experiments show that our method outperforms strong\nbaselines on three table QA benchmarks. Our code is publicly available at\nhttps://github.com/UCSB-NLP-Chang/Augment_tableQA.\n"", '  Table reasoning, which aims to generate the corresponding answer to the\nquestion following the user requirement according to the provided table, and\noptionally a text description of the table, effectively improving the\nefficiency of obtaining information. Recently, using Large Language Models\n(LLMs) has become the mainstream method for table reasoning, because it not\nonly significantly reduces the annotation cost but also exceeds the performance\nof previous methods. However, existing research still lacks a summary of\nLLM-based table reasoning works. Due to the existing lack of research,\nquestions about which techniques can improve table reasoning performance in the\nera of LLMs, why LLMs excel at table reasoning, and how to enhance table\nreasoning abilities in the future, remain largely unexplored. This gap\nsignificantly limits progress in research. To answer the above questions and\nadvance table reasoning research with LLMs, we present this survey to analyze\nexisting research, inspiring future work. In this paper, we analyze the\nmainstream techniques used to improve table reasoning performance in the LLM\nera, and the advantages of LLMs compared to pre-LLMs for solving table\nreasoning. We provide research directions from both the improvement of existing\nmethods and the expansion of practical applications to inspire future research.\n', '  Tables, typically two-dimensional and structured to store large amounts of\ndata, are essential in daily activities like database queries, spreadsheet\nmanipulations, web table question answering, and image table information\nextraction. Automating these table-centric tasks with Large Language Models\n(LLMs) or Visual Language Models (VLMs) offers significant public benefits,\ngarnering interest from academia and industry. This survey provides a\ncomprehensive overview of table-related tasks, examining both user scenarios\nand technical aspects. It covers traditional tasks like table question\nanswering as well as emerging fields such as spreadsheet manipulation and table\ndata analysis. We summarize the training techniques for LLMs and VLMs tailored\nfor table processing. Additionally, we discuss prompt engineering, particularly\nthe use of LLM-powered agents, for various table-related tasks. Finally, we\nhighlight several challenges, including processing implicit user intentions and\nextracting information from various table sources.\n']",Table Question Answering and Reasoning,Question Answering and Reasoning Systems,Artificial Intelligence and Reasoning Systems,Intelligent Systems
185,48,185_arabic_arabicaqa_arab_arablegaleval,"['arabic', 'arabicaqa', 'arab', 'arablegaleval', 'language', 'wordnet', 'moroccan', 'persian', 'arabiangpt', 'corpus']","['lemmas', 'corpus', 'language', 'linguistic', 'cultural', 'native', 'culture', 'word', 'processing', 'natural']","['  We present ALLaM: Arabic Large Language Model, a series of large language\nmodels to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is\ncarefully trained considering the values of language alignment and knowledge\ntransfer at scale. Our autoregressive decoder-only architecture models\ndemonstrate how second-language acquisition via vocabulary expansion and\npretraining on a mixture of Arabic and English text can steer a model towards a\nnew language (Arabic) without any catastrophic forgetting in the original\nlanguage (English). Furthermore, we highlight the effectiveness of using\nparallel/translated data to aid the process of knowledge alignment between\nlanguages. Finally, we show that extensive alignment with human preferences can\nsignificantly enhance the performance of a language model compared to models of\na larger scale with lower quality alignment. ALLaM achieves state-of-the-art\nperformance in various Arabic benchmarks, including MMLU Arabic, ACVA, and\nArabic Exams. Our aligned models improve both in Arabic and English from their\nbase aligned models.\n', '  Large language models (LLMs) have greatly impacted the natural language\nprocessing (NLP) field, particularly for the English language. These models\nhave demonstrated capabilities in understanding and generating human-like text.\nThe success of language models largely depends on the availability of\nhigh-quality instruction datasets, which consist of detailed task descriptions\nand corresponding responses that are essential for training the models to\naddress a variety of prompts accurately. However, the availability and quality\nof these resources vary by language. While models perform well in English, they\noften need help with languages like Arabic, due to the lack of datasets for\nfine-tuning Arabic-specific tasks. To address this issue, we introduce\nInstAr-500k, a new Arabic instruction dataset created by generating and\ncollecting content that covers several domains and instruction types. We assess\nthis dataset by fine-tuning an open-source Gemma-7B model on several downstream\ntasks to improve its functionality. Based on multiple evaluations, our\nfine-tuned model achieves excellent performance on several Arabic NLP\nbenchmarks. These outcomes emphasize the effectiveness of our dataset in\nelevating the capabilities of language models for Arabic. Our instruction\ndataset bridges the performance gap between English and Arabic language models\nby providing resources that amplify Arabic NLP development. Building on this\nfoundation, we developed a model, GemmAr-7B-V1, specifically tuned to excel at\na wide range of Arabic NLP tasks.\n', '  In recent years, Large Language Models have revolutionized the field of\nnatural language processing, showcasing an impressive rise predominantly in\nEnglish-centric domains. These advancements have set a global benchmark,\ninspiring significant efforts toward developing Arabic LLMs capable of\nunderstanding and generating the Arabic language with remarkable accuracy.\nDespite these advancements, a critical challenge persists: the potential bias\nin Arabic LLMs, primarily attributed to their reliance on datasets comprising\nEnglish data that has been translated into Arabic. This reliance not only\ncompromises the authenticity of the generated content but also reflects a\nbroader issue -the scarcity of original quality Arabic linguistic data. This\nstudy aims to address the data scarcity in the Arab world and to encourage the\ndevelopment of Arabic Language Models that are true to both the linguistic and\nnuances of the region. We undertook a large-scale data mining project,\nextracting a substantial volume of text from the Common Crawl WET files,\nspecifically targeting Arabic content. The extracted data underwent a rigorous\ncleaning and deduplication process, using innovative techniques to ensure the\nintegrity and uniqueness of the dataset. The result is the 101 Billion Arabic\nWords Dataset, the largest Arabic dataset available to date, which can\nsignificantly contribute to the development of authentic Arabic LLMs. This\nstudy not only highlights the potential for creating linguistically and\nculturally accurate Arabic LLMs but also sets a precedent for future research\nin enhancing the authenticity of Arabic language models.\n']",Arabic Language Models and NLP Development,Large Language Models and Cognitive Abilities,Large Language Models,Large Language Models
185,48,185_arabic_arabicaqa_arab_arablegaleval,"['arabic', 'arabicaqa', 'arab', 'arablegaleval', 'language', 'wordnet', 'moroccan', 'persian', 'arabiangpt', 'corpus']","['lemmas', 'corpus', 'language', 'linguistic', 'cultural', 'native', 'culture', 'word', 'processing', 'natural']","['  We present ALLaM: Arabic Large Language Model, a series of large language\nmodels to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is\ncarefully trained considering the values of language alignment and knowledge\ntransfer at scale. Our autoregressive decoder-only architecture models\ndemonstrate how second-language acquisition via vocabulary expansion and\npretraining on a mixture of Arabic and English text can steer a model towards a\nnew language (Arabic) without any catastrophic forgetting in the original\nlanguage (English). Furthermore, we highlight the effectiveness of using\nparallel/translated data to aid the process of knowledge alignment between\nlanguages. Finally, we show that extensive alignment with human preferences can\nsignificantly enhance the performance of a language model compared to models of\na larger scale with lower quality alignment. ALLaM achieves state-of-the-art\nperformance in various Arabic benchmarks, including MMLU Arabic, ACVA, and\nArabic Exams. Our aligned models improve both in Arabic and English from their\nbase aligned models.\n', '  Large language models (LLMs) have greatly impacted the natural language\nprocessing (NLP) field, particularly for the English language. These models\nhave demonstrated capabilities in understanding and generating human-like text.\nThe success of language models largely depends on the availability of\nhigh-quality instruction datasets, which consist of detailed task descriptions\nand corresponding responses that are essential for training the models to\naddress a variety of prompts accurately. However, the availability and quality\nof these resources vary by language. While models perform well in English, they\noften need help with languages like Arabic, due to the lack of datasets for\nfine-tuning Arabic-specific tasks. To address this issue, we introduce\nInstAr-500k, a new Arabic instruction dataset created by generating and\ncollecting content that covers several domains and instruction types. We assess\nthis dataset by fine-tuning an open-source Gemma-7B model on several downstream\ntasks to improve its functionality. Based on multiple evaluations, our\nfine-tuned model achieves excellent performance on several Arabic NLP\nbenchmarks. These outcomes emphasize the effectiveness of our dataset in\nelevating the capabilities of language models for Arabic. Our instruction\ndataset bridges the performance gap between English and Arabic language models\nby providing resources that amplify Arabic NLP development. Building on this\nfoundation, we developed a model, GemmAr-7B-V1, specifically tuned to excel at\na wide range of Arabic NLP tasks.\n', '  In recent years, Large Language Models have revolutionized the field of\nnatural language processing, showcasing an impressive rise predominantly in\nEnglish-centric domains. These advancements have set a global benchmark,\ninspiring significant efforts toward developing Arabic LLMs capable of\nunderstanding and generating the Arabic language with remarkable accuracy.\nDespite these advancements, a critical challenge persists: the potential bias\nin Arabic LLMs, primarily attributed to their reliance on datasets comprising\nEnglish data that has been translated into Arabic. This reliance not only\ncompromises the authenticity of the generated content but also reflects a\nbroader issue -the scarcity of original quality Arabic linguistic data. This\nstudy aims to address the data scarcity in the Arab world and to encourage the\ndevelopment of Arabic Language Models that are true to both the linguistic and\nnuances of the region. We undertook a large-scale data mining project,\nextracting a substantial volume of text from the Common Crawl WET files,\nspecifically targeting Arabic content. The extracted data underwent a rigorous\ncleaning and deduplication process, using innovative techniques to ensure the\nintegrity and uniqueness of the dataset. The result is the 101 Billion Arabic\nWords Dataset, the largest Arabic dataset available to date, which can\nsignificantly contribute to the development of authentic Arabic LLMs. This\nstudy not only highlights the potential for creating linguistically and\nculturally accurate Arabic LLMs but also sets a precedent for future research\nin enhancing the authenticity of Arabic language models.\n']",Arabic Language Models and NLP Development,Large Language Models and Cognitive Abilities,Large Language Models,Large Language Models
186,47,186_argumentation_argumentative_arguments_debates,"['argumentation', 'argumentative', 'arguments', 'debates', 'debater', 'annotators', 'annotations', 'argument', 'debate', 'annotation']","['argument', 'arguments', 'argumentative', 'argumentation', 'debate', 'mining', 'fallacy', 'counter', 'relations', 'debates']","['  Evaluating the quality of arguments is a crucial aspect of any system\nleveraging argument mining. However, it is a challenge to obtain reliable and\nconsistent annotations regarding argument quality, as this usually requires\ndomain-specific expertise of the annotators. Even among experts, the assessment\nof argument quality is often inconsistent due to the inherent subjectivity of\nthis task. In this paper, we study the potential of using state-of-the-art\nlarge language models (LLMs) as proxies for argument quality annotators. To\nassess the capability of LLMs in this regard, we analyze the agreement between\nmodel, human expert, and human novice annotators based on an established\ntaxonomy of argument quality dimensions. Our findings highlight that LLMs can\nproduce consistent annotations, with a moderately high agreement with human\nexperts across most of the quality dimensions. Moreover, we show that using\nLLMs as additional annotators can significantly improve the agreement between\nannotators. These results suggest that LLMs can serve as a valuable tool for\nautomated argument quality assessment, thus streamlining and accelerating the\nevaluation of large argument datasets.\n', '  Argument mining (AM) is defined as the task of automatically identifying and\nextracting argumentative components (e.g. premises, claims, etc.) and detecting\nthe existing relations among them (i.e., support, attack, no relations). Deep\nlearning models enable us to analyze arguments more efficiently than\ntraditional methods and extract their semantics. This paper presents\ncomparative studies between a few deep learning-based models in argument\nmining. The work concentrates on argument classification. The research was done\non a wide spectrum of datasets (Args.me, UKP, US2016). The main novelty of this\npaper is the ensemble model which is based on BERT architecture and ChatGPT-4\nas fine tuning model. The presented results show that BERT+ChatGPT-4\noutperforms the rest of the models including other Transformer-based and\nLSTM-based models. The observed improvement is, in most cases, greater than\n10The presented analysis can provide crucial insights into how the models for\nargument classification should be further improved. Additionally, it can help\ndevelop a prompt-based algorithm to eliminate argument classification errors.\n', '  Some of the major limitations identified in the areas of argument mining,\nargument generation, and natural language argument analysis are related to the\ncomplexity of annotating argumentatively rich data, the limited size of these\ncorpora, and the constraints that represent the different languages and domains\nin which these data is annotated. To address these limitations, in this paper\nwe present the following contributions: (i) an effective methodology for the\nautomatic generation of natural language arguments in different topics and\nlanguages, (ii) the largest publicly available corpus of natural language\nargumentation schemes, and (iii) a set of solid baselines and fine-tuned models\nfor the automatic identification of argumentation schemes.\n']",Argument Mining and Quality Assessment,Automated Information Verification and Analysis,Information Verification and Validation,Information Verification and Validation
187,47,187_optimal_reinforcement_games_equilibrium,"['optimal', 'reinforcement', 'games', 'equilibrium', 'equilibria', 'nash', 'markov', 'agents', 'game', 'complexity']","['games', 'equilibrium', 'equilibria', 'sum', 'markov', 'policy', 'agents', 'player', 'mean', 'approximate']","['  Mean Field Games (MFGs) have the ability to handle large-scale multi-agent\nsystems, but learning Nash equilibria in MFGs remains a challenging task. In\nthis paper, we propose a deep reinforcement learning (DRL) algorithm that\nachieves population-dependent Nash equilibrium without the need for averaging\nor sampling from history, inspired by Munchausen RL and Online Mirror Descent.\nThrough the design of an additional inner-loop replay buffer, the agents can\neffectively learn to achieve Nash equilibrium from any distribution, mitigating\ncatastrophic forgetting. The resulting policy can be applied to various initial\ndistributions. Numerical experiments on four canonical examples demonstrate our\nalgorithm has better convergence properties than SOTA algorithms, in particular\na DRL version of Fictitious Play for population-dependent policies.\n', '  A fundamental shortcoming of the concept of Nash equilibrium is its\ncomputational intractability: approximating Nash equilibria in normal-form\ngames is PPAD-hard. In this paper, inspired by the ideas of smoothed analysis,\nwe introduce a relaxed variant of Nash equilibrium called $\\sigma$-smooth Nash\nequilibrium, for a smoothness parameter $\\sigma$. In a $\\sigma$-smooth Nash\nequilibrium, players only need to achieve utility at least as high as their\nbest deviation to a $\\sigma$-smooth strategy, which is a distribution that does\nnot put too much mass (as parametrized by $\\sigma$) on any fixed action. We\ndistinguish two variants of $\\sigma$-smooth Nash equilibria: strong\n$\\sigma$-smooth Nash equilibria, in which players are required to play\n$\\sigma$-smooth strategies under equilibrium play, and weak $\\sigma$-smooth\nNash equilibria, where there is no such requirement.\n  We show that both weak and strong $\\sigma$-smooth Nash equilibria have\nsuperior computational properties to Nash equilibria: when $\\sigma$ as well as\nan approximation parameter $\\epsilon$ and the number of players are all\nconstants, there is a constant-time randomized algorithm to find a weak\n$\\epsilon$-approximate $\\sigma$-smooth Nash equilibrium in normal-form games.\nIn the same parameter regime, there is a polynomial-time deterministic\nalgorithm to find a strong $\\epsilon$-approximate $\\sigma$-smooth Nash\nequilibrium in a normal-form game. These results stand in contrast to the\noptimal algorithm for computing $\\epsilon$-approximate Nash equilibria, which\ncannot run in faster than quasipolynomial-time. We complement our upper bounds\nby showing that when either $\\sigma$ or $\\epsilon$ is an inverse polynomial,\nfinding a weak $\\epsilon$-approximate $\\sigma$-smooth Nash equilibria becomes\ncomputationally intractable.\n', '  Reinforcement learning for multi-agent games has attracted lots of attention\nrecently. However, given the challenge of solving Nash equilibria for large\npopulation games, existing works with guaranteed polynomial complexities either\nfocus on variants of zero-sum and potential games, or aim at solving (coarse)\ncorrelated equilibria, or require access to simulators, or rely on certain\nassumptions that are hard to verify. This work proposes MF-OML (Mean-Field\nOccupation-Measure Learning), an online mean-field reinforcement learning\nalgorithm for computing approximate Nash equilibria of large population\nsequential symmetric games. MF-OML is the first fully polynomial multi-agent\nreinforcement learning algorithm for provably solving Nash equilibria (up to\nmean-field approximation gaps that vanish as the number of players $N$ goes to\ninfinity) beyond variants of zero-sum and potential games. When evaluated by\nthe cumulative deviation from Nash equilibria, the algorithm is shown to\nachieve a high probability regret bound of $\\tilde{O}(M^{3/4}+N^{-1/2}M)$ for\ngames with the strong Lasry-Lions monotonicity condition, and a regret bound of\n$\\tilde{O}(M^{11/12}+N^{- 1/6}M)$ for games with only the Lasry-Lions\nmonotonicity condition, where $M$ is the total number of episodes and $N$ is\nthe number of agents of the game. As a byproduct, we also obtain the first\ntractable globally convergent computational algorithm for computing approximate\nNash equilibria of monotone mean-field games.\n']",Mean-Field Games and Nash Equilibria,Artificial Intelligence in Sports and Games,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
187,47,187_optimal_reinforcement_games_equilibrium,"['optimal', 'reinforcement', 'games', 'equilibrium', 'equilibria', 'nash', 'markov', 'agents', 'game', 'complexity']","['games', 'equilibrium', 'equilibria', 'sum', 'markov', 'policy', 'agents', 'player', 'mean', 'approximate']","['  Mean Field Games (MFGs) have the ability to handle large-scale multi-agent\nsystems, but learning Nash equilibria in MFGs remains a challenging task. In\nthis paper, we propose a deep reinforcement learning (DRL) algorithm that\nachieves population-dependent Nash equilibrium without the need for averaging\nor sampling from history, inspired by Munchausen RL and Online Mirror Descent.\nThrough the design of an additional inner-loop replay buffer, the agents can\neffectively learn to achieve Nash equilibrium from any distribution, mitigating\ncatastrophic forgetting. The resulting policy can be applied to various initial\ndistributions. Numerical experiments on four canonical examples demonstrate our\nalgorithm has better convergence properties than SOTA algorithms, in particular\na DRL version of Fictitious Play for population-dependent policies.\n', '  A fundamental shortcoming of the concept of Nash equilibrium is its\ncomputational intractability: approximating Nash equilibria in normal-form\ngames is PPAD-hard. In this paper, inspired by the ideas of smoothed analysis,\nwe introduce a relaxed variant of Nash equilibrium called $\\sigma$-smooth Nash\nequilibrium, for a smoothness parameter $\\sigma$. In a $\\sigma$-smooth Nash\nequilibrium, players only need to achieve utility at least as high as their\nbest deviation to a $\\sigma$-smooth strategy, which is a distribution that does\nnot put too much mass (as parametrized by $\\sigma$) on any fixed action. We\ndistinguish two variants of $\\sigma$-smooth Nash equilibria: strong\n$\\sigma$-smooth Nash equilibria, in which players are required to play\n$\\sigma$-smooth strategies under equilibrium play, and weak $\\sigma$-smooth\nNash equilibria, where there is no such requirement.\n  We show that both weak and strong $\\sigma$-smooth Nash equilibria have\nsuperior computational properties to Nash equilibria: when $\\sigma$ as well as\nan approximation parameter $\\epsilon$ and the number of players are all\nconstants, there is a constant-time randomized algorithm to find a weak\n$\\epsilon$-approximate $\\sigma$-smooth Nash equilibrium in normal-form games.\nIn the same parameter regime, there is a polynomial-time deterministic\nalgorithm to find a strong $\\epsilon$-approximate $\\sigma$-smooth Nash\nequilibrium in a normal-form game. These results stand in contrast to the\noptimal algorithm for computing $\\epsilon$-approximate Nash equilibria, which\ncannot run in faster than quasipolynomial-time. We complement our upper bounds\nby showing that when either $\\sigma$ or $\\epsilon$ is an inverse polynomial,\nfinding a weak $\\epsilon$-approximate $\\sigma$-smooth Nash equilibria becomes\ncomputationally intractable.\n', '  Reinforcement learning for multi-agent games has attracted lots of attention\nrecently. However, given the challenge of solving Nash equilibria for large\npopulation games, existing works with guaranteed polynomial complexities either\nfocus on variants of zero-sum and potential games, or aim at solving (coarse)\ncorrelated equilibria, or require access to simulators, or rely on certain\nassumptions that are hard to verify. This work proposes MF-OML (Mean-Field\nOccupation-Measure Learning), an online mean-field reinforcement learning\nalgorithm for computing approximate Nash equilibria of large population\nsequential symmetric games. MF-OML is the first fully polynomial multi-agent\nreinforcement learning algorithm for provably solving Nash equilibria (up to\nmean-field approximation gaps that vanish as the number of players $N$ goes to\ninfinity) beyond variants of zero-sum and potential games. When evaluated by\nthe cumulative deviation from Nash equilibria, the algorithm is shown to\nachieve a high probability regret bound of $\\tilde{O}(M^{3/4}+N^{-1/2}M)$ for\ngames with the strong Lasry-Lions monotonicity condition, and a regret bound of\n$\\tilde{O}(M^{11/12}+N^{- 1/6}M)$ for games with only the Lasry-Lions\nmonotonicity condition, where $M$ is the total number of episodes and $N$ is\nthe number of agents of the game. As a byproduct, we also obtain the first\ntractable globally convergent computational algorithm for computing approximate\nNash equilibria of monotone mean-field games.\n']",Mean-Field Games and Nash Equilibria,Artificial Intelligence in Sports and Games,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
188,47,188_blockchain_cryptocurrencies_bitcoin_ethereum,"['blockchain', 'cryptocurrencies', 'bitcoin', 'ethereum', 'fraud', 'frauds', 'cryptocurrency', 'ponzi', 'vulnerabilities', 'phishing']","['fraud', 'blockchain', 'transaction', 'transactions', 'contracts', 'smart', 'laundering', 'financial', 'money', 'detection']","[""  As blockchain technology becomes more and more popular, a typical financial\nscam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum.\nThis Ponzi scheme deployed through smart contracts, also known as the smart\nPonzi scheme, has caused a lot of economic losses and negative impacts.\nExisting methods for detecting smart Ponzi schemes on Ethereum mainly rely on\nbytecode features, opcode features, account features, and transaction behavior\nfeatures of smart contracts, which are unable to truly characterize the\nbehavioral features of Ponzi schemes, and thus generally perform poorly in\nterms of detection accuracy and false alarm rates. In this paper, we propose\nSourceP, a method to detect smart Ponzi schemes on the Ethereum platform using\npre-trained models and data flow, which only requires using the source code of\nsmart contracts as features. SourceP reduces the difficulty of data acquisition\nand feature extraction of existing detection methods. Specifically, we first\nconvert the source code of a smart contract into a data flow graph and then\nintroduce a pre-trained model based on learning code representations to build a\nclassification model to identify Ponzi schemes in smart contracts. The\nexperimental results show that SourceP achieves 87.2% recall and 90.7% F-score\nfor detecting smart Ponzi schemes within Ethereum's smart contract dataset,\noutperforming state-of-the-art methods in terms of performance and\nsustainability. We also demonstrate through additional experiments that\npre-trained models and data flow play an important contribution to SourceP, as\nwell as proving that SourceP has a good generalization ability.\n"", ""  The rampant fraudulent activities on Ethereum hinder the healthy development\nof the blockchain ecosystem, necessitating the reinforcement of regulations.\nHowever, multiple imbalances involving account interaction frequencies and\ninteraction types in the Ethereum transaction environment pose significant\nchallenges to data mining-based fraud detection research. To address this, we\nfirst propose the concept of meta-interactions to refine interaction behaviors\nin Ethereum, and based on this, we present a dual self-supervision enhanced\nEthereum fraud detection framework, named Meta-IFD. This framework initially\nintroduces a generative self-supervision mechanism to augment the interaction\nfeatures of accounts, followed by a contrastive self-supervision mechanism to\ndifferentiate various behavior patterns, and ultimately characterizes the\nbehavioral representations of accounts and mines potential fraud risks through\nmulti-view interaction feature learning. Extensive experiments on real Ethereum\ndatasets demonstrate the effectiveness and superiority of our framework in\ndetecting common Ethereum fraud behaviors such as Ponzi schemes and phishing\nscams. Additionally, the generative module can effectively alleviate the\ninteraction distribution imbalance in Ethereum data, while the contrastive\nmodule significantly enhances the framework's ability to distinguish different\nbehavior patterns. The source code will be released on GitHub soon.\n"", '  The Ponzi scheme, an old-fashioned fraud, is now popular on the Ethereum\nblockchain, causing considerable financial losses to many crypto investors. A\nfew Ponzi detection methods have been proposed in the literature, most of which\ndetect a Ponzi scheme based on its smart contract source code. This\ncontract-code-based approach, while achieving very high accuracy, is not robust\nbecause a Ponzi developer can fool a detection model by obfuscating the opcode\nor inventing a new profit distribution logic that cannot be detected. On the\ncontrary, a transaction-based approach could improve the robustness of\ndetection because transactions, unlike smart contracts, are harder to be\nmanipulated. However, the current transaction-based detection models achieve\nfairly low accuracy. In this paper, we aim to improve the accuracy of the\ntransaction-based models by employing time-series features, which turn out to\nbe crucial in capturing the life-time behaviour a Ponzi application but were\ncompletely overlooked in previous works. We propose a new set of 85 features\n(22 known account-based and 63 new time-series features), which allows\noff-the-shelf machine learning algorithms to achieve up to 30% higher F1-scores\ncompared to existing works.\n']",Detecting Ponzi Schemes on Ethereum Blockchain,Blockchain Security and Fraud Detection,Machine Learning and Blockchain for Security and Fraud Detection,Machine Learning and Blockchain for Security and Fraud Detection
189,47,189_retrieval_nearest_search_locality,"['retrieval', 'nearest', 'search', 'locality', 'indexing', 'similarity', 'embeddings', 'hashing', 'algorithms', 'indexes']","['nearest', 'search', 'neighbor', 'vector', 'vectors', 'index', 'approximate', 'similarity', 'query', 'retrieval']","[""  Vector search systems, pivotal in AI applications, often rely on the\nHierarchical Navigable Small Worlds (HNSW) algorithm. However, the behaviour of\nHNSW under real-world scenarios using vectors generated with deep learning\nmodels remains under-explored. Existing Approximate Nearest Neighbours (ANN)\nbenchmarks and research typically has an over-reliance on simplistic datasets\nlike MNIST or SIFT1M and fail to reflect the complexity of current use-cases.\nOur investigation focuses on HNSW's efficacy across a spectrum of datasets,\nincluding synthetic vectors tailored to mimic specific intrinsic\ndimensionalities, widely-used retrieval benchmarks with popular embedding\nmodels, and proprietary e-commerce image data with CLIP models. We survey the\nmost popular HNSW vector databases and collate their default parameters to\nprovide a realistic fixed parameterisation for the duration of the paper.\n  We discover that the recall of approximate HNSW search, in comparison to\nexact K Nearest Neighbours (KNN) search, is linked to the vector space's\nintrinsic dimensionality and significantly influenced by the data insertion\nsequence. Our methodology highlights how insertion order, informed by\nmeasurable properties such as the pointwise Local Intrinsic Dimensionality\n(LID) or known categories, can shift recall by up to 12 percentage points. We\nalso observe that running popular benchmark datasets with HNSW instead of KNN\ncan shift rankings by up to three positions for some models. This work\nunderscores the need for more nuanced benchmarks and design considerations in\ndeveloping robust vector search systems using approximate vector search\nalgorithms. This study presents a number of scenarios with varying real world\napplicability which aim to better increase understanding and future development\nof ANN algorithms and embedding\n"", '  A critical piece of the modern information retrieval puzzle is approximate\nnearest neighbor search. Its objective is to return a set of $k$ data points\nthat are closest to a query point, with its accuracy measured by the proportion\nof exact nearest neighbors captured in the returned set. One popular approach\nto this question is clustering: The indexing algorithm partitions data points\ninto non-overlapping subsets and represents each partition by a point such as\nits centroid. The query processing algorithm first identifies the nearest\nclusters -- a process known as routing -- then performs a nearest neighbor\nsearch over those clusters only. In this work, we make a simple observation:\nThe routing function solves a ranking problem. Its quality can therefore be\nassessed with a ranking metric, making the function amenable to\nlearning-to-rank. Interestingly, ground-truth is often freely available: Given\na query distribution in a top-$k$ configuration, the ground-truth is the set of\nclusters that contain the exact top-$k$ vectors. We develop this insight and\napply it to Maximum Inner Product Search (MIPS). As we demonstrate empirically\non various datasets, learning a simple linear function consistently improves\nthe accuracy of clustering-based MIPS.\n', '  We define and investigate the problem of $\\textit{c-approximate window\nsearch}$: approximate nearest neighbor search where each point in the dataset\nhas a numeric label, and the goal is to find nearest neighbors to queries\nwithin arbitrary label ranges. Many semantic search problems, such as image and\ndocument search with timestamp filters, or product search with cost filters,\nare natural examples of this problem. We propose and theoretically analyze a\nmodular tree-based framework for transforming an index that solves the\ntraditional c-approximate nearest neighbor problem into a data structure that\nsolves window search. On standard nearest neighbor benchmark datasets equipped\nwith random label values, adversarially constructed embeddings, and image\nsearch embeddings with real timestamps, we obtain up to a $75\\times$ speedup\nover existing solutions at the same level of recall.\n']",Approximate Nearest Neighbor Search Algorithms,Efficient Search and Classification Algorithms,Information Retrieval and Knowledge Systems,Information Retrieval and Knowledge Systems
190,47,190_elasticity_deformation_elastic_microstructures,"['elasticity', 'deformation', 'elastic', 'microstructures', 'neural', 'multiscale', 'microstructure', 'mechanical', 'microscale', 'mechanics']","['constitutive', 'stress', 'material', 'materials', 'strain', 'homogenization', 'physics', 'microstructures', 'inelastic', 'microstructure']","['  Multiscale partial differential equations (PDEs) arise in various\napplications, and several schemes have been developed to solve them\nefficiently. Homogenization theory is a powerful methodology that eliminates\nthe small-scale dependence, resulting in simplified equations that are\ncomputationally tractable while accurately predicting the macroscopic response.\nIn the field of continuum mechanics, homogenization is crucial for deriving\nconstitutive laws that incorporate microscale physics in order to formulate\nbalance laws for the macroscopic quantities of interest. However, obtaining\nhomogenized constitutive laws is often challenging as they do not in general\nhave an analytic form and can exhibit phenomena not present on the microscale.\nIn response, data-driven learning of the constitutive law has been proposed as\nappropriate for this task. However, a major challenge in data-driven learning\napproaches for this problem has remained unexplored: the impact of\ndiscontinuities and corner interfaces in the underlying material. These\ndiscontinuities in the coefficients affect the smoothness of the solutions of\nthe underlying equations. Given the prevalence of discontinuous materials in\ncontinuum mechanics applications, it is important to address the challenge of\nlearning in this context; in particular, to develop underpinning theory that\nestablishes the reliability of data-driven methods in this scientific domain.\nThe paper addresses this unexplored challenge by investigating the learnability\nof homogenized constitutive laws for elliptic operators in the presence of such\ncomplexities. Approximation theory is presented, and numerical experiments are\nperformed which validate the theory in the context of learning the solution\noperator defined by the cell problem arising in homogenization for elliptic\nPDEs.\n', '  Accurately modeling the mechanical behavior of materials is crucial for\nnumerous engineering applications. The quality of these models depends directly\non the accuracy of the constitutive law that defines the stress-strain\nrelation. Discovering these constitutive material laws remains a significant\nchallenge, in particular when only material deformation data is available. To\naddress this challenge, unsupervised machine learning methods have been\nproposed. However, existing approaches have several limitations: they either\nfail to ensure that the learned constitutive relations are consistent with\nphysical principles, or they rely on a predefined library of constitutive\nrelations or manually crafted input features. These dependencies require\nsignificant expertise and specialized domain knowledge. Here, we introduce a\nmachine learning approach called uLED, which overcomes the limitations by using\nthe input convex neural network (ICNN) as the surrogate constitutive model. We\nimprove the optimization strategy for training ICNN, allowing it to be trained\nend-to-end using direct strain invariants as input across various materials.\nFurthermore, we utilize the nodal force equilibrium at the internal domain as\nthe training objective, which enables us to learn the constitutive relation\nsolely from temporal displacement recordings. We validate the effectiveness of\nthe proposed method on a diverse range of material laws. We demonstrate that it\nis robust to a significant level of noise and that it converges to the ground\ntruth with increasing data resolution. We also show that the model can be\neffectively trained using a displacement field from a subdomain of the test\nspecimen and that the learned constitutive relation from one material sample is\ntransferable to other samples with different geometries. The developed\nmethodology provides an effective tool for discovering constitutive relations.\n', '  Identifying constitutive parameters in engineering and biological materials,\nparticularly those with intricate geometries and mechanical behaviors, remains\na longstanding challenge. The recent advent of Physics-Informed Neural Networks\n(PINNs) offers promising solutions, but current frameworks are often limited to\nbasic constitutive laws and encounter practical constraints when combined with\nexperimental data. In this paper, we introduce a robust PINN-based framework\ndesigned to identify material parameters for soft materials, specifically those\nexhibiting complex constitutive behaviors, under large deformation in plane\nstress conditions. Distinctively, our model emphasizes training PINNs with\nmulti-modal synthetic experimental datasets consisting of full-field\ndeformation and loading history, ensuring algorithm robustness even with noisy\ndata. Our results reveal that the PINNs framework can accurately identify\nconstitutive parameters of the incompressible Arruda-Boyce model for samples\nwith intricate geometries, maintaining an error below 5%, even with an\nexperimental noise level of 5%. We believe our framework provides a robust\nmodulus identification approach for complex solids, especially for those with\ngeometrical and constitutive complexity.\n']",Multiscale Elasticity and Microstructure Modeling,Materials Modeling and Design Optimization,Optimization and Design,Optimization and Design
191,47,191_robot_robots_robotic_robotics,"['robot', 'robots', 'robotic', 'robotics', 'humanoid', 'interactive', 'cobots', 'gestures', 'conversations', 'cobot']","['robot', 'robots', 'assistive', 'cobot', 'social', 'robotic', 'robotics', 'interaction', 'participants', 'interfaces']","['  With robotics rapidly advancing, more effective human-robot interaction is\nincreasingly needed to realize the full potential of robots for society. While\nspoken language must be part of the solution, our ability to provide spoken\nlanguage interaction capabilities is still very limited. The National Science\nFoundation accordingly convened a workshop, bringing together speech, language,\nand robotics researchers to discuss what needs to be done. The result is this\nreport, in which we identify key scientific and engineering advances needed.\n  Our recommendations broadly relate to eight general themes. First, meeting\nhuman needs requires addressing new challenges in speech technology and user\nexperience design. Second, this requires better models of the social and\ninteractive aspects of language use. Third, for robustness, robots need\nhigher-bandwidth communication with users and better handling of uncertainty,\nincluding simultaneous consideration of multiple hypotheses and goals. Fourth,\nmore powerful adaptation methods are needed, to enable robots to communicate in\nnew environments, for new tasks, and with diverse user populations, without\nextensive re-engineering or the collection of massive training data. Fifth,\nsince robots are embodied, speech should function together with other\ncommunication modalities, such as gaze, gesture, posture, and motion. Sixth,\nsince robots operate in complex environments, speech components need access to\nrich yet efficient representations of what the robot knows about objects,\nlocations, noise sources, the user, and other humans. Seventh, since robots\noperate in real time, their speech and language processing components must\nalso. Eighth, in addition to more research, we need more work on infrastructure\nand resources, including shareable software modules and internal interfaces,\ninexpensive hardware, baseline systems, and diverse corpora.\n', ""  Users develop mental models of robots to conceptualize what kind of\ninteractions they can have with those robots. The conceptualizations are often\nformed before interactions with the robot and are based only on observing the\nrobot's physical design. As a result, understanding conceptualizations formed\nfrom physical design is necessary to understand how users intend to interact\nwith the robot. We propose to use multimodal features of robot embodiments to\npredict what kinds of expectations users will have about a given robot's social\nand physical capabilities. We show that using such features provides\ninformation about general mental models of the robots that generalize across\nsocially interactive robots. We describe how these models can be incorporated\ninto interaction design and physical design for researchers working with\nsocially interactive robots.\n"", ""  Assistive robots have attracted significant attention due to their potential\nto enhance the quality of life for vulnerable individuals like the elderly. The\nconvergence of computer vision, large language models, and robotics has\nintroduced the `visuolinguomotor' mode for assistive robots, where visuals and\nlinguistics are incorporated into assistive robots to enable proactive and\ninteractive assistance. This raises the question: \\textit{In circumstances\nwhere visuals become unreliable or unavailable, can we rely solely on language\nto control robots, i.e., the viability of the `linguomotor` mode for assistive\nrobots?} This work takes the initial steps to answer this question by: 1)\nevaluating the responses of assistive robots to language prompts of varying\ngranularities; and 2) exploring the necessity and feasibility of controlling\nthe robot on-the-fly. We have designed and conducted experiments on a Sawyer\ncobot to support our arguments. A Turtlebot robot case is designed to\ndemonstrate the adaptation of the solution to scenarios where assistive robots\nneed to maneuver to assist. Codes will be released on GitHub soon to benefit\nthe community.\n""]",Human-Robot Interaction and Robotics,Robotics and Artificial Intelligence,Robotics and Artificial Intelligence,Robotics and Artificial Intelligence
192,46,192_attributes_products_advertisements_attribute,"['attributes', 'products', 'advertisements', 'attribute', 'commerce', 'product', 'items', 'retailers', 'marketplaces', 'shopping']","['commerce', 'product', 'attribute', 'sellers', 'shopping', 'products', 'click', 'customer', 'customers', 'intentions']","['  Product offers on e-commerce websites often consist of a product title and a\ntextual product description. In order to enable features such as faceted\nproduct search or to generate product comparison tables, it is necessary to\nextract structured attribute-value pairs from the unstructured product titles\nand descriptions and to normalize the extracted values to a single, unified\nscale for each attribute. This paper explores the potential of using large\nlanguage models (LLMs), such as GPT-3.5 and GPT-4, to extract and normalize\nattribute values from product titles and descriptions. We experiment with\ndifferent zero-shot and few-shot prompt templates for instructing LLMs to\nextract and normalize attribute-value pairs. We introduce the Web Data Commons\n- Product Attribute Value Extraction (WDC-PAVE) benchmark dataset for our\nexperiments. WDC-PAVE consists of product offers from 59 different websites\nwhich provide schema.org annotations. The offers belong to five different\nproduct categories, each with a specific set of attributes. The dataset\nprovides manually verified attribute-value pairs in two forms: (i) directly\nextracted values and (ii) normalized attribute values. The normalization of the\nattribute values requires systems to perform the following types of operations:\nname expansion, generalization, unit of measurement conversion, and string\nwrangling. Our experiments demonstrate that GPT-4 outperforms the PLM-based\nextraction methods SU-OpenTag, AVEQA, and MAVEQA by 10%, achieving an F1-score\nof 91%. For the extraction and normalization of product attribute values, GPT-4\nachieves a similar performance to the extraction scenario, while being\nparticularly strong at string wrangling and name expansion.\n', '  E-commerce platforms rely on structured product descriptions, in the form of\nattribute/value pairs to enable features such as faceted product search and\nproduct comparison. However, vendors on these platforms often provide\nunstructured product descriptions consisting of a title and a textual\ndescription. To process such offers, e-commerce platforms must extract\nattribute/value pairs from the unstructured descriptions. State-of-the-art\nattribute/value extraction methods based on pre-trained language models (PLMs),\nsuch as BERT, face two drawbacks (i) the methods require significant amounts of\ntask-specific training data and (ii) the fine-tuned models have problems to\ngeneralize to attribute values that were not part of the training data. We\nexplore the potential of using large language models (LLMs) as a more training\ndata-efficient and more robust alternative to existing attribute/value\nextraction methods. We propose different prompt templates for instructing LLMs\nabout the target schema of the extraction, covering both zero-shot and few-shot\nscenarios. In the zero-shot scenario, textual and JSON-based approaches for\nrepresenting information about the target attributes are compared. In the\nscenario with training data, we investigate (i) the provision of example\nattribute values, (ii) the selection of in-context demonstrations, (iii)\nshuffled ensembling to prevent position bias, and (iv) fine-tuning the LLM. The\nprompt templates are evaluated in combination with hosted LLMs, such as GPT-3.5\nand GPT-4, and open-source LLMs based on Llama2 which can be run locally. The\nbest average F1-score of 86% was reached by GPT-4 using an ensemble of shuffled\nprompts that combine attribute names, attribute descriptions, example values,\nand demonstrations. Given the same amount of training data, this prompt/model\ncombination outperforms the best PLM baseline by an average of 6% F1.\n', '  Product attribute extraction is an growing field in e-commerce business, with\nseveral applications including product ranking, product recommendation, future\nassortment planning and improving online shopping customer experiences.\nUnderstanding the customer needs is critical part of online business,\nspecifically fashion products. Retailers uses assortment planning to determine\nthe mix of products to offer in each store and channel, stay responsive to\nmarket dynamics and to manage inventory and catalogs. The goal is to offer the\nright styles, in the right sizes and colors, through the right channels. When\nshoppers find products that meet their needs and desires, they are more likely\nto return for future purchases, fostering customer loyalty. Product attributes\nare a key factor in assortment planning. In this paper we present PAE, a\nproduct attribute extraction algorithm for future trend reports consisting text\nand images in PDF format. Most existing methods focus on attribute extraction\nfrom titles or product descriptions or utilize visual information from existing\nproduct images. Compared to the prior works, our work focuses on attribute\nextraction from PDF files where upcoming fashion trends are explained. This\nwork proposes a more comprehensive framework that fully utilizes the different\nmodalities for attribute extraction and help retailers to plan the assortment\nin advance. Our contributions are three-fold: (a) We develop PAE, an efficient\nframework to extract attributes from unstructured data (text and images); (b)\nWe provide catalog matching methodology based on BERT representations to\ndiscover the existing attributes using upcoming attribute values; (c) We\nconduct extensive experiments with several baselines and show that PAE is an\neffective, flexible and on par or superior (avg 92.5% F1-Score) framework to\nexisting state-of-the-art for attribute value extraction task.\n']",Product Attribute Extraction in E-commerce,Information Extraction with Large Language Models,Information Extraction,Information Extraction
193,46,193_anomaly_anomalyllm_outliers_outlier,"['anomaly', 'anomalyllm', 'outliers', 'outlier', 'graphs', 'anomalies', 'anomalous', 'graph', 'nodes', 'detecting']","['anomaly', 'anomalies', 'detection', 'normal', 'nodes', 'graph', 'anomalous', 'node', 'graphs', 'edges']","['  Graph Anomaly Detection (GAD) is a technique used to identify abnormal nodes\nwithin graphs, finding applications in network security, fraud detection,\nsocial media spam detection, and various other domains. A common method for GAD\nis Graph Auto-Encoders (GAEs), which encode graph data into node\nrepresentations and identify anomalies by assessing the reconstruction quality\nof the graphs based on these representations. However, existing GAE models are\nprimarily optimized for direct link reconstruction, resulting in nodes\nconnected in the graph being clustered in the latent space. As a result, they\nexcel at detecting cluster-type structural anomalies but struggle with more\ncomplex structural anomalies that do not conform to clusters. To address this\nlimitation, we propose a novel solution called GAD-NR, a new variant of GAE\nthat incorporates neighborhood reconstruction for graph anomaly detection.\nGAD-NR aims to reconstruct the entire neighborhood of a node, encompassing the\nlocal structure, self-attributes, and neighbor attributes, based on the\ncorresponding node representation. By comparing the neighborhood reconstruction\nloss between anomalous nodes and normal nodes, GAD-NR can effectively detect\nany anomalies. Extensive experimentation conducted on six real-world datasets\nvalidates the effectiveness of GAD-NR, showcasing significant improvements (by\nup to 30% in AUC) over state-of-the-art competitors. The source code for GAD-NR\nis openly available. Importantly, the comparative analysis reveals that the\nexisting methods perform well only in detecting one or two types of anomalies\nout of the three types studied. In contrast, GAD-NR excels at detecting all\nthree types of anomalies across the datasets, demonstrating its comprehensive\nanomaly detection capabilities.\n', '  This paper considers an important Graph Anomaly Detection (GAD) task, namely\nopen-set GAD, which aims to train a detection model using a small number of\nnormal and anomaly nodes (referred to as seen anomalies) to detect both seen\nanomalies and unseen anomalies (i.e., anomalies that cannot be illustrated the\ntraining anomalies). The availability of those labelled training data provides\ncrucial prior knowledge about abnormalities for GAD models, enabling\nsubstantially reduced detection errors. However, current methods tend to\nover-emphasise fitting the seen anomalies, leading to a weak generalisation\nability to detect the unseen anomalies. Further, they were introduced to handle\nEuclidean data, failing to effectively capture important information on graph\nstructure and node attributes for GAD. In this work, we propose a novel\nopen-set GAD approach, namely Normal Structure Regularisation (NSReg) to\nachieve generalised detection ability to unseen anomalies, while maintaining\nits effectiveness on detecting seen anomalies. The key idea in NSReg is to\nintroduce a regularisation term that enforces the learning of compact,\nsemantically-rich representations of normal nodes based on their structural\nrelations to other nodes. When being optimised with supervised anomaly\ndetection losses, the regularisation term helps incorporate strong normality\ninto the modelling, and thus, it effectively avoids the overfitting the seen\nanomalies solely. In doing so, it helps learn better normality decision\nboundary, reducing the errors of detecting unseen anomalies as normal.\nExtensive empirical results on seven real-world datasets show the superiority\nof NSReg for open-set GAD.\n', '  Real-world graphs are complex to process for performing effective analysis,\nsuch as anomaly detection. However, recently, there have been several research\nefforts addressing the issues surrounding graph-based anomaly detection. In\nthis paper, we discuss a comprehensive overview of anomaly detection techniques\non graph data. We also discuss the various application domains which use those\nanomaly detection techniques. We present a new taxonomy that categorizes the\ndifferent state-of-the-art anomaly detection methods based on assumptions and\ntechniques. Within each category, we discuss the fundamental research ideas\nthat have been done to improve anomaly detection. We further discuss the\nadvantages and disadvantages of current anomaly detection techniques. Finally,\nwe present potential future research directions in anomaly detection on\ngraph-structured data.\n']",Graph Anomaly Detection Techniques,Graph Analysis and Processing Techniques,Data Analysis and Pattern Discovery,Data Analysis and Pattern Discovery
194,46,194_neural_networks_computability_deepinfer,"['neural', 'networks', 'computability', 'deepinfer', 'neuralsat', 'robustness', 'neurons', 'dnns', 'dnn', 'deep']","['verification', 'verifying', 'formal', 'safety', 'robustness', 'neural', 'networks', 'specifications', 'inputs', 'preimage']","['  The ubiquity of deep learning algorithms in various applications has\namplified the need for assuring their robustness against small input\nperturbations such as those occurring in adversarial attacks. Existing complete\nverification techniques offer provable guarantees for all robustness queries\nbut struggle to scale beyond small neural networks. To overcome this\ncomputational intractability, incomplete verification methods often rely on\nconvex relaxation to over-approximate the nonlinearities in neural networks.\nProgress in tighter approximations has been achieved for piecewise linear\nfunctions. However, robustness verification of neural networks for general\nactivation functions (e.g., Sigmoid, Tanh) remains under-explored and poses new\nchallenges. Typically, these networks are verified using convex relaxation\ntechniques, which involve computing linear upper and lower bounds of the\nnonlinear activation functions. In this work, we propose a novel parameter\nsearch method to improve the quality of these linear approximations.\nSpecifically, we show that using a simple search method, carefully adapted to\nthe given verification problem through state-of-the-art algorithm configuration\ntechniques, improves the average global lower bound by 25% on average over the\ncurrent state of the art on several commonly used local robustness verification\nbenchmarks.\n', '  Probabilistic verification of neural networks is concerned with formally\nanalysing the output distribution of a neural network under a probability\ndistribution of the inputs. Examples of probabilistic verification include\nverifying the demographic parity fairness notion or quantifying the safety of a\nneural network. We present a new algorithm for the probabilistic verification\nof neural networks based on an algorithm for computing and iteratively refining\nlower and upper bounds on probabilities over the outputs of a neural network.\nBy applying state-of-the-art bound propagation and branch and bound techniques\nfrom non-probabilistic neural network verification, our algorithm significantly\noutpaces existing probabilistic verification algorithms, reducing solving times\nfor various benchmarks from the literature from tens of minutes to tens of\nseconds. Furthermore, our algorithm compares favourably even to dedicated\nalgorithms for restricted subsets of probabilistic verification. We complement\nour empirical evaluation with a theoretical analysis, proving that our\nalgorithm is sound and, under mildly restrictive conditions, also complete when\nusing a suitable set of heuristics.\n', '  Formal verification of neural networks is essential before their deployment\nin safety-critical applications. However, existing methods for formally\nverifying neural networks are not yet scalable enough to handle practical\nproblems involving a large number of neurons. We address this challenge by\nintroducing a fully automatic and sound reduction of neural networks using\nreachability analysis. The soundness ensures that the verification of the\nreduced network entails the verification of the original network. To the best\nof our knowledge, we present the first sound reduction approach that is\napplicable to neural networks with any type of element-wise activation\nfunction, such as ReLU, sigmoid, and tanh. The network reduction is computed on\nthe fly while simultaneously verifying the original network and its\nspecifications. All parameters are automatically tuned to minimize the network\nsize without compromising verifiability. We further show the applicability of\nour approach to convolutional neural networks by explicitly exploiting similar\nneighboring pixels. Our evaluation shows that our approach can reduce the\nnumber of neurons to a fraction of the original number of neurons with minor\nouter-approximation and thus reduce the verification time to a similar degree.\n']",Robustness Verification of Neural Networks,Explainability and Interpretability in Neural Networks,Explainable Artificial Intelligence,Artificial Intelligence and Machine Learning Interpretability and Explainability
195,46,195_vision_driving_lanesegnet_trafficvlm,"['vision', 'driving', 'lanesegnet', 'trafficvlm', 'vehicles', 'cameras', 'camera', 'vehicle', 'hdmaps', 'road']","['lane', 'perception', 'driving', 'vehicle', 'vehicles', 'camera', 'autonomous', 'traffic', 'road', 'cooperative']","['  Lane detection is a vital task for vehicles to navigate and localize their\nposition on the road. To ensure reliable driving, lane detection models must\nhave robust generalization performance in various road environments. However,\ndespite the advanced performance in the trained domain, their generalization\nperformance still falls short of expectations due to the domain discrepancy. To\nbridge this gap, we propose a novel generative framework using HD Maps for\nSingle-Source Domain Generalization (SSDG) in lane detection. We first generate\nnumerous front-view images from lane markings of HD Maps. Next, we\nstrategically select a core subset among the generated images using (i) lane\nstructure and (ii) road surrounding criteria to maximize their diversity. In\nthe end, utilizing this core set, we train lane detection models to boost their\ngeneralization performance. We validate that our generative framework from HD\nMaps outperforms the Domain Adaptation model MLDA with +3.01%p accuracy\nimprovement, even though we do not access the target domain images.\n', '  Vision-based ego-lane inference using High-Definition (HD) maps is essential\nin autonomous driving and advanced driver assistance systems. The traditional\napproach necessitates well-calibrated cameras, which confines variation of\ncamera configuration, as the algorithm relies on intrinsic and extrinsic\ncalibration. In this paper, we propose a learning-based ego-lane inference by\ndirectly estimating the ego-lane index from a single image. To enhance robust\nperformance, our model incorporates the two-head structure inferring ego-lane\nin two perspectives simultaneously. Furthermore, we utilize an attention\nmechanism guided by vanishing point-and-line to adapt to changes in viewpoint\nwithout requiring accurate calibration. The high adaptability of our model was\nvalidated in diverse environments, devices, and camera mounting points and\norientations.\n', '  Sharing and joint processing of camera feeds and sensor measurements, known\nas Cooperative Perception (CP), has emerged as a new technique to achieve\nhigher perception qualities. CP can enhance the safety of Autonomous Vehicles\n(AVs) where their individual visual perception quality is compromised by\nadverse weather conditions (haze as foggy weather), low illumination, winding\nroads, and crowded traffic. To cover the limitations of former methods, in this\npaper, we propose a novel approach to realize an optimized CP under constrained\ncommunications. At the core of our approach is recruiting the best helper from\nthe available list of front vehicles to augment the visual range and enhance\nthe Object Detection (OD) accuracy of the ego vehicle. In this two-step\nprocess, we first select the helper vehicles that contribute the most to CP\nbased on their visual range and lowest motion blur. Next, we implement a radio\nblock optimization among the candidate vehicles to further improve\ncommunication efficiency. We specifically focus on pedestrian detection as an\nexemplary scenario. To validate our approach, we used the CARLA simulator to\ncreate a dataset of annotated videos for different driving scenarios where\npedestrian detection is challenging for an AV with compromised vision. Our\nresults demonstrate the efficacy of our two-step optimization process in\nimproving the overall performance of cooperative perception in challenging\nscenarios, substantially improving driving safety under adverse conditions.\nFinally, we note that the networking assumptions are adopted from LTE Release\n14 Mode 4 side-link communication, commonly used for Vehicle-to-Vehicle (V2V)\ncommunication. Nonetheless, our method is flexible and applicable to arbitrary\nV2V communications.\n']",Autonomous Driving with Vision and HD Maps,Computer Vision Applications,Computer Vision,Computer Vision
196,46,196_regularization_regularized_sparse_factorization,"['regularization', 'regularized', 'sparse', 'factorization', 'matrix', 'completion', 'matrices', 'overparameterization', 'minimization', 'rank']","['matrix', 'factorization', 'rank', 'completion', 'nonnegative', 'entries', 'columns', 'low', 'minimization', 'regularized']","['  When applying nonnegative matrix factorization (NMF), generally the rank\nparameter is unknown. Such rank in NMF, called the nonnegative rank, is usually\nestimated heuristically since computing the exact value of it is NP-hard. In\nthis work, we propose an approximation method to estimate such rank while\nsolving NMF on-the-fly. We use sum-of-norm (SON), a group-lasso structure that\nencourages pairwise similarity, to reduce the rank of a factor matrix where the\nrank is overestimated at the beginning. On various datasets, SON-NMF is able to\nreveal the correct nonnegative rank of the data without any prior knowledge nor\ntuning.\n  SON-NMF is a nonconvx nonsmmoth non-separable non-proximable problem, solving\nit is nontrivial. First, as rank estimation in NMF is NP-hard, the proposed\napproach does not enjoy a lower computational complexity. Using a\ngraph-theoretic argument, we prove that the complexity of the SON-NMF is almost\nirreducible. Second, the per-iteration cost of any algorithm solving SON-NMF is\npossibly high, which motivated us to propose a first-order BCD algorithm to\napproximately solve SON-NMF with a low per-iteration cost, in which we do so by\nthe proximal average operator. Lastly, we propose a simple greedy method for\npost-processing.\n  SON-NMF exhibits favourable features for applications. Beside the ability to\nautomatically estimate the rank from data, SON-NMF can deal with rank-deficient\ndata matrix, can detect weak component with small energy. Furthermore, on the\napplication of hyperspectral imaging, SON-NMF handle the issue of spectral\nvariability naturally.\n', '  This paper considers the problem of estimating a low-rank matrix from the\nobservation of all or a subset of its entries in the presence of Poisson noise.\nWhen we observe all entries, this is a problem of matrix denoising; when we\nobserve only a subset of the entries, this is a problem of matrix completion.\nIn both cases, we exploit an assumption that the underlying matrix is low-rank.\nSpecifically, we analyze several estimators, including a constrained\nnuclear-norm minimization program, nuclear-norm regularized least squares, and\na nonconvex constrained low-rank optimization problem. We show that for all\nthree estimators, with high probability, we have an upper error bound (in the\nFrobenius norm error metric) that depends on the matrix rank, the fraction of\nthe elements observed, and maximal row and column sums of the true matrix. We\nfurthermore show that the above results are minimax optimal (within a universal\nconstant) in classes of matrices with low rank and bounded row and column sums.\nWe also extend these results to handle the case of matrix multinomial denoising\nand completion.\n', '  In this paper, we develop a relative error bound for nuclear norm regularized\nmatrix completion, with the focus on the completion of full-rank matrices.\nUnder the assumption that the top eigenspaces of the target matrix are\nincoherent, we derive a relative upper bound for recovering the best low-rank\napproximation of the unknown matrix. Although multiple works have been devoted\nto analyzing the recovery error of full-rank matrix completion, their error\nbounds are usually additive, making it impossible to obtain the perfect\nrecovery case and more generally difficult to leverage the skewed distribution\nof eigenvalues. Our analysis is built upon the optimality condition of the\nregularized formulation and existing guarantees for low-rank matrix completion.\nTo the best of our knowledge, this is the first relative bound that has been\nproved for the regularized formulation of matrix completion.\n']",Low-Rank Matrix Factorization and Completion,Knowledge Representation and Matrix Completion Methods,Tensor and Matrix Methods for Data Representation and Completion,Tensor and Matrix Methods for Data Representation and Completion
197,44,197_emissions_carbon_energy_co2,"['emissions', 'carbon', 'energy', 'co2', 'efficiency', 'efficient', 'models', 'emission', 'environmentally', 'footprint']","['energy', 'carbon', 'consumption', 'emissions', 'green', 'environmental', 'footprint', 'sustainable', 'sustainability', 'hardware']","['  DNN inference, known for its significant energy consumption and the resulting\nhigh carbon footprint, can be made more sustainable by adapting model size and\naccuracy to the varying carbon intensity throughout the day. Our heuristic\nalgorithm uses larger, high-accuracy models during low-intensity periods and\nsmaller, lower-accuracy ones during high-intensity periods. We also introduce a\nmetric, carbon-emission efficiency, which quantitatively measures the efficacy\nof adaptive model selection in terms of carbon footprint. The evaluation showed\nthat the proposed approach could improve the carbon emission efficiency in\nimproving the accuracy of vision recognition services by up to 80%.\n', '  By integrating Artificial Intelligence (AI) with the Internet of Things\n(IoT), Artificial Intelligence of Things (AIoT) has revolutionized many fields.\nHowever, AIoT is facing the challenges of energy consumption and carbon\nemissions due to the continuous advancement of mobile technology. Fortunately,\nGenerative AI (GAI) holds immense potential to reduce carbon emissions of AIoT\ndue to its excellent reasoning and generation capabilities. In this article, we\nexplore the potential of GAI for carbon emissions reduction and propose a novel\nGAI-enabled solution for low-carbon AIoT. Specifically, we first study the main\nimpacts that cause carbon emissions in AIoT, and then introduce GAI techniques\nand their relations to carbon emissions. We then explore the application\nprospects of GAI in low-carbon AIoT, focusing on how GAI can reduce carbon\nemissions of network components. Subsequently, we propose a Large Language\nModel (LLM)-enabled carbon emission optimization framework, in which we design\npluggable LLM and Retrieval Augmented Generation (RAG) modules to generate more\naccurate and reliable optimization problems. Furthermore, we utilize Generative\nDiffusion Models (GDMs) to identify optimal strategies for carbon emission\nreduction. Numerical results demonstrate the effectiveness of the proposed\nframework. Finally, we insightfully provide open research directions for\nlow-carbon AIoT.\n', '  Machine learning (ML) has seen tremendous advancements, but its environmental\nfootprint remains a concern. Acknowledging the growing environmental impact of\nML this paper investigates Green ML, examining various model architectures and\nhyperparameters in both training and inference phases to identify\nenergy-efficient practices. Our study leverages software-based power\nmeasurements for ease of replication across diverse configurations, models and\ndatasets. In this paper, we examine multiple models and hardware configurations\nto identify correlations across the various measurements and metrics and key\ncontributors to energy reduction. Our analysis offers practical guidelines for\nconstructing sustainable ML operations, emphasising energy consumption and\ncarbon footprint reductions while maintaining performance. As identified,\nshort-lived profiling can quantify the long-term expected energy consumption.\nMoreover, model parameters can also be used to accurately estimate the expected\ntotal energy without the need for extensive experimentation.\n']",Reducing Carbon Footprint in AI and ML,Sustainable AI and Digital Agriculture Innovations,Innovations in Sustainable Technology and Food Systems,Innovations in Sustainable Technology and Food Systems
198,44,198_locomotion_stepping_terrain_legged,"['locomotion', 'stepping', 'terrain', 'legged', 'robotics', 'walking', 'jumps', 'terrains', 'jumping', 'robotic']","['locomotion', 'robots', 'robot', 'legged', 'controller', 'motor', 'gait', 'motion', 'navigation', 'bipedal']","[""  Autonomous wheeled-legged robots have the potential to transform logistics\nsystems, improving operational efficiency and adaptability in urban\nenvironments. Navigating urban environments, however, poses unique challenges\nfor robots, necessitating innovative solutions for locomotion and navigation.\nThese challenges include the need for adaptive locomotion across varied\nterrains and the ability to navigate efficiently around complex dynamic\nobstacles. This work introduces a fully integrated system comprising adaptive\nlocomotion control, mobility-aware local navigation planning, and large-scale\npath planning within the city. Using model-free reinforcement learning (RL)\ntechniques and privileged learning, we develop a versatile locomotion\ncontroller. This controller achieves efficient and robust locomotion over\nvarious rough terrains, facilitated by smooth transitions between walking and\ndriving modes. It is tightly integrated with a learned navigation controller\nthrough a hierarchical RL framework, enabling effective navigation through\nchallenging terrain and various obstacles at high speed. Our controllers are\nintegrated into a large-scale urban navigation system and validated by\nautonomous, kilometer-scale navigation missions conducted in Zurich,\nSwitzerland, and Seville, Spain. These missions demonstrate the system's\nrobustness and adaptability, underscoring the importance of integrated control\nsystems in achieving seamless navigation in complex environments. Our findings\nsupport the feasibility of wheeled-legged robots and hierarchical RL for\nautonomous navigation, with implications for last-mile delivery and beyond.\n"", '  While most recent advancements in legged robot control have been driven by\nmodel-free reinforcement learning, we explore the potential of differentiable\nsimulation. Differentiable simulation promises faster convergence and more\nstable training by computing low-variant first-order gradients using the robot\nmodel, but so far, its use for legged robot control has remained limited to\nsimulation. The main challenge with differentiable simulation lies in the\ncomplex optimization landscape of robotic tasks due to discontinuities in\ncontact-rich environments, e.g., quadruped locomotion. This work proposes a\nnew, differentiable simulation framework to overcome these challenges. The key\nidea involves decoupling the complex whole-body simulation, which may exhibit\ndiscontinuities due to contact, into two separate continuous domains.\nSubsequently, we align the robot state resulting from the simplified model with\na more precise, non-differentiable simulator to maintain sufficient simulation\naccuracy. Our framework enables learning quadruped walking in minutes using a\nsingle simulated robot without any parallelization. When augmented with GPU\nparallelization, our approach allows the quadruped robot to master diverse\nlocomotion skills, including trot, pace, bound, and gallop, on challenging\nterrains in minutes. Additionally, our policy achieves robust locomotion\nperformance in the real world zero-shot. To the best of our knowledge, this\nwork represents the first demonstration of using differentiable simulation for\ncontrolling a real quadruped robot. This work provides several important\ninsights into using differentiable simulations for legged locomotion in the\nreal world.\n', '  Recent advances of locomotion controllers utilizing deep reinforcement\nlearning (RL) have yielded impressive results in terms of achieving rapid and\nrobust locomotion across challenging terrain, such as rugged rocks, non-rigid\nground, and slippery surfaces. However, while these controllers primarily\naddress challenges underneath the robot, relatively little research has\ninvestigated legged mobility through confined 3D spaces, such as narrow tunnels\nor irregular voids, which impose all-around constraints. The cyclic gait\npatterns resulted from existing RL-based methods to learn parameterized\nlocomotion skills characterized by motion parameters, such as velocity and body\nheight, may not be adequate to navigate robots through challenging confined 3D\nspaces, requiring both agile 3D obstacle avoidance and robust legged\nlocomotion. Instead, we propose to learn locomotion skills end-to-end from\ngoal-oriented navigation in confined 3D spaces. To address the inefficiency of\ntracking distant navigation goals, we introduce a hierarchical locomotion\ncontroller that combines a classical planner tasked with planning waypoints to\nreach a faraway global goal location, and an RL-based policy trained to follow\nthese waypoints by generating low-level motion commands. This approach allows\nthe policy to explore its own locomotion skills within the entire solution\nspace and facilitates smooth transitions between local goals, enabling\nlong-term navigation towards distant goals. In simulation, our hierarchical\napproach succeeds at navigating through demanding confined 3D environments,\noutperforming both pure end-to-end learning approaches and parameterized\nlocomotion skills. We further demonstrate the successful real-world deployment\nof our simulation-trained controller on a real robot.\n']",Legged Robot Locomotion and Navigation,Robot Navigation and Locomotion,Robotics and Artificial Intelligence,Robotics and Artificial Intelligence
199,43,199_tensors_tensor_kronecker_factorization,"['tensors', 'tensor', 'kronecker', 'factorization', 'tucker', 'multilinear', 'multidimensional', 'decompositions', 'decomposition', 'dimensional']","['tensor', 'tensors', 'decomposition', 'completion', 'rank', 'recovery', 'coseparable', 'tubal', 'multilinear', 'factor']","['  Recently, numerous tensor singular value decomposition (t-SVD)-based tensor\nrecovery methods have shown promise in processing visual data, such as color\nimages and videos. However, these methods often suffer from severe performance\ndegradation when confronted with tensor data exhibiting non-smooth changes. It\nhas been commonly observed in real-world scenarios but ignored by the\ntraditional t-SVD-based methods. In this work, we introduce a novel tensor\nrecovery model with a learnable tensor nuclear norm to address such a\nchallenge. We develop a new optimization algorithm named the Alternating\nProximal Multiplier Method (APMM) to iteratively solve the proposed tensor\ncompletion model. Theoretical analysis demonstrates the convergence of the\nproposed APMM to the Karush-Kuhn-Tucker (KKT) point of the optimization\nproblem. In addition, we propose a multi-objective tensor recovery framework\nbased on APMM to efficiently explore the correlations of tensor data across its\nvarious dimensions, providing a new perspective on extending the t-SVD-based\nmethod to higher-order tensor cases. Numerical experiments demonstrated the\neffectiveness of the proposed method in tensor completion.\n', '  We proposed the tensor-input tree (TT) method for scalar-on-tensor and\ntensor-on-tensor regression problems. We first address scalar-on-tensor problem\nby proposing scalar-output regression tree models whose input variable are\ntensors (i.e., multi-way arrays). We devised and implemented fast randomized\nand deterministic algorithms for efficient fitting of scalar-on-tensor trees,\nmaking TT competitive against tensor-input GP models. Based on scalar-on-tensor\ntree models, we extend our method to tensor-on-tensor problems using additive\ntree ensemble approaches. Theoretical justification and extensive experiments\non real and synthetic datasets are provided to illustrate the performance of\nTT.\n', ""  This paper studies the prediction task of tensor-on-tensor regression in\nwhich both covariates and responses are multi-dimensional arrays (a.k.a.,\ntensors) across time with arbitrary tensor order and data dimension. Existing\nmethods either focused on linear models without accounting for possibly\nnonlinear relationships between covariates and responses, or directly employed\nblack-box deep learning algorithms that failed to utilize the inherent tensor\nstructure. In this work, we propose a Factor Augmented Tensor-on-Tensor Neural\nNetwork (FATTNN) that integrates tensor factor models into deep neural\nnetworks. We begin with summarizing and extracting useful predictive\ninformation (represented by the ``factor tensor'') from the complex structured\ntensor covariates, and then proceed with the prediction task using the\nestimated factor tensor as input of a temporal convolutional neural network.\nThe proposed methods effectively handle nonlinearity between complex data\nstructures, and improve over traditional statistical models and conventional\ndeep learning approaches in both prediction accuracy and computational cost. By\nleveraging tensor factor models, our proposed methods exploit the underlying\nlatent factor structure to enhance the prediction, and in the meantime,\ndrastically reduce the data dimensionality that speeds up the computation. The\nempirical performances of our proposed methods are demonstrated via simulation\nstudies and real-world applications to three public datasets. Numerical results\nshow that our proposed algorithms achieve substantial increases in prediction\naccuracy and significant reductions in computational time compared to benchmark\nmethods.\n""]",Tensor Recovery and Regression Methods,Tensor Methods and Applications,Tensor and Matrix Methods for Data Representation and Completion,Tensor and Matrix Methods for Data Representation and Completion
200,43,200_pose_poses_3d_camera,"['pose', 'poses', '3d', 'camera', '3dhp', 'cameras', 'posture', '3dsp', 'human3', 'dor3d']","['motion', 'estimation', 'poses', 'posture', 'keypoints', 'body', 'joint', 'human', 'skeleton', 'occlusions']","['  We present a generative approach to forecast long-term future human behavior\nin 3D, requiring only weak supervision from readily available 2D human action\ndata. This is a fundamental task enabling many downstream applications. The\nrequired ground-truth data is hard to capture in 3D (mocap suits, expensive\nsetups) but easy to acquire in 2D (simple RGB cameras). Thus, we design our\nmethod to only require 2D RGB data at inference time while being able to\ngenerate 3D human motion sequences. We use a differentiable 2D projection\nscheme in an autoregressive manner for weak supervision, and an adversarial\nloss for 3D regularization. Our method predicts long and complex human behavior\nsequences (e.g., cooking, assembly) consisting of multiple sub-actions. We\ntackle this in a semantically hierarchical manner, jointly predicting\nhigh-level coarse action labels together with their low-level fine-grained\nrealizations as characteristic 3D human poses. We observe that these two action\nrepresentations are coupled in nature, and joint prediction benefits both\naction and pose forecasting. Our experiments demonstrate the complementary\nnature of joint action and 3D pose prediction: our joint approach outperforms\neach task treated individually, enables robust longer-term sequence prediction,\nand improves over alternative approaches to forecast actions and characteristic\n3D poses.\n', '  Monocular Human Pose Estimation (HPE) aims at determining the 3D positions of\nhuman joints from a single 2D image captured by a camera. However, a single 2D\npoint in the image may correspond to multiple points in 3D space. Typically,\nthe uniqueness of the 2D-3D relationship is approximated using an orthographic\nor weak-perspective camera model. In this study, instead of relying on\napproximations, we advocate for utilizing the full perspective camera model.\nThis involves estimating camera parameters and establishing a precise,\nunambiguous 2D-3D relationship. To do so, we introduce the EPOCH framework,\ncomprising two main components: the pose lifter network (LiftNet) and the pose\nregressor network (RegNet). LiftNet utilizes the full perspective camera model\nto precisely estimate the 3D pose in an unsupervised manner. It takes a 2D pose\nand camera parameters as inputs and produces the corresponding 3D pose\nestimation. These inputs are obtained from RegNet, which starts from a single\nimage and provides estimates for the 2D pose and camera parameters. RegNet\nutilizes only 2D pose data as weak supervision. Internally, RegNet predicts a\n3D pose, which is then projected to 2D using the estimated camera parameters.\nThis process enables RegNet to establish the unambiguous 2D-3D relationship.\nOur experiments show that modeling the lifting as an unsupervised task with a\ncamera in-the-loop results in better generalization to unseen data. We obtain\nstate-of-the-art results for the 3D HPE on the Human3.6M and MPI-INF-3DHP\ndatasets. Our code is available at: [Github link upon acceptance, see\nsupplementary materials].\n', '  Its numerous applications make multi-human 3D pose estimation a remarkably\nimpactful area of research. Nevertheless, assuming a multiple-view system\ncomposed of several regular RGB cameras, 3D multi-pose estimation presents\nseveral challenges. First of all, each person must be uniquely identified in\nthe different views to separate the 2D information provided by the cameras.\nSecondly, the 3D pose estimation process from the multi-view 2D information of\neach person must be robust against noise and potential occlusions in the\nscenario. In this work, we address these two challenges with the help of deep\nlearning. Specifically, we present a model based on Graph Neural Networks\ncapable of predicting the cross-view correspondence of the people in the\nscenario along with a Multilayer Perceptron that takes the 2D points to yield\nthe 3D poses of each person. These two models are trained in a self-supervised\nmanner, thus avoiding the need for large datasets with 3D annotations.\n']",3D Human Pose Estimation from 2D Data,3D Pose Estimation and Scene Understanding,Computer Vision and 3D Scene Understanding,Computer Vision and 3D Scene Understanding
201,43,201_transport_regularization_transportation_optimal,"['transport', 'regularization', 'transportation', 'optimal', 'optimization', 'regularized', 'distributions', 'maps', 'divergence', 'densities']","['transport', 'optimal', 'entropic', 'map', 'solver', 'solvers', 'densities', 'unbalanced', 'maps', 'formulation']","['  Within the field of optimal transport (OT), the choice of ground cost is\ncrucial to ensuring that the optimality of a transport map corresponds to\nusefulness in real-world applications. It is therefore desirable to use known\ninformation to tailor cost functions and hence learn OT maps which are adapted\nto the problem at hand. By considering a class of neural ground costs whose\nMonge maps have a known form, we construct a differentiable Monge map estimator\nwhich can be optimized to be consistent with known information about an OT map.\nIn doing so, we simultaneously learn both an OT map estimator and a\ncorresponding adapted cost function. Through suitable choices of loss function,\nour method provides a general approach for incorporating prior information\nabout the Monge map itself when learning adapted OT maps and cost functions.\n', '  Entropic optimal transport (OT) and the Sinkhorn algorithm have made it\npractical for machine learning practitioners to perform the fundamental task of\ncalculating transport distance between statistical distributions. In this work,\nwe focus on a general class of OT problems under a combination of equality and\ninequality constraints. We derive the corresponding entropy regularization\nformulation and introduce a Sinkhorn-type algorithm for such constrained OT\nproblems supported by theoretical guarantees. We first bound the approximation\nerror when solving the problem through entropic regularization, which reduces\nexponentially with the increase of the regularization parameter. Furthermore,\nwe prove a sublinear first-order convergence rate of the proposed Sinkhorn-type\nalgorithm in the dual space by characterizing the optimization procedure with a\nLyapunov function. To achieve fast and higher-order convergence under weak\nentropy regularization, we augment the Sinkhorn-type algorithm with dynamic\nregularization scheduling and second-order acceleration. Overall, this work\nsystematically combines recent theoretical and numerical advances in entropic\noptimal transport with the constrained case, allowing practitioners to derive\napproximate transport plans in complex scenarios.\n', '  Optimal Transport (OT) problem investigates a transport map that bridges two\ndistributions while minimizing a given cost function. In this regard, OT\nbetween tractable prior distribution and data has been utilized for generative\nmodeling tasks. However, OT-based methods are susceptible to outliers and face\noptimization challenges during training. In this paper, we propose a novel\ngenerative model based on the semi-dual formulation of Unbalanced Optimal\nTransport (UOT). Unlike OT, UOT relaxes the hard constraint on distribution\nmatching. This approach provides better robustness against outliers, stability\nduring training, and faster convergence. We validate these properties\nempirically through experiments. Moreover, we study the theoretical upper-bound\nof divergence between distributions in UOT. Our model outperforms existing\nOT-based generative models, achieving FID scores of 2.97 on CIFAR-10 and 6.36\non CelebA-HQ-256. The code is available at\n\\url{https://github.com/Jae-Moo/UOTM}.\n']",Optimal Transport and Regularization Methods,Optimal Transport and Wasserstein Metrics for Machine Learning and Optimization,Machine Learning and Optimization,Machine Learning and Artificial Intelligence
202,43,202_powerflownet_powerflowmultinet_networks_powergraph,"['powerflownet', 'powerflowmultinet', 'networks', 'powergraph', 'grid', 'safepowergraph', 'grids', 'flow', 'blackout', 'optimal']","['power', 'grids', 'grid', 'bus', 'flow', 'contingencies', 'contingency', 'network', 'dual', 'graph']","['  A DC OPF surrogate modeling framework is developed for Monte Carlo (MC)\nsampling-based risk quantification in power grid operation. MC simulation\nnecessitates solving a large number of DC OPF problems corresponding to the\nsamples of stochastic grid variables (power demand and renewable generation),\nwhich is computationally prohibitive. Computationally inexpensive surrogates of\nOPF provide an attractive alternative for expedited MC simulation. Graph neural\nnetwork (GNN) surrogates of DC OPF, which are especially suitable to\ngraph-structured data, are employed in this work. Previously developed DC OPF\nsurrogate models have focused on accurate operational decision-making and not\non risk quantification. Here, risk quantification-specific aspects of DC OPF\nsurrogate evaluation is the main focus. To this end, the proposed GNN\nsurrogates are evaluated using realistic joint probability distributions,\nquantification of their risk estimation accuracy, and investigation of their\ngeneralizability. Four synthetic grids (Case118, Case300, Case1354pegase, and\nCase2848rte) are used for surrogate model performance evaluation. It is shown\nthat the GNN surrogates are sufficiently accurate for predicting the\n(bus-level, branch-level and system-level) grid state and enable fast as well\nas accurate operational risk quantification for power grids. The article thus\ndevelops tools for fast reliability and risk quantification in real-world power\ngrids using GNN-based surrogates.\n', '  Optimal Power Flow (OPF) refers to a wide range of related optimization\nproblems with the goal of operating power systems efficiently and securely. In\nthe simplest setting, OPF determines how much power to generate in order to\nminimize costs while meeting demand for power and satisfying physical and\noperational constraints. In even the simplest case, power grid operators use\napproximations of the AC-OPF problem because solving the exact problem is\nprohibitively slow with state-of-the-art solvers. These approximations\nsacrifice accuracy and operational feasibility in favor of speed. This\ntrade-off leads to costly ""uplift payments"" and increased carbon emissions,\nespecially for large power grids. In the present work, we train a deep learning\nsystem (CANOS) to predict near-optimal solutions (within 1% of the true AC-OPF\ncost) without compromising speed (running in as little as 33--65 ms).\nImportantly, CANOS scales to realistic grid sizes with promising empirical\nresults on grids containing as many as 10,000 buses. Finally, because CANOS is\na Graph Neural Network, it is robust to changes in topology. We show that CANOS\nis accurate across N-1 topological perturbations of a base grid typically used\nin security-constrained analysis. This paves the way for more efficient\noptimization of more complex OPF problems which alter grid connectivity such as\nunit commitment, topology optimization and security-constrained OPF.\n', '  The AC optimal power flow (AC-OPF) problem is essential for power system\noperations, but its non-convex nature makes it challenging to solve. A widely\nused simplification is the linearized DC optimal power flow (DC-OPF) problem,\nwhich can be solved to global optimality, but whose optimal solution is always\ninfeasible in the original AC-OPF problem. Recently, neural networks (NN) have\nbeen introduced for solving the AC-OPF problem at significantly faster\ncomputation times. However, these methods necessitate extensive datasets, are\ndifficult to train, and are often viewed as black boxes, leading to resistance\nfrom operators who prefer more transparent and interpretable solutions. In this\npaper, we introduce a novel learning-based approach that merges simplicity and\ninterpretability, providing a bridge between traditional approximation methods\nand black-box learning techniques. Our approach not only provides transparency\nfor operators but also achieves competitive accuracy. Numerical results across\nvarious power networks demonstrate that our method provides accuracy comparable\nto, and often surpassing, that of neural networks, particularly when training\ndatasets are limited.\n']",Optimal Power Flow in Power Grids,Optimization Methods for Power Grid Management,Optimization and Design,Optimization and Design
203,42,203_ai_healthcare_medical_biomedical,"['ai', 'healthcare', 'medical', 'biomedical', 'medicine', 'med', 'clinical', 'trustworthy', 'patients', 'mllms']","['healthcare', 'medical', 'clinical', 'patient', 'medicine', 'ethical', 'regulatory', 'imaging', 'care', 'trustworthy']","['  As more clinical workflows continue to be augmented by artificial\nintelligence (AI), AI literacy among physicians will become a critical\nrequirement for ensuring safe and ethical AI-enabled patient care. Despite the\nevolving importance of AI in healthcare, the extent to which it has been\nadopted into traditional and often-overloaded medical curricula is currently\nunknown. In a scoping review of 1,699 articles published between January 2016\nand June 2024, we identified 18 studies which propose guiding frameworks, and\n11 studies documenting real-world instruction, centered around the integration\nof AI into medical education. We found that comprehensive guidelines will\nrequire greater clinical relevance and personalization to suit medical student\ninterests and career trajectories. Current efforts highlight discrepancies in\nthe teaching guidelines, emphasizing AI evaluation and ethics over technical\ntopics such as data science and coding. Additionally, we identified several\nchallenges associated with integrating AI training into the medical education\nprogram, including a lack of guidelines to define medical students AI literacy,\na perceived lack of proven clinical value, and a scarcity of qualified\ninstructors. With this knowledge, we propose an AI literacy framework to define\ncompetencies for medical students. To prioritize relevant and personalized AI\neducation, we categorize literacy into four dimensions: Foundational,\nPractical, Experimental, and Ethical, with tailored learning objectives to the\npre-clinical, clinical, and clinical research stages of medical education. This\nreview provides a road map for developing practical and relevant education\nstrategies for building an AI-competent healthcare workforce.\n', ""  The recent advancements in artificial intelligence (AI) combined with the\nextensive amount of data generated by today's clinical systems, has led to the\ndevelopment of imaging AI solutions across the whole value chain of medical\nimaging, including image reconstruction, medical image segmentation,\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\nfuture potential of AI in medical imaging, many stakeholders are concerned of\nthe potential risks and ethical implications of imaging AI solutions, which are\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\nin critical clinical applications. Addressing these concerns and risks, the\nFUTURE-AI framework has been proposed, which, sourced from a global\nmulti-domain expert consensus, comprises guiding principles for increased\ntrust, safety, and adoption for AI in healthcare. In this paper, we transform\nthe general FUTURE-AI healthcare principles to a concise and specific AI\nimplementation guide tailored to the needs of the medical imaging community. To\nthis end, we carefully assess each building block of the FUTURE-AI framework\nconsisting of (i) Fairness, (ii) Universality, (iii) Traceability, (iv)\nUsability, (v) Robustness and (vi) Explainability, and respectively define\nconcrete best practices based on accumulated AI implementation experiences from\nfive large European projects on AI in Health Imaging. We accompany our concrete\nstep-by-step medical imaging development guide with a practical AI solution\nmaturity checklist, thus enabling AI development teams to design, evaluate,\nmaintain, and deploy technically, clinically and ethically trustworthy imaging\nAI solutions into clinical practice.\n"", '  Despite major advances in artificial intelligence (AI) for medicine and\nhealthcare, the deployment and adoption of AI technologies remain limited in\nreal-world clinical practice. In recent years, concerns have been raised about\nthe technical, clinical, ethical and legal risks associated with medical AI. To\nincrease real world adoption, it is essential that medical AI tools are trusted\nand accepted by patients, clinicians, health organisations and authorities.\nThis work describes the FUTURE-AI guideline as the first international\nconsensus framework for guiding the development and deployment of trustworthy\nAI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and\ncurrently comprises 118 inter-disciplinary experts from 51 countries\nrepresenting all continents, including AI scientists, clinicians, ethicists,\nand social scientists. Over a two-year period, the consortium defined guiding\nprinciples and best practices for trustworthy AI through an iterative process\ncomprising an in-depth literature review, a modified Delphi survey, and online\nconsensus meetings. The FUTURE-AI framework was established based on 6 guiding\nprinciples for trustworthy AI in healthcare, i.e. Fairness, Universality,\nTraceability, Usability, Robustness and Explainability. Through consensus, a\nset of 28 best practices were defined, addressing technical, clinical, legal\nand socio-ethical dimensions. The recommendations cover the entire lifecycle of\nmedical AI, from design, development and validation to regulation, deployment,\nand monitoring. FUTURE-AI is a risk-informed, assumption-free guideline which\nprovides a structured approach for constructing medical AI tools that will be\ntrusted, deployed and adopted in real-world practice. Researchers are\nencouraged to take the recommendations into account in proof-of-concept stages\nto facilitate future translation towards clinical practice of medical AI.\n']",AI in Healthcare and Medical Education,Artificial Intelligence Ethics and Governance,Artificial Intelligence Applications and Implications,Artificial Intelligence Applications and Implications
204,42,204_learning_learned_training_learn,"['learning', 'learned', 'training', 'learn', 'learners', 'overfitting', 'regularization', 'learner', 'meta', 'multitask']","['meta', 'inductive', 'task', 'loop', 'learning', 'outer', 'tasks', 'inner', 'learn', 'agnostic']","['  There is a growing interest in the learning-to-learn paradigm, also known as\nmeta-learning, where models infer on new tasks using a few training examples.\nRecently, meta-learning based methods have been widely used in few-shot\nclassification, regression, reinforcement learning, and domain adaptation. The\nmodel-agnostic meta-learning (MAML) algorithm is a well-known algorithm that\nobtains model parameter initialization at meta-training phase. In the meta-test\nphase, this initialization is rapidly adapted to new tasks by using gradient\ndescent. However, meta-learning models are prone to overfitting since there are\ninsufficient training tasks resulting in over-parameterized models with poor\ngeneralization performance for unseen tasks. In this paper, we propose a\nBayesian neural network based MAML algorithm, which we refer to as the B-SMALL\nalgorithm. The proposed framework incorporates a sparse variational loss term\nalongside the loss function of MAML, which uses a sparsifying approximated KL\ndivergence as a regularizer. We demonstrate the performance of B-MAML using\nclassification and regression tasks, and highlight that training a sparsifying\nBNN using MAML indeed improves the parameter footprint of the model while\nperforming at par or even outperforming the MAML approach. We also illustrate\napplicability of our approach in distributed sensor networks, where sparsity\nand meta-learning can be beneficial.\n', ""  As a subset of machine learning, meta-learning, or learning to learn, aims at\nimproving the model's capabilities by employing prior knowledge and experience.\nA meta-learning paradigm can appropriately tackle the conventional challenges\nof traditional learning approaches, such as insufficient number of samples,\ndomain shifts, and generalization. These unique characteristics position\nmeta-learning as a suitable choice for developing influential solutions in\nvarious healthcare contexts, where the available data is often insufficient,\nand the data collection methodologies are different. This survey discusses\nmeta-learning broad applications in the healthcare domain to provide insight\ninto how and where it can address critical healthcare challenges. We first\ndescribe the theoretical foundations and pivotal methods of meta-learning. We\nthen divide the employed meta-learning approaches in the healthcare domain into\ntwo main categories of multi/single-task learning and many/few-shot learning\nand survey the studies. Finally, we highlight the current challenges in\nmeta-learning research, discuss the potential solutions, and provide future\nperspectives on meta-learning in healthcare.\n"", '  Few-shot learning, a challenging task in machine learning, aims to learn a\nclassifier adaptable to recognize new, unseen classes with limited labeled\nexamples. Meta-learning has emerged as a prominent framework for few-shot\nlearning. Its training framework is originally a task-level learning method,\nsuch as Model-Agnostic Meta-Learning (MAML) and Prototypical Networks. And a\nrecently proposed training paradigm called Meta-Baseline, which consists of\nsequential pre-training and meta-training stages, gains state-of-the-art\nperformance. However, as a non-end-to-end training method, indicating the\nmeta-training stage can only begin after the completion of pre-training,\nMeta-Baseline suffers from higher training cost and suboptimal performance due\nto the inherent conflicts of the two training stages. To address these\nlimitations, we propose an end-to-end training paradigm consisting of two\nalternative loops. In the outer loop, we calculate cross entropy loss on the\nentire training set while updating only the final linear layer. In the inner\nloop, we employ the original meta-learning training mode to calculate the loss\nand incorporate gradients from the outer loss to guide the parameter updates.\nThis training paradigm not only converges quickly but also outperforms existing\nbaselines, indicating that information from the overall training set and the\nmeta-learning training paradigm could mutually reinforce one another. Moreover,\nbeing model-agnostic, our framework achieves significant performance gains,\nsurpassing the baseline systems by approximate 1%.\n']",Meta-Learning and Few-Shot Learning,Meta-Learning and Few-Shot Learning in Machine Learning and NLP,Machine Learning Methodologies,Machine Learning Methodologies
205,41,205_transcriptomics_transcriptomic_transcriptome_rna,"['transcriptomics', 'transcriptomic', 'transcriptome', 'rna', 'bioinformatics', 'cell', 'cells', 'gene', 'genes', 'cellular']","['cell', 'seq', 'gene', 'expression', 'transcriptomics', 'single', 'spatial', 'omics', 'biological', 'cellular']","['  Inferring gene regulatory networks (GRNs) from single-cell RNA sequencing\n(scRNA-seq) data is a complex challenge that requires capturing the intricate\nrelationships between genes and their regulatory interactions. In this study,\nwe tackle this challenge by leveraging the single-cell BERT-based pre-trained\ntransformer model (scBERT), trained on extensive unlabeled scRNA-seq data, to\naugment structured biological knowledge from existing GRNs. We introduce a\nnovel joint graph learning approach that combines the rich contextual\nrepresentations learned by pre-trained single-cell language models with the\nstructured knowledge encoded in GRNs using graph neural networks (GNNs). By\nintegrating these two modalities, our approach effectively reasons over boththe\ngene expression level constraints provided by the scRNA-seq data and the\nstructured biological knowledge inherent in GRNs. We evaluate our method on\nhuman cell benchmark datasets from the BEELINE study with cell type-specific\nground truth networks. The results demonstrate superior performance over\ncurrent state-of-the-art baselines, offering a deeper understanding of cellular\nregulatory mechanisms.\n', '  The transformers have achieved significant accomplishments in the natural\nlanguage processing as its outstanding parallel processing capabilities and\nhighly flexible attention mechanism. In addition, increasing studies based on\ntransformers have been proposed to model single-cell data. In this review, we\nattempt to systematically summarize the single-cell language models and\napplications based on transformers. First, we provide a detailed introduction\nabout the structure and principles of transformers. Then, we review the\nsingle-cell language models and large language models for single-cell data\nanalysis. Moreover, we explore the datasets and applications of single-cell\nlanguage models in downstream tasks such as batch correction, cell clustering,\ncell type annotation, gene regulatory network inference and perturbation\nresponse. Further, we discuss the challenges of single-cell language models and\nprovide promising research directions. We hope this review will serve as an\nup-to-date reference for researchers interested in the direction of single-cell\nlanguage models.\n', '  Single-cell RNA sequencing (scRNA-seq) data are important for studying the\nlaws of life at single-cell level. However, it is still challenging to obtain\nenough high-quality scRNA-seq data. To mitigate the limited availability of\ndata, generative models have been proposed to computationally generate\nsynthetic scRNA-seq data. Nevertheless, the data generated with current models\nare not very realistic yet, especially when we need to generate data with\ncontrolled conditions. In the meantime, the Diffusion models have shown their\npower in generating data at high fidelity, providing a new opportunity for\nscRNA-seq generation.\n  In this study, we developed scDiffusion, a generative model combining\ndiffusion model and foundation model to generate high-quality scRNA-seq data\nwith controlled conditions. We designed multiple classifiers to guide the\ndiffusion process simultaneously, enabling scDiffusion to generate data under\nmultiple condition combinations. We also proposed a new control strategy called\nGradient Interpolation. This strategy allows the model to generate continuous\ntrajectories of cell development from a given cell state.\n  Experiments showed that scDiffusion can generate single-cell gene expression\ndata closely resembling real scRNA-seq data. Also, scDiffusion can\nconditionally produce data on specific cell types including rare cell types.\nFurthermore, we could use the multiple-condition generation of scDiffusion to\ngenerate cell type that was out of the training data. Leveraging the Gradient\nInterpolation strategy, we generated a continuous developmental trajectory of\nmouse embryonic cells. These experiments demonstrate that scDiffusion is a\npowerful tool for augmenting the real scRNA-seq data and can provide insights\ninto cell fate research.\n']",Single-Cell RNA Sequencing Analysis and Modeling,Computational Methods for Cancer Genomics and Transcriptomics,Computational Biology and Chemistry,Computational Biology and Chemistry
206,41,206_pathfinding_agents_planning_agent,"['pathfinding', 'agents', 'planning', 'agent', 'paths', 'planner', 'robots', 'algorithms', 'robotics', 'robot']","['robots', 'path', 'collision', 'paths', 'finding', 'planning', 'planner', 'robot', 'conflict', 'agent']","['  Multi-Agent Path Finding (MAPF) involves determining paths for multiple\nagents to travel simultaneously and collision-free through a shared area toward\ngiven goal locations. This problem is computationally complex, especially when\ndealing with large numbers of agents, as is common in realistic applications\nlike autonomous vehicle coordination. Finding an optimal solution is often\ncomputationally infeasible, making the use of approximate, suboptimal\nalgorithms essential. Adding to the complexity, agents might act in a\nself-interested and strategic way, possibly misrepresenting their goals to the\nMAPF algorithm if it benefits them. Although the field of mechanism design\noffers tools to align incentives, using these tools without careful\nconsideration can fail when only having access to approximately optimal\noutcomes. In this work, we introduce the problem of scalable mechanism design\nfor MAPF and propose three strategyproof mechanisms, two of which even use\napproximate MAPF algorithms. We test our mechanisms on realistic MAPF domains\nwith problem sizes ranging from dozens to hundreds of agents. We find that they\nimprove welfare beyond a simple baseline.\n', '  Multi-agent path finding (MAPF) is the problem of finding paths for multiple\nagents such that they do not collide. This problem manifests in numerous\nreal-world applications such as controlling transportation robots in automated\nwarehouses, moving characters in video games, and coordinating self-driving\ncars in intersections. Finding optimal solutions to MAPF is NP-Hard, yet modern\noptimal solvers can scale to hundreds of agents and even thousands in some\ncases. Different solvers employ different approaches, and there is no single\nstate-of-the-art approach for all problems. Furthermore, there are no clear,\nprovable, guidelines for choosing when each optimal MAPF solver to use. Prior\nwork employed Algorithm Selection (AS) techniques to learn such guidelines from\npast data. A major challenge when employing AS for choosing an optimal MAPF\nalgorithm is how to encode the given MAPF problem. Prior work either used\nhand-crafted features or an image representation of the problem. We explore\ngraph-based encodings of the MAPF problem and show how they can be used\non-the-fly with a modern graph embedding algorithm called FEATHER. Then, we\nshow how this encoding can be effectively joined with existing encodings,\nresulting in a novel AS method we call MAPF Algorithm selection via Graph\nembedding (MAG). An extensive experimental evaluation of MAG on several MAPF\nalgorithm selection tasks reveals that it is either on-par or significantly\nbetter than existing methods.\n', '  Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics that\nasks us to compute collision-free paths for a team of agents, all moving across\na shared map. Although many works appear on this topic, all current algorithms\nstruggle as the number of agents grows. The principal reason is that existing\napproaches typically plan free-flow optimal paths, which creates congestion. To\ntackle this issue, we propose a new approach for MAPF where agents are guided\nto their destination by following congestion-avoiding paths. We evaluate the\nidea in two large-scale settings: one-shot MAPF, where each agent has a single\ndestination, and lifelong MAPF, where agents are continuously assigned new\ndestinations. Empirically, we report large improvements in solution quality for\none-short MAPF and in overall throughput for lifelong MAPF.\n']",Multi-Agent Path Finding (MAPF),Multi-Agent Path Planning and Coordination,Multi-Agent Systems and Artificial Intelligence,Multi-Agent Systems and Artificial Intelligence
207,41,207_unlearning_federated_privacy_learning,"['unlearning', 'federated', 'privacy', 'learning', 'unlearned', 'distributed', 'disclosure', 'collaborative', 'private', 'decentralized']","['federated', 'unlearning', 'privacy', 'clients', 'client', 'centralized', 'collaborative', 'private', 'unlearned', 'global']","['  Federated learning (FL) offers a compelling framework for training large\nlanguage models (LLMs) while addressing data privacy and decentralization\nchallenges. This paper surveys recent advancements in the federated learning of\nlarge language models, with a particular focus on machine unlearning, a crucial\naspect for complying with privacy regulations like the Right to be Forgotten.\nMachine unlearning in the context of federated LLMs involves systematically and\nsecurely removing individual data contributions from the learned model without\nretraining from scratch. We explore various strategies that enable effective\nunlearning, such as perturbation techniques, model decomposition, and\nincremental learning, highlighting their implications for maintaining model\nperformance and data privacy. Furthermore, we examine case studies and\nexperimental results from recent literature to assess the effectiveness and\nefficiency of these approaches in real-world scenarios. Our survey reveals a\ngrowing interest in developing more robust and scalable federated unlearning\nmethods, suggesting a vital area for future research in the intersection of AI\nethics and distributed machine learning technologies.\n', '  We study federated unlearning, a novel problem to eliminate the impact of\nspecific clients or data points on the global model learned via federated\nlearning (FL). This problem is driven by the right to be forgotten and the\nprivacy challenges in FL. We introduce a new framework for exact federated\nunlearning that meets two essential criteria: \\textit{communication efficiency}\nand \\textit{exact unlearning provability}. To our knowledge, this is the first\nwork to tackle both aspects coherently. We start by giving a rigorous\ndefinition of \\textit{exact} federated unlearning, which guarantees that the\nunlearned model is statistically indistinguishable from the one trained without\nthe deleted data. We then pinpoint the key property that enables fast exact\nfederated unlearning: total variation (TV) stability, which measures the\nsensitivity of the model parameters to slight changes in the dataset.\nLeveraging this insight, we develop a TV-stable FL algorithm called\n\\texttt{FATS}, which modifies the classical\n\\texttt{\\underline{F}ed\\underline{A}vg} algorithm for \\underline{T}V\n\\underline{S}tability and employs local SGD with periodic averaging to lower\nthe communication round. We also design efficient unlearning algorithms for\n\\texttt{FATS} under two settings: client-level and sample-level unlearning. We\nprovide theoretical guarantees for our learning and unlearning algorithms,\nproving that they achieve exact federated unlearning with reasonable\nconvergence rates for both the original and unlearned models. We empirically\nvalidate our framework on 6 benchmark datasets, and show its superiority over\nstate-of-the-art methods in terms of accuracy, communication cost, computation\ncost, and unlearning efficacy.\n', '  Federated learning (FL), introduced in 2017, facilitates collaborative\nlearning between non-trusting parties with no need for the parties to\nexplicitly share their data among themselves. This allows training models on\nuser data while respecting privacy regulations such as GDPR and CPRA. However,\nemerging privacy requirements may mandate model owners to be able to\n\\emph{forget} some learned data, e.g., when requested by data owners or law\nenforcement. This has given birth to an active field of research called\n\\emph{machine unlearning}. In the context of FL, many techniques developed for\nunlearning in centralized settings are not trivially applicable! This is due to\nthe unique differences between centralized and distributed learning, in\nparticular, interactivity, stochasticity, heterogeneity, and limited\naccessibility in FL. In response, a recent line of work has focused on\ndeveloping unlearning mechanisms tailored to FL.\n  This SoK paper aims to take a deep look at the \\emph{federated unlearning}\nliterature, with the goal of identifying research trends and challenges in this\nemerging field. By carefully categorizing papers published on FL unlearning\n(since 2020), we aim to pinpoint the unique complexities of federated\nunlearning, highlighting limitations on directly applying centralized\nunlearning methods. We compare existing federated unlearning methods regarding\ninfluence removal and performance recovery, compare their threat models and\nassumptions, and discuss their implications and limitations. For instance, we\nanalyze the experimental setup of FL unlearning studies from various\nperspectives, including data heterogeneity and its simulation, the datasets\nused for demonstration, and evaluation metrics. Our work aims to offer insights\nand suggestions for future research on federated unlearning.\n']",Federated Learning and Unlearning for Data Privacy,Federated Learning for Privacy-Preserving Machine Learning,Machine Learning and Data Privacy,Machine Learning and Data Privacy
208,41,208_multimodal_recommender_embeddings_modality,"['multimodal', 'recommender', 'embeddings', 'modality', 'personalized', 'modal', 'recommendation', 'recommendations', 'collaborative', 'features']","['recommendation', 'multimodal', 'item', 'modality', 'modal', 'multimedia', 'modalities', 'recommender', 'user', 'items']","['  Multimodal recommendation aims to model user and item representations\ncomprehensively with the involvement of multimedia content for effective\nrecommendations. Existing research has shown that it is beneficial for\nrecommendation performance to combine (user- and item-) ID embeddings with\nmultimodal salient features, indicating the value of IDs. However, there is a\nlack of a thorough analysis of the ID embeddings in terms of feature semantics\nin the literature. In this paper, we revisit the value of ID embeddings for\nmultimodal recommendation and conduct a thorough study regarding its semantics,\nwhich we recognize as subtle features of \\emph{content} and \\emph{structure}.\nBased on our findings, we propose a novel recommendation model by incorporating\nID embeddings to enhance the salient features of both content and structure.\nSpecifically, we put forward a hierarchical attention mechanism to incorporate\nID embeddings in modality fusing, coupled with contrastive learning, to enhance\ncontent representations. Meanwhile, we propose a lightweight graph convolution\nnetwork for each modality to amalgamate neighborhood and ID embeddings for\nimproving structural representations. Finally, the content and structure\nrepresentations are combined to form the ultimate item embedding for\nrecommendation. Extensive experiments on three real-world datasets (Baby,\nSports, and Clothing) demonstrate the superiority of our method over\nstate-of-the-art multimodal recommendation methods and the effectiveness of\nfine-grained ID embeddings. Our code is available at\nhttps://anonymous.4open.science/r/IDSF-code/.\n', '  With the increasing multimedia information, multimodal recommendation has\nreceived extensive attention. It utilizes multimodal information to alleviate\nthe data sparsity problem in recommendation systems, thus improving\nrecommendation accuracy. However, the reliance on labeled data severely limits\nthe performance of multimodal recommendation models. Recently, self-supervised\nlearning has been used in multimodal recommendations to mitigate the label\nsparsity problem. Nevertheless, the state-of-the-art methods cannot avoid the\nmodality noise when aligning multimodal information due to the large\ndifferences in the distributions of different modalities. To this end, we\npropose a Multi-level sElf-supervised learNing for mulTimOdal Recommendation\n(MENTOR) method to address the label sparsity problem and the modality\nalignment problem. Specifically, MENTOR first enhances the specific features of\neach modality using the graph convolutional network (GCN) and fuses the visual\nand textual modalities. It then enhances the item representation via the item\nsemantic graph for all modalities, including the fused modality. Then, it\nintroduces two multilevel self-supervised tasks: the multilevel cross-modal\nalignment task and the general feature enhancement task. The multilevel\ncross-modal alignment task aligns each modality under the guidance of the ID\nembedding from multiple levels while maintaining the historical interaction\ninformation. The general feature enhancement task enhances the general feature\nfrom both the graph and feature perspectives to improve the robustness of our\nmodel. Extensive experiments on three publicly available datasets demonstrate\nthe effectiveness of our method. Our code is publicly available at\nhttps://github.com/Jinfeng-Xu/MENTOR.\n', '  Multi-modal recommendation greatly enhances the performance of recommender\nsystems by modeling the auxiliary information from multi-modality contents.\nMost existing multi-modal recommendation models primarily exploit multimedia\ninformation propagation processes to enrich item representations and directly\nutilize modal-specific embedding vectors independently obtained from upstream\npre-trained models. However, this might be inappropriate since the abundant\ntask-specific semantics remain unexplored, and the cross-modality semantic gap\nhinders the recommendation performance.\n  Inspired by the recent progress of the cross-modal alignment model CLIP, in\nthis paper, we propose a novel \\textbf{CLIP} \\textbf{E}nhanced\n\\textbf{R}ecommender (\\textbf{CLIPER}) framework to bridge the semantic gap\nbetween modalities and extract fine-grained multi-view semantic information.\nSpecifically, we introduce a multi-view modality-alignment approach for\nrepresentation extraction and measure the semantic similarity between\nmodalities. Furthermore, we integrate the multi-view multimedia representations\ninto downstream recommendation models. Extensive experiments conducted on three\npublic datasets demonstrate the consistent superiority of our model over\nstate-of-the-art multi-modal recommendation models.\n']",Multimodal Recommendation Systems,Multimodal Learning and Applications,Multimodal Learning and Applications,Multimodal Learning and Vision-Language Models
209,40,209_blockchain_blockchains_blockchained_mainchain,"['blockchain', 'blockchains', 'blockchained', 'mainchain', 'federated', 'ledger', 'decentralized', 'privacy', 'ipfs', 'security']","['blockchain', 'decentralized', 'security', 'federated', 'privacy', 'blockchains', 'clients', 'proof', 'consensus', 'smart']","['  Generative Artificial Intelligence (GAI) has recently emerged as a promising\nsolution to address critical challenges of blockchain technology, including\nscalability, security, privacy, and interoperability. In this paper, we first\nintroduce GAI techniques, outline their applications, and discuss existing\nsolutions for integrating GAI into blockchains. Then, we discuss emerging\nsolutions that demonstrate the effectiveness of GAI in addressing various\nchallenges of blockchain, such as detecting unknown blockchain attacks and\nsmart contract vulnerabilities, designing key secret sharing schemes, and\nenhancing privacy. Moreover, we present a case study to demonstrate that GAI,\nspecifically the generative diffusion model, can be employed to optimize\nblockchain network performance metrics. Experimental results clearly show that,\ncompared to a baseline traditional AI approach, the proposed generative\ndiffusion model approach can converge faster, achieve higher rewards, and\nsignificantly improve the throughput and latency of the blockchain network.\nAdditionally, we highlight future research directions for GAI in blockchain\napplications, including personalized GAI-enabled blockchains, GAI-blockchain\nsynergy, and privacy and security considerations within blockchain ecosystems.\n', ""  This article aims to study intrusion attacks and then develop a novel\ncyberattack detection framework to detect cyberattacks at the network layer\n(e.g., Brute Password and Flooding of Transactions) of blockchain networks.\nSpecifically, we first design and implement a blockchain network in our\nlaboratory. This blockchain network will serve two purposes, i.e., to generate\nthe real traffic data (including both normal data and attack data) for our\nlearning models and to implement real-time experiments to evaluate the\nperformance of our proposed intrusion detection framework. To the best of our\nknowledge, this is the first dataset that is synthesized in a laboratory for\ncyberattacks in a blockchain network. We then propose a novel collaborative\nlearning model that allows efficient deployment in the blockchain network to\ndetect attacks. The main idea of the proposed learning model is to enable\nblockchain nodes to actively collect data, learn the knowledge from data using\nthe Deep Belief Network, and then share the knowledge learned from its data\nwith other blockchain nodes in the network. In this way, we can not only\nleverage the knowledge from all the nodes in the network but also do not need\nto gather all raw data for training at a centralized node like conventional\ncentralized learning solutions. Such a framework can also avoid the risk of\nexposing local data's privacy as well as excessive network overhead/congestion.\nBoth intensive simulations and real-time experiments clearly show that our\nproposed intrusion detection framework can achieve an accuracy of up to 98.6%\nin detecting attacks.\n"", '  Federated Learning (FL) has recently arisen as a revolutionary approach to\ncollaborative training Machine Learning models. According to this novel\nframework, multiple participants train a global model collaboratively,\ncoordinating with a central aggregator without sharing their local data. As FL\ngains popularity in diverse domains, security, and privacy concerns arise due\nto the distributed nature of this solution. Therefore, integrating this\nstrategy with Blockchain technology has been consolidated as a preferred choice\nto ensure the privacy and security of participants.\n  This paper explores the research efforts carried out by the scientific\ncommunity to define privacy solutions in scenarios adopting Blockchain-Enabled\nFL. It comprehensively summarizes the background related to FL and Blockchain,\nevaluates existing architectures for their integration, and the primary attacks\nand possible countermeasures to guarantee privacy in this setting. Finally, it\nreviews the main application scenarios where Blockchain-Enabled FL approaches\nhave been proficiently applied. This survey can help academia and industry\npractitioners understand which theories and techniques exist to improve the\nperformance of FL through Blockchain to preserve privacy and which are the main\nchallenges and future directions in this novel and still under-explored\ncontext. We believe this work provides a novel contribution respect to the\nprevious surveys and is a valuable tool to explore the current landscape,\nunderstand perspectives, and pave the way for advancements or improvements in\nthis amalgamation of Blockchain and Federated Learning.\n']",Blockchain Security and Privacy Solutions,Blockchain Security and Fraud Detection,Machine Learning and Blockchain for Security and Fraud Detection,Machine Learning and Blockchain for Security and Fraud Detection
210,40,210_biases_bias_genderbias_stereotypes,"['biases', 'bias', 'genderbias', 'stereotypes', 'biaspainter', 'stereotype', 'debiasing', 'demographic', 'counterfactual', 'stereotyped']","['gender', 'biases', 'bias', 'race', 'intersectional', 'attributes', 'stereotypes', 'images', 'image', 'occupation']","[""  The recent advancement of large and powerful models with Text-to-Image (T2I)\ngeneration abilities -- such as OpenAI's DALLE-3 and Google's Gemini -- enables\nusers to generate high-quality images from textual prompts. However, it has\nbecome increasingly evident that even simple prompts could cause T2I models to\nexhibit conspicuous social bias in generated images. Such bias might lead to\nboth allocational and representational harms in society, further marginalizing\nminority groups. Noting this problem, a large body of recent works has been\ndedicated to investigating different dimensions of bias in T2I systems.\nHowever, an extensive review of these studies is lacking, hindering a\nsystematic understanding of current progress and research gaps. We present the\nfirst extensive survey on bias in T2I generative models. In this survey, we\nreview prior studies on dimensions of bias: Gender, Skintone, and Geo-Culture.\nSpecifically, we discuss how these works define, evaluate, and mitigate\ndifferent aspects of bias. We found that: (1) while gender and skintone biases\nare widely studied, geo-cultural bias remains under-explored; (2) most works on\ngender and skintone bias investigated occupational association, while other\naspects are less frequently studied; (3) almost all gender bias works overlook\nnon-binary identities in their studies; (4) evaluation datasets and metrics are\nscattered, with no unified framework for measuring biases; and (5) current\nmitigation methods fail to resolve biases comprehensively. Based on current\nlimitations, we point out future research directions that contribute to\nhuman-centric definitions, evaluations, and mitigation of biases. We hope to\nhighlight the importance of studying biases in T2I systems, as well as\nencourage future efforts to holistically understand and tackle biases, building\nfair and trustworthy T2I technologies for everyone.\n"", '  While vision-language models (VLMs) have achieved remarkable performance\nimprovements recently, there is growing evidence that these models also posses\nharmful biases with respect to social attributes such as gender and race. Prior\nstudies have primarily focused on probing such bias attributes individually\nwhile ignoring biases associated with intersections between social attributes.\nThis could be due to the difficulty of collecting an exhaustive set of\nimage-text pairs for various combinations of social attributes. To address this\nchallenge, we employ text-to-image diffusion models to produce counterfactual\nexamples for probing intersectional social biases at scale. Our approach\nutilizes Stable Diffusion with cross attention control to produce sets of\ncounterfactual image-text pairs that are highly similar in their depiction of a\nsubject (e.g., a given occupation) while differing only in their depiction of\nintersectional social attributes (e.g., race & gender). Through our\nover-generate-then-filter methodology, we produce SocialCounterfactuals, a\nhigh-quality dataset containing 171k image-text pairs for probing\nintersectional biases related to gender, race, and physical characteristics. We\nconduct extensive experiments to demonstrate the usefulness of our generated\ndataset for probing and mitigating intersectional social biases in\nstate-of-the-art VLMs.\n', '  Large Vision-Language Models (LVLMs) have been widely adopted in various\napplications; however, they exhibit significant gender biases. Existing\nbenchmarks primarily evaluate gender bias at the demographic group level,\nneglecting individual fairness, which emphasizes equal treatment of similar\nindividuals. This research gap limits the detection of discriminatory\nbehaviors, as individual fairness offers a more granular examination of biases\nthat group fairness may overlook. For the first time, this paper introduces the\nGenderBias-\\emph{VL} benchmark to evaluate occupation-related gender bias in\nLVLMs using counterfactual visual questions under individual fairness criteria.\nTo construct this benchmark, we first utilize text-to-image diffusion models to\ngenerate occupation images and their gender counterfactuals. Subsequently, we\ngenerate corresponding textual occupation options by identifying stereotyped\noccupation pairs with high semantic similarity but opposite gender proportions\nin real-world statistics. This method enables the creation of large-scale\nvisual question counterfactuals to expose biases in LVLMs, applicable in both\nmultimodal and unimodal contexts through modifying gender attributes in\nspecific modalities. Overall, our GenderBias-\\emph{VL} benchmark comprises\n34,581 visual question counterfactual pairs, covering 177 occupations. Using\nour benchmark, we extensively evaluate 15 commonly used open-source LVLMs (\\eg,\nLLaVA) and state-of-the-art commercial APIs, including GPT-4o and Gemini-Pro.\nOur findings reveal widespread gender biases in existing LVLMs. Our benchmark\noffers: (1) a comprehensive dataset for occupation-related gender bias\nevaluation; (2) an up-to-date leaderboard on LVLM biases; and (3) a nuanced\nunderstanding of the biases presented by these models. \\footnote{The dataset\nand code are available at the \\href{https://genderbiasvl.github.io/}{website}.}\n']",Biases in Text-to-Image Generation Models,"Diffusion Models for Text, Image, and Graph Generation",Diffusion Models for Generative Tasks,Diffusion Models for Generative Tasks
211,40,211_kalmannet_kalman_filtering_estimation,"['kalmannet', 'kalman', 'filtering', 'estimation', 'bayesian', 'filter', 'state', 'probabilistic', 'gpssm', 'states']","['uncertainty', 'filtering', 'equations', 'dynamical', 'quantification', 'posterior', 'nonlinear', 'estimation', 'uncertainties', 'physical']","[""  Bayesian filtering serves as the mainstream framework of state estimation in\ndynamic systems. Its standard version utilizes total probability rule and\nBayes' law alternatively, where how to define and compute conditional\nprobability is critical to state distribution inference. Previously, the\nconditional probability is assumed to be exactly known, which represents a\nmeasure of the occurrence probability of one event, given the second event. In\nthis paper, we find that by adding an additional event that stipulates an\ninequality condition, we can transform the conditional probability into a\nspecial integration that is analogous to convolution. Based on this\ntransformation, we show that both transition probability and output probability\ncan be generalized to convolutional forms, resulting in a more general\nfiltering framework that we call convolutional Bayesian filtering. This new\nframework encompasses standard Bayesian filtering as a special case when the\ndistance metric of the inequality condition is selected as Dirac delta\nfunction. It also allows for a more nuanced consideration of model mismatch by\nchoosing different types of inequality conditions. For instance, when the\ndistance metric is defined in a distributional sense, the transition\nprobability and output probability can be approximated by simply rescaling them\ninto fractional powers. Under this framework, a robust version of Kalman filter\ncan be constructed by only altering the noise covariance matrix, while\nmaintaining the conjugate nature of Gaussian distributions. Finally, we\nexemplify the effectiveness of our approach by reshaping classic filtering\nalgorithms into convolutional versions, including Kalman filter, extended\nKalman filter, unscented Kalman filter and particle filter.\n"", ""  The research topic is: data-driven Bayesian state estimation with compressed\nmeasurement (BSCM) of model-free process, say for a (causal) tracking\napplication. The dimension of the temporal measurement vector is lower than the\ndimension of the temporal state vector to be estimated. Hence the state\nestimation problem is an underdetermined inverse problem. The state-space-model\n(SSM) of the underlying dynamical process is assumed to be unknown and hence,\nwe use the terminology 'model-free process'. In absence of the SSM, we can not\nemploy traditional model-driven methods like Kalman Filter (KF) and Particle\nFilter (PF) and instead require data-driven methods. We first experimentally\nshow that two existing unsupervised learning-based data-driven methods fail to\naddress the BSCM problem for model-free process; they are data-driven nonlinear\nstate estimation (DANSE) method and deep Markov model (DMM) method. The\nunsupervised learning uses unlabelled data comprised of only noisy\nmeasurements. While DANSE provides a good predictive performance to model the\ntemporal measurement data as time-series, its unsupervised learning lacks a\nregularization for state estimation. We then investigate use of a\nsemi-supervised learning approach, and develop a semi-supervised learning-based\nDANSE method, referred to as SemiDANSE. In the semi-supervised learning, we use\na limited amount of labelled data along-with a large amount of unlabelled data,\nand that helps to bring the desired regularization for BSCM problem in the\nabsence of SSM. The labelled data means pairwise measurement-and-state data.\nUsing three chaotic dynamical systems (or processes) with nonlinear SSMs as\nbenchmark, we show that the data-driven SemiDANSE provides competitive\nperformance for BSCM against three SSM-informed methods - a hybrid method\ncalled KalmanNet, and two traditional model-driven methods called extended KF\nand unscented KF.\n"", '  The problem of system identification for the Kalman filter, relying on the\nexpectation-maximization (EM) procedure to learn the underlying parameters of a\ndynamical system, has largely been studied assuming that observations are\nsampled at equally-spaced time points. However, in many applications this is a\nrestrictive and unrealistic assumption. This paper addresses system\nidentification for the continuous-discrete filter, with the aim of generalizing\nlearning for the Kalman filter by relying on a solution to a continuous-time\nIt\\^o stochastic differential equation (SDE) for the latent state and\ncovariance dynamics. We introduce a novel two-filter, analytical form for the\nposterior with a Bayesian derivation, which yields analytical updates which do\nnot require the forward-pass to be pre-computed. Using this analytical and\nefficient computation of the posterior, we provide an EM procedure which\nestimates the parameters of the SDE, naturally incorporating irregularly\nsampled measurements. Generalizing the learning of latent linear dynamical\nsystems (LDS) to continuous-time may extend the use of the hybrid Kalman filter\nto data which is not regularly sampled or has intermittent missing values, and\ncan extend the power of non-linear system identification methods such as\nswitching LDS (SLDS), which rely on EM for the linear discrete-time Kalman\nfilter as a sub-unit for learning locally linearized behavior of a non-linear\nsystem. We apply the method by learning the parameters of a latent,\nmultivariate Fokker-Planck SDE representing a toggle-switch genetic circuit\nusing biologically realistic parameters, and compare the efficacy of learning\nrelative to the discrete-time Kalman filter as the step-size irregularity and\nspectral-radius of the dynamics-matrix increases.\n']",Bayesian State Estimation and Filtering,State Estimation and System Identification,Industrial Automation and Control Systems,Industrial Automation and Control Systems
212,40,212_trading_portfolio_portfolios_learning,"['trading', 'portfolio', 'portfolios', 'learning', 'stocks', 'investment', 'finance', 'agents', 'markets', 'arbitrage']","['trading', 'portfolio', 'market', 'hedging', 'financial', 'markets', 'asset', 'currency', 'investment', 'alpha']","['  In recent years, deep or reinforcement learning approaches have been applied\nto optimise investment portfolios through learning the spatial and temporal\ninformation under the dynamic financial market. Yet in most cases, the existing\napproaches may produce biased trading signals based on the conventional price\ndata due to a lot of market noises, which possibly fails to balance the\ninvestment returns and risks. Accordingly, a multi-agent and self-adaptive\nportfolio optimisation framework integrated with attention mechanisms and time\nseries, namely the MASAAT, is proposed in this work in which multiple trading\nagents are created to observe and analyse the price series and directional\nchange data that recognises the significant changes of asset prices at\ndifferent levels of granularity for enhancing the signal-to-noise ratio of\nprice series. Afterwards, by reconstructing the tokens of financial data in a\nsequence, the attention-based cross-sectional analysis module and temporal\nanalysis module of each agent can effectively capture the correlations between\nassets and the dependencies between time points. Besides, a portfolio generator\nis integrated into the proposed framework to fuse the spatial-temporal\ninformation and then summarise the portfolios suggested by all trading agents\nto produce a newly ensemble portfolio for reducing biased trading actions and\nbalancing the overall returns and risks. The experimental results clearly\ndemonstrate that the MASAAT framework achieves impressive enhancement when\ncompared with many well-known portfolio optimsation approaches on three\nchallenging data sets of DJIA, S&P 500 and CSI 300. More importantly, our\nproposal has potential strengths in many possible applications for future\nstudy.\n', '  High-frequency trading (HFT) that executes algorithmic trading in short time\nscales, has recently occupied the majority of cryptocurrency market. Besides\ntraditional quantitative trading methods, reinforcement learning (RL) has\nbecome another appealing approach for HFT due to its terrific ability of\nhandling high-dimensional financial data and solving sophisticated sequential\ndecision-making problems, \\emph{e.g.,} hierarchical reinforcement learning\n(HRL) has shown its promising performance on second-level HFT by training a\nrouter to select only one sub-agent from the agent pool to execute the current\ntransaction. However, existing RL methods for HFT still have some defects: 1)\nstandard RL-based trading agents suffer from the overfitting issue, preventing\nthem from making effective policy adjustments based on financial context; 2)\ndue to the rapid changes in market conditions, investment decisions made by an\nindividual agent are usually one-sided and highly biased, which might lead to\nsignificant loss in extreme markets. To tackle these problems, we propose a\nnovel Memory Augmented Context-aware Reinforcement learning method On HFT,\n\\emph{a.k.a.} MacroHFT, which consists of two training phases: 1) we first\ntrain multiple types of sub-agents with the market data decomposed according to\nvarious financial indicators, specifically market trend and volatility, where\neach agent owns a conditional adapter to adjust its trading policy according to\nmarket conditions; 2) then we train a hyper-agent to mix the decisions from\nthese sub-agents and output a consistently profitable meta-policy to handle\nrapid market fluctuations, equipped with a memory mechanism to enhance the\ncapability of decision-making. Extensive experiments on various cryptocurrency\nmarkets demonstrate that MacroHFT can achieve state-of-the-art performance on\nminute-level trading tasks.\n', '  As a model-free algorithm, deep reinforcement learning (DRL) agent learns and\nmakes decisions by interacting with the environment in an unsupervised way. In\nrecent years, DRL algorithms have been widely applied by scholars for portfolio\noptimization in consecutive trading periods, since the DRL agent can\ndynamically adapt to market changes and does not rely on the specification of\nthe joint dynamics across the assets. However, typical DRL agents for portfolio\noptimization cannot learn a policy that is aware of the dynamic correlation\nbetween portfolio asset returns. Since the dynamic correlations among portfolio\nassets are crucial in optimizing the portfolio, the lack of such knowledge\nmakes it difficult for the DRL agent to maximize the return per unit of risk,\nespecially when the target market permits short selling (i.e., the US stock\nmarket). In this research, we propose a hybrid portfolio optimization model\ncombining the DRL agent and the Black-Litterman (BL) model to enable the DRL\nagent to learn the dynamic correlation between the portfolio asset returns and\nimplement an efficacious long/short strategy based on the correlation.\nEssentially, the DRL agent is trained to learn the policy to apply the BL model\nto determine the target portfolio weights. To test our DRL agent, we construct\nthe portfolio based on all the Dow Jones Industrial Average constitute stocks.\nEmpirical results of the experiments conducted on real-world United States\nstock market data demonstrate that our DRL agent significantly outperforms\nvarious comparison portfolio choice strategies and alternative DRL frameworks\nby at least 42% in terms of accumulated return. In terms of the return per unit\nof risk, our DRL agent significantly outperforms various comparative portfolio\nchoice strategies and alternative strategies based on other machine learning\nframeworks.\n']",Portfolio Optimization with Reinforcement Learning,Reinforcement Learning Applications and Methodologies,Reinforcement Learning,Reinforcement Learning
213,40,213_depressive_depression_tweets_twitter,"['depressive', 'depression', 'tweets', 'twitter', 'cnn', 'nlp', 'depressed', 'facebook', 'suicide', 'lstm']","['depression', 'suicide', 'mental', 'suicidal', 'depressive', 'posts', 'media', 'anxiety', 'health', 'social']","[""  Depression is one of the most common mental disorders affecting an\nindividual's personal and professional life. In this work, we investigated the\npossibility of utilizing social media posts to identify depression in\nindividuals. To achieve this goal, we conducted a preliminary study where we\nextracted and analyzed the top Reddit posts made in 2022 from\ndepression-related forums. The collected data were labeled as depressive and\nnon-depressive using UMLS Metathesaurus. Further, the pre-processed data were\nfed to classical machine learning models, where we achieved an accuracy of\n92.28\\% in predicting the depressive and non-depressive posts.\n"", '  Depression is a widespread mental health issue, affecting an estimated 3.8%\nof the global population. It is also one of the main contributors to disability\nworldwide. Recently it is becoming popular for individuals to use social media\nplatforms (e.g., Reddit) to express their difficulties and health issues (e.g.,\ndepression) and seek support from other users in online communities. It opens\ngreat opportunities to automatically identify social media users with\ndepression by parsing millions of posts for potential interventions. Deep\nlearning methods have begun to dominate in the field of machine learning and\nnatural language processing (NLP) because of their ease of use, efficient\nprocessing, and state-of-the-art results on many NLP tasks. In this work, we\npropose a hybrid deep learning model which combines a pretrained sentence BERT\n(SBERT) and convolutional neural network (CNN) to detect individuals with\ndepression with their Reddit posts. The sentence BERT is used to learn the\nmeaningful representation of semantic information in each post. CNN enables the\nfurther transformation of those embeddings and the temporal identification of\nbehavioral patterns of users. We trained and evaluated the model performance to\nidentify Reddit users with depression by utilizing the Self-reported Mental\nHealth Diagnoses (SMHD) data. The hybrid deep learning model achieved an\naccuracy of 0.86 and an F1 score of 0.86 and outperformed the state-of-the-art\ndocumented result (F1 score of 0.79) by other machine learning models in the\nliterature. The results show the feasibility of the hybrid model to identify\nindividuals with depression. Although the hybrid model is validated to detect\ndepression with Reddit posts, it can be easily tuned and applied to other text\nclassification tasks and different clinical applications.\n', ""  The COVID-19 pandemic has escalated mental health crises worldwide, with\nsocial isolation and economic instability contributing to a rise in suicidal\nbehavior. Suicide can result from social factors such as shame, abuse,\nabandonment, and mental health conditions like depression, Post-Traumatic\nStress Disorder (PTSD), Attention-Deficit/Hyperactivity Disorder (ADHD),\nanxiety disorders, and bipolar disorders. As these conditions develop, signs of\nsuicidal ideation may manifest in social media interactions. Analyzing social\nmedia data using artificial intelligence (AI) techniques can help identify\npatterns of suicidal behavior, providing invaluable insights for suicide\nprevention agencies, professionals, and broader community awareness\ninitiatives. Machine learning algorithms for this purpose require large volumes\nof accurately labeled data. Previous research has not fully explored the\npotential of incorporating explanations in analyzing and labeling longitudinal\nsocial media data. In this study, we employed a model explanation method, Layer\nIntegrated Gradients, on top of a fine-tuned state-of-the-art language model,\nto assign each token from Reddit users' posts an attribution score for\npredicting suicidal ideation. By extracting and analyzing attributions of\ntokens from the data, we propose a methodology for preliminary screening of\nsocial media posts for suicidal ideation without using large language models\nduring inference.\n""]",Social Media Analysis for Depression Detection,Depression Detection using AI and Multimodal Data,Human Behavior Analysis with AI and Multimodal Data,Human Behavior Analysis with AI and Multimodal Data
214,39,214_homomorphic_homomorphically_privacy_encryption,"['homomorphic', 'homomorphically', 'privacy', 'encryption', 'cryptographic', 'cryptonets', 'encrypted', 'confidentiality', 'confidential', 'secure']","['homomorphic', 'encryption', 'encrypted', 'privacy', 'secure', 'preserving', 'computation', 'party', 'inference', 'times']","[""  Transfer learning is a de facto standard method for efficiently training\nmachine learning models for data-scarce problems by adding and fine-tuning new\nclassification layers to a model pre-trained on large datasets. Although\nnumerous previous studies proposed to use homomorphic encryption to resolve the\ndata privacy issue in transfer learning in the machine learning as a service\nsetting, most of them only focused on encrypted inference. In this study, we\npresent HETAL, an efficient Homomorphic Encryption based Transfer Learning\nalgorithm, that protects the client's privacy in training tasks by encrypting\nthe client data using the CKKS homomorphic encryption scheme. HETAL is the\nfirst practical scheme that strictly provides encrypted training, adopting\nvalidation-based early stopping and achieving the accuracy of nonencrypted\ntraining. We propose an efficient encrypted matrix multiplication algorithm,\nwhich is 1.8 to 323 times faster than prior methods, and a highly precise\nsoftmax approximation algorithm with increased coverage. The experimental\nresults for five well-known benchmark datasets show total training times of\n567-3442 seconds, which is less than an hour.\n"", ""  In this paper, we introduce a privacy-preserving stable diffusion framework\nleveraging homomorphic encryption, called HE-Diffusion, which primarily focuses\non protecting the denoising phase of the diffusion process. HE-Diffusion is a\ntailored encryption framework specifically designed to align with the unique\narchitecture of stable diffusion, ensuring both privacy and functionality. To\naddress the inherent computational challenges, we propose a novel\nmin-distortion method that enables efficient partial image encryption,\nsignificantly reducing the overhead without compromising the model's output\nquality. Furthermore, we adopt a sparse tensor representation to expedite\ncomputational operations, enhancing the overall efficiency of the\nprivacy-preserving diffusion process. We successfully implement HE-based\nprivacy-preserving stable diffusion inference. The experimental results show\nthat HE-Diffusion achieves 500 times speedup compared with the baseline method,\nand reduces time cost of the homomorphically encrypted inference to the minute\nlevel. Both the performance and accuracy of the HE-Diffusion are on par with\nthe plaintext counterpart. Our approach marks a significant step towards\nintegrating advanced cryptographic techniques with state-of-the-art generative\nmodels, paving the way for privacy-preserving and efficient image generation in\ncritical applications.\n"", '  Advancements in machine learning (ML) have significantly revolutionized\nmedical image analysis, prompting hospitals to rely on external ML services.\nHowever, the exchange of sensitive patient data, such as chest X-rays, poses\ninherent privacy risks when shared with third parties. Addressing this concern,\nwe propose MedBlindTuner, a privacy-preserving framework leveraging fully\nhomomorphic encryption (FHE) and a data-efficient image transformer (DEiT).\nMedBlindTuner enables the training of ML models exclusively on FHE-encrypted\nmedical images. Our experimental evaluation demonstrates that MedBlindTuner\nachieves comparable accuracy to models trained on non-encrypted images,\noffering a secure solution for outsourcing ML computations while preserving\npatient data privacy. To the best of our knowledge, this is the first work that\nuses data-efficient image transformers and fully homomorphic encryption in this\ndomain.\n']",Homomorphic Encryption for Privacy-Preserving Machine Learning,Machine Learning for Data Privacy and Security,Machine Learning and Data Privacy,Machine Learning and Data Privacy
215,39,215_parallelization_unifiednn_distributed_throughput,"['parallelization', 'unifiednn', 'distributed', 'throughput', 'bottleneck', 'cloud', 'streaming', 'memory', 'network', 'deep']","['compression', 'communication', 'bandwidth', 'topologies', 'training', 'devices', 'scale', 'parallel', 'synchronization', 'cluster']","['  With rapidly increasing distributed deep learning workloads in large-scale\ndata centers, efficient distributed deep learning framework strategies for\nresource allocation and workload scheduling have become the key to\nhigh-performance deep learning. The large-scale environment with large volumes\nof datasets, models, and computational and communication resources raises\nvarious unique challenges for resource allocation and workload scheduling in\ndistributed deep learning, such as scheduling complexity, resource and workload\nheterogeneity, and fault tolerance. To uncover these challenges and\ncorresponding solutions, this survey reviews the literature, mainly from 2019\nto 2024, on efficient resource allocation and workload scheduling strategies\nfor large-scale distributed DL. We explore these strategies by focusing on\nvarious resource types, scheduling granularity levels, and performance goals\nduring distributed training and inference processes. We highlight critical\nchallenges for each topic and discuss key insights of existing technologies. To\nillustrate practical large-scale resource allocation and workload scheduling in\nreal distributed deep learning scenarios, we use a case study of training large\nlanguage models. This survey aims to encourage computer science, artificial\nintelligence, and communications researchers to understand recent advances and\nexplore future research directions for efficient framework strategies for\nlarge-scale distributed deep learning.\n', '  The past few years have witnessed the flourishing of large-scale deep neural\nnetwork models with ever-growing parameter numbers. Training such large-scale\nmodels typically requires massive memory and computing resources that exceed\nthose of a single GPU, necessitating distributed training. As GPU performance\nhas rapidly evolved in recent years, computation time has shrunk, thereby\nincreasing the proportion of communication in the overall training time.\nTherefore, optimizing communication for distributed training has become an\nurgent issue. In this article, we briefly introduce the general architecture of\ndistributed deep neural network training and analyze relationships among\nParallelization Strategy, Collective Communication Library, and Network from\nthe perspective of communication optimization, which forms a three-layer\nparadigm. We then review current representative research advances with this\nthree-layer paradigm. We find that layers in the current three-layer paradigm\nare relatively independent, but there is a rich design space for cross-layer\ncollaborative optimization in distributed training scenarios. Therefore, we\nfurther advocate a communication-efficient five-layer paradigm underlining\nopportunities for collaboration designs and look forward to the perspectives of\n""Vertical"", ""Horizontal"", ""Intra-Inter"" and ""Host-Net"" collaboration designs.\nWe hope this article can shed some light on future research on communication\noptimization for distributed training.\n', '  With the rapid growth in the volume of data sets, models, and devices in the\ndomain of deep learning, there is increasing attention on large-scale\ndistributed deep learning. In contrast to traditional distributed deep\nlearning, the large-scale scenario poses new challenges that include fault\ntolerance, scalability of algorithms and infrastructures, and heterogeneity in\ndata sets, models, and resources. Due to intensive synchronization of models\nand sharing of data across GPUs and computing nodes during distributed training\nand inference processes, communication efficiency becomes the bottleneck for\nachieving high performance at a large scale. This article surveys the\nliterature over the period of 2018-2023 on algorithms and technologies aimed at\nachieving efficient communication in large-scale distributed deep learning at\nvarious levels, including algorithms, frameworks, and infrastructures.\nSpecifically, we first introduce efficient algorithms for model synchronization\nand communication data compression in the context of large-scale distributed\ntraining. Next, we introduce efficient strategies related to resource\nallocation and task scheduling for use in distributed training and inference.\nAfter that, we present the latest technologies pertaining to modern\ncommunication infrastructures used in distributed deep learning with a focus on\nexamining the impact of the communication overhead in a large-scale and\nheterogeneous setting. Finally, we conduct a case study on the distributed\ntraining of large language models at a large scale to illustrate how to apply\nthese technologies in real cases. This article aims to offer researchers a\ncomprehensive understanding of the current landscape of large-scale distributed\ndeep learning and to reveal promising future research directions toward\ncommunication-efficient solutions in this scope.\n']",Distributed Deep Learning Optimization,Optimization Methods for Distributed Deep Learning,Deep Learning Optimization and Training,Deep Learning Optimization and Security
216,39,216_attention_scene_visual_memory,"['attention', 'scene', 'visual', 'memory', 'semantic', 'mpnn', 'features', 'videos', 'objects', 'segmentation']","['video', 'object', 'frames', 'segmentation', 'predicate', 'objects', 'videos', 'scene', 'panoptic', 'temporal']","['  The scene graph generation (SGG) task involves detecting objects within an\nimage and predicting predicates that represent the relationships between the\nobjects. However, in SGG benchmark datasets, each subject-object pair is\nannotated with a single predicate even though a single predicate may exhibit\ndiverse semantics (i.e., semantic diversity), existing SGG models are trained\nto predict the one and only predicate for each pair. This in turn results in\nthe SGG models to overlook the semantic diversity that may exist in a\npredicate, thus leading to biased predictions. In this paper, we propose a\nnovel model-agnostic Semantic Diversity-aware Prototype-based Learning (DPL)\nframework that enables unbiased predictions based on the understanding of the\nsemantic diversity of predicates. Specifically, DPL learns the regions in the\nsemantic space covered by each predicate to distinguish among the various\ndifferent semantics that a single predicate can represent. Extensive\nexperiments demonstrate that our proposed model-agnostic DPL framework brings\nsignificant performance improvement on existing SGG models, and also\neffectively understands the semantic diversity of predicates.\n', ""  Scene Graph Generation (SGG) endeavors to predict the relationships between\nsubjects and objects in a given image. Nevertheless, the long-tail distribution\nof relations often leads to biased prediction on coarse labels, presenting a\nsubstantial hurdle in SGG. To address this issue, researchers focus on unbiased\nSGG and introduce data transfer methods to transfer coarse-grained predicates\ninto fine-grained ones across the entire dataset. However, these methods\nencounter two primary challenges: 1) They overlook the inherent context\nconstraints imposed by subject-object pairs, leading to erroneous relations\ntransfer. 2) Additional retraining process are required after the data\ntransfer, which incurs substantial computational costs. To overcome these\nlimitations, we introduce the first plug-and-play one-stage data transfer\npipeline in SGG, termed Adaptive Label Finetuning (ALF), which eliminates the\nneed for extra retraining sessions and meanwhile significantly enhance models'\nrelation recognition capability across various SGG benchmark approaches.\nSpecifically, ALF consists of two components: Adaptive Label Construction (ALC)\nand Adaptive Iterative Learning (AIL). By imposing Predicate-Context\nConstraints within relation space, ALC adaptively re-ranks and selects\ncandidate relations in reference to model's predictive logits utilizing the\nRestriction-Based Judgment techniques, achieving robust relation transfer.\nSupervised with labels transferred by ALC, AIL iteratively finetunes the SGG\nmodels in an auto-regressive manner, which mitigates the substantial\ncomputational costs arising from the retraining process. Extensive experiments\ndemonstrate that ALF achieves a 16% improvement in mR@100 compared to the\ntypical SGG method Motif, with only a 6% increase in calculation costs compared\nto the state-of-the-art method IETrans.\n"", '  Scene graph generation (SGG) models have suffered from inherent problems\nregarding the benchmark datasets such as the long-tailed predicate distribution\nand missing annotation problems. In this work, we aim to alleviate the\nlong-tailed problem of SGG by utilizing unannotated triplets. To this end, we\nintroduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels\nfor unannotated triplets based on which the SGG models are trained. While there\nhas been significant progress in self-training for image recognition, designing\na self-training framework for the SGG task is more challenging due to its\ninherent nature such as the semantic ambiguity and the long-tailed distribution\nof predicate classes. Hence, we propose a novel pseudo-labeling technique for\nSGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is\na model-agnostic framework that can be applied to any existing SGG models.\nFurthermore, we devise a graph structure learner (GSL) that is beneficial when\nadopting our proposed self-training framework to the state-of-the-art\nmessage-passing neural network (MPNN)-based SGG models. Our extensive\nexperiments verify the effectiveness of ST-SGG on various SGG models,\nparticularly in enhancing the performance on fine-grained predicate classes.\n']",Scene Graph Generation with Semantic Diversity,Computer-Generated 3D Content Creation,Artificial Intelligence for Creative Content Generation,Artificial Intelligence for Creative Content Generation
217,39,217_ssl_ssl4ns_supervised_imagenet,"['ssl', 'ssl4ns', 'supervised', 'imagenet', 'learning', 'memorization', 'ssfa', 'encoders', 'learned', 'regularization']","['supervised', 'unlabeled', 'self', 'augmentations', 'projector', 'contrastive', 'spurious', 'views', 'series', 'downstream']","[""  In recent years, the rise of generative self-supervised learning (SSL)\nparadigms has exhibited impressive performance across visual, language, and\nmulti-modal domains. While the varied designs of generative SSL objectives lead\nto distinct properties in downstream tasks, a theoretical understanding of\nthese differences remains largely unexplored. In this paper, we establish the\nfirst theoretical comparisons between two leading generative SSL paradigms:\nautoregressive SSL and masked SSL. Through establishing theoretical frameworks,\nwe elucidate the strengths and limitations of autoregressive and masked SSL\nwithin the primary evaluation tasks of classification and content generation.\nOur findings demonstrate that in classification tasks, the flexibility of\ntargeted tokens in masked SSL fosters more inter-sample connections compared to\nthe fixed position of target tokens in autoregressive SSL, which yields\nsuperior clustering performance. In content generation tasks, the misalignment\nbetween the flexible lengths of test samples and the fixed length of unmasked\ntexts in masked SSL (vs. flexible lengths of conditional texts in\nautoregressive SSL) hinders its generation performance. To leverage each\nother's strengths and mitigate weaknesses, we propose diversity-enhanced\nautoregressive and variable-length masked objectives, which substantially\nimprove the classification performance of autoregressive SSL and the generation\nperformance of masked SSL. Code is available at\nhttps://github.com/PKU-ML/LookAheadLookAround.\n"", '  Deep supervised learning algorithms typically require a large volume of\nlabeled data to achieve satisfactory performance. However, the process of\ncollecting and labeling such data can be expensive and time-consuming.\nSelf-supervised learning (SSL), a subset of unsupervised learning, aims to\nlearn discriminative features from unlabeled data without relying on\nhuman-annotated labels. SSL has garnered significant attention recently,\nleading to the development of numerous related algorithms. However, there is a\ndearth of comprehensive studies that elucidate the connections and evolution of\ndifferent SSL variants. This paper presents a review of diverse SSL methods,\nencompassing algorithmic aspects, application domains, three key trends, and\nopen research questions. Firstly, we provide a detailed introduction to the\nmotivations behind most SSL algorithms and compare their commonalities and\ndifferences. Secondly, we explore representative applications of SSL in domains\nsuch as image processing, computer vision, and natural language processing.\nLastly, we discuss the three primary trends observed in SSL research and\nhighlight the open questions that remain. A curated collection of valuable\nresources can be accessed at https://github.com/guijiejie/SSL.\n', '  Self-supervised learning (SSL) has recently achieved impressive performance\non various time series tasks. The most prominent advantage of SSL is that it\nreduces the dependence on labeled data. Based on the pre-training and\nfine-tuning strategy, even a small amount of labeled data can achieve high\nperformance. Compared with many published self-supervised surveys on computer\nvision and natural language processing, a comprehensive survey for time series\nSSL is still missing. To fill this gap, we review current state-of-the-art SSL\nmethods for time series data in this article. To this end, we first\ncomprehensively review existing surveys related to SSL and time series, and\nthen provide a new taxonomy of existing time series SSL methods by summarizing\nthem from three perspectives: generative-based, contrastive-based, and\nadversarial-based. These methods are further divided into ten subcategories\nwith detailed reviews and discussions about their key intuitions, main\nframeworks, advantages and disadvantages. To facilitate the experiments and\nvalidation of time series SSL methods, we also summarize datasets commonly used\nin time series forecasting, classification, anomaly detection, and clustering\ntasks. Finally, we present the future directions of SSL for time series\nanalysis.\n']",Self-Supervised Learning (SSL) Paradigms,Self-Supervised Learning and Representation Learning,Self-Supervised Representation Learning,Self-Supervised Representation Learning
218,39,218_sarcasm_sarcastic_emoticons_sentiment,"['sarcasm', 'sarcastic', 'emoticons', 'sentiment', 'humor', 'irony', 'multimodal', 'emojis', 'text', 'modality']","['sarcasm', 'sentiment', 'multimodal', 'modalities', 'irony', 'modality', 'modal', 'sentiments', 'analysis', 'fusion']","['  Sarcasm is a way of verbal irony where someone says the opposite of what they\nmean, often to ridicule a person, situation, or idea. It is often difficult to\ndetect sarcasm in the dialogue since detecting sarcasm should reflect the\ncontext (i.e., dialogue history). In this paper, we introduce a new dataset for\nthe Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware\nSarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and\nthe labels for this task on the last response. To build the dataset, we propose\nan efficient sarcasm detection dataset generation pipeline: 1) generating new\nsarcastic dialogues from source dialogues with large language models, 2)\nautomatic and manual filtering of abnormal and toxic dialogues, and 3) human\nannotation for the sarcasm detection task. We also provide a simple but\neffective baseline for the Korean sarcasm detection task trained on our\ndataset. Experimental results on the dataset show that our baseline system\noutperforms strong baselines like large language models, such as GPT-3.5, in\nthe Korean sarcasm detection task. We show that the sarcasm detection task\nrelies deeply on the existence of sufficient context. We will release the\ndataset at https://github.com/Yu-billie/KoCoSa_sarcasm_detection.\n', '  Sarcasm is a type of irony, characterized by an inherent mismatch between the\nliteral interpretation and the intended connotation. Though sarcasm detection\nin text has been extensively studied, there are situations in which textual\ninput alone might be insufficient to perceive sarcasm. The inclusion of\nadditional contextual cues, such as images, is essential to recognize sarcasm\nin social media data effectively. This study presents a novel framework for\nmultimodal sarcasm detection that can process input triplets. Two components of\nthese triplets comprise the input text and its associated image, as provided in\nthe datasets. Additionally, a supplementary modality is introduced in the form\nof descriptive image captions. The motivation behind incorporating this visual\nsemantic representation is to more accurately capture the discrepancies between\nthe textual and visual content, which are fundamental to the sarcasm detection\ntask. The primary contributions of this study are: (1) a robust textual feature\nextraction branch that utilizes a cross-lingual language model; (2) a visual\nfeature extraction branch that incorporates a self-regulated residual ConvNet\nintegrated with a lightweight spatially aware attention module; (3) an\nadditional modality in the form of image captions generated using an\nencoder-decoder architecture capable of reading text embedded in images; (4)\ndistinct attention modules to effectively identify the incongruities between\nthe text and two levels of image representations; (5) multi-level cross-domain\nsemantic incongruity representation achieved through feature fusion. Compared\nwith cutting-edge baselines, the proposed model achieves the best accuracy of\n92.89% and 64.48%, respectively, on the Twitter multimodal sarcasm and\nMultiBully datasets.\n', '  Social media abounds with multimodal sarcasm, and identifying sarcasm targets\nis particularly challenging due to the implicit incongruity not directly\nevident in the text and image modalities. Current methods for Multimodal\nSarcasm Target Identification (MSTI) predominantly focus on superficial\nindicators in an end-to-end manner, overlooking the nuanced understanding of\nmultimodal sarcasm conveyed through both the text and image. This paper\nproposes a versatile MSTI framework with a coarse-to-fine paradigm, by\naugmenting sarcasm explainability with reasoning and pre-training knowledge.\nInspired by the powerful capacity of Large Multimodal Models (LMMs) on\nmultimodal reasoning, we first engage LMMs to generate competing rationales for\ncoarser-grained pre-training of a small language model on multimodal sarcasm\ndetection. We then propose fine-tuning the model for finer-grained sarcasm\ntarget identification. Our framework is thus empowered to adeptly unveil the\nintricate targets within multimodal sarcasm and mitigate the negative impact\nposed by potential noise inherently in LMMs. Experimental results demonstrate\nthat our model far outperforms state-of-the-art MSTI methods, and markedly\nexhibits explainability in deciphering sarcasm as well.\n']",Multimodal Sarcasm Detection,Humor and Sarcasm in Natural Language Processing,Humor and Sarcasm in Natural Language Processing,Humor and Sarcasm in Natural Language Processing
219,39,219_imputations_imputation_missingness_imputing,"['imputations', 'imputation', 'missingness', 'imputing', 'imputed', 'completion', 'missforestpredict', 'datasets', 'imputes', 'impute']","['imputation', 'missing', 'values', 'missingness', 'impute', 'mice', 'incomplete', 'data', 'categorical', 'pipelines']","['  Missing data is a common problem in practical settings. Various imputation\nmethods have been developed to deal with missing data. However, even though the\nlabel is usually available in the training data, the common practice of\nimputation usually only relies on the input and ignores the label. In this\nwork, we illustrate how stacking the label into the input can significantly\nimprove the imputation of the input. In addition, we propose a classification\nstrategy that initializes the predicted test label with missing values and\nstacks the label with the input for imputation. This allows imputing the label\nand the input at the same time. Also, the technique is capable of handling data\ntraining with missing labels without any prior imputation and is applicable to\ncontinuous, categorical, or mixed-type data. Experiments show promising results\nin terms of accuracy.\n', '  Many datasets suffer from missing values due to various reasons,which not\nonly increases the processing difficulty of related tasks but also reduces the\naccuracy of classification. To address this problem, the mainstream approach is\nto use missing value imputation to complete the dataset. Existing imputation\nmethods estimate the missing parts based on the observed values in the original\nfeature space, and they treat all features as equally important during data\ncompletion, while in fact different features have different importance.\nTherefore, we have designed an imputation method that considers feature\nimportance. This algorithm iteratively performs matrix completion and feature\nimportance learning, and specifically, matrix completion is based on a filling\nloss that incorporates feature importance. Our experimental analysis involves\nthree types of datasets: synthetic datasets with different noisy features and\nmissing values, real-world datasets with artificially generated missing values,\nand real-world datasets originally containing missing values. The results on\nthese datasets consistently show that the proposed method outperforms the\nexisting five imputation algorithms.To the best of our knowledge, this is the\nfirst work that considers feature importance in the imputation model.\n', '  Missing values or data is one popular characteristic of real-world datasets,\nespecially healthcare data. This could be frustrating when using machine\nlearning algorithms on such datasets, simply because most machine learning\nmodels perform poorly in the presence of missing values. The aim of this study\nis to compare the performance of seven imputation techniques, namely Mean\nimputation, Median Imputation, Last Observation carried Forward (LOCF)\nimputation, K-Nearest Neighbor (KNN) imputation, Interpolation imputation,\nMissforest imputation, and Multiple imputation by Chained Equations (MICE), on\nthree healthcare datasets. Some percentage of missing values - 10\\%, 15\\%, 20\\%\nand 25\\% - were introduced into the dataset, and the imputation techniques were\nemployed to impute these missing values. The comparison of their performance\nwas evaluated by using root mean squared error (RMSE) and mean absolute error\n(MAE). The results show that Missforest imputation performs the best followed\nby MICE imputation. Additionally, we try to determine whether it is better to\nperform feature selection before imputation or vice versa by using the\nfollowing metrics - the recall, precision, f1-score and accuracy. Due to the\nfact that there are few literature on this and some debate on the subject among\nresearchers, we hope that the results from this experiment will encourage data\nscientists and researchers to perform imputation first before feature selection\nwhen dealing with data containing missing values.\n']",Missing Data Imputation Methods and Techniques,Data Imputation and Missing Value Analysis,Handling Missing or Inconsistent Data,Handling Missing or Inconsistent Data
220,38,220_chartmimic_charts_visualizations_visual,"['chartmimic', 'charts', 'visualizations', 'visual', 'chartformer', 'chart', 'chartcheck', 'visualization', 'chartqa', 'flowcharts']","['chart', 'charts', 'visualization', 'flowcharts', 'visualizations', 'visual', 'question', 'plots', 'understanding', 'natural']","[""  Recent studies customizing Multimodal Large Language Models (MLLMs) for\ndomain-specific tasks have yielded promising results, especially in the field\nof scientific chart comprehension. These studies generally utilize visual\ninstruction tuning with specialized datasets to enhance question and answer\n(QA) accuracy within the chart domain. However, they often neglect the\nfundamental discrepancy between natural image-caption pre-training data and\ndigital chart image-QA data, particularly in the models' capacity to extract\nunderlying numeric values from charts. This paper tackles this oversight by\nexploring the training processes necessary to improve MLLMs' comprehension of\ncharts. We present three key findings: (1) Incorporating raw data values in\nalignment pre-training markedly improves comprehension of chart data. (2)\nReplacing images with their textual representation randomly during end-to-end\nfine-tuning transfer the language reasoning capability to chart interpretation\nskills. (3) Requiring the model to first extract the underlying chart data and\nthen answer the question in the fine-tuning can further improve the accuracy.\nConsequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart\ncomprehension. CHOPINLLM effectively interprets various types of charts,\nincluding unannotated ones, while maintaining robust reasoning abilities.\nFurthermore, we establish a new benchmark to evaluate MLLMs' understanding of\ndifferent chart types across various comprehension levels. Experimental results\nshow that CHOPINLLM exhibits strong performance in understanding both annotated\nand unannotated charts across a wide range of types.\n"", '  Data visualization in the form of charts plays a pivotal role in data\nanalysis, offering critical insights and aiding in informed decision-making.\nAutomatic chart understanding has witnessed significant advancements with the\nrise of large foundation models in recent years. Foundation models, such as\nlarge language models, have revolutionized various natural language processing\ntasks and are increasingly being applied to chart understanding tasks. This\nsurvey paper provides a comprehensive overview of the recent developments,\nchallenges, and future directions in chart understanding within the context of\nthese foundation models. We review fundamental building blocks crucial for\nstudying chart understanding tasks. Additionally, we explore various tasks and\ntheir evaluation metrics and sources of both charts and textual inputs. Various\nmodeling strategies are then examined, encompassing both classification-based\nand generation-based approaches, along with tool augmentation techniques that\nenhance chart understanding performance. Furthermore, we discuss the\nstate-of-the-art performance of each task and discuss how we can improve the\nperformance. Challenges and future directions are addressed, highlighting the\nimportance of several topics, such as domain-specific charts, lack of efforts\nin developing evaluation metrics, and agent-oriented settings. This survey\npaper serves as a comprehensive resource for researchers and practitioners in\nthe fields of natural language processing, computer vision, and data analysis,\nproviding valuable insights and directions for future research in chart\nunderstanding leveraging large foundation models. The studies mentioned in this\npaper, along with emerging new research, will be continually updated at:\nhttps://github.com/khuangaf/Awesome-Chart-Understanding.\n', ""  Natural language is a powerful complementary modality of communication for\ndata visualizations, such as bar and line charts. To facilitate chart-based\nreasoning using natural language, various downstream tasks have been introduced\nrecently such as chart question answering, chart summarization, and\nfact-checking with charts. These tasks pose a unique challenge, demanding both\nvision-language reasoning and a nuanced understanding of chart data tables,\nvisual encodings, and natural language prompts. Despite the recent success of\nLarge Language Models (LLMs) across diverse NLP tasks, their abilities and\nlimitations in the realm of data visualization remain under-explored, possibly\ndue to their lack of multi-modal capabilities. To bridge the gap, this paper\npresents the first comprehensive evaluation of the recently developed large\nvision language models (LVLMs) for chart understanding and reasoning tasks. Our\nevaluation includes a comprehensive assessment of LVLMs, including GPT-4V and\nGemini, across four major chart reasoning tasks. Furthermore, we perform a\nqualitative evaluation of LVLMs' performance on a diverse range of charts,\naiming to provide a thorough analysis of their strengths and weaknesses. Our\nfindings reveal that LVLMs demonstrate impressive abilities in generating\nfluent texts covering high-level data insights while also encountering common\nproblems like hallucinations, factual errors, and data bias. We highlight the\nkey strengths and limitations of chart comprehension tasks, offering insights\nfor future research.\n""]",Chart Understanding and Reasoning,Data Visualization and Chart Understanding,Data Analysis and Visualization,Data Analysis and Visualization
221,38,221_survival_predicting_prediction_predict,"['survival', 'predicting', 'prediction', 'predict', 'coxtime', 'predictive', 'cox', 'hazards', 'outcomes', 'survmixclust']","['survival', 'event', 'censoring', 'death', 'hazards', 'analysis', 'time', 'index', 'proportional', 'markers']","['  Kernel survival analysis models estimate individual survival distributions\nwith the help of a kernel function, which measures the similarity between any\ntwo data points. Such a kernel function can be learned using deep kernel\nsurvival models. In this paper, we present a new deep kernel survival model\ncalled a survival kernet, which scales to large datasets in a manner that is\namenable to model interpretation and also theoretical analysis. Specifically,\nthe training data are partitioned into clusters based on a recently developed\ntraining set compression scheme for classification and regression called kernel\nnetting that we extend to the survival analysis setting. At test time, each\ndata point is represented as a weighted combination of these clusters, and each\nsuch cluster can be visualized. For a special case of survival kernets, we\nestablish a finite-sample error bound on predicted survival distributions that\nis, up to a log factor, optimal. Whereas scalability at test time is achieved\nusing the aforementioned kernel netting compression strategy, scalability\nduring training is achieved by a warm-start procedure based on tree ensembles\nsuch as XGBoost and a heuristic approach to accelerating neural architecture\nsearch. On four standard survival analysis datasets of varying sizes (up to\nroughly 3 million data points), we show that survival kernets are highly\ncompetitive compared to various baselines tested in terms of time-dependent\nconcordance index. Our code is available at:\nhttps://github.com/georgehc/survival-kernets\n', ""  Scoring systems are highly interpretable and widely used to evaluate\ntime-to-event outcomes in healthcare research. However, existing time-to-event\nscores are predominantly created ad-hoc using a few manually selected variables\nbased on clinician's knowledge, suggesting an unmet need for a robust and\nefficient generic score-generating method.\n  AutoScore was previously developed as an interpretable machine learning score\ngenerator, integrated both machine learning and point-based scores in the\nstrong discriminability and accessibility. We have further extended it to\ntime-to-event data and developed AutoScore-Survival, for automatically\ngenerating time-to-event scores with right-censored survival data. Random\nsurvival forest provides an efficient solution for selecting variables, and Cox\nregression was used for score weighting. We illustrated our method in a\nreal-life study of 90-day mortality of patients in intensive care units and\ncompared its performance with survival models (i.e., Cox) and the random\nsurvival forest.\n  The AutoScore-Survival-derived scoring model was more parsimonious than\nsurvival models built using traditional variable selection methods (e.g.,\npenalized likelihood approach and stepwise variable selection), and its\nperformance was comparable to survival models using the same set of variables.\nAlthough AutoScore-Survival achieved a comparable integrated area under the\ncurve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores\ngenerated are favorable in clinical applications because they are easier to\ncompute and interpret.\n  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use\nmachine learning-based clinical score generator to studies of time-to-event\noutcomes. It provides a systematic guideline to facilitate the future\ndevelopment of time-to-event scores for clinical applications.\n"", '  Survival analysis stands as a pivotal process in cancer treatment research,\ncrucial for predicting patient survival rates accurately. Recent advancements\nin data collection techniques have paved the way for enhancing survival\npredictions by integrating information from multiple modalities. However,\nreal-world scenarios often present challenges with incomplete data,\nparticularly when dealing with censored survival labels. Prior works have\naddressed missing modalities but have overlooked incomplete labels, which can\nintroduce bias and limit model efficacy. To bridge this gap, we introduce a\nnovel framework that simultaneously handles incomplete data across modalities\nand censored survival labels. Our approach employs advanced foundation models\nto encode individual modalities and align them into a universal representation\nspace for seamless fusion. By generating pseudo labels and incorporating\nuncertainty, we significantly enhance predictive accuracy. The proposed method\ndemonstrates outstanding prediction accuracy in two survival analysis tasks on\nboth employed datasets. This innovative approach overcomes limitations\nassociated with disparate modalities and improves the feasibility of\ncomprehensive survival analysis using multiple large foundation models.\n']",Survival Analysis and Prediction Models,Predictive Modeling for Time-to-Event Outcomes and Battery Health,Predictive Modeling and Forecasting,Predictive Modeling and Forecasting
222,38,222_gans_gan_generative_cyclegan,"['gans', 'gan', 'generative', 'cyclegan', 'imaging', 'mri', 'echogan', 'segmentation', 'tomography', 'adversarial']","['medical', 'segmentation', 'images', 'ultrasound', 'imaging', 'image', 'synthetic', 'diffusion', 'generative', 'scans']","['  Large annotated datasets are required for training deep learning models, but\nin medical imaging data sharing is often complicated due to ethics,\nanonymization and data protection legislation. Generative AI models, such as\ngenerative adversarial networks (GANs) and diffusion models, can today produce\nvery realistic synthetic images, and can potentially facilitate data sharing.\nHowever, in order to share synthetic medical images it must first be\ndemonstrated that they can be used for training different networks with\nacceptable performance. Here, we therefore comprehensively evaluate four GANs\n(progressive GAN, StyleGAN 1-3) and a diffusion model for the task of brain\ntumor segmentation (using two segmentation networks, U-Net and a Swin\ntransformer). Our results show that segmentation networks trained on synthetic\nimages reach Dice scores that are 80% - 90% of Dice scores when training with\nreal images, but that memorization of the training images can be a problem for\ndiffusion models if the original dataset is too small. Our conclusion is that\nsharing synthetic medical images is a viable option to sharing real images, but\nthat further work is required. The trained generative models and the generated\nsynthetic images are shared on AIDA data hub\n', '  Efficient and accurate brain ventricle segmentation from clinical CT scans is\ncritical for emergency surgeries like ventriculostomy. With the challenges in\npoor soft tissue contrast and a scarcity of well-annotated databases for\nclinical brain CTs, we introduce a novel uncertainty-aware ventricle\nsegmentation technique without the need of CT segmentation ground truths by\nleveraging diffusion-model-based domain adaptation. Specifically, our method\nemploys the diffusion Schr\\""odinger Bridge and an attention recurrent residual\nU-Net to capitalize on unpaired CT and MRI scans to derive automatic CT\nsegmentation from those of the MRIs, which are more accessible. Importantly, we\npropose an end-to-end, joint training framework of image translation and\nsegmentation tasks, and demonstrate its benefit over training individual tasks\nseparately. By comparing the proposed method against similar setups using two\ndifferent GAN models for domain adaptation (CycleGAN and CUT), we also reveal\nthe advantage of diffusion models towards improved segmentation and image\ntranslation quality. With a Dice score of 0.78$\\pm$0.27, our proposed method\noutperformed the compared methods, including SynSeg-Net, while providing\nintuitive uncertainty measures to further facilitate quality control of the\nautomatic segmentation outcomes. The implementation of our proposed method is\navailable at: https://github.com/HealthX-Lab/DiffusionSynCTSeg.\n', ""  In many clinical settings, the use of both Computed Tomography (CT) and\nMagnetic Resonance (MRI) is necessary to pursue a thorough understanding of the\npatient's anatomy and to plan a suitable therapeutical strategy; this is often\nthe case in MRI-based radiotherapy, where CT is always necessary to prepare the\ndose delivery, as it provides the essential information about the radiation\nabsorption properties of the tissues. Sometimes, MRI is preferred to contour\nthe target volumes. However, this approach is often not the most efficient, as\nit is more expensive, time-consuming and, most importantly, stressful for the\npatients. To overcome this issue, in this work, we analyse the capabilities of\ndifferent configurations of Deep Learning models to generate synthetic CT scans\nfrom MRI, leveraging the power of Generative Adversarial Networks (GANs) and,\nin particular, the CycleGAN architecture, capable of working in an unsupervised\nmanner and without paired images, which were not available. Several CycleGAN\nmodels were trained unsupervised to generate CT scans from different MRI\nmodalities with and without contrast agents. To overcome the problem of not\nhaving a ground truth, distribution-based metrics were used to assess the\nmodel's performance quantitatively, together with a qualitative evaluation\nwhere physicians were asked to differentiate between real and synthetic images\nto understand how realistic the generated images were. The results show how,\ndepending on the input modalities, the models can have very different\nperformances; however, models with the best quantitative results, according to\nthe distribution-based metrics used, can generate very difficult images to\ndistinguish from the real ones, even for physicians, demonstrating the\napproach's potential.\n""]",Medical Imaging with Generative Adversarial Networks,Generative Adversarial Networks (GANs) and Their Applications,Generative Modeling and Artificial Intelligence,Generative Modeling and Artificial Intelligence
223,38,223_concepts_cnn_imagenet_cnns,"['concepts', 'cnn', 'imagenet', 'cnns', 'neural', 'concept', 'explainability', 'explanations', 'classification', 'interpretability']","['concept', 'explanations', 'concepts', 'explanation', 'explainability', 'explainable', 'abstraction', 'relevance', 'interpretable', 'interpretability']","[""  With the wide proliferation of Deep Neural Networks in high-stake\napplications, there is a growing demand for explainability behind their\ndecision-making process. Concept learning models attempt to learn high-level\n'concepts' - abstract entities that align with human understanding, and thus\nprovide interpretability to DNN architectures. However, in this paper, we\ndemonstrate that present SOTA concept learning approaches suffer from two major\nproblems - lack of concept fidelity wherein the models fail to learn consistent\nconcepts among similar classes and limited concept interoperability wherein the\nmodels fail to generalize learned concepts to new domains for the same task.\nKeeping these in mind, we propose a novel self-explaining architecture for\nconcept learning across domains which - i) incorporates a new concept saliency\nnetwork for representative concept selection, ii) utilizes contrastive learning\nto capture representative domain invariant concepts, and iii) uses a novel\nprototype-based concept grounding regularization to improve concept alignment\nacross domains. We demonstrate the efficacy of our proposed approach over\ncurrent SOTA concept learning approaches on four widely used real-world\ndatasets. Empirical results show that our method improves both concept fidelity\nmeasured through concept overlap and concept interoperability measured through\ndomain adaptation performance.\n"", '  Analysis of how semantic concepts are represented within Convolutional Neural\nNetworks (CNNs) is a widely used approach in Explainable Artificial\nIntelligence (XAI) for interpreting CNNs. A motivation is the need for\ntransparency in safety-critical AI-based systems, as mandated in various\ndomains like automated driving. However, to use the concept representations for\nsafety-relevant purposes, like inspection or error retrieval, these must be of\nhigh quality and, in particular, stable. This paper focuses on two stability\ngoals when working with concept representations in computer vision CNNs:\nstability of concept retrieval and of concept attribution. The guiding use-case\nis a post-hoc explainability framework for object detection (OD) CNNs, towards\nwhich existing concept analysis (CA) methods are successfully adapted. To\naddress concept retrieval stability, we propose a novel metric that considers\nboth concept separation and consistency, and is agnostic to layer and concept\nrepresentation dimensionality. We then investigate impacts of concept\nabstraction level, number of concept training samples, CNN size, and concept\nrepresentation dimensionality on stability. For concept attribution stability\nwe explore the effect of gradient instability on gradient-based explainability\nmethods. The results on various CNNs for classification and object detection\nyield the main findings that (1) the stability of concept retrieval can be\nenhanced through dimensionality reduction via data aggregation, and (2) in\nshallow layers where gradient instability is more pronounced, gradient\nsmoothing techniques are advised. Finally, our approach provides valuable\ninsights into selecting the appropriate layer and concept representation\ndimensionality, paving the way towards CA in safety-critical XAI applications.\n', ""  The focus of recent research has shifted from merely improving the metrics\nbased performance of Deep Neural Networks (DNNs) to DNNs which are more\ninterpretable to humans. The field of eXplainable Artificial Intelligence (XAI)\nhas observed various techniques, including saliency-based and concept-based\napproaches. These approaches explain the model's decisions in simple human\nunderstandable terms called Concepts. Concepts are known to be the thinking\nground of humans}. Explanations in terms of concepts enable detecting spurious\ncorrelations, inherent biases, or clever-hans. With the advent of concept-based\nexplanations, a range of concept representation methods and automatic concept\ndiscovery algorithms have been introduced. Some recent works also use concepts\nfor model improvement in terms of interpretability and generalization. We\nprovide a systematic review and taxonomy of various concept representations and\ntheir discovery algorithms in DNNs, specifically in vision. We also provide\ndetails on concept-based model improvement literature marking the first\ncomprehensive survey of these methods.\n""]",Concept Learning and Explainability in Neural Networks,Explainability and Interpretability in Neural Networks,Explainable Artificial Intelligence,Artificial Intelligence and Machine Learning Interpretability and Explainability
224,38,224_multitask_merging_merge_combine,"['multitask', 'merging', 'merge', 'combine', 'taskonomy', 'tasking', 'models', 'merged', 'tasks', 'task']","['merging', 'task', 'merged', 'arithmetic', 'multitask', 'parameter', 'multi', 'tasks', 'specific', 'multiple']","['  Model merging has emerged as an effective approach to combine multiple\nsingle-task models, fine-tuned from the same pre-trained model, into a\nmultitask model. This process typically involves computing a weighted average\nof the model parameters without any additional training. Existing model-merging\nmethods focus on enhancing average task accuracy. However, interference and\nconflicts between the objectives of different tasks can lead to trade-offs\nduring model merging. In real-world applications, a set of solutions with\nvarious trade-offs can be more informative, helping practitioners make\ndecisions based on diverse preferences. In this paper, we introduce a novel\nlow-compute algorithm, Model Merging with Amortized Pareto Front (MAP). MAP\nidentifies a Pareto set of scaling coefficients for merging multiple models to\nreflect the trade-offs. The core component of MAP is approximating the\nevaluation metrics of the various tasks using a quadratic approximation\nsurrogate model derived from a pre-selected set of scaling coefficients,\nenabling amortized inference. Experimental results on vision and natural\nlanguage processing tasks show that MAP can accurately identify the Pareto\nfront. To further reduce the required computation of MAP, we propose (1) a\nBayesian adaptive sampling algorithm and (2) a nested merging scheme with\nmultiple stages.\n', '  Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.\n', ""  In this paper, we introduce a novel approach for large language model merging\nvia black-box multi-objective optimization algorithms. The goal of model\nmerging is to combine multiple models, each excelling in different tasks, into\na single model that outperforms any of the individual source models. However,\nmodel merging faces two significant challenges: First, existing methods rely\nheavily on human intuition and customized strategies to tackle multiple tasks.\nSecond, it's difficult to search for the great model merging configuration in\nlimited evaluations. To address these challenges, we propose a multi-objective\noptimization based model merging method named MM-MO. The proposed method can\nautomatically search merging configurations for multiple tasks with\nmulti-objective optimization algorithms. Moreover, to obtain high-quality model\nmerging configurations within a limited number of evaluation iterations, we\nhave made several improvements to multi-objective Bayesian optimization\nspecifically for model merging scenarios. First, we introduced a weak-to-strong\nmethod to improve the acquisition strategy. Second, we employed Fisher\ninformation to select configurations, further increasing the chances of\ndiscovering superior model merging configurations. Third, we designed a\nsparsity metric as an additional optimization objective to enhance the model's\ngeneralization performance across different tasks. We conducted comprehensive\nexperiments with other mainstream model merging methods, demonstrating that our\nmethod consistently outperforms them. Moreover, performance improvements are\nobserved even on the tasks not explicitly targeted as optimization objectives,\nindicating that our method enhances the overall potential of the model. ...\n""]",Model Merging for Multitask Learning,Multimodal Learning and Model Integration for Healthcare and Multitask Applications,Multimodal Learning and Applications,Multimodal Learning and Vision-Language Models
225,38,225_tools_toolkits_toollens_toolsets,"['tools', 'toolkits', 'toollens', 'toolsets', 'tool', 'toolflow', 'toolnet', 'toolbench', 'toolset', 'toolname']","['tool', 'tools', 'calling', 'usage', 'external', 'planning', 'retrieval', 'toolset', 'queries', 'capabilities']","['  Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. In general, we hope this paper could inspire\nfuture research in integrating tools with foundation models.\n', '  Large language models (LLMs) have garnered significant attention due to their\nimpressive natural language processing (NLP) capabilities. Recently, many\nstudies have focused on the tool utilization ability of LLMs. They primarily\ninvestigated how LLMs effectively collaborate with given specific tools.\nHowever, in scenarios where LLMs serve as intelligent agents, as seen in\napplications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate\ndecision-making processes that involve deciding whether to employ a tool and\nselecting the most suitable tool(s) from a collection of available tools to\nfulfill user requests. Therefore, in this paper, we introduce MetaTool, a\nbenchmark designed to evaluate whether LLMs have tool usage awareness and can\ncorrectly choose tools. Specifically, we create a dataset called ToolE within\nthe benchmark. This dataset contains various types of user queries in the form\nof prompts that trigger LLMs to use tools, including both single-tool and\nmulti-tool scenarios. Subsequently, we set the tasks for both tool usage\nawareness and tool selection. We define four subtasks from different\nperspectives in tool selection, including tool selection with similar choices,\ntool selection in specific scenarios, tool selection with possible reliability\nissues, and multi-tool selection. We conduct experiments involving eight\npopular LLMs and find that the majority of them still struggle to effectively\nselect tools, highlighting the existing gaps between LLMs and genuine\nintelligent agents. However, through the error analysis, we found there is\nstill significant room for improvement. Finally, we conclude with insights for\ntool developers -- we strongly recommend that tool developers choose an\nappropriate rewrite model for generating new descriptions based on the\ndownstream LLM the tool will apply to. Our code is in\nhttps://github.com/HowieHwong/MetaTool.\n', ""  Tool learning aims to enhance and expand large language models' (LLMs)\ncapabilities with external tools, which has gained significant attention\nrecently. Current methods have shown that LLMs can effectively handle a certain\namount of tools through in-context learning or fine-tuning. However, in\nreal-world scenarios, the number of tools is typically extensive and\nirregularly updated, emphasizing the necessity for a dedicated tool retrieval\ncomponent. Tool retrieval is nontrivial due to the following challenges: 1)\ncomplex user instructions and tool descriptions; 2) misalignment between tool\nretrieval and tool usage models. To address the above issues, we propose to\nenhance tool retrieval with iterative feedback from the large language model.\nSpecifically, we prompt the tool usage model, i.e., the LLM, to provide\nfeedback for the tool retriever model in multi-round, which could progressively\nimprove the tool retriever's understanding of instructions and tools and reduce\nthe gap between the two standalone components. We build a unified and\ncomprehensive benchmark to evaluate tool retrieval models. The extensive\nexperiments indicate that our proposed approach achieves advanced performance\nin both in-domain evaluation and out-of-domain evaluation.\n""]",Tool Learning and Utilization in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
225,38,225_tools_toolkits_toollens_toolsets,"['tools', 'toolkits', 'toollens', 'toolsets', 'tool', 'toolflow', 'toolnet', 'toolbench', 'toolset', 'toolname']","['tool', 'tools', 'calling', 'usage', 'external', 'planning', 'retrieval', 'toolset', 'queries', 'capabilities']","['  Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. In general, we hope this paper could inspire\nfuture research in integrating tools with foundation models.\n', '  Large language models (LLMs) have garnered significant attention due to their\nimpressive natural language processing (NLP) capabilities. Recently, many\nstudies have focused on the tool utilization ability of LLMs. They primarily\ninvestigated how LLMs effectively collaborate with given specific tools.\nHowever, in scenarios where LLMs serve as intelligent agents, as seen in\napplications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate\ndecision-making processes that involve deciding whether to employ a tool and\nselecting the most suitable tool(s) from a collection of available tools to\nfulfill user requests. Therefore, in this paper, we introduce MetaTool, a\nbenchmark designed to evaluate whether LLMs have tool usage awareness and can\ncorrectly choose tools. Specifically, we create a dataset called ToolE within\nthe benchmark. This dataset contains various types of user queries in the form\nof prompts that trigger LLMs to use tools, including both single-tool and\nmulti-tool scenarios. Subsequently, we set the tasks for both tool usage\nawareness and tool selection. We define four subtasks from different\nperspectives in tool selection, including tool selection with similar choices,\ntool selection in specific scenarios, tool selection with possible reliability\nissues, and multi-tool selection. We conduct experiments involving eight\npopular LLMs and find that the majority of them still struggle to effectively\nselect tools, highlighting the existing gaps between LLMs and genuine\nintelligent agents. However, through the error analysis, we found there is\nstill significant room for improvement. Finally, we conclude with insights for\ntool developers -- we strongly recommend that tool developers choose an\nappropriate rewrite model for generating new descriptions based on the\ndownstream LLM the tool will apply to. Our code is in\nhttps://github.com/HowieHwong/MetaTool.\n', ""  Tool learning aims to enhance and expand large language models' (LLMs)\ncapabilities with external tools, which has gained significant attention\nrecently. Current methods have shown that LLMs can effectively handle a certain\namount of tools through in-context learning or fine-tuning. However, in\nreal-world scenarios, the number of tools is typically extensive and\nirregularly updated, emphasizing the necessity for a dedicated tool retrieval\ncomponent. Tool retrieval is nontrivial due to the following challenges: 1)\ncomplex user instructions and tool descriptions; 2) misalignment between tool\nretrieval and tool usage models. To address the above issues, we propose to\nenhance tool retrieval with iterative feedback from the large language model.\nSpecifically, we prompt the tool usage model, i.e., the LLM, to provide\nfeedback for the tool retriever model in multi-round, which could progressively\nimprove the tool retriever's understanding of instructions and tools and reduce\nthe gap between the two standalone components. We build a unified and\ncomprehensive benchmark to evaluate tool retrieval models. The extensive\nexperiments indicate that our proposed approach achieves advanced performance\nin both in-domain evaluation and out-of-domain evaluation.\n""]",Tool Learning and Utilization in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
225,38,225_tools_toolkits_toollens_toolsets,"['tools', 'toolkits', 'toollens', 'toolsets', 'tool', 'toolflow', 'toolnet', 'toolbench', 'toolset', 'toolname']","['tool', 'tools', 'calling', 'usage', 'external', 'planning', 'retrieval', 'toolset', 'queries', 'capabilities']","['  Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. In general, we hope this paper could inspire\nfuture research in integrating tools with foundation models.\n', '  Large language models (LLMs) have garnered significant attention due to their\nimpressive natural language processing (NLP) capabilities. Recently, many\nstudies have focused on the tool utilization ability of LLMs. They primarily\ninvestigated how LLMs effectively collaborate with given specific tools.\nHowever, in scenarios where LLMs serve as intelligent agents, as seen in\napplications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate\ndecision-making processes that involve deciding whether to employ a tool and\nselecting the most suitable tool(s) from a collection of available tools to\nfulfill user requests. Therefore, in this paper, we introduce MetaTool, a\nbenchmark designed to evaluate whether LLMs have tool usage awareness and can\ncorrectly choose tools. Specifically, we create a dataset called ToolE within\nthe benchmark. This dataset contains various types of user queries in the form\nof prompts that trigger LLMs to use tools, including both single-tool and\nmulti-tool scenarios. Subsequently, we set the tasks for both tool usage\nawareness and tool selection. We define four subtasks from different\nperspectives in tool selection, including tool selection with similar choices,\ntool selection in specific scenarios, tool selection with possible reliability\nissues, and multi-tool selection. We conduct experiments involving eight\npopular LLMs and find that the majority of them still struggle to effectively\nselect tools, highlighting the existing gaps between LLMs and genuine\nintelligent agents. However, through the error analysis, we found there is\nstill significant room for improvement. Finally, we conclude with insights for\ntool developers -- we strongly recommend that tool developers choose an\nappropriate rewrite model for generating new descriptions based on the\ndownstream LLM the tool will apply to. Our code is in\nhttps://github.com/HowieHwong/MetaTool.\n', ""  Tool learning aims to enhance and expand large language models' (LLMs)\ncapabilities with external tools, which has gained significant attention\nrecently. Current methods have shown that LLMs can effectively handle a certain\namount of tools through in-context learning or fine-tuning. However, in\nreal-world scenarios, the number of tools is typically extensive and\nirregularly updated, emphasizing the necessity for a dedicated tool retrieval\ncomponent. Tool retrieval is nontrivial due to the following challenges: 1)\ncomplex user instructions and tool descriptions; 2) misalignment between tool\nretrieval and tool usage models. To address the above issues, we propose to\nenhance tool retrieval with iterative feedback from the large language model.\nSpecifically, we prompt the tool usage model, i.e., the LLM, to provide\nfeedback for the tool retriever model in multi-round, which could progressively\nimprove the tool retriever's understanding of instructions and tools and reduce\nthe gap between the two standalone components. We build a unified and\ncomprehensive benchmark to evaluate tool retrieval models. The extensive\nexperiments indicate that our proposed approach achieves advanced performance\nin both in-domain evaluation and out-of-domain evaluation.\n""]",Tool Learning and Utilization in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
225,38,225_tools_toolkits_toollens_toolsets,"['tools', 'toolkits', 'toollens', 'toolsets', 'tool', 'toolflow', 'toolnet', 'toolbench', 'toolset', 'toolname']","['tool', 'tools', 'calling', 'usage', 'external', 'planning', 'retrieval', 'toolset', 'queries', 'capabilities']","['  Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. In general, we hope this paper could inspire\nfuture research in integrating tools with foundation models.\n', '  Large language models (LLMs) have garnered significant attention due to their\nimpressive natural language processing (NLP) capabilities. Recently, many\nstudies have focused on the tool utilization ability of LLMs. They primarily\ninvestigated how LLMs effectively collaborate with given specific tools.\nHowever, in scenarios where LLMs serve as intelligent agents, as seen in\napplications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate\ndecision-making processes that involve deciding whether to employ a tool and\nselecting the most suitable tool(s) from a collection of available tools to\nfulfill user requests. Therefore, in this paper, we introduce MetaTool, a\nbenchmark designed to evaluate whether LLMs have tool usage awareness and can\ncorrectly choose tools. Specifically, we create a dataset called ToolE within\nthe benchmark. This dataset contains various types of user queries in the form\nof prompts that trigger LLMs to use tools, including both single-tool and\nmulti-tool scenarios. Subsequently, we set the tasks for both tool usage\nawareness and tool selection. We define four subtasks from different\nperspectives in tool selection, including tool selection with similar choices,\ntool selection in specific scenarios, tool selection with possible reliability\nissues, and multi-tool selection. We conduct experiments involving eight\npopular LLMs and find that the majority of them still struggle to effectively\nselect tools, highlighting the existing gaps between LLMs and genuine\nintelligent agents. However, through the error analysis, we found there is\nstill significant room for improvement. Finally, we conclude with insights for\ntool developers -- we strongly recommend that tool developers choose an\nappropriate rewrite model for generating new descriptions based on the\ndownstream LLM the tool will apply to. Our code is in\nhttps://github.com/HowieHwong/MetaTool.\n', ""  Tool learning aims to enhance and expand large language models' (LLMs)\ncapabilities with external tools, which has gained significant attention\nrecently. Current methods have shown that LLMs can effectively handle a certain\namount of tools through in-context learning or fine-tuning. However, in\nreal-world scenarios, the number of tools is typically extensive and\nirregularly updated, emphasizing the necessity for a dedicated tool retrieval\ncomponent. Tool retrieval is nontrivial due to the following challenges: 1)\ncomplex user instructions and tool descriptions; 2) misalignment between tool\nretrieval and tool usage models. To address the above issues, we propose to\nenhance tool retrieval with iterative feedback from the large language model.\nSpecifically, we prompt the tool usage model, i.e., the LLM, to provide\nfeedback for the tool retriever model in multi-round, which could progressively\nimprove the tool retriever's understanding of instructions and tools and reduce\nthe gap between the two standalone components. We build a unified and\ncomprehensive benchmark to evaluate tool retrieval models. The extensive\nexperiments indicate that our proposed approach achieves advanced performance\nin both in-domain evaluation and out-of-domain evaluation.\n""]",Tool Learning and Utilization in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
225,38,225_tools_toolkits_toollens_toolsets,"['tools', 'toolkits', 'toollens', 'toolsets', 'tool', 'toolflow', 'toolnet', 'toolbench', 'toolset', 'toolname']","['tool', 'tools', 'calling', 'usage', 'external', 'planning', 'retrieval', 'toolset', 'queries', 'capabilities']","['  Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. In general, we hope this paper could inspire\nfuture research in integrating tools with foundation models.\n', '  Large language models (LLMs) have garnered significant attention due to their\nimpressive natural language processing (NLP) capabilities. Recently, many\nstudies have focused on the tool utilization ability of LLMs. They primarily\ninvestigated how LLMs effectively collaborate with given specific tools.\nHowever, in scenarios where LLMs serve as intelligent agents, as seen in\napplications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate\ndecision-making processes that involve deciding whether to employ a tool and\nselecting the most suitable tool(s) from a collection of available tools to\nfulfill user requests. Therefore, in this paper, we introduce MetaTool, a\nbenchmark designed to evaluate whether LLMs have tool usage awareness and can\ncorrectly choose tools. Specifically, we create a dataset called ToolE within\nthe benchmark. This dataset contains various types of user queries in the form\nof prompts that trigger LLMs to use tools, including both single-tool and\nmulti-tool scenarios. Subsequently, we set the tasks for both tool usage\nawareness and tool selection. We define four subtasks from different\nperspectives in tool selection, including tool selection with similar choices,\ntool selection in specific scenarios, tool selection with possible reliability\nissues, and multi-tool selection. We conduct experiments involving eight\npopular LLMs and find that the majority of them still struggle to effectively\nselect tools, highlighting the existing gaps between LLMs and genuine\nintelligent agents. However, through the error analysis, we found there is\nstill significant room for improvement. Finally, we conclude with insights for\ntool developers -- we strongly recommend that tool developers choose an\nappropriate rewrite model for generating new descriptions based on the\ndownstream LLM the tool will apply to. Our code is in\nhttps://github.com/HowieHwong/MetaTool.\n', ""  Tool learning aims to enhance and expand large language models' (LLMs)\ncapabilities with external tools, which has gained significant attention\nrecently. Current methods have shown that LLMs can effectively handle a certain\namount of tools through in-context learning or fine-tuning. However, in\nreal-world scenarios, the number of tools is typically extensive and\nirregularly updated, emphasizing the necessity for a dedicated tool retrieval\ncomponent. Tool retrieval is nontrivial due to the following challenges: 1)\ncomplex user instructions and tool descriptions; 2) misalignment between tool\nretrieval and tool usage models. To address the above issues, we propose to\nenhance tool retrieval with iterative feedback from the large language model.\nSpecifically, we prompt the tool usage model, i.e., the LLM, to provide\nfeedback for the tool retriever model in multi-round, which could progressively\nimprove the tool retriever's understanding of instructions and tools and reduce\nthe gap between the two standalone components. We build a unified and\ncomprehensive benchmark to evaluate tool retrieval models. The extensive\nexperiments indicate that our proposed approach achieves advanced performance\nin both in-domain evaluation and out-of-domain evaluation.\n""]",Tool Learning and Utilization in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
225,38,225_tools_toolkits_toollens_toolsets,"['tools', 'toolkits', 'toollens', 'toolsets', 'tool', 'toolflow', 'toolnet', 'toolbench', 'toolset', 'toolname']","['tool', 'tools', 'calling', 'usage', 'external', 'planning', 'retrieval', 'toolset', 'queries', 'capabilities']","['  Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. In general, we hope this paper could inspire\nfuture research in integrating tools with foundation models.\n', '  Large language models (LLMs) have garnered significant attention due to their\nimpressive natural language processing (NLP) capabilities. Recently, many\nstudies have focused on the tool utilization ability of LLMs. They primarily\ninvestigated how LLMs effectively collaborate with given specific tools.\nHowever, in scenarios where LLMs serve as intelligent agents, as seen in\napplications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate\ndecision-making processes that involve deciding whether to employ a tool and\nselecting the most suitable tool(s) from a collection of available tools to\nfulfill user requests. Therefore, in this paper, we introduce MetaTool, a\nbenchmark designed to evaluate whether LLMs have tool usage awareness and can\ncorrectly choose tools. Specifically, we create a dataset called ToolE within\nthe benchmark. This dataset contains various types of user queries in the form\nof prompts that trigger LLMs to use tools, including both single-tool and\nmulti-tool scenarios. Subsequently, we set the tasks for both tool usage\nawareness and tool selection. We define four subtasks from different\nperspectives in tool selection, including tool selection with similar choices,\ntool selection in specific scenarios, tool selection with possible reliability\nissues, and multi-tool selection. We conduct experiments involving eight\npopular LLMs and find that the majority of them still struggle to effectively\nselect tools, highlighting the existing gaps between LLMs and genuine\nintelligent agents. However, through the error analysis, we found there is\nstill significant room for improvement. Finally, we conclude with insights for\ntool developers -- we strongly recommend that tool developers choose an\nappropriate rewrite model for generating new descriptions based on the\ndownstream LLM the tool will apply to. Our code is in\nhttps://github.com/HowieHwong/MetaTool.\n', ""  Tool learning aims to enhance and expand large language models' (LLMs)\ncapabilities with external tools, which has gained significant attention\nrecently. Current methods have shown that LLMs can effectively handle a certain\namount of tools through in-context learning or fine-tuning. However, in\nreal-world scenarios, the number of tools is typically extensive and\nirregularly updated, emphasizing the necessity for a dedicated tool retrieval\ncomponent. Tool retrieval is nontrivial due to the following challenges: 1)\ncomplex user instructions and tool descriptions; 2) misalignment between tool\nretrieval and tool usage models. To address the above issues, we propose to\nenhance tool retrieval with iterative feedback from the large language model.\nSpecifically, we prompt the tool usage model, i.e., the LLM, to provide\nfeedback for the tool retriever model in multi-round, which could progressively\nimprove the tool retriever's understanding of instructions and tools and reduce\nthe gap between the two standalone components. We build a unified and\ncomprehensive benchmark to evaluate tool retrieval models. The extensive\nexperiments indicate that our proposed approach achieves advanced performance\nin both in-domain evaluation and out-of-domain evaluation.\n""]",Tool Learning and Utilization in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
226,38,226_epidemics_epidemic_outbreak_pandemics,"['epidemics', 'epidemic', 'outbreak', 'pandemics', 'pandemic', 'outbreaks', 'influenza', 'covid', 'infectious', 'pandemicllm']","['epidemic', 'pandemic', 'outbreaks', 'spreading', 'infection', 'spread', 'epidemiological', 'epidemics', 'outbreak', 'disease']","['  In the context of natural disasters, human responses inevitably intertwine\nwith natural factors. The COVID-19 pandemic, as a significant stress factor,\nhas brought to light profound variations among different countries in terms of\ntheir adaptive dynamics in addressing the spread of infection outbreaks across\ndifferent regions. This emphasizes the crucial role of cultural characteristics\nin natural disaster analysis. The theoretical understanding of large-scale\nepidemics primarily relies on mean-field kinetic models. However, conventional\nSIR-like models failed to fully explain the observed phenomena at the onset of\nthe COVID-19 outbreak. These phenomena included the unexpected cessation of\nexponential growth, the reaching of plateaus, and the occurrence of multi-wave\ndynamics. In situations where an outbreak of a highly virulent and unfamiliar\ninfection arises, it becomes crucial to respond swiftly at a non-medical level\nto mitigate the negative socio-economic impact. Here we present a theoretical\nexamination of the first wave of the epidemic based on a simple SIRSS model\n(SIR with Social Stress). We conduct an analysis of the socio-cultural features\nof na\\""ive population behaviors across various countries worldwide. The unique\ncharacteristics of each country/territory are encapsulated in only a few\nconstants within our model, derived from the fitted COVID-19 statistics. These\nconstants also reflect the societal response dynamics to the external stress\nfactor, underscoring the importance of studying the mutual behavior of humanity\nand natural factors during global social disasters. Based on these distinctive\ncharacteristics of specific regions, local authorities can optimize their\nstrategies to effectively combat epidemics until vaccines are developed.\n', '  Since the onset of the COVID-19 pandemic, there has been a growing interest\nin studying epidemiological models. Traditional mechanistic models\nmathematically describe the transmission mechanisms of infectious diseases.\nHowever, they often suffer from limitations of oversimplified or fixed\nassumptions, which could cause sub-optimal predictive power and inefficiency in\ncapturing complex relation information. Consequently, Graph Neural Networks\n(GNNs) have emerged as a progressively popular tool in epidemic research. In\nthis paper, we endeavor to furnish a comprehensive review of GNNs in epidemic\ntasks and highlight potential future directions. To accomplish this objective,\nwe introduce hierarchical taxonomies for both epidemic tasks and methodologies,\noffering a trajectory of development within this domain. For epidemic tasks, we\nestablish a taxonomy akin to those typically employed within the epidemic\ndomain. For methodology, we categorize existing work into Neural Models and\nHybrid Models. Following this, we perform an exhaustive and systematic\nexamination of the methodologies, encompassing both the tasks and their\ntechnical details. Furthermore, we discuss the limitations of existing methods\nfrom diverse perspectives and systematically propose future research\ndirections. This survey aims to bridge literature gaps and promote the\nprogression of this promising field, with a list of relevant papers at\nhttps://github.com/Emory-Melody/awesome-epidemic-modelingpapers. We hope that\nit will facilitate synergies between the communities of GNNs and epidemiology,\nand contribute to their collective progress.\n', '  In this paper, we propose a mathematical framework that governs the evolution\nof epidemic dynamics, encompassing both intra-population dynamics and\ninter-population mobility within a metapopulation network. By linearizing this\ndynamical system, we can identify the spatial starting point(s), namely the\nsource(s) (A) and the initiation time (B) of any epidemic, which we refer to as\nthe ""Big Bang"" of the epidemic. Furthermore, we introduce a novel concept of\neffective distance to track disease spread within the network. Our analysis\nreveals that the contagion geometry can be represented as a line with a\nuniversal slope, independent of disease type (R0) or mobility network\nconfiguration. The mathematical derivations presented in this framework are\ncorroborated by empirical data, including observations from the COVID-19\npandemic in Iran and the US, as well as the H1N1 outbreak worldwide. Within\nthis framework, in order to detect the Big Bang of an epidemic we require two\ntypes of data: A) A snapshot of the active infected cases in each subpopulation\nduring the linear phase. B) A coarse-grained representation of inter-population\nmobility. Also even with access to only type A data, we can still demonstrate\nthe universal contagion geometric pattern. Additionally, we can estimate errors\nand assess the precision of the estimations. This comprehensive approach\nenhances our understanding of when and where epidemics began and how they\nspread, and equips us with valuable insights for developing effective public\nhealth policies and mitigating the impact of infectious diseases on populations\nworldwide.\n']",Epidemic Modeling and Analysis,COVID-19 Research and Public Perception,COVID-19 Research and Public Perception,COVID-19 Research and Public Perception
227,38,227_processes_process_bpm_mining,"['processes', 'process', 'bpm', 'mining', 'analytics', 'discovering', 'activity', 'discovery', 'activities', 'manufacturing']","['mining', 'process', 'business', 'processes', 'logs', 'patterns', 'event', 'traces', 'discovery', 'pattern']","['  Process mining, as a high-level field in data mining, plays a crucial role in\nenhancing operational efficiency and decision-making across organizations. In\nthis survey paper, we delve into the growing significance and ongoing trends in\nthe field of process mining, advocating a specific viewpoint on its contents,\napplication, and development in modern businesses and process management,\nparticularly in cross-organizational settings. We first summarize the framework\nof process mining, common industrial applications, and the latest advances\ncombined with artificial intelligence, such as workflow optimization,\ncompliance checking, and performance analysis. Then, we propose a holistic\nframework for intelligent process analysis and outline initial methodologies in\ncross-organizational settings, highlighting both challenges and opportunities.\nThis particular perspective aims to revolutionize process mining by leveraging\nartificial intelligence to offer sophisticated solutions for complex,\nmulti-organizational data analysis. By integrating advanced machine learning\ntechniques, we can enhance predictive capabilities, streamline processes, and\nfacilitate real-time decision-making. Furthermore, we pinpoint avenues for\nfuture investigations within the research community, encouraging the\nexploration of innovative algorithms, data integration strategies, and\nprivacy-preserving methods to fully harness the potential of process mining in\ndiverse, interconnected business environments.\n', '  In the rapidly evolving field of business process management, there is a\ngrowing need for analytical tools that can transform complex data into\nactionable insights. This research introduces a novel approach by integrating\nLarge Language Models (LLMs), such as ChatGPT, into process mining tools,\nmaking process analytics more accessible to a wider audience. The study aims to\ninvestigate how ChatGPT enhances analytical capabilities, improves user\nexperience, increases accessibility, and optimizes the architectural frameworks\nof process mining tools. The key innovation of this research lies in developing\na tailored prompt engineering strategy for each process mining submodule,\nensuring that the AI-generated outputs are accurate and relevant to the\ncontext. The integration architecture follows an Extract, Transform, Load (ETL)\nprocess, which includes various process mining engine modules and utilizes\nzero-shot and optimized prompt engineering techniques. ChatGPT is connected via\nAPIs and receives structured outputs from the process mining modules, enabling\nconversational interactions. To validate the effectiveness of this approach,\nthe researchers used data from 17 companies that employ BehfaLab\'s Process\nMining Tool. The results showed significant improvements in user experience,\nwith an expert panel rating 72% of the results as ""Good"". This research\ncontributes to the advancement of business process analysis methodologies by\ncombining process mining with artificial intelligence. Future research\ndirections include further optimization of prompt engineering, exploration of\nintegration with other AI technologies, and assessment of scalability across\nvarious business environments. This study paves the way for continuous\ninnovation at the intersection of process mining and artificial intelligence,\npromising to revolutionize the way businesses analyze and optimize their\nprocesses.\n', '  The process mining community has recently recognized the potential of large\nlanguage models (LLMs) for tackling various process mining tasks. Initial\nstudies report the capability of LLMs to support process analysis and even, to\nsome extent, that they are able to reason about how processes work. This latter\nproperty suggests that LLMs could also be used to tackle process mining tasks\nthat benefit from an understanding of process behavior. Examples of such tasks\ninclude (semantic) anomaly detection and next activity prediction, which both\ninvolve considerations of the meaning of activities and their inter-relations.\nIn this paper, we investigate the capabilities of LLMs to tackle such\nsemantics-aware process mining tasks. Furthermore, whereas most works on the\nintersection of LLMs and process mining only focus on testing these models out\nof the box, we provide a more principled investigation of the utility of LLMs\nfor process mining, including their ability to obtain process mining knowledge\npost-hoc by means of in-context learning and supervised fine-tuning.\nConcretely, we define three process mining tasks that benefit from an\nunderstanding of process semantics and provide extensive benchmarking datasets\nfor each of them. Our evaluation experiments reveal that (1) LLMs fail to solve\nchallenging process mining tasks out of the box and when provided only a\nhandful of in-context examples, (2) but they yield strong performance when\nfine-tuned for these tasks, consistently surpassing smaller, encoder-based\nlanguage models.\n']",Process Mining and Analytics,Process Analysis and Adaptive Learning in Dynamic Data Environments,Machine Learning and Data-Driven Applications,Machine Learning and Data-Driven Applications
228,38,228_videos_recognition_surgicalpart_pose,"['videos', 'recognition', 'surgicalpart', 'pose', 'surgeons', 'laparoscopic', 'neurosurgical', 'camera', 'vision', 'intraoperative']","['surgical', 'surgery', 'instrument', 'instruments', 'endoscopic', 'surgeons', 'scene', 'segmentation', 'phase', 'recognition']","['  Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction,\nsurgical navigation and augmented reality visualization. Although the\nfoundation model exhibits outstanding performance in many vision tasks,\nincluding depth estimation (e.g., DINOv2), recent works observed its\nlimitations in medical and surgical domain-specific applications. This work\npresents a low-ranked adaptation (LoRA) of the foundation model for surgical\ndepth estimation. Methods: We design a foundation model-based depth estimation\nmethod, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for\ndepth estimation in endoscopic surgery. We build LoRA layers and integrate them\ninto DINO to adapt with surgery-specific domain knowledge instead of\nconventional fine-tuning. During training, we freeze the DINO image encoder,\nwhich shows excellent visual representation capacity, and only optimize the\nLoRA layers and depth decoder to integrate features from the surgical scene.\nResults: Our model is extensively validated on a MICCAI challenge dataset of\nSCARED, which is collected from da Vinci Xi endoscope surgery. We empirically\nshow that Surgical-DINO significantly outperforms all the state-of-the-art\nmodels in endoscopic depth estimation tasks. The analysis with ablation studies\nhas shown evidence of the remarkable effect of our LoRA layers and adaptation.\nConclusion: Surgical-DINO shed some light on the successful adaptation of the\nfoundation models into the surgical domain for depth estimation. There is clear\nevidence in the results that zero-shot prediction on pre-trained weights in\ncomputer vision datasets or naive fine-tuning is not sufficient to use the\nfoundation model in the surgical domain directly. Code is available at\nhttps://github.com/BeileiCui/SurgicalDINO.\n', '  Phase recognition in surgical videos is crucial for enhancing computer-aided\nsurgical systems as it enables automated understanding of sequential procedural\nstages. Existing methods often rely on fixed temporal windows for video\nanalysis to identify dynamic surgical phases. Thus, they struggle to\nsimultaneously capture short-, mid-, and long-term information necessary to\nfully understand complex surgical procedures. To address these issues, we\npropose Multi-Scale Transformers for Surgical Phase Recognition (MuST), a novel\nTransformer-based approach that combines a Multi-Term Frame encoder with a\nTemporal Consistency Module to capture information across multiple temporal\nscales of a surgical video. Our Multi-Term Frame Encoder computes\ninterdependencies across a hierarchy of temporal scales by sampling sequences\nat increasing strides around the frame of interest. Furthermore, we employ a\nlong-term Transformer encoder over the frame embeddings to further enhance\nlong-term reasoning. MuST achieves higher performance than previous\nstate-of-the-art methods on three different public benchmarks.\n', ""  Automation in surgical robotics has the potential to improve patient safety\nand surgical efficiency, but it is difficult to achieve due to the need for\nrobust perception algorithms. In particular, 6D pose estimation of surgical\ninstruments is critical to enable the automatic execution of surgical maneuvers\nbased on visual feedback. In recent years, supervised deep learning algorithms\nhave shown increasingly better performance at 6D pose estimation tasks; yet,\ntheir success depends on the availability of large amounts of annotated data.\nIn household and industrial settings, synthetic data, generated with 3D\ncomputer graphics software, has been shown as an alternative to minimize\nannotation costs of 6D pose datasets. However, this strategy does not translate\nwell to surgical domains as commercial graphics software have limited tools to\ngenerate images depicting realistic instrument-tissue interactions. To address\nthese limitations, we propose an improved simulation environment for surgical\nrobotics that enables the automatic generation of large and diverse datasets\nfor 6D pose estimation of surgical instruments. Among the improvements, we\ndeveloped an automated data generation pipeline and an improved surgical scene.\nTo show the applicability of our system, we generated a dataset of 7.5k images\nwith pose annotations of a surgical needle that was used to evaluate a\nstate-of-the-art pose estimation network. The trained model obtained a mean\ntranslational error of 2.59mm on a challenging dataset that presented varying\nlevels of occlusion. These results highlight our pipeline's success in training\nand evaluating novel vision algorithms for surgical robotics applications.\n""]",Surgical Robotics and Computer Vision,Computer Vision Applications,Computer Vision,Computer Vision
229,38,229_stances_stance_annotators_annotation,"['stances', 'stance', 'annotators', 'annotation', 'sentiment', 'semantics', 'annotated', 'contextual', 'tweets', 'bias']","['stance', 'detection', 'stances', 'topics', 'media', 'target', 'social', 'topic', 'shot', 'viewpoint']","[""  Social media platforms are rich sources of opinionated content. Stance\ndetection allows the automatic extraction of users' opinions on various topics\nfrom such content. We focus on zero-shot stance detection, where the model's\nsuccess relies on (a) having knowledge about the target topic; and (b) learning\ngeneral reasoning strategies that can be employed for new topics. We present\nStance Reasoner, an approach to zero-shot stance detection on social media that\nleverages explicit reasoning over background knowledge to guide the model's\ninference about the document's stance on a target. Specifically, our method\nuses a pre-trained language model as a source of world knowledge, with the\nchain-of-thought in-context learning approach to generate intermediate\nreasoning steps. Stance Reasoner outperforms the current state-of-the-art\nmodels on 3 Twitter datasets, including fully supervised models. It can better\ngeneralize across targets, while at the same time providing explicit and\ninterpretable explanations for its predictions.\n"", ""  Stance detection classifies stance relations (namely, Favor, Against, or\nNeither) between comments and targets. Pretrained language models (PLMs) are\nwidely used to mine the stance relation to improve the performance of stance\ndetection through pretrained knowledge. However, PLMs also embed ``bad''\npretrained knowledge concerning stance into the extracted stance relation\nsemantics, resulting in pretrained stance bias. It is not trivial to measure\npretrained stance bias due to its weak quantifiability. In this paper, we\npropose Relative Counterfactual Contrastive Learning (RCCL), in which\npretrained stance bias is mitigated as relative stance bias instead of absolute\nstance bias to overtake the difficulty of measuring bias. Firstly, we present a\nnew structural causal model for characterizing complicated relationships among\ncontext, PLMs and stance relations to locate pretrained stance bias. Then,\nbased on masked language model prediction, we present a target-aware relative\nstance sample generation method for obtaining relative bias. Finally, we use\ncontrastive learning based on counterfactual theory to mitigate pretrained\nstance bias and preserve context stance relation. Experiments show that the\nproposed method is superior to stance detection and debiasing baselines.\n"", '  Stance detection is the view towards a specific target by a given context\n(\\textit{e.g.} tweets, commercial reviews). Target-related knowledge is often\nneeded to assist stance detection models in understanding the target well and\nmaking detection correctly. However, prevailing works for knowledge-infused\nstance detection predominantly incorporate target knowledge from a singular\nsource that lacks knowledge verification in limited domain knowledge. The\nlow-resource training data further increases the challenge for the data-driven\nlarge models in this task. To address those challenges, we propose a\ncollaborative knowledge infusion approach for low-resource stance detection\ntasks, employing a combination of aligned knowledge enhancement and efficient\nparameter learning techniques. Specifically, our stance detection approach\nleverages target background knowledge collaboratively from different knowledge\nsources with the help of knowledge alignment. Additionally, we also introduce\nthe parameter-efficient collaborative adaptor with a staged optimization\nalgorithm, which collaboratively addresses the challenges associated with\nlow-resource stance detection tasks from both network structure and learning\nperspectives. To assess the effectiveness of our method, we conduct extensive\nexperiments on three public stance detection datasets, including low-resource\nand cross-target settings. The results demonstrate significant performance\nimprovements compared to the existing stance detection approaches.\n']",Stance Detection and Analysis,Human Activity and Stance Analysis with Multimodal Sensing and AI,Human Behavior Analysis with AI and Multimodal Data,Human Behavior Analysis with AI and Multimodal Data
230,38,230_deformation_deforming_registration_pose,"['deformation', 'deforming', 'registration', 'pose', 'deformations', 'registrations', 'imaging', 'deformable', 'images', 'anatomy']","['registration', 'cardiac', 'ultrasound', 'anatomical', 'shape', 'meshes', 'fetal', 'surface', 'plane', 'scanning']","[""  Medical image synthesis remains challenging due to misalignment noise during\ntraining. Existing methods have attempted to address this challenge by\nincorporating a registration-guided module. However, these methods tend to\noverlook the task-specific constraints on the synthetic and registration\nmodules, which may cause the synthetic module to still generate spatially\naligned images with misaligned target images during training, regardless of the\nregistration module's function. Therefore, this paper proposes\nregistration-guided consistency and incorporates disentanglement learning for\nmedical image synthesis. The proposed registration-guided consistency\narchitecture fosters task-specificity within the synthetic and registration\nmodules by applying identical deformation fields before and after synthesis,\nwhile enforcing output consistency through an alignment loss. Moreover, the\nsynthetic module is designed to possess the capability of disentangling\nanatomical structures and specific styles across various modalities. An anatomy\nconsistency loss is introduced to further compel the synthetic module to\npreserve geometrical integrity within latent spaces. Experiments conducted on\nboth an in-house abdominal CECT-CT dataset and a publicly available pelvic\nMR-CT dataset have demonstrated the superiority of the proposed method.\n"", '  Image registration (IR) is a process that deforms images to align them with\nrespect to a reference space, making it easier for medical practitioners to\nexamine various medical images in a standardized reference frame, such as\nhaving the same rotation and scale. This document introduces image registration\nusing a simple numeric example. It provides a definition of image registration\nalong with a space-oriented symbolic representation. This review covers various\naspects of image transformations, including affine, deformable, invertible, and\nbidirectional transformations, as well as medical image registration algorithms\nsuch as Voxelmorph, Demons, SyN, Iterative Closest Point, and SynthMorph. It\nalso explores atlas-based registration and multistage image registration\ntechniques, including coarse-fine and pyramid approaches. Furthermore, this\nsurvey paper discusses medical image registration taxonomies, datasets,\nevaluation measures, such as correlation-based metrics, segmentation-based\nmetrics, processing time, and model size. It also explores applications in\nimage-guided surgery, motion tracking, and tumor diagnosis. Finally, the\ndocument addresses future research directions, including the further\ndevelopment of transformers.\n', '  In this work, we propose a novel deformable convolutional pyramid network for\nunsupervised image registration. Specifically, the proposed network enhances\nthe traditional pyramid network by adding an additional shared auxiliary\ndecoder for image pairs. This decoder provides multi-scale high-level feature\ninformation from unblended image pairs for the registration task. During the\nregistration process, we also design a multi-scale feature fusion block to\nextract the most beneficial features for the registration task from both global\nand local contexts. Validation results indicate that this method can capture\ncomplex deformations while achieving higher registration accuracy and\nmaintaining smooth and plausible deformations.\n']",Medical Image Registration and Deformation Analysis,Medical Image Analysis,Medical Imaging and Reporting,Medical Imaging and Reporting
231,38,231_adversarial_privacy_datasets_generative,"['adversarial', 'privacy', 'datasets', 'generative', 'gans', 'anonymization', 'data', 'anonymized', 'gan', 'synthetic']","['synthetic', 'privacy', 'tabular', 'utility', 'data', 'private', 'metrics', 'concerns', 'generative', 'protection']","['  Synthetic data generation, a cornerstone of Generative Artificial\nIntelligence, promotes a paradigm shift in data science by addressing data\nscarcity and privacy while enabling unprecedented performance. As synthetic\ndata becomes more prevalent, concerns emerge regarding the accuracy of\nstatistical methods when applied to synthetic data in contrast to raw data.\nThis article explores the effectiveness of statistical methods on synthetic\ndata and the privacy risks of synthetic data. Regarding effectiveness, we\npresent the Synthetic Data Generation for Analytics framework. This framework\napplies statistical approaches to high-quality synthetic data produced by\ngenerative models like tabular diffusion models, which, initially trained on\nraw data, benefit from insights from pertinent studies through transfer\nlearning. A key finding within this framework is the generational effect, which\nreveals that the error rate of statistical methods on synthetic data decreases\nwith the addition of more synthetic data but may eventually rise or stabilize.\nThis phenomenon, stemming from the challenge of accurately mirroring raw data\ndistributions, highlights a ""reflection point""-an ideal volume of synthetic\ndata defined by specific error metrics. Through three case studies, sentiment\nanalysis, predictive modeling of structured data, and inference in tabular\ndata, we validate the superior performance of this framework compared to\nconventional approaches. On privacy, synthetic data imposes lower risks while\nsupporting the differential privacy standard. These studies underscore\nsynthetic data\'s untapped potential in redefining data science\'s landscape.\n', '  Data serves as the fundamental foundation for advancing deep learning,\nparticularly tabular data presented in a structured format, which is highly\nconducive to modeling. However, even in the era of LLM, obtaining tabular data\nfrom sensitive domains remains a challenge due to privacy or copyright\nconcerns. Hence, exploring how to effectively use models like LLMs to generate\nrealistic and privacy-preserving synthetic tabular data is urgent. In this\npaper, we take a step forward to explore LLMs for tabular data synthesis and\nprivacy protection, by introducing a new framework HARMONIC for tabular data\ngeneration and evaluation. In the tabular data generation of our framework,\nunlike previous small-scale LLM-based methods that rely on continued\npre-training, we explore the larger-scale LLMs with fine-tuning to generate\ntabular data and enhance privacy. Based on idea of the k-nearest neighbors\nalgorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to\ndiscover inter-row relationships. Then, with fine-tuning, LLMs are trained to\nremember the format and connections of the data rather than the data itself,\nwhich reduces the risk of privacy leakage. In the evaluation part of our\nframework, we develop specific privacy risk metrics DLT for LLM synthetic data\ngeneration, as well as performance evaluation metrics LLE for downstream LLM\ntasks. Our experiments find that this tabular data generation framework\nachieves equivalent performance to existing methods with better privacy, which\nalso demonstrates our evaluation framework for the effectiveness of synthetic\ndata and privacy risks in LLM scenarios.\n', '  Synthetic data from generative models emerges as the privacy-preserving\ndata-sharing solution. Such a synthetic data set shall resemble the original\ndata without revealing identifiable private information. The backbone\ntechnology of tabular synthesizers is rooted in image generative models,\nranging from Generative Adversarial Networks (GANs) to recent diffusion models.\nRecent prior work sheds light on the utility-privacy tradeoff on tabular data,\nrevealing and quantifying privacy risks on synthetic data. We first conduct an\nexhaustive empirical analysis, highlighting the utility-privacy tradeoff of\nfive state-of-the-art tabular synthesizers, against eight privacy attacks, with\na special focus on membership inference attacks. Motivated by the observation\nof high data quality but also high privacy risk in tabular diffusion, we\npropose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which\nis composed of an autoencoder network to encode the tabular data and a latent\ndiffusion model to synthesize the latent tables. Following the emerging f-DP\nframework, we apply DP-SGD to train the auto-encoder in combination with batch\nclipping and use the separation value as the privacy metric to better capture\nthe privacy gain from DP algorithms. Our empirical evaluation demonstrates that\nDP-TLDM is capable of achieving a meaningful theoretical privacy guarantee\nwhile also significantly enhancing the utility of synthetic data. Specifically,\ncompared to other DP-protected tabular generative models, DP-TLDM improves the\nsynthetic quality by an average of 35% in data resemblance, 15% in the utility\nfor downstream tasks, and 50% in data discriminability, all while preserving a\ncomparable level of privacy risk.\n']",Synthetic Data Generation and Privacy,Synthetic Data Generation and Applications,Artificial Intelligence in Data Generation and Chemical Synthesis,Artificial Intelligence in Data Generation and Chemical Synthesis
232,38,232_phishing_spam_cybersecurity_emails,"['phishing', 'spam', 'cybersecurity', 'emails', 'malicious', 'phisnet', 'threats', 'phishlang', 'security', 'attackers']","['phishing', 'spam', 'emails', 'email', 'messages', 'web', 'detection', 'cybersecurity', 'attacks', 'malicious']","['  Phishing email attacks are among the most common and most harmful\ncybersecurity attacks. With the emergence of generative AI, phishing attacks\ncan be based on emails generated automatically, making it more difficult to\ndetect them. That is, instead of a single email format sent to a large number\nof recipients, generative AI can be used to send each potential victim a\ndifferent email, making it more difficult for cybersecurity systems to identify\nthe scam email before it reaches the recipient. Here we describe a corpus of\nAI-generated phishing emails. We also use different machine learning tools to\ntest the ability of automatic text analysis to identify AI-generated phishing\nemails. The results are encouraging, and show that machine learning tools can\nidentify an AI-generated phishing email with high accuracy compared to regular\nemails or human-generated scam email. By applying descriptive analytic, the\nspecific differences between AI-generated emails and manually crafted scam\nemails are profiled, and show that AI-generated emails are different in their\nstyle from human-generated phishing email scams. Therefore, automatic\nidentification tools can be used as a warning for the user. The paper also\ndescribes the corpus of AI-generated phishing emails that is made open to the\npublic, and can be used for consequent studies. While the ability of machine\nlearning to detect AI-generated phishing email is encouraging, AI-generated\nphishing emails are different from regular phishing emails, and therefore it is\nimportant to train machine learning systems also with AI-generated emails in\norder to repel future phishing attacks that are powered by generative AI.\n', '  Phishing email is a serious cyber threat that tries to deceive users by\nsending false emails with the intention of stealing confidential information or\ncausing financial harm. Attackers, often posing as trustworthy entities,\nexploit technological advancements and sophistication to make detection and\nprevention of phishing more challenging. Despite extensive academic research,\nphishing detection remains an ongoing and formidable challenge in the\ncybersecurity landscape. Large Language Models (LLMs) and Masked Language\nModels (MLMs) possess immense potential to offer innovative solutions to\naddress long-standing challenges. In this research paper, we present an\noptimized, fine-tuned transformer-based DistilBERT model designed for the\ndetection of phishing emails. In the detection process, we work with a phishing\nemail dataset and utilize the preprocessing techniques to clean and solve the\nimbalance class issues. Through our experiments, we found that our model\neffectively achieves high accuracy, demonstrating its capability to perform\nwell. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI)\ntechniques such as Local Interpretable Model-Agnostic Explanations (LIME) and\nTransformer Interpret to explain how our model makes predictions in the context\nof text classification for phishing emails.\n', ""  The critical threat of phishing emails has been further exacerbated by the\npotential of LLMs to generate highly targeted, personalized, and automated\nspear phishing attacks. Two critical problems concerning LLM-facilitated\nphishing require further investigation: 1) Existing studies on lateral phishing\nlack specific examination of LLM integration for large-scale attacks targeting\nthe entire organization, and 2) Current anti-phishing infrastructure, despite\nits extensive development, lacks the capability to prevent LLM-generated\nattacks, potentially impacting both employees and IT security incident\nmanagement. However, the execution of such investigative studies necessitates a\nreal-world environment, one that functions during regular business operations\nand mirrors the complexity of a large organizational infrastructure. This\nsetting must also offer the flexibility required to facilitate a diverse array\nof experimental conditions, particularly the incorporation of phishing emails\ncrafted by LLMs. This study is a pioneering exploration into the use of Large\nLanguage Models (LLMs) for the creation of targeted lateral phishing emails,\ntargeting a large tier 1 university's operation and workforce of approximately\n9,000 individuals over an 11-month period. It also evaluates the capability of\nemail filtering infrastructure to detect such LLM-generated phishing attempts,\nproviding insights into their effectiveness and identifying potential areas for\nimprovement. Based on our findings, we propose machine learning-based detection\ntechniques for such emails to detect LLM-generated phishing emails that were\nmissed by the existing infrastructure, with an F1-score of 98.96.\n""]",Phishing Email Detection and Prevention in Cybersecurity,Cybersecurity Threat Detection and Prevention,Cybersecurity and Artificial Intelligence,Cybersecurity and Artificial Intelligence
233,37,233_videos_videopoet_videoscore_videodrafter,"['videos', 'videopoet', 'videoscore', 'videodrafter', 'videofeedback', 'scenes', 'videodirectorgpt', 'magicvideo', 'generative', 'video']","['video', 'videos', 'frames', 'editing', 'scene', 'frame', 'generation', 'consistency', 'temporal', 'diffusion']","['  Recent video generative models primarily rely on carefully written text\nprompts for specific tasks, like inpainting or style editing. They require\nlabor-intensive textual descriptions for input videos, hindering their\nflexibility to adapt personal/raw videos to user specifications. This paper\nproposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video\ngenerative framework that supports multiple video editing capabilities such as\nremoval, addition, and modification, through a unified pipeline. RACCooN\nconsists of two principal stages: Video-to-Paragraph (V2P) and\nParagraph-to-Video (P2V). In the V2P stage, we automatically describe video\nscenes in well-structured natural language, capturing both the holistic context\nand focused object details. Subsequently, in the P2V stage, users can\noptionally refine these descriptions to guide the video diffusion model,\nenabling various modifications to the input video, such as removing, changing\nsubjects, and/or adding new objects. The proposed approach stands out from\nother methods through several significant contributions: (1) RACCooN suggests a\nmulti-granular spatiotemporal pooling strategy to generate well-structured\nvideo descriptions, capturing both the broad context and object details without\nrequiring complex human annotations, simplifying precise video content editing\nbased on text for users. (2) Our video generative model incorporates\nauto-generated narratives or instructions to enhance the quality and accuracy\nof the generated content. It supports the addition of video objects,\ninpainting, and attribute modification within a unified framework, surpassing\nexisting video editing and inpainting benchmarks. The proposed framework\ndemonstrates impressive versatile capabilities in video-to-paragraph\ngeneration, video content editing, and can be incorporated into other SoTA\nvideo generative models for further enhancement.\n', '  Image generative models have made significant progress in generating\nrealistic and diverse images, supported by comprehensive guidance from various\nevaluation metrics. However, current video generative models struggle to\ngenerate even short video clips, with limited tools that provide insights for\nimprovements. Current video evaluation metrics are simple adaptations of image\nmetrics by switching the embeddings with video embedding networks, which may\nunderestimate the unique characteristics of video. Our analysis reveals that\nthe widely used Frechet Video Distance (FVD) has a stronger emphasis on the\nspatial aspect than the temporal naturalness of video and is inherently\nconstrained by the input size of the embedding networks used, limiting it to 16\nframes. Additionally, it demonstrates considerable instability and diverges\nfrom human evaluations. To address the limitations, we propose STREAM, a new\nvideo evaluation metric uniquely designed to independently evaluate spatial and\ntemporal aspects. This feature allows comprehensive analysis and evaluation of\nvideo generative models from various perspectives, unconstrained by video\nlength. We provide analytical and experimental evidence demonstrating that\nSTREAM provides an effective evaluation tool for both visual and temporal\nquality of videos, offering insights into area of improvement for video\ngenerative models. To the best of our knowledge, STREAM is the first evaluation\nmetric that can separately assess the temporal and spatial aspects of videos.\nOur code is available at https://github.com/pro2nit/STREAM.\n', ""  Recent text-to-video (T2V) generation methods have seen significant\nadvancements. However, the majority of these works focus on producing short\nvideo clips of a single event (i.e., single-scene videos). Meanwhile, recent\nlarge language models (LLMs) have demonstrated their capability in generating\nlayouts and programs to control downstream visual modules. This prompts an\nimportant question: can we leverage the knowledge embedded in these LLMs for\ntemporally consistent long video generation? In this paper, we propose\nVideoDirectorGPT, a novel framework for consistent multi-scene video generation\nthat uses the knowledge of LLMs for video content planning and grounded video\ngeneration. Specifically, given a single text prompt, we first ask our video\nplanner LLM (GPT-4) to expand it into a 'video plan', which includes the scene\ndescriptions, the entities with their respective layouts, the background for\neach scene, and consistency groupings of the entities. Next, guided by this\nvideo plan, our video generator, named Layout2Vid, has explicit control over\nspatial layouts and can maintain temporal consistency of entities across\nmultiple scenes, while being trained only with image-level annotations. Our\nexperiments demonstrate that our proposed VideoDirectorGPT framework\nsubstantially improves layout and movement control in both single- and\nmulti-scene video generation and can generate multi-scene videos with\nconsistency, while achieving competitive performance with SOTAs in open-domain\nsingle-scene T2V generation. Detailed ablation studies, including dynamic\nadjustment of layout control strength with an LLM and video generation with\nuser-provided images, confirm the effectiveness of each component of our\nframework and its future potential.\n""]",Video Generation and Editing Models,Text-Guided Visual Content Generation and Editing,Artificial Intelligence for Creative Content Generation,Artificial Intelligence for Creative Content Generation
234,37,234_adversarial_captioner_multimodal_captioning,"['adversarial', 'captioner', 'multimodal', 'captioning', 'embeddings', 'captions', 'visual', 'adversary', 'vulnerabilities', 'attacking']","['adversarial', 'attack', 'attacks', 'image', 'robustness', 'images', 'multimodal', 'vision', 'text', 'poisoning']","[""  Vision-enabled language models (VLMs) are now used to build autonomous\nmultimodal agents capable of taking actions in real environments. In this\npaper, we show that multimodal agents raise new safety risks, even though\nattacking agents is more challenging than prior attacks due to limited access\nto and knowledge about the environment. Our attacks use adversarial text\nstrings to guide gradient-based perturbation over one trigger image in the\nenvironment: (1) our captioner attack attacks white-box captioners if they are\nused to process images into captions as additional inputs to the VLM; (2) our\nCLIP attack attacks a set of CLIP models jointly, which can transfer to\nproprietary VLMs. To evaluate the attacks, we curated VisualWebArena-Adv, a set\nof adversarial tasks based on VisualWebArena, an environment for web-based\nmultimodal agent tasks. Within an L-infinity norm of $16/256$ on a single\nimage, the captioner attack can make a captioner-augmented GPT-4V agent execute\nthe adversarial goals with a 75% success rate. When we remove the captioner or\nuse GPT-4V to generate its own captions, the CLIP attack can achieve success\nrates of 21% and 43%, respectively. Experiments on agents based on other VLMs,\nsuch as Gemini-1.5, Claude-3, and GPT-4o, show interesting differences in their\nrobustness. Further analysis reveals several key factors contributing to the\nattack's success, and we also discuss the implications for defenses as well.\nProject page: https://chenwu.io/attack-agent Code and data:\nhttps://github.com/ChenWu98/agent-attack\n"", ""  Vision-language models (VLMs) have achieved significant strides in recent\ntimes specially in multimodal tasks, yet they remain susceptible to adversarial\nattacks on their vision components. To address this, we propose Sim-CLIP, an\nunsupervised adversarial fine-tuning method that enhances the robustness of the\nwidely-used CLIP vision encoder against such attacks while maintaining semantic\nrichness and specificity. By employing a Siamese architecture with cosine\nsimilarity loss, Sim-CLIP learns semantically meaningful and attack-resilient\nvisual representations without requiring large batch sizes or momentum\nencoders. Our results demonstrate that VLMs enhanced with Sim-CLIP's fine-tuned\nCLIP encoder exhibit significantly enhanced robustness against adversarial\nattacks, while preserving semantic meaning of the perturbed images. Notably,\nSim-CLIP does not require additional training or fine-tuning of the VLM itself;\nreplacing the original vision encoder with our fine-tuned Sim-CLIP suffices to\nprovide robustness. This work underscores the significance of reinforcing\nfoundational models like CLIP to safeguard the reliability of downstream VLM\napplications, paving the way for more secure and effective multimodal systems.\n"", '  Vision-Language Models (VLMs) are becoming increasingly vulnerable to\nadversarial attacks as various novel attack strategies are being proposed\nagainst these models. While existing defenses excel in unimodal contexts, they\ncurrently fall short in safeguarding VLMs against adversarial threats. To\nmitigate this vulnerability, we propose a novel, yet elegantly simple approach\nfor detecting adversarial samples in VLMs. Our method leverages Text-to-Image\n(T2I) models to generate images based on captions produced by target VLMs.\nSubsequently, we calculate the similarities of the embeddings of both input and\ngenerated images in the feature space to identify adversarial samples.\nEmpirical evaluations conducted on different datasets validate the efficacy of\nour approach, outperforming baseline methods adapted from image classification\ndomains. Furthermore, we extend our methodology to classification tasks,\nshowcasing its adaptability and model-agnostic nature. Theoretical analyses and\nempirical findings also show the resilience of our approach against adaptive\nattacks, positioning it as an excellent defense mechanism for real-world\ndeployment against adversarial threats.\n']",Adversarial Attacks on Multimodal Vision-Language Models,Adversarial Attacks and Defenses in AI and Machine Learning Models,Adversarial Attacks and Vulnerabilities in AI and Machine Learning,Adversarial Attacks and Vulnerabilities in AI and Machine Learning
235,36,235_fmri_neuroimaging_networks_brain,"['fmri', 'neuroimaging', 'networks', 'brain', 'connectome', 'connectivity', 'hippocampal', 'hippocampus', 'functional', 'embedding']","['brain', 'functional', 'connectivity', 'resting', 'resonance', 'autism', 'magnetic', 'imaging', 'disorder', 'connectome']","[""  The human brain is a complex, dynamic network, which is commonly studied\nusing functional magnetic resonance imaging (fMRI) and modeled as network of\nRegions of interest (ROIs) for understanding various brain functions. Recent\nstudies utilize deep learning approaches to learn the brain network\nrepresentation based on functional connectivity (FC) profile, broadly falling\ninto two main categories. The Fixed-FC approaches, utilizing the FC profile\nwhich represents the linear temporal relation within the brain network, are\nlimited by failing to capture informative brain temporal dynamics. On the other\nhand, the Dynamic-FC approaches, modeling the evolving FC profile over time,\noften exhibit less satisfactory performance due to challenges in handling the\ninherent noisy nature of fMRI data.\n  To address these challenges, we propose Brain Masked Auto-Encoder (BrainMAE)\nfor learning representations directly from fMRI time-series data. Our approach\nincorporates two essential components: a region-aware graph attention mechanism\ndesigned to capture the relationships between different brain ROIs, and a novel\nself-supervised masked autoencoding framework for effective model pre-training.\nThese components enable the model to capture rich temporal dynamics of brain\nactivity while maintaining resilience to inherent noise in fMRI data. Our\nexperiments demonstrate that BrainMAE consistently outperforms established\nbaseline methods by significant margins in four distinct downstream tasks.\nFinally, leveraging the model's inherent interpretability, our analysis of\nmodel-generated representations reveals findings that resonate with ongoing\nresearch in the field of neuroscience.\n"", '  Brain functional connectivity (FC) reveals biomarkers for identification of\nvarious neuropsychiatric disorders. Recent application of deep neural networks\n(DNNs) to connectome-based classification mostly relies on traditional\nconvolutional neural networks using input connectivity matrices on a regular\nEuclidean grid. We propose a graph deep learning framework to incorporate the\nnon-Euclidean information about graph structure for classifying functional\nmagnetic resonance imaging (fMRI)-derived brain networks in major depressive\ndisorder (MDD). We design a novel graph autoencoder (GAE) architecture based on\nthe graph convolutional networks (GCNs) to embed the topological structure and\nnode content of large-sized fMRI networks into low-dimensional latent\nrepresentations. In network construction, we employ the Ledoit-Wolf (LDW)\nshrinkage method to estimate the high-dimensional FC metrics efficiently from\nfMRI data. We consider both supervised and unsupervised approaches for the\ngraph embedding learning. The learned embeddings are then used as feature\ninputs for a deep fully-connected neural network (FCNN) to discriminate MDD\nfrom healthy controls. Evaluated on two resting-state fMRI (rs-fMRI) MDD\ndatasets, results show that the proposed GAE-FCNN model significantly\noutperforms several state-of-the-art methods for brain connectome\nclassification, achieving the best accuracy using the LDW-FC edges as node\nfeatures. The graph embeddings of fMRI FC networks learned by the GAE also\nreveal apparent group differences between MDD and HC. Our new framework\ndemonstrates feasibility of learning graph embeddings on brain networks to\nprovide discriminative information for diagnosis of brain disorders.\n', '  Resting-state functional magnetic resonance imaging (rs-fMRI) is a\nnoninvasive technique pivotal for understanding human neural mechanisms of\nintricate cognitive processes. Most rs-fMRI studies compute a single static\nfunctional connectivity matrix across brain regions of interest, or dynamic\nfunctional connectivity matrices with a sliding window approach. These\napproaches are at risk of oversimplifying brain dynamics and lack proper\nconsideration of the goal at hand. While deep learning has gained substantial\npopularity for modeling complex relational data, its application to uncovering\nthe spatiotemporal dynamics of the brain is still limited. We propose a novel\ninterpretable deep learning framework that learns goal-specific functional\nconnectivity matrix directly from time series and employs a specialized graph\nneural network for the final classification. Our model, DSAM, leverages\ntemporal causal convolutional networks to capture the temporal dynamics in both\nlow- and high-level feature representations, a temporal attention unit to\nidentify important time points, a self-attention unit to construct the\ngoal-specific connectivity matrix, and a novel variant of graph neural network\nto capture the spatial dynamics for downstream classification. To validate our\napproach, we conducted experiments on the Human Connectome Project dataset with\n1075 samples to build and interpret the model for the classification of sex\ngroup, and the Adolescent Brain Cognitive Development Dataset with 8520 samples\nfor independent testing. Compared our proposed framework with other\nstate-of-art models, results suggested this novel approach goes beyond the\nassumption of a fixed connectivity matrix and provides evidence of\ngoal-specific brain connectivity patterns, which opens up the potential to gain\ndeeper insights into how the human brain adapts its functional connectivity\nspecific to the task at hand.\n']",Brain Network Analysis using fMRI and Deep Learning,Neuroimaging and Deep Learning for Brain Disorder Diagnosis and Analysis,Deep Learning for Medical Imaging and Analysis,Deep Learning for Medical Imaging and Analysis
236,36,236_kernels_regularization_kernel_ridge,"['kernels', 'regularization', 'kernel', 'ridge', 'regularized', 'ridgeless', 'generalization', 'generalized', 'optimal', 'overfitting']","['ridge', 'kernel', 'regression', 'regularization', 'random', 'estimator', 'kernels', 'ridgeless', 'regime', 'asymptotic']","['  Motivated by the studies of neural networks (e.g.,the neural tangent kernel\ntheory), we perform a study on the large-dimensional behavior of kernel ridge\nregression (KRR) where the sample size $n \\asymp d^{\\gamma}$ for some $\\gamma >\n0$. Given an RKHS $\\mathcal{H}$ associated with an inner product kernel defined\non the sphere $\\mathbb{S}^{d}$, we suppose that the true function $f_{\\rho}^{*}\n\\in [\\mathcal{H}]^{s}$, the interpolation space of $\\mathcal{H}$ with source\ncondition $s>0$. We first determined the exact order (both upper and lower\nbound) of the generalization error of kernel ridge regression for the optimally\nchosen regularization parameter $\\lambda$. We then further showed that when\n$0<s\\le1$, KRR is minimax optimal; and when $s>1$, KRR is not minimax optimal\n(a.k.a. he saturation effect). Our results illustrate that the curves of rate\nvarying along $\\gamma$ exhibit the periodic plateau behavior and the multiple\ndescent behavior and show how the curves evolve with $s>0$. Interestingly, our\nwork provides a unified viewpoint of several recent works on kernel regression\nin the large-dimensional setting, which correspond to $s=0$ and $s=1$\nrespectively.\n', ""  Kernel ridge regression (KRR) is a popular class of machine learning models\nthat has become an important tool for understanding deep learning. Much of the\nfocus has been on studying the proportional asymptotic regime, $n \\asymp d$,\nwhere $n$ is the number of training samples and $d$ is the dimension of the\ndataset. In this regime, under certain conditions on the data distribution, the\nkernel random matrix involved in KRR exhibits behavior akin to that of a linear\nkernel. In this work, we extend the study of kernel regression to the quadratic\nasymptotic regime, where $n \\asymp d^2$. In this regime, we demonstrate that a\nbroad class of inner-product kernels exhibit behavior similar to a quadratic\nkernel. Specifically, we establish an operator norm approximation bound for the\ndifference between the original kernel random matrix and a quadratic kernel\nrandom matrix with additional correction terms compared to the Taylor expansion\nof the kernel functions. The approximation works for general data distributions\nunder a Gaussian-moment-matching assumption with a covariance structure. This\nnew approximation is utilized to obtain a limiting spectral distribution of the\noriginal kernel matrix and characterize the precise asymptotic training and\ngeneralization errors for KRR in the quadratic regime when $n/d^2$ converges to\na non-zero constant. The generalization errors are obtained for both\ndeterministic and random teacher models. Our proof techniques combine moment\nmethods, Wick's formula, orthogonal polynomials, and resolvent analysis of\nrandom matrices with correlated entries.\n"", '  Kernel ridge regression, KRR, is a generalization of linear ridge regression\nthat is non-linear in the data, but linear in the parameters. Here, we\nintroduce an equivalent formulation of the objective function of KRR, opening\nup both for using penalties other than the ridge penalty and for studying\nkernel ridge regression from the perspective of gradient descent. Using a\ncontinuous-time perspective, we derive a closed-form solution for solving\nkernel regression with gradient descent, something we refer to as kernel\ngradient flow, KGF, and theoretically bound the differences between KRR and\nKGF, where, for the latter, regularization is obtained through early stopping.\nWe also generalize KRR by replacing the ridge penalty with the $\\ell_1$ and\n$\\ell_\\infty$ penalties, respectively, and use the fact that analogous to the\nsimilarities between KGF and KRR, $\\ell_1$ regularization and forward stagewise\nregression (also known as coordinate descent), and $\\ell_\\infty$ regularization\nand sign gradient descent, follow similar solution paths. We can thus alleviate\nthe need for computationally heavy algorithms based on proximal gradient\ndescent. We show theoretically and empirically how the $\\ell_1$ and\n$\\ell_\\infty$ penalties, and the corresponding gradient-based optimization\nalgorithms, produce sparse and robust kernel regression solutions,\nrespectively.\n']",Kernel Ridge Regression Analysis,Kernel Methods for Machine Learning and Statistical Analysis,Machine Learning and Artificial Intelligence,Machine Learning and Intelligent Systems
237,36,237_minds_mentalizing_mind_reasoning,"['minds', 'mentalizing', 'mind', 'reasoning', 'cognition', 'mental', 'inferences', 'language', 'belief', 'think']","['debate', 'belief', 'agent', 'mind', 'beliefs', 'reasoning', 'agents', 'discussion', 'mental', 'theory']","['  Theory of Mind (ToM) refers to the ability of individuals to attribute mental\nstates to others. While Large Language Models (LLMs) have shown some promise\nwith ToM ability, they still struggle with complex ToM reasoning. Our approach\nleverages an external symbolic executor, specifically the SMCDEL model checker,\nand fine-tuning to improve the ToM reasoning ability of LLMs. In our approach,\nan LLM is first fine-tuned through pairs of natural language and symbolic\nformulation representation of ToM problems and is then instructed to generate\nthe symbolic formulation with a one-shot in-context example. The generated\nsymbolic formulation is then executed by the SMCDEL model checker to perform\ntransparent and verifiable ToM reasoning and give the final result. We\ndemonstrate that our approach, ToM-LM, shows a significant improvement over all\nthe constructed baselines. Our study proposes a novel view about externalizing\na particular component of ToM reasoning, mainly reasoning about beliefs, and\nsuggests generalizing it to other aspects of ToM reasoning.\n', '  Large Language Models (LLMs) have recently shown a promise and emergence of\nTheory of Mind (ToM) ability and even outperform humans in certain ToM tasks.\nTo evaluate and extend the boundaries of the ToM reasoning ability of LLMs, we\npropose a novel concept, taxonomy, and framework, the ToM reasoning with Zero,\nFinite, and Infinite Belief History and develop a multi-round text-based game,\ncalled $\\textit{Pick the Right Stuff}$, as a benchmark. We have evaluated six\nLLMs with this game and found their performance on Zero Belief History is\nconsistently better than on Finite Belief History. In addition, we have found\ntwo of the models with small parameter sizes outperform all the evaluated\nmodels with large parameter sizes. We expect this work to pave the way for\nfuture ToM benchmark development and also for the promotion and development of\nmore complex AI agents or systems which are required to be equipped with more\ncomplex ToM reasoning ability.\n', ""  Theory of Mind (ToM)-the cognitive ability to reason about mental states of\nourselves and others, is the foundation of social interaction. Although ToM\ncomes naturally to humans, it poses a significant challenge to even the most\nadvanced Large Language Models (LLMs). Due to the complex logical chains in ToM\nreasoning, especially in higher-order ToM questions, simply utilizing reasoning\nmethods like Chain of Thought (CoT) will not improve the ToM capabilities of\nLLMs. We present TimeToM, which constructs a temporal space and uses it as the\nfoundation to improve the ToM capabilities of LLMs in multiple scenarios.\nSpecifically, within the temporal space, we construct Temporal Belief State\nChain (TBSC) for each character and inspired by the cognition perspective of\nthe social world model, we divide TBSC into self-world beliefs and social world\nbeliefs, aligning with first-order ToM (first-order beliefs) and higher-order\nToM (higher-order beliefs) questions, respectively. Moreover, we design a novel\ntool-belief solver that, by considering belief communication between characters\nin temporal space, can transform a character's higher-order beliefs into\nanother character's first-order beliefs under belief communication period.\nExperimental results indicate that TimeToM can dramatically improve the\nreasoning performance of LLMs on ToM questions while taking a big step towards\ncoherent and robust ToM reasoning.\n""]",Theory of Mind in Large Language Models,Large Language Models and Cognitive Abilities,Large Language Models,Large Language Models
237,36,237_minds_mentalizing_mind_reasoning,"['minds', 'mentalizing', 'mind', 'reasoning', 'cognition', 'mental', 'inferences', 'language', 'belief', 'think']","['debate', 'belief', 'agent', 'mind', 'beliefs', 'reasoning', 'agents', 'discussion', 'mental', 'theory']","['  Theory of Mind (ToM) refers to the ability of individuals to attribute mental\nstates to others. While Large Language Models (LLMs) have shown some promise\nwith ToM ability, they still struggle with complex ToM reasoning. Our approach\nleverages an external symbolic executor, specifically the SMCDEL model checker,\nand fine-tuning to improve the ToM reasoning ability of LLMs. In our approach,\nan LLM is first fine-tuned through pairs of natural language and symbolic\nformulation representation of ToM problems and is then instructed to generate\nthe symbolic formulation with a one-shot in-context example. The generated\nsymbolic formulation is then executed by the SMCDEL model checker to perform\ntransparent and verifiable ToM reasoning and give the final result. We\ndemonstrate that our approach, ToM-LM, shows a significant improvement over all\nthe constructed baselines. Our study proposes a novel view about externalizing\na particular component of ToM reasoning, mainly reasoning about beliefs, and\nsuggests generalizing it to other aspects of ToM reasoning.\n', '  Large Language Models (LLMs) have recently shown a promise and emergence of\nTheory of Mind (ToM) ability and even outperform humans in certain ToM tasks.\nTo evaluate and extend the boundaries of the ToM reasoning ability of LLMs, we\npropose a novel concept, taxonomy, and framework, the ToM reasoning with Zero,\nFinite, and Infinite Belief History and develop a multi-round text-based game,\ncalled $\\textit{Pick the Right Stuff}$, as a benchmark. We have evaluated six\nLLMs with this game and found their performance on Zero Belief History is\nconsistently better than on Finite Belief History. In addition, we have found\ntwo of the models with small parameter sizes outperform all the evaluated\nmodels with large parameter sizes. We expect this work to pave the way for\nfuture ToM benchmark development and also for the promotion and development of\nmore complex AI agents or systems which are required to be equipped with more\ncomplex ToM reasoning ability.\n', ""  Theory of Mind (ToM)-the cognitive ability to reason about mental states of\nourselves and others, is the foundation of social interaction. Although ToM\ncomes naturally to humans, it poses a significant challenge to even the most\nadvanced Large Language Models (LLMs). Due to the complex logical chains in ToM\nreasoning, especially in higher-order ToM questions, simply utilizing reasoning\nmethods like Chain of Thought (CoT) will not improve the ToM capabilities of\nLLMs. We present TimeToM, which constructs a temporal space and uses it as the\nfoundation to improve the ToM capabilities of LLMs in multiple scenarios.\nSpecifically, within the temporal space, we construct Temporal Belief State\nChain (TBSC) for each character and inspired by the cognition perspective of\nthe social world model, we divide TBSC into self-world beliefs and social world\nbeliefs, aligning with first-order ToM (first-order beliefs) and higher-order\nToM (higher-order beliefs) questions, respectively. Moreover, we design a novel\ntool-belief solver that, by considering belief communication between characters\nin temporal space, can transform a character's higher-order beliefs into\nanother character's first-order beliefs under belief communication period.\nExperimental results indicate that TimeToM can dramatically improve the\nreasoning performance of LLMs on ToM questions while taking a big step towards\ncoherent and robust ToM reasoning.\n""]",Theory of Mind in Large Language Models,Large Language Models and Cognitive Abilities,Large Language Models,Large Language Models
238,36,238_trajectories_mobilitygpt_gps_mobility,"['trajectories', 'mobilitygpt', 'gps', 'mobility', 'trips', 'trajcl', 'trajectory', 'travel', 'trip', 'planning']","['trajectory', 'mobility', 'travel', 'trajectories', 'location', 'movement', 'migration', 'urban', 'road', 'city']","[""  Recovering intermediate missing GPS points in a sparse trajectory, while\nadhering to the constraints of the road network, could offer deep insights into\nusers' moving behaviors in intelligent transportation systems. Although recent\nstudies have demonstrated the advantages of achieving map-constrained\ntrajectory recovery via an end-to-end manner, they still face two significant\nchallenges. Firstly, existing methods are mostly sequence-based models. It is\nextremely hard for them to comprehensively capture the micro-semantics of\nindividual trajectory, including the information of each GPS point and the\nmovement between two GPS points. Secondly, existing approaches ignore the\nimpact of the macro-semantics, i.e., the road conditions and the people's\nshared travel preferences reflected by a group of trajectories. To address the\nabove challenges, we propose a Micro-Macro Spatial-Temporal Graph-based\nEncoder-Decoder (MM-STGED). Specifically, we model each trajectory as a graph\nto efficiently describe the micro-semantics of trajectory and design a novel\nmessage-passing mechanism to learn trajectory representations. Additionally, we\nextract the macro-semantics of trajectories and further incorporate them into a\nwell-designed graph-based decoder to guide trajectory recovery. Extensive\nexperiments conducted on sparse trajectories with three different sampling\nintervals that are respectively constructed from two real-world trajectory\ndatasets demonstrate the superiority of our proposed model.\n"", '  Trajectory Representation Learning (TRL) is a powerful tool for\nspatial-temporal data analysis and management. TRL aims to convert complicated\nraw trajectories into low-dimensional representation vectors, which can be\napplied to various downstream tasks, such as trajectory classification,\nclustering, and similarity computation. Existing TRL works usually treat\ntrajectories as ordinary sequence data, while some important spatial-temporal\ncharacteristics, such as temporal regularities and travel semantics, are not\nfully exploited. To fill this gap, we propose a novel Self-supervised\ntrajectory representation learning framework with TemporAl Regularities and\nTravel semantics, namely START. The proposed method consists of two stages. The\nfirst stage is a Trajectory Pattern-Enhanced Graph Attention Network (TPE-GAT),\nwhich converts the road network features and travel semantics into\nrepresentation vectors of road segments. The second stage is a Time-Aware\nTrajectory Encoder (TAT-Enc), which encodes representation vectors of road\nsegments in the same trajectory as a trajectory representation vector,\nmeanwhile incorporating temporal regularities with the trajectory\nrepresentation. Moreover, we also design two self-supervised tasks, i.e.,\nspan-masked trajectory recovery and trajectory contrastive learning, to\nintroduce spatial-temporal characteristics of trajectories into the training\nprocess of our START framework. The effectiveness of the proposed method is\nverified by extensive experiments on two large-scale real-world datasets for\nthree downstream tasks. The experiments also demonstrate that our method can be\ntransferred across different cities to adapt heterogeneous trajectory datasets.\n', ""  Understanding human mobility patterns is essential for various applications,\nfrom urban planning to public safety. The individual trajectory such as mobile\nphone location data, while rich in spatio-temporal information, often lacks\nsemantic detail, limiting its utility for in-depth mobility analysis. Existing\nmethods can infer basic routine activity sequences from this data, lacking\ndepth in understanding complex human behaviors and users' characteristics.\nAdditionally, they struggle with the dependency on hard-to-obtain auxiliary\ndatasets like travel surveys. To address these limitations, this paper defines\ntrajectory semantic inference through three key dimensions: user occupation\ncategory, activity sequence, and trajectory description, and proposes the\nTrajectory Semantic Inference with Large Language Models (TSI-LLM) framework to\nleverage LLMs infer trajectory semantics comprehensively and deeply. We adopt\nspatio-temporal attributes enhanced data formatting (STFormat) and design a\ncontext-inclusive prompt, enabling LLMs to more effectively interpret and infer\nthe semantics of trajectory data. Experimental validation on real-world\ntrajectory datasets demonstrates the efficacy of TSI-LLM in deciphering complex\nhuman mobility patterns. This study explores the potential of LLMs in enhancing\nthe semantic analysis of trajectory data, paving the way for more sophisticated\nand accessible human mobility research.\n""]",Trajectory Analysis and Mobility Patterns,Transportation and Mobility Analysis,Transportation Systems and Environmental Analytics,Transportation Systems and Environmental Analytics
239,36,239_reinforcement_rewards_learning_reward,"['reinforcement', 'rewards', 'learning', 'reward', 'optimal', 'mdps', 'optimization', 'approximation', 'policies', 'gradient']","['policy', 'mirror', 'approximation', 'value', 'reinforcement', 'function', 'algorithms', 'action', 'actions', 'rank']","['  The goal of reinforcement learning is estimating a policy that maps states to\nactions and maximizes the cumulative reward of a Markov Decision Process (MDP).\nThis is oftentimes achieved by estimating first the optimal (reward) value\nfunction (VF) associated with each state-action pair. When the MDP has an\ninfinite horizon, the optimal VFs and policies are stationary under mild\nconditions. However, in finite-horizon MDPs, the VFs (hence, the policies) vary\nwith time. This poses a challenge since the number of VFs to estimate grows not\nonly with the size of the state-action space but also with the time horizon.\nThis paper proposes a non-parametric low-rank stochastic algorithm to\napproximate the VFs of finite-horizon MDPs. First, we represent the (unknown)\nVFs as a multi-dimensional array, or tensor, where time is one of the\ndimensions. Then, we use rewards sampled from the MDP to estimate the optimal\nVFs. More precisely, we use the (truncated) PARAFAC decomposition to design an\nonline low-rank algorithm that recovers the entries of the tensor of VFs. The\nsize of the low-rank PARAFAC model grows additively with respect to each of its\ndimensions, rendering our approach efficient, as demonstrated via numerical\nexperiments.\n', ""  Most methods in reinforcement learning use a Policy Gradient (PG) approach to\nlearn a parametric stochastic policy that maps states to actions. The standard\napproach is to implement such a mapping via a neural network (NN) whose\nparameters are optimized using stochastic gradient descent. However, PG methods\nare prone to large policy updates that can render learning inefficient. Trust\nregion algorithms, like Trust Region Policy Optimization (TRPO), constrain the\npolicy update step, ensuring monotonic improvements. This paper introduces\nlow-rank matrix-based models as an efficient alternative for estimating the\nparameters of TRPO algorithms. By gathering the stochastic policy's parameters\ninto a matrix and applying matrix-completion techniques, we promote and enforce\nlow rank. Our numerical studies demonstrate that low-rank matrix-based policy\nmodels effectively reduce both computational and sample complexities compared\nto NN models, while maintaining comparable aggregated rewards.\n"", '  Neural Network based approximations of the Value function make up the core of\nleading Policy Based methods such as Trust Regional Policy Optimization (TRPO)\nand Proximal Policy Optimization (PPO). While this adds significant value when\ndealing with very complex environments, we note that in sufficiently low State\nand action space environments, a computationally expensive Neural Network\narchitecture offers marginal improvement over simpler Value approximation\nmethods. We present an implementation of Natural Actor Critic algorithms with\nactor updates through Natural Policy Gradient methods. This paper proposes that\nNatural Policy Gradient (NPG) methods with Linear Function Approximation as a\nparadigm for value approximation may surpass the performance and speed of\nNeural Network based models such as TRPO and PPO within these environments.\nOver Reinforcement Learning benchmarks Cart Pole and Acrobot, we observe that\nour algorithm trains much faster than complex neural network architectures, and\nobtains an equivalent or greater result. This allows us to recommend the use of\nNPG methods with Linear Function Approximation over TRPO and PPO for both\ntraditional and sparse reward low dimensional problems.\n']",Reinforcement Learning in MDPs,Reinforcement Learning Methods and Applications,Reinforcement Learning,Reinforcement Learning
239,36,239_reinforcement_rewards_learning_reward,"['reinforcement', 'rewards', 'learning', 'reward', 'optimal', 'mdps', 'optimization', 'approximation', 'policies', 'gradient']","['policy', 'mirror', 'approximation', 'value', 'reinforcement', 'function', 'algorithms', 'action', 'actions', 'rank']","['  The goal of reinforcement learning is estimating a policy that maps states to\nactions and maximizes the cumulative reward of a Markov Decision Process (MDP).\nThis is oftentimes achieved by estimating first the optimal (reward) value\nfunction (VF) associated with each state-action pair. When the MDP has an\ninfinite horizon, the optimal VFs and policies are stationary under mild\nconditions. However, in finite-horizon MDPs, the VFs (hence, the policies) vary\nwith time. This poses a challenge since the number of VFs to estimate grows not\nonly with the size of the state-action space but also with the time horizon.\nThis paper proposes a non-parametric low-rank stochastic algorithm to\napproximate the VFs of finite-horizon MDPs. First, we represent the (unknown)\nVFs as a multi-dimensional array, or tensor, where time is one of the\ndimensions. Then, we use rewards sampled from the MDP to estimate the optimal\nVFs. More precisely, we use the (truncated) PARAFAC decomposition to design an\nonline low-rank algorithm that recovers the entries of the tensor of VFs. The\nsize of the low-rank PARAFAC model grows additively with respect to each of its\ndimensions, rendering our approach efficient, as demonstrated via numerical\nexperiments.\n', ""  Most methods in reinforcement learning use a Policy Gradient (PG) approach to\nlearn a parametric stochastic policy that maps states to actions. The standard\napproach is to implement such a mapping via a neural network (NN) whose\nparameters are optimized using stochastic gradient descent. However, PG methods\nare prone to large policy updates that can render learning inefficient. Trust\nregion algorithms, like Trust Region Policy Optimization (TRPO), constrain the\npolicy update step, ensuring monotonic improvements. This paper introduces\nlow-rank matrix-based models as an efficient alternative for estimating the\nparameters of TRPO algorithms. By gathering the stochastic policy's parameters\ninto a matrix and applying matrix-completion techniques, we promote and enforce\nlow rank. Our numerical studies demonstrate that low-rank matrix-based policy\nmodels effectively reduce both computational and sample complexities compared\nto NN models, while maintaining comparable aggregated rewards.\n"", '  Neural Network based approximations of the Value function make up the core of\nleading Policy Based methods such as Trust Regional Policy Optimization (TRPO)\nand Proximal Policy Optimization (PPO). While this adds significant value when\ndealing with very complex environments, we note that in sufficiently low State\nand action space environments, a computationally expensive Neural Network\narchitecture offers marginal improvement over simpler Value approximation\nmethods. We present an implementation of Natural Actor Critic algorithms with\nactor updates through Natural Policy Gradient methods. This paper proposes that\nNatural Policy Gradient (NPG) methods with Linear Function Approximation as a\nparadigm for value approximation may surpass the performance and speed of\nNeural Network based models such as TRPO and PPO within these environments.\nOver Reinforcement Learning benchmarks Cart Pole and Acrobot, we observe that\nour algorithm trains much faster than complex neural network architectures, and\nobtains an equivalent or greater result. This allows us to recommend the use of\nNPG methods with Linear Function Approximation over TRPO and PPO for both\ntraditional and sparse reward low dimensional problems.\n']",Reinforcement Learning in MDPs,Reinforcement Learning Methods and Applications,Reinforcement Learning,Reinforcement Learning
240,36,240_reinforcement_sepsis_interventions_adaptive,"['reinforcement', 'sepsis', 'interventions', 'adaptive', 'reward', 'medication', 'cpr', 'medications', 'policies', 'outcomes']","['sepsis', 'treatment', 'health', 'interventions', 'patient', 'care', 'healthcare', 'medication', 'patients', 'reinforcement']","['  Reinforcement learning (RL) has garnered increasing recognition for its\npotential to optimise dynamic treatment regimes (DTRs) in personalised\nmedicine, particularly for drug dosage prescriptions and medication\nrecommendations. However, a significant challenge persists: the absence of a\nunified framework for simulating diverse healthcare scenarios and a\ncomprehensive analysis to benchmark the effectiveness of RL algorithms within\nthese contexts. To address this gap, we introduce \\textit{DTR-Bench}, a\nbenchmarking platform comprising four distinct simulation environments tailored\nto common DTR applications, including cancer chemotherapy, radiotherapy,\nglucose management in diabetes, and sepsis treatment. We evaluate various\nstate-of-the-art RL algorithms across these settings, particularly highlighting\ntheir performance amidst real-world challenges such as\npharmacokinetic/pharmacodynamic (PK/PD) variability, noise, and missing data.\nOur experiments reveal varying degrees of performance degradation among RL\nalgorithms in the presence of noise and patient variability, with some\nalgorithms failing to converge. Additionally, we observe that using temporal\nobservation representations does not consistently lead to improved performance\nin DTR settings. Our findings underscore the necessity of developing robust,\nadaptive RL algorithms capable of effectively managing these complexities to\nenhance patient-specific healthcare. We have open-sourced our benchmark and\ncode at https://github.com/GilesLuo/DTR-Bench.\n', ""  Offline reinforcement learning has shown promise for solving tasks in\nsafety-critical settings, such as clinical decision support. Its application,\nhowever, has been limited by the lack of interpretability and interactivity for\nclinicians. To address these challenges, we propose the medical decision\ntransformer (MeDT), a novel and versatile framework based on the\ngoal-conditioned reinforcement learning paradigm for sepsis treatment\nrecommendation. MeDT uses the decision transformer architecture to learn a\npolicy for drug dosage recommendation. During offline training, MeDT utilizes\ncollected treatment trajectories to predict administered treatments for each\ntime step, incorporating known treatment outcomes, target acuity scores, past\ntreatment decisions, and current and past medical states. This analysis enables\nMeDT to capture complex dependencies among a patient's medical history,\ntreatment decisions, outcomes, and short-term effects on stability. Our\nproposed conditioning uses acuity scores to address sparse reward issues and to\nfacilitate clinician-model interactions, enhancing decision-making. Following\ntraining, MeDT can generate tailored treatment recommendations by conditioning\non the desired positive outcome (survival) and user-specified short-term\nstability improvements. We carry out rigorous experiments on data from the\nMIMIC-III dataset and use off-policy evaluation to demonstrate that MeDT\nrecommends interventions that outperform or are competitive with existing\noffline reinforcement learning methods while enabling a more interpretable,\npersonalized and clinician-directed approach.\n"", '  We present ICU-Sepsis, an environment that can be used in benchmarks for\nevaluating reinforcement learning (RL) algorithms. Sepsis management is a\ncomplex task that has been an important topic in applied RL research in recent\nyears. Therefore, MDPs that model sepsis management can serve as part of a\nbenchmark to evaluate RL algorithms on a challenging real-world problem.\nHowever, creating usable MDPs that simulate sepsis care in the ICU remains a\nchallenge due to the complexities involved in acquiring and processing patient\ndata. ICU-Sepsis is a lightweight environment that models personalized care of\nsepsis patients in the ICU. The environment is a tabular MDP that is widely\ncompatible and is challenging even for state-of-the-art RL algorithms, making\nit a valuable tool for benchmarking their performance. However, we emphasize\nthat while ICU-Sepsis provides a standardized environment for evaluating RL\nalgorithms, it should not be used to draw conclusions that guide medical\npractice.\n']",Reinforcement Learning for Sepsis Treatment,Sepsis Management and Treatment Optimization,Sepsis Management and Treatment Optimization,Sepsis Management and Treatment Optimization
241,36,241_fairness_unfairness_recommender_unfair,"['fairness', 'unfairness', 'recommender', 'unfair', 'biases', 'discrimination', 'incentives', 'rankings', 'personalization', 'equitable']","['fairness', 'recommendation', 'recommender', 'creators', 'sided', 'item', 'recommendations', 'provider', 'items', 'user']","['  User-side group fairness is crucial for modern recommender systems, aiming to\nalleviate performance disparities among user groups defined by sensitive\nattributes like gender, race, or age. In the ever-evolving landscape of\nuser-item interactions, continual adaptation to newly collected data is crucial\nfor recommender systems to stay aligned with the latest user preferences.\nHowever, we observe that such continual adaptation often exacerbates\nperformance disparities. This necessitates a thorough investigation into\nuser-side fairness in dynamic recommender systems, an area that has been\nunexplored in the literature. This problem is challenging due to distribution\nshifts, frequent model updates, and non-differentiability of ranking metrics.\nTo our knowledge, this paper presents the first principled study on ensuring\nuser-side fairness in dynamic recommender systems. We start with theoretical\nanalyses on fine-tuning v.s. retraining, showing that the best practice is\nincremental fine-tuning with restart. Guided by our theoretical analyses, we\npropose FAir Dynamic rEcommender (FADE), an end-to-end fine-tuning framework to\ndynamically ensure user-side fairness over time. To overcome the\nnon-differentiability of recommendation metrics in the fairness loss, we\nfurther introduce Differentiable Hit (DH) as an improvement over the recent\nNeuralNDCG method, not only alleviating its gradient vanishing issue but also\nachieving higher efficiency. Besides that, we also address the instability\nissue of the fairness loss by leveraging the competing nature between the\nrecommendation loss and the fairness loss. Through extensive experiments on\nreal-world datasets, we demonstrate that FADE effectively and efficiently\nreduces performance disparities with little sacrifice in the overall\nrecommendation performance.\n', '  Efforts in the recommendation community are shifting from the sole emphasis\non utility to considering beyond-utility factors, such as fairness and\nrobustness. Robustness of recommendation models is typically linked to their\nability to maintain the original utility when subjected to attacks. Limited\nresearch has explored the robustness of a recommendation model in terms of\nfairness, e.g., the parity in performance across groups, under attack\nscenarios. In this paper, we aim to assess the robustness of graph-based\nrecommender systems concerning fairness, when exposed to attacks based on\nedge-level perturbations. To this end, we considered four different fairness\noperationalizations, including both consumer and provider perspectives.\nExperiments on three datasets shed light on the impact of perturbations on the\ntargeted fairness notion, uncovering key shortcomings in existing evaluation\nprotocols for robustness. As an example, we observed perturbations affect\nconsumer fairness on a higher extent than provider fairness, with alarming\nunfairness for the former. Source code:\nhttps://github.com/jackmedda/CPFairRobust\n', ""  In large-scale recommendation systems, the vast array of items makes it\ninfeasible to obtain accurate user preferences for each product, resulting in a\ncommon issue of missing labels. Typically, only items previously recommended to\nusers have associated ground truth data. Although there is extensive research\non fairness concerning fully observed user-item interactions, the challenge of\nfairness in scenarios with missing labels remains underexplored. Previous\nmethods often treat these samples missing labels as negative, which can\nsignificantly deviate from the ground truth fairness metrics. Our study\naddresses this gap by proposing a novel method employing a small randomized\ntraffic to estimate fairness metrics accurately. We present theoretical bounds\nfor the estimation error of our fairness metric and support our findings with\nempirical evidence on real data. Our numerical experiments on synthetic and\nTikTok's real-world data validate our theory and show the efficiency and\neffectiveness of our novel methods. To the best of our knowledge, we are the\nfirst to emphasize the necessity of random traffic in dataset collection for\nrecommendation fairness, the first to publish a fairness-related dataset from\nTikTok and to provide reliable estimates of fairness metrics in the context of\nlarge-scale recommendation systems with missing labels.\n""]",Fairness in Recommender Systems,Fairness in Artificial Intelligence and Machine Learning,Fairness and Bias in Artificial Intelligence,Fairness and Bias in Artificial Intelligence
242,36,242_semantic_parsing_sparql_knowledge,"['semantic', 'parsing', 'sparql', 'knowledge', 'answering', 'retrieval', 'parse', 'answerability', 'unanswerability', 'schemas']","['question', 'answering', 'questions', 'knowledge', 'parsing', 'logical', 'forms', 'query', 'schema', 'answerable']","['  Recent work integrating Large Language Models (LLMs) has led to significant\nimprovements in the Knowledge Base Question Answering (KBQA) task. However, we\nposit that existing KBQA datasets that either have simple questions, use\nsynthetically generated logical forms, or are based on small knowledge base\n(KB) schemas, do not capture the true complexity of KBQA tasks.\n  To address this, we introduce the SPINACH dataset, an expert-annotated KBQA\ndataset collected from forum discussions on Wikidata\'s ""Request a Query"" forum\nwith 320 decontextualized question-SPARQL pairs. Much more complex than\nexisting datasets, SPINACH calls for strong KBQA systems that do not rely on\ntraining data to learn the KB schema, but can dynamically explore large and\noften incomplete schemas and reason about them.\n  Along with the dataset, we introduce the SPINACH agent, a new KBQA approach\nthat mimics how a human expert would write SPARQLs for such challenging\nquestions. Experiments on existing datasets show SPINACH\'s capability in KBQA,\nachieving a new state of the art on the QALD-7, QALD-9 Plus and QALD-10\ndatasets by 30.1%, 27.0%, and 10.0% in F1, respectively, and coming within 1.6%\nof the fine-tuned LLaMA SOTA model on WikiWebQuestions. On our new SPINACH\ndataset, SPINACH agent outperforms all baselines, including the best\nGPT-4-based KBQA agent, by 38.1% in F1.\n', '  Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions over large-scale knowledge bases (KBs), which can be summarized into\ntwo crucial steps: knowledge retrieval and semantic parsing. However, three\ncore challenges remain: inefficient knowledge retrieval, mistakes of retrieval\nadversely impacting semantic parsing, and the complexity of previous KBQA\nmethods. To tackle these challenges, we introduce ChatKBQA, a novel and simple\ngenerate-then-retrieve KBQA framework, which proposes first generating the\nlogical form with fine-tuned LLMs, then retrieving and replacing entities and\nrelations with an unsupervised retrieval method, to improve both generation and\nretrieval more directly. Experimental results show that ChatKBQA achieves new\nstate-of-the-art performance on standard KBQA datasets, WebQSP, and CWQ. This\nwork can also be regarded as a new paradigm for combining LLMs with knowledge\ngraphs (KGs) for interpretable and knowledge-required question answering. Our\ncode is publicly available.\n', '  Existing Knowledge Base Question Answering (KBQA) architectures are hungry\nfor annotated data, which make them costly and time-consuming to deploy. We\nintroduce the problem of few-shot transfer learning for KBQA, where the target\ndomain offers only a few labeled examples, but a large labeled training dataset\nis available in a source domain. We propose a novel KBQA architecture called\nFuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers,\nre-ranks using an LLM and uses this as input for LLM few-shot in-context\nlearning to generate logical forms. These are further refined using\nexecution-guided feedback. Experiments over multiple source-target KBQA pairs\nof varying complexity show that FuSIC-KBQA significantly outperforms\nadaptations of SoTA KBQA models for this setting. Additional experiments show\nthat FuSIC-KBQA also outperforms SoTA KBQA models in the in-domain setting when\ntraining data is limited.\n']",Knowledge Base Question Answering (KBQA),Question Answering and Reasoning Systems,Artificial Intelligence and Reasoning Systems,Intelligent Systems
243,36,243_compression_encoder_encoding_compressing,"['compression', 'encoder', 'encoding', 'compressing', 'compressed', 'decoding', 'decoder', 'bitrate', 'coding', 'jpeg']","['compression', 'distortion', 'image', 'codecs', 'coding', 'lossy', 'perceptual', 'semantic', 'channel', 'compressed']","['  In the field of neural data compression, the prevailing focus has been on\noptimizing algorithms for either classical distortion metrics, such as PSNR or\nSSIM, or human perceptual quality. With increasing amounts of data consumed by\nmachines rather than humans, a new paradigm of machine-oriented\ncompression$\\unicode{x2013}$which prioritizes the retention of features salient\nfor machine perception over traditional human-centric\ncriteria$\\unicode{x2013}$has emerged, creating several new challenges to the\ndevelopment, evaluation, and deployment of systems utilizing lossy compression.\nIn particular, it is unclear how different approaches to lossy compression will\naffect the performance of downstream machine perception tasks. To address this\nunder-explored area, we evaluate various perception\nmodels$\\unicode{x2013}$including image classification, image segmentation,\nspeech recognition, and music source separation$\\unicode{x2013}$under severe\nlossy compression. We utilize several popular codecs spanning conventional,\nneural, and generative compression architectures. Our results indicate three\nkey findings: (1) using generative compression, it is feasible to leverage\nhighly compressed data while incurring a negligible impact on machine\nperceptual quality; (2) machine perceptual quality correlates strongly with\ndeep similarity metrics, indicating a crucial role of these metrics in the\ndevelopment of machine-oriented codecs; and (3) using lossy compressed\ndatasets, (e.g. ImageNet) for pre-training can lead to counter-intuitive\nscenarios where lossy compression increases machine perceptual quality rather\nthan degrading it. To encourage engagement on this growing area of research,\nour code and experiments are available at:\nhttps://github.com/danjacobellis/MPQ.\n', ""  In the field of medical image compression, Implicit Neural Representation\n(INR) networks have shown remarkable versatility due to their flexible\ncompression ratios, yet they are constrained by a one-to-one fitting approach\nthat results in lengthy encoding times. Our novel method,\n``\\textbf{UniCompress}'', innovatively extends the compression capabilities of\nINR by being the first to compress multiple medical data blocks using a single\nINR network. By employing wavelet transforms and quantization, we introduce a\ncodebook containing frequency domain information as a prior input to the INR\nnetwork. This enhances the representational power of INR and provides\ndistinctive conditioning for different image blocks. Furthermore, our research\nintroduces a new technique for the knowledge distillation of implicit\nrepresentations, simplifying complex model knowledge into more manageable\nformats to improve compression ratios. Extensive testing on CT and electron\nmicroscopy (EM) datasets has demonstrated that UniCompress outperforms\ntraditional INR methods and commercial compression solutions like HEVC,\nespecially in complex and high compression scenarios. Notably, compared to\nexisting INR techniques, UniCompress achieves a 4$\\sim$5 times increase in\ncompression speed, marking a significant advancement in the field of medical\nimage compression. Codes will be publicly available.\n"", '  In this age of information, images are a critical medium for storing and\ntransmitting information. With the rapid growth of image data amount, visual\ncompression and visual data perception are two important research topics\nattracting a lot attention. However, those two topics are rarely discussed\ntogether and follow separate research path. Due to the compact compressed\ndomain representation offered by learning-based image compression methods,\nthere exists possibility to have one stream targeting both efficient data\nstorage and compression, and machine perception tasks. In this paper, we\npropose a layered generative image compression model achieving high human\nvision-oriented image reconstructed quality, even at extreme compression\nratios. To obtain analysis efficiency and flexibility, a task-agnostic\nlearning-based compression model is proposed, which effectively supports\nvarious compressed domain-based analytical tasks while reserves outstanding\nreconstructed perceptual quality, compared with traditional and learning-based\ncodecs. In addition, joint optimization schedule is adopted to acquire best\nbalance point among compression ratio, reconstructed image quality, and\ndownstream perception performance. Experimental results verify that our\nproposed compressed domain-based multi-task analysis method can achieve\ncomparable analysis results against the RGB image-based methods with up to\n99.6% bit rate saving (i.e., compared with taking original RGB image as the\nanalysis model input). The practical ability of our model is further justified\nfrom model size and information fidelity aspects.\n']",Image and Data Compression Techniques,Image and Video Compression Techniques,Image and Video Processing,Image and Video Processing
244,36,244_cnn_cnns_defects_features,"['cnn', 'cnns', 'defects', 'features', 'feature', 'defect', 'inspection', 'classification', 'visual', 'yolov5']","['defect', 'wear', 'defects', 'manufacturing', 'inspection', 'industrial', 'detection', 'semiconductor', 'defective', 'inserts']","['  With the rapid growth of the PCB manufacturing industry, there is an\nincreasing demand for computer vision inspection to detect defects during\nproduction. Improving the accuracy and generalization of PCB defect detection\nmodels remains a significant challenge. This paper proposes a high-precision,\nrobust, and real-time end-to-end method for PCB defect detection based on deep\nConvolutional Neural Networks (CNN). Traditional methods often suffer from low\naccuracy and limited applicability. We propose a novel approach combining\nYOLOv5 and multiscale modules for hierarchical residual-like connections. In\nPCB defect detection, noise can confuse the background and small targets. The\nYOLOv5 model provides a strong foundation with its real-time processing and\naccurate object detection capabilities. The multi-scale module extends\ntraditional approaches by incorporating hierarchical residual-like connections\nwithin a single block, enabling multiscale feature extraction. This\nplug-and-play module significantly enhances performance by extracting features\nat multiple scales and levels, which are useful for identifying defects of\nvarying sizes and complexities. Our multi-scale architecture integrates feature\nextraction, defect localization, and classification into a unified network.\nExperiments on a large-scale PCB dataset demonstrate significant improvements\nin precision, recall, and F1-score compared to existing methods. This work\nadvances computer vision inspection for PCB defect detection, providing a\nreliable solution for high-precision, robust, real-time, and domain-adaptive\ndefect detection in the PCB manufacturing industry.\n', '  Deep learning-based semiconductor defect inspection has gained traction in\nrecent years, offering a powerful and versatile approach that provides high\naccuracy, adaptability, and efficiency in detecting and classifying nano-scale\ndefects. However, semiconductor manufacturing processes are continually\nevolving, leading to the emergence of new types of defects over time. This\npresents a significant challenge for conventional supervised defect detectors,\nas they may suffer from catastrophic forgetting when trained on new defect\ndatasets, potentially compromising performance on previously learned tasks. An\nalternative approach involves the constant storage of previously trained\ndatasets alongside pre-trained model versions, which can be utilized for\n(re-)training from scratch or fine-tuning whenever encountering a new defect\ndataset. However, adhering to such a storage template is impractical in terms\nof size, particularly when considering High-Volume Manufacturing (HVM).\nAdditionally, semiconductor defect datasets, especially those encompassing\nstochastic defects, are often limited and expensive to obtain, thus lacking\nsufficient representation of the entire universal set of defectivity. This work\nintroduces a task-agnostic, meta-learning approach aimed at addressing this\nchallenge, which enables the incremental addition of new defect classes and\nscales to create a more robust and generalized model for semiconductor defect\ninspection. We have benchmarked our approach using real resist-wafer SEM\n(Scanning Electron Microscopy) datasets for two process steps, ADI and AEI,\ndemonstrating its superior performance compared to conventional supervised\ntraining methods.\n', '  As automation advances in manufacturing, the demand for precise and\nsophisticated defect detection technologies grows. Existing vision models for\ndefect recognition methods are insufficient for handling the complexities and\nvariations of defects in contemporary manufacturing settings. These models\nespecially struggle in scenarios involving limited or imbalanced defect data.\nIn this work, we introduce MemoryMamba, a novel memory-augmented state space\nmodel (SSM), designed to overcome the limitations of existing defect\nrecognition models. MemoryMamba integrates the state space model with the\nmemory augmentation mechanism, enabling the system to maintain and retrieve\nessential defect-specific information in training. Its architecture is designed\nto capture dependencies and intricate defect characteristics, which are crucial\nfor effective defect detection. In the experiments, MemoryMamba was evaluated\nacross four industrial datasets with diverse defect types and complexities. The\nmodel consistently outperformed other methods, demonstrating its capability to\nadapt to various defect recognition scenarios.\n']",Defect Detection using Deep Learning and Computer Vision,Deep Learning for Defect Detection and Testing,Deep Learning Applications in Engineering and Computer Vision,Deep Learning Applications in Engineering and Computer Vision
245,35,245_documentation_datasets_dataset_researchers,"['documentation', 'datasets', 'dataset', 'researchers', 'developers', 'categories', 'repositories', 'frameworks', 'tools', 'guidelines']","['documentation', 'centric', 'cards', 'data', 'quality', 'reusers', 'practices', 'considerations', 'section', 'guidelines']","['  Hugging Face (HF) has established itself as a crucial platform for the\ndevelopment and sharing of machine learning (ML) models. This repository mining\nstudy, which delves into more than 380,000 models using data gathered via the\nHF Hub API, aims to explore the community engagement, evolution, and\nmaintenance around models hosted on HF, aspects that have yet to be\ncomprehensively explored in the literature. We first examine the overall growth\nand popularity of HF, uncovering trends in ML domains, framework usage, authors\ngrouping and the evolution of tags and datasets used. Through text analysis of\nmodel card descriptions, we also seek to identify prevalent themes and insights\nwithin the developer community. Our investigation further extends to the\nmaintenance aspects of models, where we evaluate the maintenance status of ML\nmodels, classify commit messages into various categories (corrective,\nperfective, and adaptive), analyze the evolution across development stages of\ncommits metrics and introduce a new classification system that estimates the\nmaintenance status of models based on multiple attributes. This study aims to\nprovide valuable insights about ML model maintenance and evolution that could\ninform future model development strategies on platforms like HF.\n', ""  Advances in machine learning are closely tied to the creation of datasets.\nWhile data documentation is widely recognized as essential to the reliability,\nreproducibility, and transparency of ML, we lack a systematic empirical\nunderstanding of current dataset documentation practices. To shed light on this\nquestion, here we take Hugging Face -- one of the largest platforms for sharing\nand collaborating on ML models and datasets -- as a prominent case study. By\nanalyzing all 7,433 dataset documentation on Hugging Face, our investigation\nprovides an overview of the Hugging Face dataset ecosystem and insights into\ndataset documentation practices, yielding 5 main findings: (1) The dataset card\ncompletion rate shows marked heterogeneity correlated with dataset popularity.\n(2) A granular examination of each section within the dataset card reveals that\nthe practitioners seem to prioritize Dataset Description and Dataset Structure\nsections, while the Considerations for Using the Data section receives the\nlowest proportion of content. (3) By analyzing the subsections within each\nsection and utilizing topic modeling to identify key topics, we uncover what is\ndiscussed in each section, and underscore significant themes encompassing both\ntechnical and social impacts, as well as limitations within the Considerations\nfor Using the Data section. (4) Our findings also highlight the need for\nimproved accessibility and reproducibility of datasets in the Usage sections.\n(5) In addition, our human annotation evaluation emphasizes the pivotal role of\ncomprehensive dataset content in shaping individuals' perceptions of a dataset\ncard's overall quality. Overall, our study offers a unique perspective on\nanalyzing dataset documentation through large-scale data science analysis and\nunderlines the need for more thorough dataset documentation in machine learning\nresearch.\n"", '  The rapidly evolving fields of Machine Learning (ML) and Artificial\nIntelligence have witnessed the emergence of platforms like Hugging Face (HF)\nas central hubs for model development and sharing. This experience report\nsynthesizes insights from two comprehensive studies conducted on HF, focusing\non carbon emissions and the evolutionary and maintenance aspects of ML models.\nOur objective is to provide a practical guide for future researchers embarking\non mining software repository studies within the HF ecosystem to enhance the\nquality of these studies. We delve into the intricacies of the replication\npackage used in our studies, highlighting the pivotal tools and methodologies\nthat facilitated our analysis. Furthermore, we propose a nuanced stratified\nsampling strategy tailored for the diverse HF Hub dataset, ensuring a\nrepresentative and comprehensive analytical approach. The report also\nintroduces preliminary guidelines, transitioning from repository mining to\ncohort studies, to establish causality in repository mining studies,\nparticularly within the ML model of HF context. This transition is inspired by\nexisting frameworks and is adapted to suit the unique characteristics of the HF\nmodel ecosystem. Our report serves as a guiding framework for researchers,\ncontributing to the responsible and sustainable advancement of ML, and\nfostering a deeper understanding of the broader implications of ML models.\n']",Machine Learning Model Development and Dataset Documentation,Machine Learning in Software Development and Engineering,Machine Learning and Data-Driven Applications,Machine Learning and Data-Driven Applications
246,35,246_adapting_adaptation_tta_test,"['adapting', 'adaptation', 'tta', 'test', 'batches', 'batch', 'cta', 'memory', 'drift', 'ctta']","['adaptation', 'test', 'shifts', 'batch', 'entropy', 'distribution', 'pseudo', 'normalization', 'domain', 'source']","['  Given a model trained on source data, Test-Time Adaptation (TTA) enables\nadaptation and inference in test data streams with domain shifts from the\nsource. Current methods predominantly optimize the model for each incoming test\ndata batch using self-training loss. While these methods yield commendable\nresults in ideal test data streams, where batches are independently and\nidentically sampled from the target distribution, they falter under more\npractical test data streams that are not independent and identically\ndistributed (non-i.i.d.). The data batches in a non-i.i.d. stream display\nprominent label shifts relative to each other. It leads to conflicting\noptimization objectives among batches during the TTA process. Given the\ninherent risks of adapting the source model to unpredictable test-time\ndistributions, we reverse the adaptation process and propose a novel\nDistribution Alignment loss for TTA. This loss guides the distributions of\ntest-time features back towards the source distributions, which ensures\ncompatibility with the well-trained source model and eliminates the pitfalls\nassociated with conflicting optimization objectives. Moreover, we devise a\ndomain shift detection mechanism to extend the success of our proposed TTA\nmethod in the continual domain shift scenarios. Our extensive experiments\nvalidate the logic and efficacy of our method. On six benchmark datasets, we\nsurpass existing methods in non-i.i.d. scenarios and maintain competitive\nperformance under the ideal i.i.d. assumption.\n', '  Test Time Adaptation (TTA) addresses the problem of distribution shift by\nenabling pretrained models to learn new features on an unseen domain at test\ntime. However, it poses a significant challenge to maintain a balance between\nlearning new features and retaining useful pretrained features. In this paper,\nwe propose Layerwise EArly STopping (LEAST) for TTA to address this problem.\nThe key idea is to stop adapting individual layers during TTA if the features\nbeing learned do not appear beneficial for the new domain. For that purpose, we\npropose using a novel gradient-based metric to measure the relevance of the\ncurrent learnt features to the new domain without the need for supervised\nlabels. More specifically, we propose to use this metric to determine\ndynamically when to stop updating each layer during TTA. This enables a more\nbalanced adaptation, restricted to layers benefiting from it, and only for a\ncertain number of steps. Such an approach also has the added effect of limiting\nthe forgetting of pretrained features useful for dealing with new domains.\nThrough extensive experiments, we demonstrate that Layerwise Early Stopping\nimproves the performance of existing TTA approaches across multiple datasets,\ndomain shifts, model architectures, and TTA losses.\n', ""  This paper proposes a novel online evaluation protocol for Test Time\nAdaptation (TTA) methods, which penalizes slower methods by providing them with\nfewer samples for adaptation. TTA methods leverage unlabeled data at test time\nto adapt to distribution shifts. Although many effective methods have been\nproposed, their impressive performance usually comes at the cost of\nsignificantly increased computation budgets. Current evaluation protocols\noverlook the effect of this extra computation cost, affecting their real-world\napplicability. To address this issue, we propose a more realistic evaluation\nprotocol for TTA methods, where data is received in an online fashion from a\nconstant-speed data stream, thereby accounting for the method's adaptation\nspeed. We apply our proposed protocol to benchmark several TTA methods on\nmultiple datasets and scenarios. Extensive experiments show that, when\naccounting for inference speed, simple and fast approaches can outperform more\nsophisticated but slower methods. For example, SHOT from 2020, outperforms the\nstate-of-the-art method SAR from 2023 in this setting. Our results reveal the\nimportance of developing practical TTA methods that are both accurate and\nefficient.\n""]",Test-Time Adaptation (TTA) Methods,Efficient Model Adaptation Techniques,Advanced Statistical and Machine Learning Methods,Advanced Statistical and Machine Learning Methods
247,35,247_perceptrons_neural_networks_imagenet1k,"['perceptrons', 'neural', 'networks', 'imagenet1k', 'neurons', 'perceptron', 'kolmogorov', 'learnable', 'convolutional', 'fcnns']","['kolmogorov', 'functions', 'polynomials', 'perceptrons', 'spline', 'wavelet', 'basis', 'layer', 'activation', 'alternative']","['  Kolmogorov-Arnold Networks (KANs) offer an efficient and interpretable\nalternative to traditional multi-layer perceptron (MLP) architectures due to\ntheir finite network topology. However, according to the results of Kolmogorov\nand Vitushkin, the representation of generic smooth functions by KAN\nimplementations using analytic functions constrained to a finite number of\ncutoff points cannot be exact. Hence, the convergence of KAN throughout the\ntraining process may be limited. This paper explores the relevance of\nsmoothness in KANs, proposing that smooth, structurally informed KANs can\nachieve equivalence to MLPs in specific function classes. By leveraging\ninherent structural knowledge, KANs may reduce the data required for training\nand mitigate the risk of generating hallucinated predictions, thereby enhancing\nmodel reliability and performance in computational biomedicine.\n', '  Kolmogorov-Arnold Networks (KANs) introduce a paradigm of neural modeling\nthat implements learnable functions on the edges of the networks, diverging\nfrom the traditional node-centric activations in neural networks. This work\nassesses the applicability and efficacy of KANs in visual modeling, focusing on\nthe image recognition task. We mainly analyze the performance and efficiency of\ndifferent network architectures built using KAN concepts along with\nconventional building blocks of convolutional and linear layers, enabling a\ncomparative analysis with the conventional models. Our findings are aimed at\ncontributing to understanding the potential of KANs in computer vision,\nhighlighting both their strengths and areas for further research. Our\nevaluation shows that whereas KAN-based architectures perform in-line with the\noriginal claims of KAN paper for performance and model-complexity in the case\nof simpler vision datasets like MNIST, the advantages seem to diminish even for\nslightly more complex datasets like CIFAR-10.\n', '  Recently, Kolmogorov-Arnold Networks (KANs) have been proposed as an\nalternative to multilayer perceptrons, suggesting advantages in performance and\ninterpretability. We study a typical binary event classification task in\nhigh-energy physics including high-level features and comment on the\nperformance and interpretability of KANs in this context. We find that the\nlearned activation functions of a one-layer KAN resemble the log-likelihood\nratio of the input features. In deeper KANs, the activations in the first KAN\nlayer differ from those in the one-layer KAN, which indicates that the deeper\nKANs learn more complex representations of the data. We study KANs with\ndifferent depths and widths and we compare them to multilayer perceptrons in\nterms of performance and number of trainable parameters. For the chosen\nclassification task, we do not find that KANs are more parameter efficient.\nHowever, small KANs may offer advantages in terms of interpretability that come\nat the cost of only a moderate loss in performance.\n']",Kolmogorov-Arnold Networks (KANs) Analysis,Machine Learning Theory and Methods,Machine Learning and Artificial Intelligence,Machine Learning and Intelligent Systems
248,34,248_spingnn_spin_ferromagnetic_isingnets,"['spingnn', 'spin', 'ferromagnetic', 'isingnets', 'magnetization', 'boltzmann', 'lattice', 'magnetic', 'neural', 'networks']","['spin', 'lattice', 'gauge', 'magnetic', 'glass', 'transition', 'phase', 'temperature', 'ground', 'percolation']","['  Frustrated itinerant magnets often exhibit complex noncollinear or\nnoncoplanar magnetic orders which support topological electronic structures. A\ncanonical example is the anomalous quantum Hall state with a chiral spin order\nstabilized by electron-spin interactions on a triangular lattice. While a\nlong-range magnetic order cannot survive thermal fluctuations in two\ndimensions, the chiral order which results from the breaking of a discrete\nIsing symmetry persists even at finite temperatures. We present a scalable\nmachine learning (ML) framework to model the complex electron-mediated\nspin-spin interactions that stabilize the chiral magnetic domains in a\ntriangular lattice. Large-scale dynamical simulations, enabled by the ML\nforce-field models, are performed to investigate the coarsening of chiral\ndomains after a thermal quench. While the chiral phase is described by a broken\n$Z_2$ Ising-type symmetry, we find that the characteristic size of chiral\ndomains increases linearly with time, in stark contrast to the expected\nAllen-Cahn domain growth law for a non-conserved Ising order parameter field.\nThe linear growth of the chiral domains is attributed to the orientational\nanisotropy of domain boundaries. Our work also demonstrates the promising\npotential of ML models for large-scale spin dynamics of itinerant magnets.\n', ""  We explore a one-to-one correspondence between a neural network (NN) and a\nstatistical mechanical spin model where neurons are mapped to Ising spins and\nweights to spin-spin couplings. The process of training an NN produces a family\nof spin Hamiltonians parameterized by training time. We study the magnetic\nphases and the melting transition temperature as training progresses. First, we\nprove analytically that the common initial state before training--an NN with\nindependent random weights--maps to a layered version of the classical\nSherrington-Kirkpatrick spin glass exhibiting a replica symmetry breaking. The\nspin-glass-to-paramagnet transition temperature is calculated. Further, we use\nthe Thouless-Anderson-Palmer (TAP) equations--a theoretical technique to\nanalyze the landscape of energy minima of random systems--to determine the\nevolution of the magnetic phases on two types of NNs (one with continuous and\none with binarized activations) trained on the MNIST dataset. The two NN types\ngive rise to similar results, showing a quick destruction of the spin glass and\nthe appearance of a phase with a hidden order, whose melting transition\ntemperature $T_c$ grows as a power law in training time. We also discuss the\nproperties of the spectrum of the spin system's bond matrix in the context of\nrich vs. lazy learning. We suggest that this statistical mechanical view of NNs\nprovides a useful unifying perspective on the training process, which can be\nviewed as selecting and strengthening a symmetry-broken state associated with\nthe training task.\n"", '  Many deep neural networks have been used to solve Ising models, including\nautoregressive neural networks, convolutional neural networks, recurrent neural\nnetworks, and graph neural networks. Learning a probability distribution of\nenergy configuration or finding the ground states of a disordered, fully\nconnected Ising model is essential for statistical mechanics and NP-hard\nproblems. Despite tremendous efforts, a neural network architecture with the\nability to high-accurately solve these fully connected and extremely\nintractable problems on larger systems is still lacking. Here we propose a\nvariational autoregressive architecture with a message passing mechanism, which\ncan effectively utilize the interactions between spin variables. The new\nnetwork trained under an annealing framework outperforms existing methods in\nsolving several prototypical Ising spin Hamiltonians, especially for larger\nspin systems at low temperatures. The advantages also come from the great\nmitigation of mode collapse during the training process of deep neural\nnetworks. Considering these extremely difficult problems to be solved, our\nmethod extends the current computational limits of unsupervised neural networks\nto solve combinatorial optimization problems.\n']",Spin Systems and Neural Networks,Quantum and Neuromorphic Computing,Computational Models of Complex Systems and Processes,Computational Models of Complex Systems and Processes
249,34,249_decoders_digits_transformers_encodings,"['decoders', 'digits', 'transformers', 'encodings', 'rnns', 'turing', 'digit', 'decoder', 'addition', 'transformer']","['transformers', 'transformer', 'digit', 'numbers', 'arithmetic', 'length', 'multiplication', 'position', 'scratchpad', 'integers']","['  The poor performance of transformers on arithmetic tasks seems to stem in\nlarge part from their inability to keep track of the exact position of each\ndigit inside of a large span of digits. We mend this problem by adding an\nembedding to each digit that encodes its position relative to the start of the\nnumber. In addition to the boost these embeddings provide on their own, we show\nthat this fix enables architectural modifications such as input injection and\nrecurrent layers to improve performance even further.\n  With positions resolved, we can study the logical extrapolation ability of\ntransformers. Can they solve arithmetic problems that are larger and more\ncomplex than those in their training data? We find that training on only 20\ndigit numbers with a single GPU for one day, we can reach state-of-the-art\nperformance, achieving up to 99% accuracy on 100 digit addition problems.\nFinally, we show that these gains in numeracy also unlock improvements on other\nmulti-step reasoning tasks including sorting and multiplication.\n', '  Length generalization refers to the ability to extrapolate from short\ntraining sequences to long test sequences and is a challenge for current large\nlanguage models. While prior work has proposed some architecture or data format\nchanges to achieve length generalization, these proposals typically apply to a\nlimited set of tasks. Building on prior scratchpad and Chain-of-Thought (CoT)\ntechniques, we propose Turing Programs, a novel CoT strategy that decomposes an\nalgorithmic task into steps mimicking the computation of a Turing Machine. This\nframework is both universal, as it can accommodate any algorithmic task, and\nsimple, requiring only copying text from the context with small modifications.\nWe show that by using Turing Programs, we obtain robust length generalization\non a range of algorithmic tasks: addition, multiplication and in-context SGD.\nWe then demonstrate that transformers achieve length generalization on random\nTuring Programs, suggesting that length generalization is possible for any\nalgorithmic task. Finally, we theoretically prove that transformers can\nimplement Turing Programs, constructing a simple RASP (Weiss et al.) program\nthat simulates an arbitrary Turing machine.\n', '  Despite the success of Transformers on language understanding, code\ngeneration, and logical reasoning, they still fail to generalize over length on\nbasic arithmetic tasks such as addition and multiplication. A major reason\nbehind this failure is the vast difference in structure between numbers and\ntext; For example, the numbers are typically parsed from right to left, and\nthere is a correspondence between digits at the same position across different\nnumbers. In contrast, for text, such symmetries are quite unnatural. In this\nwork, we propose to encode these semantics explicitly into the model via\nmodified number formatting and custom positional encodings. Empirically, our\nmethod allows a Transformer trained on numbers with at most 5-digits for\naddition and multiplication to generalize up to 50-digit numbers, without using\nadditional data for longer sequences. We further demonstrate that traditional\nabsolute positional encodings (APE) fail to generalize to longer sequences,\neven when trained with augmented data that captures task symmetries. To\nelucidate the importance of explicitly encoding structure, we prove that\nexplicit incorporation of structure via positional encodings is necessary for\nout-of-distribution generalization. Finally, we pinpoint other challenges\ninherent to length generalization beyond capturing symmetries, in particular\ncomplexity of the underlying task, and propose changes in the training\ndistribution to address them.\n']",Improving Transformers for Arithmetic Tasks with Positional Encodings,Advances in Sequence Modeling: Transformers and Alternatives,Advances in Sequence Modeling and Learning,Advances in Sequence Modeling and Learning
250,34,250_deepfake_deepfakes_multimodal_faceforensics,"['deepfake', 'deepfakes', 'multimodal', 'faceforensics', 'detecting', 'detection', 'embedders', 'dataset', 'datasets', 'faces']","['deepfake', 'deepfakes', 'fake', 'detection', 'facial', 'detectors', 'images', 'videos', 'image', 'synthetic']","['  We study universal deepfake detection. Our goal is to detect synthetic images\nfrom a range of generative AI approaches, particularly from emerging ones which\nare unseen during training of the deepfake detector. Universal deepfake\ndetection requires outstanding generalization capability. Motivated by recently\nproposed masked image modeling which has demonstrated excellent generalization\nin self-supervised pre-training, we make the first attempt to explore masked\nimage modeling for universal deepfake detection. We study spatial and frequency\ndomain masking in training deepfake detectors. Based on empirical analysis, we\npropose a novel deepfake detector via frequency masking. Our focus on frequency\ndomain is different from the majority, which primarily target spatial domain\ndetection. Our comparative analyses reveal substantial performance gains over\nexisting methods. Code and models are publicly available.\n', '  In recent years, the abuse of a face swap technique called deepfake has\nraised enormous public concerns. So far, a large number of deepfake videos\n(known as ""deepfakes"") have been crafted and uploaded to the internet, calling\nfor effective countermeasures. One promising countermeasure against deepfakes\nis deepfake detection. Several deepfake datasets have been released to support\nthe training and testing of deepfake detectors, such as DeepfakeDetection and\nFaceForensics++. While this has greatly advanced deepfake detection, most of\nthe real videos in these datasets are filmed with a few volunteer actors in\nlimited scenes, and the fake videos are crafted by researchers using a few\npopular deepfake softwares. Detectors developed on these datasets may become\nless effective against real-world deepfakes on the internet. To better support\ndetection against real-world deepfakes, in this paper, we introduce a new\ndataset WildDeepfake which consists of 7,314 face sequences extracted from 707\ndeepfake videos collected completely from the internet. WildDeepfake is a small\ndataset that can be used, in addition to existing datasets, to develop and test\nthe effectiveness of deepfake detectors against real-world deepfakes. We\nconduct a systematic evaluation of a set of baseline detection networks on both\nexisting and our WildDeepfake datasets, and show that WildDeepfake is indeed a\nmore challenging dataset, where the detection performance can decrease\ndrastically. We also propose two (eg. 2D and 3D) Attention-based Deepfake\nDetection Networks (ADDNets) to leverage the attention masks on real/fake faces\nfor improved detection. We empirically verify the effectiveness of ADDNets on\nboth existing datasets and WildDeepfake. The dataset is available at:\nhttps://github.com/OpenTAI/wild-deepfake.\n', '  Deepfake is a generative deep learning algorithm that creates or changes\nfacial features in a very realistic way making it hard to differentiate the\nreal from the fake features It can be used to make movies look better as well\nas to spread false information by imitating famous people In this paper many\ndifferent ways to make a Deepfake are explained analyzed and separated\ncategorically Using Deepfake datasets models are trained and tested for\nreliability through experiments Deepfakes are a type of facial manipulation\nthat allow people to change their entire faces identities attributes and\nexpressions The trends in the available Deepfake datasets are also discussed\nwith a focus on how they have changed Using Deep learning a general Deepfake\ndetection model is made Moreover the problems in making and detecting Deepfakes\nare also mentioned As a result of this survey it is expected that the\ndevelopment of new Deepfake based imaging tools will speed up in the future\nThis survey gives indepth review of methods for manipulating images of face and\nvarious techniques to spot altered face images Four types of facial\nmanipulation are specifically discussed which are attribute manipulation\nexpression swap entire face synthesis and identity swap Across every\nmanipulation category we yield information on manipulation techniques\nsignificant benchmarks for technical evaluation of counterfeit detection\ntechniques available public databases and a summary of the outcomes of all such\nanalyses From all of the topics in the survey we focus on the most recent\ndevelopment of Deepfake showing its advances and obstacles in detecting fake\nimages\n']",Deepfake Detection and Analysis,Deep Learning Security and Efficiency,Deep Learning Optimization and Security,Deep Learning Methodologies
251,34,251_wireless_decoding_transmit_bandwidth,"['wireless', 'decoding', 'transmit', 'bandwidth', 'channels', 'channel', 'broadcast', 'modulation', 'aircomp', 'quantization']","['wireless', 'uplink', 'communication', 'air', 'devices', 'channel', 'device', 'scheme', 'transmission', 'beamforming']","['  In this paper, we quantitatively compare these two effective communication\nschemes, i.e., digital and analog ones, for wireless federated learning (FL)\nover resource-constrained networks, highlighting their essential differences as\nwell as their respective application scenarios. We first examine both digital\nand analog transmission methods, together with a unified and fair comparison\nscheme under practical constraints. A universal convergence analysis under\nvarious imperfections is established for FL performance evaluation in wireless\nnetworks. These analytical results reveal that the fundamental difference\nbetween the two paradigms lies in whether communication and computation are\njointly designed or not. The digital schemes decouple the communication design\nfrom specific FL tasks, making it difficult to support simultaneous uplink\ntransmission of massive devices with limited bandwidth. In contrast, the analog\ncommunication allows over-the-air computation (AirComp), thus achieving\nefficient spectrum utilization. However, computation-oriented analog\ntransmission reduces power efficiency, and its performance is sensitive to\ncomputational errors. Finally, numerical simulations are conducted to verify\nthese theoretical observations.\n', '  In this paper, the performance optimization of federated learning (FL), when\ndeployed over a realistic wireless multiple-input multiple-output (MIMO)\ncommunication system with digital modulation and over-the-air computation\n(AirComp) is studied. In particular, a MIMO system is considered in which edge\ndevices transmit their local FL models (trained using their locally collected\ndata) to a parameter server (PS) using beamforming to maximize the number of\ndevices scheduled for transmission. The PS, acting as a central controller,\ngenerates a global FL model using the received local FL models and broadcasts\nit back to all devices. Due to the limited bandwidth in a wireless network,\nAirComp is adopted to enable efficient wireless data aggregation. However,\nfading of wireless channels can produce aggregate distortions in an\nAirComp-based FL scheme. To tackle this challenge, we propose a modified\nfederated averaging (FedAvg) algorithm that combines digital modulation with\nAirComp to mitigate wireless fading while ensuring the communication\nefficiency. This is achieved by a joint transmit and receive beamforming\ndesign, which is formulated as an optimization problem to dynamically adjust\nthe beamforming matrices based on current FL model parameters so as to minimize\nthe transmitting error and ensure the FL performance. To achieve this goal, we\nfirst analytically characterize how the beamforming matrices affect the\nperformance of the FedAvg in different iterations. Based on this relationship,\nan artificial neural network (ANN) is used to estimate the local FL models of\nall devices and adjust the beamforming matrices at the PS for future model\ntransmission. The algorithmic advantages and improved performance of the\nproposed methodologies are demonstrated through extensive numerical\nexperiments.\n', '  Wireless federated learning (FL) relies on efficient uplink communications to\naggregate model updates across distributed edge devices. Over-the-air\ncomputation (a.k.a. AirComp) has emerged as a promising approach for addressing\nthe scalability challenge of FL over wireless links with limited communication\nresources. Unlike conventional methods, AirComp allows multiple edge devices to\ntransmit uplink signals simultaneously, enabling the parameter server to\ndirectly decode the average global model. However, existing AirComp solutions\nare intrinsically analog, while modern wireless systems predominantly adopt\ndigital modulations. Consequently, careful constellation designs are necessary\nto accurately decode the sum model updates without ambiguity. In this paper, we\npropose an end-to-end communication system supporting AirComp with digital\nmodulation, aiming to overcome the challenges associated with accurate decoding\nof the sum signal with constellation designs. We leverage autoencoder network\nstructures and explore the joint optimization of transmitter and receiver\ncomponents. Our approach fills an important gap in the context of accurately\ndecoding the sum signal in digital modulation-based AirComp, which can advance\nthe deployment of FL in contemporary wireless systems.\n']",Wireless Federated Learning with AirComp and Digital Modulation,Federated Learning,Machine Learning and Data Privacy,Machine Learning and Data Privacy
252,34,252_hydrological_hydrology_hydrologic_flooding,"['hydrological', 'hydrology', 'hydrologic', 'flooding', 'floods', 'forecasting', 'runoff', 'flood', 'catchment', 'catchments']","['hydrological', 'water', 'flood', 'streamflow', 'flooding', 'catchments', 'groundwater', 'hydrology', 'runoff', 'hydrologic']","[""  Prediction of dynamic environmental variables in unmonitored sites remains a\nlong-standing challenge for water resources science. The majority of the\nworld's freshwater resources have inadequate monitoring of critical\nenvironmental variables needed for management. Yet, the need to have widespread\npredictions of hydrological variables such as river flow and water quality has\nbecome increasingly urgent due to climate and land use change over the past\ndecades, and their associated impacts on water resources. Modern machine\nlearning methods increasingly outperform their process-based and empirical\nmodel counterparts for hydrologic time series prediction with their ability to\nextract information from large, diverse data sets. We review relevant\nstate-of-the art applications of machine learning for streamflow, water\nquality, and other water resources prediction and discuss opportunities to\nimprove the use of machine learning with emerging methods for incorporating\nwatershed characteristics into deep learning models, transfer learning, and\nincorporating process knowledge into machine learning models. The analysis here\nsuggests most prior efforts have been focused on deep learning learning\nframeworks built on many sites for predictions at daily time scales in the\nUnited States, but that comparisons between different classes of machine\nlearning methods are few and inadequate. We identify several open questions for\ntime series predictions in unmonitored sites that include incorporating dynamic\ninputs and site characteristics, mechanistic understanding and spatial context,\nand explainable AI techniques in modern machine learning frameworks.\n"", '  In recent years, climate extremes such as floods have created significant\nenvironmental and economic hazards for Australia, causing damage to the\nenvironment and economy and losses of human and animal lives. An efficient\nmethod of forecasting floods is crucial to limit this damage. Techniques for\nflood prediction are currently based on hydrological, and hydrodynamic\n(physically-based) numerical models. Machine learning methods that include deep\nlearning offer certain advantages over conventional physically based\napproaches, including flexibility and accuracy. Deep learning methods have been\npromising for predicting small to medium-sized climate extreme events over a\nshort time horizon; however, large flooding events present a critical\nchallenge. We present an ensemble-based machine learning approach that\naddresses large-scale extreme flooding challenges using a switching mechanism\nmotivated by extreme-value theory for long-short-term-memory (LSTM) deep\nlearning models. We use a multivariate and multi-step time-series prediction\napproach to predict streamflow for multiple days ahead in the major catchments\nof Australia. The ensemble framework also employs static information to enrich\nthe time-series information, allowing for regional modelling across catchments.\nOur results demonstrate enhanced prediction of streamflow extremes, with\nnotable efficacy for large flooding scenarios in the selected Australian\ncatchments. Through comparative analysis, our methodology underscores the\npotential for deep learning models to revolutionise flood forecasting across\ndiverse regions.\n', '  The application of process-based and data-driven hydrological models is\ncrucial in modern hydrological research, especially for predicting key water\ncycle variables such as runoff, evapotranspiration (ET), and soil moisture.\nThese models provide a scientific basis for water resource management, flood\nforecasting, and ecological protection. Process-based models simulate the\nphysical mechanisms of watershed hydrological processes, while data-driven\nmodels leverage large datasets and advanced machine learning algorithms. This\npaper reviewed and compared methods for assessing and enhancing the\nextrapolability of both model types, discussing their prospects and\nlimitations. Key strategies include the use of leave-one-out cross-validation\nand similarity-based methods to evaluate model performance in ungauged regions.\nDeep learning, transfer learning, and domain adaptation techniques are also\npromising in their potential to improve model predictions in data-sparse and\nextreme conditions. Interdisciplinary collaboration and continuous algorithmic\nadvancements are also important to strengthen the global applicability and\nreliability of hydrological models.\n']",Hydrological Modeling and Flood Forecasting,Advanced Computing Methods for Environmental Prediction and Modeling,Environmental Modeling and Prediction using Advanced Computing and AI,Environmental Modeling and Prediction using Advanced Computing and AI
253,34,253_conversational_retrieval_conversations_conversation,"['conversational', 'retrieval', 'conversations', 'conversation', 'searchers', 'search', 'dialogue', 'queries', 'chat', 'seeking']","['conversational', 'search', 'query', 'queries', 'reformulation', 'retrieval', 'engines', 'rewrites', 'conversation', 'dense']","[""  With large language models (LLMs), conversational search engines shift how\nusers retrieve information from the web by enabling natural conversations to\nexpress their search intents over multiple turns. Users' natural conversation\nembodies rich but implicit signals of users' search intents and evaluation of\nsearch results to understand user experience with the system. However, it is\nunderexplored how and why users ask follow-up queries to continue conversations\nwith conversational search engines and how the follow-up queries signal users'\nsatisfaction. From qualitative analysis of 250 conversational turns from an\nin-lab user evaluation of Naver Cue:, a commercial conversational search\nengine, we propose a taxonomy of 18 users' follow-up query patterns from\nconversational search, comprising two major axes: (1) users' motivations behind\ncontinuing conversations (N = 7) and (2) actions of follow-up queries (N = 11).\nCompared to the existing literature on query reformulations, we uncovered a new\nset of motivations and actions behind follow-up queries, including asking for\nsubjective opinions or providing natural language feedback on the engine's\nresponses. To analyze conversational search logs with our taxonomy in a\nscalable and efficient manner, we built an LLM-powered classifier (73%\naccuracy). With our classifier, we analyzed 2,061 conversational tuples\ncollected from real-world usage logs of Cue: and examined how the conversation\npatterns from our taxonomy correlates with satisfaction. Our initial findings\nsuggest some signals of dissatisfactions, such as Clarifying Queries, Excluding\nCondition, and Substituting Condition with follow-up queries. We envision our\napproach could contribute to automated evaluation of conversation search\nexperience by providing satisfaction signals and grounds for realistic user\nsimulations.\n"", '  Conversational search facilitates complex information retrieval by enabling\nmulti-turn interactions between users and the system. Supporting such\ninteractions requires a comprehensive understanding of the conversational\ninputs to formulate a good search query based on historical information. In\nparticular, the search query should include the relevant information from the\nprevious conversation turns. However, current approaches for conversational\ndense retrieval primarily rely on fine-tuning a pre-trained ad-hoc retriever\nusing the whole conversational search session, which can be lengthy and noisy.\nMoreover, existing approaches are limited by the amount of manual supervision\nsignals in the existing datasets. To address the aforementioned issues, we\npropose a History-Aware Conversational Dense Retrieval (HAConvDR) system, which\nincorporates two ideas: context-denoised query reformulation and automatic\nmining of supervision signals based on the actual impact of historical turns.\nExperiments on two public conversational search datasets demonstrate the\nimproved history modeling capability of HAConvDR, in particular for long\nconversations with topic shifts.\n', '  Conversational search supports multi-turn user-system interactions to solve\ncomplex information needs. Different from the traditional single-turn ad-hoc\nsearch, conversational search encounters a more challenging problem of\ncontext-dependent query understanding with the lengthy and long-tail\nconversational history context. While conversational query rewriting methods\nleverage explicit rewritten queries to train a rewriting model to transform the\ncontext-dependent query into a stand-stone search query, this is usually done\nwithout considering the quality of search results. Conversational dense\nretrieval methods use fine-tuning to improve a pre-trained ad-hoc query\nencoder, but they are limited by the conversational search data available for\ntraining. In this paper, we leverage both rewritten queries and relevance\njudgments in the conversational search data to train a better query\nrepresentation model. The key idea is to align the query representation with\nthose of rewritten queries and relevant documents. The proposed model -- Query\nRepresentation Alignment Conversational Dense Retriever, QRACDR, is tested on\neight datasets, including various settings in conversational search and ad-hoc\nsearch. The results demonstrate the strong performance of QRACDR compared with\nstate-of-the-art methods, and confirm the effectiveness of representation\nalignment.\n']",Conversational Search and Retrieval,Conversational AI and Empathy in Human-Computer Interaction,Conversational AI and Human-Computer Interaction,Conversational AI and Human-Computer Interaction
254,34,254_supervised_segmentation_segmenting_classifier,"['supervised', 'segmentation', 'segmenting', 'classifier', 'classes', 'foreground', 'masks', 'coco', 'labels', 'background']","['segmentation', 'pseudo', 'object', 'weakly', 'masks', 'labels', 'pixel', 'supervised', 'semantic', 'class']","['  Generating reliable pseudo masks from image-level labels is challenging in\nthe weakly supervised semantic segmentation (WSSS) task due to the lack of\nspatial information. Prevalent class activation map (CAM)-based solutions are\nchallenged to discriminate the foreground (FG) objects from the suspicious\nbackground (BG) pixels (a.k.a. co-occurring) and learn the integral object\nregions. This paper proposes a simple fine-grained background representation\n(FBR) method to discover and represent diverse BG semantics and address the\nco-occurring problems. We abandon using the class prototype or pixel-level\nfeatures for BG representation. Instead, we develop a novel primitive, negative\nregion of interest (NROI), to capture the fine-grained BG semantic information\nand conduct the pixel-to-NROI contrast to distinguish the confusing BG pixels.\nWe also present an active sampling strategy to mine the FG negatives\non-the-fly, enabling efficient pixel-to-pixel intra-foreground contrastive\nlearning to activate the entire object region. Thanks to the simplicity of\ndesign and convenience in use, our proposed method can be seamlessly plugged\ninto various models, yielding new state-of-the-art results under various WSSS\nsettings across benchmarks. Leveraging solely image-level (I) labels as\nsupervision, our method achieves 73.2 mIoU and 45.6 mIoU segmentation results\non Pascal Voc and MS COCO test sets, respectively. Furthermore, by\nincorporating saliency maps as an additional supervision signal (I+S), we\nattain 74.9 mIoU on Pascal Voc test set. Concurrently, our FBR approach\ndemonstrates meaningful performance gains in weakly-supervised instance\nsegmentation (WSIS) tasks, showcasing its robustness and strong generalization\ncapabilities across diverse domains.\n', '  Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation\nmodels using image data with only image-level supervision. Since precise\npixel-level annotations are not accessible, existing methods typically focus on\nproducing pseudo masks for training segmentation models by refining CAM-like\nheatmaps. However, the produced heatmaps may capture only the discriminative\nimage regions of object categories or the associated co-occurring backgrounds.\nTo address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS)\nframework, which learns to effectively prompt the CLIP latent space to enhance\nthe semantic alignment between the segmented regions and the target object\ncategories. More specifically, we propose Contrastive Prompt Learning and\nPrompt-guided Semantic Refinement to learn the prompts that adequately describe\nand suppress the co-occurring backgrounds associated with each target object\ncategory. In this way, SemPLeS can perform better semantic alignment between\nobject regions and the associated class labels, resulting in desired pseudo\nmasks for training the segmentation model. The proposed SemPLeS framework\nachieves SOTA performance on the standard WSSS benchmarks, PASCAL VOC and MS\nCOCO, and shows compatibility with other WSSS methods. The source codes are\nprovided in the supplementary.\n', '  Image-level weakly-supervised semantic segmentation (WSSS) reduces the\nusually vast data annotation cost by surrogate segmentation masks during\ntraining. The typical approach involves training an image classification\nnetwork using global average pooling (GAP) on convolutional feature maps. This\nenables the estimation of object locations based on class activation maps\n(CAMs), which identify the importance of image regions. The CAMs are then used\nto generate pseudo-labels, in the form of segmentation masks, to supervise a\nsegmentation model in the absence of pixel-level ground truth. Our work is\nbased on two techniques for improving CAMs; importance sampling, which is a\nsubstitute for GAP, and the feature similarity loss, which utilizes a heuristic\nthat object contours almost always align with color edges in images. However,\nboth are based on the multinomial posterior with softmax, and implicitly assume\nthat classes are mutually exclusive, which turns out suboptimal in our\nexperiments. Thus, we reformulate both techniques based on binomial posteriors\nof multiple independent binary problems. This has two benefits; their\nperformance is improved and they become more general, resulting in an add-on\nmethod that can boost virtually any WSSS method. This is demonstrated on a wide\nvariety of baselines on the PASCAL VOC dataset, improving the region similarity\nand contour quality of all implemented state-of-the-art methods. Experiments on\nthe MS COCO dataset further show that our proposed add-on is well-suited for\nlarge-scale settings. Our code implementation is available at\nhttps://github.com/arvijj/hfpl.\n']",Weakly-Supervised Semantic Segmentation,Weakly Supervised Computer Vision,Computer Vision,Computer Vision
255,34,255_gaze_eyedentify_eye_eyes,"['gaze', 'eyedentify', 'eye', 'eyes', 'ocular', 'saliency', 'pupil', 'visual', 'attention', 'viewing']","['gaze', 'eye', 'tracking', 'pupil', 'saliency', 'scanpath', 'fixation', 'driver', 'estimation', 'saccades']","[""  Predicting human gaze behavior within computer vision is integral for\ndeveloping interactive systems that can anticipate user attention, address\nfundamental questions in cognitive science, and hold implications for fields\nlike human-computer interaction (HCI) and augmented/virtual reality (AR/VR)\nsystems. Despite methodologies introduced for modeling human eye gaze behavior,\napplying these models to medical imaging for scanpath prediction remains\nunexplored. Our proposed system aims to predict eye gaze sequences from\nradiology reports and CXR images, potentially streamlining data collection and\nenhancing AI systems using larger datasets. However, predicting human scanpaths\non medical images presents unique challenges due to the diverse nature of\nabnormal regions. Our model predicts fixation coordinates and durations\ncritical for medical scanpath prediction, outperforming existing models in the\ncomputer vision community. Utilizing a two-stage training process and large\npublicly available datasets, our approach generates static heatmaps and eye\ngaze videos aligned with radiology reports, facilitating comprehensive\nanalysis. We validate our approach by comparing its performance with\nstate-of-the-art methods and assessing its generalizability among different\nradiologists, introducing novel strategies to model radiologists' search\npatterns during CXR image diagnosis. Based on the radiologist's evaluation,\nMedGaze can generate human-like gaze sequences with a high focus on relevant\nregions over the CXR images. It sometimes also outperforms humans in terms of\nredundancy and randomness in the scanpaths.\n"", '  We propose a novel neural pipeline, MSGazeNet, that learns gaze\nrepresentations by taking advantage of the eye anatomy information through a\nmultistream framework. Our proposed solution comprises two components, first a\nnetwork for isolating anatomical eye regions, and a second network for\nmultistream gaze estimation. The eye region isolation is performed with a U-Net\nstyle network which we train using a synthetic dataset that contains eye region\nmasks for the visible eyeball and the iris region. The synthetic dataset used\nin this stage is procured using the UnityEyes simulator, and consists of 80,000\neye images. Successive to training, the eye region isolation network is then\ntransferred to the real domain for generating masks for the real-world eye\nimages. In order to successfully make the transfer, we exploit domain\nrandomization in the training process, which allows for the synthetic images to\nbenefit from a larger variance with the help of augmentations that resemble\nartifacts. The generated eye region masks along with the raw eye images are\nthen used together as a multistream input to our gaze estimation network, which\nconsists of wide residual blocks. The output embeddings from these encoders are\nfused in the channel dimension before feeding into the gaze regression layers.\nWe evaluate our framework on three gaze estimation datasets and achieve strong\nperformances. Our method surpasses the state-of-the-art by 7.57% and 1.85% on\ntwo datasets, and obtains competitive results on the other. We also study the\nrobustness of our method with respect to the noise in the data and demonstrate\nthat our model is less sensitive to noisy data. Lastly, we perform a variety of\nexperiments including ablation studies to evaluate the contribution of\ndifferent components and design choices in our solution.\n', '  Eye-tracking applications that utilize the human gaze in video understanding\ntasks have become increasingly important. To effectively automate the process\nof video analysis based on eye-tracking data, it is important to accurately\nreplicate human gaze behavior. However, this task presents significant\nchallenges due to the inherent complexity and ambiguity of human gaze patterns.\nIn this work, we introduce a novel method for simulating human gaze behavior.\nOur approach uses a transformer-based reinforcement learning algorithm to train\nan agent that acts as a human observer, with the primary role of watching\nvideos and simulating human gaze behavior. We employed an eye-tracking dataset\ngathered from videos generated by the VirtualHome simulator, with a primary\nfocus on activity recognition. Our experimental results demonstrate the\neffectiveness of our gaze prediction method by highlighting its capability to\nreplicate human gaze behavior and its applicability for downstream tasks where\nreal human-gaze is used as input.\n']",Predicting Human Gaze Behavior,Eye Movement and Gaze in Human Behavior and Cognition,Eye and Vision Research,Eye and Vision Research
256,33,256_hyperspectral_multispectral_spectral_supervised,"['hyperspectral', 'multispectral', 'spectral', 'supervised', 'imagery', 'sensing', 'classification', 'denoising', 'hyperview', 'images']","['hyperspectral', 'spectral', 'unmixing', 'endmembers', 'spatial', 'multispectral', 'pixel', 'abundance', 'bands', 'abundances']","['  Land cover analysis using hyperspectral images (HSI) remains an open problem\ndue to their low spatial resolution and complex spectral information. Recent\nstudies are primarily dedicated to designing Transformer-based architectures\nfor spatial-spectral long-range dependencies modeling, which is computationally\nexpensive with quadratic complexity. Selective structured state space model\n(Mamba), which is efficient for modeling long-range dependencies with linear\ncomplexity, has recently shown promising progress. However, its potential in\nhyperspectral image processing that requires handling numerous spectral bands\nhas not yet been explored. In this paper, we innovatively propose S$^2$Mamba, a\nspatial-spectral state space model for hyperspectral image classification, to\nexcavate spatial-spectral contextual features, resulting in more efficient and\naccurate land cover analysis. In S$^2$Mamba, two selective structured state\nspace models through different dimensions are designed for feature extraction,\none for spatial, and the other for spectral, along with a spatial-spectral\nmixture gate for optimal fusion. More specifically, S$^2$Mamba first captures\nspatial contextual relations by interacting each pixel with its adjacent\nthrough a Patch Cross Scanning module and then explores semantic information\nfrom continuous spectral bands through a Bi-directional Spectral Scanning\nmodule. Considering the distinct expertise of the two attributes in homogenous\nand complicated texture scenes, we realize the Spatial-spectral Mixture Gate by\na group of learnable matrices, allowing for the adaptive incorporation of\nrepresentations learned across different dimensions. Extensive experiments\nconducted on HSI classification benchmarks demonstrate the superiority and\nprospect of S$^2$Mamba. The code will be made available at:\nhttps://github.com/PURE-melo/S2Mamba.\n', '  Hyperspectral images (HSIs) contain rich spectral and spatial information.\nMotivated by the success of transformers in the field of natural language\nprocessing and computer vision where they have shown the ability to learn long\nrange dependencies within input data, recent research has focused on using\ntransformers for HSIs. However, current state-of-the-art hyperspectral\ntransformers only tokenize the input HSI sample along the spectral dimension,\nresulting in the under-utilization of spatial information. Moreover,\ntransformers are known to be data-hungry and their performance relies heavily\non large-scale pretraining, which is challenging due to limited annotated\nhyperspectral data. Therefore, the full potential of HSI transformers has not\nbeen fully realized. To overcome these limitations, we propose a novel\nfactorized spectral-spatial transformer that incorporates factorized\nself-supervised pretraining procedures, leading to significant improvements in\nperformance. The factorization of the inputs allows the spectral and spatial\ntransformers to better capture the interactions within the hyperspectral data\ncubes. Inspired by masked image modeling pretraining, we also devise efficient\nmasking strategies for pretraining each of the spectral and spatial\ntransformers. We conduct experiments on six publicly available datasets for HSI\nclassification task and demonstrate that our model achieves state-of-the-art\nperformance in all the datasets. The code for our model will be made available\nat https://github.com/csiro-robotics/factoformer.\n', '  Contrastive learning has demonstrated great effectiveness in representation\nlearning especially for image classification tasks. However, there is still a\nshortage in the studies targeting regression tasks, and more specifically\napplications on hyperspectral data. In this paper, we propose a contrastive\nlearning framework for the regression tasks for hyperspectral data. To this\nend, we provide a collection of transformations relevant for augmenting\nhyperspectral data, and investigate contrastive learning for regression.\nExperiments on synthetic and real hyperspectral datasets show that the proposed\nframework and transformations significantly improve the performance of\nregression models, achieving better scores than other state-of-the-art\ntransformations.\n']",Hyperspectral Image Analysis and Classification,Multimodal Data Analysis for Environmental and Biological Applications,Multimodal Learning and Applications,Multimodal Learning and Vision-Language Models
257,33,257_voice_audio_vocalizing_enhancement,"['voice', 'audio', 'vocalizing', 'enhancement', 'speech', 'denoising', 'speaker', 'hearing', 'noisy', 'acoustic']","['speech', 'enhancement', 'separation', 'clean', 'audio', 'diffusion', 'noise', 'reverse', 'listening', 'diarization']","['  Speech quality estimation has recently undergone a paradigm shift from\nhuman-hearing expert designs to machine-learning models. However, current\nmodels rely mainly on supervised learning, which is time-consuming and\nexpensive for label collection. To solve this problem, we propose VQScore, a\nself-supervised metric for evaluating speech based on the quantization error of\na vector-quantized-variational autoencoder (VQ-VAE). The training of VQ-VAE\nrelies on clean speech; hence, large quantization errors can be expected when\nthe speech is distorted. To further improve correlation with real quality\nscores, domain knowledge of speech processing is incorporated into the model\ndesign. We found that the vector quantization mechanism could also be used for\nself-supervised speech enhancement (SE) model training. To improve the\nrobustness of the encoder for SE, a novel self-distillation mechanism combined\nwith adversarial training is introduced. In summary, the proposed speech\nquality estimation method and enhancement models require only clean speech for\ntraining without any label requirements. Experimental results show that the\nproposed VQScore and enhancement model are competitive with supervised\nbaselines. The code will be released after publication.\n', '  In this paper, we explore a continuous modeling approach for\ndeep-learning-based speech enhancement, focusing on the denoising process. We\nuse a state variable to indicate the denoising process. The starting state is\nnoisy speech and the ending state is clean speech. The noise component in the\nstate variable decreases with the change of the state index until the noise\ncomponent is 0. During training, a UNet-like neural network learns to estimate\nevery state variable sampled from the continuous denoising process. In testing,\nwe introduce a controlling factor as an embedding, ranging from zero to one, to\nthe neural network, allowing us to control the level of noise reduction. This\napproach enables controllable speech enhancement and is adaptable to various\napplication scenarios. Experimental results indicate that preserving a small\namount of noise in the clean target benefits speech enhancement, as evidenced\nby improvements in both objective speech measures and automatic speech\nrecognition performance.\n', '  Speech enhancement systems are typically trained using pairs of clean and\nnoisy speech. In audio-visual speech enhancement (AVSE), there is not as much\nground-truth clean data available; most audio-visual datasets are collected in\nreal-world environments with background noise and reverberation, hampering the\ndevelopment of AVSE. In this work, we introduce AV2Wav, a resynthesis-based\naudio-visual speech enhancement approach that can generate clean speech despite\nthe challenges of real-world training data. We obtain a subset of nearly clean\nspeech from an audio-visual corpus using a neural quality estimator, and then\ntrain a diffusion model on this subset to generate waveforms conditioned on\ncontinuous speech representations from AV-HuBERT with noise-robust training. We\nuse continuous rather than discrete representations to retain prosody and\nspeaker information. With this vocoding task alone, the model can perform\nspeech enhancement better than a masking-based baseline. We further fine-tune\nthe diffusion model on clean/noisy utterance pairs to improve the performance.\nOur approach outperforms a masking-based baseline in terms of both automatic\nmetrics and a human listening test and is close in quality to the target speech\nin the listening test. Audio samples can be found at\nhttps://home.ttic.edu/~jcchou/demo/avse/avse_demo.html.\n']",Speech Enhancement and Quality Estimation,Speech Processing and Recognition Systems,Speech and Audio Processing,Speech and Audio Processing
258,33,258_forecast_solar_forecasting_forecasts,"['forecast', 'solar', 'forecasting', 'forecasts', 'meteorological', 'photovoltaic', 'predicting', 'prediction', 'predict', 'renewable']","['solar', 'wind', 'weather', 'forecasting', 'power', 'irradiance', 'photovoltaic', 'energy', 'renewable', 'farms']","['  Weather forecasts from numerical weather prediction models play a central\nrole in solar energy forecasting, where a cascade of physics-based models is\nused in a model chain approach to convert forecasts of solar irradiance to\nsolar power production, using additional weather variables as auxiliary\ninformation. Ensemble weather forecasts aim to quantify uncertainty in the\nfuture development of the weather, and can be used to propagate this\nuncertainty through the model chain to generate probabilistic solar energy\npredictions. However, ensemble prediction systems are known to exhibit\nsystematic errors, and thus require post-processing to obtain accurate and\nreliable probabilistic forecasts. The overarching aim of our study is to\nsystematically evaluate different strategies to apply post-processing methods\nin model chain approaches: Not applying any post-processing at all;\npost-processing only the irradiance predictions before the conversion;\npost-processing only the solar power predictions obtained from the model chain;\nor applying post-processing in both steps. In a case study based on a benchmark\ndataset for the Jacumba solar plant in the U.S., we develop statistical and\nmachine learning methods for post-processing ensemble predictions of global\nhorizontal irradiance and solar power generation. Further, we propose a neural\nnetwork-based model for direct solar power forecasting that bypasses the model\nchain. Our results indicate that post-processing substantially improves the\nsolar power generation forecasts, in particular when post-processing is applied\nto the power predictions. The machine learning methods for post-processing\nyield slightly better probabilistic forecasts, and the direct forecasting\napproach performs comparable to the post-processing strategies.\n', ""  This project presents an extension to the GraphCast model, a state-of-the-art\ngraph neural network (GNN) for global weather forecasting, by integrating solar\nenergy production forecasting capabilities. The proposed approach leverages the\nweather forecasts generated by GraphCast and trains a neural network model to\npredict the ratio of actual solar output to potential solar output based on\nvarious weather conditions. The model architecture consists of an input layer\ncorresponding to weather features (temperature, humidity, dew point, wind\nspeed, rain, barometric pressure, and altitude), two hidden layers with ReLU\nactivations, and an output layer predicting solar radiation. The model is\ntrained using a mean absolute error loss function and Adam optimizer. The\nresults demonstrate the model's effectiveness in accurately predicting solar\nradiation, with its convergence behavior, decreasing training loss, and\naccurate prediction of solar radiation patterns suggesting successful learning\nof the underlying relationships between weather conditions and solar radiation.\nThe integration of solar energy production forecasting with GraphCast offers\nvaluable insights for the renewable energy sector, enabling better planning and\ndecision-making based on expected solar energy production. Future work could\nexplore further model refinements, incorporation of additional weather\nvariables, and extension to other renewable energy sources.\n"", '  The challenges in applications of solar energy lies in its intermittency and\ndependency on meteorological parameters such as; solar radiation, ambient\ntemperature, rainfall, wind-speed etc., and many other physical parameters like\ndust accumulation etc. Hence, it is important to estimate the amount of solar\nphotovoltaic (PV) power generation for a specific geographical location.\nMachine learning (ML) models have gained importance and are widely used for\nprediction of solar power plant performance. In this paper, the impact of\nweather parameters on solar PV power generation is estimated by several\nEnsemble ML (EML) models like Bagging, Boosting, Stacking, and Voting for the\nfirst time. The performance of chosen ML algorithms is validated by field\ndataset of a 10kWp solar PV power plant in Eastern India region. Furthermore, a\ncomplete test-bed framework has been designed for data mining as well as to\nselect appropriate learning models. It also supports feature selection and\nreduction for dataset to reduce space and time complexity of the learning\nmodels. The results demonstrate greater prediction accuracy of around 96% for\nStacking and Voting EML models. The proposed work is a generalized one and can\nbe very useful for predicting the performance of large-scale solar PV power\nplants also.\n']",Solar Energy Forecasting and Prediction,Energy Forecasting and Management,Predictive Modeling and Forecasting,Predictive Modeling and Forecasting
259,33,259_memorization_memorisation_memorized_memorise,"['memorization', 'memorisation', 'memorized', 'memorise', 'linguistic', 'neural', 'language', 'sentences', 'attentional', 'lexical']","['memorisation', 'memorization', 'linguistic', 'memorized', 'attentional', 'neurons', 'layers', 'sentences', 'lexical', 'language']","['  Large Language Models (LLMs), trained on massive corpora with billions of\nparameters, show unprecedented performance in various fields. Though surprised\nby their excellent performances, researchers also noticed some special\nbehaviors of those LLMs. One of those behaviors is memorization, in which LLMs\ncan generate the same content used to train them. Though previous research has\ndiscussed memorization, the memorization of LLMs still lacks explanation,\nespecially the cause of memorization and the dynamics of generating them. In\nthis research, we comprehensively discussed memorization from various\nperspectives and extended the discussion scope to not only just the memorized\ncontent but also less and unmemorized content. Through various studies, we\nfound that: (1) Through experiments, we revealed the relation of memorization\nbetween model size, continuation size, and context size. Further, we showed how\nunmemorized sentences transition to memorized sentences. (2) Through embedding\nanalysis, we showed the distribution and decoding dynamics across model size in\nembedding space for sentences with different memorization scores. The n-gram\nstatistics analysis presents d (3) An analysis over n-gram and entropy decoding\ndynamics discovered a boundary effect when the model starts to generate\nmemorized sentences or unmemorized sentences. (4)We trained a Transformer model\nto predict the memorization of different models, showing that it is possible to\npredict memorizations by context.\n', '  Memorisation is a natural part of learning from real-world data: neural\nmodels pick up on atypical input-output combinations and store those training\nexamples in their parameter space. That this happens is well-known, but how and\nwhere are questions that remain largely unanswered. Given a multi-layered\nneural model, where does memorisation occur in the millions of parameters?\nRelated work reports conflicting findings: a dominant hypothesis based on image\nclassification is that lower layers learn generalisable features and that\ndeeper layers specialise and memorise. Work from NLP suggests this does not\napply to language models, but has been mainly focused on memorisation of facts.\nWe expand the scope of the localisation question to 12 natural language\nclassification tasks and apply 4 memorisation localisation techniques. Our\nresults indicate that memorisation is a gradual process rather than a localised\none, establish that memorisation is task-dependent, and give nuance to the\ngeneralisation first, memorisation second hypothesis.\n', ""  Understanding memorisation in language models has practical and societal\nimplications, e.g., studying models' training dynamics or preventing copyright\ninfringements. Prior work defines memorisation as the causal effect of training\nwith an instance on the model's ability to predict that instance. This\ndefinition relies on a counterfactual: the ability to observe what would have\nhappened had the model not seen that instance. Existing methods struggle to\nprovide computationally efficient and accurate estimates of this\ncounterfactual. Further, they often estimate memorisation for a model\narchitecture rather than for a specific model instance. This paper fills an\nimportant gap in the literature, proposing a new, principled, and efficient\nmethod to estimate memorisation based on the difference-in-differences design\nfrom econometrics. Using this method, we characterise a model's memorisation\nprofile--its memorisation trends across training--by only observing its\nbehaviour on a small set of instances throughout training. In experiments with\nthe Pythia model suite, we find that memorisation (i) is stronger and more\npersistent in larger models, (ii) is determined by data order and learning\nrate, and (iii) has stable trends across model sizes, thus making memorisation\nin larger models predictable from smaller ones.\n""]",Memorization in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
259,33,259_memorization_memorisation_memorized_memorise,"['memorization', 'memorisation', 'memorized', 'memorise', 'linguistic', 'neural', 'language', 'sentences', 'attentional', 'lexical']","['memorisation', 'memorization', 'linguistic', 'memorized', 'attentional', 'neurons', 'layers', 'sentences', 'lexical', 'language']","['  Large Language Models (LLMs), trained on massive corpora with billions of\nparameters, show unprecedented performance in various fields. Though surprised\nby their excellent performances, researchers also noticed some special\nbehaviors of those LLMs. One of those behaviors is memorization, in which LLMs\ncan generate the same content used to train them. Though previous research has\ndiscussed memorization, the memorization of LLMs still lacks explanation,\nespecially the cause of memorization and the dynamics of generating them. In\nthis research, we comprehensively discussed memorization from various\nperspectives and extended the discussion scope to not only just the memorized\ncontent but also less and unmemorized content. Through various studies, we\nfound that: (1) Through experiments, we revealed the relation of memorization\nbetween model size, continuation size, and context size. Further, we showed how\nunmemorized sentences transition to memorized sentences. (2) Through embedding\nanalysis, we showed the distribution and decoding dynamics across model size in\nembedding space for sentences with different memorization scores. The n-gram\nstatistics analysis presents d (3) An analysis over n-gram and entropy decoding\ndynamics discovered a boundary effect when the model starts to generate\nmemorized sentences or unmemorized sentences. (4)We trained a Transformer model\nto predict the memorization of different models, showing that it is possible to\npredict memorizations by context.\n', '  Memorisation is a natural part of learning from real-world data: neural\nmodels pick up on atypical input-output combinations and store those training\nexamples in their parameter space. That this happens is well-known, but how and\nwhere are questions that remain largely unanswered. Given a multi-layered\nneural model, where does memorisation occur in the millions of parameters?\nRelated work reports conflicting findings: a dominant hypothesis based on image\nclassification is that lower layers learn generalisable features and that\ndeeper layers specialise and memorise. Work from NLP suggests this does not\napply to language models, but has been mainly focused on memorisation of facts.\nWe expand the scope of the localisation question to 12 natural language\nclassification tasks and apply 4 memorisation localisation techniques. Our\nresults indicate that memorisation is a gradual process rather than a localised\none, establish that memorisation is task-dependent, and give nuance to the\ngeneralisation first, memorisation second hypothesis.\n', ""  Understanding memorisation in language models has practical and societal\nimplications, e.g., studying models' training dynamics or preventing copyright\ninfringements. Prior work defines memorisation as the causal effect of training\nwith an instance on the model's ability to predict that instance. This\ndefinition relies on a counterfactual: the ability to observe what would have\nhappened had the model not seen that instance. Existing methods struggle to\nprovide computationally efficient and accurate estimates of this\ncounterfactual. Further, they often estimate memorisation for a model\narchitecture rather than for a specific model instance. This paper fills an\nimportant gap in the literature, proposing a new, principled, and efficient\nmethod to estimate memorisation based on the difference-in-differences design\nfrom econometrics. Using this method, we characterise a model's memorisation\nprofile--its memorisation trends across training--by only observing its\nbehaviour on a small set of instances throughout training. In experiments with\nthe Pythia model suite, we find that memorisation (i) is stronger and more\npersistent in larger models, (ii) is determined by data order and learning\nrate, and (iii) has stable trends across model sizes, thus making memorisation\nin larger models predictable from smaller ones.\n""]",Memorization in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
259,33,259_memorization_memorisation_memorized_memorise,"['memorization', 'memorisation', 'memorized', 'memorise', 'linguistic', 'neural', 'language', 'sentences', 'attentional', 'lexical']","['memorisation', 'memorization', 'linguistic', 'memorized', 'attentional', 'neurons', 'layers', 'sentences', 'lexical', 'language']","['  Large Language Models (LLMs), trained on massive corpora with billions of\nparameters, show unprecedented performance in various fields. Though surprised\nby their excellent performances, researchers also noticed some special\nbehaviors of those LLMs. One of those behaviors is memorization, in which LLMs\ncan generate the same content used to train them. Though previous research has\ndiscussed memorization, the memorization of LLMs still lacks explanation,\nespecially the cause of memorization and the dynamics of generating them. In\nthis research, we comprehensively discussed memorization from various\nperspectives and extended the discussion scope to not only just the memorized\ncontent but also less and unmemorized content. Through various studies, we\nfound that: (1) Through experiments, we revealed the relation of memorization\nbetween model size, continuation size, and context size. Further, we showed how\nunmemorized sentences transition to memorized sentences. (2) Through embedding\nanalysis, we showed the distribution and decoding dynamics across model size in\nembedding space for sentences with different memorization scores. The n-gram\nstatistics analysis presents d (3) An analysis over n-gram and entropy decoding\ndynamics discovered a boundary effect when the model starts to generate\nmemorized sentences or unmemorized sentences. (4)We trained a Transformer model\nto predict the memorization of different models, showing that it is possible to\npredict memorizations by context.\n', '  Memorisation is a natural part of learning from real-world data: neural\nmodels pick up on atypical input-output combinations and store those training\nexamples in their parameter space. That this happens is well-known, but how and\nwhere are questions that remain largely unanswered. Given a multi-layered\nneural model, where does memorisation occur in the millions of parameters?\nRelated work reports conflicting findings: a dominant hypothesis based on image\nclassification is that lower layers learn generalisable features and that\ndeeper layers specialise and memorise. Work from NLP suggests this does not\napply to language models, but has been mainly focused on memorisation of facts.\nWe expand the scope of the localisation question to 12 natural language\nclassification tasks and apply 4 memorisation localisation techniques. Our\nresults indicate that memorisation is a gradual process rather than a localised\none, establish that memorisation is task-dependent, and give nuance to the\ngeneralisation first, memorisation second hypothesis.\n', ""  Understanding memorisation in language models has practical and societal\nimplications, e.g., studying models' training dynamics or preventing copyright\ninfringements. Prior work defines memorisation as the causal effect of training\nwith an instance on the model's ability to predict that instance. This\ndefinition relies on a counterfactual: the ability to observe what would have\nhappened had the model not seen that instance. Existing methods struggle to\nprovide computationally efficient and accurate estimates of this\ncounterfactual. Further, they often estimate memorisation for a model\narchitecture rather than for a specific model instance. This paper fills an\nimportant gap in the literature, proposing a new, principled, and efficient\nmethod to estimate memorisation based on the difference-in-differences design\nfrom econometrics. Using this method, we characterise a model's memorisation\nprofile--its memorisation trends across training--by only observing its\nbehaviour on a small set of instances throughout training. In experiments with\nthe Pythia model suite, we find that memorisation (i) is stronger and more\npersistent in larger models, (ii) is determined by data order and learning\nrate, and (iii) has stable trends across model sizes, thus making memorisation\nin larger models predictable from smaller ones.\n""]",Memorization in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
259,33,259_memorization_memorisation_memorized_memorise,"['memorization', 'memorisation', 'memorized', 'memorise', 'linguistic', 'neural', 'language', 'sentences', 'attentional', 'lexical']","['memorisation', 'memorization', 'linguistic', 'memorized', 'attentional', 'neurons', 'layers', 'sentences', 'lexical', 'language']","['  Large Language Models (LLMs), trained on massive corpora with billions of\nparameters, show unprecedented performance in various fields. Though surprised\nby their excellent performances, researchers also noticed some special\nbehaviors of those LLMs. One of those behaviors is memorization, in which LLMs\ncan generate the same content used to train them. Though previous research has\ndiscussed memorization, the memorization of LLMs still lacks explanation,\nespecially the cause of memorization and the dynamics of generating them. In\nthis research, we comprehensively discussed memorization from various\nperspectives and extended the discussion scope to not only just the memorized\ncontent but also less and unmemorized content. Through various studies, we\nfound that: (1) Through experiments, we revealed the relation of memorization\nbetween model size, continuation size, and context size. Further, we showed how\nunmemorized sentences transition to memorized sentences. (2) Through embedding\nanalysis, we showed the distribution and decoding dynamics across model size in\nembedding space for sentences with different memorization scores. The n-gram\nstatistics analysis presents d (3) An analysis over n-gram and entropy decoding\ndynamics discovered a boundary effect when the model starts to generate\nmemorized sentences or unmemorized sentences. (4)We trained a Transformer model\nto predict the memorization of different models, showing that it is possible to\npredict memorizations by context.\n', '  Memorisation is a natural part of learning from real-world data: neural\nmodels pick up on atypical input-output combinations and store those training\nexamples in their parameter space. That this happens is well-known, but how and\nwhere are questions that remain largely unanswered. Given a multi-layered\nneural model, where does memorisation occur in the millions of parameters?\nRelated work reports conflicting findings: a dominant hypothesis based on image\nclassification is that lower layers learn generalisable features and that\ndeeper layers specialise and memorise. Work from NLP suggests this does not\napply to language models, but has been mainly focused on memorisation of facts.\nWe expand the scope of the localisation question to 12 natural language\nclassification tasks and apply 4 memorisation localisation techniques. Our\nresults indicate that memorisation is a gradual process rather than a localised\none, establish that memorisation is task-dependent, and give nuance to the\ngeneralisation first, memorisation second hypothesis.\n', ""  Understanding memorisation in language models has practical and societal\nimplications, e.g., studying models' training dynamics or preventing copyright\ninfringements. Prior work defines memorisation as the causal effect of training\nwith an instance on the model's ability to predict that instance. This\ndefinition relies on a counterfactual: the ability to observe what would have\nhappened had the model not seen that instance. Existing methods struggle to\nprovide computationally efficient and accurate estimates of this\ncounterfactual. Further, they often estimate memorisation for a model\narchitecture rather than for a specific model instance. This paper fills an\nimportant gap in the literature, proposing a new, principled, and efficient\nmethod to estimate memorisation based on the difference-in-differences design\nfrom econometrics. Using this method, we characterise a model's memorisation\nprofile--its memorisation trends across training--by only observing its\nbehaviour on a small set of instances throughout training. In experiments with\nthe Pythia model suite, we find that memorisation (i) is stronger and more\npersistent in larger models, (ii) is determined by data order and learning\nrate, and (iii) has stable trends across model sizes, thus making memorisation\nin larger models predictable from smaller ones.\n""]",Memorization in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
259,33,259_memorization_memorisation_memorized_memorise,"['memorization', 'memorisation', 'memorized', 'memorise', 'linguistic', 'neural', 'language', 'sentences', 'attentional', 'lexical']","['memorisation', 'memorization', 'linguistic', 'memorized', 'attentional', 'neurons', 'layers', 'sentences', 'lexical', 'language']","['  Large Language Models (LLMs), trained on massive corpora with billions of\nparameters, show unprecedented performance in various fields. Though surprised\nby their excellent performances, researchers also noticed some special\nbehaviors of those LLMs. One of those behaviors is memorization, in which LLMs\ncan generate the same content used to train them. Though previous research has\ndiscussed memorization, the memorization of LLMs still lacks explanation,\nespecially the cause of memorization and the dynamics of generating them. In\nthis research, we comprehensively discussed memorization from various\nperspectives and extended the discussion scope to not only just the memorized\ncontent but also less and unmemorized content. Through various studies, we\nfound that: (1) Through experiments, we revealed the relation of memorization\nbetween model size, continuation size, and context size. Further, we showed how\nunmemorized sentences transition to memorized sentences. (2) Through embedding\nanalysis, we showed the distribution and decoding dynamics across model size in\nembedding space for sentences with different memorization scores. The n-gram\nstatistics analysis presents d (3) An analysis over n-gram and entropy decoding\ndynamics discovered a boundary effect when the model starts to generate\nmemorized sentences or unmemorized sentences. (4)We trained a Transformer model\nto predict the memorization of different models, showing that it is possible to\npredict memorizations by context.\n', '  Memorisation is a natural part of learning from real-world data: neural\nmodels pick up on atypical input-output combinations and store those training\nexamples in their parameter space. That this happens is well-known, but how and\nwhere are questions that remain largely unanswered. Given a multi-layered\nneural model, where does memorisation occur in the millions of parameters?\nRelated work reports conflicting findings: a dominant hypothesis based on image\nclassification is that lower layers learn generalisable features and that\ndeeper layers specialise and memorise. Work from NLP suggests this does not\napply to language models, but has been mainly focused on memorisation of facts.\nWe expand the scope of the localisation question to 12 natural language\nclassification tasks and apply 4 memorisation localisation techniques. Our\nresults indicate that memorisation is a gradual process rather than a localised\none, establish that memorisation is task-dependent, and give nuance to the\ngeneralisation first, memorisation second hypothesis.\n', ""  Understanding memorisation in language models has practical and societal\nimplications, e.g., studying models' training dynamics or preventing copyright\ninfringements. Prior work defines memorisation as the causal effect of training\nwith an instance on the model's ability to predict that instance. This\ndefinition relies on a counterfactual: the ability to observe what would have\nhappened had the model not seen that instance. Existing methods struggle to\nprovide computationally efficient and accurate estimates of this\ncounterfactual. Further, they often estimate memorisation for a model\narchitecture rather than for a specific model instance. This paper fills an\nimportant gap in the literature, proposing a new, principled, and efficient\nmethod to estimate memorisation based on the difference-in-differences design\nfrom econometrics. Using this method, we characterise a model's memorisation\nprofile--its memorisation trends across training--by only observing its\nbehaviour on a small set of instances throughout training. In experiments with\nthe Pythia model suite, we find that memorisation (i) is stronger and more\npersistent in larger models, (ii) is determined by data order and learning\nrate, and (iii) has stable trends across model sizes, thus making memorisation\nin larger models predictable from smaller ones.\n""]",Memorization in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
259,33,259_memorization_memorisation_memorized_memorise,"['memorization', 'memorisation', 'memorized', 'memorise', 'linguistic', 'neural', 'language', 'sentences', 'attentional', 'lexical']","['memorisation', 'memorization', 'linguistic', 'memorized', 'attentional', 'neurons', 'layers', 'sentences', 'lexical', 'language']","['  Large Language Models (LLMs), trained on massive corpora with billions of\nparameters, show unprecedented performance in various fields. Though surprised\nby their excellent performances, researchers also noticed some special\nbehaviors of those LLMs. One of those behaviors is memorization, in which LLMs\ncan generate the same content used to train them. Though previous research has\ndiscussed memorization, the memorization of LLMs still lacks explanation,\nespecially the cause of memorization and the dynamics of generating them. In\nthis research, we comprehensively discussed memorization from various\nperspectives and extended the discussion scope to not only just the memorized\ncontent but also less and unmemorized content. Through various studies, we\nfound that: (1) Through experiments, we revealed the relation of memorization\nbetween model size, continuation size, and context size. Further, we showed how\nunmemorized sentences transition to memorized sentences. (2) Through embedding\nanalysis, we showed the distribution and decoding dynamics across model size in\nembedding space for sentences with different memorization scores. The n-gram\nstatistics analysis presents d (3) An analysis over n-gram and entropy decoding\ndynamics discovered a boundary effect when the model starts to generate\nmemorized sentences or unmemorized sentences. (4)We trained a Transformer model\nto predict the memorization of different models, showing that it is possible to\npredict memorizations by context.\n', '  Memorisation is a natural part of learning from real-world data: neural\nmodels pick up on atypical input-output combinations and store those training\nexamples in their parameter space. That this happens is well-known, but how and\nwhere are questions that remain largely unanswered. Given a multi-layered\nneural model, where does memorisation occur in the millions of parameters?\nRelated work reports conflicting findings: a dominant hypothesis based on image\nclassification is that lower layers learn generalisable features and that\ndeeper layers specialise and memorise. Work from NLP suggests this does not\napply to language models, but has been mainly focused on memorisation of facts.\nWe expand the scope of the localisation question to 12 natural language\nclassification tasks and apply 4 memorisation localisation techniques. Our\nresults indicate that memorisation is a gradual process rather than a localised\none, establish that memorisation is task-dependent, and give nuance to the\ngeneralisation first, memorisation second hypothesis.\n', ""  Understanding memorisation in language models has practical and societal\nimplications, e.g., studying models' training dynamics or preventing copyright\ninfringements. Prior work defines memorisation as the causal effect of training\nwith an instance on the model's ability to predict that instance. This\ndefinition relies on a counterfactual: the ability to observe what would have\nhappened had the model not seen that instance. Existing methods struggle to\nprovide computationally efficient and accurate estimates of this\ncounterfactual. Further, they often estimate memorisation for a model\narchitecture rather than for a specific model instance. This paper fills an\nimportant gap in the literature, proposing a new, principled, and efficient\nmethod to estimate memorisation based on the difference-in-differences design\nfrom econometrics. Using this method, we characterise a model's memorisation\nprofile--its memorisation trends across training--by only observing its\nbehaviour on a small set of instances throughout training. In experiments with\nthe Pythia model suite, we find that memorisation (i) is stronger and more\npersistent in larger models, (ii) is determined by data order and learning\nrate, and (iii) has stable trends across model sizes, thus making memorisation\nin larger models predictable from smaller ones.\n""]",Memorization in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
260,33,260_lexical_semantic_lexicon_corpus,"['lexical', 'semantic', 'lexicon', 'corpus', 'wordnet', 'linguistics', 'contextualized', 'contextualised', 'corpora', 'words']","['senses', 'word', 'change', 'sense', 'semantic', 'usages', 'words', 'lexical', 'meaning', 'changes']","['  Word meanings change over time, and word senses evolve, emerge or die out in\nthe process. For ancient languages, where the corpora are often small and\nsparse, modelling such changes accurately proves challenging, and quantifying\nuncertainty in sense-change estimates consequently becomes important. GASC\n(Genre-Aware Semantic Change) and DiSC (Diachronic Sense Change) are existing\ngenerative models that have been used to analyse sense change for target words\nfrom an ancient Greek text corpus, using unsupervised learning without the help\nof any pre-training. These models represent the senses of a given target word\nsuch as ""kosmos"" (meaning decoration, order or world) as distributions over\ncontext words, and sense prevalence as a distribution over senses. The models\nare fitted using Markov Chain Monte Carlo (MCMC) methods to measure temporal\nchanges in these representations. This paper introduces EDiSC, an Embedded DiSC\nmodel, which combines word embeddings with DiSC to provide superior model\nperformance. It is shown empirically that EDiSC offers improved predictive\naccuracy, ground-truth recovery and uncertainty quantification, as well as\nbetter sampling efficiency and scalability properties with MCMC methods. The\nchallenges of fitting these models are also discussed.\n', '  Despite the predominance of contextualized embeddings in NLP, approaches to\ndetect semantic change relying on these embeddings and clustering methods\nunderperform simpler counterparts based on static word embeddings. This stems\nfrom the poor quality of the clustering methods to produce sense clusters --\nwhich struggle to capture word senses, especially those with low frequency.\nThis issue hinders the next step in examining how changes in word senses in one\nlanguage influence another. To address this issue, we propose a graph-based\nclustering approach to capture nuanced changes in both high- and low-frequency\nword senses across time and languages, including the acquisition and loss of\nthese senses over time. Our experimental results show that our approach\nsubstantially surpasses previous approaches in the SemEval2020 binary\nclassification task across four languages. Moreover, we showcase the ability of\nour approach as a versatile visualization tool to detect semantic changes in\nboth intra-language and inter-language setups. We make our code and data\npublicly available.\n', ""  We use contextualized word definitions generated by large language models as\nsemantic representations in the task of diachronic lexical semantic change\ndetection (LSCD). In short, generated definitions are used as `senses', and the\nchange score of a target word is retrieved by comparing their distributions in\ntwo time periods under comparison. On the material of five datasets and three\nlanguages, we show that generated definitions are indeed specific and general\nenough to convey a signal sufficient to rank sets of words by the degree of\ntheir semantic change over time. Our approach is on par with or outperforms\nprior non-supervised sense-based LSCD methods. At the same time, it preserves\ninterpretability and allows to inspect the reasons behind a specific shift in\nterms of discrete definitions-as-senses. This is another step in the direction\nof explainable semantic change modeling.\n""]",Lexical Semantic Change Detection in Corpora,Natural Language Processing and Semantic Analysis,Natural Language Processing,Natural Language Processing
261,33,261_batteryml_batteries_battery_lithium,"['batteryml', 'batteries', 'battery', 'lithium', 'predicting', 'prediction', 'prognostics', 'estimating', 'lifespan', 'degradation']","['battery', 'batteries', 'lithium', 'ion', 'life', 'electrochemical', 'degradation', 'aging', 'voltage', 'health']","['  Batteries are dynamic systems with complicated nonlinear aging, highly\ndependent on cell design, chemistry, manufacturing, and operational conditions.\nPrediction of battery cycle life and estimation of aging states is important to\naccelerate battery R&D, testing, and to further the understanding of how\nbatteries degrade. Beyond testing, battery management systems rely on real-time\nmodels and onboard diagnostics and prognostics for safe operation. Estimating\nthe state of health and remaining useful life of a battery is important to\noptimize performance and use resources optimally.\n  This tutorial begins with an overview of first-principles, machine learning,\nand hybrid battery models. Then, a typical pipeline for the development of\ninterpretable machine learning models is explained and showcased for cycle life\nprediction from laboratory testing data. We highlight the challenges of machine\nlearning models, motivating the incorporation of physics in hybrid modeling\napproaches, which are needed to decipher the aging trajectory of batteries but\nrequire more data and further work on the physics of battery degradation. The\ntutorial closes with a discussion on generalization and further research\ndirections.\n', '  Battery life estimation is critical for optimizing battery performance and\nguaranteeing minimal degradation for better efficiency and reliability of\nbattery-powered systems. The existing methods to predict the Remaining Useful\nLife(RUL) of Lithium-ion Batteries (LiBs) neglect the relational dependencies\nof the battery parameters to model the nonlinear degradation trajectories. We\npresent the Battery GraphNets framework that jointly learns to incorporate a\ndiscrete dependency graph structure between battery parameters to capture the\ncomplex interactions and the graph-learning algorithm to model the intrinsic\nbattery degradation for RUL prognosis. The proposed method outperforms several\npopular methods by a significant margin on publicly available battery datasets\nand achieves SOTA performance. We report the ablation studies to support the\nefficacy of our approach.\n', '  Lithium-ion batteries are pivotal to technological advancements in\ntransportation, electronics, and clean energy storage. The optimal operation\nand safety of these batteries require proper and reliable estimation of battery\ncapacities to monitor the state of health. Current methods for estimating the\ncapacities fail to adequately account for long-term temporal dependencies of\nkey variables (e.g., voltage, current, and temperature) associated with battery\naging and degradation. In this study, we explore the usage of transformer\nnetworks to enhance the estimation of battery capacity. We develop a\ntransformer-based battery capacity prediction model that accounts for both\nlong-term and short-term patterns in battery data. Further, to tackle the data\nscarcity issue, data augmentation is used to increase the data size, which\nhelps to improve the performance of the model. Our proposed method is validated\nwith benchmark datasets. Simulation results show the effectiveness of data\naugmentation and the transformer network in improving the accuracy and\nrobustness of battery capacity prediction.\n']",Battery Health Prediction and Degradation Analysis,Predictive Modeling for Time-to-Event Outcomes and Battery Health,Predictive Modeling and Forecasting,Predictive Modeling and Forecasting
262,32,262_recipes_cuisine_foods_recipe,"['recipes', 'cuisine', 'foods', 'recipe', 'recipemc', 'chef', 'food', 'culinary', 'meals', 'meal']","['food', 'recipes', 'recipe', 'meal', 'ingredients', 'flavor', 'cooking', 'nutritional', 'culinary', 'ingredient']","['  In the early 2000s, renowned chef Heston Blumenthal formulated his ""food\npairing"" hypothesis, positing that if foods share many flavor compounds, then\nthey tend to taste good when eaten together. In 2011, Ahn et al. conducted a\nstudy using a dataset of recipes, ingredients, and flavor compounds, finding\nthat, in Western cuisine, ingredients in recipes often share more flavor\ncompounds than expected by chance, indicating a natural tendency towards food\npairing. Building upon Ahn\'s research, our work applies state-of-the-art\ncollaborative filtering techniques to the dataset, providing a tool that can\nrecommend new foods to add in recipes, retrieve missing ingredients and advise\nagainst certain combinations. We create our recommender in two ways, by taking\ninto account ingredients appearances in recipes or shared flavor compounds\nbetween foods. While our analysis confirms the existence of food pairing, the\nrecipe-based recommender performs significantly better than the flavor-based\none, leading to the conclusion that food pairing is just one of the principles\nto take into account when creating recipes. Furthermore, and more\ninterestingly, we find that food pairing in data is mostly due to trivial\ncouplings of very similar ingredients, leading to a reconsideration of its\ncurrent role in recipes, from being an already existing feature to a key to\nopen up new scenarios in gastronomy. Our flavor-based recommender can thus\nleverage this novel concept and provide a new tool to lead culinary innovation.\n', ""  Large Multi-modal Models (LMMs) have significantly advanced a variety of\nvision-language tasks. The scalability and availability of high-quality\ntraining data play a pivotal role in the success of LMMs. In the realm of food,\nwhile comprehensive food datasets such as Recipe1M offer an abundance of\ningredient and recipe information, they often fall short of providing ample\ndata for nutritional analysis. The Recipe1M+ dataset, despite offering a subset\nfor nutritional evaluation, is limited in the scale and accuracy of nutrition\ninformation. To bridge this gap, we introduce Uni-Food, a unified food dataset\nthat comprises over 100,000 images with various food labels, including\ncategories, ingredients, recipes, and ingredient-level nutritional information.\nUni-Food is designed to provide a more holistic approach to food data analysis,\nthereby enhancing the performance and capabilities of LMMs in this domain. To\nmitigate the conflicts arising from multi-task supervision during fine-tuning\nof LMMs, we introduce a novel Linear Rectification Mixture of Diverse Experts\n(RoDE) approach. RoDE utilizes a diverse array of experts to address tasks of\nvarying complexity, thereby facilitating the coordination of trainable\nparameters, i.e., it allocates more parameters for more complex tasks and,\nconversely, fewer parameters for simpler tasks. RoDE implements linear\nrectification union to refine the router's functionality, thereby enhancing the\nefficiency of sparse task allocation. These design choices endow RoDE with\nfeatures that ensure GPU memory efficiency and ease of optimization. Our\nexperimental results validate the effectiveness of our proposed approach in\naddressing the inherent challenges of food-related multitasking.\n"", '  Food computing has emerged as a prominent multidisciplinary field of research\nin recent years. An ambitious goal of food computing is to develop end-to-end\nintelligent systems capable of autonomously producing recipe information for a\nfood image. Current image-to-recipe methods are retrieval-based and their\nsuccess depends heavily on the dataset size and diversity, as well as the\nquality of learned embeddings. Meanwhile, the emergence of powerful\nattention-based vision and language models presents a promising avenue for\naccurate and generalizable recipe generation, which has yet to be extensively\nexplored. This paper proposes FIRE, a novel multimodal methodology tailored to\nrecipe generation in the food computing domain, which generates the food title,\ningredients, and cooking instructions based on input food images. FIRE\nleverages the BLIP model to generate titles, utilizes a Vision Transformer with\na decoder for ingredient extraction, and employs the T5 model to generate\nrecipes incorporating titles and ingredients as inputs. We showcase two\npractical applications that can benefit from integrating FIRE with large\nlanguage model prompting: recipe customization to fit recipes to user\npreferences and recipe-to-code transformation to enable automated cooking\nprocesses. Our experimental findings validate the efficacy of our proposed\napproach, underscoring its potential for future advancements and widespread\nadoption in food computing.\n']",Food Pairing and Recipe Generation,Food Informatics and Recipe Generation,Innovations in Sustainable Technology and Food Systems,Innovations in Sustainable Technology and Food Systems
263,32,263_adversarial_watermarking_watermark_watermarked,"['adversarial', 'watermarking', 'watermark', 'watermarked', 'watermarks', 'unmarker', 'deeptaster', 'protect', 'tampering', 'stealing']","['watermarking', 'watermark', 'watermarks', 'ownership', 'intellectual', 'suspect', 'removal', 'attacks', 'owners', 'trigger']","['  Recently, numerous highly-valuable Deep Neural Networks (DNNs) have been\ntrained using deep learning algorithms. To protect the Intellectual Property\n(IP) of the original owners over such DNN models, backdoor-based watermarks\nhave been extensively studied. However, most of such watermarks fail upon model\nextraction attack, which utilizes input samples to query the target model and\nobtains the corresponding outputs, thus training a substitute model using such\ninput-output pairs. In this paper, we propose a novel watermark to protect IP\nof DNN models against model extraction, named MEA-Defender. In particular, we\nobtain the watermark by combining two samples from two source classes in the\ninput domain and design a watermark loss function that makes the output domain\nof the watermark within that of the main task samples. Since both the input\ndomain and the output domain of our watermark are indispensable parts of those\nof the main task samples, the watermark will be extracted into the stolen model\nalong with the main task during model extraction. We conduct extensive\nexperiments on four model extraction attacks, using five datasets and six\nmodels trained based on supervised learning and self-supervised learning\nalgorithms. The experimental results demonstrate that MEA-Defender is highly\nrobust against different model extraction attacks, and various watermark\nremoval/detection approaches.\n', '  Deep Learning (DL) models have become crucial in digital transformation, thus\nraising concerns about their intellectual property rights. Different\nwatermarking techniques have been developed to protect Deep Neural Networks\n(DNNs) from IP infringement, creating a competitive field for DNN watermarking\nand removal methods. The predominant watermarking schemes use white-box\ntechniques, which involve modifying weights by adding a unique signature to\nspecific DNN layers. On the other hand, existing attacks on white-box\nwatermarking usually require knowledge of the specific deployed watermarking\nscheme or access to the underlying data for further training and fine-tuning.\nWe propose DeepEclipse, a novel and unified framework designed to remove\nwhite-box watermarks. We present obfuscation techniques that significantly\ndiffer from the existing white-box watermarking removal schemes. DeepEclipse\ncan evade watermark detection without prior knowledge of the underlying\nwatermarking scheme, additional data, or training and fine-tuning. Our\nevaluation reveals that DeepEclipse excels in breaking multiple white-box\nwatermarking schemes, reducing watermark detection to random guessing while\nmaintaining a similar model accuracy as the original one. Our framework\nshowcases a promising solution to address the ongoing DNN watermark protection\nand removal challenges.\n', '  Watermark has been widely deployed by industry to detect AI-generated images.\nThe robustness of such watermark-based detector against evasion attacks in the\nwhite-box and black-box settings is well understood in the literature. However,\nthe robustness in the no-box setting is much less understood. In particular,\nmultiple studies claimed that image watermark is robust in such setting. In\nthis work, we propose a new transfer evasion attack to image watermark in the\nno-box setting. Our transfer attack adds a perturbation to a watermarked image\nto evade multiple surrogate watermarking models trained by the attacker itself,\nand the perturbed watermarked image also evades the target watermarking model.\nOur major contribution is to show that, both theoretically and empirically,\nwatermark-based AI-generated image detector is not robust to evasion attacks\neven if the attacker does not have access to the watermarking model nor the\ndetection API.\n']",Deep Learning Watermarking and Protection,Deep Learning Security and Efficiency,Deep Learning Optimization and Security,Deep Learning Methodologies
264,32,264_contexts_longwriter_context_retrieval,"['contexts', 'longwriter', 'context', 'retrieval', 'longbench', 'language', 'longer', 'texts', 'longins', 'short']","['long', 'context', 'length', 'contexts', 'window', 'tokens', 'longer', 'retrieval', 'benchmarks', 'sequences']","['  Improvements in language models\' capabilities have pushed their applications\ntowards longer contexts, making long-context evaluation and development an\nactive research area. However, many disparate use-cases are grouped together\nunder the umbrella term of ""long-context"", defined simply by the total length\nof the model\'s input, including - for example - Needle-in-a-Haystack tasks,\nbook summarization, and information aggregation. Given their varied difficulty,\nin this position paper we argue that conflating different tasks by their\ncontext length is unproductive. As a community, we require a more precise\nvocabulary to understand what makes long-context tasks similar or different. We\npropose to unpack the taxonomy of long-context based on the properties that\nmake them more difficult with longer contexts. We propose two orthogonal axes\nof difficulty: (I) Diffusion: How hard is it to find the necessary information\nin the context? (II) Scope: How much necessary information is there to find? We\nsurvey the literature on long-context, provide justification for this taxonomy\nas an informative descriptor, and situate the literature with respect to it. We\nconclude that the most difficult and interesting settings, whose necessary\ninformation is very long and highly diffused within the input, is severely\nunder-explored. By using a descriptive vocabulary and discussing the relevant\nproperties of difficulty in long-context, we can implement more informed\nresearch in this area. We call for a careful design of tasks and benchmarks\nwith distinctly long context, taking into account the characteristics that make\nit qualitatively different from shorter context.\n', ""  Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks but are constrained by their small context window sizes. Various\nefforts have been proposed to expand the context window to accommodate even up\nto 200K input tokens. Meanwhile, building high-quality benchmarks with much\nlonger text lengths and more demanding tasks to provide comprehensive\nevaluations is of immense practical interest to facilitate long context\nunderstanding research of LLMs. However, prior benchmarks create datasets that\nostensibly cater to long-text comprehension by expanding the input of\ntraditional tasks, which falls short to exhibit the unique characteristics of\nlong-text understanding, including long dependency tasks and longer text length\ncompatible with modern LLMs' context window size. In this paper, we introduce a\nbenchmark for extremely long context understanding with long-range\ndependencies, XL$^2$Bench, which includes three scenarios: Fiction Reading,\nPaper Reading, and Law Reading, and four tasks of increasing complexity: Memory\nRetrieval, Detailed Understanding, Overall Understanding, and Open-ended\nGeneration, covering 27 subtasks in English and Chinese. It has an average\nlength of 100K+ words (English) and 200K+ characters (Chinese). Evaluating six\nleading LLMs on XL$^2$Bench, we find that their performance significantly lags\nbehind human levels. Moreover, the observed decline in performance across both\nthe original and enhanced datasets underscores the efficacy of our approach to\nmitigating data contamination.\n"", ""  Although large language models (LLMs) demonstrate impressive performance for\nmany language tasks, most of them can only handle texts a few thousand tokens\nlong, limiting their applications on longer sequence inputs, such as books,\nreports, and codebases. Recent works have proposed methods to improve LLMs'\nlong context capabilities by extending context windows and more sophisticated\nmemory mechanisms. However, comprehensive benchmarks tailored for evaluating\nlong context understanding are lacking. In this paper, we introduce LongBench,\nthe first bilingual, multi-task benchmark for long context understanding,\nenabling a more rigorous evaluation of long context understanding. LongBench\ncomprises 21 datasets across 6 task categories in both English and Chinese,\nwith an average length of 6,711 words (English) and 13,386 characters\n(Chinese). These tasks cover key long-text application areas including\nsingle-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks,\nand code completion. All datasets in LongBench are standardized into a unified\nformat, allowing for effortless automatic evaluation of LLMs. Upon\ncomprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial\nmodel (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still\nstruggles on longer contexts. (2) Scaled position embedding and fine-tuning on\nlonger sequences lead to substantial improvement on long context understanding.\n(3) Context compression technique such as retrieval brings improvement for\nmodel with weak ability on long contexts, but the performance still lags behind\nmodels that have strong long context understanding capability. The code and\ndatasets are available at https://github.com/THUDM/LongBench.\n""]",Long Context Understanding in Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
264,32,264_contexts_longwriter_context_retrieval,"['contexts', 'longwriter', 'context', 'retrieval', 'longbench', 'language', 'longer', 'texts', 'longins', 'short']","['long', 'context', 'length', 'contexts', 'window', 'tokens', 'longer', 'retrieval', 'benchmarks', 'sequences']","['  Improvements in language models\' capabilities have pushed their applications\ntowards longer contexts, making long-context evaluation and development an\nactive research area. However, many disparate use-cases are grouped together\nunder the umbrella term of ""long-context"", defined simply by the total length\nof the model\'s input, including - for example - Needle-in-a-Haystack tasks,\nbook summarization, and information aggregation. Given their varied difficulty,\nin this position paper we argue that conflating different tasks by their\ncontext length is unproductive. As a community, we require a more precise\nvocabulary to understand what makes long-context tasks similar or different. We\npropose to unpack the taxonomy of long-context based on the properties that\nmake them more difficult with longer contexts. We propose two orthogonal axes\nof difficulty: (I) Diffusion: How hard is it to find the necessary information\nin the context? (II) Scope: How much necessary information is there to find? We\nsurvey the literature on long-context, provide justification for this taxonomy\nas an informative descriptor, and situate the literature with respect to it. We\nconclude that the most difficult and interesting settings, whose necessary\ninformation is very long and highly diffused within the input, is severely\nunder-explored. By using a descriptive vocabulary and discussing the relevant\nproperties of difficulty in long-context, we can implement more informed\nresearch in this area. We call for a careful design of tasks and benchmarks\nwith distinctly long context, taking into account the characteristics that make\nit qualitatively different from shorter context.\n', ""  Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks but are constrained by their small context window sizes. Various\nefforts have been proposed to expand the context window to accommodate even up\nto 200K input tokens. Meanwhile, building high-quality benchmarks with much\nlonger text lengths and more demanding tasks to provide comprehensive\nevaluations is of immense practical interest to facilitate long context\nunderstanding research of LLMs. However, prior benchmarks create datasets that\nostensibly cater to long-text comprehension by expanding the input of\ntraditional tasks, which falls short to exhibit the unique characteristics of\nlong-text understanding, including long dependency tasks and longer text length\ncompatible with modern LLMs' context window size. In this paper, we introduce a\nbenchmark for extremely long context understanding with long-range\ndependencies, XL$^2$Bench, which includes three scenarios: Fiction Reading,\nPaper Reading, and Law Reading, and four tasks of increasing complexity: Memory\nRetrieval, Detailed Understanding, Overall Understanding, and Open-ended\nGeneration, covering 27 subtasks in English and Chinese. It has an average\nlength of 100K+ words (English) and 200K+ characters (Chinese). Evaluating six\nleading LLMs on XL$^2$Bench, we find that their performance significantly lags\nbehind human levels. Moreover, the observed decline in performance across both\nthe original and enhanced datasets underscores the efficacy of our approach to\nmitigating data contamination.\n"", ""  Although large language models (LLMs) demonstrate impressive performance for\nmany language tasks, most of them can only handle texts a few thousand tokens\nlong, limiting their applications on longer sequence inputs, such as books,\nreports, and codebases. Recent works have proposed methods to improve LLMs'\nlong context capabilities by extending context windows and more sophisticated\nmemory mechanisms. However, comprehensive benchmarks tailored for evaluating\nlong context understanding are lacking. In this paper, we introduce LongBench,\nthe first bilingual, multi-task benchmark for long context understanding,\nenabling a more rigorous evaluation of long context understanding. LongBench\ncomprises 21 datasets across 6 task categories in both English and Chinese,\nwith an average length of 6,711 words (English) and 13,386 characters\n(Chinese). These tasks cover key long-text application areas including\nsingle-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks,\nand code completion. All datasets in LongBench are standardized into a unified\nformat, allowing for effortless automatic evaluation of LLMs. Upon\ncomprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial\nmodel (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still\nstruggles on longer contexts. (2) Scaled position embedding and fine-tuning on\nlonger sequences lead to substantial improvement on long context understanding.\n(3) Context compression technique such as retrieval brings improvement for\nmodel with weak ability on long contexts, but the performance still lags behind\nmodels that have strong long context understanding capability. The code and\ndatasets are available at https://github.com/THUDM/LongBench.\n""]",Long Context Understanding in Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
264,32,264_contexts_longwriter_context_retrieval,"['contexts', 'longwriter', 'context', 'retrieval', 'longbench', 'language', 'longer', 'texts', 'longins', 'short']","['long', 'context', 'length', 'contexts', 'window', 'tokens', 'longer', 'retrieval', 'benchmarks', 'sequences']","['  Improvements in language models\' capabilities have pushed their applications\ntowards longer contexts, making long-context evaluation and development an\nactive research area. However, many disparate use-cases are grouped together\nunder the umbrella term of ""long-context"", defined simply by the total length\nof the model\'s input, including - for example - Needle-in-a-Haystack tasks,\nbook summarization, and information aggregation. Given their varied difficulty,\nin this position paper we argue that conflating different tasks by their\ncontext length is unproductive. As a community, we require a more precise\nvocabulary to understand what makes long-context tasks similar or different. We\npropose to unpack the taxonomy of long-context based on the properties that\nmake them more difficult with longer contexts. We propose two orthogonal axes\nof difficulty: (I) Diffusion: How hard is it to find the necessary information\nin the context? (II) Scope: How much necessary information is there to find? We\nsurvey the literature on long-context, provide justification for this taxonomy\nas an informative descriptor, and situate the literature with respect to it. We\nconclude that the most difficult and interesting settings, whose necessary\ninformation is very long and highly diffused within the input, is severely\nunder-explored. By using a descriptive vocabulary and discussing the relevant\nproperties of difficulty in long-context, we can implement more informed\nresearch in this area. We call for a careful design of tasks and benchmarks\nwith distinctly long context, taking into account the characteristics that make\nit qualitatively different from shorter context.\n', ""  Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks but are constrained by their small context window sizes. Various\nefforts have been proposed to expand the context window to accommodate even up\nto 200K input tokens. Meanwhile, building high-quality benchmarks with much\nlonger text lengths and more demanding tasks to provide comprehensive\nevaluations is of immense practical interest to facilitate long context\nunderstanding research of LLMs. However, prior benchmarks create datasets that\nostensibly cater to long-text comprehension by expanding the input of\ntraditional tasks, which falls short to exhibit the unique characteristics of\nlong-text understanding, including long dependency tasks and longer text length\ncompatible with modern LLMs' context window size. In this paper, we introduce a\nbenchmark for extremely long context understanding with long-range\ndependencies, XL$^2$Bench, which includes three scenarios: Fiction Reading,\nPaper Reading, and Law Reading, and four tasks of increasing complexity: Memory\nRetrieval, Detailed Understanding, Overall Understanding, and Open-ended\nGeneration, covering 27 subtasks in English and Chinese. It has an average\nlength of 100K+ words (English) and 200K+ characters (Chinese). Evaluating six\nleading LLMs on XL$^2$Bench, we find that their performance significantly lags\nbehind human levels. Moreover, the observed decline in performance across both\nthe original and enhanced datasets underscores the efficacy of our approach to\nmitigating data contamination.\n"", ""  Although large language models (LLMs) demonstrate impressive performance for\nmany language tasks, most of them can only handle texts a few thousand tokens\nlong, limiting their applications on longer sequence inputs, such as books,\nreports, and codebases. Recent works have proposed methods to improve LLMs'\nlong context capabilities by extending context windows and more sophisticated\nmemory mechanisms. However, comprehensive benchmarks tailored for evaluating\nlong context understanding are lacking. In this paper, we introduce LongBench,\nthe first bilingual, multi-task benchmark for long context understanding,\nenabling a more rigorous evaluation of long context understanding. LongBench\ncomprises 21 datasets across 6 task categories in both English and Chinese,\nwith an average length of 6,711 words (English) and 13,386 characters\n(Chinese). These tasks cover key long-text application areas including\nsingle-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks,\nand code completion. All datasets in LongBench are standardized into a unified\nformat, allowing for effortless automatic evaluation of LLMs. Upon\ncomprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial\nmodel (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still\nstruggles on longer contexts. (2) Scaled position embedding and fine-tuning on\nlonger sequences lead to substantial improvement on long context understanding.\n(3) Context compression technique such as retrieval brings improvement for\nmodel with weak ability on long contexts, but the performance still lags behind\nmodels that have strong long context understanding capability. The code and\ndatasets are available at https://github.com/THUDM/LongBench.\n""]",Long Context Understanding in Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
264,32,264_contexts_longwriter_context_retrieval,"['contexts', 'longwriter', 'context', 'retrieval', 'longbench', 'language', 'longer', 'texts', 'longins', 'short']","['long', 'context', 'length', 'contexts', 'window', 'tokens', 'longer', 'retrieval', 'benchmarks', 'sequences']","['  Improvements in language models\' capabilities have pushed their applications\ntowards longer contexts, making long-context evaluation and development an\nactive research area. However, many disparate use-cases are grouped together\nunder the umbrella term of ""long-context"", defined simply by the total length\nof the model\'s input, including - for example - Needle-in-a-Haystack tasks,\nbook summarization, and information aggregation. Given their varied difficulty,\nin this position paper we argue that conflating different tasks by their\ncontext length is unproductive. As a community, we require a more precise\nvocabulary to understand what makes long-context tasks similar or different. We\npropose to unpack the taxonomy of long-context based on the properties that\nmake them more difficult with longer contexts. We propose two orthogonal axes\nof difficulty: (I) Diffusion: How hard is it to find the necessary information\nin the context? (II) Scope: How much necessary information is there to find? We\nsurvey the literature on long-context, provide justification for this taxonomy\nas an informative descriptor, and situate the literature with respect to it. We\nconclude that the most difficult and interesting settings, whose necessary\ninformation is very long and highly diffused within the input, is severely\nunder-explored. By using a descriptive vocabulary and discussing the relevant\nproperties of difficulty in long-context, we can implement more informed\nresearch in this area. We call for a careful design of tasks and benchmarks\nwith distinctly long context, taking into account the characteristics that make\nit qualitatively different from shorter context.\n', ""  Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks but are constrained by their small context window sizes. Various\nefforts have been proposed to expand the context window to accommodate even up\nto 200K input tokens. Meanwhile, building high-quality benchmarks with much\nlonger text lengths and more demanding tasks to provide comprehensive\nevaluations is of immense practical interest to facilitate long context\nunderstanding research of LLMs. However, prior benchmarks create datasets that\nostensibly cater to long-text comprehension by expanding the input of\ntraditional tasks, which falls short to exhibit the unique characteristics of\nlong-text understanding, including long dependency tasks and longer text length\ncompatible with modern LLMs' context window size. In this paper, we introduce a\nbenchmark for extremely long context understanding with long-range\ndependencies, XL$^2$Bench, which includes three scenarios: Fiction Reading,\nPaper Reading, and Law Reading, and four tasks of increasing complexity: Memory\nRetrieval, Detailed Understanding, Overall Understanding, and Open-ended\nGeneration, covering 27 subtasks in English and Chinese. It has an average\nlength of 100K+ words (English) and 200K+ characters (Chinese). Evaluating six\nleading LLMs on XL$^2$Bench, we find that their performance significantly lags\nbehind human levels. Moreover, the observed decline in performance across both\nthe original and enhanced datasets underscores the efficacy of our approach to\nmitigating data contamination.\n"", ""  Although large language models (LLMs) demonstrate impressive performance for\nmany language tasks, most of them can only handle texts a few thousand tokens\nlong, limiting their applications on longer sequence inputs, such as books,\nreports, and codebases. Recent works have proposed methods to improve LLMs'\nlong context capabilities by extending context windows and more sophisticated\nmemory mechanisms. However, comprehensive benchmarks tailored for evaluating\nlong context understanding are lacking. In this paper, we introduce LongBench,\nthe first bilingual, multi-task benchmark for long context understanding,\nenabling a more rigorous evaluation of long context understanding. LongBench\ncomprises 21 datasets across 6 task categories in both English and Chinese,\nwith an average length of 6,711 words (English) and 13,386 characters\n(Chinese). These tasks cover key long-text application areas including\nsingle-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks,\nand code completion. All datasets in LongBench are standardized into a unified\nformat, allowing for effortless automatic evaluation of LLMs. Upon\ncomprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial\nmodel (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still\nstruggles on longer contexts. (2) Scaled position embedding and fine-tuning on\nlonger sequences lead to substantial improvement on long context understanding.\n(3) Context compression technique such as retrieval brings improvement for\nmodel with weak ability on long contexts, but the performance still lags behind\nmodels that have strong long context understanding capability. The code and\ndatasets are available at https://github.com/THUDM/LongBench.\n""]",Long Context Understanding in Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
264,32,264_contexts_longwriter_context_retrieval,"['contexts', 'longwriter', 'context', 'retrieval', 'longbench', 'language', 'longer', 'texts', 'longins', 'short']","['long', 'context', 'length', 'contexts', 'window', 'tokens', 'longer', 'retrieval', 'benchmarks', 'sequences']","['  Improvements in language models\' capabilities have pushed their applications\ntowards longer contexts, making long-context evaluation and development an\nactive research area. However, many disparate use-cases are grouped together\nunder the umbrella term of ""long-context"", defined simply by the total length\nof the model\'s input, including - for example - Needle-in-a-Haystack tasks,\nbook summarization, and information aggregation. Given their varied difficulty,\nin this position paper we argue that conflating different tasks by their\ncontext length is unproductive. As a community, we require a more precise\nvocabulary to understand what makes long-context tasks similar or different. We\npropose to unpack the taxonomy of long-context based on the properties that\nmake them more difficult with longer contexts. We propose two orthogonal axes\nof difficulty: (I) Diffusion: How hard is it to find the necessary information\nin the context? (II) Scope: How much necessary information is there to find? We\nsurvey the literature on long-context, provide justification for this taxonomy\nas an informative descriptor, and situate the literature with respect to it. We\nconclude that the most difficult and interesting settings, whose necessary\ninformation is very long and highly diffused within the input, is severely\nunder-explored. By using a descriptive vocabulary and discussing the relevant\nproperties of difficulty in long-context, we can implement more informed\nresearch in this area. We call for a careful design of tasks and benchmarks\nwith distinctly long context, taking into account the characteristics that make\nit qualitatively different from shorter context.\n', ""  Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks but are constrained by their small context window sizes. Various\nefforts have been proposed to expand the context window to accommodate even up\nto 200K input tokens. Meanwhile, building high-quality benchmarks with much\nlonger text lengths and more demanding tasks to provide comprehensive\nevaluations is of immense practical interest to facilitate long context\nunderstanding research of LLMs. However, prior benchmarks create datasets that\nostensibly cater to long-text comprehension by expanding the input of\ntraditional tasks, which falls short to exhibit the unique characteristics of\nlong-text understanding, including long dependency tasks and longer text length\ncompatible with modern LLMs' context window size. In this paper, we introduce a\nbenchmark for extremely long context understanding with long-range\ndependencies, XL$^2$Bench, which includes three scenarios: Fiction Reading,\nPaper Reading, and Law Reading, and four tasks of increasing complexity: Memory\nRetrieval, Detailed Understanding, Overall Understanding, and Open-ended\nGeneration, covering 27 subtasks in English and Chinese. It has an average\nlength of 100K+ words (English) and 200K+ characters (Chinese). Evaluating six\nleading LLMs on XL$^2$Bench, we find that their performance significantly lags\nbehind human levels. Moreover, the observed decline in performance across both\nthe original and enhanced datasets underscores the efficacy of our approach to\nmitigating data contamination.\n"", ""  Although large language models (LLMs) demonstrate impressive performance for\nmany language tasks, most of them can only handle texts a few thousand tokens\nlong, limiting their applications on longer sequence inputs, such as books,\nreports, and codebases. Recent works have proposed methods to improve LLMs'\nlong context capabilities by extending context windows and more sophisticated\nmemory mechanisms. However, comprehensive benchmarks tailored for evaluating\nlong context understanding are lacking. In this paper, we introduce LongBench,\nthe first bilingual, multi-task benchmark for long context understanding,\nenabling a more rigorous evaluation of long context understanding. LongBench\ncomprises 21 datasets across 6 task categories in both English and Chinese,\nwith an average length of 6,711 words (English) and 13,386 characters\n(Chinese). These tasks cover key long-text application areas including\nsingle-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks,\nand code completion. All datasets in LongBench are standardized into a unified\nformat, allowing for effortless automatic evaluation of LLMs. Upon\ncomprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial\nmodel (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still\nstruggles on longer contexts. (2) Scaled position embedding and fine-tuning on\nlonger sequences lead to substantial improvement on long context understanding.\n(3) Context compression technique such as retrieval brings improvement for\nmodel with weak ability on long contexts, but the performance still lags behind\nmodels that have strong long context understanding capability. The code and\ndatasets are available at https://github.com/THUDM/LongBench.\n""]",Long Context Understanding in Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
264,32,264_contexts_longwriter_context_retrieval,"['contexts', 'longwriter', 'context', 'retrieval', 'longbench', 'language', 'longer', 'texts', 'longins', 'short']","['long', 'context', 'length', 'contexts', 'window', 'tokens', 'longer', 'retrieval', 'benchmarks', 'sequences']","['  Improvements in language models\' capabilities have pushed their applications\ntowards longer contexts, making long-context evaluation and development an\nactive research area. However, many disparate use-cases are grouped together\nunder the umbrella term of ""long-context"", defined simply by the total length\nof the model\'s input, including - for example - Needle-in-a-Haystack tasks,\nbook summarization, and information aggregation. Given their varied difficulty,\nin this position paper we argue that conflating different tasks by their\ncontext length is unproductive. As a community, we require a more precise\nvocabulary to understand what makes long-context tasks similar or different. We\npropose to unpack the taxonomy of long-context based on the properties that\nmake them more difficult with longer contexts. We propose two orthogonal axes\nof difficulty: (I) Diffusion: How hard is it to find the necessary information\nin the context? (II) Scope: How much necessary information is there to find? We\nsurvey the literature on long-context, provide justification for this taxonomy\nas an informative descriptor, and situate the literature with respect to it. We\nconclude that the most difficult and interesting settings, whose necessary\ninformation is very long and highly diffused within the input, is severely\nunder-explored. By using a descriptive vocabulary and discussing the relevant\nproperties of difficulty in long-context, we can implement more informed\nresearch in this area. We call for a careful design of tasks and benchmarks\nwith distinctly long context, taking into account the characteristics that make\nit qualitatively different from shorter context.\n', ""  Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks but are constrained by their small context window sizes. Various\nefforts have been proposed to expand the context window to accommodate even up\nto 200K input tokens. Meanwhile, building high-quality benchmarks with much\nlonger text lengths and more demanding tasks to provide comprehensive\nevaluations is of immense practical interest to facilitate long context\nunderstanding research of LLMs. However, prior benchmarks create datasets that\nostensibly cater to long-text comprehension by expanding the input of\ntraditional tasks, which falls short to exhibit the unique characteristics of\nlong-text understanding, including long dependency tasks and longer text length\ncompatible with modern LLMs' context window size. In this paper, we introduce a\nbenchmark for extremely long context understanding with long-range\ndependencies, XL$^2$Bench, which includes three scenarios: Fiction Reading,\nPaper Reading, and Law Reading, and four tasks of increasing complexity: Memory\nRetrieval, Detailed Understanding, Overall Understanding, and Open-ended\nGeneration, covering 27 subtasks in English and Chinese. It has an average\nlength of 100K+ words (English) and 200K+ characters (Chinese). Evaluating six\nleading LLMs on XL$^2$Bench, we find that their performance significantly lags\nbehind human levels. Moreover, the observed decline in performance across both\nthe original and enhanced datasets underscores the efficacy of our approach to\nmitigating data contamination.\n"", ""  Although large language models (LLMs) demonstrate impressive performance for\nmany language tasks, most of them can only handle texts a few thousand tokens\nlong, limiting their applications on longer sequence inputs, such as books,\nreports, and codebases. Recent works have proposed methods to improve LLMs'\nlong context capabilities by extending context windows and more sophisticated\nmemory mechanisms. However, comprehensive benchmarks tailored for evaluating\nlong context understanding are lacking. In this paper, we introduce LongBench,\nthe first bilingual, multi-task benchmark for long context understanding,\nenabling a more rigorous evaluation of long context understanding. LongBench\ncomprises 21 datasets across 6 task categories in both English and Chinese,\nwith an average length of 6,711 words (English) and 13,386 characters\n(Chinese). These tasks cover key long-text application areas including\nsingle-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks,\nand code completion. All datasets in LongBench are standardized into a unified\nformat, allowing for effortless automatic evaluation of LLMs. Upon\ncomprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial\nmodel (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still\nstruggles on longer contexts. (2) Scaled position embedding and fine-tuning on\nlonger sequences lead to substantial improvement on long context understanding.\n(3) Context compression technique such as retrieval brings improvement for\nmodel with weak ability on long contexts, but the performance still lags behind\nmodels that have strong long context understanding capability. The code and\ndatasets are available at https://github.com/THUDM/LongBench.\n""]",Long Context Understanding in Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
265,32,265_morality_moral_morally_ethics,"['morality', 'moral', 'morally', 'ethics', 'ethical', 'persuasion', 'judgments', 'ai', 'languages', 'language']","['moral', 'ethical', 'dilemmas', 'values', 'judgments', 'ethics', 'morality', 'value', 'alignment', 'defeasible']","[""  Large language models (LLMs) have taken centre stage in debates on Artificial\nIntelligence. Yet there remains a gap in how to assess LLMs' conformity to\nimportant human values. In this paper, we investigate whether state-of-the-art\nLLMs, GPT-4 and Claude 2.1 (Gemini Pro and LLAMA 2 did not generate valid\nresults) are moral hypocrites. We employ two research instruments based on the\nMoral Foundations Theory: (i) the Moral Foundations Questionnaire (MFQ), which\ninvestigates which values are considered morally relevant in abstract moral\njudgements; and (ii) the Moral Foundations Vignettes (MFVs), which evaluate\nmoral cognition in concrete scenarios related to each moral foundation. We\ncharacterise conflicts in values between these different abstractions of moral\nevaluation as hypocrisy. We found that both models displayed reasonable\nconsistency within each instrument compared to humans, but they displayed\ncontradictory and hypocritical behaviour when we compared the abstract values\npresent in the MFQ to the evaluation of concrete moral violations of the MFV.\n"", '  As large language models (LLMs) are deployed in more and more real-world\nsituations, it is crucial to understand their decision-making when faced with\nmoral dilemmas. Inspired by a large-scale cross-cultural study of human moral\npreferences, ""The Moral Machine Experiment"", we set up the same set of moral\nchoices for LLMs. We translate 1K vignettes of moral dilemmas, parametrically\nvaried across key axes, into 100+ languages, and reveal the preferences of LLMs\nin each of these languages. We then compare the responses of LLMs to that of\nhuman speakers of those languages, harnessing a dataset of 40 million human\nmoral judgments. We discover that LLMs are more aligned with human preferences\nin languages such as English, Korean, Hungarian, and Chinese, but less aligned\nin languages such as Hindi and Somali (in Africa). Moreover, we characterize\nthe explanations LLMs give for their moral choices and find that fairness is\nthe most dominant supporting reason behind GPT-4\'s decisions and utilitarianism\nby GPT-3. We also discover ""language inequality"" (which we define as the\nmodel\'s different development levels in different languages) in a series of\nmeta-properties of moral decision making.\n', '  Making moral judgments is an essential step toward developing ethical AI\nsystems. Prevalent approaches are mostly implemented in a bottom-up manner,\nwhich uses a large set of annotated data to train models based on crowd-sourced\nopinions about morality. These approaches have been criticized for\novergeneralizing the moral stances of a limited group of annotators and lacking\nexplainability. This work proposes a flexible top-down framework to steer\n(Large) Language Models (LMs) to perform moral reasoning with well-established\nmoral theories from interdisciplinary research. The theory-guided top-down\nframework can incorporate various moral theories. Our experiments demonstrate\nthe effectiveness of the proposed framework on datasets derived from moral\ntheories. Furthermore, we show the alignment between different moral theories\nand existing morality datasets. Our analysis exhibits the potential and flaws\nin existing resources (models and datasets) in developing explainable moral\njudgment-making systems.\n']",Moral Reasoning in Large Language Models,Large Language Models and Cognitive Abilities,Large Language Models,Large Language Models
265,32,265_morality_moral_morally_ethics,"['morality', 'moral', 'morally', 'ethics', 'ethical', 'persuasion', 'judgments', 'ai', 'languages', 'language']","['moral', 'ethical', 'dilemmas', 'values', 'judgments', 'ethics', 'morality', 'value', 'alignment', 'defeasible']","[""  Large language models (LLMs) have taken centre stage in debates on Artificial\nIntelligence. Yet there remains a gap in how to assess LLMs' conformity to\nimportant human values. In this paper, we investigate whether state-of-the-art\nLLMs, GPT-4 and Claude 2.1 (Gemini Pro and LLAMA 2 did not generate valid\nresults) are moral hypocrites. We employ two research instruments based on the\nMoral Foundations Theory: (i) the Moral Foundations Questionnaire (MFQ), which\ninvestigates which values are considered morally relevant in abstract moral\njudgements; and (ii) the Moral Foundations Vignettes (MFVs), which evaluate\nmoral cognition in concrete scenarios related to each moral foundation. We\ncharacterise conflicts in values between these different abstractions of moral\nevaluation as hypocrisy. We found that both models displayed reasonable\nconsistency within each instrument compared to humans, but they displayed\ncontradictory and hypocritical behaviour when we compared the abstract values\npresent in the MFQ to the evaluation of concrete moral violations of the MFV.\n"", '  As large language models (LLMs) are deployed in more and more real-world\nsituations, it is crucial to understand their decision-making when faced with\nmoral dilemmas. Inspired by a large-scale cross-cultural study of human moral\npreferences, ""The Moral Machine Experiment"", we set up the same set of moral\nchoices for LLMs. We translate 1K vignettes of moral dilemmas, parametrically\nvaried across key axes, into 100+ languages, and reveal the preferences of LLMs\nin each of these languages. We then compare the responses of LLMs to that of\nhuman speakers of those languages, harnessing a dataset of 40 million human\nmoral judgments. We discover that LLMs are more aligned with human preferences\nin languages such as English, Korean, Hungarian, and Chinese, but less aligned\nin languages such as Hindi and Somali (in Africa). Moreover, we characterize\nthe explanations LLMs give for their moral choices and find that fairness is\nthe most dominant supporting reason behind GPT-4\'s decisions and utilitarianism\nby GPT-3. We also discover ""language inequality"" (which we define as the\nmodel\'s different development levels in different languages) in a series of\nmeta-properties of moral decision making.\n', '  Making moral judgments is an essential step toward developing ethical AI\nsystems. Prevalent approaches are mostly implemented in a bottom-up manner,\nwhich uses a large set of annotated data to train models based on crowd-sourced\nopinions about morality. These approaches have been criticized for\novergeneralizing the moral stances of a limited group of annotators and lacking\nexplainability. This work proposes a flexible top-down framework to steer\n(Large) Language Models (LMs) to perform moral reasoning with well-established\nmoral theories from interdisciplinary research. The theory-guided top-down\nframework can incorporate various moral theories. Our experiments demonstrate\nthe effectiveness of the proposed framework on datasets derived from moral\ntheories. Furthermore, we show the alignment between different moral theories\nand existing morality datasets. Our analysis exhibits the potential and flaws\nin existing resources (models and datasets) in developing explainable moral\njudgment-making systems.\n']",Moral Reasoning in Large Language Models,Large Language Models and Cognitive Abilities,Large Language Models,Large Language Models
266,32,266_braille_impaired_accessibility_blind,"['braille', 'impaired', 'accessibility', 'blind', 'impairments', 'disabilities', 'accessible', 'sighted', 'recognition', 'visual']","['impaired', 'blind', 'braille', 'accessibility', 'people', 'reality', 'impairments', 'inclusive', 'assistive', 'disabilities']","['  In recent years, advancements in Natural Language Processing (NLP) techniques\nhave revolutionized the field of accessibility and exclusivity of testing,\nparticularly for visually impaired students (VIS). CBT has shown in years back\nits relevance in terms of administering exams electronically, making the test\nprocess easier, providing quicker and more accurate results, and offering\ngreater flexibility and accessibility for candidates. Yet, its relevance was\nnot felt by the visually impaired students as they cannot access printed\ndocuments. Hence, in this paper, we present an NLP-driven Computer-Based Test\nguide for visually impaired students. It employs a speech technology\npre-trained methods to provide real-time assistance and support to visually\nimpaired students. The system utilizes NLP technologies to convert the\ntext-based questions and the associated options in a machine-readable format.\nSubsequently, the speech technology pre-trained model processes the converted\ntext enabling the VIS to comprehend and analyze the content. Furthermore, we\nvalidated that this pre-trained model is not perverse by testing for accuracy\nusing sample audio datasets labels (A, B, C, D, E, F, G) to compare with the\nvoice recordings obtained from 20 VIS which is been predicted by the system to\nattain values for precision, recall, and F1-scores. These metrics are used to\nassess the performance of the pre-trained model and have indicated that it is\nproficient enough to give its better performance to the evaluated system. The\nmethodology adopted for this system is Object Oriented Analysis and Design\nMethodology (OOADM) where Objects are discussed and built by modeling\nreal-world instances.\n', '  Visually impaired people are a large group who can only use braille for\nreading and writing. However, the lack of special educational resources is the\nbottleneck for educating them. Educational equity is a reflection of the level\nof social civilization, cultural equality, and individual dignity. Facilitating\nand improving lifelong learning channels for the visually impaired is of great\nsignificance. Their written braille homework or exam papers cannot be\nunderstood by sighted teachers, because of the lack of a highly accurate\nbraille translation system, especially in Chinese which has tone marks. braille\nwriters often omit tone marks to save space, leading to confusion when braille\nwith the same consonants and vowels is translated into Chinese. Previous\nalgorithms were insufficient in extracting contextual information, resulting in\nlow accuracy of braille translations into Chinese. This project informatively\nfine-tuned the mT5 model with an Encoder-decoder architecture for braille to\nChinese character conversion. This research created a training set of braille\nand corresponding Chinese text from the Leipzig Corpora. This project\nsignificantly reduced the confusion in braille, achieving $62.4$ and $62.3$\nBLEU scores in the validation and test sets, with a curriculum learning\nfine-tuning method. By incorporating the braille recognition algorithm, this\nproject is the first publicly available braille translation system and can\nbenefit lots of visually impaired students and families who are preparing for\nthe Chinese College Test and help to propel their college dreams in the future.\nThere is a demo on our homepage\\footnote{\\url{https://vision-braille.com/}}.\n', ""  The prevalence of mobile technology offers unique opportunities for\naddressing healthcare challenges, especially for individuals with visual\nimpairments. This paper explores the development and implementation of a deep\nlearning-based mobile application designed to assist blind and visually\nimpaired individuals in real-time pill identification. Utilizing the YOLO\nframework, the application aims to accurately recognize and differentiate\nbetween various pill types through real-time image processing on mobile\ndevices. The system incorporates Text-to- Speech (TTS) to provide immediate\nauditory feedback, enhancing usability and independence for visually impaired\nusers. Our study evaluates the application's effectiveness in terms of\ndetection accuracy and user experience, highlighting its potential to improve\nmedication management and safety among the visually impaired community.\nKeywords-Deep Learning; YOLO Framework; Mobile Application; Visual Impairment;\nPill Identification; Healthcare\n""]",Assistive Technology for Visually Impaired Individuals,Computer Vision Applications,Computer Vision,Computer Vision
267,31,267_personas_persona_personascore_personae,"['personas', 'persona', 'personascore', 'personae', 'profiles', 'agent', 'personality', 'characters', 'character', 'roleeval']","['persona', 'playing', 'role', 'characters', 'character', 'personas', 'dialogues', 'agents', 'conversational', 'play']","['  The concept of persona, originally adopted in dialogue literature, has\nre-surged as a promising framework for tailoring large language models (LLMs)\nto specific context (e.g., personalized search, LLM-as-a-judge). However, the\ngrowing research on leveraging persona in LLMs is relatively disorganized and\nlacks a systematic taxonomy. To close the gap, we present a comprehensive\nsurvey to categorize the current state of the field. We identify two lines of\nresearch, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and\n(2) LLM Personalization, where LLMs take care of user personas. Additionally,\nwe introduce existing methods for LLM personality evaluation. To the best of\nour knowledge, we present the first survey for role-playing and personalization\nin LLMs under the unified view of persona. We continuously maintain a paper\ncollection to foster future endeavors:\nhttps://github.com/MiuLab/PersonaLLM-Survey\n', ""  With the recent introduction of Assistants API, it is expected that\ndocument-based language models will be actively used in various domains,\nespecially Role-playing. However, a key challenge lies in utilizing\nprotagonist's persona: Assistants API often fails to achieve with its search\nbecause the information extraction part is different each time and it often\nomits important information such as protagonist's backstory or relationships.\nIt is hard to maintain a consistent persona simply by using the persona\ndocument as input to the Assistants API. To address the challenge of achieving\nstable persona consistency, we propose CharacterGPT, a novel persona\nreconstruction framework to alleviate the shortcomings of the Assistants API.\nOur method involves Character Persona Training (CPT), an effective persona\nrebuilding process that updates the character persona by extracting the\ncharacter's traits from given summary of the novel for each character as if the\nstory in a novel progresses. In our experiments, we ask each character to take\nthe Big Five Inventory personality test in various settings and analyze the\nresults. To assess whether it can think outside the box, we let each character\ngenerate short novels. Extensive experiments and human evaluation demonstrate\nthat CharacterGPT presents new possibilities for role-playing agent research.\nCode and results are available at: https://github.com/Jeiyoon/charactergpt\n"", '  This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.\n']",Persona-based Role-Playing with Language Models,Persona-Based Natural Language Processing,Natural Language Processing,Natural Language Processing
268,31,268_linguistic_linguistics_psycholinguistics_syntactic,"['linguistic', 'linguistics', 'psycholinguistics', 'syntactic', 'sentences', 'psycholinguistic', 'linguists', 'grammaticality', 'judgments', 'language']","['linguistic', 'syntactic', 'word', 'surprisal', 'sentence', 'reading', 'judgements', 'linguists', 'lexical', 'order']","[""  To date, most investigations on surprisal and entropy effects in reading have\nbeen conducted on the group level, disregarding individual differences. In this\nwork, we revisit the predictive power of surprisal and entropy measures\nestimated from a range of language models (LMs) on data of human reading times\nas a measure of processing effort by incorporating information of language\nusers' cognitive capacities. To do so, we assess the predictive power of\nsurprisal and entropy estimated from generative LMs on reading data obtained\nfrom individuals who also completed a wide range of psychometric tests.\nSpecifically, we investigate if modulating surprisal and entropy relative to\ncognitive scores increases prediction accuracy of reading times, and we examine\nwhether LMs exhibit systematic biases in the prediction of reading times for\ncognitively high- or low-performing groups, revealing what type of\npsycholinguistic subject a given LM emulates. Our study finds that in most\ncases, incorporating cognitive capacities increases predictive power of\nsurprisal and entropy on reading times, and that generally, high performance in\nthe psychometric tests is associated with lower sensitivity to predictability\neffects. Finally, our results suggest that the analyzed LMs emulate readers\nwith lower verbal intelligence, suggesting that for a given target group (i.e.,\nindividuals with high verbal intelligence), these LMs provide less accurate\npredictability estimates.\n"", ""  The effect of syntactic priming exhibits three well-documented empirical\nproperties: the lexical boost, the inverse frequency effect, and the\nasymmetrical decay. We aim to show how these three empirical phenomena can be\nreconciled in a general learning framework, the hierarchical Bayesian model\n(HBM). The model represents syntactic knowledge in a hierarchical structure of\nsyntactic statistics, where a lower level represents the verb-specific biases\nof syntactic decisions, and a higher level represents the abstract bias as an\naggregation of verb-specific biases. This knowledge is updated in response to\nexperience by Bayesian inference. In simulations, we show that the HBM captures\nthe above-mentioned properties of syntactic priming. The results indicate that\nsome properties of priming which are usually explained by a residual activation\naccount can also be explained by an implicit learning account. We also discuss\nthe model's implications for the lexical basis of syntactic priming.\n"", '  There have been apparently conflicting claims over the syntax-semantics\nrelationship in child acquisition. However, few of them have assessed the\nchild\'s path toward the acquisition of recursive relative clauses (RRCs). The\nauthors of the current paper did experiments to investigate 3- to 11-year-olds\'\nmost-structured elicited production of eight Mandarin RRCs in a 4 (syntactic\ntypes)*2 (semantic conditions) design. The four syntactic types were RRCs with\na subject-gapped RC embedded in an object-gapped RC (SORRCs), RRCs with an\nobject-gapped RC embedded in another object-gapped RC (OORRCs), RRCs with an\nobject-gapped RC embedded in a subject-gapped RC (OSRRCs), and RRCs with a\nsubject-gapped RC embedded in another subject-gapped RC (SSRRCs). Each\nsyntactic type was put in two conditions differing in internal semantics:\nirreversible internal semantics (IIS) and reversible internal semantics (RIS).\nFor example, ""the balloon that [the girl that _ eats the banana] holds _"" is\nSORRCs in the IIS condition; ""the monkey that [the dog that _ bites the pig]\nhits_"" is SORRCs in the RIS condition. For each target, the participants were\nprovided with a speech-visual stimulus constructing a condition of irreversible\nexternal semantics (IES). The results showed that SSRRCs, OSRRCs and SORRCs in\nthe IIS-IES condition were produced two years earlier than their counterparts\nin the RIS-IES condition. Thus, a 2-stage development path is proposed: the\nlanguage acquisition device starts with the interface between (irreversible)\nsyntax and IIS, and ends with the interface between syntax and IES, both\nabiding by the syntax-semantic interface principle.\n']",Psycholinguistics of Sentence Processing,Natural Language Processing and Linguistics,Natural Language Processing,Natural Language Processing
269,31,269_recommender_recommendation_personalized_collaborative,"['recommender', 'recommendation', 'personalized', 'collaborative', 'domains', 'cdsr', 'interests', 'domain', 'cdr', 'hypergraph']","['domain', 'domains', 'recommendation', 'overlapping', 'cross', 'cold', 'user', 'users', 'start', 'target']","[""  Cross-domain recommendation (CDR) has been proven as a promising way to\ntackle the user cold-start problem, which aims to make recommendations for\nusers in the target domain by transferring the user preference derived from the\nsource domain. Traditional CDR studies follow the embedding and mapping (EMCDR)\nparadigm, which transfers user representations from the source to target domain\nby learning a user-shared mapping function, neglecting the user-specific\npreference. Recent CDR studies attempt to learn user-specific mapping functions\nin meta-learning paradigm, which regards each user's CDR as an individual task,\nbut neglects the preference correlations among users, limiting the beneficial\ninformation for user representations. Moreover, both of the paradigms neglect\nthe explicit user-item interactions from both domains during the mapping\nprocess. To address the above issues, this paper proposes a novel CDR framework\nwith neural process (NP), termed as CDRNP. Particularly, it develops the\nmeta-learning paradigm to leverage user-specific preference, and further\nintroduces a stochastic process by NP to capture the preference correlations\namong the overlapping and cold-start users, thus generating more powerful\nmapping functions by mapping the user-specific preference and common preference\ncorrelations to a predictive probability distribution. In addition, we also\nintroduce a preference remainer to enhance the common preference from the\noverlapping users, and finally devises an adaptive conditional decoder with\npreference modulation to make prediction for cold-start users with items in the\ntarget domain. Experimental results demonstrate that CDRNP outperforms previous\nSOTA methods in three real-world CDR scenarios.\n"", '  Cross-domain recommendation (CDR) extends conventional recommender systems by\nleveraging user-item interactions from dense domains to mitigate data sparsity\nand the cold start problem. While CDR offers substantial potential for\nenhancing recommendation performance, most existing CDR methods suffer from\nsensitivity to the ratio of overlapping users and intrinsic discrepancy between\nsource and target domains. To overcome these limitations, in this work, we\nexplore the application of graph signal processing (GSP) in CDR scenarios. We\npropose CGSP, a unified CDR framework based on GSP, which employs a\ncross-domain similarity graph constructed by flexibly combining target-only\nsimilarity and source-bridged similarity. By processing personalized graph\nsignals computed for users from either the source or target domain, our\nframework effectively supports both inter-domain and intra-domain\nrecommendations. Our empirical evaluation demonstrates that CGSP consistently\noutperforms various encoder-based CDR approaches in both intra-domain and\ninter-domain recommendation scenarios, especially when the ratio of overlapping\nusers is low, highlighting its significant practical implication in real-world\napplications.\n', ""  Cross-Domain Recommendation (CDR) is a promising paradigm inspired by\ntransfer learning to solve the cold-start problem in recommender systems.\nExisting state-of-the-art CDR methods train an explicit mapping function to\ntransfer the cold-start users from a data-rich source domain to a target\ndomain. However, a limitation of these methods is that the mapping function is\ntrained on overlapping users across domains, while only a small number of\noverlapping users are available for training. By visualizing the loss landscape\nof the existing CDR model, we find that training on a small number of\noverlapping users causes the model to converge to sharp minima, leading to poor\ngeneralization. Based on this observation, we leverage loss-geometry-based\nmachine learning approach and propose a novel CDR method called Sharpness-Aware\nCDR (SCDR). Our proposed method simultaneously optimizes recommendation loss\nand loss sharpness, leading to better generalization with theoretical\nguarantees. Empirical studies on real-world datasets demonstrate that SCDR\nsignificantly outperforms the other CDR models for cold-start recommendation\ntasks, while concurrently enhancing the model's robustness to adversarial\nattacks.\n""]",Cross-Domain Recommendation Systems,Recommender Systems and Personalization Techniques,Recommender Systems and Personalization,Recommender Systems and Personalization
270,31,270_fairness_discrimination_unfairness_ai,"['fairness', 'discrimination', 'unfairness', 'ai', 'discriminatory', 'ethical', 'bias', 'ethics', 'unfair', 'biases']","['fairness', 'algorithmic', 'bias', 'biases', 'discrimination', 'fair', 'definitions', 'discriminatory', 'decisions', 'explanations']","[""  The integration of Artificial Intelligence (AI) into education has\ntransformative potential, providing tailored learning experiences and creative\ninstructional approaches. However, the inherent biases in AI algorithms hinder\nthis improvement by unintentionally perpetuating prejudice against specific\ndemographics, especially in human-centered applications like education. This\nsurvey delves deeply into the developing topic of algorithmic fairness in\neducational contexts, providing a comprehensive evaluation of the diverse\nliterature on fairness, bias, and ethics in AI-driven educational applications.\nIt identifies the common forms of biases, such as data-related, algorithmic,\nand user-interaction, that fundamentally undermine the accomplishment of\nfairness in AI teaching aids. By outlining existing techniques for mitigating\nthese biases, ranging from varied data gathering to algorithmic fairness\ninterventions, the survey emphasizes the critical role of ethical\nconsiderations and legal frameworks in shaping a more equitable educational\nenvironment. Furthermore, it guides readers through the complexities of\nfairness measurements, methods, and datasets, shedding light on the way to bias\nreduction. Despite these gains, this survey highlights long-standing issues,\nsuch as achieving a balance between fairness and accuracy, as well as the need\nfor diverse datasets. Overcoming these challenges and ensuring the ethical and\nfair use of AI's promise in education call for a collaborative,\ninterdisciplinary approach.\n"", '  Reaching consensus on a commonly accepted definition of AI Fairness has long\nbeen a central challenge in AI ethics and governance. There is a broad spectrum\nof views across society on what the concept of fairness means and how it should\nbest be put to practice. In this workbook, we tackle this challenge by\nexploring how a context-based and society-centred approach to understanding AI\nFairness can help project teams better identify, mitigate, and manage the many\nways that unfair bias and discrimination can crop up across the AI project\nworkflow.\n  We begin by exploring how, despite the plurality of understandings about the\nmeaning of fairness, priorities of equality and non-discrimination have come to\nconstitute the broadly accepted core of its application as a practical\nprinciple. We focus on how these priorities manifest in the form of equal\nprotection from direct and indirect discrimination and from discriminatory\nharassment. These elements form ethical and legal criteria based upon which\ninstances of unfair bias and discrimination can be identified and mitigated\nacross the AI project workflow.\n  We then take a deeper dive into how the different contexts of the AI project\nlifecycle give rise to different fairness concerns. This allows us to identify\nseveral types of AI Fairness (Data Fairness, Application Fairness, Model Design\nand Development Fairness, Metric-Based Fairness, System Implementation\nFairness, and Ecosystem Fairness) that form the basis of a multi-lens approach\nto bias identification, mitigation, and management. Building on this, we\ndiscuss how to put the principle of AI Fairness into practice across the AI\nproject workflow through Bias Self-Assessment and Bias Risk Management as well\nas through the documentation of metric-based fairness criteria in a Fairness\nPosition Statement.\n', ""  The rise in the use of AI/ML applications across industries has sparked more\ndiscussions about the fairness of AI/ML in recent times. While prior research\non the fairness of AI/ML exists, there is a lack of empirical studies focused\non understanding the perspectives and experiences of AI practitioners in\ndeveloping a fair AI/ML system. Understanding AI practitioners' perspectives\nand experiences on the fairness of AI/ML systems are important because they are\ndirectly involved in its development and deployment and their insights can\noffer valuable real-world perspectives on the challenges associated with\nensuring fairness in AI/ML systems. We conducted semi-structured interviews\nwith 22 AI practitioners to investigate their understanding of what a 'fair\nAI/ML' is, the challenges they face in developing a fair AI/ML system, the\nconsequences of developing an unfair AI/ML system, and the strategies they\nemploy to ensure AI/ML system fairness. We developed a framework showcasing the\nrelationship between AI practitioners' understanding of 'fair AI/ML' system and\n(i) their challenges in its development, (ii) the consequences of developing an\nunfair AI/ML system, and (iii) strategies used to ensure AI/ML system fairness.\nBy exploring AI practitioners' perspectives and experiences, this study\nprovides actionable insights to enhance AI/ML fairness, which may promote\nfairer systems, reduce bias, and foster public trust in AI technologies.\nAdditionally, we also identify areas for further investigation and offer\nrecommendations to aid AI practitioners and AI companies in navigating\nfairness.\n""]",AI Fairness and Bias in Education and Applications,Artificial Intelligence Ethics and Governance,Artificial Intelligence Applications and Implications,Artificial Intelligence Applications and Implications
271,31,271_autoencoders_autoencoder_transcoders_sparse,"['autoencoders', 'autoencoder', 'transcoders', 'sparse', 'attention', 'features', 'interpretable', 'interpretability', 'activations', 'circuits']","['circuit', 'circuits', 'mechanistic', 'sparse', 'interpretability', 'autoencoders', 'patching', 'dictionary', 'interpretable', 'transcoders']","[""  Identifying the features learned by neural networks is a core challenge in\nmechanistic interpretability. Sparse autoencoders (SAEs), which learn a sparse,\novercomplete dictionary that reconstructs a network's internal activations,\nhave been used to identify these features. However, SAEs may learn more about\nthe structure of the datatset than the computational structure of the network.\nThere is therefore only indirect reason to believe that the directions found in\nthese dictionaries are functionally important to the network. We propose\nend-to-end (e2e) sparse dictionary learning, a method for training SAEs that\nensures the features learned are functionally important by minimizing the KL\ndivergence between the output distributions of the original model and the model\nwith SAE activations inserted. Compared to standard SAEs, e2e SAEs offer a\nPareto improvement: They explain more network performance, require fewer total\nfeatures, and require fewer simultaneously active features per datapoint, all\nwith no cost to interpretability. We explore geometric and qualitative\ndifferences between e2e SAE features and standard SAE features. E2e dictionary\nlearning brings us closer to methods that can explain network behavior\nconcisely and accurately. We release our library for training e2e SAEs and\nreproducing our analysis at https://github.com/ApolloResearch/e2e_sae\n"", ""  Circuit analysis of any certain model behavior is a central task in\nmechanistic interpretability. We introduce our circuit discovery pipeline with\nSparse Autoencoders (SAEs) and a variant called Transcoders. With these two\nmodules inserted into the model, the model's computation graph with respect to\nOV and MLP circuits becomes strictly linear. Our methods do not require linear\napproximation to compute the causal effect of each node. This fine-grained\ngraph identifies both end-to-end and local circuits accounting for either\nlogits or intermediate features. We can scalably apply this pipeline with a\ntechnique called Hierarchical Attribution. We analyze three kinds of circuits\nin GPT-2 Small: bracket, induction, and Indirect Object Identification\ncircuits. Our results reveal new findings underlying existing discoveries.\n"", '  Decomposing model activations into interpretable components is a key open\nproblem in mechanistic interpretability. Sparse autoencoders (SAEs) are a\npopular method for decomposing the internal activations of trained transformers\ninto sparse, interpretable features, and have been applied to MLP layers and\nthe residual stream. In this work we train SAEs on attention layer outputs and\nshow that also here SAEs find a sparse, interpretable decomposition. We\ndemonstrate this on transformers from several model families and up to 2B\nparameters.\n  We perform a qualitative study of the features computed by attention layers,\nand find multiple families: long-range context, short-range context and\ninduction features. We qualitatively study the role of every head in GPT-2\nSmall, and estimate that at least 90% of the heads are polysemantic, i.e. have\nmultiple unrelated roles.\n  Further, we show that Sparse Autoencoders are a useful tool that enable\nresearchers to explain model behavior in greater detail than prior work. For\nexample, we explore the mystery of why models have so many seemingly redundant\ninduction heads, use SAEs to motivate the hypothesis that some are long-prefix\nwhereas others are short-prefix, and confirm this with more rigorous analysis.\nWe use our SAEs to analyze the computation performed by the Indirect Object\nIdentification circuit (Wang et al.), validating that the SAEs find causally\nmeaningful intermediate variables, and deepening our understanding of the\nsemantics of the circuit. We open-source the trained SAEs and a tool for\nexploring arbitrary prompts through the lens of Attention Output SAEs.\n']",Interpretable Neural Networks with Sparse Autoencoders,Interpretable Deep Learning Models,Explainable Artificial Intelligence,Artificial Intelligence and Machine Learning Interpretability and Explainability
272,31,272_fairness_graphpar_bias_discriminatory,"['fairness', 'graphpar', 'bias', 'discriminatory', 'networks', 'graphgini', 'biases', 'unfair', 'unfairness', 'fairgb']","['fairness', 'fair', 'sensitive', 'graph', 'attributes', 'groups', 'bias', 'attribute', 'node', 'demographic']","['  Fairness-aware graph learning has gained increasing attention in recent\nyears. Nevertheless, there lacks a comprehensive benchmark to evaluate and\ncompare different fairness-aware graph learning methods, which blocks\npractitioners from choosing appropriate ones for broader real-world\napplications. In this paper, we present an extensive benchmark on ten\nrepresentative fairness-aware graph learning methods. Specifically, we design a\nsystematic evaluation protocol and conduct experiments on seven real-world\ndatasets to evaluate these methods from multiple perspectives, including group\nfairness, individual fairness, the balance between different fairness criteria,\nand computational efficiency. Our in-depth analysis reveals key insights into\nthe strengths and limitations of existing methods. Additionally, we provide\npractical guidance for applying fairness-aware graph learning methods in\napplications. To the best of our knowledge, this work serves as an initial step\ntowards comprehensively understanding representative fairness-aware graph\nlearning methods to facilitate future advancements in this area.\n', '  Graph Neural Networks (GNNs) have been widely used for various types of graph\ndata processing and analytical tasks in different domains. Training GNNs over\ncentralized graph data can be infeasible due to privacy concerns and regulatory\nrestrictions. Thus, federated learning (FL) becomes a trending solution to\naddress this challenge in a distributed learning paradigm. However, as GNNs may\ninherit historical bias from training data and lead to discriminatory\npredictions, the bias of local models can be easily propagated to the global\nmodel in distributed settings. This poses a new challenge in mitigating bias in\nfederated GNNs. To address this challenge, we propose $\\text{F}^2$GNN, a Fair\nFederated Graph Neural Network, that enhances group fairness of federated GNNs.\nAs bias can be sourced from both data and learning algorithms, $\\text{F}^2$GNN\naims to mitigate both types of bias under federated settings. First, we provide\ntheoretical insights on the connection between data bias in a training graph\nand statistical fairness metrics of the trained GNN models. Based on the\ntheoretical analysis, we design $\\text{F}^2$GNN which contains two key\ncomponents: a fairness-aware local model update scheme that enhances group\nfairness of the local models on the client side, and a fairness-weighted global\nmodel update scheme that takes both data bias and fairness metrics of local\nmodels into consideration in the aggregation process. We evaluate\n$\\text{F}^2$GNN empirically versus a number of baseline methods, and\ndemonstrate that $\\text{F}^2$GNN outperforms these baselines in terms of both\nfairness and model accuracy.\n', '  Graph neural networks (GNNs) have emerged as a powerful tool for analyzing\nand learning from complex data structured as graphs, demonstrating remarkable\neffectiveness in various applications, such as social network analysis,\nrecommendation systems, and drug discovery. However, despite their impressive\nperformance, the fairness problem has increasingly gained attention as a\ncrucial aspect to consider. Existing research in graph learning focuses on\neither group fairness or individual fairness. However, since each concept\nprovides unique insights into fairness from distinct perspectives, integrating\nthem into a fair graph neural network system is crucial. To the best of our\nknowledge, no study has yet to comprehensively tackle both individual and group\nfairness simultaneously. In this paper, we propose a new concept of individual\nfairness within groups and a novel framework named Fairness for Group and\nIndividual (FairGI), which considers both group fairness and individual\nfairness within groups in the context of graph learning. FairGI employs the\nsimilarity matrix of individuals to achieve individual fairness within groups,\nwhile leveraging adversarial learning to address group fairness in terms of\nboth Equal Opportunity and Statistical Parity. The experimental results\ndemonstrate that our approach not only outperforms other state-of-the-art\nmodels in terms of group fairness and individual fairness within groups, but\nalso exhibits excellent performance in population-level individual fairness,\nwhile maintaining comparable prediction accuracy.\n']",Fairness in Graph Neural Networks,Fairness in Artificial Intelligence and Machine Learning,Fairness and Bias in Artificial Intelligence,Fairness and Bias in Artificial Intelligence
273,31,273_pollution_emissions_meteorological_environmental,"['pollution', 'emissions', 'meteorological', 'environmental', 'predicting', 'forecasting', 'climate', 'aerosol', 'pollutants', 'pollutant']","['air', 'pollution', 'concentrations', 'pollutants', 'stations', 'indoor', 'radon', 'pollutant', 'outdoor', 'concentration']","['  Policymakers frequently analyze air quality and climate change in isolation,\ndisregarding their interactions. This study explores the influence of specific\nclimate factors on air quality by contrasting a regression model with K-Means\nClustering, Hierarchical Clustering, and Random Forest techniques. We employ\nPhysics-based Deep Learning (PBDL) and Long Short-Term Memory (LSTM) to examine\nthe air pollution predictions. Our analysis utilizes ten years (2009-2018) of\ndaily traffic, weather, and air pollution data from three major cities in\nNorway. Findings from feature selection reveal a correlation between rising\nheating degree days and heightened air pollution levels, suggesting increased\nheating activities in Norway are a contributing factor to worsening air\nquality. PBDL demonstrates superior accuracy in air pollution predictions\ncompared to LSTM. This paper contributes to the growing literature on PBDL\nmethods for more accurate air pollution predictions using environmental\nvariables, aiding policymakers in formulating effective data-driven climate\npolicies.\n', ""  Ambient air pollution remains a critical issue in the United Kingdom, where\ndata on air pollution concentrations form the foundation for interventions\naimed at improving air quality. However, the current air pollution monitoring\nstation network in the UK is characterized by spatial sparsity, heterogeneous\nplacement, and frequent temporal data gaps, often due to issues such as power\noutages. We introduce a scalable data-driven supervised machine learning model\nframework designed to address temporal and spatial data gaps by filling missing\nmeasurements. This approach provides a comprehensive dataset for England\nthroughout 2018 at a 1kmx1km hourly resolution. Leveraging machine learning\ntechniques and real-world data from the sparsely distributed monitoring\nstations, we generate 355,827 synthetic monitoring stations across the study\narea, yielding data valued at approximately \\pounds70 billion. Validation was\nconducted to assess the model's performance in forecasting, estimating missing\nlocations, and capturing peak concentrations. The resulting dataset is of\nparticular interest to a diverse range of stakeholders engaged in downstream\nassessments supported by outdoor air pollution concentration data for NO2, O3,\nPM10, PM2.5, and SO2. This resource empowers stakeholders to conduct studies at\na higher resolution than was previously possible.\n"", '  Ambient air pollution is a pervasive issue with wide-ranging effects on human\nhealth, ecosystem vitality, and economic structures. Utilizing data on ambient\nair pollution concentrations, researchers can perform comprehensive analyses to\nuncover the multifaceted impacts of air pollution across society. To this end,\nwe introduce Environmental Insights, an open-source Python package designed to\ndemocratize access to air pollution concentration data. This tool enables users\nto easily retrieve historical air pollution data and employ a Machine Learning\nmodel for forecasting potential future conditions. Moreover, Environmental\nInsights includes a suite of tools aimed at facilitating the dissemination of\nanalytical findings and enhancing user engagement through dynamic\nvisualizations. This comprehensive approach ensures that the package caters to\nthe diverse needs of individuals looking to explore and understand air\npollution trends and their implications.\n']",Air Pollution Analysis and Prediction,Environmental and Transportation Predictive Analytics,Transportation Systems and Environmental Analytics,Transportation Systems and Environmental Analytics
274,31,274_gestures_gesture_gestureprint_touch,"['gestures', 'gesture', 'gestureprint', 'touch', 'hands', 'hand', 'multimodal', 'touchpad', 'recognition', 'classifiers']","['gesture', 'gestures', 'hand', 'recognition', 'electromyography', 'movement', 'muscle', 'tap', 'handwashing', 'forearm']","[""  Hand gestures can provide a natural means of human-computer interaction and\nenable people who cannot speak to communicate efficiently. Existing hand\ngesture recognition methods heavily depend on pre-defined gestures, however,\nmotor-impaired individuals require new gestures tailored to each individual's\ngesture motion and style. Gesture samples collected from different persons have\ndistribution shifts due to their health conditions, the severity of the\ndisability, motion patterns of the arms, etc. In this paper, we introduce the\nLatent Embedding Exploitation (LEE) mechanism in our replay-based Few-Shot\nContinual Learning (FSCL) framework that significantly improves the performance\nof fine-tuning a model for out-of-distribution data. Our method produces a\ndiversified latent feature space by leveraging a preserved latent embedding\nknown as gesture prior knowledge, along with intra-gesture divergence derived\nfrom two additional embeddings. Thus, the model can capture latent statistical\nstructure in highly variable gestures with limited samples. We conduct an\nexperimental evaluation using the SmartWatch Gesture and the Motion Gesture\ndatasets. The proposed method results in an average test accuracy of 57.0%,\n64.6%, and 69.3% by using one, three, and five samples for six different\ngestures. Our method helps motor-impaired persons leverage wearable devices,\nand their unique styles of movement can be learned and applied in\nhuman-computer interaction and social communication. Code is available at:\nhttps://github.com/riyadRafiq/wearable-latent-embedding-exploitation\n"", '  As robots are expected to get more involved in people\'s everyday lives,\nframeworks that enable intuitive user interfaces are in demand. Hand gesture\nrecognition systems provide a natural way of communication and, thus, are an\nintegral part of seamless Human-Robot Interaction (HRI). Recent years have\nwitnessed an immense evolution of computational models powered by deep\nlearning. However, state-of-the-art models fall short in expanding across\ndifferent gesture domains, such as emblems and co-speech. In this paper, we\npropose a novel hybrid hand gesture recognition system. Our architecture\nenables learning both static and dynamic gestures: by capturing a so-called\n""snapshot"" of the gesture performance at its peak, we integrate the hand pose\nalong with the dynamic movement. Moreover, we present a method for analyzing\nthe motion profile of a gesture to uncover its dynamic characteristics and\nwhich allows regulating a static channel based on the amount of motion. Our\nevaluation demonstrates the superiority of our approach on two gesture\nbenchmarks compared to a CNNLSTM baseline. We also provide an analysis on a\ngesture class basis that unveils the potential of our Snapture architecture for\nperformance improvements. Thanks to its modular implementation, our framework\nallows the integration of other multimodal data like facial expressions and\nhead tracking, which are important cues in HRI scenarios, into one\narchitecture. Thus, our work contributes both to gesture recognition research\nand machine learning applications for non-verbal communication with robots.\n', '  Artificial intelligence (AI) has made significant advances in recent years\nand opened up new possibilities in exploring applications in various fields\nsuch as biomedical, robotics, education, industry, etc. Among these fields,\nhuman hand gesture recognition is a subject of study that has recently emerged\nas a research interest in robotic hand control using electromyography (EMG).\nSurface electromyography (sEMG) is a primary technique used in EMG, which is\npopular due to its non-invasive nature and is used to capture gesture movements\nusing signal acquisition devices placed on the surface of the forearm.\nMoreover, these signals are pre-processed to extract significant handcrafted\nfeatures through time and frequency domain analysis. These are helpful and act\nas input to machine learning (ML) models to identify hand gestures. However,\nhandling multiple classes and biases are major limitations that can affect the\nperformance of an ML model. Therefore, to address this issue, a new mixture of\nexperts extra tree (MEET) model is proposed to identify more accurate and\neffective hand gesture movements. This model combines individual ML models\nreferred to as experts, each focusing on a minimal class of two. Moreover, a\nfully trained model known as the gate is employed to weigh the output of\nindividual expert models. This amalgamation of the expert models with the gate\nmodel is known as a mixture of experts extra tree (MEET) model. In this study,\nfour subjects with six hand gesture movements have been considered and their\nidentification is evaluated among eleven models, including the MEET classifier.\nResults elucidate that the MEET classifier performed best among other\nalgorithms and identified hand gesture movement accurately.\n']",Hand Gesture Recognition Systems,Multimodal Human-Computer Interaction and Communication,Multimodal Learning and Applications,Multimodal Learning and Vision-Language Models
275,30,275_acoustic_bioacoustics_recordings_bioacoustic,"['acoustic', 'bioacoustics', 'recordings', 'bioacoustic', 'birdnet', 'audio', 'supervised', 'recognizing', 'recognition', 'underwater']","['underwater', 'bird', 'acoustic', 'animal', 'sounds', 'species', 'bioacoustics', 'bioacoustic', 'sound', 'audio']","['  This paper comprehensively reviews recent advances in underwater acoustic\nsignal denoising, an area critical for improving the reliability and clarity of\nunderwater communication and monitoring systems. Despite significant progress\nin the field, the complex nature of underwater environments poses unique\nchallenges that complicate the denoising process. We begin by outlining the\nfundamental challenges associated with underwater acoustic signal processing,\nincluding signal attenuation, noise variability, and the impact of\nenvironmental factors. The review then systematically categorizes and discusses\nvarious denoising algorithms, such as conventional, decomposition-based, and\nlearning-based techniques, highlighting their applications, advantages, and\nlimitations. Evaluation metrics and experimental datasets are also reviewed.\nThe paper concludes with a list of open questions and recommendations for\nfuture research directions, emphasizing the need for developing more robust\ndenoising techniques that can adapt to the dynamic underwater acoustic\nenvironment.\n', '  This paper presents a novel deep learning approach for analyzing massive\nunderwater acoustic data by leveraging a model trained on a broad spectrum of\nnon-underwater (aerial) sounds. Recognizing the challenge in labeling vast\namounts of underwater data, we propose a two-fold methodology to accelerate\nthis labor-intensive procedure.\n  The first part of our approach involves PCA and UMAP visualization of the\nunderwater data using the feature vectors of an aerial sound recognition model.\nThis enables us to cluster the data in a two dimensional space and listen to\npoints within these clusters to understand their defining characteristics. This\ninnovative method simplifies the process of selecting candidate labels for\nfurther training.\n  In the second part, we train a neural network model using both the selected\nunderwater data and the non-underwater dataset. We conducted a quantitative\nanalysis to measure the precision, recall, and F1 score of our model for\nrecognizing airgun sounds, a common type of underwater sound. The F1 score\nachieved by our model exceeded 84.3%, demonstrating the effectiveness of our\napproach in analyzing underwater acoustic data.\n  The methodology presented in this paper holds significant potential to reduce\nthe amount of labor required in underwater data analysis and opens up new\npossibilities for further research in the field of cross-domain data analysis.\n', '  With the rapid advancement of technology, the recognition of underwater\nacoustic signals in complex environments has become increasingly crucial.\nCurrently, mainstream underwater acoustic signal recognition relies primarily\non time-frequency analysis to extract spectral features, finding widespread\napplications in the field. However, existing recognition methods heavily depend\non expert systems, facing limitations such as restricted knowledge bases and\nchallenges in handling complex relationships. These limitations stem from the\ncomplexity and maintenance difficulties associated with rules or inference\nengines. Recognizing the potential advantages of deep learning in handling\nintricate relationships, this paper proposes a method utilizing neural networks\nfor underwater acoustic signal recognition. The proposed approach involves\ncontinual learning of features extracted from spectra for the classification of\nunderwater acoustic signals. Deep learning models can automatically learn\nabstract features from data and continually adjust weights during training to\nenhance classification performance.\n']",Underwater Acoustic Signal Processing and Recognition,Object Detection and Signal Processing in Underwater Environments,Signal Processing and Analysis in Complex Environments,Signal Processing and Analysis in Complex Environments
276,30,276_drift_drifts_driftlens_drifting,"['drift', 'drifts', 'driftlens', 'drifting', 'classifiers', 'drifted', 'streams', 'adaptive', 'stream', 'detection']","['drift', 'concept', 'drifts', 'streams', 'stream', 'detection', 'detectors', 'change', 'incremental', 'detector']","['  Continuous learning from an immense volume of data streams becomes\nexceptionally critical in the internet era. However, data streams often do not\nconform to the same distribution over time, leading to a phenomenon called\nconcept drift. Since a fixed static model is unreliable for inferring\nconcept-drifted data streams, establishing an adaptive mechanism for detecting\nconcept drift is crucial. Current methods for concept drift detection primarily\nassume that the labels or error rates of downstream models are given and/or\nunderlying statistical properties exist in data streams. These approaches,\nhowever, struggle to address high-dimensional data streams with intricate\nirregular distribution shifts, which are more prevalent in real-world\nscenarios. In this paper, we propose MCD-DD, a novel concept drift detection\nmethod based on maximum concept discrepancy, inspired by the maximum mean\ndiscrepancy. Our method can adaptively identify varying forms of concept drift\nby contrastive learning of concept embeddings without relying on labels or\nstatistical properties. With thorough experiments under synthetic and\nreal-world scenarios, we demonstrate that the proposed method outperforms\nexisting baselines in identifying concept drifts and enables qualitative\nanalysis with high explainability.\n', ""  Concept drift detection is crucial for many AI systems to ensure the system's\nreliability. These systems often have to deal with large amounts of data or\nreact in real-time. Thus, drift detectors must meet computational requirements\nor constraints with a comprehensive performance evaluation. However, so far,\nthe focus of developing drift detectors is on inference quality, e.g. accuracy,\nbut not on computational performance, such as runtime. Many of the previous\nworks consider computational performance only as a secondary objective and do\nnot have a benchmark for such evaluation. Hence, we propose and explain\nperformance engineering for unsupervised concept drift detection that reflects\non computational complexities, benchmarking, and performance analysis. We\nprovide the computational complexities of existing unsupervised drift detectors\nand discuss why further computational performance investigations are required.\nHence, we state and substantiate the aspects of a benchmark for unsupervised\ndrift detection reflecting on inference quality and computational performance.\nFurthermore, we demonstrate performance analysis practices that have proven\ntheir effectiveness in High-Performance Computing, by tracing two drift\ndetectors and displaying their performance data.\n"", '  Uncertain changes in data streams present challenges for machine learning\nmodels to dynamically adapt and uphold performance in real-time. Particularly,\nclassification boundary change, also known as real concept drift, is the major\ncause of classification performance deterioration. However, accurately\ndetecting real concept drift remains challenging because the theoretical\nfoundations of existing drift detection methods - two-sample distribution tests\nand monitoring classification error rate, both suffer from inherent limitations\nsuch as the inability to distinguish virtual drift (changes not affecting the\nclassification boundary, will introduce unnecessary model maintenance), limited\nstatistical power, or high computational cost. Furthermore, no existing\ndetection method can provide information on the trend of the drift, which could\nbe invaluable for model maintenance. This work presents a novel real concept\ndrift detection method based on Neighbor-Searching Discrepancy, a new statistic\nthat measures the classification boundary difference between two samples. The\nproposed method is able to detect real concept drift with high accuracy while\nignoring virtual drift. It can also indicate the direction of the\nclassification boundary change by identifying the invasion or retreat of a\ncertain class, which is also an indicator of separability change between\nclasses. A comprehensive evaluation of 11 experiments is conducted, including\nempirical verification of the proposed theory using artificial datasets, and\nexperimental comparisons with commonly used drift handling methods on\nreal-world datasets. The results show that the proposed theory is robust\nagainst a range of distributions and dimensions, and the drift detection method\noutperforms state-of-the-art alternative methods.\n']",Concept Drift Detection in Data Streams,Process Analysis and Adaptive Learning in Dynamic Data Environments,Machine Learning and Data-Driven Applications,Machine Learning and Data-Driven Applications
277,30,277_transcription_transcriptions_speech_voice,"['transcription', 'transcriptions', 'speech', 'voice', 'multilingual', 'decoder', 's2st', 'audio', 'decoding', 'translating']","['speech', 'translation', 'simultaneous', 'cascade', 'transducer', 'streaming', 'latency', 'end', 'text', 'parallel']","['  Speech segmentation is an essential part of speech translation (ST) systems\nin real-world scenarios. Since most ST models are designed to process speech\nsegments, long-form audio must be partitioned into shorter segments before\ntranslation. Recently, data-driven approaches for the speech segmentation task\nhave been developed. Although the approaches improve overall translation\nquality, a performance gap exists due to a mismatch between the models and ST\nsystems. In addition, the prior works require large self-supervised speech\nmodels, which consume significant computational resources. In this work, we\npropose a segmentation model that achieves better speech translation quality\nwith a small model size. We propose an ASR-with-punctuation task as an\neffective pre-training strategy for the segmentation model. We also show that\nproper integration of the speech segmentation model into the underlying ST\nsystem is critical to improve overall translation quality at inference time.\n', '  Existing speech-to-speech translation models fall into two camps: textless\nmodels trained with hundreds of hours of parallel speech data or unsupervised\nmodels that leverage text as an intermediate step. Both approaches limit\nbuilding speech-to-speech translation models for a wide range of languages, as\nthey exclude languages that are primarily spoken and language pairs that lack\nlarge-scale parallel speech data. We present a new framework for training\ntextless low-resource speech-to-speech translation (S2ST) systems that only\nneed dozens of hours of parallel speech data. We reformulate S2ST as a\nunit-to-unit seq2seq translation task, and start by pretraining a model on\nlarge-scale monolingual speech data. Then, we finetune it with a small amount\nof parallel speech data ($20-60$ hours). Lastly, we improve model performance\nthrough an unsupervised backtranslation objective. We train and evaluate our\nmodels for English-to-German, German-to-English and Marathi-to-English\ntranslation on three different domains (European Parliament, Common Voice, and\nAll India Radio) with single-speaker synthesized speech data. Evaluated using\nthe ASR-BLEU metric, our models achieve reasonable performance on all three\ndomains, with some being within 1-2 points of our supervised topline.\n', '  Simultaneous speech-to-speech translation (Simul-S2ST, a.k.a streaming speech\ntranslation) outputs target speech while receiving streaming speech inputs,\nwhich is critical for real-time communication. Beyond accomplishing translation\nbetween speech, Simul-S2ST requires a policy to control the model to generate\ncorresponding target speech at the opportune moment within speech inputs,\nthereby posing a double challenge of translation and policy. In this paper, we\npropose StreamSpeech, a direct Simul-S2ST model that jointly learns translation\nand simultaneous policy in a unified framework of multi-task learning. Adhering\nto a multi-task learning approach, StreamSpeech can perform offline and\nsimultaneous speech recognition, speech translation and speech synthesis via an\n""All-in-One"" seamless model. Experiments on CVSS benchmark demonstrate that\nStreamSpeech achieves state-of-the-art performance in both offline S2ST and\nSimul-S2ST tasks. Besides, StreamSpeech is able to present high-quality\nintermediate results (i.e., ASR or translation results) during simultaneous\ntranslation process, offering a more comprehensive real-time communication\nexperience.\n']",Speech-to-Speech Translation,Speech and Language Processing,Speech and Audio Processing,Speech and Audio Processing
278,29,278_retrieval_multimodal_embeddings_embedding,"['retrieval', 'multimodal', 'embeddings', 'embedding', 'modality', 'modal', 'discriminative', 'search', 'images', 'text']","['retrieval', 'modal', 'image', 'text', 'cross', 'query', 'images', 'queries', 'multimedia', 'recall']","['  Composed Image Retrieval (CIR) involves searching for target images based on\nan image-text pair query. While current methods treat this as a query-target\nmatching problem, we argue that CIR triplets contain additional associations\nbeyond this primary relation. In our paper, we identify two new relations\nwithin triplets, treating each triplet as a graph node. Firstly, we introduce\nthe concept of text-bridged image alignment, where the query text serves as a\nbridge between the query image and the target image. We propose a hinge-based\ncross-attention mechanism to incorporate this relation into network learning.\nSecondly, we explore complementary text reasoning, considering CIR as a form of\ncross-modal retrieval where two images compose to reason about complementary\ntext. To integrate these perspectives effectively, we design a twin\nattention-based compositor. By combining these complementary associations with\nthe explicit query pair-target image relation, we establish a comprehensive set\nof constraints for CIR. Our framework, CaLa (Complementary Association Learning\nfor Augmenting Composed Image Retrieval), leverages these insights. We evaluate\nCaLa on CIRR and FashionIQ benchmarks with multiple backbones, demonstrating\nits superiority in composed image retrieval.\n', '  Image-text matching aims to find matched cross-modal pairs accurately. While\ncurrent methods often rely on projecting cross-modal features into a common\nembedding space, they frequently suffer from imbalanced feature representations\nacross different modalities, leading to unreliable retrieval results. To\naddress these limitations, we introduce a novel Feature Enhancement Module that\nadaptively aggregates single-modal features for more balanced and robust\nimage-text retrieval. Additionally, we propose a new loss function that\novercomes the shortcomings of original triplet ranking loss, thereby\nsignificantly improving retrieval performance. The proposed model has been\nevaluated on two public datasets and achieves competitive retrieval performance\nwhen compared with several state-of-the-art models. Implementation codes can be\nfound here.\n', '  Learned Sparse Retrieval (LSR) is a group of neural methods designed to\nencode queries and documents into sparse lexical vectors. These vectors can be\nefficiently indexed and retrieved using an inverted index. While LSR has shown\npromise in text retrieval, its potential in multi-modal retrieval remains\nlargely unexplored. Motivated by this, in this work, we explore the application\nof LSR in the multi-modal domain, i.e., we focus on Multi-Modal Learned Sparse\nRetrieval (MLSR). We conduct experiments using several MLSR model\nconfigurations and evaluate the performance on the image suggestion task. We\nfind that solving the task solely based on the image content is challenging.\nEnriching the image content with its caption improves the model performance\nsignificantly, implying the importance of image captions to provide\nfine-grained concepts and context information of images. Our approach presents\na practical and effective solution for training LSR retrieval models in\nmulti-modal settings.\n']",Multimodal Image Retrieval,Multimodal Learning and Applications,Multimodal Learning and Applications,Multimodal Learning and Vision-Language Models
279,29,279_magnetosphere_solar_flare_geomagnetic,"['magnetosphere', 'solar', 'flare', 'geomagnetic', 'flares', 'solarcnn', 'sun', 'observatory', 'thermosphere', 'magnetogram']","['solar', 'flares', 'flare', 'magnetic', 'magnetograms', 'coronal', 'weather', 'geomagnetic', 'regions', 'events']","[""  Image super-resolution has been an important subject in image processing and\nrecognition. Here, we present an attention-aided convolutional neural network\n(CNN) for solar image super-resolution. Our method, named SolarCNN, aims to\nenhance the quality of line-of-sight (LOS) magnetograms of solar active regions\n(ARs) collected by the Michelson Doppler Imager (MDI) on board the Solar and\nHeliospheric Observatory (SOHO). The ground-truth labels used for training\nSolarCNN are the LOS magnetograms collected by the Helioseismic and Magnetic\nImager (HMI) on board the Solar Dynamics Observatory (SDO). Solar ARs consist\nof strong magnetic fields in which magnetic energy can suddenly be released to\nproduce extreme space weather events, such as solar flares, coronal mass\nejections, and solar energetic particles. SOHO/MDI covers Solar Cycle 23, which\nis stronger with more eruptive events than Cycle 24. Enhanced SOHO/MDI\nmagnetograms allow for better understanding and forecasting of violent events\nof space weather. Experimental results show that SolarCNN improves the quality\nof SOHO/MDI magnetograms in terms of the structural similarity index measure\n(SSIM), Pearson's correlation coefficient (PCC), and the peak signal-to-noise\nratio (PSNR).\n"", '  Solar flares, especially C, M, and X class, pose significant risks to\nsatellite operations, communication systems, and power grids. We present a\nnovel approach for predicting extreme solar flares using HMI intensitygrams and\nmagnetograms. By detecting sunspots from intensitygrams and extracting magnetic\nfield patches from magnetograms, we train a Residual Network (ResNet) to\nclassify extreme class flares. Our model demonstrates high accuracy, offering a\nrobust tool for predicting extreme solar flares and improving space weather\nforecasting. Additionally, we show that HMI magnetograms provide more useful\ndata for deep learning compared to other SDO AIA images by better capturing\nfeatures critical for predicting flare magnitudes. This study underscores the\nimportance of identifying magnetic fields in solar flare prediction, marking a\nsignificant advancement in solar activity prediction with practical\nimplications for mitigating space weather impacts.\n', ""  In this dataset we provide a comprehensive collection of magnetograms (images\nquantifying the strength of the magnetic field) from the National Aeronautics\nand Space Administration's (NASA's) Solar Dynamics Observatory (SDO). The\ndataset incorporates data from three sources and provides SDO Helioseismic and\nMagnetic Imager (HMI) magnetograms of solar active regions (regions of large\nmagnetic flux, generally the source of eruptive events) as well as labels of\ncorresponding flaring activity. This dataset will be useful for image analysis\nor solar physics research related to magnetic structure, its evolution over\ntime, and its relation to solar flares. The dataset will be of interest to\nthose researchers investigating automated solar flare prediction methods,\nincluding supervised and unsupervised machine learning (classical and deep),\nbinary and multi-class classification, and regression. This dataset is a\nminimally processed, user configurable dataset of consistently sized images of\nsolar active regions that can serve as a benchmark dataset for solar flare\nprediction research.\n""]",Solar Flare Prediction using Magnetograms,Solar Flare Prediction using Magnetograms,Solar Activity Prediction and Analysis,Solar Activity Prediction and Analysis
280,29,280_cloud_virtualization_scheduling_workloads,"['cloud', 'virtualization', 'scheduling', 'workloads', 'workload', 'microservices', 'microservice', 'supercomputing', 'service', 'aws']","['cloud', 'service', 'scheduling', 'computing', 'job', 'microservice', 'microservices', 'resource', 'migration', 'resources']","['  The paragraph is grammatically correct and logically coherent. It discusses\nthe importance of mobile terminal cloud computing migration technology in\nmeeting the demands of evolving computer and cloud computing technologies. It\nemphasizes the need for efficient data access and storage, as well as the\nutilization of cloud computing migration technology to prevent additional time\ndelays. The paragraph also highlights the contributions of cloud computing\nmigration technology to expanding cloud computing services. Additionally, it\nacknowledges the role of virtualization as a fundamental capability of cloud\ncomputing while emphasizing that cloud computing and virtualization are not\ninherently interconnected. Finally, it introduces machine learning-based\nvirtual machine migration optimization and dynamic resource allocation as a\ncritical research direction in cloud computing, citing the limitations of\nstatic rules or manual settings in traditional cloud computing environments.\nOverall, the paragraph effectively communicates the importance of machine\nlearning technology in addressing resource allocation and virtual machine\nmigration challenges in cloud computing.\n', '  With the continuous expansion of the scale of cloud computing applications,\nartificial intelligence technologies such as Deep Learning and Reinforcement\nLearning have gradually become the key tools to solve the automated task\nscheduling of large-scale cloud computing systems. Aiming at the complexity and\nreal-time requirement of task scheduling in large-scale cloud computing system,\nthis paper proposes an automatic task scheduling scheme based on deep learning\nand reinforcement learning. Firstly, the deep learning technology is used to\nmonitor and predict the parameters in the cloud computing system in real time\nto obtain the system status information. Then, combined with reinforcement\nlearning algorithm, the task scheduling strategy is dynamically adjusted\naccording to the real-time system state and task characteristics to achieve the\noptimal utilization of system resources and the maximum of task execution\nefficiency. This paper verifies the effectiveness and performance advantages of\nthe proposed scheme in experiments, and proves the potential and application\nprospect of deep learning and reinforcement learning in automatic task\nscheduling in large-scale cloud computing systems.\n', '  In recent years, cloud computing has been widely used. Cloud computing refers\nto the centralized computing resources, users through the access to the\ncentralized resources to complete the calculation, the cloud computing center\nwill return the results of the program processing to the user. Cloud computing\nis not only for individual users, but also for enterprise users. By purchasing\na cloud server, users do not have to buy a large number of computers, saving\ncomputing costs. According to a report by China Economic News Network, the\nscale of cloud computing in China has reached 209.1 billion yuan. At present,\nthe more mature cloud service providers in China are Ali Cloud, Baidu Cloud,\nHuawei Cloud and so on. Therefore, this paper proposes an innovative approach\nto solve complex problems in cloud computing resource scheduling and management\nusing machine learning optimization techniques. Through in-depth study of\nchallenges such as low resource utilization and unbalanced load in the cloud\nenvironment, this study proposes a comprehensive solution, including\noptimization methods such as deep learning and genetic algorithm, to improve\nsystem performance and efficiency, and thus bring new breakthroughs and\nprogress in the field of cloud computing resource management.Rational\nallocation of resources plays a crucial role in cloud computing. In the\nresource allocation of cloud computing, the cloud computing center has limited\ncloud resources, and users arrive in sequence. Each user requests the cloud\ncomputing center to use a certain number of cloud resources at a specific time.\n']",Cloud Computing Resource Management and Optimization,Optimization and Management of Computing Resources and Information Systems,Optimization and Management of Complex Systems,Optimization and Decision Making in Complex Systems
281,29,281_optimizing_optimization_hyperparameters_deepsdp,"['optimizing', 'optimization', 'hyperparameters', 'deepsdp', 'multitask', 'learning', 'hyperparameter', 'minimization', 'regularization', 'algorithms']","['hyperparameter', 'hyperparameters', 'multi', 'task', 'optimization', 'objective', 'hyper', 'gradient', 'outlier', 'resampling']","['  When training deep learning models, the performance depends largely on the\nselected hyperparameters. However, hyperparameter optimization (HPO) is often\none of the most expensive parts of model design. Classical HPO methods treat\nthis as a black-box optimization problem. However, gray-box HPO methods, which\nincorporate more information about the setup, have emerged as a promising\ndirection for more efficient optimization. For example, using intermediate loss\nevaluations to terminate bad selections. In this work, we propose an HPO method\nfor neural networks using logged checkpoints of the trained weights to guide\nfuture hyperparameter selections. Our method, Forecasting Model Search (FMS),\nembeds weights into a Gaussian process deep kernel surrogate model, using a\npermutation-invariant graph metanetwork to be data-efficient with the logged\nnetwork weights. To facilitate reproducibility and further research, we\nopen-source our code at https://github.com/NVlabs/forecasting-model-search.\n', '  Neural networks are central to many emerging technologies, but verifying\ntheir correctness remains a major challenge. It is known that network outputs\ncan be sensitive and fragile to even small input perturbations, thereby\nincreasing the risk of unpredictable and undesirable behavior. Fast and\naccurate verification of neural networks is therefore critical to their\nwidespread adoption, and in recent years, various methods have been developed\nas a response to this problem. In this paper, we focus on improving\nsemidefinite programming (SDP) based techniques for neural network\nverification. Such techniques offer the power of expressing complex geometric\nconstraints while retaining a convex problem formulation, but scalability\nremains a major issue in practice. Our starting point is the DeepSDP framework\nproposed by Fazlyab et al., which uses quadratic constraints to abstract the\nverification problem into a large-scale SDP. However, solving this SDP quickly\nbecomes intractable when the network grows. Our key observation is that by\nleveraging chordal sparsity, we can decompose the primary computational\nbottleneck of DeepSDP -- a large linear matrix inequality (LMI) -- into an\nequivalent collection of smaller LMIs. We call our chordally sparse\noptimization program Chordal-DeepSDP and prove that its construction is\nidentically expressive as that of DeepSDP. Moreover, we show that additional\nanalysis of Chordal-DeepSDP allows us to further rewrite its collection of LMIs\nin a second level of decomposition that we call Chordal-DeepSDP-2 -- which\nresults in another significant computational gain. Finally, we provide\nnumerical experiments on real networks of learned cart-pole dynamics,\nshowcasing the computational advantage of Chordal-DeepSDP and Chordal-DeepSDP-2\nover DeepSDP.\n', '  Hyperparameter optimization (HPO) is an important step in machine learning\n(ML) model development, but common practices are archaic -- primarily relying\non manual or grid searches. This is partly because adopting advanced HPO\nalgorithms introduces added complexity to the workflow, leading to longer\ncomputation times. This poses a notable challenge to ML applications, as\nsuboptimal hyperparameter selections curtail the potential of ML model\nperformance, ultimately obstructing the full exploitation of ML techniques. In\nthis article, we present a two-step HPO method as a strategic solution to\ncurbing computational demands and wait times, gleaned from practical\nexperiences in applied ML parameterization work. The initial phase involves a\npreliminary evaluation of hyperparameters on a small subset of the training\ndataset, followed by a re-evaluation of the top-performing candidate models\npost-retraining with the entire training dataset. This two-step HPO method is\nuniversally applicable across HPO search algorithms, and we argue it has\nattractive efficiency gains.\n  As a case study, we present our recent application of the two-step HPO method\nto the development of neural network emulators for aerosol activation. Although\nour primary use case is a data-rich limit with many millions of samples, we\nalso find that using up to 0.0025% of the data (a few thousand samples) in the\ninitial step is sufficient to find optimal hyperparameter configurations from\nmuch more extensive sampling, achieving up to 135-times speedup. The benefits\nof this method materialize through an assessment of hyperparameters and model\nperformance, revealing the minimal model complexity required to achieve the\nbest performance. The assortment of top-performing models harvested from the\nHPO process allows us to choose a high-performing model with a low inference\ncost for efficient use in global climate models (GCMs).\n']",Hyperparameter Optimization for Deep Learning,Deep Learning Optimization Techniques,Deep Learning Optimization and Training,Deep Learning Optimization and Security
282,29,282_alignment_annotation_aligned_aligner,"['alignment', 'annotation', 'aligned', 'aligner', 'language', 'aligning', 'models', 'automated', 'position', 'aligncot']","['alignment', 'aligner', 'responses', 'human', 'self', 'constitutions', 'undesirable', 'base', 'specialization', 'feedback']","['  Alignment is the most critical step in building large language models (LLMs)\nthat meet human needs. With the rapid development of LLMs gradually surpassing\nhuman capabilities, traditional alignment methods based on human-annotation are\nincreasingly unable to meet the scalability demands. Therefore, there is an\nurgent need to explore new sources of automated alignment signals and technical\napproaches. In this paper, we systematically review the recently emerging\nmethods of automated alignment, attempting to explore how to achieve effective,\nscalable, automated alignment once the capabilities of LLMs exceed those of\nhumans. Specifically, we categorize existing automated alignment methods into 4\nmajor categories based on the sources of alignment signals and discuss the\ncurrent status and potential development of each category. Additionally, we\nexplore the underlying mechanisms that enable automated alignment and discuss\nthe essential factors that make automated alignment technologies feasible and\neffective from the fundamental role of alignment.\n', ""  The alignment process changes several properties of a large language model's\n(LLM's) output distribution. We analyze two aspects of post-alignment\ndistributional shift of LLM responses. First, we re-examine previously reported\nreductions in response diversity post-alignment. Our analysis suggests that an\napparent drop in the diversity of responses is largely explained by quality\ncontrol and information aggregation. Alignment suppresses irrelevant and\nunhelpful content while shifting the output distribution toward longer\nresponses that cover information spanning several responses from the base LLM,\nessentially presenting diverse information in a single response. Finding little\nevidence that alignment suppresses useful information, it is natural to ask the\nopposite question: do aligned models surface information that cannot be\nrecovered from base models? Our second investigation shows this is not the case\nand the behavior of aligned models is recoverable from base models without\nfine-tuning. A combination of in-context examples and lower-resolution semantic\nhints about response content can elicit responses from base LLMs that are as\nsimilar to alignment-tuned LLM responses as alignment-tuned LLM responses are\nto each other. Taken together, these results indicate that current alignment\ntechniques capture but do not extend the useful subset of assistant-like base\nLLM behavior, providing further evidence for the Superficial Alignment\nHypothesis. They also show that in-context alignment can go surprisingly far as\na strategy for imitating aligned LLMs without fine-tuning. Our code and data is\navailable at https://github.com/thomlake/investigating-alignment.\n"", '  Ensuring alignment with human preferences is a crucial characteristic of\nlarge language models (LLMs). Presently, the primary alignment methods, RLHF\nand DPO, require extensive human annotation, which is expensive despite their\nefficacy. The significant expenses associated with current alignment techniques\nmotivate researchers to investigate the development of annotation-free\nalignment training methods. In pursuit of improved alignment without relying on\nexternal annotation, we introduce Latent Distance Guided Alignment Training\n(LD-Align). This approach seeks to align the model with a high-quality\nsupervised fine-tune dataset using guidance from a latent space. The latent\nspace is generated through sample reconstruction, akin to auto-encoding.\nConsequently, we utilize the distance between sample pairs in the latent space\nto guide DPO-based alignment training. Extensive experimentation and evaluation\nshow the efficacy of our proposed method in achieving notable alignment.\n']",Automated Alignment in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
282,29,282_alignment_annotation_aligned_aligner,"['alignment', 'annotation', 'aligned', 'aligner', 'language', 'aligning', 'models', 'automated', 'position', 'aligncot']","['alignment', 'aligner', 'responses', 'human', 'self', 'constitutions', 'undesirable', 'base', 'specialization', 'feedback']","['  Alignment is the most critical step in building large language models (LLMs)\nthat meet human needs. With the rapid development of LLMs gradually surpassing\nhuman capabilities, traditional alignment methods based on human-annotation are\nincreasingly unable to meet the scalability demands. Therefore, there is an\nurgent need to explore new sources of automated alignment signals and technical\napproaches. In this paper, we systematically review the recently emerging\nmethods of automated alignment, attempting to explore how to achieve effective,\nscalable, automated alignment once the capabilities of LLMs exceed those of\nhumans. Specifically, we categorize existing automated alignment methods into 4\nmajor categories based on the sources of alignment signals and discuss the\ncurrent status and potential development of each category. Additionally, we\nexplore the underlying mechanisms that enable automated alignment and discuss\nthe essential factors that make automated alignment technologies feasible and\neffective from the fundamental role of alignment.\n', ""  The alignment process changes several properties of a large language model's\n(LLM's) output distribution. We analyze two aspects of post-alignment\ndistributional shift of LLM responses. First, we re-examine previously reported\nreductions in response diversity post-alignment. Our analysis suggests that an\napparent drop in the diversity of responses is largely explained by quality\ncontrol and information aggregation. Alignment suppresses irrelevant and\nunhelpful content while shifting the output distribution toward longer\nresponses that cover information spanning several responses from the base LLM,\nessentially presenting diverse information in a single response. Finding little\nevidence that alignment suppresses useful information, it is natural to ask the\nopposite question: do aligned models surface information that cannot be\nrecovered from base models? Our second investigation shows this is not the case\nand the behavior of aligned models is recoverable from base models without\nfine-tuning. A combination of in-context examples and lower-resolution semantic\nhints about response content can elicit responses from base LLMs that are as\nsimilar to alignment-tuned LLM responses as alignment-tuned LLM responses are\nto each other. Taken together, these results indicate that current alignment\ntechniques capture but do not extend the useful subset of assistant-like base\nLLM behavior, providing further evidence for the Superficial Alignment\nHypothesis. They also show that in-context alignment can go surprisingly far as\na strategy for imitating aligned LLMs without fine-tuning. Our code and data is\navailable at https://github.com/thomlake/investigating-alignment.\n"", '  Ensuring alignment with human preferences is a crucial characteristic of\nlarge language models (LLMs). Presently, the primary alignment methods, RLHF\nand DPO, require extensive human annotation, which is expensive despite their\nefficacy. The significant expenses associated with current alignment techniques\nmotivate researchers to investigate the development of annotation-free\nalignment training methods. In pursuit of improved alignment without relying on\nexternal annotation, we introduce Latent Distance Guided Alignment Training\n(LD-Align). This approach seeks to align the model with a high-quality\nsupervised fine-tune dataset using guidance from a latent space. The latent\nspace is generated through sample reconstruction, akin to auto-encoding.\nConsequently, we utilize the distance between sample pairs in the latent space\nto guide DPO-based alignment training. Extensive experimentation and evaluation\nshow the efficacy of our proposed method in achieving notable alignment.\n']",Automated Alignment in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
282,29,282_alignment_annotation_aligned_aligner,"['alignment', 'annotation', 'aligned', 'aligner', 'language', 'aligning', 'models', 'automated', 'position', 'aligncot']","['alignment', 'aligner', 'responses', 'human', 'self', 'constitutions', 'undesirable', 'base', 'specialization', 'feedback']","['  Alignment is the most critical step in building large language models (LLMs)\nthat meet human needs. With the rapid development of LLMs gradually surpassing\nhuman capabilities, traditional alignment methods based on human-annotation are\nincreasingly unable to meet the scalability demands. Therefore, there is an\nurgent need to explore new sources of automated alignment signals and technical\napproaches. In this paper, we systematically review the recently emerging\nmethods of automated alignment, attempting to explore how to achieve effective,\nscalable, automated alignment once the capabilities of LLMs exceed those of\nhumans. Specifically, we categorize existing automated alignment methods into 4\nmajor categories based on the sources of alignment signals and discuss the\ncurrent status and potential development of each category. Additionally, we\nexplore the underlying mechanisms that enable automated alignment and discuss\nthe essential factors that make automated alignment technologies feasible and\neffective from the fundamental role of alignment.\n', ""  The alignment process changes several properties of a large language model's\n(LLM's) output distribution. We analyze two aspects of post-alignment\ndistributional shift of LLM responses. First, we re-examine previously reported\nreductions in response diversity post-alignment. Our analysis suggests that an\napparent drop in the diversity of responses is largely explained by quality\ncontrol and information aggregation. Alignment suppresses irrelevant and\nunhelpful content while shifting the output distribution toward longer\nresponses that cover information spanning several responses from the base LLM,\nessentially presenting diverse information in a single response. Finding little\nevidence that alignment suppresses useful information, it is natural to ask the\nopposite question: do aligned models surface information that cannot be\nrecovered from base models? Our second investigation shows this is not the case\nand the behavior of aligned models is recoverable from base models without\nfine-tuning. A combination of in-context examples and lower-resolution semantic\nhints about response content can elicit responses from base LLMs that are as\nsimilar to alignment-tuned LLM responses as alignment-tuned LLM responses are\nto each other. Taken together, these results indicate that current alignment\ntechniques capture but do not extend the useful subset of assistant-like base\nLLM behavior, providing further evidence for the Superficial Alignment\nHypothesis. They also show that in-context alignment can go surprisingly far as\na strategy for imitating aligned LLMs without fine-tuning. Our code and data is\navailable at https://github.com/thomlake/investigating-alignment.\n"", '  Ensuring alignment with human preferences is a crucial characteristic of\nlarge language models (LLMs). Presently, the primary alignment methods, RLHF\nand DPO, require extensive human annotation, which is expensive despite their\nefficacy. The significant expenses associated with current alignment techniques\nmotivate researchers to investigate the development of annotation-free\nalignment training methods. In pursuit of improved alignment without relying on\nexternal annotation, we introduce Latent Distance Guided Alignment Training\n(LD-Align). This approach seeks to align the model with a high-quality\nsupervised fine-tune dataset using guidance from a latent space. The latent\nspace is generated through sample reconstruction, akin to auto-encoding.\nConsequently, we utilize the distance between sample pairs in the latent space\nto guide DPO-based alignment training. Extensive experimentation and evaluation\nshow the efficacy of our proposed method in achieving notable alignment.\n']",Automated Alignment in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
282,29,282_alignment_annotation_aligned_aligner,"['alignment', 'annotation', 'aligned', 'aligner', 'language', 'aligning', 'models', 'automated', 'position', 'aligncot']","['alignment', 'aligner', 'responses', 'human', 'self', 'constitutions', 'undesirable', 'base', 'specialization', 'feedback']","['  Alignment is the most critical step in building large language models (LLMs)\nthat meet human needs. With the rapid development of LLMs gradually surpassing\nhuman capabilities, traditional alignment methods based on human-annotation are\nincreasingly unable to meet the scalability demands. Therefore, there is an\nurgent need to explore new sources of automated alignment signals and technical\napproaches. In this paper, we systematically review the recently emerging\nmethods of automated alignment, attempting to explore how to achieve effective,\nscalable, automated alignment once the capabilities of LLMs exceed those of\nhumans. Specifically, we categorize existing automated alignment methods into 4\nmajor categories based on the sources of alignment signals and discuss the\ncurrent status and potential development of each category. Additionally, we\nexplore the underlying mechanisms that enable automated alignment and discuss\nthe essential factors that make automated alignment technologies feasible and\neffective from the fundamental role of alignment.\n', ""  The alignment process changes several properties of a large language model's\n(LLM's) output distribution. We analyze two aspects of post-alignment\ndistributional shift of LLM responses. First, we re-examine previously reported\nreductions in response diversity post-alignment. Our analysis suggests that an\napparent drop in the diversity of responses is largely explained by quality\ncontrol and information aggregation. Alignment suppresses irrelevant and\nunhelpful content while shifting the output distribution toward longer\nresponses that cover information spanning several responses from the base LLM,\nessentially presenting diverse information in a single response. Finding little\nevidence that alignment suppresses useful information, it is natural to ask the\nopposite question: do aligned models surface information that cannot be\nrecovered from base models? Our second investigation shows this is not the case\nand the behavior of aligned models is recoverable from base models without\nfine-tuning. A combination of in-context examples and lower-resolution semantic\nhints about response content can elicit responses from base LLMs that are as\nsimilar to alignment-tuned LLM responses as alignment-tuned LLM responses are\nto each other. Taken together, these results indicate that current alignment\ntechniques capture but do not extend the useful subset of assistant-like base\nLLM behavior, providing further evidence for the Superficial Alignment\nHypothesis. They also show that in-context alignment can go surprisingly far as\na strategy for imitating aligned LLMs without fine-tuning. Our code and data is\navailable at https://github.com/thomlake/investigating-alignment.\n"", '  Ensuring alignment with human preferences is a crucial characteristic of\nlarge language models (LLMs). Presently, the primary alignment methods, RLHF\nand DPO, require extensive human annotation, which is expensive despite their\nefficacy. The significant expenses associated with current alignment techniques\nmotivate researchers to investigate the development of annotation-free\nalignment training methods. In pursuit of improved alignment without relying on\nexternal annotation, we introduce Latent Distance Guided Alignment Training\n(LD-Align). This approach seeks to align the model with a high-quality\nsupervised fine-tune dataset using guidance from a latent space. The latent\nspace is generated through sample reconstruction, akin to auto-encoding.\nConsequently, we utilize the distance between sample pairs in the latent space\nto guide DPO-based alignment training. Extensive experimentation and evaluation\nshow the efficacy of our proposed method in achieving notable alignment.\n']",Automated Alignment in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
282,29,282_alignment_annotation_aligned_aligner,"['alignment', 'annotation', 'aligned', 'aligner', 'language', 'aligning', 'models', 'automated', 'position', 'aligncot']","['alignment', 'aligner', 'responses', 'human', 'self', 'constitutions', 'undesirable', 'base', 'specialization', 'feedback']","['  Alignment is the most critical step in building large language models (LLMs)\nthat meet human needs. With the rapid development of LLMs gradually surpassing\nhuman capabilities, traditional alignment methods based on human-annotation are\nincreasingly unable to meet the scalability demands. Therefore, there is an\nurgent need to explore new sources of automated alignment signals and technical\napproaches. In this paper, we systematically review the recently emerging\nmethods of automated alignment, attempting to explore how to achieve effective,\nscalable, automated alignment once the capabilities of LLMs exceed those of\nhumans. Specifically, we categorize existing automated alignment methods into 4\nmajor categories based on the sources of alignment signals and discuss the\ncurrent status and potential development of each category. Additionally, we\nexplore the underlying mechanisms that enable automated alignment and discuss\nthe essential factors that make automated alignment technologies feasible and\neffective from the fundamental role of alignment.\n', ""  The alignment process changes several properties of a large language model's\n(LLM's) output distribution. We analyze two aspects of post-alignment\ndistributional shift of LLM responses. First, we re-examine previously reported\nreductions in response diversity post-alignment. Our analysis suggests that an\napparent drop in the diversity of responses is largely explained by quality\ncontrol and information aggregation. Alignment suppresses irrelevant and\nunhelpful content while shifting the output distribution toward longer\nresponses that cover information spanning several responses from the base LLM,\nessentially presenting diverse information in a single response. Finding little\nevidence that alignment suppresses useful information, it is natural to ask the\nopposite question: do aligned models surface information that cannot be\nrecovered from base models? Our second investigation shows this is not the case\nand the behavior of aligned models is recoverable from base models without\nfine-tuning. A combination of in-context examples and lower-resolution semantic\nhints about response content can elicit responses from base LLMs that are as\nsimilar to alignment-tuned LLM responses as alignment-tuned LLM responses are\nto each other. Taken together, these results indicate that current alignment\ntechniques capture but do not extend the useful subset of assistant-like base\nLLM behavior, providing further evidence for the Superficial Alignment\nHypothesis. They also show that in-context alignment can go surprisingly far as\na strategy for imitating aligned LLMs without fine-tuning. Our code and data is\navailable at https://github.com/thomlake/investigating-alignment.\n"", '  Ensuring alignment with human preferences is a crucial characteristic of\nlarge language models (LLMs). Presently, the primary alignment methods, RLHF\nand DPO, require extensive human annotation, which is expensive despite their\nefficacy. The significant expenses associated with current alignment techniques\nmotivate researchers to investigate the development of annotation-free\nalignment training methods. In pursuit of improved alignment without relying on\nexternal annotation, we introduce Latent Distance Guided Alignment Training\n(LD-Align). This approach seeks to align the model with a high-quality\nsupervised fine-tune dataset using guidance from a latent space. The latent\nspace is generated through sample reconstruction, akin to auto-encoding.\nConsequently, we utilize the distance between sample pairs in the latent space\nto guide DPO-based alignment training. Extensive experimentation and evaluation\nshow the efficacy of our proposed method in achieving notable alignment.\n']",Automated Alignment in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
282,29,282_alignment_annotation_aligned_aligner,"['alignment', 'annotation', 'aligned', 'aligner', 'language', 'aligning', 'models', 'automated', 'position', 'aligncot']","['alignment', 'aligner', 'responses', 'human', 'self', 'constitutions', 'undesirable', 'base', 'specialization', 'feedback']","['  Alignment is the most critical step in building large language models (LLMs)\nthat meet human needs. With the rapid development of LLMs gradually surpassing\nhuman capabilities, traditional alignment methods based on human-annotation are\nincreasingly unable to meet the scalability demands. Therefore, there is an\nurgent need to explore new sources of automated alignment signals and technical\napproaches. In this paper, we systematically review the recently emerging\nmethods of automated alignment, attempting to explore how to achieve effective,\nscalable, automated alignment once the capabilities of LLMs exceed those of\nhumans. Specifically, we categorize existing automated alignment methods into 4\nmajor categories based on the sources of alignment signals and discuss the\ncurrent status and potential development of each category. Additionally, we\nexplore the underlying mechanisms that enable automated alignment and discuss\nthe essential factors that make automated alignment technologies feasible and\neffective from the fundamental role of alignment.\n', ""  The alignment process changes several properties of a large language model's\n(LLM's) output distribution. We analyze two aspects of post-alignment\ndistributional shift of LLM responses. First, we re-examine previously reported\nreductions in response diversity post-alignment. Our analysis suggests that an\napparent drop in the diversity of responses is largely explained by quality\ncontrol and information aggregation. Alignment suppresses irrelevant and\nunhelpful content while shifting the output distribution toward longer\nresponses that cover information spanning several responses from the base LLM,\nessentially presenting diverse information in a single response. Finding little\nevidence that alignment suppresses useful information, it is natural to ask the\nopposite question: do aligned models surface information that cannot be\nrecovered from base models? Our second investigation shows this is not the case\nand the behavior of aligned models is recoverable from base models without\nfine-tuning. A combination of in-context examples and lower-resolution semantic\nhints about response content can elicit responses from base LLMs that are as\nsimilar to alignment-tuned LLM responses as alignment-tuned LLM responses are\nto each other. Taken together, these results indicate that current alignment\ntechniques capture but do not extend the useful subset of assistant-like base\nLLM behavior, providing further evidence for the Superficial Alignment\nHypothesis. They also show that in-context alignment can go surprisingly far as\na strategy for imitating aligned LLMs without fine-tuning. Our code and data is\navailable at https://github.com/thomlake/investigating-alignment.\n"", '  Ensuring alignment with human preferences is a crucial characteristic of\nlarge language models (LLMs). Presently, the primary alignment methods, RLHF\nand DPO, require extensive human annotation, which is expensive despite their\nefficacy. The significant expenses associated with current alignment techniques\nmotivate researchers to investigate the development of annotation-free\nalignment training methods. In pursuit of improved alignment without relying on\nexternal annotation, we introduce Latent Distance Guided Alignment Training\n(LD-Align). This approach seeks to align the model with a high-quality\nsupervised fine-tune dataset using guidance from a latent space. The latent\nspace is generated through sample reconstruction, akin to auto-encoding.\nConsequently, we utilize the distance between sample pairs in the latent space\nto guide DPO-based alignment training. Extensive experimentation and evaluation\nshow the efficacy of our proposed method in achieving notable alignment.\n']",Automated Alignment in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
283,29,283_paragraphs_text_autoregressively_autoregressive,"['paragraphs', 'text', 'autoregressively', 'autoregressive', 'decoding', 'writing', 'planner', 'generation', 'language', 'predict']","['autoregressive', 'diffusion', 'discrete', 'text', 'generation', 'sampling', 'nucleus', 'quality', 'fluent', 'token']","['  Autoregressive models for text sometimes generate repetitive and low-quality\noutput because errors accumulate during the steps of generation. This issue is\noften attributed to exposure bias - the difference between how a model is\ntrained, and how it is used during inference. Denoising diffusion models\nprovide an alternative approach in which a model can revisit and revise its\noutput. However, they can be computationally expensive and prior efforts on\ntext have led to models that produce less fluent output compared to\nautoregressive models, especially for longer text and paragraphs. In this\npaper, we propose PLANNER, a model that combines latent semantic diffusion with\nautoregressive generation, to generate fluent text while exercising global\ncontrol over paragraphs. The model achieves this by combining an autoregressive\n""decoding"" module with a ""planning"" module that uses latent diffusion to\ngenerate semantic paragraph embeddings in a coarse-to-fine manner. The proposed\nmethod is evaluated on various conditional generation tasks, and results on\nsemantic generation, text completion and summarization show its effectiveness\nin generating high-quality long-form text in an efficient manner.\n', '  The modern autoregressive Large Language Models (LLMs) have achieved\noutstanding performance on NLP benchmarks, and they are deployed in the real\nworld. However, they still suffer from limitations of the autoregressive\ntraining paradigm. For example, autoregressive token generation is notably slow\nand can be prone to \\textit{exposure bias}. The diffusion-based language models\nwere proposed as an alternative to autoregressive generation to address some of\nthese limitations. We evaluate the recently proposed Score Entropy Discrete\nDiffusion (SEDD) approach and show it is a promising alternative to\nautoregressive generation but it has some short-comings too. We empirically\ndemonstrate the advantages and challenges of SEDD, and observe that SEDD\ngenerally matches autoregressive models in perplexity and on benchmarks such as\nHellaSwag, Arc or WinoGrande. Additionally, we show that in terms of inference\nlatency, SEDD can be up to 4.5$\\times$ more efficient than GPT-2. While SEDD\nallows conditioning on tokens at abitrary positions, SEDD appears slightly\nweaker than GPT-2 for conditional generation given short prompts. Finally, we\nreproduced the main results from the original SEDD paper.\n', '  Despite their groundbreaking performance for many generative modeling tasks,\ndiffusion models have fallen short on discrete data domains such as natural\nlanguage. Crucially, standard diffusion models rely on the well-established\ntheory of score matching, but efforts to generalize this to discrete structures\nhave not yielded the same empirical gains. In this work, we bridge this gap by\nproposing score entropy, a novel loss that naturally extends score matching to\ndiscrete spaces, integrates seamlessly to build discrete diffusion models, and\nsignificantly boosts performance. Experimentally, we test our Score Entropy\nDiscrete Diffusion models (SEDD) on standard language modeling tasks. For\ncomparable model sizes, SEDD beats existing language diffusion paradigms\n(reducing perplexity by $25$-$75$\\%) and is competitive with autoregressive\nmodels, in particular outperforming GPT-2. Furthermore, compared to\nautoregressive mdoels, SEDD generates faithful text without requiring\ndistribution annealing techniques like temperature scaling (around\n$6$-$8\\times$ better generative perplexity than un-annealed GPT-2), can trade\ncompute and quality (similar quality with $32\\times$ fewer network\nevaluations), and enables controllable infilling (matching nucleus sampling\nquality while enabling other strategies besides left to right prompting).\n']",Autoregressive Text Generation with Diffusion Models,"Diffusion Models for Text, Image, and Graph Generation",Diffusion Models for Generative Tasks,Diffusion Models for Generative Tasks
284,29,284_fraud_banking_classification_bank,"['fraud', 'banking', 'classification', 'bank', 'banks', 'credit', 'fraudulent', 'lending', 'card', 'lenders']","['credit', 'fraud', 'financial', 'card', 'banks', 'fraudulent', 'transactions', 'risk', 'transaction', 'default']","['  Financial institutions and businesses face an ongoing challenge from\nfraudulent transactions, prompting the need for effective detection methods.\nDetecting credit card fraud is crucial for identifying and preventing\nunauthorized transactions.Timely detection of fraud enables investigators to\ntake swift actions to mitigate further losses. However, the investigation\nprocess is often time-consuming, limiting the number of alerts that can be\nthoroughly examined each day. Therefore, the primary objective of a fraud\ndetection model is to provide accurate alerts while minimizing false alarms and\nmissed fraud cases. In this paper, we introduce a state-of-the-art hybrid\nensemble (ENS) dependable Machine learning (ML) model that intelligently\ncombines multiple algorithms with proper weighted optimization using Grid\nsearch, including Decision Tree (DT), Random Forest (RF), K-Nearest Neighbor\n(KNN), and Multilayer Perceptron (MLP), to enhance fraud identification. To\naddress the data imbalance issue, we employ the Instant Hardness Threshold\n(IHT) technique in conjunction with Logistic Regression (LR), surpassing\nconventional approaches. Our experiments are conducted on a publicly available\ncredit card dataset comprising 284,807 transactions. The proposed model\nachieves impressive accuracy rates of 99.66%, 99.73%, 98.56%, and 99.79%, and a\nperfect 100% for the DT, RF, KNN, MLP and ENS models, respectively. The hybrid\nensemble model outperforms existing works, establishing a new benchmark for\ndetecting fraudulent transactions in high-frequency scenarios. The results\nhighlight the effectiveness and reliability of our approach, demonstrating\nsuperior performance metrics and showcasing its exceptional potential for\nreal-world fraud detection applications.\n', '  Credit card fraud is a major cause of national concern in the Nigerian\nfinancial sector, affecting hundreds of transactions per second and impacting\ninternational ecommerce negatively. Despite the rapid spread and adoption of\nonline marketing, millions of Nigerians are prevented from transacting in\nseveral countries with local credit cards due to bans and policies directed at\nrestricting credit card fraud. Presently, a myriad of technologies exist to\ndetect fraudulent transactions, a few of which are adopted by Nigerian\nfinancial institutions to proactively manage the situation. Fraud detection\nallows institutions to restrict offenders from networks and with a centralized\nbanking identity management system, such as the Bank Verification Number used\nby the Central Bank of Nigeria, offenders who may have stolen other identities\ncan be backtraced and their bank accounts frozen. This paper aims to compare\nthe effectiveness of two fraud detection technologies that are projected to\nwork fully independent of human intervention to possibly predict and detect\nfraudulent credit card transactions. Autoencoders as an unsupervised tensorflow\nbased anomaly detection technique generally offers greater performance in\ndimensionality reduction than the Principal Component Analysis, and this theory\nwas tested out on Nigerian credit card transaction data. Results demonstrate\nthat autoencoders are better suited to analyzing complex and extensive datasets\nand offer more reliable results with minimal mislabeling than the PCA\nalgorithm.\n', '  Credit card fraud detection is a critical challenge in the financial sector,\ndemanding sophisticated approaches to accurately identify fraudulent\ntransactions. This research proposes an innovative methodology combining Neural\nNetworks (NN) and Synthet ic Minority Over-sampling Technique (SMOTE) to\nenhance the detection performance. The study addresses the inherent imbalance\nin credit card transaction data, focusing on technical advancements for robust\nand precise fraud detection. Results demonstrat e that the integration of NN\nand SMOTE exhibits superior precision, recall, and F1-score compared to\ntraditional models, highlighting its potential as an advanced solution for\nhandling imbalanced datasets in credit card fraud detection scenarios. This\nrese arch contributes to the ongoing efforts to develop effective and efficient\nmechanisms for safeguarding financial transactions from fraudulent activities.\n']",Credit Card Fraud Detection,Machine Learning for Imbalanced Classification and Fraud Detection,Machine Learning and Blockchain for Security and Fraud Detection,Machine Learning and Blockchain for Security and Fraud Detection
285,29,285_forgetting_memorization_continual_learning,"['forgetting', 'memorization', 'continual', 'learning', 'memory', 'retrieval', 'retaining', 'lifelong', 'continually', 'incrementally']","['continual', 'forgetting', 'catastrophic', 'replay', 'lifelong', 'incremental', 'knowledge', 'task', 'transfer', 'prompt']","['  Continual learning refers to the capability of a machine learning model to\nlearn and adapt to new information, without compromising its performance on\npreviously learned tasks. Although several studies have investigated continual\nlearning methods for information retrieval tasks, a well-defined task\nformulation is still lacking, and it is unclear how typical learning strategies\nperform in this context. To address this challenge, a systematic task\nformulation of continual neural information retrieval is presented, along with\na multiple-topic dataset that simulates continuous information retrieval. A\ncomprehensive continual neural information retrieval framework consisting of\ntypical retrieval models and continual learning strategies is then proposed.\nEmpirical evaluations illustrate that the proposed framework can successfully\nprevent catastrophic forgetting in neural information retrieval and enhance\nperformance on previously learned tasks. The results indicate that\nembedding-based retrieval models experience a decline in their continual\nlearning performance as the topic shift distance and dataset volume of new\ntasks increase. In contrast, pretraining-based models do not show any such\ncorrelation. Adopting suitable learning strategies can mitigate the effects of\ntopic shift and data augmentation.\n', '  This paper studies the evolving domain of Continual Learning (CL) in large\nlanguage models (LLMs), with a focus on developing strategies for efficient and\nsustainable training. Our primary emphasis is on continual domain-adaptive\npretraining, a process designed to equip LLMs with the ability to integrate new\ninformation from various domains while retaining previously learned knowledge\nand enhancing cross-domain knowledge transfer without relying on\ndomain-specific identification. Unlike previous studies, which mostly\nconcentrate on a limited selection of tasks or domains and primarily aim to\naddress the issue of forgetting, our research evaluates the adaptability and\ncapabilities of LLMs to changing data landscapes in practical scenarios. To\nthis end, we introduce a new benchmark designed to measure the adaptability of\nLLMs to these evolving data environments, offering a comprehensive framework\nfor evaluation. We examine the impact of model size on learning efficacy and\nforgetting, as well as how the progression and similarity of emerging domains\naffect the knowledge transfer within these models. Our findings uncover several\nkey insights: (i) when the sequence of domains shows semantic similarity,\ncontinual pretraining enables LLMs to better specialize in the current domain\ncompared to stand-alone fine-tuning, (ii) training across a diverse range of\ndomains enhances both backward and forward knowledge transfer, and (iii)\nsmaller models are particularly sensitive to continual pretraining, showing the\nmost significant rates of both forgetting and learning. We posit that our\nresearch marks a shift towards establishing a more realistic benchmark for\ninvestigating CL in LLMs, and has the potential to play a key role in guiding\nthe direction of future research in the field.\n', '  Recently, foundation language models (LMs) have marked significant\nachievements in the domains of natural language processing (NLP) and computer\nvision (CV). Unlike traditional neural network models, foundation LMs obtain a\ngreat ability for transfer learning by acquiring rich commonsense knowledge\nthrough pre-training on extensive unsupervised datasets with a vast number of\nparameters. However, they still can not emulate human-like continuous learning\ndue to catastrophic forgetting. Consequently, various continual learning\n(CL)-based methodologies have been developed to refine LMs, enabling them to\nadapt to new tasks without forgetting previous knowledge. However, a systematic\ntaxonomy of existing approaches and a comparison of their performance are still\nlacking, which is the gap that our survey aims to fill. We delve into a\ncomprehensive review, summarization, and classification of the existing\nliterature on CL-based approaches applied to foundation language models, such\nas pre-trained language models (PLMs), large language models (LLMs) and\nvision-language models (VLMs). We divide these studies into offline CL and\nonline CL, which consist of traditional methods, parameter-efficient-based\nmethods, instruction tuning-based methods and continual pre-training methods.\nOffline CL encompasses domain-incremental learning, task-incremental learning,\nand class-incremental learning, while online CL is subdivided into hard task\nboundary and blurry task boundary settings. Additionally, we outline the\ntypical datasets and metrics employed in CL research and provide a detailed\nanalysis of the challenges and future work for LMs-based continual learning.\n']",Continual Learning in Neural Information Retrieval,Continual Learning and Catastrophic Forgetting,Machine Learning Adaptation and Forgetting,Machine Learning Adaptation and Forgetting
286,28,286_chatbots_chatbot_conversational_conversation,"['chatbots', 'chatbot', 'conversational', 'conversation', 'chat', 'chatatc', 'ai', 'prompts', 'assistant', 'eliza']","['chatbots', 'conversational', 'chatbot', 'responses', 'chat', 'engineering', 'tools', 'users', 'conversation', 'technical']","[""  Large language models (LLMs) provide a new way to build chatbots by accepting\nnatural language prompts. Yet, it is unclear how to design prompts to power\nchatbots to carry on naturalistic conversations while pursuing a given goal,\nsuch as collecting self-report data from users. We explore what design factors\nof prompts can help steer chatbots to talk naturally and collect data reliably.\nTo this aim, we formulated four prompt designs with different structures and\npersonas. Through an online study (N = 48) where participants conversed with\nchatbots driven by different designs of prompts, we assessed how prompt designs\nand conversation topics affected the conversation flows and users' perceptions\nof chatbots. Our chatbots covered 79% of the desired information slots during\nconversations, and the designs of prompts and topics significantly influenced\nthe conversation flows and the data collection performance. We discuss the\nopportunities and challenges of building chatbots with LLMs.\n"", ""  This research provides an in-depth comprehensive review of the progress of\nchatbot technology over time, from the initial basic systems relying on rules\nto today's advanced conversational bots powered by artificial intelligence.\nSpanning many decades, the paper explores the major milestones, innovations,\nand paradigm shifts that have driven the evolution of chatbots. Looking back at\nthe very basic statistical model in 1906 via the early chatbots, such as ELIZA\nand ALICE in the 1960s and 1970s, the study traces key innovations leading to\ntoday's advanced conversational agents, such as ChatGPT and Google Bard. The\nstudy synthesizes insights from academic literature and industry sources to\nhighlight crucial milestones, including the introduction of Turing tests,\ninfluential projects such as CALO, and recent transformer-based models. Tracing\nthe path forward, the paper highlights how natural language processing and\nmachine learning have been integrated into modern chatbots for more\nsophisticated capabilities. This chronological survey of the chatbot landscape\nprovides a holistic reference to understand the technological and historical\nfactors propelling conversational AI. By synthesizing learnings from this\nhistorical analysis, the research offers important context about the\ndevelopmental trajectory of chatbots and their immense future potential across\nvarious field of application which could be the potential take ways for the\nrespective research community and stakeholders.\n"", ""  The past few decades have witnessed an upsurge in data, forming the\nfoundation for data-hungry, learning-based AI technology. Conversational\nagents, often referred to as AI chatbots, rely heavily on such data to train\nlarge language models (LLMs) and generate new content (knowledge) in response\nto user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have\nset new standards in the AI community. This paper presents a complete survey of\nthe evolution and deployment of LLM-based chatbots in various sectors. We first\nsummarize the development of foundational chatbots, followed by the evolution\nof LLMs, and then provide an overview of LLM-based chatbots currently in use\nand those in the development phase. Recognizing AI chatbots as tools for\ngenerating new knowledge, we explore their diverse applications across various\nindustries. We then discuss the open challenges, considering how the data used\nto train the LLMs and the misuse of the generated knowledge can cause several\nissues. Finally, we explore the future outlook to augment their efficiency and\nreliability in numerous applications. By addressing key milestones and the\npresent-day context of LLM-based chatbots, our survey invites readers to delve\ndeeper into this realm, reflecting on how their next generation will reshape\nconversational AI.\n""]",Chatbots and Conversational AI,Conversational AI and Chatbots,Conversational AI and Human-Computer Interaction,Conversational AI and Human-Computer Interaction
287,28,287_distillation_distilling_datasets_dataset,"['distillation', 'distilling', 'datasets', 'dataset', 'distill', 'distilled', 'imagenet', 'data', 'training', 'samples']","['distillation', 'distilled', 'dataset', 'synthetic', 'original', 'compress', 'labels', 'label', 'matching', 'teacher']","['  Data $\\textit{quality}$ is a crucial factor in the performance of machine\nlearning models, a principle that dataset distillation methods exploit by\ncompressing training datasets into much smaller counterparts that maintain\nsimilar downstream performance. Understanding how and why data distillation\nmethods work is vital not only for improving these methods but also for\nrevealing fundamental characteristics of ""good"" training data. However, a major\nchallenge in achieving this goal is the observation that distillation\napproaches, which rely on sophisticated but mostly disparate methods to\ngenerate synthetic data, have little in common with each other. In this work,\nwe highlight a largely overlooked aspect common to most of these methods: the\nuse of soft (probabilistic) labels. Through a series of ablation experiments,\nwe study the role of soft labels in depth. Our results reveal that the main\nfactor explaining the performance of state-of-the-art distillation methods is\nnot the specific techniques used to generate synthetic data but rather the use\nof soft labels. Furthermore, we demonstrate that not all soft labels are\ncreated equal; they must contain $\\textit{structured information}$ to be\nbeneficial. We also provide empirical scaling laws that characterize the\neffectiveness of soft labels as a function of images-per-class in the distilled\ndataset and establish an empirical Pareto frontier for data-efficient learning.\nCombined, our findings challenge conventional wisdom in dataset distillation,\nunderscore the importance of soft labels in learning, and suggest new\ndirections for improving distillation methods. Code for all experiments is\navailable at https://github.com/sunnytqin/no-distillation.\n', '  Dataset distillation aims at synthesizing a dataset by a small number of\nartificially generated data items, which, when used as training data, reproduce\nor approximate a machine learning (ML) model as if it were trained on the\nentire original dataset. Consequently, data distillation methods are usually\ntied to a specific ML algorithm. While recent literature deals mainly with\ndistillation of large collections of images in the context of neural network\nmodels, tabular data distillation is much less represented and mainly focused\non a theoretical perspective. The current paper explores the potential of a\nsimple distillation technique previously proposed in the context of\nLess-than-one shot learning. The main goal is to push further the performance\nof prototype-based soft-labels distillation in terms of classification\naccuracy, by integrating optimization steps in the distillation process. The\nanalysis is performed on real-world data sets with various degrees of\nimbalance. Experimental studies trace the capability of the method to distill\nthe data, but also the opportunity to act as an augmentation method, i.e. to\ngenerate new data that is able to increase model accuracy when used in\nconjunction with - as opposed to instead of - the original data.\n', '  Data-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.\n']",Dataset Distillation Methods,Model and Data Distillation Techniques,Model and Data Distillation Techniques,Model and Data Distillation Techniques
288,28,288_mandarin_chinese_languages_linguistic,"['mandarin', 'chinese', 'languages', 'linguistic', 'language', 'nlp', 'taiwanese', 'english', 'subjects', 'conversational']","['chat', 'subjects', 'questions', 'capabilities', 'evaluation', 'understanding', 'language', 'open', 'benchmark', 'linguistic']","['  Chinese Large Language Models (LLMs) have recently demonstrated impressive\ncapabilities across various NLP benchmarks and real-world applications.\nHowever, the existing benchmarks for comprehensively evaluating these LLMs are\nstill insufficient, particularly in terms of measuring knowledge that LLMs\ncapture. Current datasets collect questions from Chinese examinations across\ndifferent subjects and educational levels to address this issue. Yet, these\nbenchmarks primarily focus on objective questions such as multiple-choice\nquestions, leading to a lack of diversity in question types. To tackle this\nproblem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge\nEvaluation benchmark in this paper. LHMKE is designed to provide a\ncomprehensive evaluation of the knowledge acquisition capabilities of Chinese\nLLMs. It encompasses 10,465 questions across 75 tasks covering 30 subjects,\nranging from primary school to professional certification exams. Notably, LHMKE\nincludes both objective and subjective questions, offering a more holistic\nevaluation of the knowledge level of LLMs. We have assessed 11 Chinese LLMs\nunder the zero-shot setting, which aligns with real examinations, and compared\ntheir performance across different subjects. We also conduct an in-depth\nanalysis to check whether GPT-4 can automatically score subjective predictions.\nOur findings suggest that LHMKE is a challenging and advanced testbed for\nChinese LLMs.\n', ""  Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically\ntransformed natural language processing research and shown promising strides\ntowards Artificial General Intelligence (AGI). Nonetheless, the high costs\nassociated with training and deploying LLMs present substantial obstacles to\ntransparent, accessible academic research. While several large language models,\nsuch as LLaMA, have been open-sourced by the community, these predominantly\nfocus on English corpora, limiting their usefulness for other languages. In\nthis paper, we propose a method to augment LLaMA with capabilities for\nunderstanding and generating Chinese text and its ability to follow\ninstructions. We achieve this by extending LLaMA's existing vocabulary with an\nadditional 20,000 Chinese tokens, thereby improving its encoding efficiency and\nsemantic understanding of Chinese. We further incorporate secondary\npre-training using Chinese data and fine-tune the model with Chinese\ninstruction datasets, significantly enhancing the model's ability to comprehend\nand execute instructions. Our experimental results indicate that the newly\nproposed model markedly enhances the original LLaMA's proficiency in\nunderstanding and generating Chinese content. Additionally, the results on the\nC-Eval dataset yield competitive performance among the models with several\ntimes the size of ours. We have made our pre-trained models, training scripts,\nand other resources available through GitHub, fostering open research for our\ncommunity. Chinese LLaMA series:\n\\url{https://github.com/ymcui/Chinese-LLaMA-Alpaca} and Chinese Llama-2 series:\n\\url{https://github.com/ymcui/Chinese-LLaMA-Alpaca-2}\n"", '  With the accelerating development of Large Language Models (LLMs), many LLMs\nare beginning to be used in the Chinese K-12 education domain. The integration\nof LLMs and education is getting closer and closer, however, there is currently\nno benchmark for evaluating LLMs that focuses on the Chinese K-12 education\ndomain. Therefore, there is an urgent need for a comprehensive natural language\nprocessing benchmark to accurately assess the capabilities of various LLMs in\nthe Chinese K-12 education domain. To address this, we introduce the E-EVAL,\nthe first comprehensive evaluation benchmark specifically designed for the\nChinese K-12 education field. The E-EVAL consists of 4,351 multiple-choice\nquestions at the primary, middle, and high school levels across a wide range of\nsubjects, including Chinese, English, Politics, History, Ethics, Physics,\nChemistry, Mathematics, and Geography. We conducted a comprehensive evaluation\nof E-EVAL on advanced LLMs, including both English-dominant and\nChinese-dominant models. Findings show that Chinese-dominant models perform\nwell compared to English-dominant models, with many scoring even above the GPT\n4.0. However, almost all models perform poorly in complex subjects such as\nmathematics. We also found that most Chinese-dominant LLMs did not achieve\nhigher scores at the primary school level compared to the middle school level.\nWe observe that the mastery of higher-order knowledge by the model does not\nnecessarily imply the mastery of lower-order knowledge as well. Additionally,\nthe experimental results indicate that the Chain of Thought (CoT) technique is\neffective only for the challenging science subjects, while Few-shot prompting\nis more beneficial for liberal arts subjects. With E-EVAL, we aim to analyze\nthe strengths and limitations of LLMs in educational applications, and to\ncontribute to the progress and development of Chinese K-12 education and LLMs.\n']",Chinese Language Models Evaluation,Evaluating Large Language Models,Large Language Models,Large Language Models
288,28,288_mandarin_chinese_languages_linguistic,"['mandarin', 'chinese', 'languages', 'linguistic', 'language', 'nlp', 'taiwanese', 'english', 'subjects', 'conversational']","['chat', 'subjects', 'questions', 'capabilities', 'evaluation', 'understanding', 'language', 'open', 'benchmark', 'linguistic']","['  Chinese Large Language Models (LLMs) have recently demonstrated impressive\ncapabilities across various NLP benchmarks and real-world applications.\nHowever, the existing benchmarks for comprehensively evaluating these LLMs are\nstill insufficient, particularly in terms of measuring knowledge that LLMs\ncapture. Current datasets collect questions from Chinese examinations across\ndifferent subjects and educational levels to address this issue. Yet, these\nbenchmarks primarily focus on objective questions such as multiple-choice\nquestions, leading to a lack of diversity in question types. To tackle this\nproblem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge\nEvaluation benchmark in this paper. LHMKE is designed to provide a\ncomprehensive evaluation of the knowledge acquisition capabilities of Chinese\nLLMs. It encompasses 10,465 questions across 75 tasks covering 30 subjects,\nranging from primary school to professional certification exams. Notably, LHMKE\nincludes both objective and subjective questions, offering a more holistic\nevaluation of the knowledge level of LLMs. We have assessed 11 Chinese LLMs\nunder the zero-shot setting, which aligns with real examinations, and compared\ntheir performance across different subjects. We also conduct an in-depth\nanalysis to check whether GPT-4 can automatically score subjective predictions.\nOur findings suggest that LHMKE is a challenging and advanced testbed for\nChinese LLMs.\n', ""  Large Language Models (LLMs), such as ChatGPT and GPT-4, have dramatically\ntransformed natural language processing research and shown promising strides\ntowards Artificial General Intelligence (AGI). Nonetheless, the high costs\nassociated with training and deploying LLMs present substantial obstacles to\ntransparent, accessible academic research. While several large language models,\nsuch as LLaMA, have been open-sourced by the community, these predominantly\nfocus on English corpora, limiting their usefulness for other languages. In\nthis paper, we propose a method to augment LLaMA with capabilities for\nunderstanding and generating Chinese text and its ability to follow\ninstructions. We achieve this by extending LLaMA's existing vocabulary with an\nadditional 20,000 Chinese tokens, thereby improving its encoding efficiency and\nsemantic understanding of Chinese. We further incorporate secondary\npre-training using Chinese data and fine-tune the model with Chinese\ninstruction datasets, significantly enhancing the model's ability to comprehend\nand execute instructions. Our experimental results indicate that the newly\nproposed model markedly enhances the original LLaMA's proficiency in\nunderstanding and generating Chinese content. Additionally, the results on the\nC-Eval dataset yield competitive performance among the models with several\ntimes the size of ours. We have made our pre-trained models, training scripts,\nand other resources available through GitHub, fostering open research for our\ncommunity. Chinese LLaMA series:\n\\url{https://github.com/ymcui/Chinese-LLaMA-Alpaca} and Chinese Llama-2 series:\n\\url{https://github.com/ymcui/Chinese-LLaMA-Alpaca-2}\n"", '  With the accelerating development of Large Language Models (LLMs), many LLMs\nare beginning to be used in the Chinese K-12 education domain. The integration\nof LLMs and education is getting closer and closer, however, there is currently\nno benchmark for evaluating LLMs that focuses on the Chinese K-12 education\ndomain. Therefore, there is an urgent need for a comprehensive natural language\nprocessing benchmark to accurately assess the capabilities of various LLMs in\nthe Chinese K-12 education domain. To address this, we introduce the E-EVAL,\nthe first comprehensive evaluation benchmark specifically designed for the\nChinese K-12 education field. The E-EVAL consists of 4,351 multiple-choice\nquestions at the primary, middle, and high school levels across a wide range of\nsubjects, including Chinese, English, Politics, History, Ethics, Physics,\nChemistry, Mathematics, and Geography. We conducted a comprehensive evaluation\nof E-EVAL on advanced LLMs, including both English-dominant and\nChinese-dominant models. Findings show that Chinese-dominant models perform\nwell compared to English-dominant models, with many scoring even above the GPT\n4.0. However, almost all models perform poorly in complex subjects such as\nmathematics. We also found that most Chinese-dominant LLMs did not achieve\nhigher scores at the primary school level compared to the middle school level.\nWe observe that the mastery of higher-order knowledge by the model does not\nnecessarily imply the mastery of lower-order knowledge as well. Additionally,\nthe experimental results indicate that the Chain of Thought (CoT) technique is\neffective only for the challenging science subjects, while Few-shot prompting\nis more beneficial for liberal arts subjects. With E-EVAL, we aim to analyze\nthe strengths and limitations of LLMs in educational applications, and to\ncontribute to the progress and development of Chinese K-12 education and LLMs.\n']",Chinese Language Models Evaluation,Evaluating Large Language Models,Large Language Models,Large Language Models
289,28,289_patents_patenting_patent_patenteval,"['patents', 'patenting', 'patent', 'patenteval', 'inventors', 'invention', 'semantic', 'inventions', 'citation', 'texts']","['patent', 'patents', 'claim', 'claims', 'assignees', 'interdependence', 'technological', 'landscaping', 'inventors', 'citation']","['  Recent advancements in Artificial Intelligence (AI) and machine learning have\ndemonstrated transformative capabilities across diverse domains. This progress\nextends to the field of patent analysis and innovation, where AI-based tools\npresent opportunities to streamline and enhance important tasks in the patent\ncycle such as classification, retrieval, and valuation prediction. This not\nonly accelerates the efficiency of patent researchers and applicants but also\nopens new avenues for technological innovation and discovery. Our survey\nprovides a comprehensive summary of recent AI tools in patent analysis from\nmore than 40 papers from 26 venues between 2017 and 2023. Unlike existing\nsurveys, we include methods that work for patent image and text data.\nFurthermore, we introduce a novel taxonomy for the categorization based on the\ntasks in the patent life cycle as well as the specifics of the AI methods. This\ninterdisciplinary survey aims to serve as a resource for researchers and\npractitioners who are working at the intersection of AI and patent analysis as\nwell as the patent offices that are aiming to build efficient patent systems.\n', ""  This paper makes two contributions to the field of text-based patent\nsimilarity. First, it compares the performance of different kinds of\npatent-specific pretrained embedding models, namely static word embeddings\n(such as word2vec and doc2vec models) and contextual word embeddings (such as\ntransformers based models), on the task of patent similarity calculation.\nSecond, it compares specifically the performance of Sentence Transformers\n(SBERT) architectures with different training phases on the patent similarity\ntask. To assess the models' performance, we use information about patent\ninterferences, a phenomenon in which two or more patent claims belonging to\ndifferent patent applications are proven to be overlapping by patent examiners.\nTherefore, we use these interferences cases as a proxy for maximum similarity\nbetween two patents, treating them as ground-truth to evaluate the performance\nof the different embedding models. Our results point out that, first, Patent\nSBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer\narchitecture proposed in this research, outperforms the current\nstate-of-the-art in patent similarity. Second, they show that, in some cases,\nlarge static models performances are still comparable to contextual ones when\ntrained on extensive data; thus, we believe that the superiority in the\nperformance of contextual embeddings may not be related to the actual\narchitecture but rather to the way the training phase is performed.\n"", '  Patents, encapsulating crucial technical and legal information, present a\nrich domain for natural language processing (NLP) applications. As NLP\ntechnologies evolve, large language models (LLMs) have demonstrated outstanding\ncapabilities in general text processing and generation tasks. However, the\napplication of LLMs in the patent domain remains under-explored and\nunder-developed due to the complexity of patent processing. Understanding the\nunique characteristics of patent documents and related research in the patent\ndomain becomes essential for researchers to apply these tools effectively.\nTherefore, this paper aims to equip NLP researchers with the essential\nknowledge to navigate this complex domain efficiently. We introduce the\nrelevant fundamental aspects of patents to provide solid background\ninformation, particularly for readers unfamiliar with the patent system. In\naddition, we systematically break down the structural and linguistic\ncharacteristics unique to patents and map out how NLP can be leveraged for\npatent analysis and generation. Moreover, we demonstrate the spectrum of\ntext-based patent-related tasks, including nine patent analysis and four patent\ngeneration tasks.\n']",Patent Analysis and Innovation with AI and NLP,"Applications of Artificial Intelligence in Law, Patent Analysis, and Healthcare",Artificial Intelligence Applications and Implications,Artificial Intelligence Applications and Implications
290,28,290_reinforcement_critic_learning_optimism,"['reinforcement', 'critic', 'learning', 'optimism', 'replay', 'reward', 'exploration', 'bias', 'control', 'prioritized']","['critic', 'actor', 'value', 'policy', 'pessimism', 'critics', 'optimism', 'reinforcement', 'function', 'exploration']","['  Temporal difference (TD) learning is a fundamental technique in reinforcement\nlearning that updates value estimates for states or state-action pairs using a\nTD target. This target represents an improved estimate of the true value by\nincorporating both immediate rewards and the estimated value of subsequent\nstates. Traditionally, TD learning relies on the value of a single subsequent\nstate. We propose an enhanced multi-state TD (MSTD) target that utilizes the\nestimated values of multiple subsequent states. Building on this new MSTD\nconcept, we develop complete actor-critic algorithms that include management of\nreplay buffers in two modes, and integrate with deep deterministic policy\noptimization (DDPG) and soft actor-critic (SAC). Experimental results\ndemonstrate that algorithms employing the MSTD target significantly improve\nlearning performance compared to traditional methods.The code is provided on\nGitHub.\n', ""  Off-policy actor-critic algorithms have shown promise in deep reinforcement\nlearning for continuous control tasks. Their success largely stems from\nleveraging pessimistic state-action value function updates, which effectively\naddress function approximation errors and improve performance. However, such\npessimism can lead to under-exploration, constraining the agent's ability to\nexplore/refine its policies. Conversely, optimism can counteract\nunder-exploration, but it also carries the risk of excessive risk-taking and\npoor convergence if not properly balanced. Based on these insights, we\nintroduce Utility Soft Actor-Critic (USAC), a novel framework within the\nactor-critic paradigm that enables independent control over the degree of\npessimism/optimism for both the actor and the critic via interpretable\nparameters. USAC adapts its exploration strategy based on the uncertainty of\ncritics through a utility function that allows us to balance between pessimism\nand optimism separately. By going beyond binary choices of optimism and\npessimism, USAC represents a significant step towards achieving balance within\noff-policy actor-critic algorithms. Our experiments across various continuous\ncontrol problems show that the degree of pessimism or optimism depends on the\nnature of the task. Furthermore, we demonstrate that USAC can outperform\nstate-of-the-art algorithms for appropriately configured pessimism/optimism\nparameters.\n"", '  Actor-critic algorithms address the dual goals of reinforcement learning\n(RL), policy evaluation and improvement via two separate function\napproximators. The practicality of this approach comes at the expense of\ntraining instability, caused mainly by the destructive effect of the\napproximation errors of the critic on the actor. We tackle this bottleneck by\nemploying an existing Probably Approximately Correct (PAC) Bayesian bound for\nthe first time as the critic training objective of the Soft Actor-Critic (SAC)\nalgorithm. We further demonstrate that online learning performance improves\nsignificantly when a stochastic actor explores multiple futures by\ncritic-guided random search. We observe our resulting algorithm to compare\nfavorably against the state-of-the-art SAC implementation on multiple classical\ncontrol and locomotion tasks in terms of both sample efficiency and regret.\n']",Reinforcement Learning with Actor-Critic Algorithms,Reinforcement Learning Applications and Methodologies,Reinforcement Learning,Reinforcement Learning
291,28,291_uav_uavs_unmanned_drone,"['uav', 'uavs', 'unmanned', 'drone', 'aerial', 'swarm', 'offloading', 'optimization', 'optimizing', 'transmit']","['unmanned', 'aerial', 'terrestrial', 'vehicles', 'wireless', 'transmission', 'transmit', 'energy', 'emergency', 'slots']","['  In this paper, the problem of using one active unmanned aerial vehicle (UAV)\nand four passive UAVs to localize a 3D target UAV in real time is investigated.\nIn the considered model, each passive UAV receives reflection signals from the\ntarget UAV, which are initially transmitted by the active UAV. The received\nreflection signals allow each passive UAV to estimate the signal transmission\ndistance which will be transmitted to a base station (BS) for the estimation of\nthe position of the target UAV. Due to the movement of the target UAV, each\nactive/passive UAV must optimize its trajectory to continuously localize the\ntarget UAV. Meanwhile, since the accuracy of the distance estimation depends on\nthe signal-to-noise ratio of the transmission signals, the active UAV must\noptimize its transmit power. This problem is formulated as an optimization\nproblem whose goal is to jointly optimize the transmit power of the active UAV\nand trajectories of both active and passive UAVs so as to maximize the target\nUAV positioning accuracy. To solve this problem, a Z function decomposition\nbased reinforcement learning (ZD-RL) method is proposed. Compared to value\nfunction decomposition based RL (VD-RL), the proposed method can find the\nprobability distribution of the sum of future rewards to accurately estimate\nthe expected value of the sum of future rewards thus finding better transmit\npower of the active UAV and trajectories for both active and passive UAVs and\nimproving target UAV positioning accuracy. Simulation results show that the\nproposed ZD-RL method can reduce the positioning errors by up to 39.4% and\n64.6%, compared to VD-RL and independent deep RL methods, respectively.\n', '  Effective solutions for intelligent data collection in terrestrial cellular\nnetworks are crucial, especially in the context of Internet of Things\napplications. The limited spectrum and coverage area of terrestrial base\nstations pose challenges in meeting the escalating data rate demands of network\nusers. Unmanned aerial vehicles, known for their high agility, mobility, and\nflexibility, present an alternative means to offload data traffic from\nterrestrial BSs, serving as additional access points. This paper introduces a\nnovel approach to efficiently maximize the utilization of multiple UAVs for\ndata traffic offloading from terrestrial BSs. Specifically, the focus is on\nmaximizing user association with UAVs by jointly optimizing UAV trajectories\nand users association indicators under quality of service constraints. Since,\nthe formulated UAVs control problem is nonconvex and combinatorial, this study\nleverages the multi agent reinforcement learning framework. In this framework,\neach UAV acts as an independent agent, aiming to maintain inter UAV cooperative\nbehavior. The proposed approach utilizes the finite state Markov decision\nprocess to account for UAVs velocity constraints and the relationship between\ntheir trajectories and state space. A low complexity distributed state action\nreward state action algorithm is presented to determine UAVs optimal sequential\ndecision making policies over training episodes. The extensive simulation\nresults validate the proposed analysis and offer valuable insights into the\noptimal UAV trajectories. The derived trajectories demonstrate superior average\nUAV association performance compared to benchmark techniques such as Q learning\nand particle swarm optimization.\n', '  Recently, Unmanned Aerial Vehicles (UAVs) have attracted the attention of\nresearchers in academia and industry for providing wireless services to ground\nusers in diverse scenarios like festivals, large sporting events, natural and\nman-made disasters due to their advantages in terms of versatility and\nmaneuverability. However, the limited resources of UAVs (e.g., energy budget\nand different service requirements) can pose challenges for adopting UAVs for\nsuch applications. Our system model considers a UAV swarm that navigates an\narea, providing wireless communication to ground users with RIS support to\nimprove the coverage of the UAVs. In this work, we introduce an optimization\nmodel with the aim of maximizing the throughput and UAVs coverage through\noptimal path planning of UAVs and multi-RIS phase configurations. The\nformulated optimization is challenging to solve using standard linear\nprogramming techniques, limiting its applicability in real-time\ndecision-making. Therefore, we introduce a two-step solution using deep\nreinforcement learning and particle swarm optimization. We conduct extensive\nsimulations and compare our approach to two competitive solutions presented in\nthe recent literature. Our simulation results demonstrate that our adopted\napproach is 20 \\% better than the brute-force approach and 30\\% better than the\nbaseline solution in terms of QoS.\n']",UAV Trajectory Optimization for Wireless Communication,Unmanned Aerial Vehicle (UAV) Systems and Technologies,Aerial Robotics and Agricultural Computer Vision,Aerial Robotics and Agricultural Computer Vision
292,28,292_facetalk_lip_styletalker_audio,"['facetalk', 'lip', 'styletalker', 'audio', 'portrait', 'animations', 'talkformer', 'audio2rig', 'mouth', 'animation']","['lip', 'talking', 'facial', 'sync', 'audio', 'motion', 'animation', 'synchronization', 'face', 'head']","['  We propose StyleTalker, a novel audio-driven talking head generation model\nthat can synthesize a video of a talking person from a single reference image\nwith accurately audio-synced lip shapes, realistic head poses, and eye blinks.\nSpecifically, by leveraging a pretrained image generator and an image encoder,\nwe estimate the latent codes of the talking head video that faithfully reflects\nthe given audio. This is made possible with several newly devised components:\n1) A contrastive lip-sync discriminator for accurate lip synchronization, 2) A\nconditional sequential variational autoencoder that learns the latent motion\nspace disentangled from the lip movements, such that we can independently\nmanipulate the motions and lip movements while preserving the identity. 3) An\nauto-regressive prior augmented with normalizing flow to learn a complex\naudio-to-motion multi-modal latent space. Equipped with these components,\nStyleTalker can generate talking head videos not only in a motion-controllable\nway when another motion source video is given but also in a completely\naudio-driven manner by inferring realistic motions from the input audio.\nThrough extensive experiments and user studies, we show that our model is able\nto synthesize talking head videos with impressive perceptual quality which are\naccurately lip-synced with the input audios, largely outperforming\nstate-of-the-art baselines.\n', '  Audio-driven lip sync has recently drawn significant attention due to its\nwidespread application in the multimedia domain. Individuals exhibit distinct\nlip shapes when speaking the same utterance, attributed to the unique speaking\nstyles of individuals, posing a notable challenge for audio-driven lip sync.\nEarlier methods for such task often bypassed the modeling of personalized\nspeaking styles, resulting in sub-optimal lip sync conforming to the general\nstyles. Recent lip sync techniques attempt to guide the lip sync for arbitrary\naudio by aggregating information from a style reference video, yet they can not\npreserve the speaking styles well due to their inaccuracy in style aggregation.\nThis work proposes an innovative audio-aware style reference scheme that\neffectively leverages the relationships between input audio and reference audio\nfrom style reference video to address the style-preserving audio-driven lip\nsync. Specifically, we first develop an advanced Transformer-based model adept\nat predicting lip motion corresponding to the input audio, augmented by the\nstyle information aggregated through cross-attention layers from style\nreference video. Afterwards, to better render the lip motion into realistic\ntalking face video, we devise a conditional latent diffusion model, integrating\nlip motion through modulated convolutional layers and fusing reference facial\nimages via spatial cross-attention layers. Extensive experiments validate the\nefficacy of the proposed approach in achieving precise lip sync, preserving\nspeaking styles, and generating high-fidelity, realistic talking face videos.\n', '  Speech-driven facial animation methods usually contain two main classes, 3D\nand 2D talking face, both of which attract considerable research attention in\nrecent years. However, to the best of our knowledge, the research on 3D talking\nface does not go deeper as 2D talking face, in the aspect of\nlip-synchronization (lip-sync) and speech perception. To mind the gap between\nthe two sub-fields, we propose a learning framework named Learn2Talk, which can\nconstruct a better 3D talking face network by exploiting two expertise points\nfrom the field of 2D talking face. Firstly, inspired by the audio-video sync\nnetwork, a 3D sync-lip expert model is devised for the pursuit of lip-sync\nbetween audio and 3D facial motion. Secondly, a teacher model selected from 2D\ntalking face methods is used to guide the training of the audio-to-3D motions\nregression network to yield more 3D vertex accuracy. Extensive experiments show\nthe advantages of the proposed framework in terms of lip-sync, vertex accuracy\nand speech perception, compared with state-of-the-arts. Finally, we show two\napplications of the proposed framework: audio-visual speech recognition and\nspeech-driven 3D Gaussian Splatting based avatar animation.\n']",Audio-Driven Talking Face Generation,Speech and Voice Synthesis,Speech and Audio Processing,Speech and Audio Processing
293,27,293_partisan_political_biases_politically,"['partisan', 'political', 'biases', 'politically', 'politicians', 'elections', 'ideological', 'constituency', 'electoral', 'politician']","['political', 'democratic', 'opinions', 'ideological', 'partisan', 'voting', 'codebooks', 'left', 'parties', 'sycophancy']","[""  Instruction-finetuned Large Language Models inherit clear political leanings\nthat have been shown to influence downstream task performance. We expand this\nline of research beyond the two-party system in the US and audit Llama Chat in\nthe context of EU politics in various settings to analyze the model's political\nknowledge and its ability to reason in context. We adapt, i.e., further\nfine-tune, Llama Chat on speeches of individual euro-parties from debates in\nthe European Parliament to reevaluate its political leaning based on the EUandI\nquestionnaire. Llama Chat shows considerable knowledge of national parties'\npositions and is capable of reasoning in context. The adapted, party-specific,\nmodels are substantially re-aligned towards respective positions which we see\nas a starting point for using chat-based LLMs as data-driven conversational\nengines to assist research in political science.\n"", '  The assessment of bias within Large Language Models (LLMs) has emerged as a\ncritical concern in the contemporary discourse surrounding Artificial\nIntelligence (AI) in the context of their potential impact on societal\ndynamics. Recognizing and considering political bias within LLM applications is\nespecially important when closing in on the tipping point toward performative\nprediction. Then, being educated about potential effects and the societal\nbehavior LLMs can drive at scale due to their interplay with human operators.\nIn this way, the upcoming elections of the European Parliament will not remain\nunaffected by LLMs. We evaluate the political bias of the currently most\npopular open-source LLMs (instruct or assistant models) concerning political\nissues within the European Union (EU) from a German voter\'s perspective. To do\nso, we use the ""Wahl-O-Mat,"" a voting advice application used in Germany. From\nthe voting advice of the ""Wahl-O-Mat"" we quantize the degree of alignment of\nLLMs with German political parties. We show that larger models, such as\nLlama3-70B, tend to align more closely with left-leaning political parties,\nwhile smaller models often remain neutral, particularly when prompted in\nEnglish. The central finding is that LLMs are similarly biased, with low\nvariances in the alignment concerning a specific party. Our findings underline\nthe importance of rigorously assessing and making bias transparent in LLMs to\nsafeguard the integrity and trustworthiness of applications that employ the\ncapabilities of performative prediction and the invisible hand of machine\nlearning prediction and language generation.\n', ""  I report here a comprehensive analysis about the political preferences\nembedded in Large Language Models (LLMs). Namely, I administer 11 political\norientation tests, designed to identify the political preferences of the test\ntaker, to 24 state-of-the-art conversational LLMs, both closed and open source.\nWhen probed with questions/statements with political connotations, most\nconversational LLMs tend to generate responses that are diagnosed by most\npolitical test instruments as manifesting preferences for left-of-center\nviewpoints. This does not appear to be the case for five additional base (i.e.\nfoundation) models upon which LLMs optimized for conversation with humans are\nbuilt. However, the weak performance of the base models at coherently answering\nthe tests' questions makes this subset of results inconclusive. Finally, I\ndemonstrate that LLMs can be steered towards specific locations in the\npolitical spectrum through Supervised Fine-Tuning (SFT) with only modest\namounts of politically aligned data, suggesting SFT's potential to embed\npolitical orientation in LLMs. With LLMs beginning to partially displace\ntraditional information sources like search engines and Wikipedia, the societal\nimplications of political biases embedded in LLMs are substantial.\n""]",Political Biases in Large Language Models,Bias and Fairness in Large Language Models,Fairness and Bias in Artificial Intelligence,Fairness and Bias in Artificial Intelligence
294,27,294_forecasting_rnn_temporal_predicting,"['forecasting', 'rnn', 'temporal', 'predicting', 'prediction', 'neural', 'events', 'models', 'future', 'autoregressive']","['event', 'events', 'temporal', 'sequences', 'point', 'intensity', 'processes', 'marks', 'arrival', 'spatio']","['  An extension of the Hawkes process, the Marked Hawkes process distinguishes\nitself by featuring variable jump size across each event, in contrast to the\nconstant jump size observed in a Hawkes process without marks. While extensive\nliterature has been dedicated to the non-parametric estimation of both the\nlinear and non-linear Hawkes process, there remains a significant gap in the\nliterature regarding the marked Hawkes process. In response to this, we propose\na methodology for estimating the conditional intensity of the marked Hawkes\nprocess. We introduce two distinct models: \\textit{Shallow Neural Hawkes with\nmarks}- for Hawkes processes with excitatory kernels and \\textit{Neural Network\nfor Non-Linear Hawkes with Marks}- for non-linear Hawkes processes. Both these\napproaches take the past arrival times and their corresponding marks as the\ninput to obtain the arrival intensity. This approach is entirely\nnon-parametric, preserving the interpretability associated with the marked\nHawkes process. To validate the efficacy of our method, we subject the method\nto synthetic datasets with known ground truth. Additionally, we apply our\nmethod to model cryptocurrency order book data, demonstrating its applicability\nto real-world scenarios.\n', '  Neural Temporal Point Processes (TPPs) have emerged as the primary framework\nfor predicting sequences of events that occur at irregular time intervals, but\ntheir sequential nature can hamper performance for long-horizon forecasts. To\naddress this, we introduce a novel approach that incorporates a diffusion\ngenerative model. The model facilitates sequence-to-sequence prediction,\nallowing multi-step predictions based on historical event sequences. In\ncontrast to previous approaches, our model directly learns the joint\nprobability distribution of types and inter-arrival times for multiple events.\nThis allows us to fully leverage the high dimensional modeling capability of\nmodern generative models. Our model is composed of two diffusion processes, one\nfor the time intervals and one for the event types. These processes interact\nthrough their respective denoising functions, which can take as input\nintermediate representations from both processes, allowing the model to learn\ncomplex interactions. We demonstrate that our proposal outperforms\nstate-of-the-art baselines for long-horizon forecasting of TPP.\n', ""  Temporal Point Processes (TPPs) hold a pivotal role in modeling event\nsequences across diverse domains, including social networking and e-commerce,\nand have significantly contributed to the advancement of recommendation systems\nand information retrieval strategies. Through the analysis of events such as\nuser interactions and transactions, TPPs offer valuable insights into\nbehavioral patterns, facilitating the prediction of future trends. However,\naccurately forecasting future events remains a formidable challenge due to the\nintricate nature of these patterns. The integration of Neural Networks with\nTPPs has ushered in the development of advanced deep TPP models. While these\nmodels excel at processing complex and nonlinear temporal data, they encounter\nlimitations in modeling intensity functions, grapple with computational\ncomplexities in integral computations, and struggle to capture long-range\ntemporal dependencies effectively. In this study, we introduce the CuFun model,\nrepresenting a novel approach to TPPs that revolves around the Cumulative\nDistribution Function (CDF). CuFun stands out by uniquely employing a monotonic\nneural network for CDF representation, utilizing past events as a scaling\nfactor. This innovation significantly bolsters the model's adaptability and\nprecision across a wide range of data scenarios. Our approach addresses several\ncritical issues inherent in traditional TPP modeling: it simplifies\nlog-likelihood calculations, extends applicability beyond predefined density\nfunction forms, and adeptly captures long-range temporal patterns. Our\ncontributions encompass the introduction of a pioneering CDF-based TPP model,\nthe development of a methodology for incorporating past event information into\nfuture event prediction, and empirical validation of CuFun's effectiveness\nthrough extensive experimentation on synthetic and real-world datasets.\n""]",Temporal Point Process Modeling,Advanced Modeling of Temporal and Dynamical Systems,Machine Learning for Dynamical Systems and Differential Equations,Machine Learning for Dynamical Systems and Differential Equations
295,27,295_resumes_resume_hiring_jobs,"['resumes', 'resume', 'hiring', 'jobs', 'recruitment', 'workforce', 'employment', 'applicants', 'skills', 'occupations']","['job', 'resume', 'career', 'skill', 'skills', 'resumes', 'occupational', 'recruitment', 'market', 'descriptions']","['  [Abridged Abstract]\n  Recent technological advances underscore labor market dynamics, yielding\nsignificant consequences for employment prospects and increasing job vacancy\ndata across platforms and languages. Aggregating such data holds potential for\nvaluable insights into labor market demands, new skills emergence, and\nfacilitating job matching for various stakeholders. However, despite prevalent\ninsights in the private sector, transparent language technology systems and\ndata for this domain are lacking. This thesis investigates Natural Language\nProcessing (NLP) technology for extracting relevant information from job\ndescriptions, identifying challenges including scarcity of training data, lack\nof standardized annotation guidelines, and shortage of effective extraction\nmethods from job ads. We frame the problem, obtaining annotated data, and\nintroducing extraction methodologies. Our contributions include job description\ndatasets, a de-identification dataset, and a novel active learning algorithm\nfor efficient model training. We propose skill extraction using weak\nsupervision, a taxonomy-aware pre-training methodology adapting multilingual\nlanguage models to the job market domain, and a retrieval-augmented model\nleveraging multiple skill extraction datasets to enhance overall performance.\nFinally, we ground extracted information within a designated taxonomy.\n', '  A reliable resume-job matching system helps a company find suitable\ncandidates from a pool of resumes, and helps a job seeker find relevant jobs\nfrom a list of job posts. However, since job seekers apply only to a few jobs,\ninteraction records in resume-job datasets are sparse. Different from many\nprior work that use complex modeling techniques, we tackle this sparsity\nproblem using data augmentations and a simple contrastive learning approach.\nConFit first creates an augmented resume-job dataset by paraphrasing specific\nsections in a resume or a job post. Then, ConFit uses contrastive learning to\nfurther increase training samples from $B$ pairs per batch to $O(B^2)$ per\nbatch. We evaluate ConFit on two real-world datasets and find it outperforms\nprior methods (including BM25 and OpenAI text-ada-002) by up to 19% and 31%\nabsolute in nDCG@10 for ranking jobs and ranking resumes, respectively.\n', ""  Crafting the ideal, job-specific resume is a challenging task for many job\napplicants, especially for early-career applicants. While it is highly\nrecommended that applicants tailor their resume to the specific role they are\napplying for, manually tailoring resumes to job descriptions and role-specific\nrequirements is often (1) extremely time-consuming, and (2) prone to human\nerrors. Furthermore, performing such a tailoring step at scale while applying\nto several roles may result in a lack of quality of the edited resumes. To\ntackle this problem, in this demo paper, we propose ResumeFlow: a Large\nLanguage Model (LLM) aided tool that enables an end user to simply provide\ntheir detailed resume and the desired job posting, and obtain a personalized\nresume specifically tailored to that specific job posting in the matter of a\nfew seconds. Our proposed pipeline leverages the language understanding and\ninformation extraction capabilities of state-of-the-art LLMs such as OpenAI's\nGPT-4 and Google's Gemini, in order to (1) extract details from a job\ndescription, (2) extract role-specific details from the user-provided resume,\nand then (3) use these to refine and generate a role-specific resume for the\nuser. Our easy-to-use tool leverages the user-chosen LLM in a completely\noff-the-shelf manner, thus requiring no fine-tuning. We demonstrate the\neffectiveness of our tool via a video demo and propose novel task-specific\nevaluation metrics to control for alignment and hallucination. Our tool is\navailable at https://job-aligned-resume.streamlit.app.\n""]",Job Market Analysis and Resume Optimization,Business and Marketing Analytics,Business and Marketing Analytics,Business and Marketing Analytics
296,27,296_machining_fabrication_manufacturing_welding,"['machining', 'fabrication', 'manufacturing', 'welding', 'laser', 'machined', 'cad', 'fusion', 'porosity', 'machine']","['manufacturing', 'laser', 'metal', 'printing', 'additive', 'welding', 'temperature', 'roughness', 'porosity', 'thermal']","['  Additive manufacturing, especially laser powder bed fusion (L-PBF), is widely\nused for fabricating metal parts with intricate geometries. However, parts\nproduced via L-PBF suffer from varied surface roughness which affects the\ndynamic or fatigue properties. Accurate prediction of fatigue properties as a\nfunction of surface roughness is a critical requirement for qualifying L-PBF\nparts. In this work, an analytical methodology is put forth to predict the\nfatigue life of L-PBF components having heterogeneous surface roughness.\nThirty-six Hastelloy X specimens are printed using L-PBF followed by\nindustry-standard heat treatment procedures. Half of these specimens are built\nwith as-printed gauge sections and the other half is printed as cylinders from\nwhich fatigue specimens are extracted via machining. Specimens are printed in a\nvertical orientation and an orientation 30 degree from the vertical axis. The\nsurface roughness of the specimens is measured using computed tomography and\nparameters such as the maximum valley depth are used to build an extreme value\ndistribution. Fatigue testing is conducted at an isothermal condition of\n500-degree F. It is observed that the rough specimens fail much earlier\ncompared to the machined specimens due to the deep valleys present on the\nsurfaces of the former ones. The valleys act as notches leading to high strain\nlocalization. Following this observation, a functional relationship is\nformulated analytically that considers surface valleys as notches and\ncorrelates the strain localization around those notches with fatigue life,\nusing the Coffin-Manson-Basquin and Ramberg-Osgood equation. In conclusion, the\nproposed analytical model successfully predicts the fatigue life of L-PBF\nspecimens at an elevated temperature undergoing different strain loadings.\n', '  Metal additive manufacturing is gaining broad interest and increased use in\nthe industrial and academic fields. However, the quantification and\ncommercialization of standard parts usually require extensive experiments and\nexpensive post-characterization, which impedes the rapid development and\nadaptation of metal AM technologies. In this work, a similarity-based\nacceleration (S-acceleration) method for design of experiments is developed to\nreduce the time and costs associated with unveiling process-property (porosity\ndefects) relationships during manufacturing. With S-acceleration, part semantic\nfeatures from machine-setting parameters and physics-effects informed\ncharacteristics are explored for measuring mutual part similarities. A\nuser-defined simplification rate of experiments is proposed to purposely remove\nredundant parts before conducting experiments printing without sacrificing\ninformation gain as original full factorial experiment design. This\nS-acceleration design of experiments is demonstrated on a Concept Laser M2\nmachine for the experimental plan of modeling relationships between process\nparameters and part porosity defects. The printed part has 2 mm diameter by 4\nmm tall pin geometry considering variations in build location and orientation,\nlaser settings and powder feedstock are held constant. In total, 242 parts are\nmeasured to create a ground truth data set of porosity levels by using X-ray\ntomography microscopy. The S-acceleration method is assessed for performance\nconsidering 40%, 50%, and 60% of user-defined experiment simplification rates.\nThe repeated experiments are removed without ignoring the minority experiments\noutlier, assuring a similar process-property relation in the original\nexperiment plan. The experiment number is significantly reduced based on part\nsimilarity with minimal compromise of model accuracy and obtained knowledge.\n', '  A digital twin (DT), with the components of a physics-based model, a\ndata-driven model, and a machine learning (ML) enabled efficient surrogate,\nbehaves as a virtual twin of the real-world physical process. In terms of Laser\nPowder Bed Fusion (L-PBF) based additive manufacturing (AM), a DT can predict\nthe current and future states of the melt pool and the resulting defects\ncorresponding to the input laser parameters, evolve itself by assimilating\nin-situ sensor data, and optimize the laser parameters to mitigate defect\nformation. In this paper, we present a deep neural operator enabled\ncomputational framework of the DT for closed-loop feedback control of the L-PBF\nprocess. This is accomplished by building a high-fidelity computational model\nto accurately represent the melt pool states, an efficient surrogate model to\napproximate the melt pool solution field, followed by an physics-based\nprocedure to extract information from the computed melt pool simulation that\ncan further be correlated to the defect quantities of interest (e.g., surface\nroughness). In particular, we leverage the data generated from the\nhigh-fidelity physics-based model and train a series of Fourier neural operator\n(FNO) based ML models to effectively learn the relation between the input laser\nparameters and the corresponding full temperature field of the melt pool.\nSubsequently, a set of physics-informed variables such as the melt pool\ndimensions and the peak temperature can be extracted to compute the resulting\ndefects. An optimization algorithm is then exercised to control laser input and\nminimize defects. On the other hand, the constructed DT can also evolve with\nthe physical twin via offline finetuning and online material calibration.\nFinally, a probabilistic framework is adopted for uncertainty quantification.\nThe developed DT is envisioned to guide the AM process and facilitate\nhigh-quality manufacturing.\n']",Additive Manufacturing and Laser-Based Fabrication,Additive Manufacturing and Laser-Based Fabrication Techniques,"Advanced Technologies for Simulation, Sensing, and Fabrication","Advanced Technologies for Simulation, Sensing, and Fabrication"
296,27,296_machining_fabrication_manufacturing_welding,"['machining', 'fabrication', 'manufacturing', 'welding', 'laser', 'machined', 'cad', 'fusion', 'porosity', 'machine']","['manufacturing', 'laser', 'metal', 'printing', 'additive', 'welding', 'temperature', 'roughness', 'porosity', 'thermal']","['  Additive manufacturing, especially laser powder bed fusion (L-PBF), is widely\nused for fabricating metal parts with intricate geometries. However, parts\nproduced via L-PBF suffer from varied surface roughness which affects the\ndynamic or fatigue properties. Accurate prediction of fatigue properties as a\nfunction of surface roughness is a critical requirement for qualifying L-PBF\nparts. In this work, an analytical methodology is put forth to predict the\nfatigue life of L-PBF components having heterogeneous surface roughness.\nThirty-six Hastelloy X specimens are printed using L-PBF followed by\nindustry-standard heat treatment procedures. Half of these specimens are built\nwith as-printed gauge sections and the other half is printed as cylinders from\nwhich fatigue specimens are extracted via machining. Specimens are printed in a\nvertical orientation and an orientation 30 degree from the vertical axis. The\nsurface roughness of the specimens is measured using computed tomography and\nparameters such as the maximum valley depth are used to build an extreme value\ndistribution. Fatigue testing is conducted at an isothermal condition of\n500-degree F. It is observed that the rough specimens fail much earlier\ncompared to the machined specimens due to the deep valleys present on the\nsurfaces of the former ones. The valleys act as notches leading to high strain\nlocalization. Following this observation, a functional relationship is\nformulated analytically that considers surface valleys as notches and\ncorrelates the strain localization around those notches with fatigue life,\nusing the Coffin-Manson-Basquin and Ramberg-Osgood equation. In conclusion, the\nproposed analytical model successfully predicts the fatigue life of L-PBF\nspecimens at an elevated temperature undergoing different strain loadings.\n', '  Metal additive manufacturing is gaining broad interest and increased use in\nthe industrial and academic fields. However, the quantification and\ncommercialization of standard parts usually require extensive experiments and\nexpensive post-characterization, which impedes the rapid development and\nadaptation of metal AM technologies. In this work, a similarity-based\nacceleration (S-acceleration) method for design of experiments is developed to\nreduce the time and costs associated with unveiling process-property (porosity\ndefects) relationships during manufacturing. With S-acceleration, part semantic\nfeatures from machine-setting parameters and physics-effects informed\ncharacteristics are explored for measuring mutual part similarities. A\nuser-defined simplification rate of experiments is proposed to purposely remove\nredundant parts before conducting experiments printing without sacrificing\ninformation gain as original full factorial experiment design. This\nS-acceleration design of experiments is demonstrated on a Concept Laser M2\nmachine for the experimental plan of modeling relationships between process\nparameters and part porosity defects. The printed part has 2 mm diameter by 4\nmm tall pin geometry considering variations in build location and orientation,\nlaser settings and powder feedstock are held constant. In total, 242 parts are\nmeasured to create a ground truth data set of porosity levels by using X-ray\ntomography microscopy. The S-acceleration method is assessed for performance\nconsidering 40%, 50%, and 60% of user-defined experiment simplification rates.\nThe repeated experiments are removed without ignoring the minority experiments\noutlier, assuring a similar process-property relation in the original\nexperiment plan. The experiment number is significantly reduced based on part\nsimilarity with minimal compromise of model accuracy and obtained knowledge.\n', '  A digital twin (DT), with the components of a physics-based model, a\ndata-driven model, and a machine learning (ML) enabled efficient surrogate,\nbehaves as a virtual twin of the real-world physical process. In terms of Laser\nPowder Bed Fusion (L-PBF) based additive manufacturing (AM), a DT can predict\nthe current and future states of the melt pool and the resulting defects\ncorresponding to the input laser parameters, evolve itself by assimilating\nin-situ sensor data, and optimize the laser parameters to mitigate defect\nformation. In this paper, we present a deep neural operator enabled\ncomputational framework of the DT for closed-loop feedback control of the L-PBF\nprocess. This is accomplished by building a high-fidelity computational model\nto accurately represent the melt pool states, an efficient surrogate model to\napproximate the melt pool solution field, followed by an physics-based\nprocedure to extract information from the computed melt pool simulation that\ncan further be correlated to the defect quantities of interest (e.g., surface\nroughness). In particular, we leverage the data generated from the\nhigh-fidelity physics-based model and train a series of Fourier neural operator\n(FNO) based ML models to effectively learn the relation between the input laser\nparameters and the corresponding full temperature field of the melt pool.\nSubsequently, a set of physics-informed variables such as the melt pool\ndimensions and the peak temperature can be extracted to compute the resulting\ndefects. An optimization algorithm is then exercised to control laser input and\nminimize defects. On the other hand, the constructed DT can also evolve with\nthe physical twin via offline finetuning and online material calibration.\nFinally, a probabilistic framework is adopted for uncertainty quantification.\nThe developed DT is envisioned to guide the AM process and facilitate\nhigh-quality manufacturing.\n']",Additive Manufacturing and Laser-Based Fabrication,Additive Manufacturing and Laser-Based Fabrication Techniques,"Advanced Technologies for Simulation, Sensing, and Fabrication","Advanced Technologies for Simulation, Sensing, and Fabrication"
296,27,296_machining_fabrication_manufacturing_welding,"['machining', 'fabrication', 'manufacturing', 'welding', 'laser', 'machined', 'cad', 'fusion', 'porosity', 'machine']","['manufacturing', 'laser', 'metal', 'printing', 'additive', 'welding', 'temperature', 'roughness', 'porosity', 'thermal']","['  Additive manufacturing, especially laser powder bed fusion (L-PBF), is widely\nused for fabricating metal parts with intricate geometries. However, parts\nproduced via L-PBF suffer from varied surface roughness which affects the\ndynamic or fatigue properties. Accurate prediction of fatigue properties as a\nfunction of surface roughness is a critical requirement for qualifying L-PBF\nparts. In this work, an analytical methodology is put forth to predict the\nfatigue life of L-PBF components having heterogeneous surface roughness.\nThirty-six Hastelloy X specimens are printed using L-PBF followed by\nindustry-standard heat treatment procedures. Half of these specimens are built\nwith as-printed gauge sections and the other half is printed as cylinders from\nwhich fatigue specimens are extracted via machining. Specimens are printed in a\nvertical orientation and an orientation 30 degree from the vertical axis. The\nsurface roughness of the specimens is measured using computed tomography and\nparameters such as the maximum valley depth are used to build an extreme value\ndistribution. Fatigue testing is conducted at an isothermal condition of\n500-degree F. It is observed that the rough specimens fail much earlier\ncompared to the machined specimens due to the deep valleys present on the\nsurfaces of the former ones. The valleys act as notches leading to high strain\nlocalization. Following this observation, a functional relationship is\nformulated analytically that considers surface valleys as notches and\ncorrelates the strain localization around those notches with fatigue life,\nusing the Coffin-Manson-Basquin and Ramberg-Osgood equation. In conclusion, the\nproposed analytical model successfully predicts the fatigue life of L-PBF\nspecimens at an elevated temperature undergoing different strain loadings.\n', '  Metal additive manufacturing is gaining broad interest and increased use in\nthe industrial and academic fields. However, the quantification and\ncommercialization of standard parts usually require extensive experiments and\nexpensive post-characterization, which impedes the rapid development and\nadaptation of metal AM technologies. In this work, a similarity-based\nacceleration (S-acceleration) method for design of experiments is developed to\nreduce the time and costs associated with unveiling process-property (porosity\ndefects) relationships during manufacturing. With S-acceleration, part semantic\nfeatures from machine-setting parameters and physics-effects informed\ncharacteristics are explored for measuring mutual part similarities. A\nuser-defined simplification rate of experiments is proposed to purposely remove\nredundant parts before conducting experiments printing without sacrificing\ninformation gain as original full factorial experiment design. This\nS-acceleration design of experiments is demonstrated on a Concept Laser M2\nmachine for the experimental plan of modeling relationships between process\nparameters and part porosity defects. The printed part has 2 mm diameter by 4\nmm tall pin geometry considering variations in build location and orientation,\nlaser settings and powder feedstock are held constant. In total, 242 parts are\nmeasured to create a ground truth data set of porosity levels by using X-ray\ntomography microscopy. The S-acceleration method is assessed for performance\nconsidering 40%, 50%, and 60% of user-defined experiment simplification rates.\nThe repeated experiments are removed without ignoring the minority experiments\noutlier, assuring a similar process-property relation in the original\nexperiment plan. The experiment number is significantly reduced based on part\nsimilarity with minimal compromise of model accuracy and obtained knowledge.\n', '  A digital twin (DT), with the components of a physics-based model, a\ndata-driven model, and a machine learning (ML) enabled efficient surrogate,\nbehaves as a virtual twin of the real-world physical process. In terms of Laser\nPowder Bed Fusion (L-PBF) based additive manufacturing (AM), a DT can predict\nthe current and future states of the melt pool and the resulting defects\ncorresponding to the input laser parameters, evolve itself by assimilating\nin-situ sensor data, and optimize the laser parameters to mitigate defect\nformation. In this paper, we present a deep neural operator enabled\ncomputational framework of the DT for closed-loop feedback control of the L-PBF\nprocess. This is accomplished by building a high-fidelity computational model\nto accurately represent the melt pool states, an efficient surrogate model to\napproximate the melt pool solution field, followed by an physics-based\nprocedure to extract information from the computed melt pool simulation that\ncan further be correlated to the defect quantities of interest (e.g., surface\nroughness). In particular, we leverage the data generated from the\nhigh-fidelity physics-based model and train a series of Fourier neural operator\n(FNO) based ML models to effectively learn the relation between the input laser\nparameters and the corresponding full temperature field of the melt pool.\nSubsequently, a set of physics-informed variables such as the melt pool\ndimensions and the peak temperature can be extracted to compute the resulting\ndefects. An optimization algorithm is then exercised to control laser input and\nminimize defects. On the other hand, the constructed DT can also evolve with\nthe physical twin via offline finetuning and online material calibration.\nFinally, a probabilistic framework is adopted for uncertainty quantification.\nThe developed DT is envisioned to guide the AM process and facilitate\nhigh-quality manufacturing.\n']",Additive Manufacturing and Laser-Based Fabrication,Additive Manufacturing and Laser-Based Fabrication Techniques,"Advanced Technologies for Simulation, Sensing, and Fabrication","Advanced Technologies for Simulation, Sensing, and Fabrication"
297,27,297_speechcodes_speechx_speechgpt_voice,"['speechcodes', 'speechx', 'speechgpt', 'voice', 'voicecraft', 'voicebox', 'phonemes', 'speeches', 'speech', 'audio']","['speech', 'speaker', 'acoustic', 'shot', 'synthesis', 'audio', 'voice', 'tokens', 'naturalness', 'autoregressive']","['  We propose a novel text-to-speech (TTS) framework centered around a neural\ntransducer. Our approach divides the whole TTS pipeline into semantic-level\nsequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling\nstages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings.\nFor a robust and efficient alignment modeling, we employ a neural transducer\nnamed token transducer for the semantic token prediction, benefiting from its\nhard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR)\nspeech generator efficiently synthesizes waveforms from these semantic tokens.\nAdditionally, a reference speech controls temporal dynamics and acoustic\nconditions at each stage. This decoupled framework reduces the training\ncomplexity of TTS while allowing each stage to focus on semantic and acoustic\nmodeling. Our experimental results on zero-shot adaptive TTS demonstrate that\nour model surpasses the baseline in terms of speech quality and speaker\nsimilarity, both objectively and subjectively. We also delve into the inference\nspeed and prosody control capabilities of our approach, highlighting the\npotential of neural transducers in TTS frameworks.\n', '  Recent years have witnessed a trend that large language model (LLM) based\ntext-to-speech (TTS) emerges into the mainstream due to their high naturalness\nand zero-shot capacity. In this paradigm, speech signals are discretized into\ntoken sequences, which are modeled by an LLM with text as prompts and\nreconstructed by a token-based vocoder to waveforms. Obviously, speech tokens\nplay a critical role in LLM-based TTS models. Current speech tokens are learned\nin an unsupervised manner, which lacks explicit semantic information and\nalignment to the text. In this paper, we propose to represent speech with\nsupervised semantic tokens, which are derived from a multilingual speech\nrecognition model by inserting vector quantization into the encoder. Based on\nthe tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice,\nwhich consists of an LLM for text-to-token generation and a conditional flow\nmatching model for token-to-speech synthesis. Experimental results show that\nsupervised semantic tokens significantly outperform existing unsupervised\ntokens in terms of content consistency and speaker similarity for zero-shot\nvoice cloning. Moreover, we find that utilizing large-scale data further\nimproves the synthesis performance, indicating the scalable capacity of\nCosyVoice. To the best of our knowledge, this is the first attempt to involve\nsupervised speech tokens into TTS models.\n', '  The zero-shot text-to-speech (TTS) method, based on speaker embeddings\nextracted from reference speech using self-supervised learning (SSL) speech\nrepresentations, can reproduce speaker characteristics very accurately.\nHowever, this approach suffers from degradation in speech synthesis quality\nwhen the reference speech contains noise. In this paper, we propose a\nnoise-robust zero-shot TTS method. We incorporated adapters into the SSL model,\nwhich we fine-tuned with the TTS model using noisy reference speech. In\naddition, to further improve performance, we adopted a speech enhancement (SE)\nfront-end. With these improvements, our proposed SSL-based zero-shot TTS\nachieved high-quality speech synthesis with noisy reference speech. Through the\nobjective and subjective evaluations, we confirmed that the proposed method is\nhighly robust to noise in reference speech, and effectively works in\ncombination with SE.\n']",Text-to-Speech Synthesis Models,Speech and Language Processing,Speech and Audio Processing,Speech and Audio Processing
298,27,298_graphs_graph_beta_diffusion_graph_graphebm,"['graphs', 'graph_beta_diffusion', 'graph', 'graphebm', 'nodes', 'diffusion', 'graphaf', 'generative', 'edge', 'models']","['graph', 'graphs', 'diffusion', 'generation', 'hyperbolic', 'molecular', 'generative', 'permutation', 'latent', 'discrete']","['  Graph is a prevalent discrete data structure, whose generation has wide\napplications such as drug discovery and circuit design. Diffusion generative\nmodels, as an emerging research focus, have been applied to graph generation\ntasks. Overall, according to the space of states and time steps, diffusion\ngenerative models can be categorized into discrete-/continuous-state\ndiscrete-/continuous-time fashions. In this paper, we formulate the graph\ndiffusion generation in a discrete-state continuous-time setting, which has\nnever been studied in previous graph diffusion models. The rationale of such a\nformulation is to preserve the discrete nature of graph-structured data and\nmeanwhile provide flexible sampling trade-offs between sample quality and\nefficiency. Analysis shows that our training objective is closely related to\ngeneration quality, and our proposed generation framework enjoys ideal\ninvariant/equivariant properties concerning the permutation of node ordering.\nOur proposed model shows competitive empirical performance against\nstate-of-the-art graph generation solutions on various benchmarks and, at the\nsame time, can flexibly trade off the generation quality and efficiency in the\nsampling phase.\n', '  Generation of graphs is a major challenge for real-world tasks that require\nunderstanding the complex nature of their non-Euclidean structures. Although\ndiffusion models have achieved notable success in graph generation recently,\nthey are ill-suited for modeling the topological properties of graphs since\nlearning to denoise the noisy samples does not explicitly learn the graph\nstructures to be generated. To tackle this limitation, we propose a generative\nframework that models the topology of graphs by explicitly learning the final\ngraph structures of the diffusion process. Specifically, we design the\ngenerative process as a mixture of endpoint-conditioned diffusion processes\nwhich is driven toward the predicted graph that results in rapid convergence.\nWe further introduce a simple parameterization of the mixture process and\ndevelop an objective for learning the final graph structure, which enables\nmaximum likelihood training. Through extensive experimental validation on\ngeneral graph and 2D/3D molecule generation tasks, we show that our method\noutperforms previous generative models, generating graphs with correct topology\nwith both continuous (e.g. 3D coordinates) and discrete (e.g. atom types)\nfeatures. Our code is available at https://github.com/harryjo97/GruM.\n', '  Diffusion generative models (DMs) have achieved promising results in image\nand graph generation. However, real-world graphs, such as social networks,\nmolecular graphs, and traffic graphs, generally share non-Euclidean topologies\nand hidden hierarchies. For example, the degree distributions of graphs are\nmostly power-law distributions. The current latent diffusion model embeds the\nhierarchical data in a Euclidean space, which leads to distortions and\ninterferes with modeling the distribution. Instead, hyperbolic space has been\nfound to be more suitable for capturing complex hierarchical structures due to\nits exponential growth property. In order to simultaneously utilize the data\ngeneration capabilities of diffusion models and the ability of hyperbolic\nembeddings to extract latent hierarchical distributions, we propose a novel\ngraph generation method called, Hyperbolic Graph Diffusion Model (HGDM), which\nconsists of an auto-encoder to encode nodes into successive hyperbolic\nembeddings, and a DM that operates in the hyperbolic latent space. HGDM\ncaptures the crucial graph structure distributions by constructing a hyperbolic\npotential node space that incorporates edge information. Extensive experiments\nshow that HGDM achieves better performance in generic graph and molecule\ngeneration benchmarks, with a $48\\%$ improvement in the quality of graph\ngeneration with highly hierarchical structures.\n']",Graph Generation with Diffusion Models,"Diffusion Models for Text, Image, and Graph Generation",Diffusion Models for Generative Tasks,Diffusion Models for Generative Tasks
299,27,299_quadrotor_quadrotors_drones_drone,"['quadrotor', 'quadrotors', 'drones', 'drone', 'controllers', 'aerial', 'multirotor', 'aerodynamic', 'trajectory', 'robotics']","['quadrotor', 'aerodynamic', 'control', 'tracking', 'flight', 'autonomous', 'quadrotors', 'controller', 'simulation', 'reinforcement']","['  Learning-based methods, particularly Reinforcement Learning (RL), hold great\npromise for streamlining deployment, enhancing performance, and achieving\ngeneralization in the control of autonomous multirotor aerial vehicles. Deep RL\nhas been able to control complex systems with impressive fidelity and agility\nin simulation but the simulation-to-reality transfer often brings a\nhard-to-bridge reality gap. Moreover, RL is commonly plagued by prohibitively\nlong training times. In this work, we propose a novel asymmetric\nactor-critic-based architecture coupled with a highly reliable RL-based\ntraining paradigm for end-to-end quadrotor control. We show how curriculum\nlearning and a highly optimized simulator enhance sample complexity and lead to\nfast training times. To precisely discuss the challenges related to\nlow-level/end-to-end multirotor control, we also introduce a taxonomy that\nclassifies the existing levels of control abstractions as well as\nnon-linearities and domain parameters. Our framework enables\nSimulation-to-Reality (Sim2Real) transfer for direct RPM control after only 18\nseconds of training on a consumer-grade laptop as well as its deployment on\nmicrocontrollers to control a multirotor under real-time guarantees. Finally,\nour solution exhibits competitive performance in trajectory tracking, as\ndemonstrated through various experimental comparisons with existing\nstate-of-the-art control solutions using a real Crazyflie nano quadrotor. We\nopen source the code including a very fast multirotor dynamics simulator that\ncan simulate about 5 months of flight per second on a laptop GPU. The fast\ntraining times and deployment to a cheap, off-the-shelf quadrotor lower the\nbarriers to entry and help democratize the research and development of these\nsystems.\n', '  Ensuring the reliability and validity of data-driven quadrotor model\npredictions is essential for their accepted and practical use. This is\nespecially true for grey- and black-box models wherein the mapping of inputs to\npredictions is not transparent and subsequent reliability notoriously difficult\nto ascertain. Nonetheless, such techniques are frequently and successfully used\nto identify quadrotor models. Prediction intervals (PIs) may be employed to\nprovide insight into the consistency and accuracy of model predictions. This\npaper estimates such PIs for polynomial and Artificial Neural Network (ANN)\nquadrotor aerodynamic models. Two existing ANN PI estimation techniques - the\nbootstrap method and the quality driven method - are validated numerically for\nquadrotor aerodynamic models using an existing high-fidelity quadrotor\nsimulation. Quadrotor aerodynamic models are then identified on real quadrotor\nflight data to demonstrate their utility and explore their sensitivity to model\ninterpolation and extrapolation. It is found that the ANN-based PIs widen\nconsiderably when extrapolating and remain constant, or shrink, when\ninterpolating. While this behaviour also occurs for the polynomial PIs, it is\nof lower magnitude. The estimated PIs establish probabilistic bounds within\nwhich the quadrotor model outputs will likely lie, subject to modelling and\nmeasurement uncertainties that are reflected through the PI widths.\n', '  Motivated by the increasing use of quadrotors for payload delivery, we\nconsider a joint trajectory generation and feedback control design problem for\na quadrotor experiencing aerodynamic wrenches. Unmodeled aerodynamic drag\nforces from carried payloads can lead to catastrophic outcomes. Prior work\nmodel aerodynamic effects as residual dynamics or external disturbances in the\ncontrol problem leading to a reactive policy that could be catastrophic.\nMoreover, redesigning controllers and tuning control gains on hardware\nplatforms is a laborious effort. In this paper, we argue that adapting the\ntrajectory generation component keeping the controller fixed can improve\ntrajectory tracking for quadrotor systems experiencing drag forces. To achieve\nthis, we formulate a drag-aware planning problem by applying a suitable\nrelaxation to an optimal quadrotor control problem, introducing a tracking cost\nfunction which measures the ability of a controller to follow a reference\ntrajectory. This tracking cost function acts as a regularizer in trajectory\ngeneration and is learned from data obtained from simulation. Our experiments\nin both simulation and on the Crazyflie hardware platform show that changing\nthe planner reduces tracking error by as much as 83%. Evaluation on hardware\ndemonstrates that our planned path, as opposed to a baseline, avoids controller\nsaturation and catastrophic outcomes during aggressive maneuvers.\n']",Quadrotor Control and Aerodynamics,Unmanned Aerial Vehicle (UAV) Systems and Technologies,Aerial Robotics and Agricultural Computer Vision,Aerial Robotics and Agricultural Computer Vision
300,27,300_audioset_audiovisual_audio_visual,"['audioset', 'audiovisual', 'audio', 'visual', 'videos', 'scenes', 'supervised', 'auditory', 'embeddings', 'encoder']","['audio', 'visual', 'video', 'sound', 'localization', 'colorization', 'semantic', 'audiovisual', 'cues', 'synchronization']","['  Audio-Visual Segmentation (AVS) aims to identify, at the pixel level, the\nobject in a visual scene that produces a given sound. Current AVS methods rely\non costly fine-grained annotations of mask-audio pairs, making them impractical\nfor scalability. To address this, we introduce unsupervised AVS, eliminating\nthe need for such expensive annotation. To tackle this more challenging\nproblem, we propose an unsupervised learning method, named Modality\nCorrespondence Alignment (MoCA), which seamlessly integrates off-the-shelf\nfoundation models like DINO, SAM, and ImageBind. This approach leverages their\nknowledge complementarity and optimizes their joint usage for multi-modality\nassociation. Initially, we estimate positive and negative image pairs in the\nfeature space. For pixel-level association, we introduce an audio-visual\nadapter and a novel pixel matching aggregation strategy within the image-level\ncontrastive learning framework. This allows for a flexible connection between\nobject appearance and audio signal at the pixel level, with tolerance to\nimaging variations such as translation and rotation. Extensive experiments on\nthe AVSBench (single and multi-object splits) and AVSS datasets demonstrate\nthat our MoCA outperforms strongly designed baseline methods and approaches\nsupervised counterparts, particularly in complex scenarios with multiple\nauditory objects. Notably when comparing mIoU, MoCA achieves a substantial\nimprovement over baselines in both the AVSBench (S4: +17.24%; MS3: +67.64%) and\nAVSS (+19.23%) audio-visual segmentation challenges.\n', '  Audio-Visual Segmentation (AVS) aims to achieve pixel-level localization of\nsound sources in videos, while Audio-Visual Semantic Segmentation (AVSS), as an\nextension of AVS, further pursues semantic understanding of audio-visual\nscenes. However, since the AVSS task requires the establishment of audio-visual\ncorrespondence and semantic understanding simultaneously, we observe that\nprevious methods have struggled to handle this mashup of objectives in\nend-to-end training, resulting in insufficient learning and sub-optimization.\nTherefore, we propose a two-stage training strategy called \\textit{Stepping\nStones}, which decomposes the AVSS task into two simple subtasks from\nlocalization to semantic understanding, which are fully optimized in each stage\nto achieve step-by-step global optimization. This training strategy has also\nproved its generalization and effectiveness on existing methods. To further\nimprove the performance of AVS tasks, we propose a novel framework Adaptive\nAudio Visual Segmentation, in which we incorporate an adaptive audio query\ngenerator and integrate masked attention into the transformer decoder,\nfacilitating the adaptive fusion of visual and audio features. Extensive\nexperiments demonstrate that our methods achieve state-of-the-art results on\nall three AVS benchmarks. The project homepage can be accessed at\nhttps://gewu-lab.github.io/stepping_stones/.\n', '  Recent advances in multimodal LLMs, have led to several video-text models\nbeing proposed for critical video-related tasks. However, most of the previous\nworks support visual input only, essentially muting the audio signal in the\nvideo. Few models that support both audio and visual input, are not explicitly\ntrained on audio data. Hence, the effect of audio towards video understanding\nis largely unexplored. To this end, we propose a model architecture that\nhandles audio-visual inputs explicitly. We train our model with both audio and\nvisual data from a video instruction-tuning dataset. Comparison with\nvision-only baselines, and other audio-visual models showcase that training on\naudio data indeed leads to improved grounding of responses. For better\nevaluation of audio-visual models, we also release a human-annotated benchmark\ndataset, with audio-aware question-answer pairs.\n']",Audio-Visual Segmentation and Understanding,Audio and Speech Processing,Speech and Audio Processing,Speech and Audio Processing
301,27,301_ensemble_ensembles_ensembling_diversity,"['ensemble', 'ensembles', 'ensembling', 'diversity', 'classification', 'trained', 'fusionshot', 'fusion', 'models', 'generalization']","['ensembles', 'ensemble', 'averaging', 'diversity', 'basin', 'soups', 'weight', 'base', 'learners', 'weights']","['  This paper presents FusionShot, a focal diversity optimized few-shot ensemble\nlearning approach for boosting the robustness and generalization performance of\npre-trained few-shot models. The paper makes three original contributions.\nFirst, we explore the unique characteristics of few-shot learning to ensemble\nmultiple few-shot (FS) models by creating three alternative fusion channels.\nSecond, we introduce the concept of focal error diversity to learn the most\nefficient ensemble teaming strategy, rather than assuming that an ensemble of a\nlarger number of base models will outperform those sub-ensembles of smaller\nsize. We develop a focal-diversity ensemble pruning method to effectively prune\nout the candidate ensembles with low ensemble error diversity and recommend\ntop-$K$ FS ensembles with the highest focal error diversity. Finally, we\ncapture the complex non-linear patterns of ensemble few-shot predictions by\ndesigning the learn-to-combine algorithm, which can learn the diverse weight\nassignments for robust ensemble fusion over different member models. Extensive\nexperiments on representative few-shot benchmarks show that the top-K ensembles\nrecommended by FusionShot can outperform the representative SOTA few-shot\nmodels on novel tasks (different distributions and unknown at training), and\ncan prevail over existing few-shot learners in both cross-domain settings and\nadversarial settings. For reproducibility purposes, FusionShot trained models,\nresults, and code are made available at https://github.com/sftekin/fusionshot\n', '  The performance of deep neural networks is enhanced by ensemble methods,\nwhich average the output of several models. However, this comes at an increased\ncost at inference. Weight averaging methods aim at balancing the generalization\nof ensembling and the inference speed of a single model by averaging the\nparameters of an ensemble of models. Yet, naive averaging results in poor\nperformance as models converge to different loss basins, and aligning the\nmodels to improve the performance of the average is challenging. Alternatively,\ninspired by distributed training, methods like DART and PAPA have been proposed\nto train several models in parallel such that they will end up in the same\nbasin, resulting in good averaging accuracy. However, these methods either\ncompromise ensembling accuracy or demand significant communication between\nmodels during training. In this paper, we introduce WASH, a novel distributed\nmethod for training model ensembles for weight averaging that achieves\nstate-of-the-art image classification accuracy. WASH maintains models within\nthe same basin by randomly shuffling a small percentage of weights during\ntraining, resulting in diverse models and lower communication costs compared to\nstandard parameter averaging methods.\n', '  Classic results establish that encouraging predictive diversity improves\nperformance in ensembles of low-capacity models, e.g. through bagging or\nboosting. Here we demonstrate that these intuitions do not apply to\nhigh-capacity neural network ensembles (deep ensembles), and in fact the\nopposite is often true. In a large scale study of nearly 600 neural network\nclassification ensembles, we examine a variety of interventions that trade off\ncomponent model performance for predictive diversity. While such interventions\ncan improve the performance of small neural network ensembles (in line with\nstandard intuitions), they harm the performance of the large neural network\nensembles most often used in practice. Surprisingly, we also find that\ndiscouraging predictive diversity is often benign in large-network ensembles,\nfully inverting standard intuitions. Even when diversity-promoting\ninterventions do not sacrifice component model performance (e.g. using\nheterogeneous architectures and training paradigms), we observe an opportunity\ncost associated with pursuing increased predictive diversity. Examining over\n1000 ensembles, we observe that the performance benefits of diverse\narchitectures/training procedures are easily dwarfed by the benefits of simply\nusing higher-capacity models, despite the fact that such higher capacity models\noften yield significantly less predictive diversity. Overall, our findings\ndemonstrate that standard intuitions around predictive diversity, originally\ndeveloped for low-capacity ensembles, do not directly apply to modern\nhigh-capacity deep ensembles. This work clarifies fundamental challenges to the\ngoal of improving deep ensembles by making them more diverse, while suggesting\nan alternative path: simply forming ensembles from ever more powerful (and less\ndiverse) component models.\n']",Ensemble Methods for Deep Learning,Ensemble Methods and Classification Techniques,Machine Learning Ensembles and Multi-View Methods,Machine Learning Ensembles and Multi-View Methods
302,27,302_autoencoders_autoencoder_masking_imagenet,"['autoencoders', 'autoencoder', 'masking', 'imagenet', 'mask', 'learning', 'masked', 'supervised', 'training', 'learned']","['masking', 'curriculum', 'reconstruction', 'patches', 'supervised', 'autoencoders', 'self', 'pretext', 'downstream', 'modeling']","['  Masked Autoencoder (MAE) has demonstrated superior performance on various\nvision tasks via randomly masking image patches and reconstruction. However,\neffective data augmentation strategies for MAE still remain open questions,\ndifferent from those in contrastive learning that serve as the most important\npart. This paper studies the prevailing mixing augmentation for MAE. We first\ndemonstrate that naive mixing will in contrast degenerate model performance due\nto the increase of mutual information (MI). To address, we propose homologous\nrecognition, an auxiliary pretext task, not only to alleviate the MI\nincreasement by explicitly requiring each patch to recognize homologous\npatches, but also to perform object-aware self-supervised pre-training for\nbetter downstream dense perception performance. With extensive experiments, we\ndemonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the\nstate-of-the-art transfer results among masked image modeling (MIM)\naugmentations on different downstream tasks with significant efficiency.\nSpecifically, our MixedAE outperforms MAE by +0.3% accuracy, +1.7 mIoU and +0.9\nAP on ImageNet-1K, ADE20K and COCO respectively with a standard ViT-Base.\nMoreover, MixedAE surpasses iBOT, a strong MIM method combined with instance\ndiscrimination, while accelerating training by 2x. To our best knowledge, this\nis the very first work to consider mixing for MIM from the perspective of\npretext task design. Code will be made available.\n', '  Masked image modeling has been demonstrated as a powerful pretext task for\ngenerating robust representations that can be effectively generalized across\nmultiple downstream tasks. Typically, this approach involves randomly masking\npatches (tokens) in input images, with the masking strategy remaining unchanged\nduring training. In this paper, we propose a curriculum learning approach that\nupdates the masking strategy to continually increase the complexity of the\nself-supervised reconstruction task. We conjecture that, by gradually\nincreasing the task complexity, the model can learn more sophisticated and\ntransferable representations. To facilitate this, we introduce a novel\nlearnable masking module that possesses the capability to generate masks of\ndifferent complexities, and integrate the proposed module into masked\nautoencoders (MAE). Our module is jointly trained with the MAE, while adjusting\nits behavior during training, transitioning from a partner to the MAE\n(optimizing the same reconstruction loss) to an adversary (optimizing the\nopposite loss), while passing through a neutral state. The transition between\nthese behaviors is smooth, being regulated by a factor that is multiplied with\nthe reconstruction loss of the masking module. The resulting training procedure\ngenerates an easy-to-hard curriculum. We train our Curriculum-Learned Masked\nAutoencoder (CL-MAE) on ImageNet and show that it exhibits superior\nrepresentation learning capabilities compared to MAE. The empirical results on\nfive downstream tasks confirm our conjecture, demonstrating that curriculum\nlearning can be successfully used to self-supervise masked autoencoders. We\nrelease our code at https://github.com/ristea/cl-mae.\n', '  Masked image modeling (MIM) has been recognized as a strong self-supervised\npre-training approach in the vision domain. However, the mechanism and\nproperties of the learned representations by such a scheme, as well as how to\nfurther enhance the representations are so far not well-explored. In this\npaper, we aim to explore an interactive Masked Autoencoders (i-MAE) framework\nto enhance the representation capability from two aspects: (1) employing a\ntwo-way image reconstruction and a latent feature reconstruction with\ndistillation loss to learn better features; (2) proposing a semantics-enhanced\nsampling strategy to boost the learned semantics in MAE. Upon the proposed\ni-MAE architecture, we can address two critical questions to explore the\nbehaviors of the learned representations in MAE: (1) Whether the separability\nof latent representations in Masked Autoencoders is helpful for model\nperformance? We study it by forcing the input as a mixture of two images\ninstead of one. (2) Whether we can enhance the representations in the latent\nfeature space by controlling the degree of semantics during sampling on Masked\nAutoencoders? To this end, we propose a sampling strategy within a mini-batch\nbased on the semantics of training samples to examine this aspect. Extensive\nexperiments are conducted on CIFAR-10/100, Tiny-ImageNet and ImageNet-1K to\nverify the observations we discovered. Furthermore, in addition to\nqualitatively analyzing the characteristics of the latent representations, we\nexamine the existence of linear separability and the degree of semantics in the\nlatent space by proposing two evaluation schemes. The surprising and consistent\nresults demonstrate that i-MAE is a superior framework design for understanding\nMAE frameworks, as well as achieving better representational ability. Code is\navailable at https://github.com/vision-learning-acceleration-lab/i-mae.\n']",Masked Autoencoders for Image Representation Learning,Self-Supervised Representation Learning,Self-Supervised Representation Learning,Self-Supervised Representation Learning
303,27,303_disentangling_disentangled_disentanglement_representations,"['disentangling', 'disentangled', 'disentanglement', 'representations', 'autoencoders', 'entangled', 'generative', 'representation', 'autoencoder', 'encode']","['disentanglement', 'disentangled', 'factors', 'representation', 'latent', 'representations', 'variation', 'definitions', 'variables', 'variational']","['  Representation learning is an approach that allows to discover and extract\nthe factors of variation from the data. Intuitively, a representation is said\nto be disentangled if it separates the different factors of variation in a way\nthat is understandable to humans. Definitions of disentanglement and metrics to\nmeasure it usually assume that the factors of variation are independent of each\nother. However, this is generally false in the real world, which limits the use\nof these definitions and metrics to very specific and unrealistic scenarios. In\nthis paper we give a definition of disentanglement based on information theory\nthat is also valid when the factors of variation are not independent.\nFurthermore, we relate this definition to the Information Bottleneck Method.\nFinally, we propose a method to measure the degree of disentanglement from the\ngiven definition that works when the factors of variation are not independent.\nWe show through different experiments that the method proposed in this paper\ncorrectly measures disentanglement with non-independent factors of variation,\nwhile other methods fail in this scenario.\n', '  Current autoencoder-based disentangled representation learning methods\nachieve disentanglement by penalizing the (aggregate) posterior to encourage\nstatistical independence of the latent factors. This approach introduces a\ntrade-off between disentangled representation learning and reconstruction\nquality since the model does not have enough capacity to learn correlated\nlatent variables that capture detail information present in most image data. To\novercome this trade-off, we present a novel multi-stage modeling approach where\nthe disentangled factors are first learned using a penalty-based disentangled\nrepresentation learning method; then, the low-quality reconstruction is\nimproved with another deep generative model that is trained to model the\nmissing correlated latent variables, adding detail information while\nmaintaining conditioning on the previously learned disentangled factors. Taken\ntogether, our multi-stage modelling approach results in a single, coherent\nprobabilistic model that is theoretically justified by the principal of\nD-separation and can be realized with a variety of model classes including\nlikelihood-based models such as variational autoencoders, implicit models such\nas generative adversarial networks, and tractable models like normalizing flows\nor mixtures of Gaussians. We demonstrate that our multi-stage model has higher\nreconstruction quality than current state-of-the-art methods with equivalent\ndisentanglement performance across multiple standard benchmarks. In addition,\nwe apply the multi-stage model to generate synthetic tabular datasets,\nshowcasing an enhanced performance over benchmark models across a variety of\nmetrics. The interpretability analysis further indicates that the multi-stage\nmodel can effectively uncover distinct and meaningful features of variations\nfrom which the original distribution can be recovered.\n', '  In representation learning, a disentangled representation is highly desirable\nas it encodes generative factors of data in a separable and compact pattern.\nResearchers have advocated leveraging disentangled representations to complete\ndownstream tasks with encouraging empirical evidence. This paper further\ninvestigates the necessity of disentangled representation in downstream\napplications. Specifically, we show that dimension-wise disentangled\nrepresentations are unnecessary on a fundamental downstream task, abstract\nvisual reasoning. We provide extensive empirical evidence against the necessity\nof disentanglement, covering multiple datasets, representation learning\nmethods, and downstream network architectures. Furthermore, our findings\nsuggest that the informativeness of representations is a better indicator of\ndownstream performance than disentanglement. Finally, the positive correlation\nbetween informativeness and disentanglement explains the claimed usefulness of\ndisentangled representations in previous works. The source code is available at\nhttps://github.com/Richard-coder-Nai/disentanglement-lib-necessity.git.\n']",Disentangled Representation Learning,Self-Supervised Representation Learning,Self-Supervised Representation Learning,Self-Supervised Representation Learning
304,27,304_audiological_audio_recordings_respiratory,"['audiological', 'audio', 'recordings', 'respiratory', 'recording', 'voice', 'hearing', 'lung', 'acoustic', 'als']","['respiratory', 'sounds', 'sound', 'lung', 'audio', 'speech', 'acoustic', 'health', 'audiological', 'voice']","['  Compared with invasive examinations that require tissue sampling, respiratory\nsound testing is a non-invasive examination method that is safer and easier for\npatients to accept. In this study, we introduce Rene, a pioneering large-scale\nmodel tailored for respiratory sound recognition. Rene has been rigorously\nfine-tuned with an extensive dataset featuring a broad array of respiratory\naudio samples, targeting disease detection, sound pattern classification, and\nevent identification. Our innovative approach applies a pre-trained speech\nrecognition model to process respiratory sounds, augmented with patient medical\nrecords. The resulting multi-modal deep-learning framework addresses\ninterpretability and real-time diagnostic challenges that have hindered\nprevious respiratory-focused models. Benchmark comparisons reveal that Rene\nsignificantly outperforms existing models, achieving improvements of 10.27%,\n16.15%, 15.29%, and 18.90% in respiratory event detection and audio\nclassification on the SPRSound database. Disease prediction accuracy on the\nICBHI database improved by 23% over the baseline in both mean average and\nharmonic scores. Moreover, we have developed a real-time respiratory sound\ndiscrimination system utilizing the Rene architecture. Employing\nstate-of-the-art Edge AI technology, this system enables rapid and accurate\nresponses for respiratory sound\nauscultation(https://github.com/zpforlove/Rene).\n', '  Respiratory audio, such as coughing and breathing sounds, has predictive\npower for a wide range of healthcare applications, yet is currently\nunder-explored. The main problem for those applications arises from the\ndifficulty in collecting large labeled task-specific data for model\ndevelopment. Generalizable respiratory acoustic foundation models pretrained\nwith unlabeled data would offer appealing advantages and possibly unlock this\nimpasse. However, given the safety-critical nature of healthcare applications,\nit is pivotal to also ensure openness and replicability for any proposed\nfoundation model solution. To this end, we introduce OPERA, an OPEn Respiratory\nAcoustic foundation model pretraining and benchmarking system, as the first\napproach answering this need. We curate large-scale respiratory audio datasets\n(~136K samples, 440 hours), pretrain three pioneering foundation models, and\nbuild a benchmark consisting of 19 downstream respiratory health tasks for\nevaluation. Our pretrained models demonstrate superior performance (against\nexisting acoustic models pretrained with general audio on 16 out of 19 tasks)\nand generalizability (to unseen datasets and new respiratory audio modalities).\nThis highlights the great promise of respiratory acoustic foundation models and\nencourages more studies using OPERA as an open resource to accelerate research\non respiratory audio for health. The system is accessible from\nhttps://github.com/evelyn0414/OPERA.\n', '  This study aims to develop an auxiliary diagnostic system for classifying\nabnormal lung respiratory sounds, enhancing the accuracy of automatic abnormal\nbreath sound classification through an innovative multi-label learning approach\nand multi-head attention mechanism. Addressing the issue of class imbalance and\nlack of diversity in existing respiratory sound datasets, our study employs a\nlightweight and highly accurate model, using a two-dimensional label set to\nrepresent multiple respiratory sound characteristics. Our method achieved a\n59.2% ICBHI score in the four-category task on the ICBHI2017 dataset,\ndemonstrating its advantages in terms of lightweight and high accuracy. This\nstudy not only improves the accuracy of automatic diagnosis of lung respiratory\nsound abnormalities but also opens new possibilities for clinical applications.\n']",Respiratory Sound Analysis for Disease Detection,Audio Analysis for Health and Acoustic Applications,Audio Analysis for Health and Acoustic Applications,Audio Analysis for Health and Acoustic Applications
305,27,305_gpu_memory_gpus_sparse,"['gpu', 'memory', 'gpus', 'sparse', 'throughput', 'cpu', 'optimizer', 'hardware', 'speedup', 'efficient']","['memory', 'experts', 'tuning', 'throughput', 'adapters', 'kernels', 'parameter', 'fine', 'parameters', 'latency']","[""  Mixture-of-Experts (MoE) has emerged as a favorable architecture in the era\nof large models due to its inherent advantage, i.e., enlarging model capacity\nwithout incurring notable computational overhead. Yet, the realization of such\nbenefits often results in ineffective GPU memory utilization, as large portions\nof the model parameters remain dormant during inference. Moreover, the memory\ndemands of large models consistently outpace the memory capacity of\ncontemporary GPUs. Addressing this, we introduce SiDA-MoE\n($\\textbf{S}$parsity-$\\textbf{i}$nspired $\\textbf{D}$ata-$\\textbf{A}$ware), an\nefficient inference approach tailored for large MoE models. SiDA-MoE\njudiciously exploits both the system's main memory, which is now abundant and\nreadily scalable, and GPU memory by capitalizing on the inherent sparsity on\nexpert activation in MoE models. By adopting a data-aware perspective, SiDA-MoE\nachieves enhanced model efficiency with a neglectable performance drop.\nSpecifically, SiDA-MoE attains a remarkable speedup in MoE inference with up to\n$3.93\\times$ throughput increasing, up to $72\\%$ latency reduction, and up to\n$80\\%$ GPU memory saving with down to $1\\%$ performance drop. This work paves\nthe way for scalable and efficient deployment of large MoE models, even with\nconstrained resources. Code is available at:\nhttps://github.com/timlee0212/SiDA-MoE.\n"", ""  Large language models (LLMs) based on transformers have made significant\nstrides in recent years, the success of which is driven by scaling up their\nmodel size. Despite their high algorithmic performance, the computational and\nmemory requirements of LLMs present unprecedented challenges. To tackle the\nhigh compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture\nwas introduced which is able to scale its model size without proportionally\nscaling up its computational requirements. Unfortunately, MoE's high memory\ndemands and dynamic activation of sparse experts restrict its applicability to\nreal-world problems. Previous solutions that offload MoE's memory-hungry expert\nparameters to CPU memory fall short because the latency to migrate activated\nexperts from CPU to GPU incurs high performance overhead. Our proposed\nPre-gated MoE system effectively tackles the compute and memory challenges of\nconventional MoE architectures using our algorithm-system co-design. Pre-gated\nMoE employs our novel pre-gating function which alleviates the dynamic nature\nof sparse expert activation, allowing our proposed system to address the large\nmemory footprint of MoEs while also achieving high performance. We demonstrate\nthat Pre-gated MoE is able to improve performance, reduce GPU memory\nconsumption, while also maintaining the same level of model quality. These\nfeatures allow our Pre-gated MoE system to cost-effectively deploy large-scale\nLLMs using just a single GPU with high performance.\n"", ""  This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity\n""]",Mixture-of-Experts (MoE) Optimization for Efficient GPU Inference,Mixture of Experts (MoE) Models and Their Optimizations,Machine Learning and Optimization,Machine Learning and Artificial Intelligence
306,26,306_3d_points_supervised_lidar,"['3d', 'points', 'supervised', 'lidar', 'detr3d', 'annotations', 'scenes', 'segmentation', '2d', 'annotation']","['point', 'cloud', 'clouds', 'segmentation', 'scene', 'depth', 'object', 'labels', 'region', 'completion']","[""  Training high-accuracy 3D detectors necessitates massive labeled 3D\nannotations with 7 degree-of-freedom, which is laborious and time-consuming.\nTherefore, the form of point annotations is proposed to offer significant\nprospects for practical applications in 3D detection, which is not only more\naccessible and less expensive but also provides strong spatial information for\nobject localization. In this paper, we empirically discover that it is\nnon-trivial to merely adapt Point-DETR to its 3D form, encountering two main\nbottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it\ngenerates low-quality pseudo labels in distant regions due to the extreme\nsparsity of LiDAR points. To overcome these challenges, we introduce\nPoint-DETR3D, a teacher-student framework for weakly semi-supervised 3D\ndetection, designed to fully capitalize on point-wise supervision within a\nconstrained instance-wise annotation budget.Different from Point-DETR which\nencodes 3D positional information solely through a point encoder, we propose an\nexplicit positional query initialization strategy to enhance the positional\nprior. Considering the low quality of pseudo labels at distant regions produced\nby the teacher model, we enhance the detector's perception by incorporating\ndense imagery data through a novel Cross-Modal Deformable RoI Fusion\n(D-RoI).Moreover, an innovative point-guided self-supervised learning technique\nis proposed to allow for fully exploiting point priors, even in student\nmodels.Extensive experiments on representative nuScenes dataset demonstrate our\nPoint-DETR3D obtains significant improvements compared to previous works.\nNotably, with only 5% of labeled data, Point-DETR3D achieves over 90%\nperformance of its fully supervised counterpart.\n"", '  3D object detection plays a crucial role in various applications such as\nautonomous vehicles, robotics and augmented reality. However, training 3D\ndetectors requires a costly precise annotation, which is a hindrance to scaling\nannotation to large datasets. To address this challenge, we propose a weakly\nsupervised 3D annotator that relies solely on 2D bounding box annotations from\nimages, along with size priors. One major problem is that supervising a 3D\ndetection model using only 2D boxes is not reliable due to ambiguities between\ndifferent 3D poses and their identical 2D projection. We introduce a simple yet\neffective and generic solution: we build 3D proxy objects with annotations by\nconstruction and add them to the training dataset. Our method requires only\nsize priors to adapt to new classes. To better align 2D supervision with 3D\ndetection, our method ensures depth invariance with a novel expression of the\n2D losses. Finally, to detect more challenging instances, our annotator follows\nan offline pseudo-labelling scheme which gradually improves its 3D\npseudo-labels. Extensive experiments on the KITTI dataset demonstrate that our\nmethod not only performs on-par or above previous works on the Car category,\nbut also achieves performance close to fully supervised methods on more\nchallenging classes. We further demonstrate the effectiveness and robustness of\nour method by being the first to experiment on the more challenging nuScenes\ndataset. We additionally propose a setting where weak labels are obtained from\na 2D detector pre-trained on MS-COCO instead of human annotations.\n', '  We present a Multimodal Interlaced Transformer (MIT) that jointly considers\n2D and 3D data for weakly supervised point cloud segmentation. Research studies\nhave shown that 2D and 3D features are complementary for point cloud\nsegmentation. However, existing methods require extra 2D annotations to achieve\n2D-3D information fusion. Considering the high annotation cost of point clouds,\neffective 2D and 3D feature fusion based on weakly supervised learning is in\ngreat demand. To this end, we propose a transformer model with two encoders and\none decoder for weakly supervised point cloud segmentation using only\nscene-level class tags. Specifically, the two encoders compute the\nself-attended features for 3D point clouds and 2D multi-view images,\nrespectively. The decoder implements interlaced 2D-3D cross-attention and\ncarries out implicit 2D and 3D feature fusion. We alternately switch the roles\nof queries and key-value pairs in the decoder layers. It turns out that the 2D\nand 3D features are iteratively enriched by each other. Experiments show that\nit performs favorably against existing weakly supervised point cloud\nsegmentation methods by a large margin on the S3DIS and ScanNet benchmarks. The\nproject page will be available at https://jimmy15923.github.io/mit_web/.\n']",Weakly Supervised 3D Object Detection,Weakly Supervised Computer Vision,Computer Vision,Computer Vision
307,26,307_fusionnet_detection_images_imagery,"['fusionnet', 'detection', 'images', 'imagery', 'lidar', 'yolov5', 'yolov3', 'yolov8', 'segmentation', 'videos']","['aerial', 'adverse', 'object', 'detection', 'nighttime', 'objects', 'segmentation', 'conditions', 'small', 'backgrounds']","[""  One of the most important problems in computer vision and remote sensing is\nobject detection, which identifies particular categories of diverse things in\npictures. Two crucial data sources for public security are the thermal infrared\n(TIR) remote sensing multi-scenario photos and videos produced by unmanned\naerial vehicles (UAVs). Due to the small scale of the target, complex scene\ninformation, low resolution relative to the viewable videos, and dearth of\npublicly available labeled datasets and training models, their object detection\nprocedure is still difficult. A UAV TIR object detection framework for pictures\nand videos is suggested in this study. The Forward-looking Infrared (FLIR)\ncameras used to gather ground-based TIR photos and videos are used to create\nthe ``You Only Look Once'' (YOLO) model, which is based on CNN architecture.\nResults indicated that in the validating task, detecting human object had an\naverage precision at IOU (Intersection over Union) = 0.5, which was 72.5\\%,\nusing YOLOv7 (YOLO version 7) state of the art model \\cite{1}, while the\ndetection speed around 161 frames per second (FPS/second). The usefulness of\nthe YOLO architecture is demonstrated in the application, which evaluates the\ncross-detection performance of people in UAV TIR videos under a YOLOv7 model in\nterms of the various UAVs' observation angles. The qualitative and quantitative\nevaluation of object detection from TIR pictures and videos using deep-learning\nmodels is supported favorably by this work.\n"", '  Robust perception is crucial in autonomous vehicle navigation and\nlocalization. Visual processing tasks, like semantic segmentation, should work\nin varying weather conditions and during different times of day. Semantic\nsegmentation is where each pixel is assigned a class, which is useful for\nlocating overall features (1). Training a segmentation model requires large\namounts of data, and the labeling process for segmentation data is especially\ntedious. Additionally, many large datasets include only images taken in clear\nweather. This is a problem because training a model exclusively on clear\nweather data hinders performance in adverse weather conditions like fog or\nrain. We hypothesize that given a dataset of only clear days images, applying\nimage augmentation (such as random rain, fog, and brightness) during training\nallows for domain adaptation to diverse weather conditions. We used CARLA, a 3D\nrealistic autonomous vehicle simulator, to collect 1200 images in clear weather\ncomposed of 29 classes from 10 different towns (2). We also collected 1200\nimages of random weather effects. We trained encoder-decoder UNet models to\nperform semantic segmentation. Applying augmentations significantly improved\nsegmentation under weathered night conditions (p < 0.001). However, models\ntrained on weather data have significantly lower losses than those trained on\naugmented data in all conditions except for clear days. This shows there is\nroom for improvement in the domain adaptation approach. Future work should test\nmore types of augmentations and also use real-life images instead of CARLA.\nIdeally, the augmented model meets or exceeds the performance of the weather\nmodel.\n', '  Driving is challenging in conditions like night, rain, and snow. The lack of\ngood labeled datasets has hampered progress in scene understanding under such\nconditions. Unsupervised domain adaptation (UDA) using large labeled clear-day\ndatasets is a promising research direction in such cases. Current UDA methods,\nhowever, treat all image pixels uniformly, leading to over-reliance on the\ndominant scene backgrounds (e.g., roads, sky, sidewalks) that appear\ndramatically different across domains. As a result, they struggle to learn\neffective features of smaller and often sparse foreground objects (e.g.,\npeople, vehicles, signs).\n  In this work, we improve UDA training by using in-place image warping to\nfocus on salient object regions. Our insight is that while backgrounds vary\nsignificantly across domains (e.g., snowy night vs. clear day), object\nappearances vary to a lesser extent. Therefore, we design instance-level\nsaliency guidance to adaptively oversample object regions, which reduces\nadverse effects from background context and enhances backbone feature learning.\nWe then unwarp the better learned features while adapting from source to\ntarget. Our approach improves adaptation across geographies, lighting, and\nweather conditions, and is agnostic to the task (segmentation, detection),\ndomain adaptation algorithm, saliency guidance, and underlying model\narchitecture. Result highlights include +6.1 mAP50 for BDD100K Clear\n$\\rightarrow$ DENSE Foggy, +3.7 mAP50 for BDD100K Day $\\rightarrow$ Night, +3.0\nmAP50 for BDD100K Clear $\\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes\n$\\rightarrow$ ACDC. Our method adds minimal training memory and incurs no\nadditional inference latency. Please see Appendix for more results and\nanalysis.\n']",Object Detection in Aerial Imagery,Computer Vision Applications in Agriculture and Aerial Imagery,Aerial Robotics and Agricultural Computer Vision,Aerial Robotics and Agricultural Computer Vision
308,26,308_earthquakes_earthquake_seismic_seismicity,"['earthquakes', 'earthquake', 'seismic', 'seismicity', 'geotechnical', 'disasters', 'landslide', 'tsunami', 'ground', 'seismograms']","['earthquake', 'seismic', 'earthquakes', 'wave', 'ground', 'motion', 'magnitude', 'velocity', 'intensity', 'picking']","['  Seismograms, the fundamental seismic records, have revolutionized earthquake\nresearch and monitoring. Recent advancements in deep learning have further\nenhanced seismic signal processing, leading to even more precise and effective\nearthquake monitoring capabilities. This paper introduces a foundational deep\nlearning model, the Seismogram Transformer (SeisT), designed for a variety of\nearthquake monitoring tasks. SeisT combines multiple modules tailored to\ndifferent tasks and exhibits impressive out-of-distribution generalization\nperformance, outperforming or matching state-of-the-art models in tasks like\nearthquake detection, seismic phase picking, first-motion polarity\nclassification, magnitude estimation, back-azimuth estimation, and epicentral\ndistance estimation. The performance scores on the tasks are 0.96, 0.96, 0.68,\n0.95, 0.86, 0.55, and 0.81, respectively. The most significant improvements, in\ncomparison to existing models, are observed in phase-P picking, phase-S\npicking, and magnitude estimation, with gains of 1.7%, 9.5%, and 8.0%,\nrespectively. Our study, through rigorous experiments and evaluations, suggests\nthat SeisT has the potential to contribute to the advancement of seismic signal\nprocessing and earthquake research.\n', '  Predicting high-fidelity ground motions for future earthquakes is crucial for\nseismic hazard assessment and infrastructure resilience. Conventional empirical\nsimulations suffer from sparse sensor distribution and geographically localized\nearthquake locations, while physics-based methods are computationally intensive\nand require accurate representations of Earth structures and earthquake\nsources. We propose a novel artificial intelligence (AI) simulator, Conditional\nGenerative Modeling for Ground Motion (CGM-GM), to synthesize high-frequency\nand spatially continuous earthquake ground motion waveforms. CGM-GM leverages\nearthquake magnitudes and geographic coordinates of earthquakes and sensors as\ninputs, learning complex wave physics and Earth heterogeneities, without\nexplicit physics constraints. This is achieved through a probabilistic\nautoencoder that captures latent distributions in the time-frequency domain and\nvariational sequential models for prior and posterior distributions. We\nevaluate the performance of CGM-GM using small-magnitude earthquake records\nfrom the San Francisco Bay Area, a region with high seismic risks. CGM-GM\ndemonstrates a strong potential for outperforming a state-of-the-art\nnon-ergodic empirical ground motion model and shows great promise in seismology\nand beyond.\n', '  This article surveys the growing interest in utilizing Deep Learning (DL) as\na powerful tool to address challenging problems in earthquake engineering.\nDespite decades of advancement in domain knowledge, issues such as uncertainty\nin earthquake occurrence, unpredictable seismic loads, nonlinear structural\nresponses, and community engagement remain difficult to tackle using\ndomain-specific methods. DL offers promising solutions by leveraging its\ndata-driven capacity for nonlinear mapping, sequential data modeling, automatic\nfeature extraction, dimensionality reduction, optimal decision-making, etc.\nHowever, the literature lacks a comprehensive review that systematically covers\na consistent scope intersecting DL and earthquake engineering. To bridge the\ngap, the article first discusses methodological advances to elucidate various\napplicable DL techniques, such as multi-layer perceptron (MLP), convolutional\nneural network (CNN), recurrent neural network (RNN), generative adversarial\nnetwork (GAN), autoencoder (AE), transfer learning (TL), reinforcement learning\n(RL), and graph neural network (GNN). A thorough research landscape is then\ndisclosed by exploring various DL applications across different research\ntopics, including vision-based seismic damage assessment and structural\ncharacterization, seismic demand and damage state prediction, seismic response\nhistory prediction, regional seismic risk assessment and community resilience,\nground motion (GM) for engineering use, seismic response control, and the\ninverse problem of system/damage identification. Suitable DL techniques for\neach research topic are identified, emphasizing the preeminence of CNN for\nvision-based tasks, RNN for sequential data, RL for community resilience, and\nunsupervised learning for GM analysis. The article also discusses opportunities\nand challenges for leveraging DL in earthquake engineering research and\npractice.\n']",Earthquake Monitoring and Seismic Signal Processing,Signal Processing and Analysis in Biomedical and Geophysical Applications,Signal Processing and Analysis in Complex Environments,Signal Processing and Analysis in Complex Environments
309,26,309_tokenizers_tokenization_tokenizer_tokenisation,"['tokenizers', 'tokenization', 'tokenizer', 'tokenisation', 'tokenize', 'sentencepiece', 'morphemes', 'subwords', 'tokens', 'wordpiece']","['tokenization', 'tokenizers', 'tokenizer', 'subwords', 'morphological', 'vocabulary', 'syllable', 'tokens', 'words', 'morpheme']","['  The popular subword tokenizers of current language models, such as Byte-Pair\nEncoding (BPE), are known not to respect morpheme boundaries, which affects the\ndownstream performance of the models. While many improved tokenization\nalgorithms have been proposed, their evaluation and cross-comparison is still\nan open problem. As a solution, we propose a combined intrinsic-extrinsic\nevaluation framework for subword tokenization. Intrinsic evaluation is based on\nour new UniMorph Labeller tool that classifies subword tokenization as either\nmorphological or alien. Extrinsic evaluation, in turn, is performed via the\nOut-of-Vocabulary Generalization Challenge 1.0 benchmark, which consists of\nthree newly specified downstream text classification tasks. Our empirical\nfindings show that the accuracy of UniMorph Labeller is 98%, and that, in all\nlanguage models studied (including ALBERT, BERT, RoBERTa, and DeBERTa), alien\ntokenization leads to poorer generalizations compared to morphological\ntokenization for semantic compositionality of word meanings.\n', ""  Tokenization is a foundational step in Natural Language Processing (NLP)\ntasks, bridging raw text and language models. Existing tokenization approaches\nlike Byte-Pair Encoding (BPE) originate from the field of data compression, and\nit has been suggested that the effectiveness of BPE stems from its ability to\ncondense text into a relatively small number of tokens. We test the hypothesis\nthat fewer tokens lead to better downstream performance by introducing\nPathPiece, a new tokenizer that segments a document's text into the minimum\nnumber of tokens for a given vocabulary. Through extensive experimentation we\nfind this hypothesis not to be the case, casting doubt on the understanding of\nthe reasons for effective tokenization. To examine which other factors play a\nrole, we evaluate design decisions across all three phases of tokenization:\npre-tokenization, vocabulary construction, and segmentation, offering new\ninsights into the design of effective tokenizers. Specifically, we illustrate\nthe importance of pre-tokenization and the benefits of using BPE to initialize\nvocabulary construction. We train 64 language models with varying tokenization,\nranging in size from 350M to 2.4B parameters, all of which are made publicly\navailable.\n"", '  Subword tokenization has become the prevailing standard in the field of\nnatural language processing (NLP) over recent years, primarily due to the\nwidespread utilization of pre-trained language models. This shift began with\nByte-Pair Encoding (BPE) and was later followed by the adoption of\nSentencePiece and WordPiece. While subword tokenization consistently\noutperforms character and word-level tokenization, the precise factors\ncontributing to its success remain unclear. Key aspects such as the optimal\nsegmentation granularity for diverse tasks and languages, the influence of data\nsources on tokenizers, and the role of morphological information in\nIndo-European languages remain insufficiently explored. This is particularly\npertinent for biomedical terminology, characterized by specific rules governing\nmorpheme combinations. Despite the agglutinative nature of biomedical\nterminology, existing language models do not explicitly incorporate this\nknowledge, leading to inconsistent tokenization strategies for common terms. In\nthis paper, we seek to delve into the complexities of subword tokenization in\nFrench biomedical domain across a variety of NLP tasks and pinpoint areas where\nfurther enhancements can be made. We analyze classical tokenization algorithms,\nincluding BPE and SentencePiece, and introduce an original tokenization\nstrategy that integrates morpheme-enriched word segmentation into existing\ntokenization methods.\n']",Subword Tokenization in NLP,Natural Language Processing (NLP) Techniques and Applications,Natural Language Processing,Natural Language Processing
310,26,310_spellchecking_corpus_speller_spell,"['spellchecking', 'corpus', 'speller', 'spell', 'spelling', 'misspelled', 'pinyin', 'diacritics', 'bspell', 'phonetic']","['spelling', 'correction', 'errors', 'character', 'word', 'spell', 'phonetic', 'error', 'characters', 'correct']","['  Chinese Spelling Correction (CSC) commonly lacks large-scale high-quality\ncorpora, due to the labor-intensive labeling of spelling errors in real-life\nhuman writing or typing scenarios. Two data augmentation methods are widely\nadopted: (1) \\textit{Random Replacement} with the guidance of confusion sets\nand (2) \\textit{OCR/ASR-based Generation} that simulates character misusing.\nHowever, both methods inevitably introduce noisy data (e.g., false spelling\nerrors), potentially leading to over-correction. By carefully analyzing the two\ntypes of corpora, we find that though the latter achieves more robust\ngeneralization performance, the former yields better-calibrated CSC models. We\nthen provide a theoretical analysis of this empirical observation, based on\nwhich a corpus refining strategy is proposed. Specifically, OCR/ASR-based data\nsamples are fed into a well-calibrated CSC model trained on random\nreplacement-based corpora and then filtered based on prediction confidence. By\nlearning a simple BERT-based model on the refined OCR/ASR-based corpus, we set\nup impressive state-of-the-art performance on three widely-used benchmarks,\nwhile significantly alleviating over-correction (e.g., lowering false positive\npredictions).\n', ""  This research introduces a state-of-the-art Persian spelling correction\nsystem that seamlessly integrates deep learning techniques with phonetic\nanalysis, significantly enhancing the accuracy and efficiency of natural\nlanguage processing (NLP) for Persian. Utilizing a fine-tuned language\nrepresentation model, our methodology effectively combines deep contextual\nanalysis with phonetic insights, adeptly correcting both non-word and real-word\nspelling errors. This strategy proves particularly effective in tackling the\nunique complexities of Persian spelling, including its elaborate morphology and\nthe challenge of homophony. A thorough evaluation on a wide-ranging dataset\nconfirms our system's superior performance compared to existing methods, with\nimpressive F1-Scores of 0.890 for detecting real-word errors and 0.905 for\ncorrecting them. Additionally, the system demonstrates a strong capability in\nnon-word error correction, achieving an F1-Score of 0.891. These results\nillustrate the significant benefits of incorporating phonetic insights into\ndeep learning models for spelling correction. Our contributions not only\nadvance Persian language processing by providing a versatile solution for a\nvariety of NLP applications but also pave the way for future research in the\nfield, emphasizing the critical role of phonetic analysis in developing\neffective spelling correction system.\n"", '  Automatic spelling correction stands as a pivotal challenge within the ambit\nof natural language processing (NLP), demanding nuanced solutions. Traditional\nspelling correction techniques are typically only capable of detecting and\ncorrecting non-word errors, such as typos and misspellings. However,\ncontext-sensitive errors, also known as real-word errors, are more challenging\nto detect because they are valid words that are used incorrectly in a given\ncontext. The Persian language, characterized by its rich morphology and complex\nsyntax, presents formidable challenges to automatic spelling correction\nsystems. Furthermore, the limited availability of Persian language resources\nmakes it difficult to train effective spelling correction models. This paper\nintroduces a cutting-edge approach for precise and efficient real-word error\ncorrection in Persian text. Our methodology adopts a structured, multi-tiered\napproach, employing semantic analysis, feature selection, and advanced\nclassifiers to enhance error detection and correction efficacy. The innovative\narchitecture discovers and stores semantic similarities between words and\nphrases in Persian text. The classifiers accurately identify real-word errors,\nwhile the semantic ranking algorithm determines the most probable corrections\nfor real-word errors, taking into account specific spelling correction and\ncontext properties such as context, semantic similarity, and edit-distance\nmeasures. Evaluations have demonstrated that our proposed method surpasses\nprevious Persian real-word error correction models. Our method achieves an\nimpressive F-measure of 96.6% in the detection phase and an accuracy of 99.1%\nin the correction phase. These results clearly indicate that our approach is a\nhighly promising solution for automatic real-word error correction in Persian\ntext.\n']",Spelling Correction Techniques and Corpora,Natural Language Processing for Text Correction and Simplification,Natural Language Processing,Natural Language Processing
311,26,311_bottleneck_concepts_classification_concept,"['bottleneck', 'concepts', 'classification', 'concept', 'annotations', 'features', 'compositional_concepts', 'models', 'concept_realignment', 'representations']","['concept', 'concepts', 'bottleneck', 'understandable', 'interventions', 'interpretable', 'black', 'localities', 'intervention', 'interpretability']","['  There has been considerable recent interest in interpretable concept-based\nmodels such as Concept Bottleneck Models (CBMs), which first predict\nhuman-interpretable concepts and then map them to output classes. To reduce\nreliance on human-annotated concepts, recent works have converted pretrained\nblack-box models into interpretable CBMs post-hoc. However, these approaches\npredefine a set of concepts, assuming which concepts a black-box model encodes\nin its representations. In this work, we eliminate this assumption by\nleveraging unsupervised concept discovery to automatically extract concepts\nwithout human annotations or a predefined set of concepts. We further introduce\nan input-dependent concept selection mechanism that ensures only a small subset\nof concepts is used across all classes. We show that our approach improves\ndownstream performance and narrows the performance gap to black-box models,\nwhile using significantly fewer concepts in the classification. Finally, we\ndemonstrate how large vision-language models can intervene on the final model\nweights to correct model errors.\n', ""  Concept Bottleneck Models (CBMs) are regarded as inherently interpretable\nbecause they first predict a set of human-defined concepts which are used to\npredict a task label. For inherent interpretability to be fully realised, and\nensure trust in a model's output, it's desirable for concept predictions to use\nsemantically meaningful input features. For instance, in an image, pixels\nrepresenting a broken bone should contribute to predicting a fracture. However,\ncurrent literature suggests that concept predictions often rely on irrelevant\ninput features. We hypothesise that this occurs when dataset labels include\ninaccurate concept annotations, or the relationship between input features and\nconcepts is unclear. In general, the effect of dataset labelling on concept\nrepresentations remains an understudied area. In this paper, we demonstrate\nthat CBMs can learn to map concepts to semantically meaningful input features,\nby utilising datasets with a clear link between the input features and the\ndesired concept predictions. This is achieved, for instance, by ensuring\nmultiple concepts do not always co-occur and, therefore provide a clear\ntraining signal for the CBM to distinguish the relevant input features for each\nconcept. We validate our hypothesis on both synthetic and real-world image\ndatasets, and demonstrate under the correct conditions, CBMs can learn to\nattribute semantically meaningful input features to the correct concept\npredictions.\n"", '  Concept Bottleneck Models (CBMs) map the black-box visual representations\nextracted by deep neural networks onto a set of interpretable concepts and use\nthe concepts to make predictions, enhancing the transparency of the\ndecision-making process. Multimodal pre-trained models can match visual\nrepresentations with textual concept embeddings, allowing for obtaining the\ninterpretable concept bottleneck without the expertise concept annotations.\nRecent research has focused on the concept bank establishment and the\nhigh-quality concept selection. However, it is challenging to construct a\ncomprehensive concept bank through humans or large language models, which\nseverely limits the performance of CBMs. In this work, we propose the\nIncremental Residual Concept Bottleneck Model (Res-CBM) to address the\nchallenge of concept completeness. Specifically, the residual concept\nbottleneck model employs a set of optimizable vectors to complete missing\nconcepts, then the incremental concept discovery module converts the\ncomplemented vectors with unclear meanings into potential concepts in the\ncandidate concept bank. Our approach can be applied to any user-defined concept\nbank, as a post-hoc processing method to enhance the performance of any CBMs.\nFurthermore, to measure the descriptive efficiency of CBMs, the Concept\nUtilization Efficiency (CUE) metric is proposed. Experiments show that the\nRes-CBM outperforms the current state-of-the-art methods in terms of both\naccuracy and efficiency and achieves comparable performance to black-box models\nacross multiple datasets.\n']",Concept Bottleneck Models for Interpretable Classification,Interpretable Deep Learning Models,Explainable Artificial Intelligence,Artificial Intelligence and Machine Learning Interpretability and Explainability
312,26,312_views_view_clustering_cluster,"['views', 'view', 'clustering', 'cluster', 'clusters', 'embedding', 'mvcan', 'representations', 'feature', 'learns']","['clustering', 'view', 'views', 'anchor', 'subspace', 'consensus', 'multi', 'tensor', 'matrix', 'incomplete']","['  Multi-view clustering has attracted growing attention owing to its\ncapabilities of aggregating information from various sources and its promising\nhorizons in public affairs. Up till now, many advanced approaches have been\nproposed in recent literature. However, there are several ongoing difficulties\nto be tackled. One common dilemma occurs while attempting to align the features\nof different views. {Moreover, due to the fact that many existing multi-view\nclustering algorithms stem from spectral clustering, this results to cubic time\ncomplexity w.r.t. the number of dataset. However, we propose Anchor-based\nMulti-view Subspace Clustering with Hierarchical Feature Descent(MVSC-HFD) to\ntackle the discrepancy among views through hierarchical feature descent and\nproject to a common subspace( STAGE 1), which reveals dependency of different\nviews. We further reduce the computational complexity to linear time cost\nthrough a unified sampling strategy in the common subspace( STAGE 2), followed\nby anchor-based subspace clustering to learn the bipartite graph collectively(\nSTAGE 3). }Extensive experimental results on public benchmark datasets\ndemonstrate that our proposed model consistently outperforms the\nstate-of-the-art techniques.\n', '  Multi-view clustering has become a significant area of research, with\nnumerous methods proposed over the past decades to enhance clustering accuracy.\nHowever, in many real-world applications, it is crucial to demonstrate a clear\ndecision-making process-specifically, explaining why samples are assigned to\nparticular clusters. Consequently, there remains a notable gap in developing\ninterpretable methods for clustering multi-view data. To fill this crucial gap,\nwe make the first attempt towards this direction by introducing an\ninterpretable multi-view clustering framework. Our method begins by extracting\nembedded features from each view and generates pseudo-labels to guide the\ninitial construction of the decision tree. Subsequently, it iteratively\noptimizes the feature representation for each view along with refining the\ninterpretable decision tree. Experimental results on real datasets demonstrate\nthat our method not only provides a transparent clustering process for\nmulti-view data but also delivers performance comparable to state-of-the-art\nmulti-view clustering methods. To the best of our knowledge, this is the first\neffort to design an interpretable clustering framework specifically for\nmulti-view data, opening a new avenue in this field.\n', '  In this paper, we propose a novel multi-view clustering model, named\nDual-space Co-training Large-scale Multi-view Clustering (DSCMC). The main\nobjective of our approach is to enhance the clustering performance by\nleveraging co-training in two distinct spaces. In the original space, we learn\na projection matrix to obtain latent consistent anchor graphs from different\nviews. This process involves capturing the inherent relationships and\nstructures between data points within each view. Concurrently, we employ a\nfeature transformation matrix to map samples from various views to a shared\nlatent space. This transformation facilitates the alignment of information from\nmultiple views, enabling a comprehensive understanding of the underlying data\ndistribution. We jointly optimize the construction of the latent consistent\nanchor graph and the feature transformation to generate a discriminative anchor\ngraph. This anchor graph effectively captures the essential characteristics of\nthe multi-view data and serves as a reliable basis for subsequent clustering\nanalysis. Moreover, the element-wise method is proposed to avoid the impact of\ndiverse information between different views. Our algorithm has an approximate\nlinear computational complexity, which guarantees its successful application on\nlarge-scale datasets. Through experimental validation, we demonstrate that our\nmethod significantly reduces computational complexity while yielding superior\nclustering performance compared to existing approaches.\n']",Multi-View Clustering Methods,Multi-View Learning and Ensemble Methods,Machine Learning Ensembles and Multi-View Methods,Machine Learning Ensembles and Multi-View Methods
313,26,313_benchmarking_evaluations_language_benchmarks,"['benchmarking', 'evaluations', 'language', 'benchmarks', 'benchmark', 'evaluation', 'assessment', 'vocabulary', 'evaluating', 'tests']","['evaluation', 'turn', 'benchmarks', 'benchmark', 'instruction', 'leaderboard', 'capabilities', 'evaluations', 'tests', 'assessment']","['  Recent advancements in Large Language Models (LLMs) have sparked interest in\ntheir potential applications across various fields. This paper embarked on a\npivotal inquiry: Can existing LLMs effectively serve as ""water expert models""\nfor water engineering and research tasks? This study was the first to evaluate\nLLMs\' contributions across various water engineering and research tasks by\nestablishing a domain-specific benchmark suite, namely, WaterER. Herein, we\nprepared 983 tasks related to water engineering and research, categorized into\n""wastewater treatment"", ""environmental restoration"", ""drinking water treatment\nand distribution"", ""sanitation"", ""anaerobic digestion"" and ""contaminants\nassessment"". We evaluated the performance of seven LLMs (i.e., GPT-4, GPT-3.5,\nGemini, GLM-4, ERNIE, QWEN and Llama3) on these tasks. We highlighted the\nstrengths of GPT-4 in handling diverse and complex tasks of water engineering\nand water research, the specialized capabilities of Gemini in academic\ncontexts, Llama3\'s strongest capacity to answer Chinese water engineering\nquestions and the competitive performance of Chinese-oriented models like\nGLM-4, ERNIE and QWEN in some water engineering tasks. More specifically,\ncurrent LLMs excelled particularly in generating precise research gaps for\npapers on ""contaminants and related water quality monitoring and assessment"".\nAdditionally, they were more adept at creating appropriate titles for research\npapers on ""treatment processes for wastewaters"", ""environmental restoration"",\nand ""drinking water treatment"". Overall, this study pioneered evaluating LLMs\nin water engineering and research by introducing the WaterER benchmark to\nassess the trustworthiness of their predictions. This standardized evaluation\nframework would also drive future advancements in LLM technology by using\ntargeting datasets, propelling these models towards becoming true ""water\nexpert"".\n', ""  Large language models (LLMs) are increasingly relied upon for complex\nmulti-turn conversations across diverse real-world applications. However,\nexisting benchmarks predominantly focus on single-turn evaluations, overlooking\nthe models' capabilities in multi-turn interactions. To address this gap, we\nintroduce MT-Eval, a comprehensive benchmark designed to evaluate multi-turn\nconversational abilities. By analyzing human-LLM conversations, we categorize\ninteraction patterns into four types: recollection, expansion, refinement, and\nfollow-up. We construct multi-turn queries for each category either by\naugmenting existing datasets or by creating new examples with GPT-4 to avoid\ndata leakage. To study the factors impacting multi-turn abilities, we create\nsingle-turn versions of the 1170 multi-turn queries and compare performance.\nOur evaluation of 11 well-known LLMs shows that while closed-source models\ngenerally surpass open-source ones, certain open-source models exceed\nGPT-3.5-Turbo in specific tasks. We observe significant performance degradation\nin multi-turn settings compared to single-turn settings in most models, which\nis not correlated with the models' fundamental capabilities. Moreover, we\nidentify the distance to relevant content and susceptibility to error\npropagation as the key factors influencing multi-turn performance. MT-Eval is\nreleased publicly to encourage future research towards more robust\nconversational models.\n"", ""  Recent advancements in Language Models (LMs) have catalyzed the creation of\nmultiple benchmarks, designed to assess these models' general capabilities. A\ncrucial task, however, is assessing the validity of the benchmarks themselves.\nThis is most commonly done via Benchmark Agreement Testing (BAT), where new\nbenchmarks are validated against established ones using some agreement metric\n(e.g., rank correlation). Despite the crucial role of BAT for benchmark\nbuilders and consumers, there are no standardized procedures for such agreement\ntesting. This deficiency can lead to invalid conclusions, fostering mistrust in\nbenchmarks and upending the ability to properly choose the appropriate\nbenchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how\nsome overlooked methodological choices can significantly influence BAT results,\npotentially undermining the validity of conclusions. To address these\ninconsistencies, we propose a set of best practices for BAT and demonstrate how\nutilizing these methodologies greatly improves BAT robustness and validity. To\nfoster adoption and facilitate future research,, we introduce BenchBench, a\npython package for BAT, and release the BenchBench-leaderboard, a\nmeta-benchmark designed to evaluate benchmarks using their peers. Our findings\nunderscore the necessity for standardized BAT, ensuring the robustness and\nvalidity of benchmark evaluations in the evolving landscape of language model\nresearch.\n  BenchBench Package: https://github.com/IBM/BenchBench\n  Leaderboard: https://huggingface.co/spaces/per/BenchBench\n""]",Evaluating Language Models with Benchmarks,Evaluating Large Language Models,Large Language Models,Large Language Models
313,26,313_benchmarking_evaluations_language_benchmarks,"['benchmarking', 'evaluations', 'language', 'benchmarks', 'benchmark', 'evaluation', 'assessment', 'vocabulary', 'evaluating', 'tests']","['evaluation', 'turn', 'benchmarks', 'benchmark', 'instruction', 'leaderboard', 'capabilities', 'evaluations', 'tests', 'assessment']","['  Recent advancements in Large Language Models (LLMs) have sparked interest in\ntheir potential applications across various fields. This paper embarked on a\npivotal inquiry: Can existing LLMs effectively serve as ""water expert models""\nfor water engineering and research tasks? This study was the first to evaluate\nLLMs\' contributions across various water engineering and research tasks by\nestablishing a domain-specific benchmark suite, namely, WaterER. Herein, we\nprepared 983 tasks related to water engineering and research, categorized into\n""wastewater treatment"", ""environmental restoration"", ""drinking water treatment\nand distribution"", ""sanitation"", ""anaerobic digestion"" and ""contaminants\nassessment"". We evaluated the performance of seven LLMs (i.e., GPT-4, GPT-3.5,\nGemini, GLM-4, ERNIE, QWEN and Llama3) on these tasks. We highlighted the\nstrengths of GPT-4 in handling diverse and complex tasks of water engineering\nand water research, the specialized capabilities of Gemini in academic\ncontexts, Llama3\'s strongest capacity to answer Chinese water engineering\nquestions and the competitive performance of Chinese-oriented models like\nGLM-4, ERNIE and QWEN in some water engineering tasks. More specifically,\ncurrent LLMs excelled particularly in generating precise research gaps for\npapers on ""contaminants and related water quality monitoring and assessment"".\nAdditionally, they were more adept at creating appropriate titles for research\npapers on ""treatment processes for wastewaters"", ""environmental restoration"",\nand ""drinking water treatment"". Overall, this study pioneered evaluating LLMs\nin water engineering and research by introducing the WaterER benchmark to\nassess the trustworthiness of their predictions. This standardized evaluation\nframework would also drive future advancements in LLM technology by using\ntargeting datasets, propelling these models towards becoming true ""water\nexpert"".\n', ""  Large language models (LLMs) are increasingly relied upon for complex\nmulti-turn conversations across diverse real-world applications. However,\nexisting benchmarks predominantly focus on single-turn evaluations, overlooking\nthe models' capabilities in multi-turn interactions. To address this gap, we\nintroduce MT-Eval, a comprehensive benchmark designed to evaluate multi-turn\nconversational abilities. By analyzing human-LLM conversations, we categorize\ninteraction patterns into four types: recollection, expansion, refinement, and\nfollow-up. We construct multi-turn queries for each category either by\naugmenting existing datasets or by creating new examples with GPT-4 to avoid\ndata leakage. To study the factors impacting multi-turn abilities, we create\nsingle-turn versions of the 1170 multi-turn queries and compare performance.\nOur evaluation of 11 well-known LLMs shows that while closed-source models\ngenerally surpass open-source ones, certain open-source models exceed\nGPT-3.5-Turbo in specific tasks. We observe significant performance degradation\nin multi-turn settings compared to single-turn settings in most models, which\nis not correlated with the models' fundamental capabilities. Moreover, we\nidentify the distance to relevant content and susceptibility to error\npropagation as the key factors influencing multi-turn performance. MT-Eval is\nreleased publicly to encourage future research towards more robust\nconversational models.\n"", ""  Recent advancements in Language Models (LMs) have catalyzed the creation of\nmultiple benchmarks, designed to assess these models' general capabilities. A\ncrucial task, however, is assessing the validity of the benchmarks themselves.\nThis is most commonly done via Benchmark Agreement Testing (BAT), where new\nbenchmarks are validated against established ones using some agreement metric\n(e.g., rank correlation). Despite the crucial role of BAT for benchmark\nbuilders and consumers, there are no standardized procedures for such agreement\ntesting. This deficiency can lead to invalid conclusions, fostering mistrust in\nbenchmarks and upending the ability to properly choose the appropriate\nbenchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how\nsome overlooked methodological choices can significantly influence BAT results,\npotentially undermining the validity of conclusions. To address these\ninconsistencies, we propose a set of best practices for BAT and demonstrate how\nutilizing these methodologies greatly improves BAT robustness and validity. To\nfoster adoption and facilitate future research,, we introduce BenchBench, a\npython package for BAT, and release the BenchBench-leaderboard, a\nmeta-benchmark designed to evaluate benchmarks using their peers. Our findings\nunderscore the necessity for standardized BAT, ensuring the robustness and\nvalidity of benchmark evaluations in the evolving landscape of language model\nresearch.\n  BenchBench Package: https://github.com/IBM/BenchBench\n  Leaderboard: https://huggingface.co/spaces/per/BenchBench\n""]",Evaluating Language Models with Benchmarks,Evaluating Large Language Models,Large Language Models,Large Language Models
314,26,314_bayesian_probabilistic_inference_bayes,"['bayesian', 'probabilistic', 'inference', 'bayes', 'graphs', 'posterior', 'prior', 'learning', 'likelihood', 'causal']","['graphical', 'posterior', 'structure', 'variables', 'directed', 'acyclic', 'likelihood', 'marginal', 'conditional', 'independence']","['  Gaussian Process Networks (GPNs) are a class of directed graphical models\nwhich employ Gaussian processes as priors for the conditional expectation of\neach variable given its parents in the network. The model allows the\ndescription of continuous joint distributions in a compact but flexible manner\nwith minimal parametric assumptions on the dependencies between variables.\nBayesian structure learning of GPNs requires computing the posterior over\ngraphs of the network and is computationally infeasible even in low dimensions.\nThis work implements Monte Carlo and Markov Chain Monte Carlo methods to sample\nfrom the posterior distribution of network structures. As such, the approach\nfollows the Bayesian paradigm, comparing models via their marginal likelihood\nand computing the posterior probability of the GPN features. Simulation studies\nshow that our method outperforms state-of-the-art algorithms in recovering the\ngraphical structure of the network and provides an accurate approximation of\nits posterior distribution.\n', '  Bayesian causal structure learning aims to learn a posterior distribution\nover directed acyclic graphs (DAGs), and the mechanisms that define the\nrelationship between parent and child variables. By taking a Bayesian approach,\nit is possible to reason about the uncertainty of the causal model. The notion\nof modelling the uncertainty over models is particularly crucial for causal\nstructure learning since the model could be unidentifiable when given only a\nfinite amount of observational data. In this paper, we introduce a novel method\nto jointly learn the structure and mechanisms of the causal model using\nVariational Bayes, which we call Variational Bayes-DAG-GFlowNet (VBG). We\nextend the method of Bayesian causal structure learning using GFlowNets to\nlearn not only the posterior distribution over the structure, but also the\nparameters of a linear-Gaussian model. Our results on simulated data suggest\nthat VBG is competitive against several baselines in modelling the posterior\nover DAGs and mechanisms, while offering several advantages over existing\nmethods, including the guarantee to sample acyclic graphs, and the flexibility\nto generalize to non-linear causal mechanisms.\n', '  Estimating the structure of a Bayesian network, in the form of a directed\nacyclic graph (DAG), from observational data is a statistically and\ncomputationally hard problem with essential applications in areas such as\ncausal discovery. Bayesian approaches are a promising direction for solving\nthis task, as they allow for uncertainty quantification and deal with\nwell-known identifiability issues. From a probabilistic inference perspective,\nthe main challenges are (i) representing distributions over graphs that satisfy\nthe DAG constraint and (ii) estimating a posterior over the underlying\ncombinatorial space. We propose an approach that addresses these challenges by\nformulating a joint distribution on an augmented space of DAGs and\npermutations. We carry out posterior estimation via variational inference,\nwhere we exploit continuous relaxations of discrete distributions. We show that\nour approach performs competitively when compared with a wide range of Bayesian\nand non-Bayesian benchmarks on a range of synthetic and real datasets.\n']",Bayesian Probabilistic Graphical Models,Probabilistic Modeling and Inference,Probabilistic Machine Learning,Probabilistic Machine Learning
315,26,315_tweets_crisistransformers_twitter_microblogs,"['tweets', 'crisistransformers', 'twitter', 'microblogs', 'disasters', 'crises', 'tweet', 'disaster', 'crisisfacts', 'textual']","['disaster', 'crisis', 'media', 'social', 'events', 'event', 'informatics', 'emergencies', 'disasters', 'emergency']","['  Online social media platforms, such as Twitter, provide valuable information\nduring disaster events. Existing tweet disaster summarization approaches\nprovide a summary of these events to aid government agencies, humanitarian\norganizations, etc., to ensure effective disaster response. In the literature,\nthere are two types of approaches for disaster summarization, namely,\nsupervised and unsupervised approaches. Although supervised approaches are\ntypically more effective, they necessitate a sizable number of disaster event\nsummaries for testing and training. However, there is a lack of good number of\ndisaster summary datasets for training and evaluation. This motivates us to add\nmore datasets to make supervised learning approaches more efficient. In this\npaper, we present ADSumm, which adds annotated ground-truth summaries for eight\ndisaster events which consist of both natural and man-made disaster events\nbelonging to seven different countries. Our experimental analysis shows that\nthe newly added datasets improve the performance of the supervised\nsummarization approaches by 8-28% in terms of ROUGE-N F1-score. Moreover, in\nnewly annotated dataset, we have added a category label for each input tweet\nwhich helps to ensure good coverage from different categories in summary.\nAdditionally, we have added two other features relevance label and key-phrase,\nwhich provide information about the quality of a tweet and explanation about\nthe inclusion of the tweet into summary, respectively. For ground-truth summary\ncreation, we provide the annotation procedure adapted in detail, which has not\nbeen described in existing literature. Experimental analysis shows the quality\nof ground-truth summary is very good with Coverage, Relevance and Diversity.\n', '  In the field of crisis/disaster informatics, social media is increasingly\nbeing used for improving situational awareness to inform response and relief\nefforts. Efficient and accurate text classification tools have been a focal\narea of investigation in crisis informatics. However, current methods mostly\nrely on single-label text classification models, which fails to capture\ndifferent insights embedded in dynamic and multifaceted disaster-related social\nmedia data. This study introduces a novel approach to disaster text\nclassification by enhancing a pre-trained Large Language Model (LLM) through\ninstruction fine-tuning targeted for multi-label classification of\ndisaster-related tweets. Our methodology involves creating a comprehensive\ninstruction dataset from disaster-related tweets, which is then used to\nfine-tune an open-source LLM, thereby embedding it with disaster-specific\nknowledge. This fine-tuned model can classify multiple aspects of\ndisaster-related information simultaneously, such as the type of event,\ninformativeness, and involvement of human aid, significantly improving the\nutility of social media data for situational awareness in disasters. The\nresults demonstrate that this approach enhances the categorization of critical\ninformation from social media posts, thereby facilitating a more effective\ndeployment for situational awareness during emergencies. This research paves\nthe way for more advanced, adaptable, and robust disaster management tools,\nleveraging the capabilities of LLMs to improve real-time situational awareness\nand response strategies in disaster scenarios.\n', '  Social media platforms play an essential role in crisis communication, but\nanalyzing crisis-related social media texts is challenging due to their\ninformal nature. Transformer-based pre-trained models like BERT and RoBERTa\nhave shown success in various NLP tasks, but they are not tailored for\ncrisis-related texts. Furthermore, general-purpose sentence encoders are used\nto generate sentence embeddings, regardless of the textual complexities in\ncrisis-related texts. Advances in applications like text classification,\nsemantic search, and clustering contribute to the effective processing of\ncrisis-related texts, which is essential for emergency responders to gain a\ncomprehensive view of a crisis event, whether historical or real-time. To\naddress these gaps in crisis informatics literature, this study introduces\nCrisisTransformers, an ensemble of pre-trained language models and sentence\nencoders trained on an extensive corpus of over 15 billion word tokens from\ntweets associated with more than 30 crisis events, including disease outbreaks,\nnatural disasters, conflicts, and other critical incidents. We evaluate\nexisting models and CrisisTransformers on 18 crisis-specific public datasets.\nOur pre-trained models outperform strong baselines across all datasets in\nclassification tasks, and our best-performing sentence encoder improves the\nstate-of-the-art by 17.43% in sentence encoding tasks. Additionally, we\ninvestigate the impact of model initialization on convergence and evaluate the\nsignificance of domain-specific models in generating semantically meaningful\nsentence embeddings. The models are publicly available at:\nhttps://huggingface.co/crisistransformers\n']",Disaster Response using Social Media Text Analysis,Disaster Management and Response using AI and Data Analytics,AI and Data-Driven Approaches for Urban and Disaster Management,AI and Data-Driven Approaches for Urban and Disaster Management
316,25,316_tensors_sparse_parallelization_tensor,"['tensors', 'sparse', 'parallelization', 'tensor', 'gpus', 'gpu', 'cores', 'optimized', 'compilers', 'pytorch']","['tensor', 'sparse', 'compiler', 'operations', 'operators', 'sparsity', 'optimizations', 'hardware', 'workloads', 'speedup']","['  The ongoing trend of hardware specialization has led to a growing use of\ncustom data formats when processing sparse workloads, which are typically\nmemory-bound. These formats facilitate optimized software/hardware\nimplementations by utilizing sparsity pattern- or target-aware data structures\nand layouts to enhance memory access latency and bandwidth utilization.\nHowever, existing sparse tensor programming models and compilers offer little\nor no support for productively customizing the sparse formats. Additionally,\nbecause these frameworks represent formats using a limited set of per-dimension\nattributes, they lack the flexibility to accommodate numerous new variations of\ncustom sparse data structures and layouts. To overcome this deficiency, we\npropose UniSparse, an intermediate language that provides a unified abstraction\nfor representing and customizing sparse formats. Unlike the existing\nattribute-based frameworks, UniSparse decouples the logical representation of\nthe sparse tensor (i.e., the data structure) from its low-level memory layout,\nenabling the customization of both. As a result, a rich set of format\ncustomizations can be succinctly expressed in a small set of well-defined\nquery, mutation, and layout primitives. We also develop a compiler leveraging\nthe MLIR infrastructure, which supports adaptive customization of formats, and\nautomatic code generation of format conversion and compute operations for\nheterogeneous architectures. We demonstrate the efficacy of our approach\nthrough experiments running commonly-used sparse linear algebra operations with\nspecialized formats on multiple different hardware targets, including an Intel\nCPU, an NVIDIA GPU, an AMD Xilinx FPGA, and a simulated processing-in-memory\n(PIM) device.\n', ""  The rapid growth in the size of deep learning models strains the capabilities\nof traditional dense computation paradigms. Leveraging sparse computation has\nbecome increasingly popular for training and deploying large-scale models, but\nexisting deep learning frameworks lack extensive support for sparse operations.\nTo bridge this gap, we introduce Scorch, a library that seamlessly integrates\nefficient sparse tensor computation into the PyTorch ecosystem, with an initial\nfocus on inference workloads on CPUs. Scorch provides a flexible and intuitive\ninterface for sparse tensors, supporting diverse sparse data structures. Scorch\nintroduces a compiler stack that automates key optimizations, including\nautomatic loop ordering, tiling, and format inference. Combined with a runtime\nthat adapts its execution to both dense and sparse data, Scorch delivers\nsubstantial speedups over hand-written PyTorch Sparse (torch.sparse) operations\nwithout sacrificing usability. More importantly, Scorch enables efficient\ncomputation of complex sparse operations that lack hand-optimized PyTorch\nimplementations. This flexibility is crucial for exploring novel sparse\narchitectures. We demonstrate Scorch's ease of use and performance gains on\ndiverse deep learning models across multiple domains. With only minimal code\nchanges, Scorch achieves 1.05-5.78x speedups over PyTorch Sparse on end-to-end\ntasks. Scorch's seamless integration and performance gains make it a valuable\naddition to the PyTorch ecosystem. We believe Scorch will enable wider\nexploration of sparsity as a tool for scaling deep learning and inform the\ndevelopment of other sparse libraries.\n"", '  With the rapid development of deep learning models and hardware support for\ndense computing, the deep learning workload characteristics changed\nsignificantly from a few hot spots on compute-intensive operations to a broad\nrange of operations scattered across the models. Accelerating a few\ncompute-intensive operations using the expert-tuned implementation of\nprimitives does not fully exploit the performance potential of AI hardware.\nVarious efforts have been made to compile a full deep neural network (DNN)\ngraph. One of the biggest challenges is to achieve high-performance tensor\ncompilation by generating expert level performance code for the dense\ncompute-intensive operations and applying compilation optimization at the scope\nof DNN computation graph across multiple compute-intensive operations.\n  We present oneDNN Graph Compiler, a tensor compiler that employs a hybrid\napproach of using techniques from both compiler optimization and expert-tuned\nkernels for high performance code generation of the deep neural network graph.\noneDNN Graph Compiler addresses unique optimization challenges in the deep\nlearning domain, such as low-precision computation, aggressive fusion of graph\noperations, optimization for static tensor shapes and memory layout, constant\nweight optimization, and memory buffer reuse. Experimental results demonstrate\nsignificant performance gains over existing tensor compiler and primitives\nlibrary for performance-critical DNN computation graphs and end-to-end models\non Intel Xeon Scalable Processors.\n']",Optimized Sparse Tensor Compilers for Deep Learning,Tensor Methods and Applications,Tensor and Matrix Methods for Data Representation and Completion,Tensor and Matrix Methods for Data Representation and Completion
317,25,317_gflownet_gflownets_generative_flow,"['gflownet', 'gflownets', 'generative', 'flow', 'eflownets', 'flows', 'networks', 'learned', 'reward', 'rewards']","['gfn', 'flow', 'reward', 'objects', 'policy', 'proportional', 'rewards', 'backward', 'generative', 'compositional']","['  Generative Flow Networks (GFlowNets) treat sampling from distributions over\ncompositional discrete spaces as a sequential decision-making problem, training\na stochastic policy to construct objects step by step. Recent studies have\nrevealed strong connections between GFlowNets and entropy-regularized\nreinforcement learning. Building on these insights, we propose to enhance\nplanning capabilities of GFlowNets by applying Monte Carlo Tree Search (MCTS).\nSpecifically, we show how the MENTS algorithm (Xiao et al., 2019) can be\nadapted for GFlowNets and used during both training and inference. Our\nexperiments demonstrate that this approach improves the sample efficiency of\nGFlowNet training and the generation fidelity of pre-trained GFlowNet models.\n', '  Generative Flow Networks (GFlowNets, GFNs) are a generative framework for\nlearning unnormalized probability mass functions over discrete spaces. Since\ntheir inception, GFlowNets have proven to be useful for learning generative\nmodels in applications where the majority of the discrete space is unvisited\nduring training. This has inspired some to hypothesize that GFlowNets, when\npaired with deep neural networks (DNNs), have favourable generalization\nproperties. In this work, we empirically verify some of the hypothesized\nmechanisms of generalization of GFlowNets. In particular, we find that the\nfunctions that GFlowNets learn to approximate have an implicit underlying\nstructure which facilitate generalization. We also find that GFlowNets are\nsensitive to being trained offline and off-policy; however, the reward\nimplicitly learned by GFlowNets is robust to changes in the training\ndistribution.\n', '  The Generative Flow Network (GFlowNet) is a probabilistic framework in which\nan agent learns a stochastic policy and flow functions to sample objects with\nprobability proportional to an unnormalized reward function. GFlowNets share a\nstrong resemblance to reinforcement learning (RL), that typically aims to\nmaximize reward, due to their sequential decision-making processes. Recent\nworks have studied connections between GFlowNets and maximum entropy (MaxEnt)\nRL, which modifies the standard objective of RL agents by learning an\nentropy-regularized objective. However, a critical theoretical gap persists:\ndespite the apparent similarities in their sequential decision-making nature, a\ndirect link between GFlowNets and standard RL has yet to be discovered, while\nbridging this gap could further unlock the potential of both fields. In this\npaper, we establish a new connection between GFlowNets and policy evaluation\nfor a uniform policy. Surprisingly, we find that the resulting value function\nfor the uniform policy has a close relationship to the flows in GFlowNets.\nLeveraging these insights, we further propose a novel rectified policy\nevaluation (RPE) algorithm, which achieves the same reward-matching effect as\nGFlowNets, offering a new perspective. We compare RPE, MaxEnt RL, and GFlowNets\nin a number of benchmarks, and show that RPE achieves competitive results\ncompared to previous approaches. This work sheds light on the previously\nunexplored connection between (non-MaxEnt) RL and GFlowNets, potentially\nopening new avenues for future research in both fields.\n']",Generative Flow Networks (GFlowNets),Generative Modeling Techniques,Generative Modeling and Artificial Intelligence,Generative Modeling and Artificial Intelligence
318,25,318_cnns_deepsets_neural_representations,"['cnns', 'deepsets', 'neural', 'representations', 'deep', 'deepdrk', 'networks', 'convolutional', 'deeper', 'representational']","['layers', 'representations', 'similarity', 'features', 'architectures', 'deep', 'hierarchy', 'simplified', 'networks', 'layer']","['  Computer vision (CV) is one of the most crucial fields in artificial\nintelligence. In recent years, a variety of deep learning models based on\nconvolutional neural networks (CNNs) and Transformers have been designed to\ntackle diverse problems in CV. These algorithms have found practical\napplications in areas such as robotics and facial recognition. Despite the\nincreasing power of current CV models, several fundamental questions remain\nunresolved: Why do CNNs require deep layers? What ensures the generalization\nability of CNNs? Why do residual-based networks outperform fully convolutional\nnetworks like VGG? What is the fundamental difference between residual-based\nCNNs and Transformer-based networks? Why can CNNs utilize LoRA and pruning\ntechniques? The root cause of these questions lies in the lack of a robust\ntheoretical foundation for deep learning models in CV. To address these\ncritical issues and techniques, we employ the Universal Approximation Theorem\n(UAT) to provide a theoretical basis for convolution- and Transformer-based\nmodels in CV. By doing so, we aim to elucidate these questions from a\ntheoretical perspective.\n', '  In this paper, we provide a theoretical analysis of the inductive biases in\nconvolutional neural networks (CNNs). We start by examining the universality of\nCNNs, i.e., the ability to approximate any continuous functions. We prove that\na depth of $\\mathcal{O}(\\log d)$ suffices for deep CNNs to achieve this\nuniversality, where $d$ in the input dimension. Additionally, we establish that\nlearning sparse functions with CNNs requires only\n$\\widetilde{\\mathcal{O}}(\\log^2d)$ samples, indicating that deep CNNs can\nefficiently capture {\\em long-range} sparse correlations. These results are\nmade possible through a novel combination of the multichanneling and\ndownsampling when increasing the network depth. We also delve into the distinct\nroles of weight sharing and locality in CNNs. To this end, we compare the\nperformance of CNNs, locally-connected networks (LCNs), and fully-connected\nnetworks (FCNs) on a simple regression task, where LCNs can be viewed as CNNs\nwithout weight sharing. On the one hand, we prove that LCNs require\n${\\Omega}(d)$ samples while CNNs need only $\\widetilde{\\mathcal{O}}(\\log^2d)$\nsamples, highlighting the critical role of weight sharing. On the other hand,\nwe prove that FCNs require $\\Omega(d^2)$ samples, whereas LCNs need only\n$\\widetilde{\\mathcal{O}}(d)$ samples, underscoring the importance of locality.\nThese provable separations quantify the difference between the two biases, and\nthe major observation behind our proof is that weight sharing and locality\nbreak different symmetries in the learning process.\n', '  Analyzing the similarity of internal representations within and across\ndifferent models has been an important technique for understanding the behavior\nof deep neural networks. Most existing methods for analyzing the similarity\nbetween representations of high dimensions, such as those based on Canonical\nCorrelation Analysis (CCA) and widely used Centered Kernel Alignment (CKA),\nrely on statistical properties of the representations for a set of data points.\nIn this paper, we focus on transformer models and study the similarity of\nrepresentations between the hidden layers of individual transformers. In this\ncontext, we show that a simple sample-wise cosine similarity metric is capable\nof capturing the similarity and aligns with the complicated CKA. Our\nexperimental results on common transformers reveal that representations across\nlayers are positively correlated, albeit the similarity decreases when layers\nare far apart. We then propose an aligned training approach to enhance the\nsimilarity between internal representations, with trained models that enjoy the\nfollowing properties: (1) the last-layer classifier can be directly applied\nright after any hidden layers, yielding intermediate layer accuracies much\nhigher than those under standard training, (2) the layer-wise accuracies\nmonotonically increase and reveal the minimal depth needed for the given task,\n(3) when served as multi-exit models, they achieve on-par performance with\nstandard multi-exit architectures which consist of additional classifiers\ndesigned for early exiting in shallow layers. To our knowledge, our work is the\nfirst to show that one common classifier is sufficient for multi-exit models.\nWe conduct experiments on both vision and NLP tasks to demonstrate the\nperformance of the proposed aligned training.\n']",Theoretical Foundations of Convolutional Neural Networks,Deep Learning Theory and Foundations,Machine Learning and Artificial Intelligence,Machine Learning and Intelligent Systems
319,25,319_wirelessllm_generative_ai_wireless,"['wirelessllm', 'generative', 'ai', 'wireless', 'networks', 'network', 'mobile', 'communications', '6g', 'telecom']","['wireless', 'telecom', 'communication', 'semantic', 'networks', 'communications', 'sixth', 'enabled', 'intelligence', 'native']","['  Intelligent communications have played a pivotal role in shaping the\nevolution of 6G networks. Native artificial intelligence (AI) within green\ncommunication systems must meet stringent real-time requirements. To achieve\nthis, deploying lightweight and resource-efficient AI models is necessary.\nHowever, as wireless networks generate a multitude of data fields and\nindicators during operation, only a fraction of them imposes significant impact\non the network AI models. Therefore, real-time intelligence of communication\nsystems heavily relies on a small but critical set of the data that profoundly\ninfluences the performance of network AI models. These challenges underscore\nthe need for innovative architectures and solutions. In this paper, we propose\na solution, termed the pervasive multi-level (PML) native AI architecture,\nwhich integrates the concept of knowledge graph (KG) into the intelligent\noperational manipulations of mobile networks, resulting in the establishment of\na wireless data KG. Leveraging the wireless data KG, we characterize the\nmassive and complex data collected from wireless communication networks and\nanalyze the relationships among various data fields. The obtained graph of data\nfield relations enables the on-demand generation of minimal and effective\ndatasets, referred to as feature datasets, tailored to specific application\nrequirements. Consequently, this architecture not only enhances AI training,\ninference, and validation processes but also significantly reduces resource\nwastage and overhead for communication networks. To implement this\narchitecture, we have developed a specific solution comprising a\nspatio-temporal heterogeneous graph attention neural network model (STREAM) as\nwell as a feature dataset generation algorithm. Experiments are conducted to\nvalidate the effectiveness of the proposed architecture.\n', '  Generative artificial intelligence (GenAI) and communication networks are\nexpected to have groundbreaking synergies in 6G. Connecting GenAI agents over a\nwireless network can potentially unleash the power of collective intelligence\nand pave the way for artificial general intelligence (AGI). However, current\nwireless networks are designed as a ""data pipe"" and are not suited to\naccommodate and leverage the power of GenAI. In this paper, we propose the\nGenAINet framework in which distributed GenAI agents communicate knowledge\n(high-level concepts or abstracts) to accomplish arbitrary tasks. We first\nprovide a network architecture integrating GenAI capabilities to manage both\nnetwork protocols and applications. Building on this, we investigate effective\ncommunication and reasoning problems by proposing a semantic-native GenAINet.\nSpecifically, GenAI agents extract semantic concepts from multi-modal raw data,\nbuild a knowledgebase representing their semantic relations, which is retrieved\nby GenAI models for planning and reasoning. Under this paradigm, an agent can\nlearn fast from other agents\' experience for making better decisions with\nefficient communications. Furthermore, we conduct two case studies where in\nwireless device query, we show that extracting and transferring knowledge can\nimprove query accuracy with reduced communication; and in wireless power\ncontrol, we show that distributed agents can improve decisions via\ncollaborative reasoning. Finally, we address that developing a hierarchical\nsemantic level Telecom world model is a key path towards network of collective\nintelligence.\n', '  Wireless communications advance hand-in-hand with artificial intelligence\n(AI), indicating an interconnected advancement where each facilitates and\nbenefits from the other. This synergy is particularly evident in the\ndevelopment of the sixth-generation technology standard for mobile networks\n(6G), envisioned to be AI-native. Generative-AI (GenAI), a novel technology\ncapable of producing various types of outputs, including text, images, and\nvideos, offers significant potential for wireless communications, with its\ndistinctive features. Traditionally, conventional AI techniques have been\nemployed for predictions, classifications, and optimization, while GenAI has\nmore to offer. This article introduces the concept of strategic demand-planning\nthrough demand-labeling, demand-shaping, and demand-rescheduling. Accordingly,\nGenAI is proposed as a powerful tool to facilitate demand-shaping in wireless\nnetworks. More specifically, GenAI is used to compress and convert the content\nof various kind (e.g., from a higher bandwidth mode to a lower one, such as\nfrom a video to text), which subsequently enhances performance of wireless\nnetworks in various usage scenarios such as cell-switching, user association\nand load balancing, interference management, and disaster scenarios management.\nTherefore, GenAI can serve a function in saving energy and spectrum in wireless\nnetworks. With recent advancements in AI, including sophisticated algorithms\nlike large-language-models and the development of more powerful hardware built\nexclusively for AI tasks, such as AI accelerators, the concept of\ndemand-planning, particularly demand-shaping through GenAI, becomes\nincreasingly relevant. Furthermore, recent efforts to make GenAI accessible on\ndevices, such as user terminals, make the implementation of this concept even\nmore straightforward and feasible.\n']",AI-Driven Wireless Networks and 6G Communications,Artificial Intelligence in Technology and Engineering,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
319,25,319_wirelessllm_generative_ai_wireless,"['wirelessllm', 'generative', 'ai', 'wireless', 'networks', 'network', 'mobile', 'communications', '6g', 'telecom']","['wireless', 'telecom', 'communication', 'semantic', 'networks', 'communications', 'sixth', 'enabled', 'intelligence', 'native']","['  Intelligent communications have played a pivotal role in shaping the\nevolution of 6G networks. Native artificial intelligence (AI) within green\ncommunication systems must meet stringent real-time requirements. To achieve\nthis, deploying lightweight and resource-efficient AI models is necessary.\nHowever, as wireless networks generate a multitude of data fields and\nindicators during operation, only a fraction of them imposes significant impact\non the network AI models. Therefore, real-time intelligence of communication\nsystems heavily relies on a small but critical set of the data that profoundly\ninfluences the performance of network AI models. These challenges underscore\nthe need for innovative architectures and solutions. In this paper, we propose\na solution, termed the pervasive multi-level (PML) native AI architecture,\nwhich integrates the concept of knowledge graph (KG) into the intelligent\noperational manipulations of mobile networks, resulting in the establishment of\na wireless data KG. Leveraging the wireless data KG, we characterize the\nmassive and complex data collected from wireless communication networks and\nanalyze the relationships among various data fields. The obtained graph of data\nfield relations enables the on-demand generation of minimal and effective\ndatasets, referred to as feature datasets, tailored to specific application\nrequirements. Consequently, this architecture not only enhances AI training,\ninference, and validation processes but also significantly reduces resource\nwastage and overhead for communication networks. To implement this\narchitecture, we have developed a specific solution comprising a\nspatio-temporal heterogeneous graph attention neural network model (STREAM) as\nwell as a feature dataset generation algorithm. Experiments are conducted to\nvalidate the effectiveness of the proposed architecture.\n', '  Generative artificial intelligence (GenAI) and communication networks are\nexpected to have groundbreaking synergies in 6G. Connecting GenAI agents over a\nwireless network can potentially unleash the power of collective intelligence\nand pave the way for artificial general intelligence (AGI). However, current\nwireless networks are designed as a ""data pipe"" and are not suited to\naccommodate and leverage the power of GenAI. In this paper, we propose the\nGenAINet framework in which distributed GenAI agents communicate knowledge\n(high-level concepts or abstracts) to accomplish arbitrary tasks. We first\nprovide a network architecture integrating GenAI capabilities to manage both\nnetwork protocols and applications. Building on this, we investigate effective\ncommunication and reasoning problems by proposing a semantic-native GenAINet.\nSpecifically, GenAI agents extract semantic concepts from multi-modal raw data,\nbuild a knowledgebase representing their semantic relations, which is retrieved\nby GenAI models for planning and reasoning. Under this paradigm, an agent can\nlearn fast from other agents\' experience for making better decisions with\nefficient communications. Furthermore, we conduct two case studies where in\nwireless device query, we show that extracting and transferring knowledge can\nimprove query accuracy with reduced communication; and in wireless power\ncontrol, we show that distributed agents can improve decisions via\ncollaborative reasoning. Finally, we address that developing a hierarchical\nsemantic level Telecom world model is a key path towards network of collective\nintelligence.\n', '  Wireless communications advance hand-in-hand with artificial intelligence\n(AI), indicating an interconnected advancement where each facilitates and\nbenefits from the other. This synergy is particularly evident in the\ndevelopment of the sixth-generation technology standard for mobile networks\n(6G), envisioned to be AI-native. Generative-AI (GenAI), a novel technology\ncapable of producing various types of outputs, including text, images, and\nvideos, offers significant potential for wireless communications, with its\ndistinctive features. Traditionally, conventional AI techniques have been\nemployed for predictions, classifications, and optimization, while GenAI has\nmore to offer. This article introduces the concept of strategic demand-planning\nthrough demand-labeling, demand-shaping, and demand-rescheduling. Accordingly,\nGenAI is proposed as a powerful tool to facilitate demand-shaping in wireless\nnetworks. More specifically, GenAI is used to compress and convert the content\nof various kind (e.g., from a higher bandwidth mode to a lower one, such as\nfrom a video to text), which subsequently enhances performance of wireless\nnetworks in various usage scenarios such as cell-switching, user association\nand load balancing, interference management, and disaster scenarios management.\nTherefore, GenAI can serve a function in saving energy and spectrum in wireless\nnetworks. With recent advancements in AI, including sophisticated algorithms\nlike large-language-models and the development of more powerful hardware built\nexclusively for AI tasks, such as AI accelerators, the concept of\ndemand-planning, particularly demand-shaping through GenAI, becomes\nincreasingly relevant. Furthermore, recent efforts to make GenAI accessible on\ndevices, such as user terminals, make the implementation of this concept even\nmore straightforward and feasible.\n']",AI-Driven Wireless Networks and 6G Communications,Artificial Intelligence in Technology and Engineering,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
320,25,320_augmentation_mixup_mix_learning,"['augmentation', 'mixup', 'mix', 'learning', 'mixing', 'trained', 'mixda', 'classifier', 'augmix', 'generalization']","['mixup', 'transfer', 'transferability', 'target', 'source', 'estimation', 'mix', 'augmentation', 'affine', 'distance']","['  Mixup and its variants form a popular class of data augmentation\ntechniques.Using a random sample pair, it generates a new sample by linear\ninterpolation of the inputs and labels. However, generating only one single\ninterpolation may limit its augmentation ability. In this paper, we propose a\nsimple yet effective extension called multi-mix, which generates multiple\ninterpolations from a sample pair. With an ordered sequence of generated\nsamples, multi-mix can better guide the training process than standard mixup.\nMoreover, theoretically, this can also reduce the stochastic gradient variance.\nExtensive experiments on a number of synthetic and large-scale data sets\ndemonstrate that multi-mix outperforms various mixup variants and\nnon-mixup-based baselines in terms of generalization, robustness, and\ncalibration.\n', ""  Mixup is a data augmentation strategy that employs convex combinations of\ntraining instances and their respective labels to augment the robustness and\ncalibration of deep neural networks. Despite its widespread adoption, the\nnuanced mechanisms that underpin its success are not entirely understood. The\nobserved phenomenon of Neural Collapse, where the last-layer activations and\nclassifier of deep networks converge to a simplex equiangular tight frame\n(ETF), provides a compelling motivation to explore whether mixup induces\nalternative geometric configurations and whether those could explain its\nsuccess. In this study, we delve into the last-layer activations of training\ndata for deep networks subjected to mixup, aiming to uncover insights into its\noperational efficacy. Our investigation, spanning various architectures and\ndataset pairs, reveals that mixup's last-layer activations predominantly\nconverge to a distinctive configuration different than one might expect. In\nthis configuration, activations from mixed-up examples of identical classes\nalign with the classifier, while those from different classes delineate\nchannels along the decision boundary. Moreover, activations in earlier layers\nexhibit patterns, as if trained with manifold mixup. These findings are\nunexpected, as mixed-up features are not simple convex combinations of feature\nclass means (as one might get, for example, by training mixup with the mean\nsquared error loss). By analyzing this distinctive geometric configuration, we\nelucidate the mechanisms by which mixup enhances model calibration. To further\nvalidate our empirical observations, we conduct a theoretical analysis under\nthe assumption of an unconstrained features model, utilizing the mixup loss.\nThrough this, we characterize and derive the optimal last-layer features under\nthe assumption that the classifier forms a simplex ETF.\n"", '  We study the problem of robust data augmentation for regression tasks in the\npresence of noisy data. Data augmentation is essential for generalizing deep\nlearning models, but most of the techniques like the popular Mixup are\nprimarily designed for classification tasks on image data. Recently, there are\nalso Mixup techniques that are specialized to regression tasks like C-Mixup. In\ncomparison to Mixup, which takes linear interpolations of pairs of samples,\nC-Mixup is more selective in which samples to mix based on their label\ndistances for better regression performance. However, C-Mixup does not\ndistinguish noisy versus clean samples, which can be problematic when mixing\nand lead to suboptimal model performance. At the same time, robust training has\nbeen heavily studied where the goal is to train accurate models against noisy\ndata through multiple rounds of model training. We thus propose our data\naugmentation strategy RC-Mixup, which tightly integrates C-Mixup with\nmulti-round robust training methods for a synergistic effect. In particular,\nC-Mixup improves robust training in identifying clean data, while robust\ntraining provides cleaner data to C-Mixup for it to perform better. A key\nadvantage of RC-Mixup is that it is data-centric where the robust model\ntraining algorithm itself does not need to be modified, but can simply benefit\nfrom data mixing. We show in our experiments that RC-Mixup significantly\noutperforms C-Mixup and robust training baselines on noisy data benchmarks and\ncan be integrated with various robust training methods.\n']",Mixup Data Augmentation Techniques,Data Augmentation Techniques for Deep Learning,Deep Learning Optimization and Security,Deep Learning Methodologies
321,25,321_lasso_lassoglm_regularization_sparse,"['lasso', 'lassoglm', 'regularization', 'sparse', 'penalized', 'glmnet', 'predictors', 'robust', 'shrinkage', 'regression']","['lasso', 'screening', 'covariance', 'sparse', 'shrinkage', 'group', 'penalty', 'regression', 'estimation', 'rule']","['  This paper introduces a new regularized version of the robust\n$\\tau$-regression estimator for analyzing high-dimensional datasets subject to\ngross contamination in the response variables and covariates (explanatory\nvariables). The resulting estimator, termed adaptive $\\tau$-Lasso, is robust to\noutliers and high-leverage points. It also incorporates an adaptive\n$\\ell_1$-norm penalty term, which enables the selection of relevant variables\nand reduces the bias associated with large true regression coefficients. More\nspecifically, this adaptive $\\ell_1$-norm penalty term assigns a weight to each\nregression coefficient. For a fixed number of predictors $p$, we show that the\nadaptive $\\tau$-Lasso has the oracle property, ensuring both variable-selection\nconsistency and asymptotic normality. Asymptotic normality applies only to the\nentries of the regression vector corresponding to the true support, assuming\nknowledge of the true regression vector support. We characterize its robustness\nby establishing the finite-sample breakdown point and the influence function.\nWe carry out extensive simulations and observe that the class of $\\tau$-Lasso\nestimators exhibits robustness and reliable performance in both contaminated\nand uncontaminated data settings. We also validate our theoretical findings on\nrobustness properties through simulations. In the face of outliers and\nhigh-leverage points, the adaptive $\\tau$-Lasso and $\\tau$-Lasso estimators\nachieve the best performance or close-to-best performance in terms of\nprediction and variable selection accuracy compared to other competing\nregularized estimators for all scenarios considered in this study. Therefore,\nthe adaptive $\\tau$-Lasso and $\\tau$-Lasso estimators provide attractive tools\nfor a variety of sparse linear regression problems, particularly in\nhigh-dimensional settings and when the data is contaminated by outliers and\nhigh-leverage points.\n', '  This paper presents a comprehensive exploration of the theoretical properties\ninherent in the Adaptive Lasso and the Transfer Lasso. The Adaptive Lasso, a\nwell-established method, employs regularization divided by initial estimators\nand is characterized by asymptotic normality and variable selection\nconsistency. In contrast, the recently proposed Transfer Lasso employs\nregularization subtracted by initial estimators with the demonstrated capacity\nto curtail non-asymptotic estimation errors. A pivotal question thus emerges:\nGiven the distinct ways the Adaptive Lasso and the Transfer Lasso employ\ninitial estimators, what benefits or drawbacks does this disparity confer upon\neach method? This paper conducts a theoretical examination of the asymptotic\nproperties of the Transfer Lasso, thereby elucidating its differentiation from\nthe Adaptive Lasso. Informed by the findings of this analysis, we introduce a\nnovel method, one that amalgamates the strengths and compensates for the\nweaknesses of both methods. The paper concludes with validations of our theory\nand comparisons of the methods via simulation experiments.\n', '  The sparse-group lasso performs both variable and group selection, making\nsimultaneous use of the strengths of the lasso and group lasso. It has found\nwidespread use in genetics, a field that regularly involves the analysis of\nhigh-dimensional data, due to its sparse-group penalty, which allows it to\nutilize grouping information. However, the sparse-group lasso can be\ncomputationally more expensive than both the lasso and group lasso, due to the\nadded shrinkage complexity, and its additional hyper-parameter that needs\ntuning. In this paper a novel dual feature reduction method, Dual Feature\nReduction (DFR), is presented that uses strong screening rules for the\nsparse-group lasso and the adaptive sparse-group lasso to reduce their input\nspace before optimization. DFR applies two layers of screening and is based on\nthe dual norms of the sparse-group lasso and adaptive sparse-group lasso.\nThrough synthetic and real numerical studies, it is shown that the proposed\nfeature reduction approach is able to drastically reduce the computational cost\nin many different scenarios.\n']",Robust Sparse Regression Methods,Robust Regression Methods,Advanced Statistical and Machine Learning Methods,Advanced Statistical and Machine Learning Methods
322,25,322_attention_generative_blurring_conditioned,"['attention', 'generative', 'blurring', 'conditioned', 'images', 'conditioning', 'models', 'guidance', 'denoised', 'diffusion']","['diffusion', 'guidance', 'conditional', 'unconditional', 'image', 'bokeh', 'generation', 'controls', 'conditioning', 'condition']","['  Conditional diffusion models have shown remarkable success in visual content\ngeneration, producing high-quality samples across various domains, largely due\nto classifier-free guidance (CFG). Recent attempts to extend guidance to\nunconditional models have relied on heuristic techniques, resulting in\nsuboptimal generation quality and unintended effects. In this work, we propose\nSmoothed Energy Guidance (SEG), a novel training- and condition-free approach\nthat leverages the energy-based perspective of the self-attention mechanism to\nenhance image generation. By defining the energy of self-attention, we\nintroduce a method to reduce the curvature of the energy landscape of attention\nand use the output as the unconditional prediction. Practically, we control the\ncurvature of the energy landscape by adjusting the Gaussian kernel parameter\nwhile keeping the guidance scale parameter fixed. Additionally, we present a\nquery blurring method that is equivalent to blurring the entire attention\nweights without incurring quadratic complexity in the number of tokens. In our\nexperiments, SEG achieves a Pareto improvement in both quality and the\nreduction of side effects. The code is available at\n\\url{https://github.com/SusungHong/SEG-SDXL}.\n', '  Classifier-free guidance (CFG) is a fundamental tool in modern diffusion\nmodels for text-guided generation. Although effective, CFG has notable\ndrawbacks. For instance, DDIM with CFG lacks invertibility, complicating image\nediting; furthermore, high guidance scales, essential for high-quality outputs,\nfrequently result in issues like mode collapse. Contrary to the widespread\nbelief that these are inherent limitations of diffusion models, this paper\nreveals that the problems actually stem from the off-manifold phenomenon\nassociated with CFG, rather than the diffusion models themselves. More\nspecifically, inspired by the recent advancements of diffusion model-based\ninverse problem solvers (DIS), we reformulate text-guidance as an inverse\nproblem with a text-conditioned score matching loss, and develop CFG++, a novel\napproach that tackles the off-manifold challenges inherent in traditional CFG.\nCFG++ features a surprisingly simple fix to CFG, yet it offers significant\nimprovements, including better sample quality for text-to-image generation,\ninvertibility, smaller guidance scales, reduced mode collapse, etc.\nFurthermore, CFG++ enables seamless interpolation between unconditional and\nconditional sampling at lower guidance scales, consistently outperforming\ntraditional CFG at all scales. Experimental results confirm that our method\nsignificantly enhances performance in text-to-image generation, DDIM inversion,\nediting, and solving inverse problems, suggesting a wide-ranging impact and\npotential applications in various fields that utilize text guidance. Project\nPage: https://cfgpp-diffusion.github.io/.\n', ""  Recent studies have demonstrated that diffusion models are capable of\ngenerating high-quality samples, but their quality heavily depends on sampling\nguidance techniques, such as classifier guidance (CG) and classifier-free\nguidance (CFG). These techniques are often not applicable in unconditional\ngeneration or in various downstream tasks such as image restoration. In this\npaper, we propose a novel sampling guidance, called Perturbed-Attention\nGuidance (PAG), which improves diffusion sample quality across both\nunconditional and conditional settings, achieving this without requiring\nadditional training or the integration of external modules. PAG is designed to\nprogressively enhance the structure of samples throughout the denoising\nprocess. It involves generating intermediate samples with degraded structure by\nsubstituting selected self-attention maps in diffusion U-Net with an identity\nmatrix, by considering the self-attention mechanisms' ability to capture\nstructural information, and guiding the denoising process away from these\ndegraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves\nsample quality in conditional and even unconditional scenarios. Moreover, PAG\nsignificantly improves the baseline performance in various downstream tasks\nwhere existing guidances such as CG or CFG cannot be fully utilized, including\nControlNet with empty prompts and image restoration such as inpainting and\ndeblurring.\n""]",Diffusion Models for Image Generation with Guidance Techniques,Diffusion Models for Image and Video Generation and Manipulation,Diffusion Models for Generative Tasks,Diffusion Models for Generative Tasks
323,25,323_accelerator_accelerators_hardware_cnn,"['accelerator', 'accelerators', 'hardware', 'cnn', 'dataflows', 'cores', 'throughput', 'dataflow', 'memory', 'dnns']","['accelerators', 'dataflow', 'hardware', 'chip', 'accelerator', 'memory', 'systolic', 'energy', 'latency', 'fault']","['  Deep Neural Networks (DNNs) excel in learning hierarchical representations\nfrom raw data, such as images, audio, and text. To compute these DNN models\nwith high performance and energy efficiency, these models are usually deployed\nonto customized hardware accelerators. Among various accelerator designs,\ndataflow architecture has shown promising performance due to its\nlayer-pipelined structure and its scalability in data parallelism.\n  Exploiting weights and activations sparsity can further enhance memory\nstorage and computation efficiency. However, existing approaches focus on\nexploiting sparsity in non-dataflow accelerators, which cannot be applied onto\ndataflow accelerators because of the large hardware design space introduced. As\nsuch, this could miss opportunities to find an optimal combination of sparsity\nfeatures and hardware designs.\n  In this paper, we propose a novel approach to exploit unstructured weights\nand activations sparsity for dataflow accelerators, using software and hardware\nco-optimization. We propose a Hardware-Aware Sparsity Search (HASS) to\nsystematically determine an efficient sparsity solution for dataflow\naccelerators. Over a set of models, we achieve an efficiency improvement\nranging from 1.3$\\times$ to 4.2$\\times$ compared to existing sparse designs,\nwhich are either non-dataflow or non-hardware-aware. Particularly, the\nthroughput of MobileNetV3 can be optimized to 4895 images per second. HASS is\nopen-source: \\url{https://github.com/Yu-Zhewen/HASS}\n', '  Systolic array has emerged as a prominent architecture for Deep Neural\nNetwork (DNN) hardware accelerators, providing high-throughput and low-latency\nperformance essential for deploying DNNs across diverse applications. However,\nwhen used in safety-critical applications, reliability assessment is mandatory\nto guarantee the correct behavior of DNN accelerators. While fault injection\nstands out as a well-established practical and robust method for reliability\nassessment, it is still a very time-consuming process. This paper addresses the\ntime efficiency issue by introducing a novel hierarchical software-based\nhardware-aware fault injection strategy tailored for systolic array-based DNN\naccelerators.\n', ""  The stringent requirements for the Deep Neural Networks (DNNs) accelerator's\nreliability stand along with the need for reducing the computational burden on\nthe hardware platforms, i.e. reducing the energy consumption and execution time\nas well as increasing the efficiency of DNN accelerators. Moreover, the growing\ndemand for specialized DNN accelerators with tailored requirements,\nparticularly for safety-critical applications, necessitates a comprehensive\ndesign space exploration to enable the development of efficient and robust\naccelerators that meet those requirements. Therefore, the trade-off between\nhardware performance, i.e. area and delay, and the reliability of the DNN\naccelerator implementation becomes critical and requires tools for analysis.\nThis paper presents a comprehensive methodology for exploring and enabling a\nholistic assessment of the trilateral impact of quantization on model accuracy,\nactivation fault reliability, and hardware efficiency. A fully automated\nframework is introduced that is capable of applying various quantization-aware\ntechniques, fault injection, and hardware implementation, thus enabling the\nmeasurement of hardware parameters. Moreover, this paper proposes a novel\nlightweight protection technique integrated within the framework to ensure the\ndependable deployment of the final systolic-array-based FPGA implementation.\nThe experiments on established benchmarks demonstrate the analysis flow and the\nprofound implications of quantization on reliability, hardware performance, and\nnetwork accuracy, particularly concerning the transient faults in the network's\nactivations.\n""]",Efficient Deep Neural Network Accelerators,Efficient Deep Learning Architectures and Acceleration Techniques,Artificial Intelligence and Machine Learning Hardware Acceleration and Optimization,Artificial Intelligence and Machine Learning Hardware Acceleration and Optimization
324,25,324_quantization_quantized_diffusion_imagenet,"['quantization', 'quantized', 'diffusion', 'imagenet', 'denoising', 'pixel', 'deblurring', 'noising', 'generative', 'images']","['diffusion', 'quantization', 'restoration', 'patches', 'denoising', 'image', 'resolution', 'patch', 'degradation', 'bit']","['  Diffusion models have revolutionized image synthesis, setting new benchmarks\nin quality and creativity. However, their widespread adoption is hindered by\nthe intensive computation required during the iterative denoising process.\nPost-training quantization (PTQ) presents a solution to accelerate sampling,\naibeit at the expense of sample quality, extremely in low-bit settings.\nAddressing this, our study introduces a unified Quantization Noise Correction\nScheme (QNCD), aimed at minishing quantization noise throughout the sampling\nprocess. We identify two primary quantization challenges: intra and inter\nquantization noise. Intra quantization noise, mainly exacerbated by embeddings\nin the resblock module, extends activation quantization ranges, increasing\ndisturbances in each single denosing step. Besides, inter quantization noise\nstems from cumulative quantization deviations across the entire denoising\nprocess, altering data distributions step-by-step. QNCD combats these through\nembedding-derived feature smoothing for eliminating intra quantization noise\nand an effective runtime noise estimatiation module for dynamicly filtering\ninter quantization noise. Extensive experiments demonstrate that our method\noutperforms previous quantization methods for diffusion models, achieving\nlossless results in W4A8 and W8A8 quantization settings on ImageNet (LDM-4).\nCode is available at: https://github.com/huanpengchu/QNCD\n', '  Diffusion models have achieved great success in image generation tasks\nthrough iterative noise estimation. However, the heavy denoising process and\ncomplex neural networks hinder their low-latency applications in real-world\nscenarios. Quantization can effectively reduce model complexity, and\npost-training quantization (PTQ), which does not require fine-tuning, is highly\npromising in accelerating the denoising process. Unfortunately, we find that\ndue to the highly dynamic distribution of activations in different denoising\nsteps, existing PTQ methods for diffusion models suffer from distribution\nmismatch issues at both calibration sample level and reconstruction output\nlevel, which makes the performance far from satisfactory, especially in low-bit\ncases. In this paper, we propose Enhanced Distribution Alignment for\nPost-Training Quantization of Diffusion Models (EDA-DM) to address the above\nissues. Specifically, at the calibration sample level, we select calibration\nsamples based on the density and diversity in the latent space, thus\nfacilitating the alignment of their distribution with the overall samples; and\nat the reconstruction output level, we propose Fine-grained Block\nReconstruction, which can align the outputs of the quantized model and the\nfull-precision model at different network granularity. Extensive experiments\ndemonstrate that EDA-DM outperforms the existing post-training quantization\nframeworks in both unconditional and conditional generation scenarios. At\nlow-bit precision, the quantized models with our method even outperform the\nfull-precision models on most datasets.\n', '  Recent advancements in diffusion models, particularly the trend of\narchitectural transformation from UNet-based Diffusion to Diffusion Transformer\n(DiT), have significantly improved the quality and scalability of image\nsynthesis. Despite the incredible generative quality, the large computational\nrequirements of these large-scale models significantly hinder the deployments\nin real-world scenarios. Post-training Quantization (PTQ) offers a promising\nsolution by compressing model sizes and speeding up inference for the\npretrained models while eliminating model retraining. However, we have observed\nthe existing PTQ frameworks exclusively designed for both ViT and conventional\nDiffusion models fall into biased quantization and result in remarkable\nperformance degradation. In this paper, we find that the DiTs typically exhibit\nconsiderable variance in terms of both weight and activation, which easily runs\nout of the limited numerical representations. To address this issue, we devise\nQ-DiT, which seamlessly integrates three techniques: fine-grained quantization\nto manage substantial variance across input channels of weights and\nactivations, an automatic search strategy to optimize the quantization\ngranularity and mitigate redundancies, and dynamic activation quantization to\ncapture the activation changes across timesteps. Extensive experiments on the\nImageNet dataset demonstrate the effectiveness of the proposed Q-DiT.\nSpecifically, when quantizing DiT-XL/2 to W8A8 on ImageNet 256x256, Q-DiT\nachieves a remarkable reduction in FID by 1.26 compared to the baseline. Under\na W4A8 setting, it maintains high fidelity in image generation, showcasing only\na marginal increase in FID and setting a new benchmark for efficient,\nhigh-quality quantization in diffusion transformers. Code is available at\n\\href{https://github.com/Juanerx/Q-DiT}{https://github.com/Juanerx/Q-DiT}.\n']",Quantization Techniques for Diffusion Models,Quantization and Compression Techniques for Deep Learning Models,Deep Learning Optimization and Security,Deep Learning Methodologies
325,25,325_memristor_memristors_memory_memristive,"['memristor', 'memristors', 'memory', 'memristive', 'baynn', 'hardware', 'neural', 'baynns', 'bayesnns', 'spintronics']","['hardware', 'memristor', 'idealities', 'memory', 'energy', 'stochastic', 'memristors', 'memristive', 'crossbar', 'computing']","['  Internet of Things (IoT) and smart wearable devices for personalized\nhealthcare will require storing and computing ever-increasing amounts of data.\nThe key requirements for these devices are ultra-low-power, high-processing\ncapabilities, autonomy at low cost, as well as reliability and accuracy to\nenable Green AI at the edge. Artificial Intelligence (AI) models, especially\nBayesian Neural Networks (BayNNs) are resource-intensive and face challenges\nwith traditional computing architectures due to the memory wall problem.\nComputing-in-Memory (CIM) with emerging resistive memories offers a solution by\ncombining memory blocks and computing units for higher efficiency and lower\npower consumption. However, implementing BayNNs on CIM hardware, particularly\nwith spintronic technologies, presents technical challenges due to variability\nand manufacturing defects. The NeuSPIN project aims to address these challenges\nthrough full-stack hardware and software co-design, developing novel\nalgorithmic and circuit design approaches to enhance the performance,\nenergy-efficiency and robustness of BayNNs on sprintronic-based CIM platforms.\n', '  The performance of deep learning algorithms such as neural networks (NNs) has\nincreased tremendously recently, and they can achieve state-of-the-art\nperformance in many domains. However, due to memory and computation resource\nconstraints, implementing NNs on edge devices is a challenging task. Therefore,\nhardware accelerators such as computation-in-memory (CIM) with memristive\ndevices have been developed to accelerate the most common operations, i.e.,\nmatrix-vector multiplication. However, due to inherent device properties,\nexternal environmental factors such as temperature, and an immature fabrication\nprocess, memristors suffer from various non-idealities, including defects and\nvariations occurring during manufacturing and runtime. Consequently, there is a\nlack of complete confidence in the predictions made by the model. To improve\nconfidence in NN predictions made by hardware accelerators in the presence of\ndevice non-idealities, in this paper, we propose a Bayesian test vector\ngeneration framework that can estimate the model uncertainty of NNs implemented\non memristor-based CIM hardware. Compared to the conventional point estimate\ntest vector generation method, our method is more generalizable across\ndifferent model dimensions and requires storing only one test Bayesian vector\nin the hardware. Our method is evaluated on different model dimensions, tasks,\nfault rates, and variation noise to show that it can consistently achieve\n$100\\%$ coverage with only $0.024$ MB of memory overhead.\n', '  Uncertainty estimation in Neural Networks (NNs) is vital in improving\nreliability and confidence in predictions, particularly in safety-critical\napplications. Bayesian Neural Networks (BayNNs) with Dropout as an\napproximation offer a systematic approach to quantifying uncertainty, but they\ninherently suffer from high hardware overhead in terms of power, memory, and\ncomputation. Thus, the applicability of BayNNs to edge devices with limited\nresources or to high-performance applications is challenging. Some of the\ninherent costs of BayNNs can be reduced by accelerating them in hardware on a\nComputation-In-Memory (CIM) architecture with spintronic memories and\nbinarizing their parameters. However, numerous stochastic units are required to\nimplement conventional dropout-based BayNN. In this paper, we propose the Scale\nDropout, a novel regularization technique for Binary Neural Networks (BNNs),\nand Monte Carlo-Scale Dropout (MC-Scale Dropout)-based BayNNs for efficient\nuncertainty estimation. Our approach requires only one stochastic unit for the\nentire model, irrespective of the model size, leading to a highly scalable\nBayesian NN. Furthermore, we introduce a novel Spintronic memory-based CIM\narchitecture for the proposed BayNN that achieves more than $100\\times$ energy\nsavings compared to the state-of-the-art. We validated our method to show up to\na $1\\%$ improvement in predictive performance and superior uncertainty\nestimates compared to related works.\n']",Bayesian Neural Networks on Memristor-based Hardware,Bayesian Neural Networks,Probabilistic Machine Learning,Probabilistic Machine Learning
326,25,326_variational_unnormalized_inference_posteriors,"['variational', 'unnormalized', 'inference', 'posteriors', 'gradient', 'probabilistic', 'divergence', 'likelihood', 'reparameterization', 'posterior']","['variational', 'posterior', 'inference', 'amortized', 'gradient', 'families', 'approximation', 'stochastic', 'convergence', 'black']","['  Variational families with full-rank covariance approximations are known not\nto work well in black-box variational inference (BBVI), both empirically and\ntheoretically. In fact, recent computational complexity results for BBVI have\nestablished that full-rank variational families scale poorly with the\ndimensionality of the problem compared to e.g. mean-field families. This is\nparticularly critical to hierarchical Bayesian models with local variables;\ntheir dimensionality increases with the size of the datasets. Consequently, one\ngets an iteration complexity with an explicit (\\mathcal{O}(N^2)) dependence on\nthe dataset size (N). In this paper, we explore a theoretical middle ground\nbetween mean-field variational families and full-rank families: structured\nvariational families. We rigorously prove that certain scale matrix structures\ncan achieve a better iteration complexity of (\\mathcal{O}\\left(N\\right)),\nimplying better scaling with respect to (N). We empirically verify our\ntheoretical results on large-scale hierarchical models.\n', '  Estimating a distribution given access to its unnormalized density is pivotal\nin Bayesian inference, where the posterior is generally known only up to an\nunknown normalizing constant. Variational inference and Markov chain Monte\nCarlo methods are the predominant tools for this task; however, both methods\nare often challenging to apply reliably, particularly when the posterior has\ncomplex geometry. Here, we introduce Soft Contrastive Variational Inference\n(SoftCVI), which allows a family of variational objectives to be derived\nthrough a contrastive estimation framework. These objectives have zero variance\ngradient when the variational approximation is exact, without the need for\nspecialized gradient estimators. The approach involves parameterizing a\nclassifier in terms of the variational distribution, which allows the inference\ntask to be reframed as a contrastive estimation problem, aiming to identify a\nsingle true posterior sample among a set of samples. Despite this framing, we\ndo not require positive or negative samples, but rather learn by sampling the\nvariational distribution and computing ground truth soft classification labels\nfrom the unnormalized posterior itself. We empirically investigate the\nperformance on a variety of Bayesian inference tasks, using both using both\nsimple (e.g. normal) and expressive (normalizing flow) variational\ndistributions. We find that SoftCVI objectives often outperform other commonly\nused variational objectives.\n', '  We provide the first convergence guarantee for full black-box variational\ninference (BBVI), also known as Monte Carlo variational inference. While\npreliminary investigations worked on simplified versions of BBVI (e.g., bounded\ndomain, bounded support, only optimizing for the scale, and such), our setup\ndoes not need any such algorithmic modifications. Our results hold for\nlog-smooth posterior densities with and without strong log-concavity and the\nlocation-scale variational family. Also, our analysis reveals that certain\nalgorithm design choices commonly employed in practice, particularly, nonlinear\nparameterizations of the scale of the variational approximation, can result in\nsuboptimal convergence rates. Fortunately, running BBVI with proximal\nstochastic gradient descent fixes these limitations, and thus achieves the\nstrongest known convergence rate guarantees. We evaluate this theoretical\ninsight by comparing proximal SGD against other standard implementations of\nBBVI on large-scale Bayesian inference problems.\n']",Variational Inference for Bayesian Models,Variational Methods for Bayesian Modeling and Generative Learning,Generative Modeling and Artificial Intelligence,Generative Modeling and Artificial Intelligence
327,25,327_adversarial_adversary_attackers_attacks,"['adversarial', 'adversary', 'attackers', 'attacks', 'attacker', 'adversaries', 'attack', 'defenses', 'threat', 'reinforcement']","['victim', 'attack', 'attacks', 'adversarial', 'attacker', 'agent', 'agents', 'policies', 'defenses', 'reinforcement']","[""  To ensure the usefulness of Reinforcement Learning (RL) in real systems, it\nis crucial to ensure they are robust to noise and adversarial attacks. In\nadversarial RL, an external attacker has the power to manipulate the victim\nagent's interaction with the environment. We study the full class of online\nmanipulation attacks, which include (i) state attacks, (ii) observation attacks\n(which are a generalization of perceived-state attacks), (iii) action attacks,\nand (iv) reward attacks. We show the attacker's problem of designing a stealthy\nattack that maximizes its own expected reward, which often corresponds to\nminimizing the victim's value, is captured by a Markov Decision Process (MDP)\nthat we call a meta-MDP since it is not the true environment but a higher level\nenvironment induced by the attacked interaction. We show that the attacker can\nderive optimal attacks by planning in polynomial time or learning with\npolynomial sample complexity using standard RL techniques. We argue that the\noptimal defense policy for the victim can be computed as the solution to a\nstochastic Stackelberg game, which can be further simplified into a\npartially-observable turn-based stochastic game (POTBSG). Neither the attacker\nnor the victim would benefit from deviating from their respective optimal\npolicies, thus such solutions are truly robust. Although the defense problem is\nNP-hard, we show that optimal Markovian defenses can be computed (learned) in\npolynomial time (sample complexity) in many scenarios.\n"", '  Most existing works focus on direct perturbations to the victim\'s\nstate/action or the underlying transition dynamics to demonstrate the\nvulnerability of reinforcement learning agents to adversarial attacks. However,\nsuch direct manipulations may not be always realizable. In this paper, we\nconsider a multi-agent setting where a well-trained victim agent $\\nu$ is\nexploited by an attacker controlling another agent $\\alpha$ with an\n\\textit{adversarial policy}. Previous models do not account for the possibility\nthat the attacker may only have partial control over $\\alpha$ or that the\nattack may produce easily detectable ""abnormal"" behaviors. Furthermore, there\nis a lack of provably efficient defenses against these adversarial policies. To\naddress these limitations, we introduce a generalized attack framework that has\nthe flexibility to model to what extent the adversary is able to control the\nagent, and allows the attacker to regulate the state distribution shift and\nproduce stealthier adversarial policies. Moreover, we offer a provably\nefficient defense with polynomial convergence to the most robust victim policy\nthrough adversarial training with timescale separation. This stands in sharp\ncontrast to supervised learning, where adversarial training typically provides\nonly \\textit{empirical} defenses. Using the Robosumo competition experiments,\nwe show that our generalized attack formulation results in much stealthier\nadversarial policies when maintaining the same winning rate as baselines.\nAdditionally, our adversarial training approach yields stable learning dynamics\nand less exploitable victim policies.\n', ""  This study considers the attack on reinforcement learning agents where the\nadversary aims to control the victim's behavior as specified by the adversary\nby adding adversarial modifications to the victim's state observation. While\nsome attack methods reported success in manipulating the victim agent's\nbehavior, these methods often rely on environment-specific heuristics. In\naddition, all existing attack methods require white-box access to the victim's\npolicy. In this study, we propose a novel method for manipulating the victim\nagent in the black-box (i.e., the adversary is allowed to observe the victim's\nstate and action only) and no-box (i.e., the adversary is allowed to observe\nthe victim's state only) setting without requiring environment-specific\nheuristics. Our attack method is formulated as a bi-level optimization problem\nthat is reduced to a distribution matching problem and can be solved by an\nexisting imitation learning algorithm in the black-box and no-box settings.\nEmpirical evaluations on several reinforcement learning benchmarks show that\nour proposed method has superior attack performance to baselines.\n""]",Adversarial Attacks on Reinforcement Learning,Adversarial Attacks and Defenses in AI and Machine Learning Models,Adversarial Attacks and Vulnerabilities in AI and Machine Learning,Adversarial Attacks and Vulnerabilities in AI and Machine Learning
328,24,328_architectural_designs_architecture_architects,"['architectural', 'designs', 'architecture', 'architects', 'design', 'facades', 'generative', 'designers', 'designer', 'creativity']","['design', 'architectural', 'designers', 'designs', 'layout', 'schematic', 'generative', 'virtual', 'facade', 'evolutionary']","['  3D shape generation techniques leveraging deep learning have garnered\nsignificant interest from both the computer vision and architectural design\ncommunities, promising to enrich the content in the virtual environment.\nHowever, research on virtual architectural design remains limited, particularly\nregarding designer-AI collaboration and deep learning-assisted design. In our\nsurvey, we reviewed 149 related articles (81.2% of articles published between\n2019 and 2023) covering architectural design, 3D shape techniques, and virtual\nenvironments. Through scrutinizing the literature, we first identify the\nprinciples of virtual architecture and illuminate its current production\nchallenges, including datasets, multimodality, design intuition, and generative\nframeworks. We then introduce the latest approaches to designing and generating\nvirtual buildings leveraging 3D shape generation and summarize four\ncharacteristics of various approaches to virtual architecture. Based on our\nanalysis, we expound on four research agendas, including agency, communication,\nuser consideration, and integrating tools. Additionally, we highlight four\nimportant enablers of ubiquitous interaction with immersive systems in deep\nlearning-assisted architectural generation. Our work contributes to fostering\nunderstanding between designers and deep learning techniques, broadening access\nto designer-AI collaboration. We advocate for interdisciplinary efforts to\naddress this timely research topic, facilitating content designing and\ngeneration in the virtual environment.\n', '  Generative Artificial Intelligence (AI) has pioneered new methodological\nparadigms in architectural design, significantly expanding the innovative\npotential and efficiency of the design process. This paper explores the\nextensive applications of generative AI technologies in architectural design, a\ntrend that has benefited from the rapid development of deep generative models.\nThis article provides a comprehensive review of the basic principles of\ngenerative AI and large-scale models and highlights the applications in the\ngeneration of 2D images, videos, and 3D models. In addition, by reviewing the\nlatest literature from 2020, this paper scrutinizes the impact of generative AI\ntechnologies at different stages of architectural design, from generating\ninitial architectural 3D forms to producing final architectural imagery. The\nmarked trend of research growth indicates an increasing inclination within the\narchitectural design community towards embracing generative AI, thereby\ncatalyzing a shared enthusiasm for research. These research cases and\nmethodologies have not only proven to enhance efficiency and innovation\nsignificantly but have also posed challenges to the conventional boundaries of\narchitectural creativity. Finally, we point out new directions for design\ninnovation and articulate fresh trajectories for applying generative AI in the\narchitectural domain. This article provides the first comprehensive literature\nreview about generative AI for architectural design, and we believe this work\ncan facilitate more research work on this significant topic in architecture.\n', '  The determination of space layout is one of the primary activities in the\nschematic design stage of an architectural project. The initial layout planning\ndefines the shape, dimension, and circulation pattern of internal spaces; which\ncan also affect performance and cost of the construction. When carried out\nmanually, space layout planning can be complicated, repetitive and time\nconsuming. In this work, a generative design framework for the automatic\ngeneration of spatial architectural layout has been developed. The proposed\napproach integrates a novel physics-inspired parametric model for space layout\nplanning and an evolutionary optimisation metaheuristic. Results revealed that\nsuch a generative design framework can generate a wide variety of design\nsuggestions at the schematic design stage, applicable to complex design\nproblems.\n']",Generative Architectural Design,Generative Modeling Techniques,Generative Modeling and Artificial Intelligence,Generative Modeling and Artificial Intelligence
329,24,329_graphinstruct_instructgraph_graphreader_graphwiz,"['graphinstruct', 'instructgraph', 'graphreader', 'graphwiz', 'graphlm', 'graphtoken', 'graphs', 'grapheval2000', 'nlgraph', 'graphtranslator']","['graph', 'reasoning', 'graphs', 'structured', 'abilities', 'structures', 'tasks', 'prompting', 'node', 'structure']","[""  Large language models (LLMs) have achieved impressive success across several\nfields, but their proficiency in understanding and resolving complex graph\nproblems is less explored. To bridge this gap, we introduce GraphInstruct, a\nnovel and comprehensive instruction-tuning dataset designed to equip language\nmodels with the ability to tackle a broad spectrum of graph problems using\nexplicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an\nopen-source language model capable of resolving various graph problem types\nwhile generating clear reasoning processes. To enhance the model's capability\nand reliability, we incorporate the Direct Preference Optimization (DPO)\nframework into the graph problem-solving context. The enhanced model,\nGraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with\ndifferent complexity levels, surpassing GPT-4 which has an average accuracy of\n43.8%. Moreover, our research delves into the delicate balance between training\ndata volume and model performance, highlighting the potential for overfitting\nwith increased data. We also explore the transferability of the model's\nreasoning ability across different graph tasks, indicating the model's\nadaptability and practical application potential. Our investigation offers a\nnew blueprint and valuable insights for developing LLMs specialized in graph\nreasoning and problem-solving.\n"", '  Evaluating and enhancing the general capabilities of large language models\n(LLMs) has been an important research topic. Graph is a common data structure\nin the real world, and understanding graph data is a crucial part for advancing\ngeneral intelligence. To evaluate and enhance the graph understanding abilities\nof LLMs, in this paper, we propose a benchmark named GraphInstruct, which\ncomprehensively includes 21 classical graph reasoning tasks, providing diverse\ngraph generation pipelines and detailed reasoning steps. Based on\nGraphInstruct, we further construct GraphLM through efficient\ninstruction-tuning, which shows prominent graph understanding capability. In\norder to enhance the LLM with graph reasoning capability as well, we propose a\nstep mask training strategy, and construct a model named GraphLM+. As one of\nthe pioneering efforts to enhance the graph understanding and reasoning\nabilities of LLMs, extensive experiments have demonstrated the superiority of\nGraphLM and GraphLM+ over other LLMs. We look forward to more researchers\nexploring the potential of LLMs in the graph data mining domain through\nGraphInstruct. Our code for generating GraphInstruct is released publicly at:\nhttps://github.com/CGCL-codes/GraphInstruct.\n', '  Large language models (LLMs) are increasingly adopted for a variety of tasks\nwith implicit graphical structures, such as planning in robotics, multi-hop\nquestion answering or knowledge probing, structured commonsense reasoning, and\nmore. While LLMs have advanced the state-of-the-art on these tasks with\nstructure implications, whether LLMs could explicitly process textual\ndescriptions of graphs and structures, map them to grounded conceptual spaces,\nand perform structured operations remains underexplored. To this end, we\npropose NLGraph (Natural Language Graph), a comprehensive benchmark of\ngraph-based problem solving designed in natural language. NLGraph contains\n29,370 problems, covering eight graph reasoning tasks with varying complexity\nfrom simple tasks such as connectivity and shortest path up to complex problems\nsuch as maximum flow and simulating graph neural networks. We evaluate LLMs\n(GPT-3/4) with various prompting approaches on the NLGraph benchmark and find\nthat 1) language models do demonstrate preliminary graph reasoning abilities,\n2) the benefit of advanced prompting and in-context learning diminishes on more\ncomplex graph problems, while 3) LLMs are also (un)surprisingly brittle in the\nface of spurious correlations in graph and problem settings. We then propose\nBuild-a-Graph Prompting and Algorithmic Prompting, two instruction-based\napproaches to enhance LLMs in solving natural language graph problems.\nBuild-a-Graph and Algorithmic prompting improve the performance of LLMs on\nNLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to\nsolve the most complicated graph reasoning tasks in our setup with language\nmodels remains an open research question. The NLGraph benchmark and evaluation\ncode are available at https://github.com/Arthur-Heng/NLGraph.\n']",Graph Reasoning and Problem-Solving with Language Models,"Reasoning and Problem-Solving with Logic, Language, and Graphs",Artificial Intelligence and Reasoning Systems,Intelligent Systems
330,24,330_genderless_gender_genders_translations,"['genderless', 'gender', 'genders', 'translations', 'gendered', 'multilingual', 'translating', 'masculine', 'translation', 'feminine']","['gender', 'translation', 'pronoun', 'pronouns', 'bias', 'neutral', 'translations', 'gendered', 'masculine', 'neopronouns']","['  In machine translation, the problem of ambiguously gendered input has been\npointed out, where the gender of an entity is not available in the source\nsentence. To address this ambiguity issue, the task of controlled translation\nthat takes the gender of the ambiguous entity as additional input have been\nproposed. However, most existing works have only considered a simplified setup\nof one target gender for input. In this paper, we tackle controlled translation\nin a more realistic setting of inputs with multiple entities and propose\nGender-of-Entity (GoE) prompting method for LLMs. Our proposed method instructs\nthe model with fine-grained entity-level gender information to translate with\ncorrect gender inflections. By utilizing four evaluation benchmarks, we\ninvestigate the controlled translation capability of LLMs in multiple\ndimensions and find that LLMs reach state-of-the-art performance in controlled\ntranslation. Furthermore, we discover an emergence of gender interference\nphenomenon when controlling the gender of multiple entities. Finally, we\naddress the limitations of existing gender accuracy evaluation metrics and\npropose leveraging LLMs as an evaluator for gender inflection in machine\ntranslation.\n', ""  While machine translation (MT) systems have seen significant improvements, it\nis still common for translations to reflect societal biases, such as gender\nbias. Decoder-only Large Language Models (LLMs) have demonstrated potential in\nMT, albeit with performance slightly lagging behind traditional encoder-decoder\nNeural Machine Translation (NMT) systems. However, LLMs offer a unique\nadvantage: the ability to control the properties of the output through prompts.\nIn this study, we leverage this flexibility to explore LLaMa's capability to\nproduce gender-specific translations. Our results indicate that LLaMa can\ngenerate gender-specific translations with translation accuracy and gender bias\ncomparable to NLLB, a state-of-the-art multilingual NMT system. Furthermore,\nour experiments reveal that LLaMa's gender-specific translations rely on\ncoreference resolution to determine gender, showing higher gender variance in\ngender-ambiguous datasets but maintaining consistency in less ambiguous\ncontexts. This research investigates the potential and challenges of using LLMs\nfor gender-specific translations as an instance of the controllability of\noutputs offered by LLMs.\n"", '  Gender bias has been a focal point in the study of bias in machine\ntranslation and language models. Existing machine translation gender bias\nevaluations are primarily focused on male and female genders, limiting the\nscope of the evaluation. To assess gender bias accurately, these studies often\nrely on calculating the accuracy of gender pronouns or the masculine and\nfeminine attributes of grammatical gender via the stereotypes triggered by\noccupations or sentiment words ({\\em i.e.}, clear positive or negative\nattitude), which cannot extend to non-binary groups. This study presents a\nbenchmark AmbGIMT (Gender-Inclusive Machine Translation with Ambiguous attitude\nwords), which assesses gender bias beyond binary gender. Meanwhile, we propose\na novel process to evaluate gender bias based on the Emotional Attitude Score\n(EAS), which is used to quantify ambiguous attitude words. In evaluating three\nrecent and effective open-source LLMs and one powerful multilingual\ntranslation-specific model, our main observations are: (1) The translation\nperformance within non-binary gender contexts is markedly inferior in terms of\ntranslation quality and exhibits more negative attitudes than binary-gender\ncontexts. (2) The analysis experiments indicate that incorporating constraint\ncontext in prompts for gender identity terms can substantially reduce\ntranslation bias, while the bias remains evident despite the presence of the\nconstraints. The code is publicly available at\n\\url{https://github.com/pppa2019/ambGIMT}.\n']",Gender in Machine Translation,Natural Language Processing for Text Analysis and Generation,Natural Language Processing,Natural Language Processing
331,24,331_ai_intelligence_decisions_explanations,"['ai', 'intelligence', 'decisions', 'explanations', 'assisted', 'misrepresentations', 'behavior', 'strategies', 'predictions', 'artificial']","['decision', 'making', 'assistance', 'reliance', 'human', 'advice', 'misrepresentations', 'complementarity', 'umpires', 'assistances']","[""  Humans frequently make decisions with the aid of artificially intelligent\n(AI) systems. A common pattern is for the AI to recommend an action to the\nhuman who retains control over the final decision. Researchers have identified\nensuring that a human has appropriate reliance on an AI as a critical component\nof achieving complementary performance. We argue that the current definition of\nappropriate reliance used in such research lacks formal statistical grounding\nand can lead to contradictions. We propose a formal definition of reliance,\nbased on statistical decision theory, which separates the concepts of reliance\nas the probability the decision-maker follows the AI's recommendation from\nchallenges a human may face in differentiating the signals and forming accurate\nbeliefs about the situation. Our definition gives rise to a framework that can\nbe used to guide the design and interpretation of studies on human-AI\ncomplementarity and reliance. Using recent AI-advised decision making studies\nfrom literature, we demonstrate how our framework can be used to separate the\nloss due to mis-reliance from the loss due to not accurately differentiating\nthe signals. We evaluate these losses by comparing to a baseline and a\nbenchmark for complementary performance defined by the expected payoff achieved\nby a rational decision-maker facing the same decision task as the behavioral\ndecision-makers.\n"", '  Imagine if AI decision-support tools not only complemented our ability to\nmake accurate decisions, but also improved our skills, boosted collaboration,\nand elevated the joy we derive from our tasks. Despite the potential to\noptimize a broad spectrum of such human-centric objectives, the design of\ncurrent AI tools remains focused on decision accuracy alone. We propose offline\nreinforcement learning (RL) as a general approach for modeling human-AI\ndecision-making to optimize human-AI interaction for diverse objectives. RL can\noptimize such objectives by tailoring decision support, providing the right\ntype of assistance to the right person at the right time. We instantiated our\napproach with two objectives: human-AI accuracy on the decision-making task and\nhuman learning about the task and learned decision support policies from\nprevious human-AI interaction data. We compared the optimized policies against\nseveral baselines in AI-assisted decision-making. Across two experiments (N=316\nand N=964), our results demonstrated that people interacting with policies\noptimized for accuracy achieve significantly better accuracy -- and even\nhuman-AI complementarity -- compared to those interacting with any other type\nof AI support. Our results further indicated that human learning was more\ndifficult to optimize than accuracy, with participants who interacted with\nlearning-optimized policies showing significant learning improvement only at\ntimes. Our research (1) demonstrates offline RL to be a promising approach to\nmodel human-AI decision-making, leading to policies that may optimize\nhuman-centric objectives and provide novel insights about the AI-assisted\ndecision-making space, and (2) emphasizes the importance of considering\nhuman-centric objectives beyond decision accuracy in AI-assisted\ndecision-making, opening up the novel research challenge of optimizing human-AI\ninteraction for such objectives.\n', ""  With the rapid development of AI-based decision aids, different forms of AI\nassistance have been increasingly integrated into the human decision making\nprocesses. To best support humans in decision making, it is essential to\nquantitatively understand how diverse forms of AI assistance influence humans'\ndecision making behavior. To this end, much of the current research focuses on\nthe end-to-end prediction of human behavior using ``black-box'' models, often\nlacking interpretations of the nuanced ways in which AI assistance impacts the\nhuman decision making process. Meanwhile, methods that prioritize the\ninterpretability of human behavior predictions are often tailored for one\nspecific form of AI assistance, making adaptations to other forms of assistance\ndifficult. In this paper, we propose a computational framework that can provide\nan interpretable characterization of the influence of different forms of AI\nassistance on decision makers in AI-assisted decision making. By\nconceptualizing AI assistance as the ``{\\em nudge}'' in human decision making\nprocesses, our approach centers around modelling how different forms of AI\nassistance modify humans' strategy in weighing different information in making\ntheir decisions. Evaluations on behavior data collected from real human\ndecision makers show that the proposed framework outperforms various baselines\nin accurately predicting human behavior in AI-assisted decision making. Based\non the proposed framework, we further provide insights into how individuals\nwith different cognitive styles are nudged by AI assistance differently.\n""]",Human-AI Decision Making and Assistance,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
331,24,331_ai_intelligence_decisions_explanations,"['ai', 'intelligence', 'decisions', 'explanations', 'assisted', 'misrepresentations', 'behavior', 'strategies', 'predictions', 'artificial']","['decision', 'making', 'assistance', 'reliance', 'human', 'advice', 'misrepresentations', 'complementarity', 'umpires', 'assistances']","[""  Humans frequently make decisions with the aid of artificially intelligent\n(AI) systems. A common pattern is for the AI to recommend an action to the\nhuman who retains control over the final decision. Researchers have identified\nensuring that a human has appropriate reliance on an AI as a critical component\nof achieving complementary performance. We argue that the current definition of\nappropriate reliance used in such research lacks formal statistical grounding\nand can lead to contradictions. We propose a formal definition of reliance,\nbased on statistical decision theory, which separates the concepts of reliance\nas the probability the decision-maker follows the AI's recommendation from\nchallenges a human may face in differentiating the signals and forming accurate\nbeliefs about the situation. Our definition gives rise to a framework that can\nbe used to guide the design and interpretation of studies on human-AI\ncomplementarity and reliance. Using recent AI-advised decision making studies\nfrom literature, we demonstrate how our framework can be used to separate the\nloss due to mis-reliance from the loss due to not accurately differentiating\nthe signals. We evaluate these losses by comparing to a baseline and a\nbenchmark for complementary performance defined by the expected payoff achieved\nby a rational decision-maker facing the same decision task as the behavioral\ndecision-makers.\n"", '  Imagine if AI decision-support tools not only complemented our ability to\nmake accurate decisions, but also improved our skills, boosted collaboration,\nand elevated the joy we derive from our tasks. Despite the potential to\noptimize a broad spectrum of such human-centric objectives, the design of\ncurrent AI tools remains focused on decision accuracy alone. We propose offline\nreinforcement learning (RL) as a general approach for modeling human-AI\ndecision-making to optimize human-AI interaction for diverse objectives. RL can\noptimize such objectives by tailoring decision support, providing the right\ntype of assistance to the right person at the right time. We instantiated our\napproach with two objectives: human-AI accuracy on the decision-making task and\nhuman learning about the task and learned decision support policies from\nprevious human-AI interaction data. We compared the optimized policies against\nseveral baselines in AI-assisted decision-making. Across two experiments (N=316\nand N=964), our results demonstrated that people interacting with policies\noptimized for accuracy achieve significantly better accuracy -- and even\nhuman-AI complementarity -- compared to those interacting with any other type\nof AI support. Our results further indicated that human learning was more\ndifficult to optimize than accuracy, with participants who interacted with\nlearning-optimized policies showing significant learning improvement only at\ntimes. Our research (1) demonstrates offline RL to be a promising approach to\nmodel human-AI decision-making, leading to policies that may optimize\nhuman-centric objectives and provide novel insights about the AI-assisted\ndecision-making space, and (2) emphasizes the importance of considering\nhuman-centric objectives beyond decision accuracy in AI-assisted\ndecision-making, opening up the novel research challenge of optimizing human-AI\ninteraction for such objectives.\n', ""  With the rapid development of AI-based decision aids, different forms of AI\nassistance have been increasingly integrated into the human decision making\nprocesses. To best support humans in decision making, it is essential to\nquantitatively understand how diverse forms of AI assistance influence humans'\ndecision making behavior. To this end, much of the current research focuses on\nthe end-to-end prediction of human behavior using ``black-box'' models, often\nlacking interpretations of the nuanced ways in which AI assistance impacts the\nhuman decision making process. Meanwhile, methods that prioritize the\ninterpretability of human behavior predictions are often tailored for one\nspecific form of AI assistance, making adaptations to other forms of assistance\ndifficult. In this paper, we propose a computational framework that can provide\nan interpretable characterization of the influence of different forms of AI\nassistance on decision makers in AI-assisted decision making. By\nconceptualizing AI assistance as the ``{\\em nudge}'' in human decision making\nprocesses, our approach centers around modelling how different forms of AI\nassistance modify humans' strategy in weighing different information in making\ntheir decisions. Evaluations on behavior data collected from real human\ndecision makers show that the proposed framework outperforms various baselines\nin accurately predicting human behavior in AI-assisted decision making. Based\non the proposed framework, we further provide insights into how individuals\nwith different cognitive styles are nudged by AI assistance differently.\n""]",Human-AI Decision Making and Assistance,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
332,24,332_copyrightability_copyright_copyrights_copyrighted,"['copyrightability', 'copyright', 'copyrights', 'copyrighted', 'infringing', 'infringement', 'copying', 'creators', 'ai', 'generative']","['copyright', 'generative', 'infringement', 'originality', 'creators', 'asset', 'protection', 'intellectual', 'rights', 'owners']","['  In the rapidly evolving landscape of generative artificial intelligence (AI),\nthe increasingly pertinent issue of copyright infringement arises as AI\nadvances to generate content from scraped copyrighted data, prompting questions\nabout ownership and protection that impact professionals across various\ncareers. With this in mind, this survey provides an extensive examination of\ncopyright infringement as it pertains to generative AI, aiming to stay abreast\nof the latest developments and open problems. Specifically, it will first\noutline methods of detecting copyright infringement in mediums such as text,\nimage, and video. Next, it will delve an exploration of existing techniques\naimed at safeguarding copyrighted works from generative models. Furthermore,\nthis survey will discuss resources and tools for users to evaluate copyright\nviolations. Finally, insights into ongoing regulations and proposals for AI\nwill be explored and compared. Through combining these disciplines, the\nimplications of AI-driven content and copyright are thoroughly illustrated and\nbrought into question.\n', '  This paper addresses the contentious issue of copyright infringement in\nimages generated by text-to-image models, sparking debates among AI developers,\ncontent creators, and legal entities. State-of-the-art models create\nhigh-quality content without crediting original creators, causing concern in\nthe artistic community. To mitigate this, we propose the \\copyright Plug-in\nAuthorization framework, introducing three operations: addition, extraction,\nand combination. Addition involves training a \\copyright plug-in for specific\ncopyright, facilitating proper credit attribution. Extraction allows creators\nto reclaim copyright from infringing models, and combination enables users to\nmerge different \\copyright plug-ins. These operations act as permits,\nincentivizing fair use and providing flexibility in authorization. We present\ninnovative approaches,""Reverse LoRA"" for extraction and ""EasyMerge"" for\nseamless combination. Experiments in artist-style replication and cartoon IP\nrecreation demonstrate \\copyright plug-ins\' effectiveness, offering a valuable\nsolution for human copyright protection in the age of generative AIs.\n', '  The advent of Generative Artificial Intelligence (GenAI) models, including\nGitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content\ncreation, enabling non-professionals to produce high-quality content across\nvarious domains. This transformative technology has led to a surge of synthetic\ncontent and sparked legal disputes over copyright infringement. To address\nthese challenges, this paper introduces a novel approach that leverages the\nlearning capacity of GenAI models for copyright legal analysis, demonstrated\nwith GPT2 and Stable Diffusion models. Copyright law distinguishes between\noriginal expressions and generic ones (Sc\\`enes \\`a faire), protecting the\nformer and permitting reproduction of the latter. However, this distinction has\nhistorically been challenging to make consistently, leading to over-protection\nof copyrighted works. GenAI offers an unprecedented opportunity to enhance this\nlegal analysis by revealing shared patterns in preexisting works. We propose a\ndata-driven approach to identify the genericity of works created by GenAI,\nemploying ""data-driven bias"" to assess the genericity of expressive\ncompositions. This approach aids in copyright scope determination by utilizing\nthe capabilities of GenAI to identify and prioritize expressive elements and\nrank them according to their frequency in the model\'s dataset. The potential\nimplications of measuring expressive genericity for copyright law are profound.\nSuch scoring could assist courts in determining copyright scope during\nlitigation, inform the registration practices of Copyright Offices, allowing\nregistration of only highly original synthetic works, and help copyright owners\nsignal the value of their works and facilitate fairer licensing deals. More\ngenerally, this approach offers valuable insights to policymakers grappling\nwith adapting copyright law to the challenges posed by the era of GenAI.\n']",Copyright Infringement in Generative AI,Artificial Intelligence in Creative Industries,Artificial Intelligence in Creative Industries,Artificial Intelligence in Creative Industries
333,24,333_gpus_scheduling_gpu_cluster,"['gpus', 'scheduling', 'gpu', 'cluster', 'clusters', 'scheduler', 'processors', 'schedulers', 'schedulability', 'schedule']","['scheduling', 'accelerators', 'parallelism', 'jobs', 'scheduler', 'workloads', 'partitioning', 'job', 'cluster', 'gang']","['  GPU-based heterogeneous architectures are now commonly used in HPC clusters.\nDue to their architectural simplicity specialized for data-level parallelism,\nGPUs can offer much higher computational throughput and memory bandwidth than\nCPUs in the same generation do. However, as the available resources in GPUs\nhave increased exponentially over the past decades, it has become increasingly\ndifficult for a single program to fully utilize them. As a consequence, the\nindustry has started supporting several resource partitioning features in order\nto improve the resource utilization by co-scheduling multiple programs on the\nsame GPU die at the same time. Driven by the technological trend, this paper\nfocuses on hierarchical resource partitioning on modern GPUs, and as an\nexample, we utilize a combination of two different features available on recent\nNVIDIA GPUs in a hierarchical manner: MPS (Multi-Process Service), a\nfiner-grained logical partitioning; and MIG (Multi-Instance GPU), a\ncoarse-grained physical partitioning. We propose a method for comprehensively\nco-optimizing the setup of hierarchical partitioning and the selection of\nco-scheduling groups from a given set of jobs, based on reinforcement learning\nusing their profiles. Our thorough experimental results demonstrate that our\napproach can successfully set up job concurrency, partitioning, and\nco-scheduling group selections simultaneously. This results in a maximum\nthroughput improvement by a factor of 1.87 compared to the time-sharing\nscheduling.\n', '  Joint consideration of scheduling and adaptive parallelism offers great\nopportunities for improving the training efficiency of large models on\nheterogeneous GPU clusters. However, integrating adaptive parallelism into a\ncluster scheduler expands the cluster scheduling space. The new space is the\nproduct of the original scheduling space and the parallelism exploration space\nof adaptive parallelism (also a product of pipeline, data, and tensor\nparallelism). The exponentially enlarged scheduling space and ever-changing\noptimal parallelism plan from adaptive parallelism together result in the\ncontradiction between low-overhead and accurate performance data acquisition\nfor efficient cluster scheduling. This paper presents Crius, a training system\nfor efficiently scheduling multiple large models with adaptive parallelism in a\nheterogeneous cluster. Crius proposes a novel scheduling granularity called\nCell. It represents a job with deterministic resources and pipeline stages. The\nexploration space of Cell is shrunk to the product of only data and tensor\nparallelism, thus exposing the potential for accurate and low-overhead\nperformance estimation. Crius then accurately estimates Cells and efficiently\nschedules training jobs. When a Cell is selected as a scheduling choice, its\nrepresented job runs with the optimal parallelism plan explored. Experimental\nresults show that Crius reduces job completion time by up to 48.9% and\nschedules large models with up to 1.49x cluster throughput improvement.\n', '  Training large-scale models relies on a vast number of computing resources.\nFor example, training the GPT-4 model (1.8 trillion parameters) requires 25000\nA100 GPUs . It is a challenge to build a large-scale cluster with one type of\nGPU-accelerator. Using multiple types of GPU-accelerators to construct a\nlarge-scale cluster is an effective way to solve the problem of insufficient\nhomogeneous GPU-accelerators. However, the existing distributed training\nsystems for large-scale models only support homogeneous GPU-accelerators, not\nsupport heterogeneous GPU-accelerators. To address the problem, this paper\nproposes a distributed training system with hybrid parallelism, HETHUB, for\nlarge-scale models, which supports heterogeneous cluster, including AMD, Nvidia\nGPU and other types of GPU-accelerators . It introduces a distributed unified\ncommunicator to realize the communication between heterogeneous\nGPU-accelerators, a distributed performance predictor, and an automatic\nparallel planner to develop and train models efficiently with heterogeneous\nGPU-accelerators. Compared to the distributed training system with homogeneous\nGPU-accelerators, our system can support six combinations of heterogeneous\nGPU-accelerators. We train the Llama-140B model on a heterogeneous cluster with\n768 GPU-accelerators(128 AMD and 640 GPU-accelerator A). The experiment results\nshow that the optimal performance of our system in the heterogeneous cluster\nhas achieved up to 97.49% of the theoretical upper bound performance.\n']",GPU Scheduling in Heterogeneous Clusters,Advanced Scheduling Techniques for Efficient Resource Utilization,Optimization and Management of Complex Systems,Optimization and Decision Making in Complex Systems
334,24,334_contamination_language_corpora_contaminated,"['contamination', 'language', 'corpora', 'contaminated', 'evading', 'benchmark', 'models', 'benchmarks', 'data', 'detection']","['contamination', 'contaminated', 'benchmarks', 'inflated', 'evaluation', 'benchmark', 'partition', 'quiz', 'issue', 'test']","[""  Recent statements about the impressive capabilities of large language models\n(LLMs) are usually supported by evaluating on open-access benchmarks.\nConsidering the vast size and wide-ranging sources of LLMs' training data, it\ncould explicitly or implicitly include test data, leading to LLMs being more\nsusceptible to data contamination. However, due to the opacity of training\ndata, the black-box access of models, and the rapid growth of synthetic\ntraining data, detecting and mitigating data contamination for LLMs faces\nsignificant challenges. In this paper, we propose CDD, which stands for\nContamination Detection via output Distribution for LLMs. CDD necessitates only\nthe sampled texts to detect data contamination, by identifying the peakedness\nof LLM's output distribution. To mitigate the impact of data contamination in\nevaluation, we also present TED: Trustworthy Evaluation via output\nDistribution, based on the correction of LLM's output distribution. To\nfacilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval,\nfor data contamination detection and contamination mitigation evaluation tasks.\nExtensive experimental results show that CDD achieves the average relative\nimprovements of 21.8\\%-30.2\\% over other contamination detection approaches in\nterms of Accuracy, F1 Score, and AUC metrics, and can effectively detect\nimplicit contamination. TED substantially mitigates performance improvements up\nto 66.9\\% attributed to data contamination across various contamination setups.\nIn real-world applications, we reveal that ChatGPT exhibits a high potential to\nsuffer from data contamination on HumanEval benchmark.\n"", '  Data contamination in model evaluation has become increasingly prevalent with\nthe growing popularity of large language models. It allows models to ""cheat""\nvia memorisation instead of displaying true capabilities. Therefore,\ncontamination analysis has become an crucial part of reliable model evaluation\nto validate results. However, existing contamination analysis is usually\nconducted internally by large language model developers and often lacks\ntransparency and completeness. This paper presents an extensive data\ncontamination report for over 15 popular large language models across six\npopular multiple-choice QA benchmarks. We also introduce an open-source\npipeline that enables the community to perform contamination analysis on\ncustomised data and models. Our experiments reveal varying contamination levels\nranging from 1\\% to 45\\% across benchmarks, with the contamination degree\nincreasing rapidly over time. Performance analysis of large language models\nindicates that data contamination does not necessarily lead to increased model\nmetrics: while significant accuracy boosts of up to 14\\% and 7\\% are observed\non contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is\nnoted on contaminated MMLU. We also find larger models seem able to gain more\nadvantages than smaller models on contaminated test sets.\n', ""  Language models pre-trained on web-scale corpora demonstrate impressive\ncapabilities on diverse downstream tasks. However, there is increasing concern\nwhether such capabilities might arise from evaluation datasets being included\nin the pre-training corpus -- a phenomenon known as \\textit{data contamination}\n-- in a manner that artificially increases performance. There has been little\nunderstanding of how this potential contamination might influence LMs'\nperformance on downstream tasks. In this paper, we explore the impact of data\ncontamination at the pre-training stage by pre-training a series of GPT-2\nmodels \\textit{from scratch}. We highlight the effect of both text\ncontamination (\\textit{i.e.}\\ input text of the evaluation samples) and\nground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and\nthe desired outputs) from evaluation data. We also investigate the effects of\nrepeating contamination for various downstream tasks. Additionally, we examine\nthe prevailing n-gram-based definitions of contamination within current LLM\nreports, pinpointing their limitations and inadequacy. Our findings offer new\ninsights into data contamination's effects on language model capabilities and\nunderscore the need for independent, comprehensive contamination assessments in\nLLM studies.\n""]",Data Contamination in Language Models,"Large Language Models: Safety, Security, and Ethics",Large Language Models,Large Language Models
334,24,334_contamination_language_corpora_contaminated,"['contamination', 'language', 'corpora', 'contaminated', 'evading', 'benchmark', 'models', 'benchmarks', 'data', 'detection']","['contamination', 'contaminated', 'benchmarks', 'inflated', 'evaluation', 'benchmark', 'partition', 'quiz', 'issue', 'test']","[""  Recent statements about the impressive capabilities of large language models\n(LLMs) are usually supported by evaluating on open-access benchmarks.\nConsidering the vast size and wide-ranging sources of LLMs' training data, it\ncould explicitly or implicitly include test data, leading to LLMs being more\nsusceptible to data contamination. However, due to the opacity of training\ndata, the black-box access of models, and the rapid growth of synthetic\ntraining data, detecting and mitigating data contamination for LLMs faces\nsignificant challenges. In this paper, we propose CDD, which stands for\nContamination Detection via output Distribution for LLMs. CDD necessitates only\nthe sampled texts to detect data contamination, by identifying the peakedness\nof LLM's output distribution. To mitigate the impact of data contamination in\nevaluation, we also present TED: Trustworthy Evaluation via output\nDistribution, based on the correction of LLM's output distribution. To\nfacilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval,\nfor data contamination detection and contamination mitigation evaluation tasks.\nExtensive experimental results show that CDD achieves the average relative\nimprovements of 21.8\\%-30.2\\% over other contamination detection approaches in\nterms of Accuracy, F1 Score, and AUC metrics, and can effectively detect\nimplicit contamination. TED substantially mitigates performance improvements up\nto 66.9\\% attributed to data contamination across various contamination setups.\nIn real-world applications, we reveal that ChatGPT exhibits a high potential to\nsuffer from data contamination on HumanEval benchmark.\n"", '  Data contamination in model evaluation has become increasingly prevalent with\nthe growing popularity of large language models. It allows models to ""cheat""\nvia memorisation instead of displaying true capabilities. Therefore,\ncontamination analysis has become an crucial part of reliable model evaluation\nto validate results. However, existing contamination analysis is usually\nconducted internally by large language model developers and often lacks\ntransparency and completeness. This paper presents an extensive data\ncontamination report for over 15 popular large language models across six\npopular multiple-choice QA benchmarks. We also introduce an open-source\npipeline that enables the community to perform contamination analysis on\ncustomised data and models. Our experiments reveal varying contamination levels\nranging from 1\\% to 45\\% across benchmarks, with the contamination degree\nincreasing rapidly over time. Performance analysis of large language models\nindicates that data contamination does not necessarily lead to increased model\nmetrics: while significant accuracy boosts of up to 14\\% and 7\\% are observed\non contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is\nnoted on contaminated MMLU. We also find larger models seem able to gain more\nadvantages than smaller models on contaminated test sets.\n', ""  Language models pre-trained on web-scale corpora demonstrate impressive\ncapabilities on diverse downstream tasks. However, there is increasing concern\nwhether such capabilities might arise from evaluation datasets being included\nin the pre-training corpus -- a phenomenon known as \\textit{data contamination}\n-- in a manner that artificially increases performance. There has been little\nunderstanding of how this potential contamination might influence LMs'\nperformance on downstream tasks. In this paper, we explore the impact of data\ncontamination at the pre-training stage by pre-training a series of GPT-2\nmodels \\textit{from scratch}. We highlight the effect of both text\ncontamination (\\textit{i.e.}\\ input text of the evaluation samples) and\nground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and\nthe desired outputs) from evaluation data. We also investigate the effects of\nrepeating contamination for various downstream tasks. Additionally, we examine\nthe prevailing n-gram-based definitions of contamination within current LLM\nreports, pinpointing their limitations and inadequacy. Our findings offer new\ninsights into data contamination's effects on language model capabilities and\nunderscore the need for independent, comprehensive contamination assessments in\nLLM studies.\n""]",Data Contamination in Language Models,"Large Language Models: Safety, Security, and Ethics",Large Language Models,Large Language Models
335,24,335_checkpointing_checkpoint_checkpoints_backup,"['checkpointing', 'checkpoint', 'checkpoints', 'backup', 'queue', 'storage', 'writes', 'parallelism', 'saving', 'persistent']","['checkpoint', 'checkpointing', 'checkpoints', 'storage', 'loading', 'fault', 'tolerance', 'failures', 'parallelism', 'prefill']","['  As large language models continue to scale up, the imperative for fault\ntolerance in distributed deep learning systems intensifies, becoming a focal\narea of AI infrastructure research. Checkpoint has emerged as the predominant\nfault tolerance strategy, with extensive studies dedicated to optimizing its\nefficiency. However, the advent of the sparse Mixture-of-Experts (MoE) model\npresents new challenges for traditional checkpoint techniques due to the\nsubstantial increase in model size, despite comparable computational demands to\ndense models. Breaking new ground in the realm of efficient fault tolerance for\nMoE model training, we introduce a novel Partial Experts Checkpoint (PEC)\nmechanism alongside a corresponding PEC fault-tolerant system. Our approach\nstrategically checkpoints a selected subset of experts, thereby significantly\nreducing the checkpoint size for MoE models to a level comparable with that of\ndense models. The empirical analysis on our 8-expert GPT-MoE model demonstrates\nthat the proposed PEC approach facilitates a substantial 54.2% decrease in the\nsize of non-redundant checkpoint (no data-parallel duplication), without\ncompromising the final model quality. Moreover, our PEC fault-tolerant system\nachieves a 76.9% reduction in checkpoint workload per data-parallel distributed\nrank, thereby correspondingly diminishing the checkpointing time and\nfacilitating complete overlap with the training process.\n', '  Existing checkpointing approaches seem ill-suited for distributed training\neven though hardware limitations make model parallelism, i.e., sharding model\nstate across multiple accelerators, a requirement for model scaling.\nConsolidating distributed model state into a single checkpoint unacceptably\nslows down training, and is impractical at extreme scales. Distributed\ncheckpoints, in contrast, are tightly coupled to the model parallelism and\nhardware configurations of the training run, and thus unusable on different\nconfigurations. To address this problem, we propose Universal Checkpointing, a\ntechnique that enables efficient checkpoint creation while providing the\nflexibility of resuming on arbitrary parallelism strategy and hardware\nconfigurations. Universal Checkpointing unlocks unprecedented capabilities for\nlarge-scale training such as improved resilience to hardware failures through\ncontinued training on remaining healthy hardware, and reduced training time\nthrough opportunistic exploitation of elastic capacity.\n  The key insight of Universal Checkpointing is the selection of the optimal\nrepresentation in each phase of the checkpointing life cycle: distributed\nrepresentation for saving, and consolidated representation for loading. This is\nachieved using two key mechanisms. First, the universal checkpoint format,\nwhich consists of a consolidated representation of each model parameter and\nmetadata for mapping parameter fragments into training ranks of arbitrary\nmodel-parallelism configuration. Second, the universal checkpoint language, a\nsimple but powerful specification language for converting distributed\ncheckpoints into the universal checkpoint format. Our evaluation demonstrates\nthe effectiveness and generality of Universal Checkpointing on state-of-the-art\nmodel architectures and a wide range of parallelism techniques.\n', ""  The development of real-world Large Language Models (LLMs) necessitates\ncheckpointing of training states in persistent storage to mitigate potential\nsoftware and hardware failures, as well as to facilitate checkpoint\ntransferring within the training pipeline and across various tasks. Due to the\nimmense size of LLMs, saving and loading checkpoints often incur intolerable\nminute-level stalls, significantly diminishing training efficiency. Besides,\nwhen transferring checkpoints across tasks, checkpoint resharding, defined as\nloading checkpoints into parallel configurations differing from those used for\nsaving, is often required according to the characteristics and resource quota\nof specific tasks. Previous checkpointing systems [16,3,33,6] assume consistent\nparallel configurations, failing to address the complexities of checkpoint\ntransformation during resharding. Furthermore, in the industry platform,\ndevelopers create checkpoints from different training frameworks[23,36,21,11],\neach with its own unique storage and I/O logic. This diversity complicates the\nimplementation of unified checkpoint management and optimization. To address\nthese challenges, we introduce ByteCheckpoint, a PyTorch-native multi-framework\nLLM checkpointing system that supports automatic online checkpoint resharding.\nByteCheckpoint employs a data/metadata disaggregated storage architecture,\ndecoupling checkpoint storage from the adopted parallelism strategies and\ntraining frameworks. We design an efficient asynchronous tensor merging\ntechnique to settle the irregular tensor sharding problem and propose several\nI/O performance optimizations to significantly enhance the efficiency of\ncheckpoint saving and loading. Experimental results demonstrate\nByteCheckpoint's substantial advantages in reducing checkpoint saving (by up to\n529.22X) and loading (by up to 3.51X) costs, compared to baseline methods.\n""]",Efficient Checkpointing for Large-Scale Deep Learning,Deep Learning Security and Efficiency,Deep Learning Optimization and Security,Deep Learning Methodologies
336,24,336_ai_explainers_explanations_explainability,"['ai', 'explainers', 'explanations', 'explainability', 'understandability', 'dialogue', 'conversational', 'explainer', 'conversation', 'dialogues']","['explanations', 'explanation', 'explainable', 'users', 'explainees', 'explainee', 'user', 'intelligence', 'artificial', 'interaction']","['  Explainable Artificial Intelligence (XAI) aims to improve the transparency of\nautonomous decision-making through explanations. Recent literature has\nemphasised users\' need for holistic ""multi-shot"" explanations and the ability\nto personalise their engagement with XAI systems. We refer to this user-centred\ninteraction as an XAI Experience. Despite advances in creating XAI experiences,\nevaluating them in a user-centred manner has remained challenging. To address\nthis, we introduce the XAI Experience Quality (XEQ) Scale (pronounced ""Seek""\nScale), for evaluating the user-centred quality of XAI experiences.\nFurthermore, XEQ quantifies the quality of experiences across four evaluation\ndimensions: learning, utility, fulfilment and engagement. These contributions\nextend the state-of-the-art of XAI evaluation, moving beyond the\none-dimensional metrics frequently developed to assess single-shot\nexplanations. In this paper, we present the XEQ scale development and\nvalidation process, including content validation with XAI experts as well as\ndiscriminant and construct validation through a large-scale pilot study. Out\npilot study results offer strong evidence that establishes the XEQ Scale as a\ncomprehensive framework for evaluating user-centred XAI experiences.\n', ""  The goal of Explainable AI (XAI) is to design methods to provide insights\ninto the reasoning process of black-box models, such as deep neural networks,\nin order to explain them to humans. Social science research states that such\nexplanations should be conversational, similar to human-to-human explanations.\nIn this work, we show how to incorporate XAI in a conversational agent, using a\nstandard design for the agent comprising natural language understanding and\ngeneration components. We build upon an XAI question bank, which we extend by\nquality-controlled paraphrases, to understand the user's information needs. We\nfurther systematically survey the literature for suitable explanation methods\nthat provide the information to answer those questions, and present a\ncomprehensive list of suggestions. Our work is the first step towards truly\nnatural conversations about machine learning models with an explanation agent.\nThe comprehensive list of XAI questions and the corresponding explanation\nmethods may support other researchers in providing the necessary information to\naddress users' demands. To facilitate future work, we release our source code\nand data.\n"", ""  The field of eXplainable Artificial Intelligence (XAI) is increasingly\nrecognizing the need to personalize and/or interactively adapt the explanation\nto better reflect users' explanation needs. While dialogue-based approaches to\nXAI have been proposed recently, the state-of-the-art in XAI is still\ncharacterized by what we call one-shot, non-personalized and one-way\nexplanations. In contrast, dialogue-based systems that can adapt explanations\nthrough interaction with a user promise to be superior to GUI-based or\ndashboard explanations as they offer a more intuitive way of requesting\ninformation. In general, while interactive XAI systems are often evaluated in\nterms of user satisfaction, there are limited studies that access user's\nobjective model understanding. This is in particular the case for\ndialogue-based XAI approaches. In this paper, we close this gap by carrying out\ncontrolled experiments within a dialogue framework in which we measure\nunderstanding of users in three phases by asking them to simulate the\npredictions of the model they are learning about. By this, we can quantify the\nlevel of (improved) understanding w.r.t. how the model works, comparing the\nstate prior, and after the interaction. We further analyze the data to reveal\npatterns of how the interaction between groups with high vs. low understanding\ngain differ. Overall, our work thus contributes to our understanding about the\neffectiveness of XAI approaches.\n""]",Explainable AI (XAI) and Conversational Explanations,Explainable Artificial Intelligence (XAI),Artificial Intelligence and Machine Learning Interpretability and Explainability,Explainable AI and Machine Learning
337,23,337_twins_twinning_twin_digital,"['twins', 'twinning', 'twin', 'digital', 'cyber', 'ai', 'cybersecurity', 'network', 'replicas', 'technologies']","['twins', 'digital', 'twin', 'twinning', 'wireless', 'physical', 'maintenance', 'technologies', 'assets', 'biotechnology']","['  Digital twin, which enables emulation, evaluation, and optimization of\nphysical entities through synchronized digital replicas, has gained increasing\nattention as a promising technology for intricate wireless networks. For 6G,\nnumerous innovative wireless technologies and network architectures have posed\nnew challenges in establishing wireless network digital twins. To tackle these\nchallenges, artificial intelligence (AI), particularly the flourishing\ngenerative AI, emerges as a potential solution. In this article, we discuss\nemerging prerequisites for wireless network digital twins considering the\ncomplicated network architecture, tremendous network scale, extensive coverage,\nand diversified application scenarios in the 6G era. We further explore the\napplications of generative AI, such as Transformer and diffusion model, to\nempower the 6G digital twin from multiple perspectives including\nphysical-digital modeling, synchronization, and slicing capability.\nSubsequently, we propose a hierarchical generative AI-enabled wireless network\ndigital twin at both the message-level and policy-level, and provide a typical\nuse case with numerical results to validate the effectiveness and efficiency.\nFinally, open research issues for wireless network digital twins in the 6G era\nare discussed.\n', ""  The potential of digital twin technology is yet to be fully realized due to\nits diversity and untapped potential. Digital twins enable systems' analysis,\ndesign, optimization, and evolution to be performed digitally or in conjunction\nwith a cyber-physical approach to improve speed, accuracy, and efficiency over\ntraditional engineering methods. Industry 4.0, factories of the future, and\ndigital twins continue to benefit from the technology and provide enhanced\nefficiency within existing systems. Due to the lack of information and security\nstandards associated with the transition to cyber digitization, cybercriminals\nhave been able to take advantage of the situation. Access to a digital twin of\na product or service is equivalent to threatening the entire collection. There\nis a robust interaction between digital twins and artificial intelligence\ntools, which leads to strong interaction between these technologies, so it can\nbe used to improve the cybersecurity of these digital platforms based on their\nintegration with these technologies. This study aims to investigate the role of\nartificial intelligence in providing cybersecurity for digital twin versions of\nvarious industries, as well as the risks associated with these versions. In\naddition, this research serves as a road map for researchers and others\ninterested in cybersecurity and digital security.\n"", '  In recent years, digital twins have been proposed and implemented in various\nfields with potential applications ranging from prototyping to maintenance.\nGoing forward, they are to enable numerous efficient and sustainable\ntechnologies, among them autonomous cars. However, despite a large body of\nresearch in many fields, academics have yet to agree on what exactly a digital\ntwin is -- and as a result, what its capabilities and limitations might be. To\nfurther our understanding, we explore the capabilities of digital twins\nconcerning diagnosis in the field of transportation. We conduct a systematic\nmapping study including digital twins of vehicles and their components, as well\nas transportation infrastructure. We discovered that few papers on digital\ntwins describe any diagnostic process. Furthermore, most existing approaches\nappear limited to system monitoring or fault detection. These findings suggest\nthat we need more research for diagnostic reasoning utilizing digital twins.\n']",Digital Twins and AI in Cybersecurity and Networking,Cybersecurity and Artificial Intelligence in Emerging Technologies,Cybersecurity and Artificial Intelligence,Cybersecurity and Artificial Intelligence
338,23,338_evaluations_evaluation_evaluator_evaluating,"['evaluations', 'evaluation', 'evaluator', 'evaluating', 'assessment', 'evaluators', 'assessing', 'assessments', 'judgments', 'ranking']","['evaluators', 'evaluator', 'evaluation', 'comparisons', 'assessment', 'favoritism', 'pairwise', 'comparative', 'metrics', 'ratings']","[""  Large Language Models (LLMs) excel in various Natural Language Processing\n(NLP) tasks, yet their evaluation, particularly in languages beyond the top\n$20$, remains inadequate due to existing benchmarks and metrics limitations.\nEmploying LLMs as evaluators to rank or score other models' outputs emerges as\na viable solution, addressing the constraints tied to human annotators and\nestablished benchmarks. In this study, we explore the potential of LLM-based\nevaluators, specifically GPT-4 in enhancing multilingual evaluation by\ncalibrating them against $20$K human judgments across three text-generation\ntasks, five metrics, and eight languages. Our analysis reveals a bias in\nGPT4-based evaluators towards higher scores, underscoring the necessity of\ncalibration with native speaker judgments, especially in low-resource and\nnon-Latin script languages, to ensure accurate evaluation of LLM performance\nacross diverse languages.\n"", '  Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\njudgement, revealing that existing calibration methods aimed at mitigating\nbiases are insufficient for effectively aligning LLM evaluators. Inspired by\nthe use of preference data in RLHF, we formulate the evaluation as a ranking\nproblem and introduce Pairwise-preference Search (PairS), an uncertainty-guided\nsearch method that employs LLMs to conduct pairwise comparisons and efficiently\nranks candidate texts. PairS achieves state-of-the-art performance on\nrepresentative evaluation tasks and demonstrates significant improvements over\ndirect scoring. Furthermore, we provide insights into the role of pairwise\npreference in quantifying the transitivity of LLMs and demonstrate how PairS\nbenefits from calibration.\n', '  The zero-shot capability of Large Language Models (LLMs) has enabled highly\nflexible, reference-free metrics for various tasks, making LLM evaluators\ncommon tools in NLP. However, the robustness of these LLM evaluators remains\nrelatively understudied; existing work mainly pursued optimal performance in\nterms of correlating LLM scores with human expert scores. In this paper, we\nconduct a series of analyses using the SummEval dataset and confirm that LLMs\nare biased evaluators as they: (1) exhibit familiarity bias-a preference for\ntext with lower perplexity, (2) show skewed and biased distributions of\nratings, and (3) experience anchoring effects for multi-attribute judgments. We\nalso found that LLMs are inconsistent evaluators, showing low ""inter-sample""\nagreement and sensitivity to prompt differences that are insignificant to human\nunderstanding of text quality. Furthermore, we share recipes for configuring\nLLM evaluators to mitigate these limitations. Experimental results on the RoSE\ndataset demonstrate improvements over the state-of-the-art LLM evaluators.\n']",Evaluating Large Language Models,Evaluating Large Language Models,Large Language Models,Large Language Models
338,23,338_evaluations_evaluation_evaluator_evaluating,"['evaluations', 'evaluation', 'evaluator', 'evaluating', 'assessment', 'evaluators', 'assessing', 'assessments', 'judgments', 'ranking']","['evaluators', 'evaluator', 'evaluation', 'comparisons', 'assessment', 'favoritism', 'pairwise', 'comparative', 'metrics', 'ratings']","[""  Large Language Models (LLMs) excel in various Natural Language Processing\n(NLP) tasks, yet their evaluation, particularly in languages beyond the top\n$20$, remains inadequate due to existing benchmarks and metrics limitations.\nEmploying LLMs as evaluators to rank or score other models' outputs emerges as\na viable solution, addressing the constraints tied to human annotators and\nestablished benchmarks. In this study, we explore the potential of LLM-based\nevaluators, specifically GPT-4 in enhancing multilingual evaluation by\ncalibrating them against $20$K human judgments across three text-generation\ntasks, five metrics, and eight languages. Our analysis reveals a bias in\nGPT4-based evaluators towards higher scores, underscoring the necessity of\ncalibration with native speaker judgments, especially in low-resource and\nnon-Latin script languages, to ensure accurate evaluation of LLM performance\nacross diverse languages.\n"", '  Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\njudgement, revealing that existing calibration methods aimed at mitigating\nbiases are insufficient for effectively aligning LLM evaluators. Inspired by\nthe use of preference data in RLHF, we formulate the evaluation as a ranking\nproblem and introduce Pairwise-preference Search (PairS), an uncertainty-guided\nsearch method that employs LLMs to conduct pairwise comparisons and efficiently\nranks candidate texts. PairS achieves state-of-the-art performance on\nrepresentative evaluation tasks and demonstrates significant improvements over\ndirect scoring. Furthermore, we provide insights into the role of pairwise\npreference in quantifying the transitivity of LLMs and demonstrate how PairS\nbenefits from calibration.\n', '  The zero-shot capability of Large Language Models (LLMs) has enabled highly\nflexible, reference-free metrics for various tasks, making LLM evaluators\ncommon tools in NLP. However, the robustness of these LLM evaluators remains\nrelatively understudied; existing work mainly pursued optimal performance in\nterms of correlating LLM scores with human expert scores. In this paper, we\nconduct a series of analyses using the SummEval dataset and confirm that LLMs\nare biased evaluators as they: (1) exhibit familiarity bias-a preference for\ntext with lower perplexity, (2) show skewed and biased distributions of\nratings, and (3) experience anchoring effects for multi-attribute judgments. We\nalso found that LLMs are inconsistent evaluators, showing low ""inter-sample""\nagreement and sensitivity to prompt differences that are insignificant to human\nunderstanding of text quality. Furthermore, we share recipes for configuring\nLLM evaluators to mitigate these limitations. Experimental results on the RoSE\ndataset demonstrate improvements over the state-of-the-art LLM evaluators.\n']",Evaluating Large Language Models,Evaluating Large Language Models,Large Language Models,Large Language Models
338,23,338_evaluations_evaluation_evaluator_evaluating,"['evaluations', 'evaluation', 'evaluator', 'evaluating', 'assessment', 'evaluators', 'assessing', 'assessments', 'judgments', 'ranking']","['evaluators', 'evaluator', 'evaluation', 'comparisons', 'assessment', 'favoritism', 'pairwise', 'comparative', 'metrics', 'ratings']","[""  Large Language Models (LLMs) excel in various Natural Language Processing\n(NLP) tasks, yet their evaluation, particularly in languages beyond the top\n$20$, remains inadequate due to existing benchmarks and metrics limitations.\nEmploying LLMs as evaluators to rank or score other models' outputs emerges as\na viable solution, addressing the constraints tied to human annotators and\nestablished benchmarks. In this study, we explore the potential of LLM-based\nevaluators, specifically GPT-4 in enhancing multilingual evaluation by\ncalibrating them against $20$K human judgments across three text-generation\ntasks, five metrics, and eight languages. Our analysis reveals a bias in\nGPT4-based evaluators towards higher scores, underscoring the necessity of\ncalibration with native speaker judgments, especially in low-resource and\nnon-Latin script languages, to ensure accurate evaluation of LLM performance\nacross diverse languages.\n"", '  Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\njudgement, revealing that existing calibration methods aimed at mitigating\nbiases are insufficient for effectively aligning LLM evaluators. Inspired by\nthe use of preference data in RLHF, we formulate the evaluation as a ranking\nproblem and introduce Pairwise-preference Search (PairS), an uncertainty-guided\nsearch method that employs LLMs to conduct pairwise comparisons and efficiently\nranks candidate texts. PairS achieves state-of-the-art performance on\nrepresentative evaluation tasks and demonstrates significant improvements over\ndirect scoring. Furthermore, we provide insights into the role of pairwise\npreference in quantifying the transitivity of LLMs and demonstrate how PairS\nbenefits from calibration.\n', '  The zero-shot capability of Large Language Models (LLMs) has enabled highly\nflexible, reference-free metrics for various tasks, making LLM evaluators\ncommon tools in NLP. However, the robustness of these LLM evaluators remains\nrelatively understudied; existing work mainly pursued optimal performance in\nterms of correlating LLM scores with human expert scores. In this paper, we\nconduct a series of analyses using the SummEval dataset and confirm that LLMs\nare biased evaluators as they: (1) exhibit familiarity bias-a preference for\ntext with lower perplexity, (2) show skewed and biased distributions of\nratings, and (3) experience anchoring effects for multi-attribute judgments. We\nalso found that LLMs are inconsistent evaluators, showing low ""inter-sample""\nagreement and sensitivity to prompt differences that are insignificant to human\nunderstanding of text quality. Furthermore, we share recipes for configuring\nLLM evaluators to mitigate these limitations. Experimental results on the RoSE\ndataset demonstrate improvements over the state-of-the-art LLM evaluators.\n']",Evaluating Large Language Models,Evaluating Large Language Models,Large Language Models,Large Language Models
338,23,338_evaluations_evaluation_evaluator_evaluating,"['evaluations', 'evaluation', 'evaluator', 'evaluating', 'assessment', 'evaluators', 'assessing', 'assessments', 'judgments', 'ranking']","['evaluators', 'evaluator', 'evaluation', 'comparisons', 'assessment', 'favoritism', 'pairwise', 'comparative', 'metrics', 'ratings']","[""  Large Language Models (LLMs) excel in various Natural Language Processing\n(NLP) tasks, yet their evaluation, particularly in languages beyond the top\n$20$, remains inadequate due to existing benchmarks and metrics limitations.\nEmploying LLMs as evaluators to rank or score other models' outputs emerges as\na viable solution, addressing the constraints tied to human annotators and\nestablished benchmarks. In this study, we explore the potential of LLM-based\nevaluators, specifically GPT-4 in enhancing multilingual evaluation by\ncalibrating them against $20$K human judgments across three text-generation\ntasks, five metrics, and eight languages. Our analysis reveals a bias in\nGPT4-based evaluators towards higher scores, underscoring the necessity of\ncalibration with native speaker judgments, especially in low-resource and\nnon-Latin script languages, to ensure accurate evaluation of LLM performance\nacross diverse languages.\n"", '  Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\njudgement, revealing that existing calibration methods aimed at mitigating\nbiases are insufficient for effectively aligning LLM evaluators. Inspired by\nthe use of preference data in RLHF, we formulate the evaluation as a ranking\nproblem and introduce Pairwise-preference Search (PairS), an uncertainty-guided\nsearch method that employs LLMs to conduct pairwise comparisons and efficiently\nranks candidate texts. PairS achieves state-of-the-art performance on\nrepresentative evaluation tasks and demonstrates significant improvements over\ndirect scoring. Furthermore, we provide insights into the role of pairwise\npreference in quantifying the transitivity of LLMs and demonstrate how PairS\nbenefits from calibration.\n', '  The zero-shot capability of Large Language Models (LLMs) has enabled highly\nflexible, reference-free metrics for various tasks, making LLM evaluators\ncommon tools in NLP. However, the robustness of these LLM evaluators remains\nrelatively understudied; existing work mainly pursued optimal performance in\nterms of correlating LLM scores with human expert scores. In this paper, we\nconduct a series of analyses using the SummEval dataset and confirm that LLMs\nare biased evaluators as they: (1) exhibit familiarity bias-a preference for\ntext with lower perplexity, (2) show skewed and biased distributions of\nratings, and (3) experience anchoring effects for multi-attribute judgments. We\nalso found that LLMs are inconsistent evaluators, showing low ""inter-sample""\nagreement and sensitivity to prompt differences that are insignificant to human\nunderstanding of text quality. Furthermore, we share recipes for configuring\nLLM evaluators to mitigate these limitations. Experimental results on the RoSE\ndataset demonstrate improvements over the state-of-the-art LLM evaluators.\n']",Evaluating Large Language Models,Evaluating Large Language Models,Large Language Models,Large Language Models
339,23,339_radar_radars_lidar_targets,"['radar', 'radars', 'lidar', 'targets', 'range', 'doppler', 'target', 'sensing', 'echoes', 'drones']","['radar', 'radars', 'waveforms', 'lidar', 'signal', 'resolution', 'range', 'targets', 'subwavelength', 'band']","['  This paper presents a novel deep-learning-based approach to improve\nlocalizing radar measurements against lidar maps. Although the state of the art\nfor localization is matching lidar data to lidar maps, radar has been\nconsidered as a promising alternative. This is largely due to radar being more\nresilient against adverse weather such as precipitation and heavy fog. To make\nuse of existing high-quality lidar maps, while maintaining performance in\nadverse weather, it is of interest to match radar data to lidar maps. However,\nowing in part to the unique artefacts present in radar measurements,\nradar-lidar localization has struggled to achieve comparable performance to\nlidar-lidar systems, preventing it from being viable for autonomous driving.\nThis work builds on an ICP-based radar-lidar localization system by including a\nlearned preprocessing step that weights radar points based on high-level scan\ninformation. Combining a proven analytical approach with a learned weight\nreduces localization errors in radar-lidar ICP results run on real-world\nautonomous driving data by up to 54.94% in translation and 68.39% in rotation,\nwhile maintaining interpretability and robustness.\n', '  Millimeter-wave (mmWave) radars are indispensable for perception tasks of\nautonomous vehicles, thanks to their resilience in challenging weather\nconditions. Yet, their deployment is often limited by insufficient spatial\nresolution for precise semantic scene interpretation. Classical\nsuper-resolution techniques adapted from optical imaging inadequately address\nthe distinct characteristics of radar signal data. In response, our study\nredefines radar imaging super-resolution as a one-dimensional (1D) signal\nsuper-resolution spectra estimation problem by harnessing the radar signal\nprocessing domain knowledge, introducing innovative data normalization and a\ndomain-informed signal-to-noise ratio (SNR)-guided loss function. Our tailored\ndeep learning network for automotive radar imaging exhibits remarkable\nscalability, parameter efficiency and fast inference speed, alongside enhanced\nperformance in terms of radar imaging quality and resolution. Extensive testing\nconfirms that our SR-SPECNet sets a new benchmark in producing high-resolution\nradar range-azimuth images, outperforming existing methods across varied\nantenna configurations and dataset sizes. Source code and new radar dataset\nwill be made publicly available online.\n', '  Simulation is an invaluable tool for radio-frequency system designers that\nenables rapid prototyping of various algorithms for imaging, target detection,\nclassification, and tracking. However, simulating realistic radar scans is a\nchallenging task that requires an accurate model of the scene, radio frequency\nmaterial properties, and a corresponding radar synthesis function. Rather than\nspecifying these models explicitly, we propose DART - Doppler Aided Radar\nTomography, a Neural Radiance Field-inspired method which uses radar-specific\nphysics to create a reflectance and transmittance-based rendering pipeline for\nrange-Doppler images. We then evaluate DART by constructing a custom data\ncollection platform and collecting a novel radar dataset together with accurate\nposition and instantaneous velocity measurements from lidar-based localization.\nIn comparison to state-of-the-art baselines, DART synthesizes superior radar\nrange-Doppler images from novel views across all datasets and additionally can\nbe used to generate high quality tomographic images.\n']",Radar and Lidar for Autonomous Driving and Sensing,Sensor Fusion and Perception for Autonomous Driving,Autonomous Systems and Safety Assessment,Autonomous Systems and Safety Assessment
340,23,340_prompts_prompt_attention_prompting,"['prompts', 'prompt', 'attention', 'prompting', 'language', 'pretrained', 'tuning', 'optimizing', 'tasks', 'monopara']","['prompt', 'soft', 'prompts', 'tuning', 'instruction', 'trainable', 'downstream', 'fine', 'tokens', 'task']","['  Soft prompt tuning is a widely studied parameter-efficient fine-tuning\nmethod. However, it has a clear drawback: many soft tokens must be inserted\ninto the input sequences to guarantee downstream performance. As a result, soft\nprompt tuning is less considered than Low-rank adaptation (LoRA) in the large\nlanguage modeling (LLM) era. In this work, we propose a novel prompt tuning\nmethod, Instruction-Aware Prompt Tuning (IAPT), that requires only four soft\ntokens. First, we install a parameter-efficient soft prompt generator at each\nTransformer layer to generate idiosyncratic soft prompts for each input\ninstruction. The generated soft prompts can be seen as a semantic summary of\nthe input instructions and can effectively guide the output generation. Second,\nthe soft prompt generators are modules with a bottleneck architecture\nconsisting of a self-attention pooling operation, two linear projections, and\nan activation function. Pilot experiments show that prompt generators at\ndifferent Transformer layers require different activation functions. Thus, we\npropose to learn the idiosyncratic activation functions for prompt generators\nautomatically with the help of rational functions. We have conducted\nexperiments on various tasks, and the experimental results demonstrate that (a)\nour IAPT method can outperform the recent baselines with comparable tunable\nparameters. (b) Our IAPT method is more efficient than LoRA under the\nsingle-backbone multi-tenant setting.\n', '  Recent advances in prompt optimization have notably enhanced the performance\nof pre-trained language models (PLMs) on downstream tasks. However, the\npotential of optimized prompts on domain generalization has been\nunder-explored. To explore the nature of prompt generalization on unknown\ndomains, we conduct pilot experiments and find that (i) Prompts gaining more\nattention weight from PLMs\' deep layers are more generalizable and (ii) Prompts\nwith more stable attention distributions in PLMs\' deep layers are more\ngeneralizable. Thus, we offer a fresh objective towards domain-generalizable\nprompts optimization named ""Concentration"", which represents the ""lookback""\nattention from the current decoding token to the prompt tokens, to increase the\nattention strength on prompts and reduce the fluctuation of attention\ndistribution. We adapt this new objective to popular soft prompt and hard\nprompt optimization methods, respectively. Extensive experiments demonstrate\nthat our idea improves comparison prompt optimization methods by 1.42% for soft\nprompt generalization and 2.16% for hard prompt generalization in accuracy on\nthe multi-source domain generalization setting, while maintaining satisfying\nin-domain performance. The promising results validate the effectiveness of our\nproposed prompt optimization objective and provide key insights into\ndomain-generalizable prompts.\n', '  Prompt tuning is a promising method to fine-tune a pre-trained language model\nwithout retraining its large-scale parameters. Instead, it attaches a soft\nprompt to the input text, whereby downstream tasks can be well adapted by\nmerely learning the embeddings of prompt tokens. Nevertheless, existing methods\nstill suffer from two challenges: (i) they are hard to balance accuracy and\nefficiency. A longer (shorter) soft prompt generally leads to a better(worse)\naccuracy but at the cost of more (less) training time. (ii)The performance may\nnot be consistent when adapting to different downstream tasks. We attribute it\nto the same embedding space but responsible for different requirements of\ndownstream tasks. To address these issues, we propose an Efficient Prompt\nTuning method (EPT) by multi-space projection and prompt fusion. Specifically,\nit decomposes a given soft prompt into a shorter prompt and two low-rank\nmatrices, significantly reducing the training time. Accuracy is also enhanced\nby leveraging low-rank matrices and the short prompt as additional knowledge\nsources to enrich the semantics of the original short prompt. In addition, we\nproject the soft prompt into multiple subspaces to improve the performance\nconsistency, and then adaptively learn the combination weights of different\nspaces through a gating network. Experiments on 13 natural language processing\ndownstream tasks show that our method significantly and consistently\noutperforms 11 comparison methods with the relative percentage of improvements\nup to 12.9%, and training time decreased by 14%.\n']",Prompt Tuning for Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
340,23,340_prompts_prompt_attention_prompting,"['prompts', 'prompt', 'attention', 'prompting', 'language', 'pretrained', 'tuning', 'optimizing', 'tasks', 'monopara']","['prompt', 'soft', 'prompts', 'tuning', 'instruction', 'trainable', 'downstream', 'fine', 'tokens', 'task']","['  Soft prompt tuning is a widely studied parameter-efficient fine-tuning\nmethod. However, it has a clear drawback: many soft tokens must be inserted\ninto the input sequences to guarantee downstream performance. As a result, soft\nprompt tuning is less considered than Low-rank adaptation (LoRA) in the large\nlanguage modeling (LLM) era. In this work, we propose a novel prompt tuning\nmethod, Instruction-Aware Prompt Tuning (IAPT), that requires only four soft\ntokens. First, we install a parameter-efficient soft prompt generator at each\nTransformer layer to generate idiosyncratic soft prompts for each input\ninstruction. The generated soft prompts can be seen as a semantic summary of\nthe input instructions and can effectively guide the output generation. Second,\nthe soft prompt generators are modules with a bottleneck architecture\nconsisting of a self-attention pooling operation, two linear projections, and\nan activation function. Pilot experiments show that prompt generators at\ndifferent Transformer layers require different activation functions. Thus, we\npropose to learn the idiosyncratic activation functions for prompt generators\nautomatically with the help of rational functions. We have conducted\nexperiments on various tasks, and the experimental results demonstrate that (a)\nour IAPT method can outperform the recent baselines with comparable tunable\nparameters. (b) Our IAPT method is more efficient than LoRA under the\nsingle-backbone multi-tenant setting.\n', '  Recent advances in prompt optimization have notably enhanced the performance\nof pre-trained language models (PLMs) on downstream tasks. However, the\npotential of optimized prompts on domain generalization has been\nunder-explored. To explore the nature of prompt generalization on unknown\ndomains, we conduct pilot experiments and find that (i) Prompts gaining more\nattention weight from PLMs\' deep layers are more generalizable and (ii) Prompts\nwith more stable attention distributions in PLMs\' deep layers are more\ngeneralizable. Thus, we offer a fresh objective towards domain-generalizable\nprompts optimization named ""Concentration"", which represents the ""lookback""\nattention from the current decoding token to the prompt tokens, to increase the\nattention strength on prompts and reduce the fluctuation of attention\ndistribution. We adapt this new objective to popular soft prompt and hard\nprompt optimization methods, respectively. Extensive experiments demonstrate\nthat our idea improves comparison prompt optimization methods by 1.42% for soft\nprompt generalization and 2.16% for hard prompt generalization in accuracy on\nthe multi-source domain generalization setting, while maintaining satisfying\nin-domain performance. The promising results validate the effectiveness of our\nproposed prompt optimization objective and provide key insights into\ndomain-generalizable prompts.\n', '  Prompt tuning is a promising method to fine-tune a pre-trained language model\nwithout retraining its large-scale parameters. Instead, it attaches a soft\nprompt to the input text, whereby downstream tasks can be well adapted by\nmerely learning the embeddings of prompt tokens. Nevertheless, existing methods\nstill suffer from two challenges: (i) they are hard to balance accuracy and\nefficiency. A longer (shorter) soft prompt generally leads to a better(worse)\naccuracy but at the cost of more (less) training time. (ii)The performance may\nnot be consistent when adapting to different downstream tasks. We attribute it\nto the same embedding space but responsible for different requirements of\ndownstream tasks. To address these issues, we propose an Efficient Prompt\nTuning method (EPT) by multi-space projection and prompt fusion. Specifically,\nit decomposes a given soft prompt into a shorter prompt and two low-rank\nmatrices, significantly reducing the training time. Accuracy is also enhanced\nby leveraging low-rank matrices and the short prompt as additional knowledge\nsources to enrich the semantics of the original short prompt. In addition, we\nproject the soft prompt into multiple subspaces to improve the performance\nconsistency, and then adaptively learn the combination weights of different\nspaces through a gating network. Experiments on 13 natural language processing\ndownstream tasks show that our method significantly and consistently\noutperforms 11 comparison methods with the relative percentage of improvements\nup to 12.9%, and training time decreased by 14%.\n']",Prompt Tuning for Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
340,23,340_prompts_prompt_attention_prompting,"['prompts', 'prompt', 'attention', 'prompting', 'language', 'pretrained', 'tuning', 'optimizing', 'tasks', 'monopara']","['prompt', 'soft', 'prompts', 'tuning', 'instruction', 'trainable', 'downstream', 'fine', 'tokens', 'task']","['  Soft prompt tuning is a widely studied parameter-efficient fine-tuning\nmethod. However, it has a clear drawback: many soft tokens must be inserted\ninto the input sequences to guarantee downstream performance. As a result, soft\nprompt tuning is less considered than Low-rank adaptation (LoRA) in the large\nlanguage modeling (LLM) era. In this work, we propose a novel prompt tuning\nmethod, Instruction-Aware Prompt Tuning (IAPT), that requires only four soft\ntokens. First, we install a parameter-efficient soft prompt generator at each\nTransformer layer to generate idiosyncratic soft prompts for each input\ninstruction. The generated soft prompts can be seen as a semantic summary of\nthe input instructions and can effectively guide the output generation. Second,\nthe soft prompt generators are modules with a bottleneck architecture\nconsisting of a self-attention pooling operation, two linear projections, and\nan activation function. Pilot experiments show that prompt generators at\ndifferent Transformer layers require different activation functions. Thus, we\npropose to learn the idiosyncratic activation functions for prompt generators\nautomatically with the help of rational functions. We have conducted\nexperiments on various tasks, and the experimental results demonstrate that (a)\nour IAPT method can outperform the recent baselines with comparable tunable\nparameters. (b) Our IAPT method is more efficient than LoRA under the\nsingle-backbone multi-tenant setting.\n', '  Recent advances in prompt optimization have notably enhanced the performance\nof pre-trained language models (PLMs) on downstream tasks. However, the\npotential of optimized prompts on domain generalization has been\nunder-explored. To explore the nature of prompt generalization on unknown\ndomains, we conduct pilot experiments and find that (i) Prompts gaining more\nattention weight from PLMs\' deep layers are more generalizable and (ii) Prompts\nwith more stable attention distributions in PLMs\' deep layers are more\ngeneralizable. Thus, we offer a fresh objective towards domain-generalizable\nprompts optimization named ""Concentration"", which represents the ""lookback""\nattention from the current decoding token to the prompt tokens, to increase the\nattention strength on prompts and reduce the fluctuation of attention\ndistribution. We adapt this new objective to popular soft prompt and hard\nprompt optimization methods, respectively. Extensive experiments demonstrate\nthat our idea improves comparison prompt optimization methods by 1.42% for soft\nprompt generalization and 2.16% for hard prompt generalization in accuracy on\nthe multi-source domain generalization setting, while maintaining satisfying\nin-domain performance. The promising results validate the effectiveness of our\nproposed prompt optimization objective and provide key insights into\ndomain-generalizable prompts.\n', '  Prompt tuning is a promising method to fine-tune a pre-trained language model\nwithout retraining its large-scale parameters. Instead, it attaches a soft\nprompt to the input text, whereby downstream tasks can be well adapted by\nmerely learning the embeddings of prompt tokens. Nevertheless, existing methods\nstill suffer from two challenges: (i) they are hard to balance accuracy and\nefficiency. A longer (shorter) soft prompt generally leads to a better(worse)\naccuracy but at the cost of more (less) training time. (ii)The performance may\nnot be consistent when adapting to different downstream tasks. We attribute it\nto the same embedding space but responsible for different requirements of\ndownstream tasks. To address these issues, we propose an Efficient Prompt\nTuning method (EPT) by multi-space projection and prompt fusion. Specifically,\nit decomposes a given soft prompt into a shorter prompt and two low-rank\nmatrices, significantly reducing the training time. Accuracy is also enhanced\nby leveraging low-rank matrices and the short prompt as additional knowledge\nsources to enrich the semantics of the original short prompt. In addition, we\nproject the soft prompt into multiple subspaces to improve the performance\nconsistency, and then adaptively learn the combination weights of different\nspaces through a gating network. Experiments on 13 natural language processing\ndownstream tasks show that our method significantly and consistently\noutperforms 11 comparison methods with the relative percentage of improvements\nup to 12.9%, and training time decreased by 14%.\n']",Prompt Tuning for Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
340,23,340_prompts_prompt_attention_prompting,"['prompts', 'prompt', 'attention', 'prompting', 'language', 'pretrained', 'tuning', 'optimizing', 'tasks', 'monopara']","['prompt', 'soft', 'prompts', 'tuning', 'instruction', 'trainable', 'downstream', 'fine', 'tokens', 'task']","['  Soft prompt tuning is a widely studied parameter-efficient fine-tuning\nmethod. However, it has a clear drawback: many soft tokens must be inserted\ninto the input sequences to guarantee downstream performance. As a result, soft\nprompt tuning is less considered than Low-rank adaptation (LoRA) in the large\nlanguage modeling (LLM) era. In this work, we propose a novel prompt tuning\nmethod, Instruction-Aware Prompt Tuning (IAPT), that requires only four soft\ntokens. First, we install a parameter-efficient soft prompt generator at each\nTransformer layer to generate idiosyncratic soft prompts for each input\ninstruction. The generated soft prompts can be seen as a semantic summary of\nthe input instructions and can effectively guide the output generation. Second,\nthe soft prompt generators are modules with a bottleneck architecture\nconsisting of a self-attention pooling operation, two linear projections, and\nan activation function. Pilot experiments show that prompt generators at\ndifferent Transformer layers require different activation functions. Thus, we\npropose to learn the idiosyncratic activation functions for prompt generators\nautomatically with the help of rational functions. We have conducted\nexperiments on various tasks, and the experimental results demonstrate that (a)\nour IAPT method can outperform the recent baselines with comparable tunable\nparameters. (b) Our IAPT method is more efficient than LoRA under the\nsingle-backbone multi-tenant setting.\n', '  Recent advances in prompt optimization have notably enhanced the performance\nof pre-trained language models (PLMs) on downstream tasks. However, the\npotential of optimized prompts on domain generalization has been\nunder-explored. To explore the nature of prompt generalization on unknown\ndomains, we conduct pilot experiments and find that (i) Prompts gaining more\nattention weight from PLMs\' deep layers are more generalizable and (ii) Prompts\nwith more stable attention distributions in PLMs\' deep layers are more\ngeneralizable. Thus, we offer a fresh objective towards domain-generalizable\nprompts optimization named ""Concentration"", which represents the ""lookback""\nattention from the current decoding token to the prompt tokens, to increase the\nattention strength on prompts and reduce the fluctuation of attention\ndistribution. We adapt this new objective to popular soft prompt and hard\nprompt optimization methods, respectively. Extensive experiments demonstrate\nthat our idea improves comparison prompt optimization methods by 1.42% for soft\nprompt generalization and 2.16% for hard prompt generalization in accuracy on\nthe multi-source domain generalization setting, while maintaining satisfying\nin-domain performance. The promising results validate the effectiveness of our\nproposed prompt optimization objective and provide key insights into\ndomain-generalizable prompts.\n', '  Prompt tuning is a promising method to fine-tune a pre-trained language model\nwithout retraining its large-scale parameters. Instead, it attaches a soft\nprompt to the input text, whereby downstream tasks can be well adapted by\nmerely learning the embeddings of prompt tokens. Nevertheless, existing methods\nstill suffer from two challenges: (i) they are hard to balance accuracy and\nefficiency. A longer (shorter) soft prompt generally leads to a better(worse)\naccuracy but at the cost of more (less) training time. (ii)The performance may\nnot be consistent when adapting to different downstream tasks. We attribute it\nto the same embedding space but responsible for different requirements of\ndownstream tasks. To address these issues, we propose an Efficient Prompt\nTuning method (EPT) by multi-space projection and prompt fusion. Specifically,\nit decomposes a given soft prompt into a shorter prompt and two low-rank\nmatrices, significantly reducing the training time. Accuracy is also enhanced\nby leveraging low-rank matrices and the short prompt as additional knowledge\nsources to enrich the semantics of the original short prompt. In addition, we\nproject the soft prompt into multiple subspaces to improve the performance\nconsistency, and then adaptively learn the combination weights of different\nspaces through a gating network. Experiments on 13 natural language processing\ndownstream tasks show that our method significantly and consistently\noutperforms 11 comparison methods with the relative percentage of improvements\nup to 12.9%, and training time decreased by 14%.\n']",Prompt Tuning for Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
340,23,340_prompts_prompt_attention_prompting,"['prompts', 'prompt', 'attention', 'prompting', 'language', 'pretrained', 'tuning', 'optimizing', 'tasks', 'monopara']","['prompt', 'soft', 'prompts', 'tuning', 'instruction', 'trainable', 'downstream', 'fine', 'tokens', 'task']","['  Soft prompt tuning is a widely studied parameter-efficient fine-tuning\nmethod. However, it has a clear drawback: many soft tokens must be inserted\ninto the input sequences to guarantee downstream performance. As a result, soft\nprompt tuning is less considered than Low-rank adaptation (LoRA) in the large\nlanguage modeling (LLM) era. In this work, we propose a novel prompt tuning\nmethod, Instruction-Aware Prompt Tuning (IAPT), that requires only four soft\ntokens. First, we install a parameter-efficient soft prompt generator at each\nTransformer layer to generate idiosyncratic soft prompts for each input\ninstruction. The generated soft prompts can be seen as a semantic summary of\nthe input instructions and can effectively guide the output generation. Second,\nthe soft prompt generators are modules with a bottleneck architecture\nconsisting of a self-attention pooling operation, two linear projections, and\nan activation function. Pilot experiments show that prompt generators at\ndifferent Transformer layers require different activation functions. Thus, we\npropose to learn the idiosyncratic activation functions for prompt generators\nautomatically with the help of rational functions. We have conducted\nexperiments on various tasks, and the experimental results demonstrate that (a)\nour IAPT method can outperform the recent baselines with comparable tunable\nparameters. (b) Our IAPT method is more efficient than LoRA under the\nsingle-backbone multi-tenant setting.\n', '  Recent advances in prompt optimization have notably enhanced the performance\nof pre-trained language models (PLMs) on downstream tasks. However, the\npotential of optimized prompts on domain generalization has been\nunder-explored. To explore the nature of prompt generalization on unknown\ndomains, we conduct pilot experiments and find that (i) Prompts gaining more\nattention weight from PLMs\' deep layers are more generalizable and (ii) Prompts\nwith more stable attention distributions in PLMs\' deep layers are more\ngeneralizable. Thus, we offer a fresh objective towards domain-generalizable\nprompts optimization named ""Concentration"", which represents the ""lookback""\nattention from the current decoding token to the prompt tokens, to increase the\nattention strength on prompts and reduce the fluctuation of attention\ndistribution. We adapt this new objective to popular soft prompt and hard\nprompt optimization methods, respectively. Extensive experiments demonstrate\nthat our idea improves comparison prompt optimization methods by 1.42% for soft\nprompt generalization and 2.16% for hard prompt generalization in accuracy on\nthe multi-source domain generalization setting, while maintaining satisfying\nin-domain performance. The promising results validate the effectiveness of our\nproposed prompt optimization objective and provide key insights into\ndomain-generalizable prompts.\n', '  Prompt tuning is a promising method to fine-tune a pre-trained language model\nwithout retraining its large-scale parameters. Instead, it attaches a soft\nprompt to the input text, whereby downstream tasks can be well adapted by\nmerely learning the embeddings of prompt tokens. Nevertheless, existing methods\nstill suffer from two challenges: (i) they are hard to balance accuracy and\nefficiency. A longer (shorter) soft prompt generally leads to a better(worse)\naccuracy but at the cost of more (less) training time. (ii)The performance may\nnot be consistent when adapting to different downstream tasks. We attribute it\nto the same embedding space but responsible for different requirements of\ndownstream tasks. To address these issues, we propose an Efficient Prompt\nTuning method (EPT) by multi-space projection and prompt fusion. Specifically,\nit decomposes a given soft prompt into a shorter prompt and two low-rank\nmatrices, significantly reducing the training time. Accuracy is also enhanced\nby leveraging low-rank matrices and the short prompt as additional knowledge\nsources to enrich the semantics of the original short prompt. In addition, we\nproject the soft prompt into multiple subspaces to improve the performance\nconsistency, and then adaptively learn the combination weights of different\nspaces through a gating network. Experiments on 13 natural language processing\ndownstream tasks show that our method significantly and consistently\noutperforms 11 comparison methods with the relative percentage of improvements\nup to 12.9%, and training time decreased by 14%.\n']",Prompt Tuning for Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
340,23,340_prompts_prompt_attention_prompting,"['prompts', 'prompt', 'attention', 'prompting', 'language', 'pretrained', 'tuning', 'optimizing', 'tasks', 'monopara']","['prompt', 'soft', 'prompts', 'tuning', 'instruction', 'trainable', 'downstream', 'fine', 'tokens', 'task']","['  Soft prompt tuning is a widely studied parameter-efficient fine-tuning\nmethod. However, it has a clear drawback: many soft tokens must be inserted\ninto the input sequences to guarantee downstream performance. As a result, soft\nprompt tuning is less considered than Low-rank adaptation (LoRA) in the large\nlanguage modeling (LLM) era. In this work, we propose a novel prompt tuning\nmethod, Instruction-Aware Prompt Tuning (IAPT), that requires only four soft\ntokens. First, we install a parameter-efficient soft prompt generator at each\nTransformer layer to generate idiosyncratic soft prompts for each input\ninstruction. The generated soft prompts can be seen as a semantic summary of\nthe input instructions and can effectively guide the output generation. Second,\nthe soft prompt generators are modules with a bottleneck architecture\nconsisting of a self-attention pooling operation, two linear projections, and\nan activation function. Pilot experiments show that prompt generators at\ndifferent Transformer layers require different activation functions. Thus, we\npropose to learn the idiosyncratic activation functions for prompt generators\nautomatically with the help of rational functions. We have conducted\nexperiments on various tasks, and the experimental results demonstrate that (a)\nour IAPT method can outperform the recent baselines with comparable tunable\nparameters. (b) Our IAPT method is more efficient than LoRA under the\nsingle-backbone multi-tenant setting.\n', '  Recent advances in prompt optimization have notably enhanced the performance\nof pre-trained language models (PLMs) on downstream tasks. However, the\npotential of optimized prompts on domain generalization has been\nunder-explored. To explore the nature of prompt generalization on unknown\ndomains, we conduct pilot experiments and find that (i) Prompts gaining more\nattention weight from PLMs\' deep layers are more generalizable and (ii) Prompts\nwith more stable attention distributions in PLMs\' deep layers are more\ngeneralizable. Thus, we offer a fresh objective towards domain-generalizable\nprompts optimization named ""Concentration"", which represents the ""lookback""\nattention from the current decoding token to the prompt tokens, to increase the\nattention strength on prompts and reduce the fluctuation of attention\ndistribution. We adapt this new objective to popular soft prompt and hard\nprompt optimization methods, respectively. Extensive experiments demonstrate\nthat our idea improves comparison prompt optimization methods by 1.42% for soft\nprompt generalization and 2.16% for hard prompt generalization in accuracy on\nthe multi-source domain generalization setting, while maintaining satisfying\nin-domain performance. The promising results validate the effectiveness of our\nproposed prompt optimization objective and provide key insights into\ndomain-generalizable prompts.\n', '  Prompt tuning is a promising method to fine-tune a pre-trained language model\nwithout retraining its large-scale parameters. Instead, it attaches a soft\nprompt to the input text, whereby downstream tasks can be well adapted by\nmerely learning the embeddings of prompt tokens. Nevertheless, existing methods\nstill suffer from two challenges: (i) they are hard to balance accuracy and\nefficiency. A longer (shorter) soft prompt generally leads to a better(worse)\naccuracy but at the cost of more (less) training time. (ii)The performance may\nnot be consistent when adapting to different downstream tasks. We attribute it\nto the same embedding space but responsible for different requirements of\ndownstream tasks. To address these issues, we propose an Efficient Prompt\nTuning method (EPT) by multi-space projection and prompt fusion. Specifically,\nit decomposes a given soft prompt into a shorter prompt and two low-rank\nmatrices, significantly reducing the training time. Accuracy is also enhanced\nby leveraging low-rank matrices and the short prompt as additional knowledge\nsources to enrich the semantics of the original short prompt. In addition, we\nproject the soft prompt into multiple subspaces to improve the performance\nconsistency, and then adaptively learn the combination weights of different\nspaces through a gating network. Experiments on 13 natural language processing\ndownstream tasks show that our method significantly and consistently\noutperforms 11 comparison methods with the relative percentage of improvements\nup to 12.9%, and training time decreased by 14%.\n']",Prompt Tuning for Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
341,23,341_emotions_emotion_affective_emotional,"['emotions', 'emotion', 'affective', 'emotional', 'sentiment', 'affect', 'valence', 'nlp', 'categorizing', 'categorization']","['emotion', 'emotions', 'affective', 'emotional', 'languages', 'intensity', 'sentiment', 'classification', 'affect', 'ontology']","[""  Human emotions are often not expressed directly, but regulated according to\ninternal processes and social display rules. For affective computing systems,\nan understanding of how users regulate their emotions can be highly useful, for\nexample to provide feedback in job interview training, or in psychotherapeutic\nscenarios. However, at present no method to automatically classify different\nemotion regulation strategies in a cross-user scenario exists. At the same\ntime, recent studies showed that instruction-tuned Large Language Models (LLMs)\ncan reach impressive performance across a variety of affect recognition tasks\nsuch as categorical emotion recognition or sentiment analysis. While these\nresults are promising, it remains unclear to what extent the representational\npower of LLMs can be utilized in the more subtle task of classifying users'\ninternal emotion regulation strategy. To close this gap, we make use of the\nrecently introduced \\textsc{Deep} corpus for modeling the social display of the\nemotion shame, where each point in time is annotated with one of seven\ndifferent emotion regulation classes. We fine-tune Llama2-7B as well as the\nrecently introduced Gemma model using Low-rank Optimization on prompts\ngenerated from different sources of information on the \\textsc{Deep} corpus.\nThese include verbal and nonverbal behavior, person factors, as well as the\nresults of an in-depth interview after the interaction. Our results show, that\na fine-tuned Llama2-7B LLM is able to classify the utilized emotion regulation\nstrategy with high accuracy (0.84) without needing access to data from\npost-interaction interviews. This represents a significant improvement over\nprevious approaches based on Bayesian Networks and highlights the importance of\nmodeling verbal behavior in emotion regulation.\n"", '  Emotion detection in textual data has received growing interest in recent\nyears, as it is pivotal for developing empathetic human-computer interaction\nsystems. This paper introduces a method for categorizing emotions from text,\nwhich acknowledges and differentiates between the diversified similarities and\ndistinctions of various emotions. Initially, we establish a baseline by\ntraining a transformer-based model for standard emotion classification,\nachieving state-of-the-art performance. We argue that not all\nmisclassifications are of the same importance, as there are perceptual\nsimilarities among emotional classes. We thus redefine the emotion labeling\nproblem by shifting it from a traditional classification model to an ordinal\nclassification one, where discrete emotions are arranged in a sequential order\naccording to their valence levels. Finally, we propose a method that performs\nordinal classification in the two-dimensional emotion space, considering both\nvalence and arousal scales. The results show that our approach not only\npreserves high accuracy in emotion prediction but also significantly reduces\nthe magnitude of errors in cases of misclassification.\n', '  We propose leveraging cognitive science research on emotions and\ncommunication to improve language models for emotion analysis. First, we\npresent the main emotion theories in psychology and cognitive science. Then, we\nintroduce the main methods of emotion annotation in natural language processing\nand their connections to psychological theories. We also present the two main\ntypes of analyses of emotional communication in cognitive pragmatics. Finally,\nbased on the cognitive science research presented, we propose directions for\nimproving language models for emotion analysis. We suggest that these research\nefforts pave the way for constructing new annotation schemes and a possible\nbenchmark for emotional understanding, considering different facets of human\nemotion and communication.\n']",Emotion Analysis and Recognition in Text,Emotion Analysis and Recognition in Human-Computer Interaction,Human Behavior and Emotion Analysis through Language and Interaction,Human Behavior and Emotion Analysis through Language and Interaction
342,23,342_tweets_twitter_pandemic_covid,"['tweets', 'twitter', 'pandemic', 'covid', 'sentiment', 'vaccine', 'vaccination', 'coronavirus', 'vaccines', 'retweets']","['misinformation', 'vaccination', 'media', 'social', 'public', 'pandemic', 'vaccines', 'vaccine', 'chambers', 'stances']","[""  The Covid-19 pandemic had an enormous effect on our lives, especially on\npeople's interactions. By introducing Covid-19 vaccines, both positive and\nnegative opinions were raised over the subject of taking vaccines or not. In\nthis paper, using data gathered from Twitter, including tweets and user\nprofiles, we offer a comprehensive analysis of public opinion in Iran about the\nCoronavirus vaccines. For this purpose, we applied a search query technique\ncombined with a topic modeling approach to extract vaccine-related tweets. We\nutilized transformer-based models to classify the content of the tweets and\nextract themes revolving around vaccination. We also conducted an emotion\nanalysis to evaluate the public happiness and anger around this topic. Our\nresults demonstrate that Covid-19 vaccination has attracted considerable\nattention from different angles, such as governmental issues, safety or\nhesitancy, and side effects. Moreover, Coronavirus-relevant phenomena like\npublic vaccination and the rate of infection deeply impacted public emotional\nstatus and users' interactions.\n"", '  Misinformation has emerged as a major societal threat in recent years in\ngeneral; specifically in the context of the COVID-19 pandemic, it has wrecked\nhavoc, for instance, by fuelling vaccine hesitancy. Cost-effective, scalable\nsolutions for combating misinformation are the need of the hour. This work\nexplored how existing information obtained from social media and augmented with\nmore curated fact checked data repositories can be harnessed to facilitate\nautomated rebuttal of misinformation at scale. While the ideas herein can be\ngeneralized and reapplied in the broader context of misinformation mitigation\nusing a multitude of information sources and catering to the spectrum of social\nmedia platforms, this work serves as a proof of concept, and as such, it is\nconfined in its scope to only rebuttal of tweets, and in the specific context\nof misinformation regarding COVID-19. It leverages two publicly available\ndatasets, viz. FaCov (fact-checked articles) and misleading (social media\nTwitter) data on COVID-19 Vaccination.\n', ""  A drastic rise in potentially life-threatening misinformation has been a\nby-product of the COVID-19 pandemic. Computational support to identify false\ninformation within the massive body of data on the topic is crucial to prevent\nharm. Researchers proposed many methods for flagging online misinformation\nrelated to COVID-19. However, these methods predominantly target specific\ncontent types (e.g., news) or platforms (e.g., Twitter). The methods'\ncapabilities to generalize were largely unclear so far. We evaluate fifteen\nTransformer-based models on five COVID-19 misinformation datasets that include\nsocial media posts, news articles, and scientific papers to fill this gap. We\nshow tokenizers and models tailored to COVID-19 data do not provide a\nsignificant advantage over general-purpose ones. Our study provides a realistic\nassessment of models for detecting COVID-19 misinformation. We expect that\nevaluating a broad spectrum of datasets and models will benefit future research\nin developing misinformation detection systems.\n""]",COVID-19 Vaccine Sentiment on Twitter,COVID-19 Research and Public Perception,COVID-19 Research and Public Perception,COVID-19 Research and Public Perception
343,23,343_drug_pharmaceutical_drugclip_pharmacology,"['drug', 'pharmaceutical', 'drugclip', 'pharmacology', 'drugs', 'graphs', 'predicting', 'graph', 'proteins', 'molecular']","['drug', 'drugs', 'repurposing', 'repositioning', 'graph', 'diseases', 'protein', 'phyla', 'biomedical', 'disease']","[""  Drug development is a lengthy process with a high failure rate. Increasingly,\nmachine learning is utilized to facilitate the drug development processes.\nThese models aim to enhance our understanding of drug characteristics,\nincluding their activity in biological contexts. However, a major challenge in\ndrug response (DR) prediction is model interpretability as it aids in the\nvalidation of findings. This is important in biomedicine, where models need to\nbe understandable in comparison with established knowledge of drug interactions\nwith proteins. drGAT, a graph deep learning model, leverages a heterogeneous\ngraph composed of relationships between proteins, cell lines, and drugs. drGAT\nis designed with two objectives: DR prediction as a binary sensitivity\nprediction and elucidation of drug mechanism from attention coefficients. drGAT\nhas demonstrated superior performance over existing models, achieving 78\\%\naccuracy (and precision), and 76\\% F1 score for 269 DNA-damaging compounds of\nthe NCI60 drug response dataset. To assess the model's interpretability, we\nconducted a review of drug-gene co-occurrences in Pubmed abstracts in\ncomparison to the top 5 genes with the highest attention coefficients for each\ndrug. We also examined whether known relationships were retained in the model\nby inspecting the neighborhoods of topoisomerase-related drugs. For example,\nour model retained TOP1 as a highly weighted predictive feature for irinotecan\nand topotecan, in addition to other genes that could potentially be regulators\nof the drugs. Our method can be used to accurately predict sensitivity to drugs\nand may be useful in the identification of biomarkers relating to the treatment\nof cancer patients.\n"", '  Drug-drug interaction prediction is a crucial issue in molecular biology.\nTraditional methods of observing drug-drug interactions through medical\nexperiments require significant resources and labor. This paper presents a\nmedical knowledge graph question answering model, dubbed MedKGQA, that predicts\ndrug-drug interaction by employing machine reading comprehension from\nclosed-domain literature and constructing a knowledge graph of drug-protein\ntriplets from open-domain documents. The model vectorizes the drug-protein\ntarget attributes in the graph using entity embeddings and establishes directed\nconnections between drug and protein entities based on the metabolic\ninteraction pathways of protein targets in the human body. This aligns multiple\nexternal knowledge and applies it to learn the graph neural network. Without\nbells and whistles, the proposed model achieved a 4.5% improvement in terms of\ndrug-drug interaction prediction accuracy compared to previous state-of-the-art\nmodels on the Qangaroo MedHop dataset. Experimental results demonstrate the\nefficiency and effectiveness of the model and verify the feasibility of\nintegrating external knowledge in machine reading comprehension tasks.\n', '  Drug synergy arises when the combined impact of two drugs exceeds the sum of\ntheir individual effects. While single-drug effects on cell lines are\nwell-documented, the scarcity of data on drug synergy, considering the vast\narray of potential drug combinations, prompts a growing interest in\ncomputational approaches for predicting synergies in untested drug pairs. We\nintroduce a Graph Neural Network (\\textit{GNN}) based model for drug synergy\nprediction, which utilizes drug chemical structures and cell line gene\nexpression data. We extract data from the largest available drug combination\ndatabase (DrugComb) and generate multiple synergy scores (commonly used in the\nliterature) to create seven datasets that serve as a reliable benchmark with\nhigh confidence. In contrast to conventional models relying on pre-computed\nchemical features, our GNN-based approach learns task-specific drug\nrepresentations directly from the graph structure of the drugs, providing\nsuperior performance in predicting drug synergies. Our work suggests that\nlearning task-specific drug representations and leveraging a diverse dataset is\na promising approach to advancing our understanding of drug-drug interaction\nand synergy.\n']",Predicting Drug Interactions and Synergies,Machine Learning for Pharmaceutical and Healthcare Applications,Machine Learning and Data-Driven Applications,Machine Learning and Data-Driven Applications
344,23,344_feature_features_supervised_selection,"['feature', 'features', 'supervised', 'selection', 'lasso', 'predictive', 'datasets', 'embedding', 'evaluator', 'selecting']","['feature', 'selection', 'features', 'subset', 'prototype', 'set', 'redundancy', 'space', 'search', 'evaluator']","['  Feature selection prepares the AI-readiness of data by eliminating redundant\nfeatures. Prior research falls into two primary categories: i) Supervised\nFeature Selection, which identifies the optimal feature subset based on their\nrelevance to the target variable; ii) Unsupervised Feature Selection, which\nreduces the feature space dimensionality by capturing the essential information\nwithin the feature set instead of using target variable. However, SFS\napproaches suffer from time-consuming processes and limited generalizability\ndue to the dependence on the target variable and downstream ML tasks. UFS\nmethods are constrained by the deducted feature space is latent and\nuntraceable. To address these challenges, we introduce an innovative framework\nfor feature selection, which is guided by knockoff features and optimized\nthrough reinforcement learning, to identify the optimal and effective feature\nsubset. In detail, our method involves generating ""knockoff"" features that\nreplicate the distribution and characteristics of the original features but are\nindependent of the target variable. Each feature is then assigned a pseudo\nlabel based on its correlation with all the knockoff features, serving as a\nnovel metric for feature evaluation. Our approach utilizes these pseudo labels\nto guide the feature selection process in 3 novel ways, optimized by a single\nreinforced agent: 1). A deep Q-network, pre-trained with the original features\nand their corresponding pseudo labels, is employed to improve the efficacy of\nthe exploration process in feature selection. 2). We introduce unsupervised\nrewards to evaluate the feature subset quality based on the pseudo labels and\nthe feature space reconstruction loss to reduce dependencies on the target\nvariable. 3). A new {\\epsilon}-greedy strategy is used, incorporating insights\nfrom the pseudo labels to make the feature selection process more effective.\n', '  Feature selection aims to identify the most pattern-discriminative feature\nsubset. In prior literature, filter (e.g., backward elimination) and embedded\n(e.g., Lasso) methods have hyperparameters (e.g., top-K, score thresholding)\nand tie to specific models, thus, hard to generalize; wrapper methods search a\nfeature subset in a huge discrete space and is computationally costly. To\ntransform the way of feature selection, we regard a selected feature subset as\na selection decision token sequence and reformulate feature selection as a deep\nsequential generative learning task that distills feature knowledge and\ngenerates decision sequences. Our method includes three steps: (1) We develop a\ndeep variational transformer model over a joint of sequential reconstruction,\nvariational, and performance evaluator losses. Our model can distill feature\nselection knowledge and learn a continuous embedding space to map feature\nselection decision sequences into embedding vectors associated with utility\nscores. (2) We leverage the trained feature subset utility evaluator as a\ngradient provider to guide the identification of the optimal feature subset\nembedding;(3) We decode the optimal feature subset embedding to\nautoregressively generate the best feature selection decision sequence with\nautostop. Extensive experimental results show this generative perspective is\neffective and generic, without large discrete search space and expert-specific\nhyperparameters.\n', '  Feature selection aims to identify the optimal feature subset for enhancing\ndownstream models. Effective feature selection can remove redundant features,\nsave computational resources, accelerate the model learning process, and\nimprove the model overall performance. However, existing works are often\ntime-intensive to identify the effective feature subset within high-dimensional\nfeature spaces. Meanwhile, these methods mainly utilize a single downstream\ntask performance as the selection criterion, leading to the selected subsets\nthat are not only redundant but also lack generalizability. To bridge these\ngaps, we reformulate feature selection through a neuro-symbolic lens and\nintroduce a novel generative framework aimed at identifying short and effective\nfeature subsets. More specifically, we found that feature ID tokens of the\nselected subset can be formulated as symbols to reflect the intricate\ncorrelations among features. Thus, in this framework, we first create a data\ncollector to automatically collect numerous feature selection samples\nconsisting of feature ID tokens, model performance, and the measurement of\nfeature subset redundancy. Building on the collected data, an\nencoder-decoder-evaluator learning paradigm is developed to preserve the\nintelligence of feature selection into a continuous embedding space for\nefficient search. Within the learned embedding space, we leverage a\nmulti-gradient search algorithm to find more robust and generalized embeddings\nwith the objective of improving model performance and reducing feature subset\nredundancy. These embeddings are then utilized to reconstruct the feature ID\ntokens for executing the final feature selection. Ultimately, comprehensive\nexperiments and case studies are conducted to validate the effectiveness of the\nproposed framework.\n']",Feature Selection Methods for Machine Learning,Feature Selection and Optimization in Machine Learning,Machine Learning and Artificial Intelligence,Machine Learning and Intelligent Systems
345,22,345_unlearning_unlearn_forgetting_unlearned,"['unlearning', 'unlearn', 'forgetting', 'unlearned', 'erase', 'forget', 'erasure', 'remembering', 'forgotten', 'amnesiac']","['unlearning', 'forget', 'forgetting', 'deletion', 'retraining', 'amnesiac', 'removal', 'machine', 'remove', 'exact']","['  To comply with AI and data regulations, the need to forget private or\ncopyrighted information from trained machine learning models is increasingly\nimportant. The key challenge in unlearning is forgetting the necessary data in\na timely manner, while preserving model performance. In this work, we address\nthe zero-shot unlearning scenario, whereby an unlearning algorithm must be able\nto remove data given only a trained model and the data to be forgotten. We\nexplore unlearning from an information theoretic perspective, connecting the\ninfluence of a sample to the information gain a model receives by observing it.\nFrom this, we derive a simple but principled zero-shot unlearning method based\non the geometry of the model. Our approach takes the form of minimising the\ngradient of a learned function with respect to a small neighbourhood around a\ntarget forget point. This induces a smoothing effect, causing forgetting by\nmoving the boundary of the classifier. We explore the intuition behind why this\napproach can jointly unlearn forget samples while preserving general model\nperformance through a series of low-dimensional experiments. We perform\nextensive empirical evaluation of our method over a range of contemporary\nbenchmarks, verifying that our method is competitive with state-of-the-art\nperformance under the strict constraints of zero-shot unlearning.\n', '  Machine unlearning is an emerging technology that has come to attract\nwidespread attention. A number of factors, including regulations and laws,\nprivacy, and usability concerns, have resulted in this need to allow a trained\nmodel to forget some of its training data. Existing studies of machine\nunlearning mainly focus on unlearning requests that forget a cluster of\ninstances or all instances from one class. While these approaches are effective\nin removing instances, they do not scale to scenarios where partial targets\nwithin an instance need to be forgotten. For example, one would like to only\nunlearn a person from all instances that simultaneously contain the person and\nother targets. Directly migrating instance-level unlearning to target-level\nunlearning will reduce the performance of the model after the unlearning\nprocess, or fail to erase information completely. To address these concerns, we\nhave proposed a more effective and efficient unlearning scheme that focuses on\nremoving partial targets from the model, which we name ""target unlearning"".\nSpecifically, we first construct an essential graph data structure to describe\nthe relationships between all important parameters that are selected based on\nthe model explanation method. After that, we simultaneously filter parameters\nthat are also important for the remaining targets and use the pruning-based\nunlearning method, which is a simple but effective solution to remove\ninformation about the target that needs to be forgotten. Experiments with\ndifferent training models on various datasets demonstrate the effectiveness of\nthe proposed approach.\n', '  In response to recent data regulation requirements, machine unlearning (MU)\nhas emerged as a critical process to remove the influence of specific examples\nfrom a given model. Although exact unlearning can be achieved through complete\nmodel retraining using the remaining dataset, the associated computational\ncosts have driven the development of efficient, approximate unlearning\ntechniques. Moving beyond data-centric MU approaches, our study introduces a\nnovel model-based perspective: model sparsification via weight pruning, which\nis capable of reducing the gap between exact unlearning and approximate\nunlearning. We show in both theory and practice that model sparsity can boost\nthe multi-criteria unlearning performance of an approximate unlearner, closing\nthe approximation gap, while continuing to be efficient. This leads to a new MU\nparadigm, termed prune first, then unlearn, which infuses a sparse model prior\ninto the unlearning process. Building on this insight, we also develop a\nsparsity-aware unlearning method that utilizes sparsity regularization to\nenhance the training process of approximate unlearning. Extensive experiments\nshow that our proposals consistently benefit MU in various unlearning\nscenarios. A notable highlight is the 77% unlearning efficacy gain of\nfine-tuning (one of the simplest unlearning methods) when using sparsity-aware\nunlearning. Furthermore, we demonstrate the practical impact of our proposed MU\nmethods in addressing other machine learning challenges, such as defending\nagainst backdoor attacks and enhancing transfer learning. Codes are available\nat https://github.com/OPTML-Group/Unlearn-Sparse.\n']",Machine Unlearning and Forgetting in AI Models,Machine Unlearning and Forgetting in Artificial Intelligence,Machine Learning Adaptation and Forgetting,Machine Learning Adaptation and Forgetting
346,22,346_tracking_tracklets_tracker_trackers,"['tracking', 'tracklets', 'tracker', 'trackers', 'tracklet', 'track', 'tracks', 'frames', 'scenes', 'detections']","['tracking', 'tracker', 'association', 'trackers', 'event', 'object', 'amodal', 'frames', 'objects', 'tracklets']","['  We observe that the performance of SOTA visual trackers surprisingly strongly\nvaries across different video attributes and datasets. No single tracker\nremains the best performer across all tracking attributes and datasets. To\nbridge this gap, for a given video sequence, we predict the ""Best of the N\nTrackers"", called the BofN meta-tracker. At its core, a Tracking Performance\nPrediction Network (TP2N) selects a predicted best performing visual tracker\nfor the given video sequence using only a few initial frames. We also introduce\na frame-level BofN meta-tracker which keeps predicting best performer after\nregular temporal intervals. The TP2N is based on self-supervised learning\narchitectures MocoV2, SwAv, BT, and DINO; experiments show that the DINO with\nViT-S as a backbone performs the best. The video-level BofN meta-tracker\noutperforms, by a large margin, existing SOTA trackers on nine standard\nbenchmarks - LaSOT, TrackingNet, GOT-10K, VOT2019, VOT2021, VOT2022, UAV123,\nOTB100, and WebUAV-3M. Further improvement is achieved by the frame-level BofN\nmeta-tracker effectively handling variations in the tracking scenarios within\nlong sequences. For instance, on GOT-10k, BofN meta-tracker average overlap is\n88.7% and 91.1% with video and frame-level settings respectively. The best\nperforming tracker, RTS, achieves 85.20% AO. On VOT2022, BofN expected average\noverlap is 67.88% and 70.98% with video and frame level settings, compared to\nthe best performing ARTrack, 64.12%. This work also presents an extensive\nevaluation of competitive tracking methods on all commonly used benchmarks,\nfollowing their protocols. The code, the trained models, and the results will\nsoon be made publicly available on\nhttps://github.com/BasitAlawode/Best_of_N_Trackers.\n', '  Current event-/frame-event based trackers undergo evaluation on short-term\ntracking datasets, however, the tracking of real-world scenarios involves\nlong-term tracking, and the performance of existing tracking algorithms in\nthese scenarios remains unclear. In this paper, we first propose a new\nlong-term and large-scale frame-event single object tracking dataset, termed\nFELT. It contains 742 videos and 1,594,474 RGB frames and event stream pairs\nand has become the largest frame-event tracking dataset to date. We re-train\nand evaluate 15 baseline trackers on our dataset for future works to compare.\nMore importantly, we find that the RGB frames and event streams are naturally\nincomplete due to the influence of challenging factors and spatially sparse\nevent flow. In response to this, we propose a novel associative memory\nTransformer network as a unified backbone by introducing modern Hopfield layers\ninto multi-head self-attention blocks to fuse both RGB and event data.\nExtensive experiments on RGB-Event (FELT), RGB-Thermal (RGBT234, LasHeR), and\nRGB-Depth (DepthTrack) datasets fully validated the effectiveness of our model.\nThe dataset and source code can be found at\n\\url{https://github.com/Event-AHU/FELT_SOT_Benchmark}.\n', '  RGB-Event based tracking is an emerging research topic, focusing on how to\neffectively integrate heterogeneous multi-modal data (synchronized exposure\nvideo frames and asynchronous pulse Event stream). Existing works typically\nemploy Transformer based networks to handle these modalities and achieve decent\naccuracy through input-level or feature-level fusion on multiple datasets.\nHowever, these trackers require significant memory consumption and\ncomputational complexity due to the use of self-attention mechanism. This paper\nproposes a novel RGB-Event tracking framework, Mamba-FETrack, based on the\nState Space Model (SSM) to achieve high-performance tracking while effectively\nreducing computational costs and realizing more efficient tracking.\nSpecifically, we adopt two modality-specific Mamba backbone networks to extract\nthe features of RGB frames and Event streams. Then, we also propose to boost\nthe interactive learning between the RGB and Event features using the Mamba\nnetwork. The fused features will be fed into the tracking head for target\nobject localization. Extensive experiments on FELT and FE108 datasets fully\nvalidated the efficiency and effectiveness of our proposed tracker.\nSpecifically, our Mamba-based tracker achieves 43.5/55.6 on the SR/PR metric,\nwhile the ViT-S based tracker (OSTrack) obtains 40.0/50.9. The GPU memory cost\nof ours and ViT-S based tracker is 13.98GB and 15.44GB, which decreased about\n$9.5\\%$. The FLOPs and parameters of ours/ViT-S based OSTrack are 59GB/1076GB\nand 7MB/60MB, which decreased about $94.5\\%$ and $88.3\\%$, respectively. We\nhope this work can bring some new insights to the tracking field and greatly\npromote the application of the Mamba architecture in tracking. The source code\nof this work will be released on\n\\url{https://github.com/Event-AHU/Mamba_FETrack}.\n']",Visual Object Tracking,Computer Vision and Object Recognition,Computer Vision,Computer Vision
347,22,347_multimodal_fusioninn_fusion_multinpe,"['multimodal', 'fusioninn', 'fusion', 'multinpe', 'cnn', 'fused', 'modality', 'multifix', 'deep', 'saliency']","['fusion', 'multimodal', 'modalities', 'fused', 'modal', 'clinical', 'modality', 'medical', 'imaging', 'images']","['  Multi-modal fusion is crucial in medical data research, enabling a\ncomprehensive understanding of diseases and improving diagnostic performance by\ncombining diverse modalities. However, multi-modal fusion faces challenges,\nincluding capturing interactions between modalities, addressing missing\nmodalities, handling erroneous modal information, and ensuring\ninterpretability. Many existing researchers tend to design different solutions\nfor these problems, often overlooking the commonalities among them. This paper\nproposes a novel multi-modal fusion framework that achieves adaptive adjustment\nover the weights of each modality by introducing the Modal-Domain Attention\n(MDA). It aims to facilitate the fusion of multi-modal information while\nallowing for the inclusion of missing modalities or intrinsic noise, thereby\nenhancing the representation of multi-modal data. We provide visualizations of\naccuracy changes and MDA weights by observing the process of modal fusion,\noffering a comprehensive analysis of its interpretability. Extensive\nexperiments on various gastrointestinal disease benchmarks, the proposed MDA\nmaintains high accuracy even in the presence of missing modalities and\nintrinsic noise. One thing worth mentioning is that the visualization of MDA is\nhighly consistent with the conclusions of existing clinical studies on the\ndependence of different diseases on various modalities. Code and dataset will\nbe made available.\n', '  Image fusion typically employs non-invertible neural networks to merge\nmultiple source images into a single fused image. However, for clinical\nexperts, solely relying on fused images may be insufficient for making\ndiagnostic decisions, as the fusion mechanism blends features from source\nimages, thereby making it difficult to interpret the underlying tumor\npathology. We introduce FusionINN, a novel decomposable image fusion framework,\ncapable of efficiently generating fused images and also decomposing them back\nto the source images. FusionINN is designed to be bijective by including a\nlatent image alongside the fused image, while ensuring minimal transfer of\ninformation from the source images to the latent representation. To the best of\nour knowledge, we are the first to investigate the decomposability of fused\nimages, which is particularly crucial for life-sensitive applications such as\nmedical image fusion compared to other tasks like multi-focus or multi-exposure\nimage fusion. Our extensive experimentation validates FusionINN over existing\ndiscriminative and generative fusion methods, both subjectively and\nobjectively. Moreover, compared to a recent denoising diffusion-based fusion\nmodel, our approach offers faster and qualitatively better fusion results.\n', '  Multimodal medical imaging plays a pivotal role in clinical diagnosis and\nresearch, as it combines information from various imaging modalities to provide\na more comprehensive understanding of the underlying pathology. Recently, deep\nlearning-based multimodal fusion techniques have emerged as powerful tools for\nimproving medical image classification. This review offers a thorough analysis\nof the developments in deep learning-based multimodal fusion for medical\nclassification tasks. We explore the complementary relationships among\nprevalent clinical modalities and outline three main fusion schemes for\nmultimodal classification networks: input fusion, intermediate fusion\n(encompassing single-level fusion, hierarchical fusion, and attention-based\nfusion), and output fusion. By evaluating the performance of these fusion\ntechniques, we provide insight into the suitability of different network\narchitectures for various multimodal fusion scenarios and application domains.\nFurthermore, we delve into challenges related to network architecture\nselection, handling incomplete multimodal data management, and the potential\nlimitations of multimodal fusion. Finally, we spotlight the promising future of\nTransformer-based multimodal fusion techniques and give recommendations for\nfuture research in this rapidly evolving field.\n']",Multimodal Medical Image Fusion Techniques,Multimodal Learning and Fusion,Multimodal Learning and Applications,Multimodal Learning and Vision-Language Models
348,22,348_grading_graded_assessments_assessment,"['grading', 'graded', 'assessments', 'assessment', 'grades', 'graders', 'grade', 'exams', 'exam', 'students']","['grading', 'rubrics', 'scoring', 'rubric', 'grade', 'answer', 'students', 'student', 'short', 'formative']","[""  Large language models (LLMs) have demonstrated strong potential in performing\nautomatic scoring for constructed response assessments. While constructed\nresponses graded by humans are usually based on given grading rubrics, the\nmethods by which LLMs assign scores remain largely unclear. It is also\nuncertain how closely AI's scoring process mirrors that of humans, or if it\nadheres to the same grading criteria. To address this gap, this paper uncovers\nthe grading rubrics that LLMs used to score students' written responses to\nscience tasks and their alignment with human scores. We also examine whether\nenhancing the alignments can improve scoring accuracy. Specifically, we prompt\nLLMs to generate analytic rubrics that they use to assign scores and study the\nalignment gap with human grading rubrics. Based on a series of experiments with\nvarious configurations of LLM settings, we reveal a notable alignment gap\nbetween human and LLM graders. While LLMs can adapt quickly to scoring tasks,\nthey often resort to shortcuts, bypassing deeper logical reasoning expected in\nhuman grading. We found that incorporating high-quality analytical rubrics\ndesigned to reflect human grading logic can mitigate this gap and enhance LLMs'\nscoring accuracy. These results caution against the simplistic application of\nLLMs in science education and highlight the importance of aligning LLM outputs\nwith human expectations to ensure efficient and accurate automatic scoring.\n"", ""  We explore the use of deep reinforcement learning to audit an automatic short\nanswer grading (ASAG) model. Automatic grading may decrease the time burden of\nrating open-ended items for educators, but a lack of robust evaluation methods\nfor these models can result in uncertainty of their quality. Current\nstate-of-the-art ASAG models are configured to match human ratings from a\ntraining set, and researchers typically assess their quality with accuracy\nmetrics that signify agreement between model and human scores. In this paper,\nwe show that a high level of agreement to human ratings does not give\nsufficient evidence that an ASAG model is infallible. We train a reinforcement\nlearning agent to revise student responses with the objective of achieving a\nhigh rating from an automatic grading model in the least number of revisions.\nBy analyzing the agent's revised responses that achieve a high grade from the\nASAG model but would not be considered a high scoring responses according to a\nscoring rubric, we discover ways in which the automated grader can be\nexploited, exposing shortcomings in the grading model.\n"", ""  While large language models (LLMs) have been used for automated grading, they\nhave not yet achieved the same level of performance as humans, especially when\nit comes to grading complex questions. Existing research on this topic focuses\non a particular step in the grading procedure: grading using predefined\nrubrics. However, grading is a multifaceted procedure that encompasses other\ncrucial steps, such as grading rubrics design and post-grading review. There\nhas been a lack of systematic research exploring the potential of LLMs to\nenhance the entire grading~process.\n  In this paper, we propose an LLM-based grading system that addresses the\nentire grading procedure, including the following key components: 1) Developing\ngrading rubrics that not only consider the questions but also the student\nanswers, which can more accurately reflect students' performance. 2) Under the\nguidance of grading rubrics, providing accurate and consistent scores for each\nstudent, along with customized feedback. 3) Conducting post-grading review to\nbetter ensure accuracy and fairness. Additionally, we collected a new dataset\nnamed OS from a university operating system course and conducted extensive\nexperiments on both our new dataset and the widely used Mohler dataset.\nExperiments demonstrate the effectiveness of our proposed approach, providing\nsome new insights for developing automated grading systems based on LLMs.\n""]",Automated Grading and Assessment with Large Language Models,Automated Assessment and Grading in Education,Artificial Intelligence in Education,Artificial Intelligence in Education
349,22,349_outlier_outliers_outlierness_inlier,"['outlier', 'outliers', 'outlierness', 'inlier', 'inliers', 'anomaly', 'supervised', 'robust', 'unsupervised', 'detection']","['outlier', 'outliers', 'inliers', 'detection', 'inlier', 'neighbors', 'negative', 'normal', 'unsupervised', 'likelihood']","['  Graph outlier detection is a prominent task of research and application in\nthe realm of graph neural networks. It identifies the outlier nodes that\nexhibit deviation from the majority in the graph. One of the fundamental\nchallenges confronting supervised graph outlier detection algorithms is the\nprevalent issue of class imbalance, where the scarcity of outlier instances\ncompared to normal instances often results in suboptimal performance.\nConventional methods mitigate the imbalance by reweighting instances in the\nestimation of the loss function, assigning higher weights to outliers and lower\nweights to inliers. Nonetheless, these strategies are prone to overfitting and\nunderfitting, respectively. Recently, generative models, especially diffusion\nmodels, have demonstrated their efficacy in synthesizing high-fidelity images.\nDespite their extraordinary generation quality, their potential in data\naugmentation for supervised graph outlier detection remains largely\nunderexplored.\n  To bridge this gap, we introduce GODM, a novel data augmentation for\nmitigating class imbalance in supervised Graph Outlier detection with latent\nDiffusion Models. Specifically, our proposed method consists of three key\ncomponents: (1) Variantioanl Encoder maps the heterogeneous information\ninherent within the graph data into a unified latent space. (2) Graph Generator\nsynthesizes graph data that are statistically similar to real outliers from\nlatent space, and (3) Latent Diffusion Model learns the latent space\ndistribution of real organic data by iterative denoising. Extensive experiments\nconducted on multiple datasets substantiate the effectiveness and efficiency of\nGODM. The case study further demonstrated the generation quality of our\nsynthetic data. To foster accessibility and reproducibility, we encapsulate\nGODM into a plug-and-play package and release it at the Python Package Index\n(PyPI).\n', '  In recent years, multi-view outlier detection (MVOD) methods have advanced\nsignificantly, aiming to identify outliers within multi-view datasets. A key\npoint is to better detect class outliers and class-attribute outliers, which\nonly exist in multi-view data. However, existing methods either is not able to\nreduce the impact of outliers when learning view-consistent information, or\nstruggle in cases with varying neighborhood structures. Moreover, most of them\ndo not apply to partial multi-view data in real-world scenarios. To overcome\nthese drawbacks, we propose a novel method named Regularized Contrastive\nPartial Multi-view Outlier Detection (RCPMOD). In this framework, we utilize\ncontrastive learning to learn view-consistent information and distinguish\noutliers by the degree of consistency. Specifically, we propose (1) An\noutlier-aware contrastive loss with a potential outlier memory bank to\neliminate their bias motivated by a theoretical analysis. (2) A neighbor\nalignment contrastive loss to capture the view-shared local structural\ncorrelation. (3) A spreading regularization loss to prevent the model from\noverfitting over outliers. With the Cross-view Relation Transfer technique, we\ncould easily impute the missing view samples based on the features of\nneighbors. Experimental results on four benchmark datasets demonstrate that our\nproposed approach could outperform state-of-the-art competitors under different\nsettings.\n', '  Discriminative learning effectively predicts true object class for image\nclassification. However, it often results in false positives for outliers,\nposing critical concerns in applications like autonomous driving and video\nsurveillance systems. Previous attempts to address this challenge involved\ntraining image classifiers through contrastive learning using actual outlier\ndata or synthesizing outliers for self-supervised learning. Furthermore,\nunsupervised generative modeling of inliers in pixel space has shown limited\nsuccess for outlier detection. In this work, we introduce a quantile-based\nmaximum likelihood objective for learning the inlier distribution to improve\nthe outlier separation during inference. Our approach fits a normalizing flow\nto pre-trained discriminative features and detects the outliers according to\nthe evaluated log-likelihood. The experimental evaluation demonstrates the\neffectiveness of our method as it surpasses the performance of the\nstate-of-the-art unsupervised methods for outlier detection. The results are\nalso competitive compared with a recent self-supervised approach for outlier\ndetection. Our work allows to reduce dependency on well-sampled negative\ntraining data, which is especially important for domains like medical\ndiagnostics or remote sensing.\n']",Outlier Detection Methods,Anomaly and Outlier Detection Methods,Data Analysis and Pattern Discovery,Data Analysis and Pattern Discovery
350,22,350_federated_fairness_fairfrs_distributed,"['federated', 'fairness', 'fairfrs', 'distributed', 'collaborative', 'incentive', 'collaboratively', 'fedms', 'equitable', 'decentralized']","['fairness', 'clients', 'client', 'federated', 'fair', 'group', 'participation', 'contribution', 'drift', 'heterogeneity']","[""  Federated Learning (FL) is an emerging paradigm in machine learning without\nexposing clients' raw data. In practical scenarios with numerous clients,\nencouraging fair and efficient client participation in federated learning is of\nutmost importance, which is also challenging given the heterogeneity in data\ndistribution and device properties. Existing works have proposed different\nclient-selection methods that consider fairness; however, they fail to select\nclients with high utilities while simultaneously achieving fair accuracy\nlevels. In this paper, we propose a fair client-selection approach that unlocks\nthreefold fairness in federated learning. In addition to having a fair\nclient-selection strategy, we enforce an equitable number of rounds for client\nparticipation and ensure a fair accuracy distribution over the clients. The\nexperimental results demonstrate that FedFair^3, in comparison to the\nstate-of-the-art baselines, achieves 18.15% less accuracy variance on the IID\ndata and 54.78% on the non-IID data, without decreasing the global accuracy.\nFurthermore, it shows 24.36% less wall-clock training time on average.\n"", ""  Federated learning (FL) has emerged as a prospective solution for\ncollaboratively learning a shared model across clients without sacrificing\ntheir data privacy. However, the federated learned model tends to be biased\nagainst certain demographic groups (e.g., racial and gender groups) due to the\ninherent FL properties, such as data heterogeneity and party selection. Unlike\ncentralized learning, mitigating bias in FL is particularly challenging as\nprivate training datasets and their sensitive attributes are typically not\ndirectly accessible. Most prior research in this field only focuses on global\nfairness while overlooking the local fairness of individual clients. Moreover,\nexisting methods often require sensitive information about the client's local\ndatasets to be shared, which is not desirable. To address these issues, we\npropose GLOCALFAIR, a client-server co-design fairness framework that can\njointly improve global and local group fairness in FL without the need for\nsensitive statistics about the client's private datasets. Specifically, we\nutilize constrained optimization to enforce local fairness on the client side\nand adopt a fairness-aware clustering-based aggregation on the server to\nfurther ensure the global model fairness across different sensitive groups\nwhile maintaining high utility. Experiments on two image datasets and one\ntabular dataset with various state-of-the-art fairness baselines show that\nGLOCALFAIR can achieve enhanced fairness under both global and local data\ndistributions while maintaining a good level of utility and client fairness.\n"", ""  Federated Learning (FL) is a privacy-enhancing technology for distributed ML.\nBy training models locally and aggregating updates - a federation learns\ntogether, while bypassing centralised data collection. FL is increasingly\npopular in healthcare, finance and personal computing. However, it inherits\nfairness challenges from classical ML and introduces new ones, resulting from\ndifferences in data quality, client participation, communication constraints,\naggregation methods and underlying hardware. Fairness remains an unresolved\nissue in FL and the community has identified an absence of succinct definitions\nand metrics to quantify fairness; to address this, we propose Federated\nFairness Analytics - a methodology for measuring fairness. Our definition of\nfairness comprises four notions with novel, corresponding metrics. They are\nsymptomatically defined and leverage techniques originating from XAI,\ncooperative game-theory and networking engineering. We tested a range of\nexperimental settings, varying the FL approach, ML task and data settings. The\nresults show that statistical heterogeneity and client participation affect\nfairness and fairness conscious approaches such as Ditto and q-FedAvg\nmarginally improve fairness-performance trade-offs. Using our techniques, FL\npractitioners can uncover previously unobtainable insights into their system's\nfairness, at differing levels of granularity in order to address fairness\nchallenges in FL. We have open-sourced our work at:\nhttps://github.com/oscardilley/federated-fairness.\n""]",Federated Learning Fairness,Federated Learning,Machine Learning and Data Privacy,Machine Learning and Data Privacy
351,22,351_multiomics_transcriptomics_genomics_biomarker,"['multiomics', 'transcriptomics', 'genomics', 'biomarker', 'genome', 'biomarkers', 'genomic', 'oncology', 'cancer', 'prognosis']","['cancer', 'omics', 'gene', 'genes', 'biomarkers', 'biological', 'subtypes', 'tumor', 'diseases', 'pan']","['  The application of machine learning to transcriptomics data has led to\nsignificant advances in cancer research. However, the high dimensionality and\ncomplexity of RNA sequencing (RNA-seq) data pose significant challenges in\npan-cancer studies. This study hypothesizes that gene sets derived from\nsingle-cell RNA sequencing (scRNA-seq) data will outperform those selected\nusing bulk RNA-seq in pan-cancer downstream tasks. We analyzed scRNA-seq data\nfrom 181 tumor biopsies across 13 cancer types. High-dimensional weighted gene\nco-expression network analysis (hdWGCNA) was performed to identify relevant\ngene sets, which were further refined using XGBoost for feature selection.\nThese gene sets were applied to downstream tasks using TCGA pan-cancer RNA-seq\ndata and compared to six reference gene sets and oncogenes from OncoKB\nevaluated with deep learning models, including multilayer perceptrons (MLPs)\nand graph neural networks (GNNs). The XGBoost-refined hdWGCNA gene set\ndemonstrated higher performance in most tasks, including tumor mutation burden\nassessment, microsatellite instability classification, mutation prediction,\ncancer subtyping, and grading. In particular, genes such as DPM1, BAD, and\nFKBP4 emerged as important pan-cancer biomarkers, with DPM1 consistently\nsignificant across tasks. This study presents a robust approach for feature\nselection in cancer genomics by integrating scRNA-seq data and advanced\nanalysis techniques, offering a promising avenue for improving predictive\naccuracy in cancer research.\n', ""  Multi-omics research has enhanced our understanding of cancer heterogeneity\nand progression. Investigating molecular data through multi-omics approaches is\ncrucial for unraveling the complex biological mechanisms underlying cancer,\nthereby enabling effective diagnosis, treatment, and prevention strategies.\nHowever, predicting patient outcomes through integration of all available\nmulti-omics data is an under-study research direction. Here, we present SeNMo\n(Self-normalizing Network for Multi-omics), a deep neural network trained on\nmulti-omics data across 33 cancer types. SeNMo is efficient in handling\nmulti-omics data characterized by high-width (many features) and low-length\n(fewer samples) attributes. We trained SeNMo for the task of overall survival\nusing pan-cancer data involving 33 cancer sites from Genomics Data Commons\n(GDC). The training data includes gene expression, DNA methylation, miRNA\nexpression, DNA mutations, protein expression modalities, and clinical data. We\nevaluated the model's performance in predicting overall survival using\nconcordance index (C-Index). SeNMo performed consistently well in training\nregime, with the validation C-Index of 0.76 on GDC's public data. In the\ntesting regime, SeNMo performed with a C-Index of 0.758 on a held-out test set.\nThe model showed an average accuracy of 99.8% on the task of classifying the\nprimary cancer type on the pan-cancer test cohort. SeNMo proved to be a\nmini-foundation model for multi-omics oncology data because it demonstrated\nrobust performance, and adaptability not only across molecular data types but\nalso on the classification task of predicting the primary cancer type of\npatients. SeNMo can be further scaled to any cancer site and molecular data\ntype. We believe SeNMo and similar models are poised to transform the oncology\nlandscape, offering hope for more effective, efficient, and patient-centric\ncancer care.\n"", '  The recent development of high-throughput sequencing creates a large\ncollection of multi-omics data, which enables researchers to better investigate\ncancer molecular profiles and cancer taxonomy based on molecular subtypes.\nIntegrating multi-omics data has been proven to be effective for building more\nprecise classification models. Current multi-omics integrative models mainly\nuse early fusion by concatenation or late fusion based on deep neural networks.\nDue to the nature of biological systems, graphs are a better representation of\nbio-medical data. Although few graph neural network (GNN) based multi-omics\nintegrative methods have been proposed, they suffer from three common\ndisadvantages. One is most of them use only one type of connection, either\ninter-omics or intra-omic connection; second, they only consider one kind of\nGNN layer, either graph convolution network (GCN) or graph attention network\n(GAT); and third, most of these methods lack testing on a more complex cancer\nclassification task. We propose a novel end-to-end multi-omics GNN framework\nfor accurate and robust cancer subtype classification. The proposed model\nutilizes multi-omics data in the form of heterogeneous multi-layer graphs that\ncombines both inter-omics and intra-omic connections from established\nbiological knowledge. The proposed model incorporates learned graph features\nand global genome features for accurate classification. We test the proposed\nmodel on TCGA Pan-cancer dataset and TCGA breast cancer dataset for molecular\nsubtype and cancer subtype classification, respectively. The proposed model\noutperforms four current state-of-the-art baseline models in multiple\nevaluation metrics. The comparative analysis of GAT-based models and GCN-based\nmodels reveals that GAT-based models are preferred for smaller graphs with less\ninformation and GCN-based models are preferred for larger graphs with extra\ninformation.\n']",Multi-Omics in Cancer Research and Genomics,Computational Methods for Cancer Genomics and Transcriptomics,Computational Biology and Chemistry,Computational Biology and Chemistry
352,22,352_timegraphs_temporal_chronological_future,"['timegraphs', 'temporal', 'chronological', 'future', 'prediction', 'knowledge', 'relational', 'predict', 'timestamps', 'reasoning']","['temporal', 'historical', 'events', 'reasoning', 'timestamps', 'history', 'facts', 'knowledge', 'quadruples', 'graph']","['  Temporal Knowledge Graph (TKG), which characterizes temporally evolving facts\nin the form of (subject, relation, object, timestamp), has attracted much\nattention recently. TKG reasoning aims to predict future facts based on given\nhistorical ones. However, existing TKG reasoning models are unable to abstain\nfrom predictions they are uncertain, which will inevitably bring risks in\nreal-world applications. Thus, in this paper, we propose an abstention\nmechanism for TKG reasoning, which helps the existing models make selective,\ninstead of indiscriminate, predictions. Specifically, we develop a confidence\nestimator, called Confidence Estimator with History (CEHis), to enable the\nexisting TKG reasoning models to first estimate their confidence in making\npredictions, and then abstain from those with low confidence. To do so, CEHis\ntakes two kinds of information into consideration, namely, the certainty of the\ncurrent prediction and the accuracy of historical predictions. Experiments with\nrepresentative TKG reasoning models on two benchmark datasets demonstrate the\neffectiveness of the proposed CEHis.\n', '  Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based\non given histories. Most recent graph-based models excel at capturing\nstructural information within TKGs but lack semantic comprehension abilities.\nNowadays, with the surge of LLMs, the LLM-based TKG prediction model has\nemerged. However, the existing LLM-based model exhibits three shortcomings: (1)\nIt only focuses on the first-order history for prediction while ignoring\nhigh-order historical information, resulting in the provided information for\nLLMs being extremely limited. (2) LLMs struggle with optimal reasoning\nperformance under heavy historical information loads. (3) For TKG prediction,\nthe temporal reasoning capability of LLM alone is limited. To address the first\ntwo challenges, we propose Chain-of-History (CoH) reasoning which explores\nhigh-order histories step-by-step, achieving effective utilization of\nhigh-order historical information for LLMs on TKG prediction. To address the\nthird issue, we design CoH as a plug-and-play module to enhance the performance\nof graph-based models for TKG prediction. Extensive experiments on three\ndatasets and backbones demonstrate the effectiveness of CoH.\n', '  Temporal Knowledge Graph (TKG) reasoning that forecasts future events based\non historical snapshots distributed over timestamps is denoted as extrapolation\nand has gained significant attention. Owing to its extreme versatility and\nvariation in spatial and temporal correlations, TKG reasoning presents a\nchallenging task, demanding efficient capture of concurrent structures and\nevolutional interactions among facts. While existing methods have made strides\nin this direction, they still fall short of harnessing the diverse forms of\nintrinsic expressive semantics of TKGs, which encompass entity correlations\nacross multiple timestamps and periodicity of temporal information. This\nlimitation constrains their ability to thoroughly reflect historical\ndependencies and future trends. In response to these drawbacks, this paper\nproposes an innovative reasoning approach that focuses on Learning Multi-graph\nStructure (LMS). Concretely, it comprises three distinct modules concentrating\non multiple aspects of graph structure knowledge within TKGs, including\nconcurrent and evolutional patterns along timestamps, query-specific\ncorrelations across timestamps, and semantic dependencies of timestamps, which\ncapture TKG features from various perspectives. Besides, LMS incorporates an\nadaptive gate for merging entity representations both along and across\ntimestamps effectively. Moreover, it integrates timestamp semantics into graph\nattention calculations and time-aware decoders, in order to impose temporal\nconstraints on events and narrow down prediction scopes with historical\nstatistics. Extensive experimental results on five event-based benchmark\ndatasets demonstrate that LMS outperforms state-of-the-art extrapolation\nmodels, indicating the superiority of modeling a multi-graph perspective for\nTKG reasoning.\n']",Temporal Knowledge Graph Reasoning,Temporal Knowledge Graph Reasoning and Question Answering,Artificial Intelligence and Reasoning Systems,Intelligent Systems
353,22,353_federated_learning_distributed_generalization,"['federated', 'learning', 'distributed', 'generalization', 'adaptive', 'algorithms', 'training', 'centralized', 'fedams', 'gradients']","['clients', 'federated', 'convergence', 'communication', 'local', 'heterogeneity', 'gradients', 'server', 'round', 'convex']","['  There are two paradigms in Federated Learning (FL): parallel FL (PFL), where\nmodels are trained in a parallel manner across clients; and sequential FL\n(SFL), where models are trained in a sequential manner across clients. In\ncontrast to that of PFL, the convergence theory of SFL on heterogeneous data is\nstill lacking. To resolve the theoretical dilemma of SFL, we establish sharp\nconvergence guarantees for SFL on heterogeneous data with both upper and lower\nbounds. Specifically, we derive the upper bounds for strongly convex, general\nconvex and non-convex objective functions, and construct the matching lower\nbounds for the strongly convex and general convex objective functions. Then, we\ncompare the upper bounds of SFL with those of PFL, showing that SFL outperforms\nPFL (at least, when the level of heterogeneity is relatively high).\nExperimental results on quadratic functions and real data sets validate the\ncounterintuitive comparison result.\n', '  There are two categories of methods in Federated Learning (FL) for joint\ntraining across multiple clients: i) parallel FL (PFL), where clients train\nmodels in a parallel manner; and ii) sequential FL (SFL), where clients train\nmodels in a sequential manner. In contrast to that of PFL, the convergence\ntheory of SFL on heterogeneous data is still lacking. In this paper, we\nestablish the convergence guarantees of SFL for strongly/general/non-convex\nobjectives on heterogeneous data. The convergence guarantees of SFL are better\nthan that of PFL on heterogeneous data with both full and partial client\nparticipation. Experimental results validate the counterintuitive analysis\nresult that SFL outperforms PFL on extremely heterogeneous data in cross-device\nsettings.\n', '  Federated learning is a paradigm of distributed machine learning in which\nmultiple clients coordinate with a central server to learn a model, without\nsharing their own training data. Standard federated optimization methods such\nas Federated Averaging (FedAvg) ensure balance among the clients by using the\nsame stepsize for local updates on all clients. However, this means that all\nclients need to respect the global geometry of the function which could yield\nslow convergence. In this work, we propose locally adaptive federated learning\nalgorithms, that leverage the local geometric information for each client\nfunction. We show that such locally adaptive methods with uncoordinated\nstepsizes across all clients can be particularly efficient in interpolated\n(overparameterized) settings, and analyze their convergence in the presence of\nheterogeneous data for convex and strongly convex settings. We validate our\ntheoretical claims by performing illustrative experiments for both i.i.d.\nnon-i.i.d. cases. Our proposed algorithms match the optimization performance of\ntuned FedAvg in the convex setting, outperform FedAvg as well as\nstate-of-the-art adaptive federated algorithms like FedAMS for non-convex\nexperiments, and come with superior generalization performance.\n']",Federated Learning Algorithms and Convergence Guarantees,Federated Learning,Machine Learning and Data Privacy,Machine Learning and Data Privacy
354,21,354_nlg_evaluations_evaluation_generation,"['nlg', 'evaluations', 'evaluation', 'generation', 'generated', 'dialogue', 'evaluating', 'evaluator', 'eval', 'gram']","['evaluation', 'references', 'metrics', 'evaluators', 'reference', 'generation', 'automatic', 'texts', 'quality', 'judgments']","[""  Natural Language Generation (NLG) typically involves evaluating the generated\ntext in various aspects (e.g., consistency and naturalness) to obtain a\ncomprehensive assessment. However, multi-aspect evaluation remains challenging\nas it may require the evaluator to generalize to any given evaluation aspect\neven if it's absent during training. In this paper, we introduce X-Eval, a\ntwo-stage instruction tuning framework to evaluate the text in both seen and\nunseen aspects customized by end users. X-Eval consists of two learning stages:\nthe vanilla instruction tuning stage that improves the model's ability to\nfollow evaluation instructions, and an enhanced instruction tuning stage that\nexploits the connections between fine-grained evaluation aspects to better\nassess text quality. To support the training of X-Eval, we collect\nAspectInstruct, the first instruction tuning dataset tailored for multi-aspect\nNLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance\ntask diversity, we devise an augmentation strategy that converts human rating\nannotations into diverse forms of NLG evaluation tasks, including scoring,\ncomparison, ranking, and Boolean question answering. Extensive experiments\nacross three essential categories of NLG tasks: dialogue generation,\nsummarization, and data-to-text coupled with 21 aspects in meta-evaluation,\ndemonstrate that our X-Eval enables even a lightweight language model to\nachieve a comparable if not higher correlation with human judgments compared to\nthe state-of-the-art NLG evaluators, such as GPT-4.\n"", '  Evaluating natural language generation (NLG) is a vital but challenging\nproblem in artificial intelligence. Traditional evaluation metrics mainly\ncapturing content (e.g. n-gram) overlap between system outputs and references\nare far from satisfactory, and large language models (LLMs) such as ChatGPT\nhave demonstrated great potential in NLG evaluation in recent years. Various\nautomatic evaluation methods based on LLMs have been proposed, including\nmetrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled\nevaluation data. In this survey, we first give a taxonomy of LLM-based NLG\nevaluation methods, and discuss their pros and cons, respectively. We also\ndiscuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several\nopen problems in this area and point out future research directions.\n', '  The evaluation of natural language generation (NLG) tasks is a significant\nand longstanding research issue. With the recent emergence of powerful large\nlanguage models (LLMs), some studies have turned to LLM-based automatic\nevaluation methods, which demonstrate great potential to become a new\nevaluation paradigm following traditional string-based and model-based metrics.\nHowever, despite the improved performance of existing methods, they still\npossess some deficiencies, such as dependency on references and limited\nevaluation flexibility. Therefore, in this paper, we meticulously construct a\nlarge-scale NLG evaluation corpus NLG-Eval with human and GPT-4 annotations to\nalleviate the lack of relevant data in this field. Furthermore, we propose\nThemis, an LLM dedicated to NLG evaluation, which has been trained with our\ndesigned multi-perspective consistency and rating-oriented preference alignment\nmethods. Themis can conduct flexible and interpretable evaluations without\nreferences, and it exhibits superior evaluation performance on various NLG\ntasks, simultaneously generalizing well to unseen tasks and surpassing other\nevaluation models, including GPT-4.\n']",Natural Language Generation Evaluation,Natural Language Processing for Text Generation and Evaluation,Natural Language Processing,Natural Language Processing
355,21,355_explanations_counterfactuals_reasonability_implicatures,"['explanations', 'counterfactuals', 'reasonability', 'implicatures', 'explaining', 'counterfactual', 'explainer', 'readability', 'language', 'predictions']","['explanations', 'faithfulness', 'plausibility', 'rationales', 'explanation', 'readability', 'concept', 'faithful', 'implicatures', 'maxims']","[""  Large language models (LLMs) are becoming bigger to boost performance.\nHowever, little is known about how explainability is affected by this trend.\nThis work explores LIME explanations for DeBERTaV3 models of four different\nsizes on natural language inference (NLI) and zero-shot classification (ZSC)\ntasks. We evaluate the explanations based on their faithfulness to the models'\ninternal decision processes and their plausibility, i.e. their agreement with\nhuman explanations. The key finding is that increased model size does not\ncorrelate with plausibility despite improved model performance, suggesting a\nmisalignment between the LIME explanations and the models' internal processes\nas model size increases. Our results further suggest limitations regarding\nfaithfulness metrics in NLI contexts.\n"", ""  Instruction-tuned Large Language Models (LLMs) excel at many tasks and will\neven explain their reasoning, so-called self-explanations. However, convincing\nand wrong self-explanations can lead to unsupported confidence in LLMs, thus\nincreasing risk. Therefore, it's important to measure if self-explanations\ntruly reflect the model's behavior. Such a measure is called\ninterpretability-faithfulness and is challenging to perform since the ground\ntruth is inaccessible, and many LLMs only have an inference API. To address\nthis, we propose employing self-consistency checks to measure faithfulness. For\nexample, if an LLM says a set of words is important for making a prediction,\nthen it should not be able to make its prediction without these words. While\nself-consistency checks are a common approach to faithfulness, they have not\npreviously been successfully applied to LLM self-explanations for\ncounterfactual, feature attribution, and redaction explanations. Our results\ndemonstrate that faithfulness is explanation, model, and task-dependent,\nshowing self-explanations should not be trusted in general. For example, with\nsentiment classification, counterfactuals are more faithful for Llama2, feature\nattribution for Mistral, and redaction for Falcon 40B.\n"", '  Large Language Models (LLMs) are deployed as powerful tools for several\nnatural language processing (NLP) applications. Recent works show that modern\nLLMs can generate self-explanations (SEs), which elicit their intermediate\nreasoning steps for explaining their behavior. Self-explanations have seen\nwidespread adoption owing to their conversational and plausible nature.\nHowever, there is little to no understanding of their faithfulness. In this\nwork, we discuss the dichotomy between faithfulness and plausibility in SEs\ngenerated by LLMs. We argue that while LLMs are adept at generating plausible\nexplanations -- seemingly logical and coherent to human users -- these\nexplanations do not necessarily align with the reasoning processes of the LLMs,\nraising concerns about their faithfulness. We highlight that the current trend\ntowards increasing the plausibility of explanations, primarily driven by the\ndemand for user-friendly interfaces, may come at the cost of diminishing their\nfaithfulness. We assert that the faithfulness of explanations is critical in\nLLMs employed for high-stakes decision-making. Moreover, we emphasize the need\nfor a systematic characterization of faithfulness-plausibility requirements of\ndifferent real-world applications and ensure explanations meet those needs.\nWhile there are several approaches to improving plausibility, improving\nfaithfulness is an open challenge. We call upon the community to develop novel\nmethods to enhance the faithfulness of self explanations thereby enabling\ntransparent deployment of LLMs in diverse high-stakes settings.\n']",Evaluating Faithfulness in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
355,21,355_explanations_counterfactuals_reasonability_implicatures,"['explanations', 'counterfactuals', 'reasonability', 'implicatures', 'explaining', 'counterfactual', 'explainer', 'readability', 'language', 'predictions']","['explanations', 'faithfulness', 'plausibility', 'rationales', 'explanation', 'readability', 'concept', 'faithful', 'implicatures', 'maxims']","[""  Large language models (LLMs) are becoming bigger to boost performance.\nHowever, little is known about how explainability is affected by this trend.\nThis work explores LIME explanations for DeBERTaV3 models of four different\nsizes on natural language inference (NLI) and zero-shot classification (ZSC)\ntasks. We evaluate the explanations based on their faithfulness to the models'\ninternal decision processes and their plausibility, i.e. their agreement with\nhuman explanations. The key finding is that increased model size does not\ncorrelate with plausibility despite improved model performance, suggesting a\nmisalignment between the LIME explanations and the models' internal processes\nas model size increases. Our results further suggest limitations regarding\nfaithfulness metrics in NLI contexts.\n"", ""  Instruction-tuned Large Language Models (LLMs) excel at many tasks and will\neven explain their reasoning, so-called self-explanations. However, convincing\nand wrong self-explanations can lead to unsupported confidence in LLMs, thus\nincreasing risk. Therefore, it's important to measure if self-explanations\ntruly reflect the model's behavior. Such a measure is called\ninterpretability-faithfulness and is challenging to perform since the ground\ntruth is inaccessible, and many LLMs only have an inference API. To address\nthis, we propose employing self-consistency checks to measure faithfulness. For\nexample, if an LLM says a set of words is important for making a prediction,\nthen it should not be able to make its prediction without these words. While\nself-consistency checks are a common approach to faithfulness, they have not\npreviously been successfully applied to LLM self-explanations for\ncounterfactual, feature attribution, and redaction explanations. Our results\ndemonstrate that faithfulness is explanation, model, and task-dependent,\nshowing self-explanations should not be trusted in general. For example, with\nsentiment classification, counterfactuals are more faithful for Llama2, feature\nattribution for Mistral, and redaction for Falcon 40B.\n"", '  Large Language Models (LLMs) are deployed as powerful tools for several\nnatural language processing (NLP) applications. Recent works show that modern\nLLMs can generate self-explanations (SEs), which elicit their intermediate\nreasoning steps for explaining their behavior. Self-explanations have seen\nwidespread adoption owing to their conversational and plausible nature.\nHowever, there is little to no understanding of their faithfulness. In this\nwork, we discuss the dichotomy between faithfulness and plausibility in SEs\ngenerated by LLMs. We argue that while LLMs are adept at generating plausible\nexplanations -- seemingly logical and coherent to human users -- these\nexplanations do not necessarily align with the reasoning processes of the LLMs,\nraising concerns about their faithfulness. We highlight that the current trend\ntowards increasing the plausibility of explanations, primarily driven by the\ndemand for user-friendly interfaces, may come at the cost of diminishing their\nfaithfulness. We assert that the faithfulness of explanations is critical in\nLLMs employed for high-stakes decision-making. Moreover, we emphasize the need\nfor a systematic characterization of faithfulness-plausibility requirements of\ndifferent real-world applications and ensure explanations meet those needs.\nWhile there are several approaches to improving plausibility, improving\nfaithfulness is an open challenge. We call upon the community to develop novel\nmethods to enhance the faithfulness of self explanations thereby enabling\ntransparent deployment of LLMs in diverse high-stakes settings.\n']",Evaluating Faithfulness in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
355,21,355_explanations_counterfactuals_reasonability_implicatures,"['explanations', 'counterfactuals', 'reasonability', 'implicatures', 'explaining', 'counterfactual', 'explainer', 'readability', 'language', 'predictions']","['explanations', 'faithfulness', 'plausibility', 'rationales', 'explanation', 'readability', 'concept', 'faithful', 'implicatures', 'maxims']","[""  Large language models (LLMs) are becoming bigger to boost performance.\nHowever, little is known about how explainability is affected by this trend.\nThis work explores LIME explanations for DeBERTaV3 models of four different\nsizes on natural language inference (NLI) and zero-shot classification (ZSC)\ntasks. We evaluate the explanations based on their faithfulness to the models'\ninternal decision processes and their plausibility, i.e. their agreement with\nhuman explanations. The key finding is that increased model size does not\ncorrelate with plausibility despite improved model performance, suggesting a\nmisalignment between the LIME explanations and the models' internal processes\nas model size increases. Our results further suggest limitations regarding\nfaithfulness metrics in NLI contexts.\n"", ""  Instruction-tuned Large Language Models (LLMs) excel at many tasks and will\neven explain their reasoning, so-called self-explanations. However, convincing\nand wrong self-explanations can lead to unsupported confidence in LLMs, thus\nincreasing risk. Therefore, it's important to measure if self-explanations\ntruly reflect the model's behavior. Such a measure is called\ninterpretability-faithfulness and is challenging to perform since the ground\ntruth is inaccessible, and many LLMs only have an inference API. To address\nthis, we propose employing self-consistency checks to measure faithfulness. For\nexample, if an LLM says a set of words is important for making a prediction,\nthen it should not be able to make its prediction without these words. While\nself-consistency checks are a common approach to faithfulness, they have not\npreviously been successfully applied to LLM self-explanations for\ncounterfactual, feature attribution, and redaction explanations. Our results\ndemonstrate that faithfulness is explanation, model, and task-dependent,\nshowing self-explanations should not be trusted in general. For example, with\nsentiment classification, counterfactuals are more faithful for Llama2, feature\nattribution for Mistral, and redaction for Falcon 40B.\n"", '  Large Language Models (LLMs) are deployed as powerful tools for several\nnatural language processing (NLP) applications. Recent works show that modern\nLLMs can generate self-explanations (SEs), which elicit their intermediate\nreasoning steps for explaining their behavior. Self-explanations have seen\nwidespread adoption owing to their conversational and plausible nature.\nHowever, there is little to no understanding of their faithfulness. In this\nwork, we discuss the dichotomy between faithfulness and plausibility in SEs\ngenerated by LLMs. We argue that while LLMs are adept at generating plausible\nexplanations -- seemingly logical and coherent to human users -- these\nexplanations do not necessarily align with the reasoning processes of the LLMs,\nraising concerns about their faithfulness. We highlight that the current trend\ntowards increasing the plausibility of explanations, primarily driven by the\ndemand for user-friendly interfaces, may come at the cost of diminishing their\nfaithfulness. We assert that the faithfulness of explanations is critical in\nLLMs employed for high-stakes decision-making. Moreover, we emphasize the need\nfor a systematic characterization of faithfulness-plausibility requirements of\ndifferent real-world applications and ensure explanations meet those needs.\nWhile there are several approaches to improving plausibility, improving\nfaithfulness is an open challenge. We call upon the community to develop novel\nmethods to enhance the faithfulness of self explanations thereby enabling\ntransparent deployment of LLMs in diverse high-stakes settings.\n']",Evaluating Faithfulness in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
355,21,355_explanations_counterfactuals_reasonability_implicatures,"['explanations', 'counterfactuals', 'reasonability', 'implicatures', 'explaining', 'counterfactual', 'explainer', 'readability', 'language', 'predictions']","['explanations', 'faithfulness', 'plausibility', 'rationales', 'explanation', 'readability', 'concept', 'faithful', 'implicatures', 'maxims']","[""  Large language models (LLMs) are becoming bigger to boost performance.\nHowever, little is known about how explainability is affected by this trend.\nThis work explores LIME explanations for DeBERTaV3 models of four different\nsizes on natural language inference (NLI) and zero-shot classification (ZSC)\ntasks. We evaluate the explanations based on their faithfulness to the models'\ninternal decision processes and their plausibility, i.e. their agreement with\nhuman explanations. The key finding is that increased model size does not\ncorrelate with plausibility despite improved model performance, suggesting a\nmisalignment between the LIME explanations and the models' internal processes\nas model size increases. Our results further suggest limitations regarding\nfaithfulness metrics in NLI contexts.\n"", ""  Instruction-tuned Large Language Models (LLMs) excel at many tasks and will\neven explain their reasoning, so-called self-explanations. However, convincing\nand wrong self-explanations can lead to unsupported confidence in LLMs, thus\nincreasing risk. Therefore, it's important to measure if self-explanations\ntruly reflect the model's behavior. Such a measure is called\ninterpretability-faithfulness and is challenging to perform since the ground\ntruth is inaccessible, and many LLMs only have an inference API. To address\nthis, we propose employing self-consistency checks to measure faithfulness. For\nexample, if an LLM says a set of words is important for making a prediction,\nthen it should not be able to make its prediction without these words. While\nself-consistency checks are a common approach to faithfulness, they have not\npreviously been successfully applied to LLM self-explanations for\ncounterfactual, feature attribution, and redaction explanations. Our results\ndemonstrate that faithfulness is explanation, model, and task-dependent,\nshowing self-explanations should not be trusted in general. For example, with\nsentiment classification, counterfactuals are more faithful for Llama2, feature\nattribution for Mistral, and redaction for Falcon 40B.\n"", '  Large Language Models (LLMs) are deployed as powerful tools for several\nnatural language processing (NLP) applications. Recent works show that modern\nLLMs can generate self-explanations (SEs), which elicit their intermediate\nreasoning steps for explaining their behavior. Self-explanations have seen\nwidespread adoption owing to their conversational and plausible nature.\nHowever, there is little to no understanding of their faithfulness. In this\nwork, we discuss the dichotomy between faithfulness and plausibility in SEs\ngenerated by LLMs. We argue that while LLMs are adept at generating plausible\nexplanations -- seemingly logical and coherent to human users -- these\nexplanations do not necessarily align with the reasoning processes of the LLMs,\nraising concerns about their faithfulness. We highlight that the current trend\ntowards increasing the plausibility of explanations, primarily driven by the\ndemand for user-friendly interfaces, may come at the cost of diminishing their\nfaithfulness. We assert that the faithfulness of explanations is critical in\nLLMs employed for high-stakes decision-making. Moreover, we emphasize the need\nfor a systematic characterization of faithfulness-plausibility requirements of\ndifferent real-world applications and ensure explanations meet those needs.\nWhile there are several approaches to improving plausibility, improving\nfaithfulness is an open challenge. We call upon the community to develop novel\nmethods to enhance the faithfulness of self explanations thereby enabling\ntransparent deployment of LLMs in diverse high-stakes settings.\n']",Evaluating Faithfulness in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
355,21,355_explanations_counterfactuals_reasonability_implicatures,"['explanations', 'counterfactuals', 'reasonability', 'implicatures', 'explaining', 'counterfactual', 'explainer', 'readability', 'language', 'predictions']","['explanations', 'faithfulness', 'plausibility', 'rationales', 'explanation', 'readability', 'concept', 'faithful', 'implicatures', 'maxims']","[""  Large language models (LLMs) are becoming bigger to boost performance.\nHowever, little is known about how explainability is affected by this trend.\nThis work explores LIME explanations for DeBERTaV3 models of four different\nsizes on natural language inference (NLI) and zero-shot classification (ZSC)\ntasks. We evaluate the explanations based on their faithfulness to the models'\ninternal decision processes and their plausibility, i.e. their agreement with\nhuman explanations. The key finding is that increased model size does not\ncorrelate with plausibility despite improved model performance, suggesting a\nmisalignment between the LIME explanations and the models' internal processes\nas model size increases. Our results further suggest limitations regarding\nfaithfulness metrics in NLI contexts.\n"", ""  Instruction-tuned Large Language Models (LLMs) excel at many tasks and will\neven explain their reasoning, so-called self-explanations. However, convincing\nand wrong self-explanations can lead to unsupported confidence in LLMs, thus\nincreasing risk. Therefore, it's important to measure if self-explanations\ntruly reflect the model's behavior. Such a measure is called\ninterpretability-faithfulness and is challenging to perform since the ground\ntruth is inaccessible, and many LLMs only have an inference API. To address\nthis, we propose employing self-consistency checks to measure faithfulness. For\nexample, if an LLM says a set of words is important for making a prediction,\nthen it should not be able to make its prediction without these words. While\nself-consistency checks are a common approach to faithfulness, they have not\npreviously been successfully applied to LLM self-explanations for\ncounterfactual, feature attribution, and redaction explanations. Our results\ndemonstrate that faithfulness is explanation, model, and task-dependent,\nshowing self-explanations should not be trusted in general. For example, with\nsentiment classification, counterfactuals are more faithful for Llama2, feature\nattribution for Mistral, and redaction for Falcon 40B.\n"", '  Large Language Models (LLMs) are deployed as powerful tools for several\nnatural language processing (NLP) applications. Recent works show that modern\nLLMs can generate self-explanations (SEs), which elicit their intermediate\nreasoning steps for explaining their behavior. Self-explanations have seen\nwidespread adoption owing to their conversational and plausible nature.\nHowever, there is little to no understanding of their faithfulness. In this\nwork, we discuss the dichotomy between faithfulness and plausibility in SEs\ngenerated by LLMs. We argue that while LLMs are adept at generating plausible\nexplanations -- seemingly logical and coherent to human users -- these\nexplanations do not necessarily align with the reasoning processes of the LLMs,\nraising concerns about their faithfulness. We highlight that the current trend\ntowards increasing the plausibility of explanations, primarily driven by the\ndemand for user-friendly interfaces, may come at the cost of diminishing their\nfaithfulness. We assert that the faithfulness of explanations is critical in\nLLMs employed for high-stakes decision-making. Moreover, we emphasize the need\nfor a systematic characterization of faithfulness-plausibility requirements of\ndifferent real-world applications and ensure explanations meet those needs.\nWhile there are several approaches to improving plausibility, improving\nfaithfulness is an open challenge. We call upon the community to develop novel\nmethods to enhance the faithfulness of self explanations thereby enabling\ntransparent deployment of LLMs in diverse high-stakes settings.\n']",Evaluating Faithfulness in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
355,21,355_explanations_counterfactuals_reasonability_implicatures,"['explanations', 'counterfactuals', 'reasonability', 'implicatures', 'explaining', 'counterfactual', 'explainer', 'readability', 'language', 'predictions']","['explanations', 'faithfulness', 'plausibility', 'rationales', 'explanation', 'readability', 'concept', 'faithful', 'implicatures', 'maxims']","[""  Large language models (LLMs) are becoming bigger to boost performance.\nHowever, little is known about how explainability is affected by this trend.\nThis work explores LIME explanations for DeBERTaV3 models of four different\nsizes on natural language inference (NLI) and zero-shot classification (ZSC)\ntasks. We evaluate the explanations based on their faithfulness to the models'\ninternal decision processes and their plausibility, i.e. their agreement with\nhuman explanations. The key finding is that increased model size does not\ncorrelate with plausibility despite improved model performance, suggesting a\nmisalignment between the LIME explanations and the models' internal processes\nas model size increases. Our results further suggest limitations regarding\nfaithfulness metrics in NLI contexts.\n"", ""  Instruction-tuned Large Language Models (LLMs) excel at many tasks and will\neven explain their reasoning, so-called self-explanations. However, convincing\nand wrong self-explanations can lead to unsupported confidence in LLMs, thus\nincreasing risk. Therefore, it's important to measure if self-explanations\ntruly reflect the model's behavior. Such a measure is called\ninterpretability-faithfulness and is challenging to perform since the ground\ntruth is inaccessible, and many LLMs only have an inference API. To address\nthis, we propose employing self-consistency checks to measure faithfulness. For\nexample, if an LLM says a set of words is important for making a prediction,\nthen it should not be able to make its prediction without these words. While\nself-consistency checks are a common approach to faithfulness, they have not\npreviously been successfully applied to LLM self-explanations for\ncounterfactual, feature attribution, and redaction explanations. Our results\ndemonstrate that faithfulness is explanation, model, and task-dependent,\nshowing self-explanations should not be trusted in general. For example, with\nsentiment classification, counterfactuals are more faithful for Llama2, feature\nattribution for Mistral, and redaction for Falcon 40B.\n"", '  Large Language Models (LLMs) are deployed as powerful tools for several\nnatural language processing (NLP) applications. Recent works show that modern\nLLMs can generate self-explanations (SEs), which elicit their intermediate\nreasoning steps for explaining their behavior. Self-explanations have seen\nwidespread adoption owing to their conversational and plausible nature.\nHowever, there is little to no understanding of their faithfulness. In this\nwork, we discuss the dichotomy between faithfulness and plausibility in SEs\ngenerated by LLMs. We argue that while LLMs are adept at generating plausible\nexplanations -- seemingly logical and coherent to human users -- these\nexplanations do not necessarily align with the reasoning processes of the LLMs,\nraising concerns about their faithfulness. We highlight that the current trend\ntowards increasing the plausibility of explanations, primarily driven by the\ndemand for user-friendly interfaces, may come at the cost of diminishing their\nfaithfulness. We assert that the faithfulness of explanations is critical in\nLLMs employed for high-stakes decision-making. Moreover, we emphasize the need\nfor a systematic characterization of faithfulness-plausibility requirements of\ndifferent real-world applications and ensure explanations meet those needs.\nWhile there are several approaches to improving plausibility, improving\nfaithfulness is an open challenge. We call upon the community to develop novel\nmethods to enhance the faithfulness of self explanations thereby enabling\ntransparent deployment of LLMs in diverse high-stakes settings.\n']",Evaluating Faithfulness in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
356,21,356_retrosynthesis_retrosynthetic_synthesisability_synthesis,"['retrosynthesis', 'retrosynthetic', 'synthesisability', 'synthesis', 'synthesize', 'retrosig', 'retrograph', 'retrowise', 'retrogfn', 'synthons']","['retrosynthesis', 'template', 'reactions', 'reaction', 'synthesis', 'routes', 'molecule', 'step', 'molecules', 'planning']","['  Retrosynthesis is a fundamental but challenging task in organic chemistry,\nwith broad applications in fields such as drug design and synthesis. Given a\ntarget molecule, the goal of retrosynthesis is to find out a series of\nreactions which could be assembled into a synthetic route which starts from\npurchasable molecules and ends at the target molecule. The uncertainty of\nreactions used in retrosynthetic planning, which is caused by hallucinations of\nbackward models, has recently been noticed. In this paper we propose a succinct\nprobabilistic model to describe such uncertainty. Based on the model, we\npropose a new retrosynthesis planning algorithm called retro-prob to maximize\nthe successful synthesis probability of target molecules, which acquires high\nefficiency by utilizing the chain rule of derivatives. Experiments on the\nParoutes benchmark show that retro-prob outperforms previous algorithms, retro*\nand retro-fallback, both in speed and in the quality of synthesis plans.\n', '  Retrosynthesis planning is a fundamental challenge in chemistry which aims at\ndesigning reaction pathways from commercially available starting materials to a\ntarget molecule. Each step in multi-step retrosynthesis planning requires\naccurate prediction of possible precursor molecules given the target molecule\nand confidence estimates to guide heuristic search algorithms. We model\nsingle-step retrosynthesis planning as a distribution learning problem in a\ndiscrete state space. First, we introduce the Markov Bridge Model, a generative\nframework aimed to approximate the dependency between two intractable discrete\ndistributions accessible via a finite sample of coupled data points. Our\nframework is based on the concept of a Markov bridge, a Markov process pinned\nat its endpoints. Unlike diffusion-based methods, our Markov Bridge Model does\nnot need a tractable noise distribution as a sampling proxy and directly\noperates on the input product molecules as samples from the intractable prior\ndistribution. We then address the retrosynthesis planning problem with our\nnovel framework and introduce RetroBridge, a template-free retrosynthesis\nmodeling approach that achieves state-of-the-art results on standard evaluation\nbenchmarks.\n', '  Retrosynthesis consists of breaking down a chemical compound recursively\nstep-by-step into molecular precursors until a set of commercially available\nmolecules is found with the goal to provide a synthesis route. Its two primary\nresearch directions, single-step retrosynthesis prediction, which models the\nchemical reaction logic, and multi-step synthesis planning, which tries to find\nthe correct sequence of reactions, are inherently intertwined. Still, this\nconnection is not reflected in contemporary research. In this work, we combine\nthese two major research directions by applying multiple single-step\nretrosynthesis models within multi-step synthesis planning and analyzing their\nimpact using public and proprietary reaction data. We find a disconnection\nbetween high single-step performance and potential route-finding success,\nsuggesting that single-step models must be evaluated within synthesis planning\nin the future. Furthermore, we show that the commonly used single-step\nretrosynthesis benchmark dataset USPTO-50k is insufficient as this evaluation\ntask does not represent model performance and scalability on larger and more\ndiverse datasets. For multi-step synthesis planning, we show that the choice of\nthe single-step model can improve the overall success rate of synthesis\nplanning by up to +28% compared to the commonly used baseline model. Finally,\nwe show that each single-step model finds unique synthesis routes, and differs\nin aspects such as route-finding success, the number of found synthesis routes,\nand chemical validity, making the combination of single-step retrosynthesis\nprediction and multi-step synthesis planning a crucial aspect when developing\nfuture methods.\n']",Retrosynthesis Planning in Organic Chemistry,Artificial Intelligence in Organic Chemistry Synthesis,Artificial Intelligence in Data Generation and Chemical Synthesis,Artificial Intelligence in Data Generation and Chemical Synthesis
357,21,357_humordb_funnynet_humor_humour,"['humordb', 'funnynet', 'humor', 'humour', 'humorous', 'jokes', 'subtitles', 'laugh', 'funnier', 'laughter']","['humor', 'funny', 'humorous', 'humour', 'jokes', 'comic', 'moments', 'mischief', 'laugh', 'video']","['  In this paper, we explore the generation of one-liner jokes through\nmulti-step reasoning. Our work involved reconstructing the process behind\ncreating humorous one-liners and developing a working prototype for humor\ngeneration. We conducted comprehensive experiments with human participants to\nevaluate our approach, comparing it with human-created jokes, zero-shot GPT-4\ngenerated humor, and other baselines. The evaluation focused on the quality of\nhumor produced, using human labeling as a benchmark. Our findings demonstrate\nthat the multi-step reasoning approach consistently improves the quality of\ngenerated humor. We present the results and share the datasets used in our\nexperiments, offering insights into enhancing humor generation with artificial\nintelligence.\n', ""  Humor is a fundamental facet of human cognition and interaction. Yet, despite\nrecent advances in natural language processing, humor detection remains a\nchallenging task that is complicated by the scarcity of datasets that pair\nhumorous texts with similar non-humorous counterparts. In our work, we\ninvestigate whether large language models (LLMs), can generate synthetic data\nfor humor detection via editing texts. We benchmark LLMs on an existing human\ndataset and show that current LLMs display an impressive ability to 'unfun'\njokes, as judged by humans and as measured on the downstream task of humor\ndetection. We extend our approach to a code-mixed English-Hindi humor dataset,\nwhere we find that GPT-4's synthetic data is highly rated by bilingual\nannotators and provides challenging adversarial examples for humor classifiers.\n"", '  Humor understanding is an important and challenging research in natural\nlanguage processing. As the popularity of pre-trained language models (PLMs),\nsome recent work makes preliminary attempts to adopt PLMs for humor recognition\nand generation. However, these simple attempts do not substantially answer the\nquestion: {\\em whether PLMs are capable of humor understanding?} This paper is\nthe first work that systematically investigates the humor understanding ability\nof PLMs. For this purpose, a comprehensive framework with three evaluation\nsteps and four evaluation tasks is designed. We also construct a comprehensive\nChinese humor dataset, which can fully meet all the data requirements of the\nproposed evaluation framework. Our empirical study on the Chinese humor dataset\nyields some valuable observations, which are of great guiding value for future\noptimization of PLMs in humor understanding and generation.\n']",Humor Generation and Detection,Humor and Sarcasm in Natural Language Processing,Humor and Sarcasm in Natural Language Processing,Humor and Sarcasm in Natural Language Processing
358,21,358_langevin_mcmc_stochastic_sampling,"['langevin', 'mcmc', 'stochastic', 'sampling', 'diffusion', 'dmc', 'mc', 'subsampling', 'hessian', 'samplers']","['sampling', 'concave', 'log', 'distributions', 'divergence', 'smooth', 'convex', 'density', 'samplers', 'distance']","['  We study the task of efficiently sampling from a Gibbs distribution $d \\pi^*\n= e^{-h} d {vol}_g$ over a Riemannian manifold $M$ via (geometric) Langevin\nMCMC; this algorithm involves computing exponential maps in random Gaussian\ndirections and is efficiently implementable in practice. The key to our\nanalysis of Langevin MCMC is a bound on the discretization error of the\ngeometric Euler-Murayama scheme, assuming $\\nabla h$ is Lipschitz and $M$ has\nbounded sectional curvature. Our error bound matches the error of Euclidean\nEuler-Murayama in terms of its stepsize dependence. Combined with a contraction\nguarantee for the geometric Langevin Diffusion under Kendall-Cranston coupling,\nwe prove that the Langevin MCMC iterates lie within $\\epsilon$-Wasserstein\ndistance of $\\pi^*$ after $\\tilde{O}(\\epsilon^{-2})$ steps, which matches the\niteration complexity for Euclidean Langevin MCMC. Our results apply in general\nsettings where $h$ can be nonconvex and $M$ can have negative Ricci curvature.\nUnder additional assumptions that the Riemannian curvature tensor has bounded\nderivatives, and that $\\pi^*$ satisfies a $CD(\\cdot,\\infty)$ condition, we\nanalyze the stochastic gradient version of Langevin MCMC, and bound its\niteration complexity by $\\tilde{O}(\\epsilon^{-2})$ as well.\n', '  Understanding the dimension dependency of computational complexity in\nhigh-dimensional sampling problem is a fundamental problem, both from a\npractical and theoretical perspective. Compared with samplers with unbiased\nstationary distribution, e.g., Metropolis-adjusted Langevin algorithm (MALA),\nbiased samplers, e.g., Underdamped Langevin Dynamics (ULD), perform better in\nlow-accuracy cases just because a lower dimension dependency in their\ncomplexities. Along this line, Freund et al. (2022) suggest that the modified\nLangevin algorithm with prior diffusion is able to converge dimension\nindependently for strongly log-concave target distributions. Nonetheless, it\nremains open whether such property establishes for more general cases. In this\npaper, we investigate the prior diffusion technique for the target\ndistributions satisfying log-Sobolev inequality (LSI), which covers a much\nbroader class of distributions compared to the strongly log-concave ones. In\nparticular, we prove that the modified Langevin algorithm can also obtain the\ndimension-independent convergence of KL divergence with different step size\nschedules. The core of our proof technique is a novel construction of an\ninterpolating SDE, which significantly helps to conduct a more accurate\ncharacterization of the discrete updates of the overdamped Langevin dynamics.\nOur theoretical analysis demonstrates the benefits of prior diffusion for a\nbroader class of target distributions and provides new insights into developing\nfaster sampling algorithms.\n', '  Stochastic gradients have been widely integrated into Langevin-based methods\nto improve their scalability and efficiency in solving large-scale sampling\nproblems. However, the proximal sampler, which exhibits much faster convergence\nthan Langevin-based algorithms in the deterministic setting Lee et al. (2021),\nhas yet to be explored in its stochastic variants. In this paper, we study the\nStochastic Proximal Samplers (SPS) for sampling from non-log-concave\ndistributions. We first establish a general framework for implementing\nstochastic proximal samplers and establish the convergence theory accordingly.\nWe show that the convergence to the target distribution can be guaranteed as\nlong as the second moment of the algorithm trajectory is bounded and restricted\nGaussian oracles can be well approximated. We then provide two implementable\nvariants based on Stochastic gradient Langevin dynamics (SGLD) and\nMetropolis-adjusted Langevin algorithm (MALA), giving rise to SPS-SGLD and\nSPS-MALA. We further show that SPS-SGLD and SPS-MALA can achieve\n$\\epsilon$-sampling error in total variation (TV) distance within\n$\\tilde{\\mathcal{O}}(d\\epsilon^{-2})$ and\n$\\tilde{\\mathcal{O}}(d^{1/2}\\epsilon^{-2})$ gradient complexities, which\noutperform the best-known result by at least an $\\tilde{\\mathcal{O}}(d^{1/3})$\nfactor. This enhancement in performance is corroborated by our empirical\nstudies on synthetic data with various dimensions, demonstrating the efficiency\nof our proposed algorithm.\n']",Langevin MCMC and Stochastic Sampling Methods,Stochastic Methods for Sampling and Dynamics,Probabilistic Methods and Stochastic Processes,Probabilistic Methods and Stochastic Processes
359,21,359_forgetting_continual_forget_languages,"['forgetting', 'continual', 'forget', 'languages', 'catastrophic', 'pretraining', 'training', 'language', 'tuning', 'tuned']","['forgetting', 'catastrophic', 'fine', 'tuning', 'continual', 'general', 'domain', 'instruction', 'knowledge', 'phenomenon']","['  Recent advances in Large Language Models (LLMs) have exhibited remarkable\nproficiency across various tasks. Given the potent applications of LLMs in\nnumerous fields, there has been a surge in LLM development. In developing LLMs,\na common practice involves continual pre-training on previously fine-tuned\nmodels. However, this can lead to catastrophic forgetting. In our work, we\ninvestigate the phenomenon of forgetting that occurs during continual\npre-training on an existing fine-tuned LLM. We evaluate the impact of\ncontinuous pre-training on the fine-tuned LLM across various dimensions,\nincluding output format, knowledge, and reliability. Experiment results\nhighlight the non-trivial challenge of addressing catastrophic forgetting\nduring continual pre-training, especially the repetition issue.\n', '  We study and quantify the problem of forgetting when fine-tuning pre-trained\nlarge language models (LLMs) on a downstream task. We find that\nparameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters\n(LoRA), still suffer from catastrophic forgetting. In particular, we identify a\nstrong inverse linear relationship between the fine-tuning performance and the\namount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise\nscaling laws that show forgetting increases as a shifted power law in the\nnumber of parameters fine-tuned and the number of update steps. We also examine\nthe impact of forgetting on knowledge, reasoning, and the safety guardrails\ntrained into Llama 2 7B chat. Our study suggests that forgetting cannot be\navoided through early stopping or by varying the number of parameters\nfine-tuned. We believe this opens up an important safety-critical direction for\nfuture research to evaluate and develop fine-tuning schemes which mitigate\nforgetting\n', ""  Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\nwhen a model forgets previously learned information while acquiring new\nknowledge. As large language models (LLMs) have demonstrated remarkable\nperformance, it is intriguing to investigate whether CF exists during the\ncontinual instruction tuning of LLMs. This study empirically evaluates the\nforgetting phenomenon in LLMs' knowledge during continual instruction tuning\nfrom the perspectives of domain knowledge, reasoning, and reading\ncomprehension. The experiments reveal that catastrophic forgetting is generally\nobserved in LLMs ranging from 1b to 7b parameters. Moreover, as the model scale\nincreases, the severity of forgetting intensifies. Comparing the decoder-only\nmodel BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less\nforgetting and retains more knowledge. Interestingly, we also observe that LLMs\ncan mitigate language biases, such as gender bias, during continual\nfine-tuning. Furthermore, our findings indicate that ALPACA maintains more\nknowledge and capacity compared to LLAMA during continual fine-tuning,\nsuggesting that general instruction tuning can help alleviate the forgetting\nphenomenon in LLMs during subsequent fine-tuning processes.\n""]",Catastrophic Forgetting in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
359,21,359_forgetting_continual_forget_languages,"['forgetting', 'continual', 'forget', 'languages', 'catastrophic', 'pretraining', 'training', 'language', 'tuning', 'tuned']","['forgetting', 'catastrophic', 'fine', 'tuning', 'continual', 'general', 'domain', 'instruction', 'knowledge', 'phenomenon']","['  Recent advances in Large Language Models (LLMs) have exhibited remarkable\nproficiency across various tasks. Given the potent applications of LLMs in\nnumerous fields, there has been a surge in LLM development. In developing LLMs,\na common practice involves continual pre-training on previously fine-tuned\nmodels. However, this can lead to catastrophic forgetting. In our work, we\ninvestigate the phenomenon of forgetting that occurs during continual\npre-training on an existing fine-tuned LLM. We evaluate the impact of\ncontinuous pre-training on the fine-tuned LLM across various dimensions,\nincluding output format, knowledge, and reliability. Experiment results\nhighlight the non-trivial challenge of addressing catastrophic forgetting\nduring continual pre-training, especially the repetition issue.\n', '  We study and quantify the problem of forgetting when fine-tuning pre-trained\nlarge language models (LLMs) on a downstream task. We find that\nparameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters\n(LoRA), still suffer from catastrophic forgetting. In particular, we identify a\nstrong inverse linear relationship between the fine-tuning performance and the\namount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise\nscaling laws that show forgetting increases as a shifted power law in the\nnumber of parameters fine-tuned and the number of update steps. We also examine\nthe impact of forgetting on knowledge, reasoning, and the safety guardrails\ntrained into Llama 2 7B chat. Our study suggests that forgetting cannot be\navoided through early stopping or by varying the number of parameters\nfine-tuned. We believe this opens up an important safety-critical direction for\nfuture research to evaluate and develop fine-tuning schemes which mitigate\nforgetting\n', ""  Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\nwhen a model forgets previously learned information while acquiring new\nknowledge. As large language models (LLMs) have demonstrated remarkable\nperformance, it is intriguing to investigate whether CF exists during the\ncontinual instruction tuning of LLMs. This study empirically evaluates the\nforgetting phenomenon in LLMs' knowledge during continual instruction tuning\nfrom the perspectives of domain knowledge, reasoning, and reading\ncomprehension. The experiments reveal that catastrophic forgetting is generally\nobserved in LLMs ranging from 1b to 7b parameters. Moreover, as the model scale\nincreases, the severity of forgetting intensifies. Comparing the decoder-only\nmodel BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less\nforgetting and retains more knowledge. Interestingly, we also observe that LLMs\ncan mitigate language biases, such as gender bias, during continual\nfine-tuning. Furthermore, our findings indicate that ALPACA maintains more\nknowledge and capacity compared to LLAMA during continual fine-tuning,\nsuggesting that general instruction tuning can help alleviate the forgetting\nphenomenon in LLMs during subsequent fine-tuning processes.\n""]",Catastrophic Forgetting in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
359,21,359_forgetting_continual_forget_languages,"['forgetting', 'continual', 'forget', 'languages', 'catastrophic', 'pretraining', 'training', 'language', 'tuning', 'tuned']","['forgetting', 'catastrophic', 'fine', 'tuning', 'continual', 'general', 'domain', 'instruction', 'knowledge', 'phenomenon']","['  Recent advances in Large Language Models (LLMs) have exhibited remarkable\nproficiency across various tasks. Given the potent applications of LLMs in\nnumerous fields, there has been a surge in LLM development. In developing LLMs,\na common practice involves continual pre-training on previously fine-tuned\nmodels. However, this can lead to catastrophic forgetting. In our work, we\ninvestigate the phenomenon of forgetting that occurs during continual\npre-training on an existing fine-tuned LLM. We evaluate the impact of\ncontinuous pre-training on the fine-tuned LLM across various dimensions,\nincluding output format, knowledge, and reliability. Experiment results\nhighlight the non-trivial challenge of addressing catastrophic forgetting\nduring continual pre-training, especially the repetition issue.\n', '  We study and quantify the problem of forgetting when fine-tuning pre-trained\nlarge language models (LLMs) on a downstream task. We find that\nparameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters\n(LoRA), still suffer from catastrophic forgetting. In particular, we identify a\nstrong inverse linear relationship between the fine-tuning performance and the\namount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise\nscaling laws that show forgetting increases as a shifted power law in the\nnumber of parameters fine-tuned and the number of update steps. We also examine\nthe impact of forgetting on knowledge, reasoning, and the safety guardrails\ntrained into Llama 2 7B chat. Our study suggests that forgetting cannot be\navoided through early stopping or by varying the number of parameters\nfine-tuned. We believe this opens up an important safety-critical direction for\nfuture research to evaluate and develop fine-tuning schemes which mitigate\nforgetting\n', ""  Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\nwhen a model forgets previously learned information while acquiring new\nknowledge. As large language models (LLMs) have demonstrated remarkable\nperformance, it is intriguing to investigate whether CF exists during the\ncontinual instruction tuning of LLMs. This study empirically evaluates the\nforgetting phenomenon in LLMs' knowledge during continual instruction tuning\nfrom the perspectives of domain knowledge, reasoning, and reading\ncomprehension. The experiments reveal that catastrophic forgetting is generally\nobserved in LLMs ranging from 1b to 7b parameters. Moreover, as the model scale\nincreases, the severity of forgetting intensifies. Comparing the decoder-only\nmodel BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less\nforgetting and retains more knowledge. Interestingly, we also observe that LLMs\ncan mitigate language biases, such as gender bias, during continual\nfine-tuning. Furthermore, our findings indicate that ALPACA maintains more\nknowledge and capacity compared to LLAMA during continual fine-tuning,\nsuggesting that general instruction tuning can help alleviate the forgetting\nphenomenon in LLMs during subsequent fine-tuning processes.\n""]",Catastrophic Forgetting in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
359,21,359_forgetting_continual_forget_languages,"['forgetting', 'continual', 'forget', 'languages', 'catastrophic', 'pretraining', 'training', 'language', 'tuning', 'tuned']","['forgetting', 'catastrophic', 'fine', 'tuning', 'continual', 'general', 'domain', 'instruction', 'knowledge', 'phenomenon']","['  Recent advances in Large Language Models (LLMs) have exhibited remarkable\nproficiency across various tasks. Given the potent applications of LLMs in\nnumerous fields, there has been a surge in LLM development. In developing LLMs,\na common practice involves continual pre-training on previously fine-tuned\nmodels. However, this can lead to catastrophic forgetting. In our work, we\ninvestigate the phenomenon of forgetting that occurs during continual\npre-training on an existing fine-tuned LLM. We evaluate the impact of\ncontinuous pre-training on the fine-tuned LLM across various dimensions,\nincluding output format, knowledge, and reliability. Experiment results\nhighlight the non-trivial challenge of addressing catastrophic forgetting\nduring continual pre-training, especially the repetition issue.\n', '  We study and quantify the problem of forgetting when fine-tuning pre-trained\nlarge language models (LLMs) on a downstream task. We find that\nparameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters\n(LoRA), still suffer from catastrophic forgetting. In particular, we identify a\nstrong inverse linear relationship between the fine-tuning performance and the\namount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise\nscaling laws that show forgetting increases as a shifted power law in the\nnumber of parameters fine-tuned and the number of update steps. We also examine\nthe impact of forgetting on knowledge, reasoning, and the safety guardrails\ntrained into Llama 2 7B chat. Our study suggests that forgetting cannot be\navoided through early stopping or by varying the number of parameters\nfine-tuned. We believe this opens up an important safety-critical direction for\nfuture research to evaluate and develop fine-tuning schemes which mitigate\nforgetting\n', ""  Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\nwhen a model forgets previously learned information while acquiring new\nknowledge. As large language models (LLMs) have demonstrated remarkable\nperformance, it is intriguing to investigate whether CF exists during the\ncontinual instruction tuning of LLMs. This study empirically evaluates the\nforgetting phenomenon in LLMs' knowledge during continual instruction tuning\nfrom the perspectives of domain knowledge, reasoning, and reading\ncomprehension. The experiments reveal that catastrophic forgetting is generally\nobserved in LLMs ranging from 1b to 7b parameters. Moreover, as the model scale\nincreases, the severity of forgetting intensifies. Comparing the decoder-only\nmodel BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less\nforgetting and retains more knowledge. Interestingly, we also observe that LLMs\ncan mitigate language biases, such as gender bias, during continual\nfine-tuning. Furthermore, our findings indicate that ALPACA maintains more\nknowledge and capacity compared to LLAMA during continual fine-tuning,\nsuggesting that general instruction tuning can help alleviate the forgetting\nphenomenon in LLMs during subsequent fine-tuning processes.\n""]",Catastrophic Forgetting in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
359,21,359_forgetting_continual_forget_languages,"['forgetting', 'continual', 'forget', 'languages', 'catastrophic', 'pretraining', 'training', 'language', 'tuning', 'tuned']","['forgetting', 'catastrophic', 'fine', 'tuning', 'continual', 'general', 'domain', 'instruction', 'knowledge', 'phenomenon']","['  Recent advances in Large Language Models (LLMs) have exhibited remarkable\nproficiency across various tasks. Given the potent applications of LLMs in\nnumerous fields, there has been a surge in LLM development. In developing LLMs,\na common practice involves continual pre-training on previously fine-tuned\nmodels. However, this can lead to catastrophic forgetting. In our work, we\ninvestigate the phenomenon of forgetting that occurs during continual\npre-training on an existing fine-tuned LLM. We evaluate the impact of\ncontinuous pre-training on the fine-tuned LLM across various dimensions,\nincluding output format, knowledge, and reliability. Experiment results\nhighlight the non-trivial challenge of addressing catastrophic forgetting\nduring continual pre-training, especially the repetition issue.\n', '  We study and quantify the problem of forgetting when fine-tuning pre-trained\nlarge language models (LLMs) on a downstream task. We find that\nparameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters\n(LoRA), still suffer from catastrophic forgetting. In particular, we identify a\nstrong inverse linear relationship between the fine-tuning performance and the\namount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise\nscaling laws that show forgetting increases as a shifted power law in the\nnumber of parameters fine-tuned and the number of update steps. We also examine\nthe impact of forgetting on knowledge, reasoning, and the safety guardrails\ntrained into Llama 2 7B chat. Our study suggests that forgetting cannot be\navoided through early stopping or by varying the number of parameters\nfine-tuned. We believe this opens up an important safety-critical direction for\nfuture research to evaluate and develop fine-tuning schemes which mitigate\nforgetting\n', ""  Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\nwhen a model forgets previously learned information while acquiring new\nknowledge. As large language models (LLMs) have demonstrated remarkable\nperformance, it is intriguing to investigate whether CF exists during the\ncontinual instruction tuning of LLMs. This study empirically evaluates the\nforgetting phenomenon in LLMs' knowledge during continual instruction tuning\nfrom the perspectives of domain knowledge, reasoning, and reading\ncomprehension. The experiments reveal that catastrophic forgetting is generally\nobserved in LLMs ranging from 1b to 7b parameters. Moreover, as the model scale\nincreases, the severity of forgetting intensifies. Comparing the decoder-only\nmodel BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less\nforgetting and retains more knowledge. Interestingly, we also observe that LLMs\ncan mitigate language biases, such as gender bias, during continual\nfine-tuning. Furthermore, our findings indicate that ALPACA maintains more\nknowledge and capacity compared to LLAMA during continual fine-tuning,\nsuggesting that general instruction tuning can help alleviate the forgetting\nphenomenon in LLMs during subsequent fine-tuning processes.\n""]",Catastrophic Forgetting in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
359,21,359_forgetting_continual_forget_languages,"['forgetting', 'continual', 'forget', 'languages', 'catastrophic', 'pretraining', 'training', 'language', 'tuning', 'tuned']","['forgetting', 'catastrophic', 'fine', 'tuning', 'continual', 'general', 'domain', 'instruction', 'knowledge', 'phenomenon']","['  Recent advances in Large Language Models (LLMs) have exhibited remarkable\nproficiency across various tasks. Given the potent applications of LLMs in\nnumerous fields, there has been a surge in LLM development. In developing LLMs,\na common practice involves continual pre-training on previously fine-tuned\nmodels. However, this can lead to catastrophic forgetting. In our work, we\ninvestigate the phenomenon of forgetting that occurs during continual\npre-training on an existing fine-tuned LLM. We evaluate the impact of\ncontinuous pre-training on the fine-tuned LLM across various dimensions,\nincluding output format, knowledge, and reliability. Experiment results\nhighlight the non-trivial challenge of addressing catastrophic forgetting\nduring continual pre-training, especially the repetition issue.\n', '  We study and quantify the problem of forgetting when fine-tuning pre-trained\nlarge language models (LLMs) on a downstream task. We find that\nparameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters\n(LoRA), still suffer from catastrophic forgetting. In particular, we identify a\nstrong inverse linear relationship between the fine-tuning performance and the\namount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise\nscaling laws that show forgetting increases as a shifted power law in the\nnumber of parameters fine-tuned and the number of update steps. We also examine\nthe impact of forgetting on knowledge, reasoning, and the safety guardrails\ntrained into Llama 2 7B chat. Our study suggests that forgetting cannot be\navoided through early stopping or by varying the number of parameters\nfine-tuned. We believe this opens up an important safety-critical direction for\nfuture research to evaluate and develop fine-tuning schemes which mitigate\nforgetting\n', ""  Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\nwhen a model forgets previously learned information while acquiring new\nknowledge. As large language models (LLMs) have demonstrated remarkable\nperformance, it is intriguing to investigate whether CF exists during the\ncontinual instruction tuning of LLMs. This study empirically evaluates the\nforgetting phenomenon in LLMs' knowledge during continual instruction tuning\nfrom the perspectives of domain knowledge, reasoning, and reading\ncomprehension. The experiments reveal that catastrophic forgetting is generally\nobserved in LLMs ranging from 1b to 7b parameters. Moreover, as the model scale\nincreases, the severity of forgetting intensifies. Comparing the decoder-only\nmodel BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less\nforgetting and retains more knowledge. Interestingly, we also observe that LLMs\ncan mitigate language biases, such as gender bias, during continual\nfine-tuning. Furthermore, our findings indicate that ALPACA maintains more\nknowledge and capacity compared to LLAMA during continual fine-tuning,\nsuggesting that general instruction tuning can help alleviate the forgetting\nphenomenon in LLMs during subsequent fine-tuning processes.\n""]",Catastrophic Forgetting in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
360,21,360_querysets_queries_relational_entities,"['querysets', 'queries', 'relational', 'entities', 'entity', 'query2gmm', 'query', 'relations', 'databases', 'knowledge']","['query', 'logical', 'queries', 'graphs', 'answering', 'hop', 'cardinality', 'entities', 'reasoning', 'incomplete']","['  Answering logical queries on knowledge graphs (KG) poses a significant\nchallenge for machine reasoning. The primary obstacle in this task stems from\nthe inherent incompleteness of KGs. Existing research has predominantly focused\non addressing the issue of missing edges in KGs, thereby neglecting another\naspect of incompleteness: the emergence of new entities. Furthermore, most of\nthe existing methods tend to reason over each logical operator separately,\nrather than comprehensively analyzing the query as a whole during the reasoning\nprocess. In this paper, we propose a query-aware prompt-fused framework named\nPro-QE, which could incorporate existing query embedding methods and address\nthe embedding of emerging entities through contextual information aggregation.\nAdditionally, a query prompt, which is generated by encoding the symbolic\nquery, is introduced to gather information relevant to the query from a\nholistic perspective. To evaluate the efficacy of our model in the inductive\nsetting, we introduce two new challenging benchmarks. Experimental results\ndemonstrate that our model successfully handles the issue of unseen entities in\nlogical queries. Furthermore, the ablation study confirms the efficacy of the\naggregator and prompt components.\n', '  Answering first-order logical (FOL) queries over knowledge graphs (KG)\nremains a challenging task mainly due to KG incompleteness. Query embedding\napproaches this problem by computing the low-dimensional vector representations\nof entities, relations, and logical queries. KGs exhibit relational patterns\nsuch as symmetry and composition and modeling the patterns can further enhance\nthe performance of query embedding models. However, the role of such patterns\nin answering FOL queries by query embedding models has not been yet studied in\nthe literature. In this paper, we fill in this research gap and empower FOL\nqueries reasoning with pattern inference by introducing an inductive bias that\nallows for learning relation patterns. To this end, we develop a novel query\nembedding method, RoConE, that defines query regions as geometric cones and\nalgebraic query operators by rotations in complex space. RoConE combines the\nadvantages of Cone as a well-specified geometric representation for query\nembedding, and also the rotation operator as a powerful algebraic operation for\npattern inference. Our experimental results on several benchmark datasets\nconfirm the advantage of relational patterns for enhancing logical query\nanswering task.\n', '  Complex logical query answering is a challenging task in knowledge graphs\n(KGs) that has been widely studied. The ability to perform complex logical\nreasoning is essential and supports various graph reasoning-based downstream\ntasks, such as search engines. Recent approaches are proposed to represent KG\nentities and logical queries into embedding vectors and find answers to logical\nqueries from the KGs. However, existing proposed methods mainly focus on\nquerying a single KG and cannot be applied to multiple graphs. In addition,\ndirectly sharing KGs with sensitive information may incur privacy risks, making\nit impractical to share and construct an aggregated KG for reasoning to\nretrieve query answers. Thus, it remains unknown how to answer queries on\nmulti-source KGs. An entity can be involved in various knowledge graphs and\nreasoning on multiple KGs and answering complex queries on multi-source KGs is\nimportant in discovering knowledge cross graphs. Fortunately, federated\nlearning is utilized in knowledge graphs to collaboratively learn\nrepresentations with privacy preserved. Federated knowledge graph embeddings\nenrich the relations in knowledge graphs to improve the representation quality.\nHowever, these methods only focus on one-hop relations and cannot perform\ncomplex reasoning tasks. In this paper, we apply federated learning to complex\nquery-answering tasks to reason over multi-source knowledge graphs while\npreserving privacy. We propose a Federated Complex Query Answering framework\n(FedCQA), to reason over multi-source KGs avoiding sensitive raw data\ntransmission to protect privacy. We conduct extensive experiments on three\nreal-world datasets and evaluate retrieval performance on various types of\ncomplex queries.\n']",Query Answering on Knowledge Graphs,Question Answering and Reasoning Systems,Artificial Intelligence and Reasoning Systems,Intelligent Systems
361,21,361_contracts_optimal_incentive_learns,"['contracts', 'optimal', 'incentive', 'learns', 'agents', 'bandit', 'incentives', 'contract', 'reward', 'agent']","['principal', 'contract', 'agent', 'contracts', 'regret', 'follower', 'leader', 'utility', 'equilibrium', 'followers']","[""  We study a repeated contracting setting in which a Principal adaptively\nchooses amongst $k$ Agents at each of $T$ rounds. The Agents are non-myopic,\nand so a mechanism for the Principal induces a $T$-round extensive form game\namongst the Agents. We give several results aimed at understanding an\nunder-explored aspect of contract theory -- the game induced when choosing an\nAgent to contract with. First, we show that this game admits a pure-strategy\n\\emph{non-responsive} equilibrium amongst the Agents -- informally an\nequilibrium in which the Agent's actions depend on the history of realized\nstates of nature, but not on the history of each other's actions, and so avoids\nthe complexities of collusion and threats. Next, we show that if the Principal\nselects Agents using a \\emph{monotone} bandit algorithm, then for any concave\ncontract, in any such equilibrium, the Principal obtains no regret to\ncontracting with the best Agent in hindsight -- not just given their realized\nactions, but also to the counterfactual world in which they had offered a\nguaranteed $T$-round contract to the best Agent in hindsight, which would have\ninduced a different sequence of actions. Finally, we show that if the Principal\nselects Agents using a monotone bandit algorithm which guarantees no\nswap-regret, then the Principal can additionally offer only limited liability\ncontracts (in which the Agent never needs to pay the Principal) while getting\nno-regret to the counterfactual world in which she offered a linear contract to\nthe best Agent in hindsight -- despite the fact that linear contracts are not\nlimited liability. We instantiate this theorem by demonstrating the existence\nof a monotone no swap-regret bandit algorithm, which to our knowledge has not\npreviously appeared in the literature.\n"", ""  We study principal-agent problems in which a principal commits to an\noutcome-dependent payment scheme -- called contract -- in order to induce an\nagent to take a costly, unobservable action leading to favorable outcomes. We\nconsider a generalization of the classical (single-round) version of the\nproblem in which the principal interacts with the agent by committing to\ncontracts over multiple rounds. The principal has no information about the\nagent, and they have to learn an optimal contract by only observing the outcome\nrealized at each round. We focus on settings in which the size of the agent's\naction space is small. We design an algorithm that learns an\napproximately-optimal contract with high probability in a number of rounds\npolynomial in the size of the outcome space, when the number of actions is\nconstant. Our algorithm solves an open problem by Zhu et al.[2022]. Moreover,\nit can also be employed to provide a $\\tilde{\\mathcal{O}}(T^{4/5})$ regret\nbound in the related online learning setting in which the principal aims at\nmaximizing their cumulative utility, thus considerably improving\npreviously-known regret bounds.\n"", ""  Generalized principal-agent problems, including Stackelberg games, contract\ndesign, and Bayesian persuasion, are a class of economic problems where an\nagent best responds to a principal's committed strategy. We study repeated\ngeneralized principal-agent problems under the assumption that the principal\ndoes not have commitment power and the agent uses algorithms to learn to\nrespond to the principal. We reduce this problem to a one-shot generalized\nprincipal-agent problem with an approximately-best-responding agent. Using this\nreduction, we show that: (1) if the agent uses contextual no-regret learning\nalgorithms, then the principal can guarantee a utility that is at least the\nprincipal's optimal utility in the classic non-learning model minus the square\nroot of the agent's regret; (2) if the agent uses contextual no-swap-regret\nlearning algorithms, then the principal cannot obtain any utility more than the\noptimal utility in the non-learning model plus the agent's swap regret. But (3)\nif the agent uses mean-based learning algorithms (which can be no-regret but\nnot no-swap-regret), then the principal can do significantly better than the\nnon-learning model. These general results not only refine previous results in\nStackelberg games and contract design with learning agents but also lead to new\nresults for Bayesian persuasion with a learning agent.\n""]",Contract Theory and Incentive Design,Optimization of Resource Allocation and Incentives in Dynamic Systems,Optimization and Management of Complex Systems,Optimization and Decision Making in Complex Systems
362,21,362_edge_efficientnetv2b0_memory_swapnet,"['edge', 'efficientnetv2b0', 'memory', 'swapnet', 'edgeai', 'mobilenetv2', 'deep', 'dnn', 'devices', 'batteryless']","['edge', 'devices', 'latency', 'device', 'lightweight', 'inference', 'batteryless', 'energy', 'memory', 'microcontrollers']","['  The proliferation of edge devices necessitates efficient computational\narchitectures for lightweight tasks, particularly deep neural network (DNN)\ninference. Traditional NPUs, though effective for such operations, face\nchallenges in power, cost, and area when integrated into lightweight edge\ndevices. The RISC-V architecture, known for its modularity and open-source\nnature, offers a viable alternative. This paper introduces the RISC-V\nR-extension, a novel approach to enhancing DNN process efficiency on edge\ndevices. The extension features rented-pipeline stages and architectural\npipeline registers (APR), which optimize critical operation execution, thereby\nreducing latency and memory access frequency. Furthermore, this extension\nincludes new custom instructions to support these architectural improvements.\nThrough comprehensive analysis, this study demonstrates the boost of\nR-extension in edge device processing, setting the stage for more responsive\nand intelligent edge applications.\n', '  Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on ""converting"" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a ""converting"" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\n', '  Executing deep neural networks (DNNs) on edge artificial intelligence (AI)\ndevices enables various autonomous mobile computing applications. However, the\nmemory budget of edge AI devices restricts the number and complexity of DNNs\nallowed in such applications. Existing solutions, such as model compression or\ncloud offloading, reduce the memory footprint of DNN inference at the cost of\ndecreased model accuracy or autonomy. To avoid these drawbacks, we divide DNN\ninto blocks and swap them in and out in order, such that large DNNs can execute\nwithin a small memory budget. Nevertheless, naive swapping on edge AI devices\ninduces significant delays due to the redundant memory operations in the DNN\ndevelopment ecosystem for edge AI devices. To this end, we develop SwapNet, an\nefficient DNN block swapping middleware for edge AI devices. We systematically\neliminate the unnecessary memory operations during block swapping while\nretaining compatible with the deep learning frameworks, GPU backends, and\nhardware architectures of edge AI devices. We further showcase the utility of\nSwapNet via a multi-DNN scheduling scheme. Evaluations on eleven DNN inference\ntasks in three applications demonstrate that SwapNet achieves almost the same\nlatency as the case with sufficient memory even when DNNs demand 2.32x to 5.81x\nmemory beyond the available budget. The design of SwapNet also provides novel\nand feasible insights for deploying large language models (LLMs) on edge AI\ndevices in the future.\n']",Efficient DNN Inference on Edge Devices,Efficient Deep Learning Architectures and Acceleration Techniques,Artificial Intelligence and Machine Learning Hardware Acceleration and Optimization,Artificial Intelligence and Machine Learning Hardware Acceleration and Optimization
363,21,363_grading_essays_assessment_essay,"['grading', 'essays', 'assessment', 'essay', 'scores', 'writing', 'learners', 'scoring', 'score', 'evaluation']","['essay', 'scoring', 'essays', 'writing', 'grammatical', 'proficiency', 'grading', 'feedback', 'scores', 'learners']","[""  Automated Essay Scoring (AES) holds significant promise in the field of\neducation, helping educators to mark larger volumes of essays and provide\ntimely feedback. However, Arabic AES research has been limited by the lack of\npublicly available essay data. This study introduces AR-AES, an Arabic AES\nbenchmark dataset comprising 2046 undergraduate essays, including gender\ninformation, scores, and transparent rubric-based evaluation guidelines,\nproviding comprehensive insights into the scoring process. These essays come\nfrom four diverse courses, covering both traditional and online exams.\nAdditionally, we pioneer the use of AraBERT for AES, exploring its performance\non different question types. We find encouraging results, particularly for\nEnvironmental Chemistry and source-dependent essay questions. For the first\ntime, we examine the scale of errors made by a BERT-based AES system, observing\nthat 96.15 percent of the errors are within one point of the first human\nmarker's prediction, on a scale of one to five, with 79.49 percent of\npredictions matching exactly. In contrast, additional human markers did not\nexceed 30 percent exact matches with the first marker, with 62.9 percent within\none mark. These findings highlight the subjectivity inherent in essay grading,\nand underscore the potential for current AES technology to assist human markers\nto grade consistently across large classes.\n"", '  Individual feedback can help students improve their essay writing skills.\nHowever, the manual effort required to provide such feedback limits\nindividualization in practice. Automatically-generated essay feedback may serve\nas an alternative to guide students at their own pace, convenience, and desired\nfrequency. Large language models (LLMs) have demonstrated strong performance in\ngenerating coherent and contextually relevant text. Yet, their ability to\nprovide helpful essay feedback is unclear. This work explores several prompting\nstrategies for LLM-based zero-shot and few-shot generation of essay feedback.\nInspired by Chain-of-Thought prompting, we study how and to what extent\nautomated essay scoring (AES) can benefit the quality of generated feedback. We\nevaluate both the AES performance that LLMs can achieve with prompting only and\nthe helpfulness of the generated essay feedback. Our results suggest that\ntackling AES and feedback generation jointly improves AES performance. However,\nwhile our manual evaluation emphasizes the quality of the generated essay\nfeedback, the impact of essay scoring on the generated feedback remains low\nultimately.\n', '  Automated essay scoring (AES) involves predicting a score that reflects the\nwriting quality of an essay. Most existing AES systems produce only a single\noverall score. However, users and L2 learners expect scores across different\ndimensions (e.g., vocabulary, grammar, coherence) for English essays in\nreal-world applications. To address this need, we have developed two models\nthat automatically score English essays across multiple dimensions by employing\nfine-tuning and other strategies on two large datasets. The results demonstrate\nthat our systems achieve impressive performance in evaluation using three\ncriteria: precision, F1 score, and Quadratic Weighted Kappa. Furthermore, our\nsystem outperforms existing methods in overall scoring.\n']",Automated Essay Scoring and Assessment,Automated Assessment and Grading in Education,Artificial Intelligence in Education,Artificial Intelligence in Education
364,20,364_gaussian_gps_kernels_ensemble,"['gaussian', 'gps', 'kernels', 'ensemble', 'gpr', 'kernel', 'gp', 'svgp', 'bayesian', 'lfgp']","['kernel', 'kernels', 'likelihood', 'sales', 'process', 'tutorial', 'regression', 'parametric', 'probability', 'pharmaceutical']","['  Recently, there has been a growing interest for mixed-categorical meta-models\nbased on Gaussian process (GP) surrogates. In this setting, several existing\napproaches use different strategies either by using continuous kernels (e.g.,\ncontinuous relaxation and Gower distance based GP) or by using a direct\nestimation of the correlation matrix. In this paper, we present a kernel-based\napproach that extends continuous exponential kernels to handle\nmixed-categorical variables. The proposed kernel leads to a new GP surrogate\nthat generalizes both the continuous relaxation and the Gower distance based GP\nmodels. We demonstrate, on both analytical and engineering problems, that our\nproposed GP model gives a higher likelihood and a smaller residual error than\nthe other kernel-based state-of-the-art models. Our method is available in the\nopen-source software SMT.\n', '  Gaussian process (GP) models have received increasingly attentions in recent\nyears due to their superb prediction accuracy and modeling flexibility. To\naddress the computational burdens of GP models for large-scale datasets,\ndistributed learning for GPs are often adopted. Current aggregation models for\ndistributed GPs are not time-efficient when incorporating correlations between\nGP experts. In this work, we propose a novel approach for aggregated prediction\nin distributed GPs. The technique is suitable for both the exact and sparse\nvariational GPs. The proposed method incorporates correlations among experts,\nleading to better prediction accuracy with manageable computational\nrequirements. As demonstrated by empirical studies, the proposed approach\nresults in more stable predictions in less time than state-of-the-art\nconsistent aggregation models.\n', '  We present a new strategy for learning the functional relation between a pair\nof variables, while addressing inhomogeneities in the correlation structure of\nthe available data, by modelling the sought function as a sample function of a\nnon-stationary Gaussian Process (GP), that nests within itself multiple other\nGPs, each of which we prove can be stationary, thereby establishing sufficiency\nof two GP layers. In fact, a non-stationary kernel is envisaged, with each\nhyperparameter set as dependent on the sample function drawn from the outer\nnon-stationary GP, such that a new sample function is drawn at every pair of\ninput values at which the kernel is computed. However, such a model cannot be\nimplemented, and we substitute this by recalling that the average effect of\ndrawing different sample functions from a given GP is equivalent to that of\ndrawing a sample function from each of a set of GPs that are rendered\ndifferent, as updated during the equilibrium stage of the undertaken inference\n(via MCMC). The kernel is fully non-parametric, and it suffices to learn one\nhyperparameter per layer of GP, for each dimension of the input variable. We\nillustrate this new learning strategy on a real dataset.\n']",Gaussian Process Models and Kernels,Gaussian Process and Mixture Models,Probabilistic Machine Learning,Probabilistic Machine Learning
365,20,365_crowdsourcing_crowdsourced_crowdworker_crowdworkers,"['crowdsourcing', 'crowdsourced', 'crowdworker', 'crowdworkers', 'crowdshipping', 'crowdsensing', 'crowds', 'annotators', 'annotations', 'annotation']","['crowdsourcing', 'workers', 'worker', 'truth', 'annotation', 'crowdworkers', 'crowd', 'labels', 'quality', 'vote']","['  Annotation through crowdsourcing draws incremental attention, which relies on\nan effective selection scheme given a pool of workers. Existing methods propose\nto select workers based on their performance on tasks with ground truth, while\ntwo important points are missed. 1) The historical performances of workers in\nother tasks. In real-world scenarios, workers need to solve a new task whose\ncorrelation with previous tasks is not well-known before the training, which is\ncalled cross-domain. 2) The dynamic worker performance as workers will learn\nfrom the ground truth. In this paper, we consider both factors in designing an\nallocation scheme named cross-domain-aware worker selection with training\napproach. Our approach proposes two estimation modules to both statistically\nanalyze the cross-domain correlation and simulate the learning gain of workers\ndynamically. A framework with a theoretical analysis of the worker elimination\nprocess is given. To validate the effectiveness of our methods, we collect two\nnovel real-world datasets and generate synthetic datasets. The experiment\nresults show that our method outperforms the baselines on both real-world and\nsynthetic datasets.\n', '  Whether Large Language Models (LLMs) can outperform crowdsourcing on the data\nannotation task is attracting interest recently. Some works verified this issue\nwith the average performance of individual crowd workers and LLM workers on\nsome specific NLP tasks by collecting new datasets. However, on the one hand,\nexisting datasets for the studies of annotation quality in crowdsourcing are\nnot yet utilized in such evaluations, which potentially provide reliable\nevaluations from a different viewpoint. On the other hand, the quality of these\naggregated labels is crucial because, when utilizing crowdsourcing, the\nestimated labels aggregated from multiple crowd labels to the same instances\nare the eventually collected labels. Therefore, in this paper, we first\ninvestigate which existing crowdsourcing datasets can be used for a comparative\nstudy and create a benchmark. We then compare the quality between individual\ncrowd labels and LLM labels and make the evaluations on the aggregated labels.\nIn addition, we propose a Crowd-LLM hybrid label aggregation method and verify\nthe performance. We find that adding LLM labels from good LLMs to existing\ncrowdsourcing datasets can enhance the quality of the aggregated labels of the\ndatasets, which is also higher than the quality of LLM labels themselves.\n', ""  For the purpose of efficient and cost-effective large-scale data labeling,\ncrowdsourcing is increasingly being utilized. To guarantee the quality of data\nlabeling, multiple annotations need to be collected for each data sample, and\ntruth inference algorithms have been developed to accurately infer the true\nlabels. Despite previous studies having released public datasets to evaluate\nthe efficacy of truth inference algorithms, these have typically focused on a\nsingle type of crowdsourcing task and neglected the temporal information\nassociated with workers' annotation activities. These limitations significantly\nrestrict the practical applicability of these algorithms, particularly in the\ncontext of long-term and online truth inference. In this paper, we introduce a\nsubstantial crowdsourcing annotation dataset collected from a real-world\ncrowdsourcing platform. This dataset comprises approximately two thousand\nworkers, one million tasks, and six million annotations. The data was gathered\nover a period of approximately six months from various types of tasks, and the\ntimestamps of each annotation were preserved. We analyze the characteristics of\nthe dataset from multiple perspectives and evaluate the effectiveness of\nseveral representative truth inference algorithms on this dataset. We\nanticipate that this dataset will stimulate future research on tracking\nworkers' abilities over time in relation to different types of tasks, as well\nas enhancing online truth inference.\n""]",Crowdsourcing and Annotation Quality,Active Learning and Crowdsourcing for Efficient Data Annotation,Machine Learning Methodologies,Machine Learning Methodologies
366,20,366_vocoders_vocoder_voice_audio,"['vocoders', 'vocoder', 'voice', 'audio', 'vocal', 'audiostylegan', 'gans', 'adversarial', 'generative', 'vocoding']","['vocoder', 'vocoders', 'audio', 'speech', 'discriminator', 'synthesis', 'waveform', 'upsampling', 'adversarial', 'fidelity']","['  This paper presents a neural vocoder based on a denoising diffusion\nprobabilistic model (DDPM) incorporating explicit periodic signals as auxiliary\nconditioning signals. Recently, DDPM-based neural vocoders have gained\nprominence as non-autoregressive models that can generate high-quality\nwaveforms. The neural vocoders based on DDPM have the advantage of training\nwith a simple time-domain loss. In practical applications, such as singing\nvoice synthesis, there is a demand for neural vocoders to generate\nhigh-fidelity speech waveforms with flexible pitch control. However,\nconventional DDPM-based neural vocoders struggle to generate speech waveforms\nunder such conditions. Our proposed model aims to accurately capture the\nperiodic structure of speech waveforms by incorporating explicit periodic\nsignals. Experimental results show that our model improves sound quality and\nprovides better pitch control than conventional DDPM-based neural vocoders.\n', '  Neural vocoders model the raw audio waveform and synthesize high-quality\naudio, but even the highly efficient ones, like MB-MelGAN and LPCNet, fail to\nrun real-time on a low-end device like a smartglass. A pure digital signal\nprocessing (DSP) based vocoder can be implemented via lightweight fast Fourier\ntransforms (FFT), and therefore, is a magnitude faster than any neural vocoder.\nA DSP vocoder often gets a lower audio quality due to consuming over-smoothed\nacoustic model predictions of approximate representations for the vocal tract.\nIn this paper, we propose an ultra-lightweight differential DSP (DDSP) vocoder\nthat uses a jointly optimized acoustic model with a DSP vocoder, and learns\nwithout an extracted spectral feature for the vocal tract. The model achieves\naudio quality comparable to neural vocoders with a high average MOS of 4.36\nwhile being efficient as a DSP vocoder. Our C++ implementation, without any\nhardware-specific optimization, is at 15 MFLOPS, surpasses MB-MelGAN by 340\ntimes in terms of FLOPS, and achieves a vocoder-only RTF of 0.003 and overall\nRTF of 0.044 while running single-threaded on a 2GHz Intel Xeon CPU.\n', '  Since the introduction of Generative Adversarial Networks (GANs) in speech\nsynthesis, remarkable achievements have been attained. In a thorough\nexploration of vocoders, it has been discovered that audio waveforms can be\ngenerated at speeds exceeding real-time while maintaining high fidelity,\nachieved through the utilization of GAN-based models. Typically, the inputs to\nthe vocoder consist of band-limited spectral information, which inevitably\nsacrifices high-frequency details. To address this, we adopt the full-band Mel\nspectrogram information as input, aiming to provide the vocoder with the most\ncomprehensive information possible. However, previous studies have revealed\nthat the use of full-band spectral information as input can result in the issue\nof over-smoothing, compromising the naturalness of the synthesized speech. To\ntackle this challenge, we propose VNet, a GAN-based neural vocoder network that\nincorporates full-band spectral information and introduces a Multi-Tier\nDiscriminator (MTD) comprising multiple sub-discriminators to generate\nhigh-resolution signals. Additionally, we introduce an asymptotically\nconstrained method that modifies the adversarial loss of the generator and\ndiscriminator, enhancing the stability of the training process. Through\nrigorous experiments, we demonstrate that the VNet model is capable of\ngenerating high-fidelity speech and significantly improving the performance of\nthe vocoder.\n']",Neural Vocoders for High-Quality Speech Synthesis,Speech and Voice Synthesis,Speech and Audio Processing,Speech and Audio Processing
367,20,367_augmentation_augmenting_augmentations_auggpt,"['augmentation', 'augmenting', 'augmentations', 'auggpt', 'aug', 'improving', 'augmented', 'improve', 'text', 'autoaugment']","['augmentation', 'augmentations', 'text', 'protein', 'data', 'natural', 'substitution', 'reproducibility', 'technique', 'classification']","['  Data augmentation is one of the regularization strategies for the training of\ndeep learning models, which enhances generalizability and prevents overfitting,\nleading to performance improvement. Although researchers have proposed various\ndata augmentation techniques, they often lack consideration for the difficulty\nof augmented data. Recently, another line of research suggests incorporating\nthe concept of curriculum learning with data augmentation in the field of\nnatural language processing. In this study, we adopt curriculum data\naugmentation for image data augmentation and propose colorful cutout, which\ngradually increases the noise and difficulty introduced in the augmented image.\nOur experimental results highlight the possibility of curriculum data\naugmentation for image data. We publicly released our source code to improve\nthe reproducibility of our study.\n', '  Large models, encompassing large language and diffusion models, have shown\nexceptional promise in approximating human-level intelligence, garnering\nsignificant interest from both academic and industrial spheres. However, the\ntraining of these large models necessitates vast quantities of high-quality\ndata, and with continuous updates to these models, the existing reservoir of\nhigh-quality data may soon be depleted. This challenge has catalyzed a surge in\nresearch focused on data augmentation methods. Leveraging large models, these\ndata augmentation techniques have outperformed traditional approaches. This\npaper offers an exhaustive review of large model-driven data augmentation\nmethods, adopting a comprehensive perspective. We begin by establishing a\nclassification of relevant studies into three main categories: image\naugmentation, text augmentation, and paired data augmentation. Following this,\nwe delve into various data post-processing techniques pertinent to large\nmodel-based data augmentation. Our discussion then expands to encompass the\narray of applications for these data augmentation methods within natural\nlanguage processing, computer vision, and audio signal processing. We proceed\nto evaluate the successes and limitations of large model-based data\naugmentation across different scenarios. Concluding our review, we highlight\nprospective challenges and avenues for future exploration in the field of data\naugmentation. Our objective is to furnish researchers with critical insights,\nultimately contributing to the advancement of more sophisticated large models.\nWe consistently maintain the related open-source materials at:\nhttps://github.com/MLGroup-JLU/LLM-data-aug-survey.\n', '  Augmentation is an effective alternative to utilize the small amount of\nlabeled protein data. However, most of the existing work focuses on design-ing\nnew architectures or pre-training tasks, and relatively little work has studied\ndata augmentation for proteins. This paper extends data augmentation techniques\npreviously used for images and texts to proteins and then benchmarks these\ntechniques on a variety of protein-related tasks, providing the first\ncomprehensive evaluation of protein augmentation. Furthermore, we propose two\nnovel semantic-level protein augmentation methods, namely Integrated Gradients\nSubstitution and Back Translation Substitution, which enable protein\nsemantic-aware augmentation through saliency detection and biological\nknowledge. Finally, we integrate extended and proposed augmentations into an\naugmentation pool and propose a simple but effective framework, namely\nAutomated Protein Augmentation (APA), which can adaptively select the most\nsuitable augmentation combinations for different tasks. Extensive experiments\nhave shown that APA enhances the performance of five protein related tasks by\nan average of 10.55% across three architectures compared to vanilla\nimplementations without augmentation, highlighting its potential to make a\ngreat impact on the field.\n']",Data Augmentation Techniques,Data Augmentation Techniques for Deep Learning,Deep Learning Optimization and Security,Deep Learning Methodologies
368,20,368_backpropagation_neural_forwardgnn_neurons,"['backpropagation', 'neural', 'forwardgnn', 'neurons', 'neuronal', 'backward', 'forward', 'cortex', 'backprop', 'learning']","['forward', 'backpropagation', 'biological', 'plausible', 'propagation', 'backward', 'algorithm', 'credit', 'cortical', 'feedback']","['  ""Forward-only"" algorithms, which train neural networks while avoiding a\nbackward pass, have recently gained attention as a way of solving the\nbiologically unrealistic aspects of backpropagation. Here, we first address\ncompelling challenges related to the ""forward-only"" rules, which include\nreducing the performance gap with backpropagation and providing an analytical\nunderstanding of their dynamics. To this end, we show that the forward-only\nalgorithm with top-down feedback is well-approximated by an\n""adaptive-feedback-alignment"" algorithm, and we analytically track its\nperformance during learning in a prototype high-dimensional setting. Then, we\ncompare different versions of forward-only algorithms, focusing on the\nForward-Forward and PEPITA frameworks, and we show that they share the same\nlearning principles. Overall, our work unveils the connections between three\nkey neuro-inspired learning rules, providing a link between ""forward-only""\nalgorithms, i.e., Forward-Forward and PEPITA, and an approximation of\nbackpropagation, i.e., Feedback Alignment.\n', '  Graph neural networks (GNNs) have achieved remarkable success across a wide\nrange of applications, such as recommendation, drug discovery, and question\nanswering. Behind the success of GNNs lies the backpropagation (BP) algorithm,\nwhich is the de facto standard for training deep neural networks (NNs).\nHowever, despite its effectiveness, BP imposes several constraints, which are\nnot only biologically implausible, but also limit the scalability, parallelism,\nand flexibility in learning NNs. Examples of such constraints include storage\nof neural activities computed in the forward pass for use in the subsequent\nbackward pass, and the dependence of parameter updates on non-local signals. To\naddress these limitations, the forward-forward algorithm (FF) was recently\nproposed as an alternative to BP in the image classification domain, which\ntrains NNs by performing two forward passes over positive and negative data.\nInspired by this advance, we propose ForwardGNN in this work, a new forward\nlearning procedure for GNNs, which avoids the constraints imposed by BP via an\neffective layer-wise local forward training. ForwardGNN extends the original FF\nto deal with graph data and GNNs, and makes it possible to operate without\ngenerating negative inputs (hence no longer forward-forward). Further,\nForwardGNN enables each layer to learn from both the bottom-up and top-down\nsignals without relying on the backpropagation of errors. Extensive experiments\non real-world datasets show the effectiveness and generality of the proposed\nforward graph learning framework. We release our code at\nhttps://github.com/facebookresearch/forwardgnn.\n', '  The Backpropagation algorithm has often been criticised for its lack of\nbiological realism. In an attempt to find a more biologically plausible\nalternative, the recently introduced Forward-Forward algorithm replaces the\nforward and backward passes of Backpropagation with two forward passes. In this\nwork, we show that the internal representations obtained by the Forward-Forward\nalgorithm can organise into category-specific ensembles exhibiting high\nsparsity - composed of a low number of active units. This situation is\nreminiscent of what has been observed in cortical sensory areas, where neuronal\nensembles are suggested to serve as the functional building blocks for\nperception and action. Interestingly, while this sparse pattern does not\ntypically arise in models trained with standard Backpropagation, it can emerge\nin networks trained with Backpropagation on the same objective proposed for the\nForward-Forward algorithm. These results suggest that the learning procedure\nproposed by Forward-Forward may be superior to Backpropagation in modelling\nlearning in the cortex, even when a backward pass is used.\n']",Forward-Only Neural Network Learning Algorithms,Neural Network Optimization Techniques,Deep Learning Optimization and Training,Deep Learning Optimization and Security
369,19,369_imagenet_robust_dm_generated_image_detection_deepaugment_generative,"['imagenet', 'robust_dm_generated_image_detection', 'deepaugment', 'generative', 'imagenet_d', 'augmentation', 'encode', 'gan', 'augment', 'augmentations']","['diffusion', 'image', 'images', 'bagging', 'augmentation', 'classifier', 'consistency', 'synthetic', 'classifiers', 'diversity']","['  Bagging has achieved great success in the field of machine learning by\nintegrating multiple base classifiers to build a single strong classifier to\nreduce model variance. The performance improvement of bagging mainly relies on\nthe number and diversity of base classifiers. However, traditional deep\nlearning model training methods are expensive to train individually and\ndifficult to train multiple models with low similarity in a restricted dataset.\nRecently, diffusion models, which have been tremendously successful in the\nfields of imaging and vision, have been found to be effective in generating\nneural network model weights and biases with diversity. We creatively propose a\nBagging deep learning training algorithm based on Efficient Neural network\nDiffusion (BEND). The originality of BEND comes from the first use of a neural\nnetwork diffusion model to efficiently build base classifiers for bagging. Our\napproach is simple but effective, first using multiple trained model weights\nand biases as inputs to train autoencoder and latent diffusion model to realize\na diffusion model from noise to valid neural network parameters. Subsequently,\nwe generate several base classifiers using the trained diffusion model.\nFinally, we integrate these ba se classifiers for various inference tasks using\nthe Bagging method. Resulting experiments on multiple models and datasets show\nthat our proposed BEND algorithm can consistently outperform the mean and\nmedian accuracies of both the original trained model and the diffused model. At\nthe same time, new models diffused using the diffusion model have higher\ndiversity and lower cost than multiple models trained using traditional\nmethods. The BEND approach successfully introduces diffusion models into the\nnew deep learning training domain and provides a new paradigm for future deep\nlearning training and inference.\n', '  The evolution of Diffusion Models has dramatically improved image generation\nquality, making it increasingly difficult to differentiate between real and\ngenerated images. This development, while impressive, also raises significant\nprivacy and security concerns. In response to this, we propose a novel Latent\nREconstruction error guided feature REfinement method (LaRE^2) for detecting\nthe diffusion-generated images. We come up with the Latent Reconstruction Error\n(LaRE), the first reconstruction-error based feature in the latent space for\ngenerated image detection. LaRE surpasses existing methods in terms of feature\nextraction efficiency while preserving crucial cues required to differentiate\nbetween the real and the fake. To exploit LaRE, we propose an Error-Guided\nfeature REfinement module (EGRE), which can refine the image feature guided by\nLaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an\nalign-then-refine mechanism, which effectively refines the image feature for\ngenerated-image detection from both spatial and channel perspectives. Extensive\nexperiments on the large-scale GenImage benchmark demonstrate the superiority\nof our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1%\naverage ACC/AP across 8 different image generators. LaRE also surpasses\nexisting methods in terms of feature extraction cost, delivering an impressive\nspeed enhancement of 8 times.\n', '  Existing image augmentation methods consist of two categories:\nperturbation-based methods and generative methods. Perturbation-based methods\napply pre-defined perturbations to augment an original image, but only locally\nvary the image, thus lacking image diversity. In contrast, generative methods\nbring more image diversity in the augmented images but may not preserve\nsemantic consistency, thus incorrectly changing the essential semantics of the\noriginal image. To balance image diversity and semantic consistency in\naugmented images, we propose SGID, a Semantic-guided Generative Image\naugmentation method with Diffusion models for image classification.\nSpecifically, SGID employs diffusion models to generate augmented images with\ngood image diversity. More importantly, SGID takes image labels and captions as\nguidance to maintain semantic consistency between the augmented and original\nimages. Experimental results show that SGID outperforms the best augmentation\nbaseline by 1.72% on ResNet-50 (from scratch), 0.33% on ViT (ImageNet-21k), and\n0.14% on CLIP-ViT (LAION-2B). Moreover, SGID can be combined with other image\naugmentation baselines and further improves the overall performance. We\ndemonstrate the semantic consistency and image diversity of SGID through\nquantitative human and automated evaluations, as well as qualitative case\nstudies.\n']",Diffusion Models for Image Generation and Augmentation,Diffusion Models for Image and Video Generation and Manipulation,Diffusion Models for Generative Tasks,Diffusion Models for Generative Tasks
370,19,370_kernels_nonparametric_kernel_stein,"['kernels', 'nonparametric', 'kernel', 'stein', 'compression', 'distributions', 'unnormalised', 'gaussian', 'statistical', 'ksd']","['kernel', 'goodness', 'tests', 'divergences', 'test', 'thinning', 'discrepancy', 'distance', 'fit', 'statistics']","['  Over the last decade, an approach that has gained a lot of popularity to\ntackle nonparametric testing problems on general (i.e., non-Euclidean) domains\nis based on the notion of reproducing kernel Hilbert space (RKHS) embedding of\nprobability distributions. The main goal of our work is to understand the\noptimality of two-sample tests constructed based on this approach. First, we\nshow the popular MMD (maximum mean discrepancy) two-sample test to be not\noptimal in terms of the separation boundary measured in Hellinger distance.\nSecond, we propose a modification to the MMD test based on spectral\nregularization by taking into account the covariance information (which is not\ncaptured by the MMD test) and prove the proposed test to be minimax optimal\nwith a smaller separation boundary than that achieved by the MMD test. Third,\nwe propose an adaptive version of the above test which involves a data-driven\nstrategy to choose the regularization parameter and show the adaptive test to\nbe almost minimax optimal up to a logarithmic factor. Moreover, our results\nhold for the permutation variant of the test where the test threshold is chosen\nelegantly through the permutation of the samples. Through numerical experiments\non synthetic and real data, we demonstrate the superior performance of the\nproposed test in comparison to the MMD test and other popular tests in the\nliterature.\n', '  Modern compression methods can summarize a target distribution $\\mathbb{P}$\nmore succinctly than i.i.d. sampling but require access to a low-bias input\nsequence like a Markov chain converging quickly to $\\mathbb{P}$. We introduce a\nnew suite of compression methods suitable for compression with biased input\nsequences. Given $n$ points targeting the wrong distribution and quadratic\ntime, Stein kernel thinning (SKT) returns $\\sqrt{n}$ equal-weighted points with\n$\\widetilde{O}(n^{-1/2})$ maximum mean discrepancy (MMD) to $\\mathbb{P}$. For\nlarger-scale compression tasks, low-rank SKT achieves the same feat in\nsub-quadratic time using an adaptive low-rank debiasing procedure that may be\nof independent interest. For downstream tasks that support simplex or\nconstant-preserving weights, Stein recombination and Stein Cholesky achieve\neven greater parsimony, matching the guarantees of SKT with as few as\n$\\text{poly-log}(n)$ weighted points. Underlying these advances are new\nguarantees for the quality of simplex-weighted coresets, the spectral decay of\nkernel matrices, and the covering numbers of Stein kernel Hilbert spaces. In\nour experiments, our techniques provide succinct and accurate posterior\nsummaries while overcoming biases due to burn-in, approximate Markov chain\nMonte Carlo, and tempering.\n', '  Kernel methods underpin many of the most successful approaches in data\nscience and statistics, and they allow representing probability measures as\nelements of a reproducing kernel Hilbert space without loss of information.\nRecently, the kernel Stein discrepancy (KSD), which combines Stein\'s method\nwith kernel techniques, gained considerable attention. Through the Stein\noperator, KSD allows the construction of powerful goodness-of-fit tests where\nit is sufficient to know the target distribution up to a multiplicative\nconstant. However, the typical U- and V-statistic-based KSD estimators suffer\nfrom a quadratic runtime complexity, which hinders their application in\nlarge-scale settings. In this work, we propose a Nystr\\""om-based KSD\nacceleration -- with runtime $\\mathcal O\\!\\left(mn+m^3\\right)$ for $n$ samples\nand $m\\ll n$ Nystr\\""om points -- , show its $\\sqrt{n}$-consistency under the\nnull with a classical sub-Gaussian assumption, and demonstrate its\napplicability for goodness-of-fit testing on a suite of benchmarks.\n']",Kernel Methods for Nonparametric Testing,Kernel Methods for Machine Learning and Statistical Analysis,Machine Learning and Artificial Intelligence,Machine Learning and Intelligent Systems
371,19,371_ai_alignmentsurvey_alignment_intelligence,"['ai', 'alignmentsurvey', 'alignment', 'intelligence', 'humanity', 'agent', 'agents', 'aligned', 'ethical', 'ethics']","['alignment', 'values', 'ethics', 'moral', 'humans', 'preferences', 'value', 'agents', 'norms', 'human']","['  Discussion of AI alignment (alignment between humans and AI systems) has\nfocused on value alignment, broadly referring to creating AI systems that share\nhuman values. We argue that before we can even attempt to align values, it is\nimperative that AI systems and humans align the concepts they use to understand\nthe world. We integrate ideas from philosophy, cognitive science, and deep\nlearning to explain the need for concept alignment, not just value alignment,\nbetween humans and machines. We summarize existing accounts of how humans and\nmachines currently learn concepts, and we outline opportunities and challenges\nin the path towards shared concepts. Finally, we explain how we can leverage\nthe tools already being developed in cognitive science and AI research to\naccelerate progress towards concept alignment.\n', ""  How can we build AI systems that are aligned with human values to avoid\ncausing harm or violating societal standards for acceptable behavior? We argue\nthat representational alignment between humans and AI agents facilitates value\nalignment. Making AI systems learn human-like representations of the world has\nmany known benefits, including improving generalization, robustness to domain\nshifts, and few-shot learning performance. We propose that this kind of\nrepresentational alignment between machine learning (ML) models and humans can\nalso support value alignment, allowing ML systems to conform to human values\nand societal norms. We focus on ethics as one aspect of value alignment and\ntrain ML agents using a variety of methods in a multi-armed bandit setting,\nwhere rewards reflect the moral acceptability of the chosen action. We use a\nsynthetic experiment to demonstrate that agents' representational alignment\nwith the environment bounds their learning performance. We then repeat this\nprocedure in a realistic setting, using textual action descriptions and\nsimilarity judgments collected from humans and a variety of language models, to\nshow that the results generalize and are model-agnostic when grounded in an\nethically relevant context.\n"", '  Recent advancements in general-purpose AI have highlighted the importance of\nguiding AI systems towards the intended goals, ethical principles, and values\nof individuals and groups, a concept broadly recognized as alignment. However,\nthe lack of clarified definitions and scopes of human-AI alignment poses a\nsignificant obstacle, hampering collaborative efforts across research domains\nto achieve this alignment. In particular, ML- and philosophy-oriented alignment\nresearch often views AI alignment as a static, unidirectional process (i.e.,\naiming to ensure that AI systems\' objectives match humans) rather than an\nongoing, mutual alignment problem. This perspective largely neglects the\nlong-term interaction and dynamic changes of alignment. To understand these\ngaps, we introduce a systematic review of over 400 papers published between\n2019 and January 2024, spanning multiple domains such as Human-Computer\nInteraction (HCI), Natural Language Processing (NLP), Machine Learning (ML). We\ncharacterize, define and scope human-AI alignment. From this, we present a\nconceptual framework of ""Bidirectional Human-AI Alignment"" to organize the\nliterature from a human-centered perspective. This framework encompasses both\n1) conventional studies of aligning AI to humans that ensures AI produces the\nintended outcomes determined by humans, and 2) a proposed concept of aligning\nhumans to AI, which aims to help individuals and society adjust to AI\nadvancements both cognitively and behaviorally. Additionally, we articulate the\nkey findings derived from literature analysis, including literature gaps and\ntrends, human values, and interaction techniques. To pave the way for future\nstudies, we envision three key challenges and give recommendations for future\nresearch.\n']",Human-AI Alignment and Ethics,Artificial Intelligence Ethics and Governance,Artificial Intelligence Applications and Implications,Artificial Intelligence Applications and Implications
372,19,372_metaphors_metaphor_metaphorical_similes,"['metaphors', 'metaphor', 'metaphorical', 'similes', 'metaphorically', 'chinese_metaphor_explanation', 'simile', 'figurative', 'linguistics', 'linguistically']","['metaphor', 'metaphors', 'figurative', 'infographics', 'metaphorical', 'conceptual', 'tenors', 'inapt', 'grounds', 'literal']","[""  Metaphors are considered to pose challenges for a wide spectrum of NLP tasks.\nThis gives rise to the area of computational metaphor processing. However, it\nremains unclear what types of metaphors challenge current state-of-the-art\nmodels. In this paper, we test various NLP models on the VUA metaphor dataset\nand quantify to what extent metaphors affect models' performance on various\ndownstream tasks. Analysis reveals that VUA includes a large number of\nmetaphors that pose little difficulty to downstream tasks. We would like to\nshift the attention of researchers away from these metaphors to instead focus\non challenging metaphors. To identify hard metaphors, we propose an automatic\npipeline that identifies metaphors that challenge a particular model. Our\nanalysis demonstrates that our detected hard metaphors contrast significantly\nwith VUA and reduce the accuracy of machine translation by 16\\%, QA performance\nby 4\\%, NLI by 7\\%, and metaphor identification recall by over 14\\% for various\npopular NLP systems.\n"", '  This paper presents the results of the shared task on Chinese metaphor\ngeneration, hosted at the 13th CCF Conference on Natural Language Processing\nand Chinese Computing (NLPCC 2024). The goal of this shared task is to generate\nChinese metaphors using machine learning techniques and effectively identifying\nbasic components of metaphorical sentences. It is divided into two subtasks: 1)\nMetaphor Generation, which involves creating a metaphor from a provided tuple\nconsisting of TENOR, GROUND, and VEHICLE. The goal here is to synthesize a\nmetaphor that connects the subject (i.e. TENOR) with the object (i.e. VEHICLE),\nguided by the concept of the GROUND. 2) Metaphor Components Identification,\nwhich extracts the most fitting TENORs, GROUNDs, and VEHICLEs from a\nmetaphorical sentence. This component requires the identification of the most\nfitting metaphor elements that correspond to the specified grounds. In addition\nto overall results, we report on the setup and insights from the metaphor\ngeneration shared task, which attracted a total of 4 participating teams across\nboth subtasks.\n', ""  Metaphors, although occasionally unperceived, are ubiquitous in our everyday\nlanguage. Thus, it is crucial for Language Models to be able to grasp the\nunderlying meaning of this kind of figurative language. In this work, we\npresent Meta4XNLI, a novel parallel dataset for the tasks of metaphor detection\nand interpretation that contains metaphor annotations in both Spanish and\nEnglish. We investigate language models' metaphor identification and\nunderstanding abilities through a series of monolingual and cross-lingual\nexperiments by leveraging our proposed corpus. In order to comprehend how these\nnon-literal expressions affect models' performance, we look over the results\nand perform an error analysis. Additionally, parallel data offers many\npotential opportunities to investigate metaphor transferability between these\nlanguages and the impact of translation on the development of multilingual\nannotated resources.\n""]",Metaphor Processing in NLP,Natural Language Processing and Semantic Analysis,Natural Language Processing,Natural Language Processing
373,19,373_memory_smartphone_devices_smartphones,"['memory', 'smartphone', 'devices', 'smartphones', 'mobile', 'ios', 'device', 'mobilellm', 'annotation', 'cloud']","['device', 'devices', 'mobile', 'cloud', 'personalized', 'personalization', 'latency', 'personal', 'hardware', 'user']","['  Computer systems are becoming increasingly heterogeneous with the emergence\nof new memory technologies and compute devices. GPUs alongside CPUs have become\ncommonplace and CXL is poised to be a mainstay of cloud systems. The operating\nsystem is responsible for managing these hardware resources, requiring\nmodification every time a new device is released. Years of research and\ndevelopment are sunk into tuning the OS for high performance with each new\nheterogeneous device. With the recent explosion in memory technologies and\ndomain-specific accelerators, it would be beneficial to have an OS that could\nprovide high performance for new devices without significant effort.\n  We propose LLaMaS which can adapt to new devices easily. LLaMaS uses Large\nLanguage Models (LLMs) to extract the useful features of new devices from their\ntextual description and uses these features to make operating system decisions\nat runtime. Adding support to LLaMaS for a new device is as simple as\ndescribing the system and new device properties in plaintext.\n  LLaMaS reduces the burden on system administrators to enable easy integration\nof new devices into production systems.\n  Preliminary evaluation using ChatGPT shows that LLMs are capable of\nextracting device features from text and make correct OS decisions based on\nthose features.\n', ""  On-device large language models (LLMs) are catalyzing novel mobile\napplications such as UI task automation and personalized email auto-reply,\nwithout giving away users' private data. However, on-device LLMs still suffer\nfrom unacceptably long inference latency, especially the time to first token\n(prefill stage) due to the need of long context for accurate, personalized\ncontent generation, as well as the lack of parallel computing capacity of\nmobile CPU/GPU.\n  To enable practical on-device LLM, we present mllm-NPU, the first-of-its-kind\nLLM inference system that efficiently leverages on-device Neural Processing\nUnit (NPU) offloading. Essentially, mllm-NPU is an algorithm-system co-design\nthat tackles a few semantic gaps between the LLM architecture and contemporary\nNPU design. Specifically, it re-constructs the prompt and model in three\nlevels: (1) At prompt level, it divides variable-length prompts into multiple\nfixed-sized chunks while maintaining data dependencies; (2) At tensor level, it\nidentifies and extracts significant outliers to run on the CPU/GPU in parallel\nwith minimal overhead; (3) At block level, it schedules Transformer blocks in\nan out-of-order manner to the CPU/GPU and NPU based on their hardware affinity\nand sensitivity to accuracy. Compared to competitive baselines, mllm-NPU\nachieves 22.4x faster prefill speed and 30.7x energy savings on average, and up\nto 32.8x speedup in an end-to-end real-world application. For the first time,\nmllm-NPU achieves more than 1,000 tokens/sec prefilling for a billion-sized\nmodel (Qwen1.5-1.8B), paving the way towards practical on-device LLM.\n"", '  Transformers have revolutionized the machine learning landscape, gradually\nmaking their way into everyday tasks and equipping our computers with ""sparks\nof intelligence"". However, their runtime requirements have prevented them from\nbeing broadly deployed on mobile. As personal devices become increasingly\npowerful and prompt privacy becomes an ever more pressing issue, we explore the\ncurrent state of mobile execution of Large Language Models (LLMs). To achieve\nthis, we have created our own automation infrastructure, MELT, which supports\nthe headless execution and benchmarking of LLMs on device, supporting different\nmodels, devices and frameworks, including Android, iOS and Nvidia Jetson\ndevices. We evaluate popular instruction fine-tuned LLMs and leverage different\nframeworks to measure their end-to-end and granular performance, tracing their\nmemory and energy requirements along the way. Our analysis is the first\nsystematic study of on-device LLM execution, quantifying performance, energy\nefficiency and accuracy across various state-of-the-art models and showcases\nthe state of on-device intelligence in the era of hyperscale models. Results\nhighlight the performance heterogeneity across targets and corroborates that\nLLM inference is largely memory-bound. Quantization drastically reduces memory\nrequirements and renders execution viable, but at a non-negligible accuracy\ncost. Drawing from its energy footprint and thermal behavior, the continuous\nexecution of LLMs remains elusive, as both factors negatively affect user\nexperience. Last, our experience shows that the ecosystem is still in its\ninfancy, and algorithmic as well as hardware breakthroughs can significantly\nshift the execution cost. We expect NPU acceleration, and framework-hardware\nco-design to be the biggest bet towards efficient standalone execution, with\nthe alternative of offloading tailored towards edge deployments.\n']",Efficient Large Language Models for Mobile Devices,"Large Language Models: Safety, Security, and Ethics",Large Language Models,Large Language Models
373,19,373_memory_smartphone_devices_smartphones,"['memory', 'smartphone', 'devices', 'smartphones', 'mobile', 'ios', 'device', 'mobilellm', 'annotation', 'cloud']","['device', 'devices', 'mobile', 'cloud', 'personalized', 'personalization', 'latency', 'personal', 'hardware', 'user']","['  Computer systems are becoming increasingly heterogeneous with the emergence\nof new memory technologies and compute devices. GPUs alongside CPUs have become\ncommonplace and CXL is poised to be a mainstay of cloud systems. The operating\nsystem is responsible for managing these hardware resources, requiring\nmodification every time a new device is released. Years of research and\ndevelopment are sunk into tuning the OS for high performance with each new\nheterogeneous device. With the recent explosion in memory technologies and\ndomain-specific accelerators, it would be beneficial to have an OS that could\nprovide high performance for new devices without significant effort.\n  We propose LLaMaS which can adapt to new devices easily. LLaMaS uses Large\nLanguage Models (LLMs) to extract the useful features of new devices from their\ntextual description and uses these features to make operating system decisions\nat runtime. Adding support to LLaMaS for a new device is as simple as\ndescribing the system and new device properties in plaintext.\n  LLaMaS reduces the burden on system administrators to enable easy integration\nof new devices into production systems.\n  Preliminary evaluation using ChatGPT shows that LLMs are capable of\nextracting device features from text and make correct OS decisions based on\nthose features.\n', ""  On-device large language models (LLMs) are catalyzing novel mobile\napplications such as UI task automation and personalized email auto-reply,\nwithout giving away users' private data. However, on-device LLMs still suffer\nfrom unacceptably long inference latency, especially the time to first token\n(prefill stage) due to the need of long context for accurate, personalized\ncontent generation, as well as the lack of parallel computing capacity of\nmobile CPU/GPU.\n  To enable practical on-device LLM, we present mllm-NPU, the first-of-its-kind\nLLM inference system that efficiently leverages on-device Neural Processing\nUnit (NPU) offloading. Essentially, mllm-NPU is an algorithm-system co-design\nthat tackles a few semantic gaps between the LLM architecture and contemporary\nNPU design. Specifically, it re-constructs the prompt and model in three\nlevels: (1) At prompt level, it divides variable-length prompts into multiple\nfixed-sized chunks while maintaining data dependencies; (2) At tensor level, it\nidentifies and extracts significant outliers to run on the CPU/GPU in parallel\nwith minimal overhead; (3) At block level, it schedules Transformer blocks in\nan out-of-order manner to the CPU/GPU and NPU based on their hardware affinity\nand sensitivity to accuracy. Compared to competitive baselines, mllm-NPU\nachieves 22.4x faster prefill speed and 30.7x energy savings on average, and up\nto 32.8x speedup in an end-to-end real-world application. For the first time,\nmllm-NPU achieves more than 1,000 tokens/sec prefilling for a billion-sized\nmodel (Qwen1.5-1.8B), paving the way towards practical on-device LLM.\n"", '  Transformers have revolutionized the machine learning landscape, gradually\nmaking their way into everyday tasks and equipping our computers with ""sparks\nof intelligence"". However, their runtime requirements have prevented them from\nbeing broadly deployed on mobile. As personal devices become increasingly\npowerful and prompt privacy becomes an ever more pressing issue, we explore the\ncurrent state of mobile execution of Large Language Models (LLMs). To achieve\nthis, we have created our own automation infrastructure, MELT, which supports\nthe headless execution and benchmarking of LLMs on device, supporting different\nmodels, devices and frameworks, including Android, iOS and Nvidia Jetson\ndevices. We evaluate popular instruction fine-tuned LLMs and leverage different\nframeworks to measure their end-to-end and granular performance, tracing their\nmemory and energy requirements along the way. Our analysis is the first\nsystematic study of on-device LLM execution, quantifying performance, energy\nefficiency and accuracy across various state-of-the-art models and showcases\nthe state of on-device intelligence in the era of hyperscale models. Results\nhighlight the performance heterogeneity across targets and corroborates that\nLLM inference is largely memory-bound. Quantization drastically reduces memory\nrequirements and renders execution viable, but at a non-negligible accuracy\ncost. Drawing from its energy footprint and thermal behavior, the continuous\nexecution of LLMs remains elusive, as both factors negatively affect user\nexperience. Last, our experience shows that the ecosystem is still in its\ninfancy, and algorithmic as well as hardware breakthroughs can significantly\nshift the execution cost. We expect NPU acceleration, and framework-hardware\nco-design to be the biggest bet towards efficient standalone execution, with\nthe alternative of offloading tailored towards edge deployments.\n']",Efficient Large Language Models for Mobile Devices,"Large Language Models: Safety, Security, and Ethics",Large Language Models,Large Language Models
374,19,374_spectral_graphs_spectrally_networks,"['spectral', 'graphs', 'spectrally', 'networks', 'graph', 'filters', 'convolutions', 'filtering', 'nodes', 'filter']","['spectral', 'filters', 'graph', 'polynomial', 'convolution', 'filter', 'polynomials', 'bases', 'basis', 'frequency']","['  Spectral Graph Neural Networks (GNNs) have achieved tremendous success in\ngraph learning. As an essential part of spectral GNNs, spectral graph\nconvolution extracts crucial frequency information in graph data, leading to\nsuperior performance of spectral GNNs in downstream tasks. However, in this\npaper, we show that existing spectral GNNs remain critical drawbacks in\nperforming the spectral graph convolution. Specifically, considering the\nspectral graph convolution as a construction operation towards target output,\nwe prove that existing popular convolution paradigms cannot construct the\ntarget output with mild conditions on input graph signals, causing spectral\nGNNs to fall into suboptimal solutions. To address the issues, we rethink the\nspectral graph convolution from a more general two-dimensional (2-D) signal\nconvolution perspective and propose a new convolution paradigm, named 2-D graph\nconvolution. We prove that 2-D graph convolution unifies existing graph\nconvolution paradigms, and is capable to construct arbitrary target output.\nBased on the proposed 2-D graph convolution, we further propose ChebNet2D, an\nefficient and effective GNN implementation of 2-D graph convolution through\napplying Chebyshev interpolation. Extensive experiments on benchmark datasets\ndemonstrate both effectiveness and efficiency of the ChebNet2D.\n', '  Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded\nin the spectral domain, their practical reliance on polynomial approximation\nimplies a profound linkage to the spatial domain. As previous studies rarely\nexamine spectral GNNs from the spatial perspective, their spatial-domain\ninterpretability remains elusive, e.g., what information is essentially encoded\nby spectral GNNs in the spatial domain? In this paper, to answer this question,\nwe investigate the theoretical connection between spectral filtering and\nspatial aggregation, unveiling an intrinsic interaction that spectral filtering\nimplicitly leads the original graph to an adapted new graph, explicitly\ncomputed for spatial aggregation. Both theoretical and empirical investigations\nreveal that the adapted new graph not only exhibits non-locality but also\naccommodates signed edge weights to reflect label consistency among nodes.\nThese findings thus highlight the interpretable role of spectral GNNs in the\nspatial domain and inspire us to rethink graph spectral filters beyond the\nfixed-order polynomials, which neglect global information. Built upon the\ntheoretical findings, we revisit the state-of-the-art spectral GNNs and propose\na novel Spatially Adaptive Filtering (SAF) framework, which leverages the\nadapted new graph by spectral filtering for an auxiliary non-local aggregation.\nNotably, our SAF comprehensively models both node similarity and dissimilarity\nfrom a global perspective, therefore alleviating persistent deficiencies of\nGNNs related to long-range dependencies and graph heterophily. Extensive\nexperiments over 13 node classification benchmarks demonstrate the superiority\nof our proposed framework to the state-of-the-art methods.\n', '  With the recent advancements in graph neural networks (GNNs), spectral GNNs\nhave received increasing popularity by virtue of their specialty in capturing\ngraph signals in the frequency domain, demonstrating promising capability in\nspecific tasks. However, few systematic studies have been conducted on\nassessing their spectral characteristics. This emerging family of models also\nvaries in terms of designs and settings, leading to difficulties in comparing\ntheir performance and deciding on the suitable model for specific scenarios,\nespecially for large-scale tasks. In this work, we extensively benchmark\nspectral GNNs with a focus on the frequency perspective. We analyze and\ncategorize over 30 GNNs with 27 corresponding filters. Then, we implement these\nspectral models under a unified framework with dedicated graph computations and\nefficient training schemes. Thorough experiments are conducted on the spectral\nmodels with inclusive metrics on effectiveness and efficiency, offering\npractical guidelines on evaluating and selecting spectral GNNs with desirable\nperformance. Our implementation enables application on larger graphs with\ncomparable performance and less overhead, which is available at:\nhttps://github.com/gdmnl/Spectral-GNN-Benchmark.\n']",Spectral Graph Neural Networks,Graph Neural Networks and Deep Learning on Graph-Structured Data,Graph Representation Learning and Neural Networks,Graph Representation Learning and Neural Networks
375,19,375_discourse_linguistic_linguistics_corpus,"['discourse', 'linguistic', 'linguistics', 'corpus', 'treebank', 'annotation', 'parsing', 'parsers', 'rhetorical', 'connectives']","['discourse', 'rhetorical', 'connectives', 'relations', 'relation', 'parsing', 'structures', 'topic', 'dependency', 'implicit']","[""  This paper aims to quantitatively evaluate the performance of ChatGPT, an\ninteractive large language model, on inter-sentential relations such as\ntemporal relations, causal relations, and discourse relations. Given ChatGPT's\npromising performance across various tasks, we proceed to carry out thorough\nevaluations on the whole test sets of 11 datasets, including temporal and\ncausal relations, PDTB2.0-based, and dialogue-based discourse relations. To\nensure the reliability of our findings, we employ three tailored prompt\ntemplates for each task, including the zero-shot prompt template, zero-shot\nprompt engineering (PE) template, and in-context learning (ICL) prompt\ntemplate, to establish the initial baseline scores for all popular\nsentence-pair relation classification tasks for the first time. Through our\nstudy, we discover that ChatGPT exhibits exceptional proficiency in detecting\nand reasoning about causal relations, albeit it may not possess the same level\nof expertise in identifying the temporal order between two events. While it is\ncapable of identifying the majority of discourse relations with existing\nexplicit discourse connectives, the implicit discourse relation remains a\nformidable challenge. Concurrently, ChatGPT demonstrates subpar performance in\nthe dialogue discourse parsing task that requires structural understanding in a\ndialogue before being aware of the discourse relation.\n"", ""  The development of different theories of discourse structure has led to the\nestablishment of discourse corpora based on these theories. However, the\nexistence of discourse corpora established on different theoretical bases\ncreates challenges when it comes to exploring them in a consistent and cohesive\nway. This study has as its primary focus the conversion of PDTB annotations\ninto dependency structures. It employs refined BERT-based discourse parsers to\ntest the validity of the dependency data derived from the PDTB-style corpora in\nEnglish, Chinese, and several other languages. By converting both PDTB and RST\nannotations for the same texts into dependencies, this study also applies\n``dependency distance'' metrics to examine the correlation between RST\ndependencies and PDTB dependencies in English. The results show that the PDTB\ndependency data is valid and that there is a strong correlation between the two\ntypes of dependency distance. This study presents a comprehensive approach for\nanalyzing and evaluating discourse corpora by employing discourse dependencies\nto achieve unified analysis. By applying dependency representations, we can\nextract data from PDTB, RST, and SDRT corpora in a coherent and unified manner.\nMoreover, the cross-linguistic validation establishes the framework's\ngeneralizability beyond English. The establishment of this comprehensive\ndependency framework overcomes limitations of existing discourse corpora,\nsupporting a diverse range of algorithms and facilitating further studies in\ncomputational discourse analysis and language sciences.\n"", '  In this article we present Enhanced Rhetorical Structure Theory (eRST), a new\ntheoretical framework for computational discourse analysis, based on an\nexpansion of Rhetorical Structure Theory (RST). The framework encompasses\ndiscourse relation graphs with tree-breaking, nonprojective and concurrent\nrelations, as well as implicit and explicit signals which give explainable\nrationales to our analyses. We survey shortcomings of RST and other existing\nframeworks, such as Segmented Discourse Representation Theory (SDRT), the Penn\nDiscourse Treebank (PDTB) and Discourse Dependencies, and address these using\nconstructs in the proposed theory. We provide annotation, search and\nvisualization tools for data, and present and evaluate a freely available\ncorpus of English annotated according to our framework, encompassing 12 spoken\nand written genres with over 200K tokens. Finally, we discuss automatic\nparsing, evaluation metrics and applications for data in our framework.\n']",Discourse Analysis and Linguistic Parsing,Natural Language Processing and Linguistics,Natural Language Processing,Natural Language Processing
376,19,376_malicious_recommender_adversary_adversarial,"['malicious', 'recommender', 'adversary', 'adversarial', 'recommenders', 'attacks', 'vulnerabilities', 'threats', 'attacker', 'attackers']","['recommender', 'poisoning', 'attacks', 'attack', 'item', 'robustness', 'systems', 'malicious', 'items', 'attackers']","[""  Modern recommender systems (RS) have profoundly enhanced user experience\nacross digital platforms, yet they face significant threats from poisoning\nattacks. These attacks, aimed at manipulating recommendation outputs for\nunethical gains, exploit vulnerabilities in RS through injecting malicious data\nor intervening model training. This survey presents a unique perspective by\nexamining these threats through the lens of an attacker, offering fresh\ninsights into their mechanics and impacts. Concretely, we detail a systematic\npipeline that encompasses four stages of a poisoning attack: setting attack\ngoals, assessing attacker capabilities, analyzing victim architecture, and\nimplementing poisoning strategies. The pipeline not only aligns with various\nattack tactics but also serves as a comprehensive taxonomy to pinpoint focuses\nof distinct poisoning attacks. Correspondingly, we further classify defensive\nstrategies into two main categories: poisoning data filtering and robust\ntraining from the defender's perspective. Finally, we highlight existing\nlimitations and suggest innovative directions for further exploration in this\nfield.\n"", ""  To make room for privacy and efficiency, the deployment of many recommender\nsystems is experiencing a shift from central servers to personal devices, where\nthe federated recommender systems (FedRecs) and decentralized collaborative\nrecommender systems (DecRecs) are arguably the two most representative\nparadigms. While both leverage knowledge (e.g., gradients) sharing to\nfacilitate learning local models, FedRecs rely on a central server to\ncoordinate the optimization process, yet in DecRecs, the knowledge sharing\ndirectly happens between clients. Knowledge sharing also opens a backdoor for\nmodel poisoning attacks, where adversaries disguise themselves as benign\nclients and disseminate polluted knowledge to achieve malicious goals like\npromoting an item's exposure rate. Although research on such poisoning attacks\nprovides valuable insights into finding security loopholes and corresponding\ncountermeasures, existing attacks mostly focus on FedRecs, and are either\ninapplicable or ineffective for DecRecs. Compared with FedRecs where the\ntampered information can be universally distributed to all clients once\nuploaded to the cloud, each adversary in DecRecs can only communicate with\nneighbor clients of a small size, confining its impact to a limited range. To\nfill the gap, we present a novel attack method named Poisoning with Adaptive\nMalicious Neighbors (PAMN). With item promotion in top-K recommendation as the\nattack objective, PAMN effectively boosts target items' ranks with several\nadversaries that emulate benign clients and transfers adaptively crafted\ngradients conditioned on each adversary's neighbors. Moreover, with the\nvulnerabilities of DecRecs uncovered, a dedicated defensive mechanism based on\nuser-level gradient clipping with sparsified updating is proposed. Extensive\nexperiments demonstrate the effectiveness of the poisoning attack and the\nrobustness of our defensive mechanism.\n"", ""  Recommender systems have become an integral part of online services to help\nusers locate specific information in a sea of data. However, existing studies\nshow that some recommender systems are vulnerable to poisoning attacks,\nparticularly those that involve learning schemes. A poisoning attack is where\nan adversary injects carefully crafted data into the process of training a\nmodel, with the goal of manipulating the system's final recommendations. Based\non recent advancements in artificial intelligence, such attacks have gained\nimportance recently. While numerous countermeasures to poisoning attacks have\nbeen developed, they have not yet been systematically linked to the properties\nof the attacks. Consequently, assessing the respective risks and potential\nsuccess of mitigation strategies is difficult, if not impossible. This survey\naims to fill this gap by primarily focusing on poisoning attacks and their\ncountermeasures. This is in contrast to prior surveys that mainly focus on\nattacks and their detection methods. Through an exhaustive literature review,\nwe provide a novel taxonomy for poisoning attacks, formalise its dimensions,\nand accordingly organise 30+ attacks described in the literature. Further, we\nreview 40+ countermeasures to detect and/or prevent poisoning attacks,\nevaluating their effectiveness against specific types of attacks. This\ncomprehensive survey should serve as a point of reference for protecting\nrecommender systems against poisoning attacks. The article concludes with a\ndiscussion on open issues in the field and impactful directions for future\nresearch. A rich repository of resources associated with poisoning attacks is\navailable at https://github.com/tamlhp/awesome-recsys-poisoning.\n""]",Recommender System Poisoning Attacks and Defenses,Advances in Recommender Systems,Recommender Systems and Personalization,Recommender Systems and Personalization
377,19,377_imagenet_contrastive_learning_embeddings,"['imagenet', 'contrastive', 'learning', 'embeddings', 'unlearning', 'views', 'supervised', 'view', 'representations', 'unsupervised']","['contrastive', 'views', 'positive', 'supervised', 'epochs', 'negative', 'downstream', 'augmentation', 'augmentations', 'learning']","['  In contrastive learning, two views of an original image, generated by\ndifferent augmentations, are considered a positive pair, and their similarity\nis required to be high. Similarly, two views of distinct images form a negative\npair, with encouraged low similarity. Typically, a single similarity measure,\nprovided by a lone projection head, evaluates positive and negative sample\npairs. However, due to diverse augmentation strategies and varying intra-sample\nsimilarity, views from the same image may not always be similar. Additionally,\nowing to inter-sample similarity, views from different images may be more akin\nthan those from the same image. Consequently, enforcing high similarity for\npositive pairs and low similarity for negative pairs may be unattainable, and\nin some cases, such enforcement could detrimentally impact performance. To\naddress this challenge, we propose using multiple projection heads, each\nproducing a distinct set of features. Our pre-training loss function emerges\nfrom a solution to the maximum likelihood estimation over head-wise posterior\ndistributions of positive samples given observations. This loss incorporates\nthe similarity measure over positive and negative pairs, each re-weighted by an\nindividual adaptive temperature, regulated to prevent ill solutions. Our\napproach, Adaptive Multi-Head Contrastive Learning (AMCL), can be applied to\nand experimentally enhances several popular contrastive learning methods such\nas SimCLR, MoCo, and Barlow Twins. The improvement remains consistent across\nvarious backbones and linear probing epochs, and becomes more significant when\nemploying multiple augmentation methods.\n', '  In the era of big data and Artificial Intelligence, an emerging paradigm is\nto utilize contrastive self-supervised learning to model large-scale\nheterogeneous data. Many existing foundation models benefit from the\ngeneralization capability of contrastive self-supervised learning by learning\ncompact and high-quality representations without relying on any label\ninformation. Amidst the explosive advancements in foundation models across\nmultiple domains, including natural language processing and computer vision, a\nthorough survey on heterogeneous contrastive learning for the foundation model\nis urgently needed. In response, this survey critically evaluates the current\nlandscape of heterogeneous contrastive learning for foundation models,\nhighlighting the open challenges and future trends of contrastive learning. In\nparticular, we first present how the recent advanced contrastive learning-based\nmethods deal with view heterogeneity and how contrastive learning is applied to\ntrain and fine-tune the multi-view foundation models. Then, we move to\ncontrastive learning methods for task heterogeneity, including pretraining\ntasks and downstream tasks, and show how different tasks are combined with\ncontrastive learning loss for different purposes. Finally, we conclude this\nsurvey by discussing the open challenges and shedding light on the future\ndirections of contrastive learning.\n', '  Contrastive learning is a paradigm for learning representations from\nunlabelled data that has been highly successful for image and text data.\nSeveral recent works have examined contrastive losses to claim that contrastive\nmodels effectively learn spectral embeddings, while few works show relations\nbetween (wide) contrastive models and kernel principal component analysis\n(PCA). However, it is not known if trained contrastive models indeed correspond\nto kernel methods or PCA. In this work, we analyze the training dynamics of\ntwo-layer contrastive models, with non-linear activation, and answer when these\nmodels are close to PCA or kernel methods. It is well known in the supervised\nsetting that neural networks are equivalent to neural tangent kernel (NTK)\nmachines, and that the NTK of infinitely wide networks remains constant during\ntraining. We provide the first convergence results of NTK for contrastive\nlosses, and present a nuanced picture: NTK of wide networks remains almost\nconstant for cosine similarity based contrastive losses, but not for losses\nbased on dot product similarity. We further study the training dynamics of\ncontrastive models with orthogonality constraints on output layer, which is\nimplicitly assumed in works relating contrastive learning to spectral\nembedding. Our deviation bounds suggest that representations learned by\ncontrastive models are close to the principal components of a certain matrix\ncomputed from random features. We empirically show that our theoretical results\npossibly hold beyond two-layer networks.\n']",Contrastive Learning for Representation,Self-Supervised Representation Learning,Self-Supervised Representation Learning,Self-Supervised Representation Learning
378,19,378_memecraft_memes_multimodal_memeguard,"['memecraft', 'memes', 'multimodal', 'memeguard', 'mememqacorpus', 'hateful', 'meme', 'hatesieve', 'embeddings', 'hatefuldiscussions']","['memes', 'meme', 'hateful', 'harmful', 'hate', 'multimodal', 'hatefulness', 'internet', 'content', 'social']","['  Warning: This paper contains memes that may be offensive to some readers.\n  Multimodal Internet Memes are now a ubiquitous fixture in online discourse.\nOne strand of meme-based research is the classification of memes according to\nvarious affects, such as sentiment and hate, supported by manually compiled\nmeme datasets. Understanding the unique characteristics of memes is crucial for\nmeme classification. Unlike other user-generated content, memes spread via\nmemetics, i.e. the process by which memes are imitated and transformed into\nsymbols used to create new memes. In effect, there exists an ever-evolving pool\nof visual and linguistic symbols that underpin meme culture and are crucial to\ninterpreting the meaning of individual memes. The current approach of training\nsupervised learning models on static datasets, without taking memetics into\naccount, limits the depth and accuracy of meme interpretation. We argue that\nmeme datasets must contain genuine memes, as defined via memetics, so that\neffective meme classifiers can be built. In this work, we develop a meme\nidentification protocol which distinguishes meme from non-memetic content by\nrecognising the memetics within it. We apply our protocol to random samplings\nof the leading 7 meme classification datasets and observe that more than half\n(50. 4\\%) of the evaluated samples were found to contain no signs of memetics.\nOur work also provides a meme typology grounded in memetics, providing the\nbasis for more effective approaches to the interpretation of memes and the\ncreation of meme datasets.\n', '  Recent advances show that two-stream approaches have achieved outstanding\nperformance in hateful meme detection. However, hateful memes constantly evolve\nas new memes emerge by fusing progressive cultural ideas, making existing\nmethods obsolete or ineffective. In this work, we explore the potential of\nLarge Multimodal Models (LMMs) for hateful meme detection. To this end, we\npropose Evolver, which incorporates LMMs via Chain-of-Evolution (CoE)\nPrompting, by integrating the evolution attribute and in-context information of\nmemes. Specifically, Evolver simulates the evolving and expressing process of\nmemes and reasons through LMMs in a step-by-step manner. First, an evolutionary\npair mining module retrieves the top-k most similar memes in the external\ncurated meme set with the input meme. Second, an evolutionary information\nextractor is designed to summarize the semantic regularities between the paired\nmemes for prompting. Finally, a contextual relevance amplifier enhances the\nin-context hatefulness information to boost the search for evolutionary\nprocesses. Extensive experiments on public FHM, MAMI, and HarM datasets show\nthat CoE prompting can be incorporated into existing LMMs to improve their\nperformance. More encouragingly, it can serve as an interpretive tool to\npromote the understanding of the evolution of social memes.\n', '  Multimedia content on social media is rapidly evolving, with memes gaining\nprominence as a distinctive form. Unfortunately, some malicious users exploit\nmemes to target individuals or vulnerable communities, making it imperative to\nidentify and address such instances of hateful memes. Extensive research has\nbeen conducted to address this issue by developing hate meme detection models.\nHowever, a notable limitation of traditional machine/deep learning models is\nthe requirement for labeled datasets for accurate classification. Recently, the\nresearch community has witnessed the emergence of several visual language\nmodels that have exhibited outstanding performance across various tasks. In\nthis study, we aim to investigate the efficacy of these visual language models\nin handling intricate tasks such as hate meme detection. We use various prompt\nsettings to focus on zero-shot classification of hateful/harmful memes. Through\nour analysis, we observe that large VLMs are still vulnerable for zero-shot\nhate meme detection.\n']",Hateful Meme Detection and Analysis,Social Media Misbehavior Detection and Analysis,Misbehavior and Toxicity in Online Content,Misbehavior and Toxicity in Online Content
379,19,379_trust_trustworthiness_trusting_distrust,"['trust', 'trustworthiness', 'trusting', 'distrust', 'trustworthy', 'trustors', 'ai', 'confidence', 'faith', 'validated']","['trust', 'trustworthy', 'trustworthiness', 'uncalibrated', 'distrust', 'trustees', 'trustors', 'collaboration', 'disuse', 'users']","[""  The rationale of this work is based on the current user trust discourse of\nArtificial Intelligence (AI). We aim to produce novel HCI approaches that use\ntrust as a facilitator for the uptake (or appropriation) of current\ntechnologies. We propose a framework (HCTFrame) to guide non-experts to unlock\nthe full potential of user trust in AI design. Results derived from a data\ntriangulation of findings from three literature reviews demystify some\nmisconceptions of user trust in computer science and AI discourse, and three\ncase studies are conducted to assess the effectiveness of a psychometric scale\nin mapping potential users' trust breakdowns and concerns. This work primarily\ncontributes to the fight against the tendency to design technical-centered\nvulnerable interactions, which can eventually lead to additional real and\nperceived breaches of trust. The proposed framework can be used to guide system\ndesigners on how to map and define user trust and the socioethical and\norganisational needs and characteristics of AI system design. It can also guide\nAI system designers on how to develop a prototype and operationalise a solution\nthat meets user trust requirements. The article ends by providing some user\nresearch tools that can be employed to measure users' trust intentions and\nbehaviours towards a proposed solution.\n"", ""  Humans' trust in AI constitutes a pivotal element in fostering a synergistic\nrelationship between humans and AI. This is particularly significant in the\ncontext of systems that leverage AI technology, such as autonomous driving\nsystems and human-robot interaction. Trust facilitates appropriate utilization\nof these systems, thereby optimizing their potential benefits. If humans\nover-trust or under-trust an AI, serious problems such as misuse and accidents\noccur. To prevent over/under-trust, it is necessary to predict trust dynamics.\nHowever, trust is an internal state of humans and hard to directly observe.\nTherefore, we propose a prediction model for trust dynamics using dynamic\nstructure equation modeling, which extends SEM that can handle time-series\ndata. A path diagram, which shows causalities between variables, is developed\nin an exploratory way and the resultant path diagram is optimized for effective\npath structures. Over/under-trust was predicted with 90\\% accuracy in a drone\nsimulator task,, and it was predicted with 99\\% accuracy in an autonomous\ndriving task. These results show that our proposed method outperformed the\nconventional method including an auto regression family.\n"", ""  Trust is not just a cognitive issue but also an emotional one, yet the\nresearch in human-AI interactions has primarily focused on the cognitive route\nof trust development. Recent work has highlighted the importance of studying\naffective trust towards AI, especially in the context of emerging human-like\nLLMs-powered conversational agents. However, there is a lack of validated and\ngeneralizable measures for the two-dimensional construct of trust in AI agents.\nTo address this gap, we developed and validated a set of 27-item semantic\ndifferential scales for affective and cognitive trust through a scenario-based\nsurvey study. We then further validated and applied the scale through an\nexperiment study. Our empirical findings showed how the emotional and cognitive\naspects of trust interact with each other and collectively shape a person's\noverall trust in AI agents. Our study methodology and findings also provide\ninsights into the capability of the state-of-art LLMs to foster trust through\ndifferent routes.\n""]",Human Trust in Artificial Intelligence (AI) Systems,Artificial Intelligence Ethics and Governance,Artificial Intelligence Applications and Implications,Artificial Intelligence Applications and Implications
380,18,380_modality_multimodal_entities_entity,"['modality', 'multimodal', 'entities', 'entity', 'modal', 'semantic', 'semantics', 'modalities', 'knowledge', 'relational']","['modal', 'entity', 'entities', 'modality', 'multi', 'relational', 'modalities', 'fusion', 'knowledge', 'alignment']","['  Multi-modal entity alignment (MMEA) aims to identify equivalent entity pairs\nacross different multi-modal knowledge graphs (MMKGs). Existing approaches\nfocus on how to better encode and aggregate information from different\nmodalities. However, it is not trivial to leverage multi-modal knowledge in\nentity alignment due to the modal heterogeneity. In this paper, we propose a\nMulti-Grained Interaction framework for Multi-Modal Entity Alignment (MIMEA),\nwhich effectively realizes multi-granular interaction within the same modality\nor between different modalities. MIMEA is composed of four modules: i) a\nMulti-modal Knowledge Embedding module, which extracts modality-specific\nrepresentations with multiple individual encoders; ii) a Probability-guided\nModal Fusion module, which employs a probability guided approach to integrate\nuni-modal representations into joint-modal embeddings, while considering the\ninteraction between uni-modal representations; iii) an Optimal Transport Modal\nAlignment module, which introduces an optimal transport mechanism to encourage\nthe interaction between uni-modal and joint-modal embeddings; iv) a\nModal-adaptive Contrastive Learning module, which distinguishes the embeddings\nof equivalent entities from those of non-equivalent ones, for each modality.\nExtensive experiments conducted on two real-world datasets demonstrate the\nstrong performance of MIMEA compared to the SoTA. Datasets and code have been\nsubmitted as supplementary materials.\n', '  Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween multi-modal knowledge graphs (MMKGs), where the entities can be\nassociated with related images. Most existing studies integrate multi-modal\ninformation heavily relying on the automatically-learned fusion module, rarely\nsuppressing the redundant information for MMEA explicitly. To this end, we\nexplore variational information bottleneck for multi-modal entity alignment\n(IBMEA), which emphasizes the alignment-relevant information and suppresses the\nalignment-irrelevant information in generating entity representations.\nSpecifically, we devise multi-modal variational encoders to generate\nmodal-specific entity representations as probability distributions. Then, we\npropose four modal-specific information bottleneck regularizers, limiting the\nmisleading clues in refining modal-specific entity representations. Finally, we\npropose a modal-hybrid information contrastive regularizer to integrate all the\nrefined modal-specific representations, enhancing the entity similarity between\nMMKGs to achieve MMEA. We conduct extensive experiments on two cross-KG and\nthree bilingual MMEA datasets. Experimental results demonstrate that our model\nconsistently outperforms previous state-of-the-art methods, and also shows\npromising and robust performance in low-resource and high-noise data scenarios.\n', '  Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween two multi-modal knowledge graphs (MMKGs), whose entities can be\nassociated with relational triples and related images. Most previous studies\ntreat the graph structure as a special modality, and fuse different modality\ninformation with separate uni-modal encoders, neglecting valuable relational\nassociations in modalities. Other studies refine each uni-modal information\nwith graph structures, but may introduce unnecessary relations in specific\nmodalities. To this end, we propose a novel local-to-global interaction network\nfor MMEA, termed as LoginMEA. Particularly, we first fuse local multi-modal\ninteractions to generate holistic entity semantics and then refine them with\nglobal relational interactions of entity neighbors. In this design, the\nuni-modal information is fused adaptively, and can be refined with relations\naccordingly. To enrich local interactions of multi-modal entity information, we\ndevice modality weights and low-rank interactive fusion, allowing diverse\nimpacts and element-level interactions among modalities. To capture global\ninteractions of graph structures, we adopt relation reflection graph attention\nnetworks, which fully capture relational associations between entities.\nExtensive experiments demonstrate superior results of our method over 5\ncross-KG or bilingual benchmark datasets, indicating the effectiveness of\ncapturing local and global interactions.\n']",Multi-Modal Entity Alignment,Entity Alignment and Ontology Learning with Semantic Technologies,Entity Understanding and Semantic Knowledge Integration,Entity Understanding and Semantic Knowledge Integration
381,18,381_videos_filmmaking_animations_animation,"['videos', 'filmmaking', 'animations', 'animation', 'motions', 'motion', '3d', 'camera', 'generative', 'mofa_video']","['motion', 'video', 'camera', 'animation', 'pose', 'videos', 'timeline', 'animations', 'character', 'object']","['  We present MOFA-Video, an advanced controllable image animation method that\ngenerates video from the given image using various additional controllable\nsignals (such as human landmarks reference, manual trajectories, and another\neven provided video) or their combinations. This is different from previous\nmethods which only can work on a specific motion domain or show weak control\nabilities with diffusion prior. To achieve our goal, we design several\ndomain-aware motion field adapters (\\ie, MOFA-Adapters) to control the\ngenerated motions in the video generation pipeline. For MOFA-Adapters, we\nconsider the temporal motion consistency of the video and generate the dense\nmotion flow from the given sparse control conditions first, and then, the\nmulti-scale features of the given image are wrapped as a guided feature for\nstable video diffusion generation. We naively train two motion adapters for the\nmanual trajectories and the human landmarks individually since they both\ncontain sparse information about the control. After training, the MOFA-Adapters\nin different domains can also work together for more controllable video\ngeneration. Project Page: https://myniuuu.github.io/MOFA_Video/\n', ""  Filmmaking and animation production often require sophisticated techniques\nfor coordinating camera transitions and object movements, typically involving\nlabor-intensive real-world capturing. Despite advancements in generative AI for\nvideo creation, achieving precise control over motion for interactive video\nasset generation remains challenging. To this end, we propose Image Conductor,\na method for precise control of camera transitions and object movements to\ngenerate video assets from a single image. An well-cultivated training strategy\nis proposed to separate distinct camera and object motion by camera LoRA\nweights and object LoRA weights. To further address cinematographic variations\nfrom ill-posed trajectories, we introduce a camera-free guidance technique\nduring inference, enhancing object movements while eliminating camera\ntransitions. Additionally, we develop a trajectory-oriented video motion data\ncuration pipeline for training. Quantitative and qualitative experiments\ndemonstrate our method's precision and fine-grained control in generating\nmotion-controllable videos from images, advancing the practical application of\ninteractive video synthesis. Project webpage available at\nhttps://liyaowei-stu.github.io/project/ImageConductor/\n"", '  Human image animation involves generating videos from a character photo,\nallowing user control and unlocking potential for video and movie production.\nWhile recent approaches yield impressive results using high-quality training\ndata, the inaccessibility of these datasets hampers fair and transparent\nbenchmarking. Moreover, these approaches prioritize 2D human motion and\noverlook the significance of camera motions in videos, leading to limited\ncontrol and unstable video generation. To demystify the training data, we\npresent HumanVid, the first large-scale high-quality dataset tailored for human\nimage animation, which combines crafted real-world and synthetic data. For the\nreal-world data, we compile a vast collection of copyright-free real-world\nvideos from the internet. Through a carefully designed rule-based filtering\nstrategy, we ensure the inclusion of high-quality videos, resulting in a\ncollection of 20K human-centric videos in 1080P resolution. Human and camera\nmotion annotation is accomplished using a 2D pose estimator and a SLAM-based\nmethod. For the synthetic data, we gather 2,300 copyright-free 3D avatar assets\nto augment existing available 3D assets. Notably, we introduce a rule-based\ncamera trajectory generation method, enabling the synthetic pipeline to\nincorporate diverse and precise camera motion annotation, which can rarely be\nfound in real-world data. To verify the effectiveness of HumanVid, we establish\na baseline model named CamAnimate, short for Camera-controllable Human\nAnimation, that considers both human and camera motions as conditions. Through\nextensive experimentation, we demonstrate that such simple baseline training on\nour HumanVid achieves state-of-the-art performance in controlling both human\npose and camera motions, setting a new benchmark. Code and data will be\npublicly available at https://github.com/zhenzhiwang/HumanVid/.\n']",Controllable Video Animation,Video and Image Processing Techniques,Image and Video Processing,Image and Video Processing
382,18,382_neural_fmri_brains_brain,"['neural', 'fmri', 'brains', 'brain', 'cognitive', 'language', 'neuroscience', 'representations', 'brainscore', 'iq']","['brain', 'cognitive', 'processing', 'alignment', 'language', 'untrained', 'similarity', 'encoding', 'activity', 'listening']","['  Given the remarkable capabilities of large language models (LLMs), there has\nbeen a growing interest in evaluating their similarity to the human brain. One\napproach towards quantifying this similarity is by measuring how well a model\npredicts neural signals, also called ""brain score"". Internal representations\nfrom LLMs achieve state-of-the-art brain scores, leading to speculation that\nthey share computational principles with human language processing. This\ninference is only valid if the subset of neural activity predicted by LLMs\nreflects core elements of language processing. Here, we question this\nassumption by analyzing three neural datasets used in an impactful study on\nLLM-to-brain mappings, with a particular focus on an fMRI dataset where\nparticipants read short passages. We first find that when using shuffled\ntrain-test splits, as done in previous studies with these datasets, a trivial\nfeature that encodes temporal autocorrelation not only outperforms LLMs but\nalso accounts for the majority of neural variance that LLMs explain. We\ntherefore use contiguous splits moving forward. Second, we explain the\nsurprisingly high brain scores of untrained LLMs by showing they do not account\nfor additional neural variance beyond two simple features: sentence length and\nsentence position. This undermines evidence used to claim that the transformer\narchitecture biases computations to be more brain-like. Third, we find that\nbrain scores of trained LLMs on this dataset can largely be explained by\nsentence length, position, and pronoun-dereferenced static word embeddings; a\nsmall, additional amount is explained by sense-specific embeddings and\ncontextual representations of sentence structure. We conclude that\nover-reliance on brain scores can lead to over-interpretations of similarity\nbetween LLMs and brains, and emphasize the importance of deconstructing what\nLLMs are mapping to in neural signals.\n', ""  Large Language Models (LLMs) have been shown to be effective models of the\nhuman language system, with some models predicting most explainable variance of\nbrain activity in current datasets. Even in untrained models, the\nrepresentations induced by architectural priors can exhibit reasonable\nalignment to brain data. In this work, we investigate the key architectural\ncomponents driving the surprising alignment of untrained models. To estimate\nLLM-to-brain similarity, we first select language-selective units within an\nLLM, similar to how neuroscientists identify the language network in the human\nbrain. We then benchmark the brain alignment of these LLM units across five\ndifferent brain recording datasets. By isolating critical components of the\nTransformer architecture, we identify tokenization strategy and multihead\nattention as the two major components driving brain alignment. A simple form of\nrecurrence further improves alignment. We further demonstrate this quantitative\nbrain alignment of our model by reproducing landmark studies in the language\nneuroscience field, showing that localized model units -- just like language\nvoxels measured empirically in the human brain -- discriminate more reliably\nbetween lexical than syntactic differences, and exhibit similar response\nprofiles under the same experimental conditions. Finally, we demonstrate the\nutility of our model's representations for language modeling, achieving\nimproved sample and parameter efficiency over comparable architectures. Our\nmodel's estimates of surprisal sets a new state-of-the-art in the behavioral\nalignment to human reading times. Taken together, we propose a highly brain-\nand behaviorally-aligned model that conceptualizes the human language system as\nan untrained shallow feature encoder, with structural priors, combined with a\ntrained decoder to achieve efficient and performant language processing.\n"", '  Large Language Models (LLMs) have demonstrated remarkable abilities in text\ncomprehension and logical reasoning, indicating that the text representations\nlearned by LLMs can facilitate their language processing capabilities. In\ncognitive science, brain cognitive processing signals are typically utilized to\nstudy human language processing. Therefore, it is natural to ask how well the\ntext embeddings from LLMs align with the brain cognitive processing signals,\nand how training strategies affect the LLM-brain alignment? In this paper, we\nemploy Representational Similarity Analysis (RSA) to measure the alignment\nbetween 23 mainstream LLMs and fMRI signals of the brain to evaluate how\neffectively LLMs simulate cognitive language processing. We empirically\ninvestigate the impact of various factors (e.g., pre-training data size, model\nscaling, alignment training, and prompts) on such LLM-brain alignment.\nExperimental results indicate that pre-training data size and model scaling are\npositively correlated with LLM-brain similarity, and alignment training can\nsignificantly improve LLM-brain similarity. Explicit prompts contribute to the\nconsistency of LLMs with brain cognitive language processing, while nonsensical\nnoisy prompts may attenuate such alignment. Additionally, the performance of a\nwide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated\nwith the LLM-brain similarity.\n']",LLMs and Brain Similarity in Language Processing,Comparing Large Language Models to Human Brain Function in Language Processing,Large Language Models,Large Language Models
382,18,382_neural_fmri_brains_brain,"['neural', 'fmri', 'brains', 'brain', 'cognitive', 'language', 'neuroscience', 'representations', 'brainscore', 'iq']","['brain', 'cognitive', 'processing', 'alignment', 'language', 'untrained', 'similarity', 'encoding', 'activity', 'listening']","['  Given the remarkable capabilities of large language models (LLMs), there has\nbeen a growing interest in evaluating their similarity to the human brain. One\napproach towards quantifying this similarity is by measuring how well a model\npredicts neural signals, also called ""brain score"". Internal representations\nfrom LLMs achieve state-of-the-art brain scores, leading to speculation that\nthey share computational principles with human language processing. This\ninference is only valid if the subset of neural activity predicted by LLMs\nreflects core elements of language processing. Here, we question this\nassumption by analyzing three neural datasets used in an impactful study on\nLLM-to-brain mappings, with a particular focus on an fMRI dataset where\nparticipants read short passages. We first find that when using shuffled\ntrain-test splits, as done in previous studies with these datasets, a trivial\nfeature that encodes temporal autocorrelation not only outperforms LLMs but\nalso accounts for the majority of neural variance that LLMs explain. We\ntherefore use contiguous splits moving forward. Second, we explain the\nsurprisingly high brain scores of untrained LLMs by showing they do not account\nfor additional neural variance beyond two simple features: sentence length and\nsentence position. This undermines evidence used to claim that the transformer\narchitecture biases computations to be more brain-like. Third, we find that\nbrain scores of trained LLMs on this dataset can largely be explained by\nsentence length, position, and pronoun-dereferenced static word embeddings; a\nsmall, additional amount is explained by sense-specific embeddings and\ncontextual representations of sentence structure. We conclude that\nover-reliance on brain scores can lead to over-interpretations of similarity\nbetween LLMs and brains, and emphasize the importance of deconstructing what\nLLMs are mapping to in neural signals.\n', ""  Large Language Models (LLMs) have been shown to be effective models of the\nhuman language system, with some models predicting most explainable variance of\nbrain activity in current datasets. Even in untrained models, the\nrepresentations induced by architectural priors can exhibit reasonable\nalignment to brain data. In this work, we investigate the key architectural\ncomponents driving the surprising alignment of untrained models. To estimate\nLLM-to-brain similarity, we first select language-selective units within an\nLLM, similar to how neuroscientists identify the language network in the human\nbrain. We then benchmark the brain alignment of these LLM units across five\ndifferent brain recording datasets. By isolating critical components of the\nTransformer architecture, we identify tokenization strategy and multihead\nattention as the two major components driving brain alignment. A simple form of\nrecurrence further improves alignment. We further demonstrate this quantitative\nbrain alignment of our model by reproducing landmark studies in the language\nneuroscience field, showing that localized model units -- just like language\nvoxels measured empirically in the human brain -- discriminate more reliably\nbetween lexical than syntactic differences, and exhibit similar response\nprofiles under the same experimental conditions. Finally, we demonstrate the\nutility of our model's representations for language modeling, achieving\nimproved sample and parameter efficiency over comparable architectures. Our\nmodel's estimates of surprisal sets a new state-of-the-art in the behavioral\nalignment to human reading times. Taken together, we propose a highly brain-\nand behaviorally-aligned model that conceptualizes the human language system as\nan untrained shallow feature encoder, with structural priors, combined with a\ntrained decoder to achieve efficient and performant language processing.\n"", '  Large Language Models (LLMs) have demonstrated remarkable abilities in text\ncomprehension and logical reasoning, indicating that the text representations\nlearned by LLMs can facilitate their language processing capabilities. In\ncognitive science, brain cognitive processing signals are typically utilized to\nstudy human language processing. Therefore, it is natural to ask how well the\ntext embeddings from LLMs align with the brain cognitive processing signals,\nand how training strategies affect the LLM-brain alignment? In this paper, we\nemploy Representational Similarity Analysis (RSA) to measure the alignment\nbetween 23 mainstream LLMs and fMRI signals of the brain to evaluate how\neffectively LLMs simulate cognitive language processing. We empirically\ninvestigate the impact of various factors (e.g., pre-training data size, model\nscaling, alignment training, and prompts) on such LLM-brain alignment.\nExperimental results indicate that pre-training data size and model scaling are\npositively correlated with LLM-brain similarity, and alignment training can\nsignificantly improve LLM-brain similarity. Explicit prompts contribute to the\nconsistency of LLMs with brain cognitive language processing, while nonsensical\nnoisy prompts may attenuate such alignment. Additionally, the performance of a\nwide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated\nwith the LLM-brain similarity.\n']",LLMs and Brain Similarity in Language Processing,Comparing Large Language Models to Human Brain Function in Language Processing,Large Language Models,Large Language Models
383,18,383_incidents_automation_automated_monitoring,"['incidents', 'automation', 'automated', 'monitoring', 'faultprofit', 'incident', 'alerts', 'maintenance', 'microservices', 'faults']","['incident', 'root', 'incidents', 'alerts', 'cloud', 'microservice', 'cause', 'fault', 'microservices', 'service']","[""  Despite significant reliability efforts, large-scale cloud services\ninevitably experience production incidents that can significantly impact\nservice availability and customer's satisfaction. Worse, in many cases one\nincident can lead to multiple downstream failures due to cascading effects that\ncreates several related incidents across different dependent services. Often\ntime On-call Engineers (OCEs) examine these incidents in silos that lead to\nsignificant amount of manual toil and increase the overall time-to-mitigate\nincidents. Therefore, developing efficient incident linking models is of\nparamount importance for grouping related incidents into clusters so as to\nquickly resolve major outages and reduce on-call fatigue. Existing incident\nlinking methods mostly leverages textual and contextual information of\nincidents (e.g., title, description, severity, impacted components), thus\nfailing to leverage the inter-dependencies between services. In this paper, we\npropose the dependency-aware incident linking (DiLink) framework which\nleverages both textual and service dependency graph information to improve the\naccuracy and coverage of incident links not only coming from same service, but\nalso from different services and workloads. Furthermore, we propose a novel\nmethod to align the embeddings of multi-modal (i.e., textual and graphical)\ndata using Orthogonal Procrustes. Extensive experimental results on real-world\nincidents from 5 workloads of Microsoft demonstrate that our alignment method\nhas an F1-score of 0.96 (14% gain over current state-of-the-art methods). We\nare also in the process of deploying this solution across 610 services from\nthese 5 workloads for continuously supporting OCEs improving incident\nmanagement and reducing manual toil.\n"", ""  Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis\nprocess for cloud services, requiring on-call engineers to identify the primary\nissues and implement corrective actions to prevent future recurrences.\nImproving the incident RCA process is vital for minimizing service downtime,\ncustomer impact and manual toil. Recent advances in artificial intelligence\nhave introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which\nhave proven effective in tackling various AIOps problems, ranging from code\nauthoring to incident management. Nonetheless, the GPT-4 model's immense size\npresents challenges when trying to fine-tune it on user data because of the\nsignificant GPU resource demand and the necessity for continuous model\nfine-tuning with the emergence of new data. To address the high cost of\nfine-tuning LLM, we propose an in-context learning approach for automated root\ncausing, which eliminates the need for fine-tuning. We conduct extensive study\nover 100,000 production incidents, comparing several large language models\nusing multiple metrics. The results reveal that our in-context learning\napproach outperforms the previous fine-tuned large language models such as\nGPT-3 by an average of 24.8\\% across all metrics, with an impressive 49.7\\%\nimprovement over the zero-shot model. Moreover, human evaluation involving\nactual incident owners demonstrates its superiority over the fine-tuned model,\nachieving a 43.5\\% improvement in correctness and an 8.7\\% enhancement in\nreadability. The impressive results demonstrate the viability of utilizing a\nvanilla GPT model for the RCA task, thereby avoiding the high computational and\nmaintenance costs associated with a fine-tuned model.\n"", ""  The growing complexity of cloud based software systems has resulted in\nincident management becoming an integral part of the software development\nlifecycle. Root cause analysis (RCA), a critical part of the incident\nmanagement process, is a demanding task for on-call engineers, requiring deep\ndomain knowledge and extensive experience with a team's specific services.\nAutomation of RCA can result in significant savings of time, and ease the\nburden of incident management on on-call engineers. Recently, researchers have\nutilized Large Language Models (LLMs) to perform RCA, and have demonstrated\npromising results. However, these approaches are not able to dynamically\ncollect additional diagnostic information such as incident related logs,\nmetrics or databases, severely restricting their ability to diagnose root\ncauses. In this work, we explore the use of LLM based agents for RCA to address\nthis limitation. We present a thorough empirical evaluation of a ReAct agent\nequipped with retrieval tools, on an out-of-distribution dataset of production\nincidents collected at Microsoft. Results show that ReAct performs\ncompetitively with strong retrieval and reasoning baselines, but with highly\nincreased factual accuracy. We then extend this evaluation by incorporating\ndiscussions associated with incident reports as additional inputs for the\nmodels, which surprisingly does not yield significant performance improvements.\nLastly, we conduct a case study with a team at Microsoft to equip the ReAct\nagent with tools that give it access to external diagnostic services that are\nused by the team for manual RCA. Our results show how agents can overcome the\nlimitations of prior work, and practical considerations for implementing such a\nsystem in practice.\n""]",Automated Incident Management and Root Cause Analysis,Disaster Management and Response using AI and Data Analytics,AI and Data-Driven Approaches for Urban and Disaster Management,AI and Data-Driven Approaches for Urban and Disaster Management
384,18,384_games_strategy_strategic_game,"['games', 'strategy', 'strategic', 'game', 'equilibria', 'optimality', 'information', 'subgames', 'equilibrium', 'subgame']","['games', 'player', 'equilibrium', 'game', 'equilibria', 'strategic', 'players', 'agents', 'observable', 'perfect']","['  We study zero-sum differential games with state constraints and one-sided\ninformation, where the informed player (Player 1) has a categorical payoff type\nunknown to the uninformed player (Player 2). The goal of Player 1 is to\nminimize his payoff without violating the constraints, while that of Player 2\nis to violate the state constraints if possible, or to maximize the payoff\notherwise. One example of the game is a man-to-man matchup in football. Without\nstate constraints, Cardaliaguet (2007) showed that the value of such a game\nexists and is convex to the common belief of players. Our theoretical\ncontribution is an extension of this result to games with state constraints and\nthe derivation of the primal and dual subdynamic principles necessary for\ncomputing behavioral strategies. Different from existing works that are\nconcerned about the scalability of no-regret learning in games with discrete\ndynamics, our study reveals the underlying structure of strategies for belief\nmanipulation resulting from information asymmetry and state constraints. This\nstructure will be necessary for scalable learning on games with continuous\nactions and long time windows. We use a simplified football game to demonstrate\nthe utility of this work, where we reveal player positions and belief states in\nwhich the attacker should (or should not) play specific random deceptive moves\nto take advantage of information asymmetry, and compute how the defender should\nrespond.\n', '  While Nash equilibrium has emerged as the central game-theoretic solution\nconcept, many important games contain several Nash equilibria and we must\ndetermine how to select between them in order to create real strategic agents.\nSeveral Nash equilibrium refinement concepts have been proposed and studied for\nsequential imperfect-information games, the most prominent being trembling-hand\nperfect equilibrium, quasi-perfect equilibrium, and recently one-sided\nquasi-perfect equilibrium. These concepts are robust to certain arbitrarily\nsmall mistakes, and are guaranteed to always exist; however, we argue that\nneither of these is the correct concept for developing strong agents in\nsequential games of imperfect information. We define a new equilibrium\nrefinement concept for extensive-form games called observable perfect\nequilibrium in which the solution is robust over trembles in\npublicly-observable action probabilities (not necessarily over all action\nprobabilities that may not be observable by opposing players). Observable\nperfect equilibrium correctly captures the assumption that the opponent is\nplaying as rationally as possible given mistakes that have been observed (while\nprevious solution concepts do not). We prove that observable perfect\nequilibrium is always guaranteed to exist, and demonstrate that it leads to a\ndifferent solution than the prior extensive-form refinements in no-limit poker.\nWe expect observable perfect equilibrium to be a useful equilibrium refinement\nconcept for modeling many important imperfect-information games of interest in\nartificial intelligence.\n', ""  We introduce Boolean Observation Games, a subclass of multi-player finite\nstrategic games with incomplete information and qualitative objectives. In\nBoolean observation games, each player is associated with a finite set of\npropositional variables of which only it can observe the value, and it controls\nwhether and to whom it can reveal that value. It does not control the given,\nfixed, value of variables. Boolean observation games are a generalization of\nBoolean games, a well-studied subclass of strategic games but with complete\ninformation, and wherein each player controls the value of its variables.\n  In Boolean observation games, player goals describe multi-agent knowledge of\nvariables. As in classical strategic games, players choose their strategies\nsimultaneously and therefore observation games capture aspects of both\nimperfect and incomplete information. They require reasoning about sets of\noutcomes given sets of indistinguishable valuations of variables. An outcome\nrelation between such sets determines what the Nash equilibria are. We present\nvarious outcome relations, including a qualitative variant of ex-post\nequilibrium. We identify conditions under which, given an outcome relation,\nNash equilibria are guaranteed to exist. We also study the complexity of\nchecking for the existence of Nash equilibria and of verifying if a strategy\nprofile is a Nash equilibrium. We further study the subclass of Boolean\nobservation games with `knowing whether' goal formulas, for which the\nsatisfaction does not depend on the value of variables. We show that each such\nBoolean observation game corresponds to a Boolean game and vice versa, by a\ndifferent correspondence, and that both correspondences are precise in terms of\nexistence of Nash equilibria.\n""]",Game Theory and Strategic Decision Making,Game Theory and Strategic Decision Making,Decision Making and Optimization under Uncertainty,Decision Making and Optimization under Uncertainty
385,18,385_bots_botnet_bot_botartist,"['bots', 'botnet', 'bot', 'botartist', 'adversarial', 'sebot', 'twibot', 'botsscl', 'tweets', 'twitter']","['bot', 'bots', 'social', 'accounts', 'detection', 'media', 'content', 'disinformation', 'profile', 'profiles']","[""  Recent advancements in social bot detection have been driven by the adoption\nof Graph Neural Networks. The social graph, constructed from social network\ninteractions, contains benign and bot accounts that influence each other.\nHowever, previous graph-based detection methods that follow the transductive\nmessage-passing paradigm may not fully utilize hidden graph information and are\nvulnerable to adversarial bot behavior. The indiscriminate message passing\nbetween nodes from different categories and communities results in excessively\nhomogeneous node representations, ultimately reducing the effectiveness of\nsocial bot detectors. In this paper, we propose SEBot, a novel multi-view\ngraph-based contrastive learning-enabled social bot detector. In particular, we\nuse structural entropy as an uncertainty metric to optimize the entire graph's\nstructure and subgraph-level granularity, revealing the implicitly existing\nhierarchical community structure. And we design an encoder to enable message\npassing beyond the homophily assumption, enhancing robustness to adversarial\nbehaviors of social bots. Finally, we employ multi-view contrastive learning to\nmaximize mutual information between different views and enhance the detection\nperformance through multi-task learning. Experimental results demonstrate that\nour approach significantly improves the performance of social bot detection\ncompared with SOTA methods.\n"", '  Social bots remain a major vector for spreading disinformation on social\nmedia and a menace to the public. Despite the progress made in developing\nmultiple sophisticated social bot detection algorithms and tools, bot detection\nremains a challenging, unsolved problem that is fraught with uncertainty due to\nthe heterogeneity of bot behaviors, training data, and detection algorithms.\nDetection models often disagree on whether to label the same account as bot or\nhuman-controlled. However, they do not provide any measure of uncertainty to\nindicate how much we should trust their results. We propose to address both bot\ndetection and the quantification of uncertainty at the account level - a novel\nfeature of this research. This dual focus is crucial as it allows us to\nleverage additional information related to the quantified uncertainty of each\nprediction, thereby enhancing decision-making and improving the reliability of\nbot classifications. Specifically, our approach facilitates targeted\ninterventions for bots when predictions are made with high confidence and\nsuggests caution (e.g., gathering more data) when predictions are uncertain.\n', '  Social bots play a significant role in many online social networks (OSN) as\nthey imitate human behavior. This fact raises difficult questions about their\ncapabilities and potential risks. Given the recent advances in Generative AI\n(GenAI), social bots are capable of producing highly realistic and complex\ncontent that mimics human creativity. As the malicious social bots emerge to\ndeceive people with their unrealistic content, identifying them and\ndistinguishing the content they produce has become an actual challenge for\nnumerous social platforms. Several approaches to this problem have already been\nproposed in the literature, but the proposed solutions have not been widely\nevaluated. To address this issue, we evaluate the behavior of a text-based bot\ndetector in a competitive environment where some scenarios are proposed:\n\\textit{First}, the tug-of-war between a bot and a bot detector is examined. It\nis interesting to analyze which party is more likely to prevail and which\ncircumstances influence these expectations. In this regard, we model the\nproblem as a synthetic adversarial game in which a conversational bot and a bot\ndetector are engaged in strategic online interactions. \\textit{Second}, the bot\ndetection model is evaluated under attack examples generated by a social bot;\nto this end, we poison the dataset with attack examples and evaluate the model\nperformance under this condition. \\textit{Finally}, to investigate the impact\nof the dataset, a cross-domain analysis is performed. Through our comprehensive\nevaluation of different categories of social bots using two benchmark datasets,\nwe were able to demonstrate some achivement that could be utilized in future\nworks.\n']",Social Bot Detection and Analysis,Social Media Misbehavior Detection and Analysis,Misbehavior and Toxicity in Online Content,Misbehavior and Toxicity in Online Content
386,18,386_estimators_estimate_modelic_controllers,"['estimators', 'estimate', 'modelic', 'controllers', 'lasso', 'identification', 'systems', 'complexity', 'identifiability', 'controller']","['identification', 'asymptotic', 'linear', 'datatic', 'matrices', 'finite', 'switching', 'identifiability', 'squares', 'sample']","[""  Machine Learning (ML) and linear System Identification (SI) have been\nhistorically developed independently. In this paper, we leverage\nwell-established ML tools - especially the automatic differentiation framework\n- to introduce SIMBa, a family of discrete linear multi-step-ahead state-space\nSI methods using backpropagation. SIMBa relies on a novel\nLinear-Matrix-Inequality-based free parametrization of Schur matrices to ensure\nthe stability of the identified model.\n  We show how SIMBa generally outperforms traditional linear state-space SI\nmethods, and sometimes significantly, although at the price of a higher\ncomputational burden. This performance gap is particularly remarkable compared\nto other SI methods with stability guarantees, where the gain is frequently\nabove 25% in our investigations, hinting at SIMBa's ability to simultaneously\nachieve state-of-the-art fitting performance and enforce stability.\nInterestingly, these observations hold for a wide variety of input-output\nsystems and on both simulated and real-world data, showcasing the flexibility\nof the proposed approach. We postulate that this new SI paradigm presents a\ngreat extension potential to identify structured nonlinear models from data,\nand we hence open-source SIMBa on https://github.com/Cemempamoi/simba.\n"", '  The focus of this paper is on linear system identification in the setting\nwhere it is known that the underlying partially-observed linear dynamical\nsystem lies within a finite collection of known candidate models. We first\nconsider the problem of identification from a given trajectory, which in this\nsetting reduces to identifying the index of the true model with high\nprobability. We characterize the finite-time sample complexity of this problem\nby leveraging recent advances in the non-asymptotic analysis of linear\nleast-square methods in the literature. In comparison to the earlier results\nthat assume no prior knowledge of the system, our approach takes advantage of\nthe smaller hypothesis class and leads to the design of a learner with a\ndimension-free sample complexity bound. Next, we consider the switching control\nof linear systems, where there is a candidate controller for each of the\ncandidate models and data is collected through interaction of the system with a\ncollection of potentially destabilizing controllers. We develop a\ndimension-dependent criterion that can detect those destabilizing controllers\nin finite time. By leveraging these results, we propose a data-driven switching\nstrategy that identifies the unknown parameters of the underlying system. We\nthen provide a non-asymptotic analysis of its performance and discuss its\nimplications on the classical method of estimator-based supervisory control.\n', '  We study non-parametric frequency-domain system identification from a\nfinite-sample perspective. We assume an open loop scenario where the excitation\ninput is periodic and consider the Empirical Transfer Function Estimate (ETFE),\nwhere the goal is to estimate the frequency response at certain desired\n(evenly-spaced) frequencies, given input-output samples. We show that under\nsub-Gaussian colored noise (in time-domain) and stability assumptions, the ETFE\nestimates are concentrated around the true values. The error rate is of the\norder of\n$\\mathcal{O}((d_{\\mathrm{u}}+\\sqrt{d_{\\mathrm{u}}d_{\\mathrm{y}}})\\sqrt{M/N_{\\mathrm{tot}}})$,\nwhere $N_{\\mathrm{tot}}$ is the total number of samples, $M$ is the number of\ndesired frequencies, and $d_{\\mathrm{u}},\\,d_{\\mathrm{y}}$ are the dimensions\nof the input and output signals respectively. This rate remains valid for\ngeneral irrational transfer functions and does not require a finite order\nstate-space representation. By tuning $M$, we obtain a\n$N_{\\mathrm{tot}}^{-1/3}$ finite-sample rate for learning the frequency\nresponse over all frequencies in the $ \\mathcal{H}_{\\infty}$ norm. Our result\ndraws upon an extension of the Hanson-Wright inequality to semi-infinite\nmatrices. We study the finite-sample behavior of ETFE in simulations.\n']",System Identification and Estimation Methods,State Estimation and System Identification,Industrial Automation and Control Systems,Industrial Automation and Control Systems
387,18,387_clustering_softmax_predictions_cluster_online_hard_clustering_clustering,"['clustering_softmax_predictions', 'cluster', 'online_hard_clustering', 'clustering', 'clusters', 'supervised', 'embedding', 'deep', 'unsupervised', 'autoencoder']","['clustering', 'cluster', 'deep', 'assignments', 'tabular', 'autoencoder', 'clusters', 'feature', 'unsupervised', 'simplex']","['  Distribution learning finds probability density functions from a set of data\nsamples, whereas clustering aims to group similar data points to form clusters.\nAlthough there are deep clustering methods that employ distribution learning\nmethods, past work still lacks theoretical analysis regarding the relationship\nbetween clustering and distribution learning. Thus, in this work, we provide a\ntheoretical analysis to guide the optimization of clustering via distribution\nlearning. To achieve better results, we embed deep clustering guided by a\ntheoretical analysis. Furthermore, the distribution learning method cannot\nalways be directly applied to data. To overcome this issue, we introduce a\nclustering-oriented distribution learning method called Monte-Carlo\nMarginalization for Clustering. We integrate Monte-Carlo Marginalization for\nClustering into Deep Clustering, resulting in Deep Clustering via Distribution\nLearning (DCDL). Eventually, the proposed DCDL achieves promising results\ncompared to state-of-the-art methods on popular datasets. Considering a\nclustering task, the new distribution learning method outperforms previous\nmethods as well.\n', '  Deep clustering methods improve the performance of clustering tasks by\njointly optimizing deep representation learning and clustering. While numerous\ndeep clustering algorithms have been proposed, most of them rely on\nartificially constructed pseudo targets for performing clustering. This\nconstruction process requires some prior knowledge, and it is challenging to\ndetermine a suitable pseudo target for clustering. To address this issue, we\npropose a deep embedding clustering algorithm driven by sample stability\n(DECS), which eliminates the requirement of pseudo targets. Specifically, we\nstart by constructing the initial feature space with an autoencoder and then\nlearn the cluster-oriented embedding feature constrained by sample stability.\nThe sample stability aims to explore the deterministic relationship between\nsamples and all cluster centroids, pulling samples to their respective clusters\nand keeping them away from other clusters with high determinacy. We analyzed\nthe convergence of the loss using Lipschitz continuity in theory, which\nverifies the validity of the model. The experimental results on five datasets\nillustrate that the proposed method achieves superior performance compared to\nstate-of-the-art clustering approaches.\n', '  In the face of complex natural images, existing deep clustering algorithms\nfall significantly short in terms of clustering accuracy when compared to\nsupervised classification methods, making them less practical. This paper\nintroduces an image clustering algorithm based on self-supervised pretrained\nmodels and latent feature distribution optimization, substantially enhancing\nclustering performance. It is found that: (1) For complex natural images, we\neffectively enhance the discriminative power of latent features by leveraging\nself-supervised pretrained models and their fine-tuning, resulting in improved\nclustering performance. (2) In the latent feature space, by searching for\nk-nearest neighbor images for each training sample and shortening the distance\nbetween the training sample and its nearest neighbor, the discriminative power\nof latent features can be further enhanced, and clustering performance can be\nimproved. (3) In the latent feature space, reducing the distance between sample\nfeatures and the nearest predefined cluster centroids can optimize the\ndistribution of latent features, therefore further improving clustering\nperformance. Through experiments on multiple datasets, our approach outperforms\nthe latest clustering algorithms and achieves state-of-the-art clustering\nresults. When the number of categories in the datasets is small, such as\nCIFAR-10 and STL-10, and there are significant differences between categories,\nour clustering algorithm has similar accuracy to supervised methods without\nusing pretrained models, slightly lower than supervised methods using\npre-trained models. The code linked algorithm is\nhttps://github.com/LihengHu/semi.\n']",Deep Clustering Methods and Distribution Learning,Deep Learning for Out-of-Distribution Detection and Robustness,Deep Learning Optimization and Security,Deep Learning Methodologies
388,18,388_intrusions_attacks_intrusion_security,"['intrusions', 'attacks', 'intrusion', 'security', 'defenders', 'reinforcement', 'defence', 'defense', 'attack', 'attacker']","['defender', 'attacker', 'intrusion', 'cyber', 'infrastructure', 'attack', 'strategies', 'defense', 'defenders', 'game']","[""  This paper addresses a significant gap in Autonomous Cyber Operations (ACO)\nliterature: the absence of effective edge-blocking ACO strategies in dynamic,\nreal-world networks. It specifically targets the cybersecurity vulnerabilities\nof organizational Active Directory (AD) systems. Unlike the existing literature\non edge-blocking defenses which considers AD systems as static entities, our\nstudy counters this by recognizing their dynamic nature and developing advanced\nedge-blocking defenses through a Stackelberg game model between attacker and\ndefender. We devise a Reinforcement Learning (RL)-based attack strategy and an\nRL-assisted Evolutionary Diversity Optimization-based defense strategy, where\nthe attacker and defender improve each other strategy via parallel gameplay. To\naddress the computational challenges of training attacker-defender strategies\non numerous dynamic AD graphs, we propose an RL Training Facilitator that\nprunes environments and neural networks to eliminate irrelevant elements,\nenabling efficient and scalable training for large graphs. We extensively train\nthe attacker strategy, as a sophisticated attacker model is essential for a\nrobust defense. Our empirical results successfully demonstrate that our\nproposed approach enhances defender's proficiency in hardening dynamic AD\ngraphs while ensuring scalability for large-scale AD.\n"", '  We study automated intrusion prevention using reinforcement learning.\nFollowing a novel approach, we formulate the problem of intrusion prevention as\nan (optimal) multiple stopping problem. This formulation gives us insight into\nthe structure of optimal policies, which we show to have threshold properties.\nFor most practical cases, it is not feasible to obtain an optimal defender\npolicy using dynamic programming. We therefore develop a reinforcement learning\napproach to approximate an optimal threshold policy. We introduce T-SPSA, an\nefficient reinforcement learning algorithm that learns threshold policies\nthrough stochastic approximation. We show that T-SPSA outperforms\nstate-of-the-art algorithms for our use case. Our overall method for learning\nand validating policies includes two systems: a simulation system where\ndefender policies are incrementally learned and an emulation system where\nstatistics are produced that drive simulation runs and where learned policies\nare evaluated. We show that this approach can produce effective defender\npolicies for a practical IT infrastructure.\n', '  We study automated intrusion response and formulate the interaction between\nan attacker and a defender as an optimal stopping game where attack and defense\nstrategies evolve through reinforcement learning and self-play. The\ngame-theoretic modeling enables us to find defender strategies that are\neffective against a dynamic attacker, i.e. an attacker that adapts its strategy\nin response to the defender strategy. Further, the optimal stopping formulation\nallows us to prove that optimal strategies have threshold properties. To obtain\nnear-optimal defender strategies, we develop Threshold Fictitious Self-Play\n(T-FP), a fictitious self-play algorithm that learns Nash equilibria through\nstochastic approximation. We show that T-FP outperforms a state-of-the-art\nalgorithm for our use case. The experimental part of this investigation\nincludes two systems: a simulation system where defender strategies are\nincrementally learned and an emulation system where statistics are collected\nthat drive simulation runs and where learned strategies are evaluated. We argue\nthat this approach can produce effective defender strategies for a practical IT\ninfrastructure.\n']",Intrusion Detection and Defense Strategies,Intrusion Detection and Defense Strategies for Cybersecurity,Cybersecurity and Artificial Intelligence,Cybersecurity and Artificial Intelligence
389,18,389_autoencoders_autoencoder_autoenocoder_generative,"['autoencoders', 'autoencoder', 'autoenocoder', 'generative', 'encoder', 'variational', 'decoder', 'regularization', 'priors', 'latent']","['variational', 'posterior', 'latent', 'autoencoders', 'distributions', 'collapse', 'divergence', 'likelihood', 'autoencoder', 'generative']","[""  We consider the task of estimating variational autoencoders (VAEs) when the\ntraining data is incomplete. We show that missing data increases the complexity\nof the model's posterior distribution over the latent variables compared to the\nfully-observed case. The increased complexity may adversely affect the fit of\nthe model due to a mismatch between the variational and model posterior\ndistributions. We introduce two strategies based on (i) finite\nvariational-mixture and (ii) imputation-based variational-mixture distributions\nto address the increased posterior complexity. Through a comprehensive\nevaluation of the proposed approaches, we show that variational mixtures are\neffective at improving the accuracy of VAE estimation from incomplete data.\n"", ""  Traditional Variational Autoencoders (VAEs) are constrained by the\nlimitations of the Evidence Lower Bound (ELBO) formulation, particularly when\nutilizing simplistic, non-analytic, or unknown prior distributions. These\nlimitations inhibit the VAE's ability to generate high-quality samples and\nprovide clear, interpretable latent representations. This work introduces the\nEntropy Decomposed Variational Autoencoder (ED-VAE), a novel re-formulation of\nthe ELBO that explicitly includes entropy and cross-entropy components. This\nreformulation significantly enhances model flexibility, allowing for the\nintegration of complex and non-standard priors. By providing more detailed\ncontrol over the encoding and regularization of latent spaces, ED-VAE not only\nimproves interpretability but also effectively captures the complex\ninteractions between latent variables and observed data, thus leading to better\ngenerative performance.\n"", '  The posterior collapse phenomenon in variational autoencoder (VAE), where the\nvariational posterior distribution closely matches the prior distribution, can\nhinder the quality of the learned latent variables. As a consequence of\nposterior collapse, the latent variables extracted by the encoder in VAE\npreserve less information from the input data and thus fail to produce\nmeaningful representations as input to the reconstruction process in the\ndecoder. While this phenomenon has been an actively addressed topic related to\nVAE performance, the theory for posterior collapse remains underdeveloped,\nespecially beyond the standard VAE. In this work, we advance the theoretical\nunderstanding of posterior collapse to two important and prevalent yet less\nstudied classes of VAE: conditional VAE and hierarchical VAE. Specifically, via\na non-trivial theoretical analysis of linear conditional VAE and hierarchical\nVAE with two levels of latent, we prove that the cause of posterior collapses\nin these models includes the correlation between the input and output of the\nconditional VAE and the effect of learnable encoder variance in the\nhierarchical VAE. We empirically validate our theoretical findings for linear\nconditional and hierarchical VAE and demonstrate that these results are also\npredictive for non-linear cases with extensive experiments.\n']",Variational Autoencoders and Latent Variable Models,Variational Methods for Bayesian Modeling and Generative Learning,Generative Modeling and Artificial Intelligence,Generative Modeling and Artificial Intelligence
390,18,390_gestures_gesture_gesturegpt_gesturing,"['gestures', 'gesture', 'gesturegpt', 'gesturing', 'multimodal', 'nonverbal', 'speech', 'embodied', 'audio', 'cues']","['gestures', 'gesture', 'speech', 'nonverbal', 'beat', 'audio', 'motion', 'body', 'verbal', 'emotion']","[""  This paper focuses on enhancing human-agent communication by integrating\nspatial context into virtual agents' non-verbal behaviors, specifically\ngestures. Recent advances in co-speech gesture generation have primarily\nutilized data-driven methods, which create natural motion but limit the scope\nof gestures to those performed in a void. Our work aims to extend these methods\nby enabling generative models to incorporate scene information into\nspeech-driven gesture synthesis. We introduce a novel synthetic gesture dataset\ntailored for this purpose. This development represents a critical step toward\ncreating embodied conversational agents that interact more naturally with their\nenvironment and users.\n"", '  Gesture synthesis has gained significant attention as a critical research\nfield, aiming to produce contextually appropriate and natural gestures\ncorresponding to speech or textual input. Although deep learning-based\napproaches have achieved remarkable progress, they often overlook the rich\nsemantic information present in the text, leading to less expressive and\nmeaningful gestures. In this letter, we propose GesGPT, a novel approach to\ngesture generation that leverages the semantic analysis capabilities of large\nlanguage models , such as ChatGPT. By capitalizing on the strengths of LLMs for\ntext analysis, we adopt a controlled approach to generate and integrate\nprofessional gestures and base gestures through a text parsing script,\nresulting in diverse and meaningful gestures. Firstly, our approach involves\nthe development of prompt principles that transform gesture generation into an\nintention classification problem using ChatGPT. We also conduct further\nanalysis on emphasis words and semantic words to aid in gesture generation.\nSubsequently, we construct a specialized gesture lexicon with multiple semantic\nannotations, decoupling the synthesis of gestures into professional gestures\nand base gestures. Finally, we merge the professional gestures with base\ngestures. Experimental results demonstrate that GesGPT effectively generates\ncontextually appropriate and expressive gestures.\n', ""  Gestures are inherent to human interaction and often complement speech in\nface-to-face communication, forming a multimodal communication system. An\nimportant task in gesture analysis is detecting a gesture's beginning and end.\nResearch on automatic gesture detection has primarily focused on visual and\nkinematic information to detect a limited set of isolated or silent gestures\nwith low variability, neglecting the integration of speech and vision signals\nto detect gestures that co-occur with speech. This work addresses this gap by\nfocusing on co-speech gesture detection, emphasising the synchrony between\nspeech and co-speech hand gestures. We address three main challenges: the\nvariability of gesture forms, the temporal misalignment between gesture and\nspeech onsets, and differences in sampling rate between modalities. We\ninvestigate extended speech time windows and employ separate backbone models\nfor each modality to address the temporal misalignment and sampling rate\ndifferences. We utilize Transformer encoders in cross-modal and early fusion\ntechniques to effectively align and integrate speech and skeletal sequences.\nThe study results show that combining visual and speech information\nsignificantly enhances gesture detection performance. Our findings indicate\nthat expanding the speech buffer beyond visual time segments improves\nperformance and that multimodal integration using cross-modal and early fusion\ntechniques outperforms baseline methods using unimodal and late fusion methods.\nAdditionally, we find a correlation between the models' gesture prediction\nconfidence and low-level speech frequency features potentially associated with\ngestures. Overall, the study provides a better understanding and detection\nmethods for co-speech gestures, facilitating the analysis of multimodal\ncommunication.\n""]",Gesture Synthesis and Analysis in Multimodal Communication,Multimodal Human-Computer Interaction and Communication,Multimodal Learning and Applications,Multimodal Learning and Vision-Language Models
391,18,391_svm_svms_classification_kernelized,"['svm', 'svms', 'classification', 'kernelized', 'svr', 'vector', 'rademacher', 'outliers', 'norm', 'optimization']","['vector', 'support', 'loss', 'twin', 'margin', 'machines', 'function', 'commitment', 'hinge', 'outliers']","['  Support vector machine (SVM) has achieved many successes in machine learning,\nespecially for a small sample problem. As a famous extension of the traditional\nSVM, the $\\nu$ support vector machine ($\\nu$-SVM) has shown outstanding\nperformance due to its great model interpretability. However, it still faces\nchallenges in training overhead for large-scale problems. To address this\nissue, we propose a safe screening rule with bi-level optimization for\n$\\nu$-SVM (SRBO-$\\nu$-SVM) which can screen out inactive samples before\ntraining and reduce the computational cost without sacrificing the prediction\naccuracy. Our SRBO-$\\nu$-SVM is strictly deduced by integrating the\nKarush-Kuhn-Tucker (KKT) conditions, the variational inequalities of convex\nproblems and the $\\nu$-property. Furthermore, we develop an efficient dual\ncoordinate descent method (DCDM) to further improve computational speed.\nFinally, a unified framework for SRBO is proposed to accelerate many SVM-type\nmodels, and it is successfully applied to one-class SVM. Experimental results\non 6 artificial data sets and 30 benchmark data sets have verified the\neffectiveness and safety of our proposed methods in supervised and unsupervised\ntasks.\n', '  The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss\nSVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the\ndegree of penalty for the correctly classified samples within the margin. This\noversight affects the generalization ability of the SVM classifier to some\nextent. To address this limitation, from the perspective of confidence margin,\nwe propose a novel Slide loss function ($\\ell_s$) to construct the support\nvector machine classifier($\\ell_s$-SVM). By introducing the concept of proximal\nstationary point, and utilizing the property of Lipschitz continuity, we derive\nthe first-order optimality conditions for $\\ell_s$-SVM. Based on this, we\ndefine the $\\ell_s$ support vectors and working set of $\\ell_s$-SVM. To\nefficiently handle $\\ell_s$-SVM, we devise a fast alternating direction method\nof multipliers with the working set ($\\ell_s$-ADMM), and provide the\nconvergence analysis. The numerical experiments on real world datasets confirm\nthe robustness and effectiveness of the proposed method.\n', '  Loss function plays a vital role in supervised learning frameworks. The\nselection of the appropriate loss function holds the potential to have a\nsubstantial impact on the proficiency attained by the acquired model. The\ntraining of supervised learning algorithms inherently adheres to predetermined\nloss functions during the optimization process. In this paper, we present a\nnovel contribution to the realm of supervised machine learning: an asymmetric\nloss function named wave loss. It exhibits robustness against outliers,\ninsensitivity to noise, boundedness, and a crucial smoothness property.\nTheoretically, we establish that the proposed wave loss function manifests the\nessential characteristic of being classification-calibrated. Leveraging this\nbreakthrough, we incorporate the proposed wave loss function into the least\nsquares setting of support vector machines (SVM) and twin support vector\nmachines (TSVM), resulting in two robust and smooth models termed Wave-SVM and\nWave-TSVM, respectively. To address the optimization problem inherent in\nWave-SVM, we utilize the adaptive moment estimation (Adam) algorithm. It is\nnoteworthy that this paper marks the first instance of the Adam algorithm\napplication to solve an SVM model. Further, we devise an iterative algorithm to\nsolve the optimization problems of Wave-TSVM. To empirically showcase the\neffectiveness of the proposed Wave-SVM and Wave-TSVM, we evaluate them on\nbenchmark UCI and KEEL datasets (with and without feature noise) from diverse\ndomains. Moreover, to exemplify the applicability of Wave-SVM in the biomedical\ndomain, we evaluate it on the Alzheimer Disease Neuroimaging Initiative (ADNI)\ndataset. The experimental outcomes unequivocally reveal the prowess of Wave-SVM\nand Wave-TSVM in achieving superior prediction accuracy against the baseline\nmodels.\n']",Support Vector Machines (SVM) Optimization and Variants,Efficient Search and Classification Algorithms,Information Retrieval and Knowledge Systems,Information Retrieval and Knowledge Systems
392,18,392_graphprompt_graphs_graph4gui_subgraph,"['graphprompt', 'graphs', 'graph4gui', 'subgraph', 'graph', 'networks', 'pretraining', 'pretext', 'pretexts', 'nodes']","['prompt', 'pre', 'graph', 'pretext', 'downstream', 'prompting', 'graphs', 'tuning', 'gap', 'tasks']","[""  In recent years, prompt tuning has sparked a research surge in adapting\npre-trained models. Unlike the unified pre-training strategy employed in the\nlanguage field, the graph field exhibits diverse pre-training strategies,\nposing challenges in designing appropriate prompt-based tuning methods for\ngraph neural networks. While some pioneering work has devised specialized\nprompting functions for models that employ edge prediction as their\npre-training tasks, these methods are limited to specific pre-trained GNN\nmodels and lack broader applicability. In this paper, we introduce a universal\nprompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained\nGNN models under any pre-training strategy. GPF operates on the input graph's\nfeature space and can theoretically achieve an equivalent effect to any form of\nprompting function. Consequently, we no longer need to illustrate the prompting\nfunction corresponding to each pre-training strategy explicitly. Instead, we\nemploy GPF to obtain the prompted graph for the downstream task in an adaptive\nmanner. We provide rigorous derivations to demonstrate the universality of GPF\nand make guarantee of its effectiveness. The experimental results under various\npre-training strategies indicate that our method performs better than\nfine-tuning, with an average improvement of about 1.4% in full-shot scenarios\nand about 3.2% in few-shot scenarios. Moreover, our method significantly\noutperforms existing specialized prompt-based tuning methods when applied to\nmodels utilizing the pre-training strategy they specialize in. These numerous\nadvantages position our method as a compelling alternative to fine-tuning for\ndownstream adaptations.\n"", '  In recent years, graph prompt learning/tuning has garnered increasing\nattention in adapting pre-trained models for graph representation learning. As\na kind of universal graph prompt learning method, Graph Prompt Feature (GPF)\nhas achieved remarkable success in adapting pre-trained models for Graph Neural\nNetworks (GNNs). By fixing the parameters of a pre-trained GNN model, the aim\nof GPF is to modify the input graph data by adding some (learnable) prompt\nvectors into graph node features to better align with the downstream tasks on\nthe smaller dataset. However, existing GPFs generally suffer from two main\nlimitations. First, GPFs generally focus on node prompt learning which ignore\nthe prompting for graph edges. Second, existing GPFs generally conduct the\nprompt learning on all nodes equally which fails to capture the importances of\ndifferent nodes and may perform sensitively w.r.t noisy nodes in aligning with\nthe downstream tasks. To address these issues, in this paper, we propose a new\nunified Graph Selective Prompt Feature learning (GSPF) for GNN fine-tuning. The\nproposed GSPF integrates the prompt learning on both graph node and edge\ntogether, which thus provides a unified prompt model for the graph data.\nMoreover, it conducts prompt learning selectively on nodes and edges by\nconcentrating on the important nodes and edges for prompting which thus make\nour model be more reliable and compact. Experimental results on many benchmark\ndatasets demonstrate the effectiveness and advantages of the proposed GSPF\nmethod.\n', '  Graphs have emerged as a natural choice to represent and analyze the\nintricate patterns and rich information of the Web, enabling applications such\nas online page classification and social recommendation. The prevailing\n""pre-train, fine-tune"" paradigm has been widely adopted in graph machine\nlearning tasks, particularly in scenarios with limited labeled nodes. However,\nthis approach often exhibits a misalignment between the training objectives of\npretext tasks and those of downstream tasks. This gap can result in the\n""negative transfer"" problem, wherein the knowledge gained from pre-training\nadversely affects performance in the downstream tasks. The surge in\nprompt-based learning within Natural Language Processing (NLP) suggests the\npotential of adapting a ""pre-train, prompt"" paradigm to graphs as an\nalternative. However, existing graph prompting techniques are tailored to\nhomogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To\nbridge this gap, we propose HetGPT, a general post-training prompting framework\nto improve the predictive performance of pre-trained heterogeneous graph neural\nnetworks (HGNNs). The key is the design of a novel prompting function that\nintegrates a virtual class prompt and a heterogeneous feature prompt, with the\naim to reformulate downstream tasks to mirror pretext tasks. Moreover, HetGPT\nintroduces a multi-view neighborhood aggregation mechanism, capturing the\ncomplex neighborhood structure in heterogeneous graphs. Extensive experiments\non three benchmark datasets demonstrate HetGPT\'s capability to enhance the\nperformance of state-of-the-art HGNNs on semi-supervised node classification.\n']",Graph Prompt Learning for GNNs,Graph Neural Networks (GNNs) and Graph Data Analysis,Graph Representation Learning and Neural Networks,Graph Representation Learning and Neural Networks
393,18,393_saliency_salient_cnn_imagenet,"['saliency', 'salient', 'cnn', 'imagenet', 'explanations', 'neural', 'recognition', 'ai', 'classificationmetricsforimageexplanations', 'explainability']","['saliency', 'maps', 'explanation', 'explanations', 'gradient', 'map', 'interpretation', 'gradients', 'black', 'masks']","['  Deep learning models have performed well on many NLP tasks. However, their\ninternal mechanisms are typically difficult for humans to understand. The\ndevelopment of methods to explain models has become a key issue in the\nreliability of deep learning models in many important applications. Various\nsaliency explanation methods, which give each feature of input a score\nproportional to the contribution of output, have been proposed to determine the\npart of the input which a model values most. Despite a considerable body of\nwork on the evaluation of saliency methods, whether the results of various\nevaluation metrics agree with human cognition remains an open question. In this\nstudy, we propose a new human-based method to evaluate saliency methods in NLP\nby crowdsourcing. We recruited 800 crowd workers and empirically evaluated\nseven saliency methods on two datasets with the proposed method. We analyzed\nthe performance of saliency methods, compared our results with existing\nautomated evaluation methods, and identified notable differences between NLP\nand computer vision (CV) fields when using saliency methods. The instance-level\ndata of our crowdsourced experiments and the code to reproduce the explanations\nare available at https://github.com/xtlu/lreccoling_evaluation.\n', '  Gradient-based saliency maps are widely used to explain deep neural network\ndecisions. However, as models become deeper and more black-box, such as in\nclosed-source APIs like ChatGPT, computing gradients become challenging,\nhindering conventional explanation methods. In this work, we introduce a novel\nunified framework for estimating gradients in black-box settings and generating\nsaliency maps to interpret model decisions. We employ the likelihood ratio\nmethod to estimate output-to-input gradients and utilize them for saliency map\ngeneration. Additionally, we propose blockwise computation techniques to\nenhance estimation accuracy. Extensive experiments in black-box settings\nvalidate the effectiveness of our method, demonstrating accurate gradient\nestimation and explainability of generated saliency maps. Furthermore, we\nshowcase the scalability of our approach by applying it to explain GPT-Vision,\nrevealing the continued relevance of gradient-based explanation methods in the\nera of large, closed-source, and black-box models.\n', '  Input gradients have a pivotal role in a variety of applications, including\nadversarial attack algorithms for evaluating model robustness, explainable AI\ntechniques for generating Saliency Maps, and counterfactual\nexplanations.However, Saliency Maps generated by traditional neural networks\nare often noisy and provide limited insights. In this paper, we demonstrate\nthat, on the contrary, the Saliency Maps of 1-Lipschitz neural networks,\nlearned with the dual loss of an optimal transportation problem, exhibit\ndesirable XAI properties:They are highly concentrated on the essential parts of\nthe image with low noise, significantly outperforming state-of-the-art\nexplanation approaches across various models and metrics. We also prove that\nthese maps align unprecedentedly well with human explanations on ImageNet.To\nexplain the particularly beneficial properties of the Saliency Map for such\nmodels, we prove this gradient encodes both the direction of the transportation\nplan and the direction towards the nearest adversarial attack. Following the\ngradient down to the decision boundary is no longer considered an adversarial\nattack, but rather a counterfactual explanation that explicitly transports the\ninput from one class to another. Thus, Learning with such a loss jointly\noptimizes the classification objective and the alignment of the gradient, i.e.\nthe Saliency Map, to the transportation plan direction.These networks were\npreviously known to be certifiably robust by design, and we demonstrate that\nthey scale well for large problems and models, and are tailored for\nexplainability using a fast and straightforward method.\n']",Evaluating Saliency Methods for Explainable AI,Explainable AI and Machine Learning,Artificial Intelligence and Machine Learning Interpretability and Explainability,Explainable AI and Machine Learning
394,18,394_privacy_graphpub_graphs_private,"['privacy', 'graphpub', 'graphs', 'private', 'edge', 'graph', 'adjacency', 'gnndelete', 'nodes', 'neighbors']","['privacy', 'graph', 'node', 'edges', 'edge', 'utility', 'attacks', 'protection', 'private', 'leakage']","['  Graph neural networks (GNNs) play a key role in learning representations from\ngraph-structured data and are demonstrated to be useful in many applications.\nHowever, the GNN training pipeline has been shown to be vulnerable to node\nfeature leakage and edge extraction attacks. This paper investigates a scenario\nwhere an attacker aims to recover private edge information from a trained GNN\nmodel. Previous studies have employed differential privacy (DP) to add noise\ndirectly to the adjacency matrix or a compact graph representation. The added\nperturbations cause the graph structure to be substantially morphed, reducing\nthe model utility. We propose a new privacy-preserving GNN training algorithm,\nEclipse, that maintains good model utility while providing strong privacy\nprotection on edges. Eclipse is based on two key observations. First, adjacency\nmatrices in graph structures exhibit low-rank behavior. Thus, Eclipse trains\nGNNs with a low-rank format of the graph via singular values decomposition\n(SVD), rather than the original graph. Using the low-rank format, Eclipse\npreserves the primary graph topology and removes the remaining residual edges.\nEclipse adds noise to the low-rank singular values instead of the entire graph,\nthereby preserving the graph privacy while still maintaining enough of the\ngraph structure to maintain model utility. We theoretically show Eclipse\nprovide formal DP guarantee on edges. Experiments on benchmark graph datasets\nshow that Eclipse achieves significantly better privacy-utility tradeoff\ncompared to existing privacy-preserving GNN training methods. In particular,\nunder strong privacy constraints ($\\epsilon$ < 4), Eclipse shows significant\ngains in the model utility by up to 46%. We further demonstrate that Eclipse\nalso has better resilience against common edge attacks (e.g., LPA), lowering\nthe attack AUC by up to 5% compared to other state-of-the-art baselines.\n', '  Graph Neural Networks (GNNs) have achieved great success in learning with\ngraph-structured data. Privacy concerns have also been raised for the trained\nmodels which could expose the sensitive information of graphs including both\nnode features and the structure information. In this paper, we aim to achieve\nnode-level differential privacy (DP) for training GNNs so that a node and its\nedges are protected. Node DP is inherently difficult for GNNs because all\ndirect and multi-hop neighbors participate in the calculation of gradients for\neach node via layer-wise message passing and there is no bound on how many\ndirect and multi-hop neighbors a node can have, so existing DP methods will\nresult in high privacy cost or poor utility due to high node sensitivity. We\npropose a Decoupled GNN with Differentially Private Approximate Personalized\nPageRank (DPAR) for training GNNs with an enhanced privacy-utility tradeoff.\nThe key idea is to decouple the feature projection and message passing via a DP\nPageRank algorithm which learns the structure information and uses the top-$K$\nneighbors determined by the PageRank for feature aggregation. By capturing the\nmost important neighbors for each node and avoiding the layer-wise message\npassing, it bounds the node sensitivity and achieves improved privacy-utility\ntradeoff compared to layer-wise perturbation based methods. We theoretically\nanalyze the node DP guarantee for the two processes combined together and\nempirically demonstrate better utilities of DPAR with the same level of node DP\ncompared with state-of-the-art methods.\n', ""  Differentially private GNNs (Graph Neural Networks) have been recently\nstudied to provide high accuracy in various tasks on graph data while strongly\nprotecting user privacy. In particular, a recent study proposes an algorithm to\nprotect each user's feature vector in an attributed graph, which includes\nfeature vectors along with node IDs and edges, with LDP (Local Differential\nPrivacy), a strong privacy notion without a trusted third party. However, this\nalgorithm does not protect edges (friendships) in a social graph, hence cannot\nprotect user privacy in unattributed graphs, which include only node IDs and\nedges. How to provide strong privacy with high accuracy in unattributed graphs\nremains open. In this paper, we propose a novel LDP algorithm called the DPRR\n(Degree-Preserving Randomized Response) to provide LDP for edges in GNNs. Our\nDPRR preserves each user's degree hence a graph structure while providing edge\nLDP. Technically, our DPRR uses Warner's RR (Randomized Response) and strategic\nedge sampling, where each user's sampling probability is automatically tuned\nusing the Laplacian mechanism to preserve the degree information under edge\nLDP. We also propose a privacy budget allocation method to make the noise in\nboth Warner's RR and the Laplacian mechanism small. We focus on graph\nclassification as a task of GNNs and evaluate the DPRR using three social graph\ndatasets. Our experimental results show that the DPRR significantly outperforms\nthree baselines and provides accuracy close to a non-private algorithm in all\ndatasets with a reasonable privacy budget, e.g., epsilon=1. Finally, we\nintroduce data poisoning attacks to our DPRR and a defense against the attacks.\nWe evaluate them using the three social graph datasets and discuss the\nexperimental results.\n""]",Differentially Private Graph Neural Networks,Differential Privacy in Machine Learning,Machine Learning and Data Privacy,Machine Learning and Data Privacy
395,17,395_edge_cloud_iot_bandwidth,"['edge', 'cloud', 'iot', 'bandwidth', 'mobile', 'devices', 'gaisnet', 'network', 'throughput', 'ai']","['edge', 'cloud', 'inference', 'computing', 'latency', 'collaborative', 'sustainable', 'resource', 'devices', 'device']","['  On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest owing to their superior privacy,\nreduced latency, and bandwidth saving. Nonetheless, the capabilities of\non-device LLMs are intrinsically constrained by the limited capacity of edge\ndevices compared to the much more powerful cloud centers. To bridge the gap\nbetween cloud-based and on-device AI, mobile edge intelligence (MEI) presents a\nviable solution to this problem by provisioning AI capabilities within the edge\nof mobile networks with improved privacy and latency relative to cloud\ncomputing. MEI sits between on-device AI and cloud-based AI, featuring wireless\ncommunications and more powerful computing resources than end devices. This\narticle provides a contemporary survey on harnessing MEI for LLMs. We first\ncover the preliminaries of LLMs, starting with LLMs and MEI, followed by\nresource-efficient LLM techniques. We then illustrate several killer\napplications to demonstrate the need for deploying LLMs at the network edge and\npresent an architectural overview of MEI for LLMs (MEI4LLM). Subsequently, we\ndelve into various aspects of MEI4LLM, extensively covering edge LLM caching\nand delivery, edge LLM training, and edge LLM inference. Finally, we identify\nfuture research opportunities. We aim to inspire researchers in the field to\nleverage mobile edge computing to facilitate LLM deployment in close proximity\nto users, thereby unleashing the potential of LLMs across various privacy- and\ndelay-sensitive applications.\n', ""  Big Artificial Intelligence (AI) models have emerged as a crucial element in\nvarious intelligent applications at the edge, such as voice assistants in smart\nhomes and autonomous robotics in smart factories. Training big AI models, e.g.,\nfor personalized fine-tuning and continual model refinement, poses significant\nchallenges to edge devices due to the inherent conflict between limited\ncomputing resources and intensive workload associated with training. Despite\nthe constraints of on-device training, traditional approaches usually resort to\naggregating training data and sending it to a remote cloud for centralized\ntraining. Nevertheless, this approach is neither sustainable, which strains\nlong-range backhaul transmission and energy-consuming datacenters, nor safely\nprivate, which shares users' raw data with remote infrastructures. To address\nthese challenges, we alternatively observe that prevalent edge environments\nusually contain a diverse collection of trusted edge devices with untapped idle\nresources, which can be leveraged for edge training acceleration. Motivated by\nthis, in this article, we propose collaborative edge training, a novel training\nmechanism that orchestrates a group of trusted edge devices as a resource pool\nfor expedited, sustainable big AI model training at the edge. As an initial\nstep, we present a comprehensive framework for building collaborative edge\ntraining systems and analyze in-depth its merits and sustainable scheduling\nchoices following its workflow. To further investigate the impact of its\nparallelism design, we empirically study a case of four typical parallelisms\nfrom the perspective of energy demand with realistic testbeds. Finally, we\ndiscuss open challenges for sustainable collaborative edge training to point to\nfuture directions of edge-centric big AI model training.\n"", '  Artificial intelligence (AI) technologies have emerged as pivotal enablers\nacross a multitude of industries largely due to their significant resurgence\nover the past decade. The transformative power of AI is primarily derived from\nthe utilization of deep neural networks (DNNs), which require extensive data\nfor training and substantial computational resources for processing.\nConsequently, DNN models are typically trained and deployed on resource-rich\ncloud servers. However, due to potential latency issues associated with cloud\ncommunications, deep learning (DL) workflows are increasingly being\ntransitioned to wireless edge networks in proximity to end-user devices (EUDs).\nThis shift is designed to support latency-sensitive applications and has given\nrise to a new paradigm of edge AI, which will play a critical role in upcoming\nsixth-generation (6G) networks to support ubiquitous AI applications. Despite\nits considerable potential, edge AI faces substantial challenges, mostly due to\nthe dichotomy between the resource limitations of wireless edge networks and\nthe resource-intensive nature of DL. Specifically, the acquisition of\nlarge-scale data, as well as the training and inference processes of DNNs, can\nrapidly deplete the battery energy of EUDs. This necessitates an\nenergy-conscious approach to edge AI to ensure both optimal and sustainable\nperformance. In this paper, we present a contemporary survey on green edge AI.\nWe commence by analyzing the principal energy consumption components of edge AI\nsystems to identify the fundamental design principles of green edge AI. Guided\nby these principles, we then explore energy-efficient design methodologies for\nthe three critical tasks in edge AI systems, including training data\nacquisition, edge training, and edge inference. Finally, we underscore\npotential future research directions to further enhance the energy efficiency\nof edge AI.\n']",Edge AI and Mobile Networks,Edge Computing and Artificial Intelligence for IoT and Mobile Networks,Edge Computing and Artificial Intelligence for IoT and Mobile Networks,Edge Computing and Artificial Intelligence for IoT and Mobile Networks
396,17,396_entities_entity_matching_matched,"['entities', 'entity', 'matching', 'matched', 'records', 'match', 'record', 'matchers', 'linking', 'similarity']","['matching', 'entity', 'records', 'record', 'linkage', 'duplication', 'string', 'packages', 'matchers', 'fuzzy']","['  Entity matching (EM) is a critical step in entity resolution (ER). Recently,\nentity matching based on large language models (LLMs) has shown great promise.\nHowever, current LLM-based entity matching approaches typically follow a binary\nmatching paradigm that ignores the global consistency between record\nrelationships. In this paper, we investigate various methodologies for\nLLM-based entity matching that incorporate record interactions from different\nperspectives. Specifically, we comprehensively compare three representative\nstrategies: matching, comparing, and selecting, and analyze their respective\nadvantages and challenges in diverse scenarios. Based on our findings, we\nfurther design a compound entity matching framework (ComEM) that leverages the\ncomposition of multiple strategies and LLMs. ComEM benefits from the advantages\nof different sides and achieves improvements in both effectiveness and\nefficiency. Experimental results on 8 ER datasets and 9 LLMs verify the\nsuperiority of incorporating record interactions through the selecting\nstrategy, as well as the further cost-effectiveness brought by ComEM.\n', '  Entity Matching is the task of deciding whether two entity descriptions refer\nto the same real-world entity and is a central step in most data integration\npipelines. Many state-of-the-art entity matching methods rely on pre-trained\nlanguage models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these\nmodels for entity matching are that (i) the models require significant amounts\nof task-specific training data and (ii) the fine-tuned models are not robust\nconcerning out-of-distribution entities. This paper investigates using\ngenerative large language models (LLMs) as a less task-specific training\ndata-dependent and more robust alternative to PLM-based matchers. Our study\ncovers hosted and open-source LLMs, which can be run locally. We evaluate these\nmodels in a zero-shot scenario and a scenario where task-specific training data\nis available. We compare different prompt designs and the prompt sensitivity of\nthe models and show that there is no single best prompt but needs to be tuned\nfor each model/dataset combination. We further investigate (i) the selection of\nin-context demonstrations, (ii) the generation of matching rules, as well as\n(iii) fine-tuning a hosted LLM using the same pool of training data. Our\nexperiments show that the best LLMs require no or only a few training examples\nto perform similarly to PLMs that were fine-tuned using thousands of examples.\nLLM-based matchers further exhibit higher robustness to unseen entities. We\nshow that GPT4 can generate structured explanations for matching decisions. The\nmodel can automatically identify potential causes of matching errors by\nanalyzing explanations of wrong decisions. We demonstrate that the model can\ngenerate meaningful textual descriptions of the identified error classes, which\ncan help data engineers improve entity matching pipelines.\n', '  Entity matching is the task of linking records from different sources that\nrefer to the same real-world entity. Past work has primarily treated entity\nlinking as a standard supervised learning problem. However, supervised entity\nmatching models often do not generalize well to new data, and collecting\nexhaustive labeled training data is often cost prohibitive. Further, recent\nefforts have adopted LLMs for this task in few/zero-shot settings, exploiting\ntheir general knowledge. But LLMs are prohibitively expensive for performing\ninference at scale for real-world entity matching tasks.\n  As an efficient alternative, we re-cast entity matching as a conditional\ngeneration task as opposed to binary classification. This enables us to\n""distill"" LLM reasoning into smaller entity matching models via natural\nlanguage explanations. This approach achieves strong performance, especially on\nout-of-domain generalization tests (10.85% F-1) where standalone generative\nmethods struggle. We perform ablations that highlight the importance of\nexplanations, both for performance and model robustness.\n']",Entity Matching and Linking,Entity Understanding and Resolution,Entity Understanding and Semantic Knowledge Integration,Entity Understanding and Semantic Knowledge Integration
397,17,397_contextual_learning_context_attention,"['contextual', 'learning', 'context', 'attention', 'icl', 'transformers', 'classes', 'icll', 'icicl', 'predictor']","['heads', 'context', 'transformers', 'induction', 'transformer', 'token', 'plateaus', 'function', 'regular', 'contextual']","[""  State of the art foundation models such as GPT-4 perform surprisingly well at\nin-context learning (ICL), a variant of meta-learning concerning the learned\nability to solve tasks during a neural network forward pass, exploiting\ncontextual information provided as input to the model. This useful ability\nemerges as a side product of the foundation model's massive pretraining. While\ntransformer models are currently the state of the art in ICL, this work\nprovides empirical evidence that Mamba, a newly proposed state space model\nwhich scales better than transformers w.r.t. the input sequence length, has\nsimilar ICL capabilities. We evaluated Mamba on tasks involving simple function\napproximation as well as more complex natural language processing problems. Our\nresults demonstrate that, across both categories of tasks, Mamba closely\nmatches the performance of transformer models for ICL. Further analysis reveals\nthat, like transformers, Mamba appears to solve ICL problems by incrementally\noptimizing its internal representations. Overall, our work suggests that Mamba\ncan be an efficient alternative to transformers for ICL tasks involving long\ninput sequences. This is an exciting finding in meta-learning and may enable\ngeneralizations of in-context learned AutoML algorithms (like TabPFN or\nOptformer) to long input sequences.\n"", '  In-context learning (ICL) is one of the surprising and useful features of\nlarge language models and subject of intense research. Recently, stylized\nmeta-learning-like ICL setups have been devised that train transformers on\nsequences of input-output pairs $(x, f(x))$. The function $f$ comes from a\nfunction class and generalization is checked by evaluating on sequences\ngenerated from unseen functions from the same class. One of the main\ndiscoveries in this line of research has been that for several function\nclasses, such as linear regression, transformers successfully generalize to new\nfunctions in the class. However, the inductive biases of these models resulting\nin this behavior are not clearly understood. A model with unlimited training\ndata and compute is a Bayesian predictor: it learns the pretraining\ndistribution. In this paper we empirically examine how far this Bayesian\nperspective can help us understand ICL. To this end, we generalize the previous\nmeta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple\ntask families. We instantiate this setup on a diverse range of linear and\nnonlinear function families and find that transformers can do ICL in this\nsetting as well. Where Bayesian inference is tractable, we find evidence that\nhigh-capacity transformers mimic the Bayesian predictor. The Bayesian\nperspective provides insights into the inductive bias of ICL and how\ntransformers perform a particular task when they are trained on multiple tasks.\nWe also find that transformers can learn to generalize to new function classes\nthat were not seen during pretraining. This involves deviation from the\nBayesian predictor. We examine these deviations in more depth offering new\ninsights and hypotheses.\n', '  Large-scale neural language models exhibit a remarkable capacity for\nin-context learning (ICL): they can infer novel functions from datasets\nprovided as input. Most of our current understanding of when and how ICL arises\ncomes from LMs trained on extremely simple learning problems like linear\nregression and associative recall. There remains a significant gap between\nthese model problems and the ""real"" ICL exhibited by LMs trained on large text\ncorpora, which involves not just retrieval and function approximation but\nfree-form generation of language and other structured outputs. In this paper,\nwe study ICL through the lens of a new family of model problems we term in\ncontext language learning (ICLL). In ICLL, LMs are presented with a set of\nstrings from a formal language, and must generate additional strings from the\nsame language. We focus on in-context learning of regular languages generated\nby random finite automata. We evaluate a diverse set of neural sequence models\n(including several RNNs, Transformers, and state-space model variants) on\nregular ICLL tasks, aiming to answer three questions: (1) Which model classes\nare empirically capable of ICLL? (2) What algorithmic solutions do successful\nmodels implement to perform ICLL? (3) What architectural changes can improve\nICLL in less performant models? We first show that Transformers significantly\noutperform neural sequence models with recurrent or convolutional\nrepresentations on ICLL tasks. Next, we provide evidence that their ability to\ndo so relies on specialized ""n-gram heads"" (higher-order variants of induction\nheads) that compute input-conditional next-token distributions. Finally, we\nshow that hard-wiring these heads into neural models improves performance not\njust on ICLL, but natural language modeling -- improving the perplexity of\n340M-parameter models by up to 1.14 points (6.7%) on the SlimPajama dataset.\n']",In-Context Learning with Transformers,In-Context Learning with Transformers,Advances in Sequence Modeling and Learning,Advances in Sequence Modeling and Learning
397,17,397_contextual_learning_context_attention,"['contextual', 'learning', 'context', 'attention', 'icl', 'transformers', 'classes', 'icll', 'icicl', 'predictor']","['heads', 'context', 'transformers', 'induction', 'transformer', 'token', 'plateaus', 'function', 'regular', 'contextual']","[""  State of the art foundation models such as GPT-4 perform surprisingly well at\nin-context learning (ICL), a variant of meta-learning concerning the learned\nability to solve tasks during a neural network forward pass, exploiting\ncontextual information provided as input to the model. This useful ability\nemerges as a side product of the foundation model's massive pretraining. While\ntransformer models are currently the state of the art in ICL, this work\nprovides empirical evidence that Mamba, a newly proposed state space model\nwhich scales better than transformers w.r.t. the input sequence length, has\nsimilar ICL capabilities. We evaluated Mamba on tasks involving simple function\napproximation as well as more complex natural language processing problems. Our\nresults demonstrate that, across both categories of tasks, Mamba closely\nmatches the performance of transformer models for ICL. Further analysis reveals\nthat, like transformers, Mamba appears to solve ICL problems by incrementally\noptimizing its internal representations. Overall, our work suggests that Mamba\ncan be an efficient alternative to transformers for ICL tasks involving long\ninput sequences. This is an exciting finding in meta-learning and may enable\ngeneralizations of in-context learned AutoML algorithms (like TabPFN or\nOptformer) to long input sequences.\n"", '  In-context learning (ICL) is one of the surprising and useful features of\nlarge language models and subject of intense research. Recently, stylized\nmeta-learning-like ICL setups have been devised that train transformers on\nsequences of input-output pairs $(x, f(x))$. The function $f$ comes from a\nfunction class and generalization is checked by evaluating on sequences\ngenerated from unseen functions from the same class. One of the main\ndiscoveries in this line of research has been that for several function\nclasses, such as linear regression, transformers successfully generalize to new\nfunctions in the class. However, the inductive biases of these models resulting\nin this behavior are not clearly understood. A model with unlimited training\ndata and compute is a Bayesian predictor: it learns the pretraining\ndistribution. In this paper we empirically examine how far this Bayesian\nperspective can help us understand ICL. To this end, we generalize the previous\nmeta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple\ntask families. We instantiate this setup on a diverse range of linear and\nnonlinear function families and find that transformers can do ICL in this\nsetting as well. Where Bayesian inference is tractable, we find evidence that\nhigh-capacity transformers mimic the Bayesian predictor. The Bayesian\nperspective provides insights into the inductive bias of ICL and how\ntransformers perform a particular task when they are trained on multiple tasks.\nWe also find that transformers can learn to generalize to new function classes\nthat were not seen during pretraining. This involves deviation from the\nBayesian predictor. We examine these deviations in more depth offering new\ninsights and hypotheses.\n', '  Large-scale neural language models exhibit a remarkable capacity for\nin-context learning (ICL): they can infer novel functions from datasets\nprovided as input. Most of our current understanding of when and how ICL arises\ncomes from LMs trained on extremely simple learning problems like linear\nregression and associative recall. There remains a significant gap between\nthese model problems and the ""real"" ICL exhibited by LMs trained on large text\ncorpora, which involves not just retrieval and function approximation but\nfree-form generation of language and other structured outputs. In this paper,\nwe study ICL through the lens of a new family of model problems we term in\ncontext language learning (ICLL). In ICLL, LMs are presented with a set of\nstrings from a formal language, and must generate additional strings from the\nsame language. We focus on in-context learning of regular languages generated\nby random finite automata. We evaluate a diverse set of neural sequence models\n(including several RNNs, Transformers, and state-space model variants) on\nregular ICLL tasks, aiming to answer three questions: (1) Which model classes\nare empirically capable of ICLL? (2) What algorithmic solutions do successful\nmodels implement to perform ICLL? (3) What architectural changes can improve\nICLL in less performant models? We first show that Transformers significantly\noutperform neural sequence models with recurrent or convolutional\nrepresentations on ICLL tasks. Next, we provide evidence that their ability to\ndo so relies on specialized ""n-gram heads"" (higher-order variants of induction\nheads) that compute input-conditional next-token distributions. Finally, we\nshow that hard-wiring these heads into neural models improves performance not\njust on ICLL, but natural language modeling -- improving the perplexity of\n340M-parameter models by up to 1.14 points (6.7%) on the SlimPajama dataset.\n']",In-Context Learning with Transformers,In-Context Learning with Transformers,Advances in Sequence Modeling and Learning,Advances in Sequence Modeling and Learning
398,17,398_classifiers_classifier_annotations_spuriousness,"['classifiers', 'classifier', 'annotations', 'spuriousness', 'generalization', 'misclassifies', 'attributes', 'features', 'spurious', 'labels']","['spurious', 'group', 'correlations', 'subgroups', 'biased', 'spuriousness', 'worst', 'labels', 'nuisances', 'bias']","['  Machine learning models are known to learn spurious correlations, i.e.,\nfeatures having strong relations with class labels but no causal relation.\nRelying on those correlations leads to poor performance in the data groups\nwithout these correlations and poor generalization ability. To improve the\nrobustness of machine learning models to spurious correlations, we propose an\napproach to extract a subnetwork from a fully trained network that does not\nrely on spurious correlations. The subnetwork is found by the assumption that\ndata points with the same spurious attribute will be close to each other in the\nrepresentation space when training with ERM, then we employ supervised\ncontrastive loss in a novel way to force models to unlearn the spurious\nconnections. The increase in the worst-group performance of our approach\ncontributes to strengthening the hypothesis that there exists a subnetwork in a\nfully trained dense network that is responsible for using only invariant\nfeatures in classification tasks, therefore erasing the influence of spurious\nfeatures even in the setup of multi spurious attributes and no prior knowledge\nof attributes labels.\n', '  Standard empirical risk minimization (ERM) models may prioritize learning\nspurious correlations between spurious features and true labels, leading to\npoor accuracy on groups where these correlations do not hold. Mitigating this\nissue often requires expensive spurious attribute (group) labels or relies on\ntrained ERM models to infer group labels when group information is unavailable.\nHowever, the significant performance gap in worst-group accuracy between using\npseudo group labels and using oracle group labels inspires us to consider\nfurther improving group robustness through preciser group inference. Therefore,\nwe propose GIC, a novel method that accurately infers group labels, resulting\nin improved worst-group performance. GIC trains a spurious attribute classifier\nbased on two key properties of spurious correlations: (1) high correlation\nbetween spurious attributes and true labels, and (2) variability in this\ncorrelation between datasets with different group distributions. Empirical\nstudies on multiple datasets demonstrate the effectiveness of GIC in inferring\ngroup labels, and combining GIC with various downstream invariant learning\nmethods improves worst-group accuracy, showcasing its powerful flexibility.\nAdditionally, through analyzing the misclassifications in GIC, we identify an\ninteresting phenomenon called semantic consistency, which may contribute to\nbetter decoupling the association between spurious attributes and labels,\nthereby mitigating spurious correlation. The code for GIC is available at\nhttps://github.com/yujinhanml/GIC.\n', '  Deep neural classifiers tend to rely on spurious correlations between\nspurious attributes of inputs and targets to make predictions, which could\njeopardize their generalization capability. Training classifiers robust to\nspurious correlations typically relies on annotations of spurious correlations\nin data, which are often expensive to get. In this paper, we tackle an\nannotation-free setting and propose a self-guided spurious correlation\nmitigation framework. Our framework automatically constructs fine-grained\ntraining labels tailored for a classifier obtained with empirical risk\nminimization to improve its robustness against spurious correlations. The\nfine-grained training labels are formulated with different prediction behaviors\nof the classifier identified in a novel spuriousness embedding space. We\nconstruct the space with automatically detected conceptual attributes and a\nnovel spuriousness metric which measures how likely a class-attribute\ncorrelation is exploited for predictions. We demonstrate that training the\nclassifier to distinguish different prediction behaviors reduces its reliance\non spurious correlations without knowing them a priori and outperforms prior\nmethods on five real-world datasets.\n']",Mitigating Spurious Correlations in Machine Learning Classifiers,Mitigating Spurious Correlations in Machine Learning,Machine Learning Reliability and Uncertainty,Machine Learning Reliability and Uncertainty
399,17,399_embeddings_embedding_visualizing_visualizations,"['embeddings', 'embedding', 'visualizing', 'visualizations', 'visualization', 'visualize', 'visualisation', 'visualise', 'dimensionality', 'sne']","['dimensional', 'dimensionality', 'visualization', 'reduction', 'visualisation', 'kernel', 'projections', 'visualizations', 'attraction', 'embeddings']","['  This paper presents a new insight into improving the performance of\nStochastic Neighbour Embedding (t-SNE) by using Isolation kernel instead of\nGaussian kernel. Isolation kernel outperforms Gaussian kernel in two aspects.\nFirst, the use of Isolation kernel in t-SNE overcomes the drawback of\nmisrepresenting some structures in the data, which often occurs when Gaussian\nkernel is applied in t-SNE. This is because Gaussian kernel determines each\nlocal bandwidth based on one local point only, while Isolation kernel is\nderived directly from the data based on space partitioning. Second, the use of\nIsolation kernel yields a more efficient similarity computation because\ndata-dependent Isolation kernel has only one parameter that needs to be tuned.\nIn contrast, the use of data-independent Gaussian kernel increases the\ncomputational cost by determining n bandwidths for a dataset of n points. As\nthe root cause of these deficiencies in t-SNE is Gaussian kernel, we show that\nsimply replacing Gaussian kernel with Isolation kernel in t-SNE significantly\nimproves the quality of the final visualisation output (without creating\nmisrepresented structures) and removes one key obstacle that prevents t-SNE\nfrom processing large datasets. Moreover, Isolation kernel enables t-SNE to\ndeal with large-scale datasets in less runtime without trading off accuracy,\nunlike existing methods in speeding up t-SNE.\n', '  We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle\ninfinite data streams. The core idea behind S+t-SNE is to update the t-SNE\nembedding incrementally as new data arrives, ensuring scalability and\nadaptability to handle streaming scenarios. By selecting the most important\npoints at each step, the algorithm ensures scalability while keeping\ninformative visualisations. Employing a blind method for drift management\nadjusts the embedding space, facilitating continuous visualisation of evolving\ndata dynamics. Our experimental evaluations demonstrate the effectiveness and\nefficiency of S+t-SNE. The results highlight its ability to capture patterns in\na streaming scenario. We hope our approach offers researchers and practitioners\na real-time tool for understanding and interpreting high-dimensional data.\n', ""  t-Distributed Stochastic Neighbor Embedding (t-SNE) for the visualization of\nmultidimensional data has proven to be a popular approach, with successful\napplications in a wide range of domains. Despite their usefulness, t-SNE\nprojections can be hard to interpret or even misleading, which hurts the\ntrustworthiness of the results. Understanding the details of t-SNE itself and\nthe reasons behind specific patterns in its output may be a daunting task,\nespecially for non-experts in dimensionality reduction. In this work, we\npresent t-viSNE, an interactive tool for the visual exploration of t-SNE\nprojections that enables analysts to inspect different aspects of their\naccuracy and meaning, such as the effects of hyper-parameters, distance and\nneighborhood preservation, densities and costs of specific neighborhoods, and\nthe correlations between dimensions and visual patterns. We propose a coherent,\naccessible, and well-integrated collection of different views for the\nvisualization of t-SNE projections. The applicability and usability of t-viSNE\nare demonstrated through hypothetical usage scenarios with real data sets.\nFinally, we present the results of a user study where the tool's effectiveness\nwas evaluated. By bringing to light information that would normally be lost\nafter running t-SNE, we hope to support analysts in using t-SNE and making its\nresults better understandable.\n""]",t-SNE Visualization and Dimensionality Reduction,Dimensionality Reduction and Data Visualization Techniques,Data Analysis and Visualization,Data Analysis and Visualization
400,17,400_text_stylistic_nlp_styles,"['text', 'stylistic', 'nlp', 'styles', 'sentiment', 'sentences', 'style', 'content', 'politeness', 'tst']","['style', 'transfer', 'formality', 'authorship', 'styles', 'text', 'parallel', 'content', 'stylistic', 'meaning']","['  Text style transfer (TST) is an important task in controllable text\ngeneration, which aims to control selected attributes of language use, such as\npoliteness, formality, or sentiment, without altering the style-independent\ncontent of the text. The field has received considerable research attention in\nrecent years and has already been covered in several reviews, but the focus has\nmostly been on the development of new algorithms and learning from different\ntypes of data (supervised, unsupervised, out-of-domain, etc.) and not so much\non the application side. However, TST-related technologies are gradually\nreaching a production- and deployment-ready level, and therefore, the inclusion\nof the application perspective in TST research becomes crucial. Similarly, the\noften overlooked ethical considerations of TST technology have become a\npressing issue. This paper presents a comprehensive review of TST applications\nthat have been researched over the years, using both traditional linguistic\napproaches and more recent deep learning methods. We discuss current\nchallenges, future research directions, and ethical implications of TST\napplications in text generation. By providing a holistic overview of the\nlandscape of TST applications, we hope to stimulate further research and\ncontribute to a better understanding of the potential as well as ethical\nconsiderations associated with TST.\n', '  Text style transfer (TST) aims to vary the style polarity of text while\npreserving the semantic content. Although recent advancements have demonstrated\nremarkable progress in short TST, it remains a relatively straightforward task\nwith limited practical applications. The more comprehensive long TST task\npresents two challenges: (1) existing methods encounter difficulties in\naccurately evaluating content attributes in multiple words, leading to content\ndegradation; (2) the conventional vanilla style classifier loss encounters\nobstacles in maintaining consistent style across multiple generated sentences.\n  In this paper, we propose a novel method SC2, where a multilayer Joint\nStyle-Content Weighed (JSCW) module and a Style Consistency loss are designed\nto address the two issues. The JSCW simultaneously assesses the amounts of\nstyle and content attributes within a token, aiming to acquire a lossless\ncontent representation and thereby enhancing content preservation. The multiple\nJSCW layers further progressively refine content representations. We design a\nstyle consistency loss to ensure the generated multiple sentences consistently\nreflect the target style polarity. Moreover, we incorporate a denoising\nnon-autoregressive decoder to accelerate the training. We conduct plentiful\nexperiments and the results show significant improvements of SC2 over\ncompetitive baselines. Our code: https://github.com/jiezhao6/SC2.\n', '  Text Style Transfer (TST) is a pivotal task in natural language generation to\nmanipulate text style attributes while preserving style-independent content.\nThe attributes targeted in TST can vary widely, including politeness,\nauthorship, mitigation of offensive language, modification of feelings, and\nadjustment of text formality. TST has become a widely researched topic with\nsubstantial advancements in recent years. This paper provides an introductory\noverview of TST, addressing its challenges, existing approaches, datasets,\nevaluation measures, subtasks, and applications. This fundamental overview\nimproves understanding of the background and fundamentals of text style\ntransfer.\n']",Text Style Transfer in Natural Language Processing,Natural Language Processing for Text Analysis and Generation,Natural Language Processing,Natural Language Processing
401,17,401_entities_entity_embeddings_alignments,"['entities', 'entity', 'embeddings', 'alignments', 'semantic', 'embedding', 'alignment', 'similarities', 'relations', 'subgraph']","['entity', 'entities', 'alignment', 'relation', 'equivalent', 'neighbor', 'graphs', 'embeddings', 'aggregation', 'alignments']","['  The flourishing of knowledge graph applications has driven the need for\nentity alignment (EA) across KGs. However, the heterogeneity of practical KGs,\ncharacterized by differing scales, structures, and limited overlapping\nentities, greatly surpasses that of existing EA datasets. This discrepancy\nhighlights an oversimplified heterogeneity in current EA datasets, which\nobstructs a full understanding of the advancements achieved by recent EA\nmethods. In this paper, we study the performance of EA methods in practical\nsettings, specifically focusing on the alignment of highly heterogeneous KGs\n(HHKGs). Firstly, we address the oversimplified heterogeneity settings of\ncurrent datasets and propose two new HHKG datasets that closely mimic practical\nEA scenarios. Then, based on these datasets, we conduct extensive experiments\nto evaluate previous representative EA methods. Our findings reveal that, in\naligning HHKGs, valuable structure information can hardly be exploited through\nmessage-passing and aggregation mechanisms. This phenomenon leads to inferior\nperformance of existing EA methods, especially those based on GNNs. These\nfindings shed light on the potential problems associated with the conventional\napplication of GNN-based methods as a panacea for all EA datasets.\nConsequently, in light of these observations and to elucidate what EA\nmethodology is genuinely beneficial in practical scenarios, we undertake an\nin-depth analysis by implementing a simple but effective approach: Simple-HHEA.\nThis method adaptly integrates entity name, structure, and temporal information\nto navigate the challenges posed by HHKGs. Our experiment results conclude that\nthe key to the future EA model design in practice lies in their adaptability\nand efficiency to varying information quality conditions, as well as their\ncapability to capture patterns across HHKGs.\n', ""  Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent\nentities across diverse knowledge graphs (KGs) using only a limited number of\nseed alignments. Despite substantial advances in aggregation-based weakly\nsupervised EA, the underlying mechanisms in this setting remain unexplored. In\nthis paper, we present a propagation perspective to analyze weakly supervised\nEA and explain the existing aggregation-based EA models. Our theoretical\nanalysis reveals that these models essentially seek propagation operators for\npairwise entity similarities. We further prove that, despite the structural\nheterogeneity of different KGs, the potentially aligned entities within\naggregation-based EA models have isomorphic subgraphs, which is the core\npremise of EA but has not been investigated. Leveraging this insight, we\nintroduce a potential isomorphism propagation operator to enhance the\npropagation of neighborhood information across KGs. We develop a general EA\nframework, PipEA, incorporating this operator to improve the accuracy of every\ntype of aggregation-based model without altering the learning process.\nExtensive experiments substantiate our theoretical findings and demonstrate\nPipEA's significant performance gains over state-of-the-art weakly supervised\nEA methods. Our work not only advances the field but also enhances our\ncomprehension of aggregation-based weakly supervised EA.\n"", ""  Entity Alignment (EA) aims to match equivalent entities in different\nKnowledge Graphs (KGs), which is essential for knowledge fusion and\nintegration. Recently, embedding-based EA has attracted significant attention\nand many approaches have been proposed. Early approaches primarily focus on\nlearning entity embeddings from the structural features of KGs, defined by\nrelation triples. Later methods incorporated entities' names and attributes as\nauxiliary information to enhance embeddings for EA. However, these approaches\noften used different techniques to encode structural and attribute information,\nlimiting their interaction and mutual enhancement. In this work, we propose a\ndense entity retrieval framework for EA, leveraging language models to\nuniformly encode various features of entities and facilitate nearest entity\nsearch across KGs. Alignment candidates are first generated through entity\nretrieval, which are subsequently reranked to determine the final alignments.\nWe conduct comprehensive experiments on both cross-lingual and monolingual EA\ndatasets, demonstrating that our approach achieves state-of-the-art performance\ncompared to existing EA methods.\n""]",Entity Alignment in Knowledge Graphs,Entity Alignment and Ontology Learning with Semantic Technologies,Entity Understanding and Semantic Knowledge Integration,Entity Understanding and Semantic Knowledge Integration
402,17,402_corrections_grammar_corpus_sentences,"['corrections', 'grammar', 'corpus', 'sentences', 'annotated', 'annotation', 'edits', 'correction', 'fluently', 'evaluation']","['grammatical', 'correction', 'sentences', 'corrections', 'error', 'errors', 'grammar', 'commercial', 'sentence', 'corpus']","['  The paper focuses on improving the interpretability of Grammatical Error\nCorrection (GEC) metrics, which receives little attention in previous studies.\nTo bridge the gap, we propose CLEME2.0, a reference-based evaluation strategy\nthat can describe four elementary dimensions of GEC systems, namely\nhit-correction, error-correction, under-correction, and over-correction. They\ncollectively contribute to revealing the critical characteristics and locating\ndrawbacks of GEC systems. Evaluating systems by Combining these dimensions\nleads to high human consistency over other reference-based and reference-less\nmetrics. Extensive experiments on 2 human judgement datasets and 6 reference\ndatasets demonstrate the effectiveness and robustness of our method. All the\ncodes will be released after the peer review.\n', '  This paper investigates the application of GPT-3.5 for Grammatical Error\nCorrection (GEC) in multiple languages in several settings: zero-shot GEC,\nfine-tuning for GEC, and using GPT-3.5 to re-rank correction hypotheses\ngenerated by other GEC models. In the zero-shot setting, we conduct automatic\nevaluations of the corrections proposed by GPT-3.5 using several methods:\nestimating grammaticality with language models (LMs), the Scribendi test, and\ncomparing the semantic embeddings of sentences. GPT-3.5 has a known tendency to\nover-correct erroneous sentences and propose alternative corrections. For\nseveral languages, such as Czech, German, Russian, Spanish, and Ukrainian,\nGPT-3.5 substantially alters the source sentences, including their semantics,\nwhich presents significant challenges for evaluation with reference-based\nmetrics. For English, GPT-3.5 demonstrates high recall, generates fluent\ncorrections, and generally preserves sentence semantics. However, human\nevaluation for both English and Russian reveals that, despite its strong\nerror-detection capabilities, GPT-3.5 struggles with several error types,\nincluding punctuation mistakes, tense errors, syntactic dependencies between\nwords, and lexical compatibility at the sentence level.\n', '  Thanks to recent advances in generative AI, we are able to prompt large\nlanguage models (LLMs) to produce texts which are fluent and grammatical. In\naddition, it has been shown that we can elicit attempts at grammatical error\ncorrection (GEC) from LLMs when prompted with ungrammatical input sentences. We\nevaluate how well LLMs can perform at GEC by measuring their performance on\nestablished benchmark datasets. We go beyond previous studies, which only\nexamined GPT* models on a selection of English GEC datasets, by evaluating\nseven open-source and three commercial LLMs on four established GEC benchmarks.\nWe investigate model performance and report results against individual error\ntypes. Our results indicate that LLMs do not always outperform supervised\nEnglish GEC models except in specific contexts -- namely commercial LLMs on\nbenchmarks annotated with fluency corrections as opposed to minimal edits. We\nfind that several open-source models outperform commercial ones on minimal edit\nbenchmarks, and that in some settings zero-shot prompting is just as\ncompetitive as few-shot prompting.\n']",Grammatical Error Correction Evaluation,Natural Language Processing for Text Correction and Simplification,Natural Language Processing,Natural Language Processing
403,17,403_knowledge_answering_reasoninglm_graphqa,"['knowledge', 'answering', 'reasoninglm', 'graphqa', 'subgraph', 'textual', 'reasoning', 'graphs', 'answers', 'learnersourced']","['reasoning', 'graph', 'answering', 'question', 'knowledge', 'subgraph', 'answer', 'graphs', 'answers', 'questions']","['  The fusion of language models (LMs) and knowledge graphs (KGs) is widely used\nin commonsense question answering, but generating faithful explanations remains\nchallenging. Current methods often overlook path decoding faithfulness, leading\nto divergence between graph encoder outputs and model predictions. We identify\nconfounding effects and LM-KG misalignment as key factors causing spurious\nexplanations. To address this, we introduce the LM-KG Fidelity metric to assess\nKG representation reliability and propose the LM-KG Distribution-aware\nAlignment (\\textit{LKDA}) algorithm to improve explanation faithfulness.\nWithout ground truth, we evaluate KG explanations using the proposed\nFidelity-Sparsity Trade-off Curve. Experiments on CommonsenseQA and OpenBookQA\nshow that LKDA significantly enhances explanation fidelity and model\nperformance, highlighting the need to address distributional misalignment for\nreliable commonsense reasoning.\n', ""  Knowledge Graph (KG) powered question answering (QA) performs complex\nreasoning over language semantics as well as knowledge facts. Graph Neural\nNetworks (GNNs) learn to aggregate information from the underlying KG, which is\ncombined with Language Models (LMs) for effective reasoning with the given\nquestion. However, GNN-based methods for QA rely on the graph information of\nthe candidate answer nodes, which limits their effectiveness in more\nchallenging settings where critical answer information is not included in the\nKG. We propose a simple graph pooling approach that learns useful semantics of\nthe KG that can aid the LM's reasoning and that its effectiveness is robust\nunder graph perturbations. Our method, termed SemPool, represents KG facts with\npre-trained LMs, learns to aggregate their semantic information, and fuses it\nat different layers of the LM. Our experimental results show that SemPool\noutperforms state-of-the-art GNN-based methods by 2.27% accuracy points on\naverage when answer information is missing from the KG. In addition, SemPool\noffers interpretability on what type of graph information is fused at different\nLM layers.\n"", '  Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form\nof triplets (head, relation, tail), which collectively form a graph. Question\nAnswering over KGs (KGQA) is the task of answering natural questions grounding\nthe reasoning to the information provided by the KG. Large Language Models\n(LLMs) are the state-of-the-art models for QA tasks due to their remarkable\nability to understand natural language. On the other hand, Graph Neural\nNetworks (GNNs) have been widely used for KGQA as they can handle the complex\ngraph information stored in the KG. In this work, we introduce GNN-RAG, a novel\nmethod for combining language understanding abilities of LLMs with the\nreasoning abilities of GNNs in a retrieval-augmented generation (RAG) style.\nFirst, a GNN reasons over a dense KG subgraph to retrieve answer candidates for\na given question. Second, the shortest paths in the KG that connect question\nentities and answer candidates are extracted to represent KG reasoning paths.\nThe extracted paths are verbalized and given as input for LLM reasoning with\nRAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to\nextract useful graph information, while the LLM leverages its natural language\nprocessing ability for ultimate KGQA. Furthermore, we develop a retrieval\naugmentation (RA) technique to further boost KGQA performance with GNN-RAG.\nExperimental results show that GNN-RAG achieves state-of-the-art performance in\ntwo widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching\nGPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop\nand multi-entity questions outperforming competing approaches by 8.9--15.5%\npoints at answer F1.\n']",Knowledge Graph Question Answering,Question Answering and Reasoning Systems,Artificial Intelligence and Reasoning Systems,Intelligent Systems
403,17,403_knowledge_answering_reasoninglm_graphqa,"['knowledge', 'answering', 'reasoninglm', 'graphqa', 'subgraph', 'textual', 'reasoning', 'graphs', 'answers', 'learnersourced']","['reasoning', 'graph', 'answering', 'question', 'knowledge', 'subgraph', 'answer', 'graphs', 'answers', 'questions']","['  The fusion of language models (LMs) and knowledge graphs (KGs) is widely used\nin commonsense question answering, but generating faithful explanations remains\nchallenging. Current methods often overlook path decoding faithfulness, leading\nto divergence between graph encoder outputs and model predictions. We identify\nconfounding effects and LM-KG misalignment as key factors causing spurious\nexplanations. To address this, we introduce the LM-KG Fidelity metric to assess\nKG representation reliability and propose the LM-KG Distribution-aware\nAlignment (\\textit{LKDA}) algorithm to improve explanation faithfulness.\nWithout ground truth, we evaluate KG explanations using the proposed\nFidelity-Sparsity Trade-off Curve. Experiments on CommonsenseQA and OpenBookQA\nshow that LKDA significantly enhances explanation fidelity and model\nperformance, highlighting the need to address distributional misalignment for\nreliable commonsense reasoning.\n', ""  Knowledge Graph (KG) powered question answering (QA) performs complex\nreasoning over language semantics as well as knowledge facts. Graph Neural\nNetworks (GNNs) learn to aggregate information from the underlying KG, which is\ncombined with Language Models (LMs) for effective reasoning with the given\nquestion. However, GNN-based methods for QA rely on the graph information of\nthe candidate answer nodes, which limits their effectiveness in more\nchallenging settings where critical answer information is not included in the\nKG. We propose a simple graph pooling approach that learns useful semantics of\nthe KG that can aid the LM's reasoning and that its effectiveness is robust\nunder graph perturbations. Our method, termed SemPool, represents KG facts with\npre-trained LMs, learns to aggregate their semantic information, and fuses it\nat different layers of the LM. Our experimental results show that SemPool\noutperforms state-of-the-art GNN-based methods by 2.27% accuracy points on\naverage when answer information is missing from the KG. In addition, SemPool\noffers interpretability on what type of graph information is fused at different\nLM layers.\n"", '  Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form\nof triplets (head, relation, tail), which collectively form a graph. Question\nAnswering over KGs (KGQA) is the task of answering natural questions grounding\nthe reasoning to the information provided by the KG. Large Language Models\n(LLMs) are the state-of-the-art models for QA tasks due to their remarkable\nability to understand natural language. On the other hand, Graph Neural\nNetworks (GNNs) have been widely used for KGQA as they can handle the complex\ngraph information stored in the KG. In this work, we introduce GNN-RAG, a novel\nmethod for combining language understanding abilities of LLMs with the\nreasoning abilities of GNNs in a retrieval-augmented generation (RAG) style.\nFirst, a GNN reasons over a dense KG subgraph to retrieve answer candidates for\na given question. Second, the shortest paths in the KG that connect question\nentities and answer candidates are extracted to represent KG reasoning paths.\nThe extracted paths are verbalized and given as input for LLM reasoning with\nRAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to\nextract useful graph information, while the LLM leverages its natural language\nprocessing ability for ultimate KGQA. Furthermore, we develop a retrieval\naugmentation (RA) technique to further boost KGQA performance with GNN-RAG.\nExperimental results show that GNN-RAG achieves state-of-the-art performance in\ntwo widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching\nGPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop\nand multi-entity questions outperforming competing approaches by 8.9--15.5%\npoints at answer F1.\n']",Knowledge Graph Question Answering,Question Answering and Reasoning Systems,Artificial Intelligence and Reasoning Systems,Intelligent Systems
404,17,404_queries_dbmss_dbms_optimizers,"['queries', 'dbmss', 'dbms', 'optimizers', 'databases', 'sql', 'optimizer', 'database', 'query', 'rdbms']","['query', 'database', 'workload', 'workloads', 'plans', 'queries', 'plan', 'execution', 'optimizers', 'optimizer']","['  Query optimizers in relational database management systems (RDBMSs) search\nfor execution plans expected to be optimal for a given queries. They use\nparameter estimates, often inaccurate, and make assumptions that may not hold\nin practice. Consequently, they may select execution plans that are suboptimal\nat runtime, when these estimates and assumptions are not valid, which may\nresult in poor query performance. Therefore, query optimizers do not\nsufficiently support robust query optimization. Recent years have seen a surge\nof interest in using machine learning (ML) to improve efficiency of data\nsystems and reduce their maintenance overheads, with promising results obtained\nin the area of query optimization in particular. In this paper, inspired by\nthese advancements, and based on several years of experience of IBM Db2 in this\njourney, we propose Robust Optimization of Queries, (Roq), a holistic framework\nthat enables robust query optimization based on a risk-aware learning approach.\nRoq includes a novel formalization of the notion of robustness in the context\nof query optimization and a principled approach for its quantification and\nmeasurement based on approximate probabilistic ML. It also includes novel\nstrategies and algorithms for query plan evaluation and selection. Roq also\nincludes a novel learned cost model that is designed to predict query execution\ncost and the associated risks and performs query optimization accordingly. We\ndemonstrate experimentally that Roq provides significant improvements to robust\nquery optimization compared to the state-of-the-art.\n', ""  Query optimization in relational database management systems (DBMSs) is\ncritical for fast query processing. The query optimizer relies on precise\nselectivity and cost estimates to effectively optimize queries prior to\nexecution. While this strategy is effective for relational DBMSs, it is not\nsufficient for DBMSs tailored for processing machine learning (ML) queries. In\nML-centric DBMSs, query optimization is challenging for two reasons. First, the\nperformance bottleneck of the queries shifts to user-defined functions (UDFs)\nthat often wrap around deep learning models, making it difficult to accurately\nestimate UDF statistics without profiling the query. This leads to inaccurate\nstatistics and sub-optimal query plans. Second, the optimal query plan for ML\nqueries is data-dependent, necessitating DBMSs to adapt the query plan on the\nfly during execution. So, a static query plan is not sufficient for such\nqueries.\n  In this paper, we present Hydro, an ML-centric DBMS that utilizes adaptive\nquery processing (AQP) for efficiently processing ML queries. Hydro is designed\nto quickly evaluate UDF-based query predicates by ensuring optimal predicate\nevaluation order and improving the scalability of UDF execution. By integrating\nAQP, Hydro continuously monitors UDF statistics, routes data to predicates in\nan optimal order, and dynamically allocates resources for evaluating\npredicates. We demonstrate Hydro's efficacy through four illustrative use\ncases, delivering up to 11.52x speedup over a baseline system.\n"", '  Modern database systems rely on cost-based query optimizers to come up with\ngood execution plans for input queries. Such query optimizers rely on cost\nmodels to estimate the costs of candidate query execution plans. A cost model\nrepresents a function from a set of cost units to query execution cost, where\neach cost unit specifies the unit cost of executing a certain type of query\nprocessing operation (such as table scan or join). These cost units are\ntraditionally viewed as constants, whose values only depend on the platform\nconfiguration where the database system runs on top of but are invariant for\nqueries processed by the database system. In this paper, we challenge this\nclassic view by thinking of these cost units as variables instead. We show\nthat, by varying the cost-unit values one can obtain query plans that\nsignificantly outperform the default query plans returned by the query\noptimizer when viewing the cost units as constants. We term this cost-unit\ntuning process ""query tuning"" (QT) and show that it is similar to the\nwell-known hyper-parameter optimization (HPO) problem in AutoML. As a result,\nany state-of-the-art HPO technologies can be applied to QT. We study the QT\nproblem in the context of anytime tuning, which is desirable in practice by\nconstraining the total time spent on QT within a given budget -- we call this\nproblem budget-aware query tuning. We further extend our study from tuning a\nsingle query to tuning a workload with multiple queries, and we call this\ngeneralized problem budget-aware workload tuning (WT), which aims for\nminimizing the execution time of the entire workload. WT is more challenging as\none needs to further prioritize individual query tuning within the given time\nbudget. We propose solutions to both QT and WT and experimental evaluation\nusing both benchmark and real workloads demonstrates the efficacy of our\nproposed solutions.\n']",Query Optimization in Database Management Systems,Optimization and Management of Computing Resources and Information Systems,Optimization and Management of Complex Systems,Optimization and Decision Making in Complex Systems
405,16,405_modeling_dynamics_nonlinear_odes,"['modeling', 'dynamics', 'nonlinear', 'odes', 'dynamical', 'autoencoder', 'differential', 'dimensionally', 'dimensional', 'pdes']","['equations', 'dynamics', 'latent', 'nonlinear', 'differential', 'variables', 'algebraic', 'autoencoder', 'identification', 'ordinary']","['  Data-driven modeling of dynamical systems often faces numerous data-related\nchallenges. A fundamental requirement is the existence of a unique set of\nparameters for a chosen model structure, an issue commonly referred to as\nidentifiability. Although this problem is well studied for ordinary\ndifferential equations (ODEs), few studies have focused on the more general\nclass of systems described by differential-algebraic equations (DAEs). Examples\nof DAEs include dynamical systems with algebraic equations representing\nconservation laws or approximating fast dynamics. This work introduces a novel\nidentifiability test for models characterized by nonlinear DAEs. Unlike\nprevious approaches, our test only requires prior knowledge of the system\nequations and does not need nonlinear transformation, index reduction, or\nnumerical integration of the DAEs. We employed our identifiability analysis\nacross a diverse range of DAE models, illustrating how system identifiability\ndepends on the choices of sensors, experimental conditions, and model\nstructures. Given the added challenges involved in identifying DAEs when\ncompared to ODEs, we anticipate that our findings will have broad applicability\nand contribute significantly to the development and validation of data-driven\nmethods for DAEs and other structure-preserving models.\n', '  The simulation of many complex phenomena in engineering and science requires\nsolving expensive, high-dimensional systems of partial differential equations\n(PDEs). To circumvent this, reduced-order models (ROMs) have been developed to\nspeed up computations. However, when governing equations are unknown or\npartially known, typically ROMs lack interpretability and reliability of the\npredicted solutions.\n  In this work we present a data-driven, non-intrusive framework for building\nROMs where the latent variables and dynamics are identified in an interpretable\nmanner and uncertainty is quantified. Starting from a limited amount of\nhigh-dimensional, noisy data the proposed framework constructs an efficient ROM\nby leveraging variational autoencoders for dimensionality reduction along with\na newly introduced, variational version of sparse identification of nonlinear\ndynamics (SINDy), which we refer to as Variational Identification of Nonlinear\nDynamics (VINDy).\n  In detail, the method consists of Variational Encoding of Noisy Inputs (VENI)\nto identify the distribution of reduced coordinates. Simultaneously, we learn\nthe distribution of the coefficients of a pre-determined set of candidate\nfunctions by VINDy. Once trained offline, the identified model can be queried\nfor new parameter instances and new initial conditions to compute the\ncorresponding full-time solutions. The probabilistic setup enables uncertainty\nquantification as the online testing consists of Variational Inference\nnaturally providing Certainty Intervals (VICI). In this work we showcase the\neffectiveness of the newly proposed VINDy method in identifying interpretable\nand accurate dynamical system for the R\\""ossler system with different noise\nintensities and sources. Then the performance of the overall method - named\nVENI, VINDy, VICI - is tested on PDE benchmarks including structural mechanics\nand fluid dynamics.\n', '  Numerical solvers of partial differential equations (PDEs) have been widely\nemployed for simulating physical systems. However, the computational cost\nremains a major bottleneck in various scientific and engineering applications,\nwhich has motivated the development of reduced-order models (ROMs). Recently,\nmachine-learning-based ROMs have gained significant popularity and are\npromising for addressing some limitations of traditional ROM methods,\nespecially for advection dominated systems. In this chapter, we focus on a\nparticular framework known as Latent Space Dynamics Identification (LaSDI),\nwhich transforms the high-fidelity data, governed by a PDE, to simpler and\nlow-dimensional latent-space data, governed by ordinary differential equations\n(ODEs). These ODEs can be learned and subsequently interpolated to make ROM\npredictions. Each building block of LaSDI can be easily modulated depending on\nthe application, which makes the LaSDI framework highly flexible. In\nparticular, we present strategies to enforce the laws of thermodynamics into\nLaSDI models (tLaSDI), enhance robustness in the presence of noise through the\nweak form (WLaSDI), select high-fidelity training data efficiently through\nactive learning (gLaSDI, GPLaSDI), and quantify the ROM prediction uncertainty\nthrough Gaussian processes (GPLaSDI). We demonstrate the performance of\ndifferent LaSDI approaches on Burgers equation, a non-linear heat conduction\nproblem, and a plasma physics problem, showing that LaSDI algorithms can\nachieve relative errors of less than a few percent and up to thousands of times\nspeed-ups.\n']",Data-driven Modeling of Dynamical Systems,Advanced Modeling of Temporal and Dynamical Systems,Machine Learning for Dynamical Systems and Differential Equations,Machine Learning for Dynamical Systems and Differential Equations
406,16,406_reinforcement_cooperation_rewards_cooperative,"['reinforcement', 'cooperation', 'rewards', 'cooperative', 'games', 'agents', 'incentives', 'emergence', 'agent', 'game']","['cooperation', 'opponent', 'games', 'cooperative', 'agents', 'punishment', 'game', 'agent', 'dilemmas', 'reinforcement']","[""  The significance of network structures in promoting group cooperation within\nsocial dilemmas has been widely recognized. Prior studies attribute this\nfacilitation to the assortment of strategies driven by spatial interactions.\nAlthough reinforcement learning has been employed to investigate the impact of\ndynamic interaction on the evolution of cooperation, there remains a lack of\nunderstanding about how agents develop neighbour selection behaviours and the\nformation of strategic assortment within an explicit interaction structure. To\naddress this, our study introduces a computational framework based on\nmulti-agent reinforcement learning in the spatial Prisoner's Dilemma game. This\nframework allows agents to select dilemma strategies and interacting neighbours\nbased on their long-term experiences, differing from existing research that\nrelies on preset social norms or external incentives. By modelling each agent\nusing two distinct Q-networks, we disentangle the coevolutionary dynamics\nbetween cooperation and interaction. The results indicate that long-term\nexperience enables agents to develop the ability to identify non-cooperative\nneighbours and exhibit a preference for interaction with cooperative ones. This\nemergent self-organizing behaviour leads to the clustering of agents with\nsimilar strategies, thereby increasing network reciprocity and enhancing group\ncooperation.\n"", ""  Understanding the emergence of cooperation in systems of computational agents\nis crucial for the development of effective cooperative AI. Interaction among\nindividuals in real-world settings are often sparse and occur within a broad\nspectrum of incentives, which often are only partially known. In this work, we\nexplore how cooperation can arise among reinforcement learning agents in\nscenarios characterised by infrequent encounters, and where agents face\nuncertainty about the alignment of their incentives with those of others. To do\nso, we train the agents under a wide spectrum of environments ranging from\nfully competitive, to fully cooperative, to mixed-motives. Under this type of\nuncertainty we study the effects of mechanisms, such as reputation and\nintrinsic rewards, that have been proposed in the literature to foster\ncooperation in mixed-motives environments. Our findings show that uncertainty\nsubstantially lowers the agents' ability to engage in cooperative behaviour,\nwhen that would be the best course of action. In this scenario, the use of\neffective reputation mechanisms and intrinsic rewards boosts the agents'\ncapability to act nearly-optimally in cooperative environments, while greatly\nenhancing cooperation in mixed-motive environments as well.\n"", ""  Cooperation is fundamental in Multi-Agent Systems (MAS) and Multi-Agent\nReinforcement Learning (MARL), often requiring agents to balance individual\ngains with collective rewards. In this regard, this paper aims to investigate\nstrategies to invoke cooperation in game-theoretic scenarios, namely the\nIterated Prisoner's Dilemma, where agents must optimize both individual and\ngroup outcomes. Existing cooperative strategies are analyzed for their\neffectiveness in promoting group-oriented behavior in repeated games.\nModifications are proposed where encouraging group rewards will also result in\na higher individual gain, addressing real-world dilemmas seen in distributed\nsystems. The study extends to scenarios with exponentially growing agent\npopulations ($N \\longrightarrow +\\infty$), where traditional computation and\nequilibrium determination are challenging. Leveraging mean-field game theory,\nequilibrium solutions and reward structures are established for infinitely\nlarge agent sets in repeated games. Finally, practical insights are offered\nthrough simulations using the Multi Agent-Posthumous Credit Assignment trainer,\nand the paper explores adapting simulation algorithms to create scenarios\nfavoring cooperation for group rewards. These practical implementations bridge\ntheoretical concepts with real-world applications.\n""]",Cooperation in Multi-Agent Reinforcement Learning,Multi-Agent Decision Making and Reinforcement Learning,Multi-Agent Systems and Artificial Intelligence,Multi-Agent Systems and Artificial Intelligence
407,16,407_astroinformatics_astronomy_astronomical_astrophysical,"['astroinformatics', 'astronomy', 'astronomical', 'astrophysical', 'astropt', 'galaxies', 'astrophysics', 'astrollama', 'telescopes', 'universetbd']","['astronomy', 'astroinformatics', 'webinar', 'scientific', 'astronomical', 'presidency', 'universe', 'panels', 'abstracts', 'science']","['  Policy Brief on ""Long Term Space Data and Informatics Needs"", distilled from\nthe corresponding panel that was part of the discussions during S20 Policy\nWebinar on Astroinformatics for Sustainable Development held on 6-7 July 2023.\n  Persistent space data gathering, retention, transmission, and analysis play a\npivotal role in deepening our grasp of the Universe and fostering the\nachievement of global sustainable development goals. Long-term data storage and\ncuration is crucial not only to make the wide range of burgeoning data sets\navailable to the global science community, but also to stabilize those data\nsets, enabling new science in the future to analyse long-term trends over\nunprecedented time spans. In addition to this, over the long-term, the\nimperative to store all data on the ground should be ameliorated by use of\nspace-based data stores --maintained and seen to be as reliable as any other\ndata archive. This concept is sometimes referred to as Memory of the Sky.\nStoring the data must be accompanied by the ability to analyse them. Several\nconcepts covered below acknowledge roots and inspiration based in the Virtual\nObservatory effort. Within this policy document, we delve into the complexities\nsurrounding the long-term utilization of space data and informatics, shedding\nlight on the challenges and opportunities inherent in this endeavour. Further,\nwe present a series of pragmatic recommendations designed to address these\nchallenges proactively.\n  The policy webinar took place during the G20 presidency in India (2023). A\nsummary based on the seven panels can be found here: arxiv:2401.04623.\n', '  Policy Brief on ""AstroInformatics, Recommendations for Global Collaboration"",\ndistilled from panel discussions during S20 Policy Webinar on Astroinformatics\nfor Sustainable Development held on 6-7 July 2023.\n  The deliberations encompassed a wide array of topics, including broad\nastroinformatics, sky surveys, large-scale international initiatives, global\ndata repositories, space-related data, regional and international collaborative\nefforts, as well as workforce development within the field. These discussions\ncomprehensively addressed the current status, notable achievements, and the\nmanifold challenges that the field of astroinformatics currently confronts.\n  The G20 nations present a unique opportunity due to their abundant human and\ntechnological capabilities, coupled with their widespread geographical\nrepresentation. Leveraging these strengths, significant strides can be made in\nvarious domains. These include, but are not limited to, the advancement of STEM\neducation and workforce development, the promotion of equitable resource\nutilization, and contributions to fields such as Earth Science and Climate\nScience.\n  We present a concise overview, followed by specific recommendations that\npertain to both ground-based and space data initiatives. Our team remains\nreadily available to furnish further elaboration on any of these proposals as\nrequired. Furthermore, we anticipate further engagement during the upcoming G20\npresidencies in Brazil (2024) and South Africa (2025) to ensure the continued\ndiscussion and realization of these objectives.\n  The policy webinar took place during the G20 presidency in India (2023).\nNotes based on the seven panels will be separately published.\n', '  Policy Brief on ""Global Data in Astronomy: Challenges and Opportunities"",\ndistilled from the corresponding panel that was part of the discussions during\nS20 Policy Webinar on Astroinformatics for Sustainable Development held on 6-7\nJuly 2023.\n  Astronomy is increasingly becoming a data-driven science. Advances in our\nunderstanding of the physical mechanisms at work in the Universe require\nbuilding ever-more sensitive telescopes to gather observations of the cosmos to\ntest and advance our theoretical models of how the universe works. To confront\nthe observed data with our theoretical models we require data hosting,\narchiving and storage and high-performance computing resources to run the\ntheoretical calculations and compare our simulated and observed universe. We\nalso require the sophisticated development of highly skilled human resources.\nNewer large projects are often run through international collaborations and\npartnerships, driving a need for \'open science\' and collaborative structure\nacross national boundaries. While astronomical data are useful scientifically,\nthe data do not come with the same ethical/privacy-related restrictions as\nmedical/biological data. Moreover, the ability to use data for new scientific\nanalysis extends and expands the impact and reach of scientific surveys -- this\nis a strength that national funding agencies should capitalize on. We discuss\nthe management and analysis of such large volumes of data and the corresponding\nsignificant challenges that require policy-level preparations.\n  The policy webinar took place during the G20 presidency in India (2023). A\nsummary based on the seven panels can be found here: arxiv:2401.04623.\n']",Astroinformatics for Sustainable Development,Astroinformatics and Cosmology,Astrophysics and Astronomy Informatics,Astrophysics and Astronomy Informatics
408,16,408_forgetting_continual_graphs_learning,"['forgetting', 'continual', 'graphs', 'learning', 'memory', 'graph', 'forget', 'nodes', 'replaying', 'continually']","['continual', 'replay', 'graph', 'incremental', 'memory', 'topological', 'nodes', 'buffer', 'catastrophic', 'forgetting']","[""  Continual learning (CL) is the research field that aims to build machine\nlearning models that can accumulate knowledge continuously over different tasks\nwithout retraining from scratch. Previous studies have shown that pre-training\ngraph neural networks (GNN) may lead to negative transfer (Hu et al., 2020)\nafter fine-tuning, a setting which is closely related to CL. Thus, we focus on\nstudying GNN in the continual graph learning (CGL) setting. We propose the\nfirst continual graph learning benchmark for spatio-temporal graphs and use it\nto benchmark well-known CGL methods in this novel setting. The benchmark is\nbased on the N-UCLA and NTU-RGB+D datasets for skeleton-based action\nrecognition. Beyond benchmarking for standard performance metrics, we study the\nclass and task-order sensitivity of CGL methods, i.e., the impact of learning\norder on each class/task's performance, and the architectural sensitivity of\nCGL methods with backbone GNN at various widths and depths. We reveal that\ntask-order robust methods can still be class-order sensitive and observe\nresults that contradict previous empirical observations on architectural\nsensitivity in CL.\n"", '  When handling streaming graphs, existing graph representation learning models\nencounter a catastrophic forgetting problem, where previously learned knowledge\nof these models is easily overwritten when learning with newly incoming graphs.\nIn response, Continual Graph Learning (CGL) emerges as a novel paradigm\nenabling graph representation learning from streaming graphs. Our prior work,\nCondense and Train (CaT) is a replay-based CGL framework with a balanced\ncontinual learning procedure, which designs a small yet effective memory bankn\nfor replaying. Although the CaT alleviates the catastrophic forgetting problem,\nthere exist three issues: (1) The graph condensation only focuses on labelled\nnodes while neglecting abundant information carried by unlabelled nodes; (2)\nThe continual training scheme of the CaT overemphasises on the previously\nlearned knowledge, limiting the model capacity to learn from newly added\nmemories; (3) Both the condensation process and replaying process of the CaT\nare time-consuming. In this paper, we propose a PsUdo-label guided Memory bAnk\n(PUMA) CGL framework, extending from the CaT to enhance its efficiency and\neffectiveness by overcoming the above-mentioned weaknesses and limits. To fully\nexploit the information in a graph, PUMA expands the coverage of nodes during\ngraph condensation with both labelled and unlabelled nodes. Furthermore, a\ntraining-from-scratch strategy is proposed to upgrade the previous continual\nlearning scheme for a balanced training between the historical and the new\ngraphs. Besides, PUMA uses a one-time prorogation and wide graph encoders to\naccelerate the graph condensation and the graph encoding process in the\ntraining stage to improve the efficiency of the whole framework. Extensive\nexperiments on six datasets for the node classification task demonstrate the\nstate-of-the-art performance and efficiency over existing methods.\n', '  Continual learning on graph data has recently attracted paramount attention\nfor its aim to resolve the catastrophic forgetting problem on existing tasks\nwhile adapting the sequentially updated model to newly emerged graph tasks.\nWhile there have been efforts to summarize progress on continual learning\nresearch over Euclidean data, e.g., images and texts, a systematic review of\nprogress in continual learning on graphs, a.k.a, continual graph learning (CGL)\nor lifelong graph learning, is still demanding. Graph data are far more complex\nin terms of data structures and application scenarios, making CGL task\nsettings, model designs, and applications extremely challenging. To bridge the\ngap, we provide a comprehensive review of existing continual graph learning\n(CGL) algorithms by elucidating the different task settings and categorizing\nthe existing methods based on their characteristics. We compare the CGL methods\nwith traditional continual learning techniques and analyze the applicability of\nthe traditional continual learning techniques to CGL tasks. Additionally, we\nreview the benchmark works that are crucial to CGL research. Finally, we\ndiscuss the remaining challenges and propose several future directions. We will\nmaintain an up-to-date GitHub repository featuring a comprehensive list of CGL\nalgorithms, accessible at\nhttps://github.com/UConn-DSIS/Survey-of-Continual-Learning-on-Graphs.\n']",Continual Graph Learning,Continual Learning and Catastrophic Forgetting,Machine Learning Adaptation and Forgetting,Machine Learning Adaptation and Forgetting
409,16,409_probabilistic_inferences_bayesian_circuits,"['probabilistic', 'inferences', 'bayesian', 'circuits', 'inference', 'inceptionpcs', 'learnspn', 'tractability', 'computing', 'tractable']","['probabilistic', 'circuits', 'tractable', 'circuit', 'tractability', 'variables', 'playlist', 'expressive', 'polynomial', 'probability']","['  Probabilistic models based on continuous latent spaces, such as variational\nautoencoders, can be understood as uncountable mixture models where components\ndepend continuously on the latent code. They have proven to be expressive tools\nfor generative and probabilistic modelling, but are at odds with tractable\nprobabilistic inference, that is, computing marginals and conditionals of the\nrepresented probability distribution. Meanwhile, tractable probabilistic models\nsuch as probabilistic circuits (PCs) can be understood as hierarchical discrete\nmixture models, and thus are capable of performing exact inference efficiently\nbut often show subpar performance in comparison to continuous latent-space\nmodels. In this paper, we investigate a hybrid approach, namely continuous\nmixtures of tractable models with a small latent dimension. While these models\nare analytically intractable, they are well amenable to numerical integration\nschemes based on a finite set of integration points. With a large enough number\nof integration points the approximation becomes de-facto exact. Moreover, for a\nfinite set of integration points, the integration method effectively compiles\nthe continuous mixture into a standard PC. In experiments, we show that this\nsimple scheme proves remarkably effective, as PCs learnt this way set new state\nof the art for tractable models on many standard density estimation benchmarks.\n', '  We present a comprehensive survey of the advancements and techniques in the\nfield of tractable probabilistic generative modeling, primarily focusing on\nProbabilistic Circuits (PCs). We provide a unified perspective on the inherent\ntrade-offs between expressivity and tractability, highlighting the design\nprinciples and algorithmic extensions that have enabled building expressive and\nefficient PCs, and provide a taxonomy of the field. We also discuss recent\nefforts to build deep and hybrid PCs by fusing notions from deep neural models,\nand outline the challenges and open questions that can guide future research in\nthis evolving field.\n', '  Zhang et al. (ICML 2021, PLMR 139, pp. 12447-1245) introduced probabilistic\ngenerating circuits (PGCs) as a probabilistic model to unify probabilistic\ncircuits (PCs) and determinantal point processes (DPPs). At a first glance,\nPGCs store a distribution in a very different way, they compute the probability\ngenerating polynomial instead of the probability mass function and it seems\nthat this is the main reason why PGCs are more powerful than PCs or DPPs.\nHowever, PGCs also allow for negative weights, whereas classical PCs assume\nthat all weights are nonnegative. One of the main insights of our paper is that\nthe negative weights are responsible for the power of PGCs and not the\ndifferent representation. PGCs are PCs in disguise, in particular, we show how\nto transform any PGC into a PC with negative weights with only polynomial\nblowup.\n  PGCs were defined by Zhang et al. only for binary random variables. As our\nsecond main result, we show that there is a good reason for this: we prove that\nPGCs for categorial variables with larger image size do not support tractable\nmarginalization unless NP = P. On the other hand, we show that we can model\ncategorial variables with larger image size as PC with negative weights\ncomputing set-multilinear polynomials. These allow for tractable\nmarginalization. In this sense, PCs with negative weights strictly subsume\nPGCs.\n']",Probabilistic Circuits and Tractable Models,Probabilistic Modeling and Inference,Probabilistic Machine Learning,Probabilistic Machine Learning
410,16,410_geolocalization_geolocators_geolocate_geolocating,"['geolocalization', 'geolocators', 'geolocate', 'geolocating', 'geolocation', 'geoclip', 'geo', 'geospatial', 'geoestimation', 'geographic']","['geolocalization', 'location', 'geolocation', 'street', 'country', 'image', 'images', 'view', 'retrieval', 'geographic']","['  Geolocating images of a ground-level scene entails estimating the location on\nEarth where the picture was taken, in absence of GPS or other location\nmetadata. Typically, methods are evaluated by measuring the Great Circle\nDistance (GCD) between a predicted location and ground truth. However, this\nmeasurement is limited because it only evaluates a single point, not estimates\nof regions or score heatmaps. This is especially important in applications to\nrural, wilderness and under-sampled areas, where finding the exact location may\nnot be possible, and when used in aggregate systems that progressively narrow\ndown locations.\n  In this paper, we introduce a novel metric, Recall vs Area (RvA), which\nmeasures the accuracy of estimated distributions of locations. RvA treats image\ngeolocation results similarly to document retrieval, measuring recall as a\nfunction of area: For a ranked list of (possibly non-contiguous) predicted\nregions, we measure the accumulated area required for the region to contain the\nground truth coordinate. This produces a curve similar to a precision-recall\ncurve, where ""precision"" is replaced by square kilometers area, allowing\nevaluation of performance for different downstream search area budgets.\n  Following directly from this view of the problem, we then examine a simple\nensembling approach to global-scale image geolocation, which incorporates\ninformation from multiple sources to help address domain shift, and can readily\nincorporate multiple models, attribute predictors, and data sources. We study\nits effectiveness by combining the geolocation models GeoEstimation and the\ncurrent SOTA GeoCLIP, with attribute predictors based on ORNL LandScan and\nESA-CCI Land Cover. We find significant improvements in image geolocation for\nareas that are under-represented in the training set, particularly non-urban\nareas, on both Im2GPS3k and Street View images.\n', '  Worldwide geolocalization aims to locate the precise location at the\ncoordinate level of photos taken anywhere on the Earth. It is very challenging\ndue to 1) the difficulty of capturing subtle location-aware visual semantics,\nand 2) the heterogeneous geographical distribution of image data. As a result,\nexisting studies have clear limitations when scaled to a worldwide context.\nThey may easily confuse distant images with similar visual contents, or cannot\nadapt to various locations worldwide with different amounts of relevant data.\nTo resolve these limitations, we propose G3, a novel framework based on\nRetrieval-Augmented Generation (RAG). In particular, G3 consists of three\nsteps, i.e., Geo-alignment, Geo-diversification, and Geo-verification to\noptimize both retrieval and generation phases of worldwide geolocalization.\nDuring Geo-alignment, our solution jointly learns expressive multi-modal\nrepresentations for images, GPS and textual descriptions, which allows us to\ncapture location-aware semantics for retrieving nearby images for a given\nquery. During Geo-diversification, we leverage a prompt ensembling method that\nis robust to inconsistent retrieval performance for different image queries.\nFinally, we combine both retrieved and generated GPS candidates in\nGeo-verification for location prediction. Experiments on two well-established\ndatasets IM2GPS3k and YFCC4k verify the superiority of G3 compared to other\nstate-of-the-art methods.\n', ""  Planet-scale image geolocalization remains a challenging problem due to the\ndiversity of images originating from anywhere in the world. Although approaches\nbased on vision transformers have made significant progress in geolocalization\naccuracy, success in prior literature is constrained to narrow distributions of\nimages of landmarks, and performance has not generalized to unseen places. We\npresent a new geolocalization system that combines semantic geocell creation,\nmulti-task contrastive pretraining, and a novel loss function. Additionally,\nour work is the first to perform retrieval over location clusters for guess\nrefinements. We train two models for evaluations on street-level data and\ngeneral-purpose image geolocalization; the first model, PIGEON, is trained on\ndata from the game of Geoguessr and is capable of placing over 40% of its\nguesses within 25 kilometers of the target location globally. We also develop a\nbot and deploy PIGEON in a blind experiment against humans, ranking in the top\n0.01% of players. We further challenge one of the world's foremost professional\nGeoguessr players to a series of six matches with millions of viewers, winning\nall six games. Our second model, PIGEOTTO, differs in that it is trained on a\ndataset of images from Flickr and Wikipedia, achieving state-of-the-art results\non a wide range of image geolocalization benchmarks, outperforming the previous\nSOTA by up to 7.7 percentage points on the city accuracy level and up to 38.8\npercentage points on the country level. Our findings suggest that PIGEOTTO is\nthe first image geolocalization model that effectively generalizes to unseen\nplaces and that our approach can pave the way for highly accurate, planet-scale\nimage geolocalization systems. Our code is available on GitHub.\n""]",Image Geolocalization Techniques,Geospatial Information Retrieval and Analysis,Information Retrieval and Knowledge Systems,Information Retrieval and Knowledge Systems
411,16,411_audio_interaural_recordings_binaural,"['audio', 'interaural', 'recordings', 'binaural', 'acoustical', 'soundscape', 'auditory', 'microphone', 'acoustic', 'microphones']","['sound', 'acoustic', 'room', 'binaural', 'reverberation', 'audio', 'reverberant', 'localization', 'rooms', 'microphones']","[""  In the Sound Event Localization and Detection (SELD) task, Transformer-based\nmodels have demonstrated impressive capabilities. However, the quadratic\ncomplexity of the Transformer's self-attention mechanism results in\ncomputational inefficiencies. In this paper, we propose a network architecture\nfor SELD called SELD-Mamba, which utilizes Mamba, a selective state-space\nmodel. We adopt the Event-Independent Network V2 (EINV2) as the foundational\nframework and replace its Conformer blocks with bidirectional Mamba blocks to\ncapture a broader range of contextual information while maintaining\ncomputational efficiency. Additionally, we implement a two-stage training\nmethod, with the first stage focusing on Sound Event Detection (SED) and\nDirection of Arrival (DoA) estimation losses, and the second stage\nreintroducing the Source Distance Estimation (SDE) loss. Our experimental\nresults on the 2024 DCASE Challenge Task3 dataset demonstrate the effectiveness\nof the selective state-space model in SELD and highlight the benefits of the\ntwo-stage training approach in enhancing SELD performance.\n"", '  Sound event localization and detection (SELD) is an important task in machine\nlistening. Major advancements rely on simulated data with sound events in\nspecific rooms and strong spatio-temporal labels. SELD data is simulated by\nconvolving spatialy-localized room impulse responses (RIRs) with sound\nwaveforms to place sound events in a soundscape. However, RIRs require manual\ncollection in specific rooms. We present SpatialScaper, a library for SELD data\nsimulation and augmentation. Compared to existing tools, SpatialScaper emulates\nvirtual rooms via parameters such as size and wall absorption. This allows for\nparameterized placement (including movement) of foreground and background sound\nsources. SpatialScaper also includes data augmentation pipelines that can be\napplied to existing SELD data. As a case study, we use SpatialScaper to add\nrooms to the DCASE SELD data. Training a model with our data led to progressive\nperformance improves as a direct function of acoustic diversity. These results\nshow that SpatialScaper is valuable to train robust SELD models.\n', '  Sound Event Detection and Localization (SELD) is a combined task of\nidentifying sound events and their corresponding direction-of-arrival (DOA).\nWhile this task has numerous applications and has been extensively researched\nin recent years, it fails to provide full information about the sound source\nposition. In this paper, we overcome this problem by extending the task to\nSound Event Detection, Localization with Distance Estimation (3D SELD). We\nstudy two ways of integrating distance estimation within the SELD core - a\nmulti-task approach, in which the problem is tackled by a separate model\noutput, and a single-task approach obtained by extending the multi-ACCDOA\nmethod to include distance information. We investigate both methods for the\nAmbisonic and binaural versions of STARSS23: Sony-TAU Realistic Spatial\nSoundscapes 2023. Moreover, our study involves experiments on the loss function\nrelated to the distance estimation part. Our results show that it is possible\nto perform 3D SELD without any degradation of performance in sound event\ndetection and DOA estimation.\n']",Sound Event Localization and Detection (SELD),Audio Analysis for Health and Acoustic Applications,Audio Analysis for Health and Acoustic Applications,Audio Analysis for Health and Acoustic Applications
412,16,412_summarizer_summarizing_summarization_summaries,"['summarizer', 'summarizing', 'summarization', 'summaries', 'videos', 'smmarization', 'videoxum', 'caption', 'youtube', 'audiovisual']","['video', 'summarization', 'summary', 'summarizer', 'summaries', 'videos', 'scenes', 'newscast', 'summarizing', 'frames']","['  With the surge in the amount of video data, video summarization techniques,\nincluding visual-modal(VM) and textual-modal(TM) summarization, are attracting\nmore and more attention. However, unimodal summarization inevitably loses the\nrich semantics of the video. In this paper, we focus on a more comprehensive\nvideo summarization task named Bimodal Semantic Summarization of Videos\n(BiSSV). Specifically, we first construct a large-scale dataset, BIDS, in\n(video, VM-Summary, TM-Summary) triplet format. Unlike traditional processing\nmethods, our construction procedure contains a VM-Summary extraction algorithm\naiming to preserve the most salient content within long videos. Based on BIDS,\nwe propose a Unified framework UBiSS for the BiSSV task, which models the\nsaliency information in the video and generates a TM-summary and VM-summary\nsimultaneously. We further optimize our model with a list-wise ranking-based\nobjective to improve its capacity to capture highlights. Lastly, we propose a\nmetric, $NDCG_{MS}$, to provide a joint evaluation of the bimodal summary.\nExperiments show that our unified framework achieves better performance than\nmulti-stage summarization pipelines. Code and data are available at\nhttps://github.com/MeiYutingg/UBiSS.\n', '  Video summarization aims to distill the most important information from a\nsource video to produce either an abridged clip or a textual narrative.\nTraditionally, different methods have been proposed depending on whether the\noutput is a video or text, thus ignoring the correlation between the two\nsemantically related tasks of visual summarization and textual summarization.\nWe propose a new joint video and text summarization task. The goal is to\ngenerate both a shortened video clip along with the corresponding textual\nsummary from a long video, collectively referred to as a cross-modal summary.\nThe generated shortened video clip and text narratives should be semantically\nwell aligned. To this end, we first build a large-scale human-annotated dataset\n-- VideoXum (X refers to different modalities). The dataset is reannotated\nbased on ActivityNet. After we filter out the videos that do not meet the\nlength requirements, 14,001 long videos remain in our new dataset. Each video\nin our reannotated dataset has human-annotated video summaries and the\ncorresponding narrative summaries. We then design a novel end-to-end model --\nVTSUM-BILP to address the challenges of our proposed task. Moreover, we propose\na new metric called VT-CLIPScore to help evaluate the semantic consistency of\ncross-modality summary. The proposed model achieves promising performance on\nthis new task and establishes a benchmark for future research.\n', ""  Video summarization aims to create short, accurate, and cohesive summaries of\nlonger videos. Despite the existence of various video summarization datasets, a\nnotable limitation is their limited amount of source videos, which hampers the\neffective fine-tuning of advanced large vision-language models (VLMs).\nAdditionally, most existing datasets are created for video-to-video\nsummarization, overlooking the contemporary need for multimodal video content\nsummarization. Recent efforts have been made to expand from unimodal to\nmultimodal video summarization, categorizing the task into three sub-tasks\nbased on the summary's modality: video-to-video (V2V), video-to-text (V2T), and\na combination of video and text summarization (V2VT). However, the textual\nsummaries in previous multimodal datasets are inadequate. To address these\nissues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset\nfeaturing 30,000 diverse videos sourced from YouTube, with lengths ranging from\n40 to 940 seconds and an average summarization ratio of 16.39\\%. Each video\nsummary in Instruct-V2Xum is paired with a textual summary that references\nspecific frame indexes, facilitating the generation of aligned video and\ntextual summaries. In addition, we propose a new video summarization framework\nnamed V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the\nfirst framework that unifies different video summarization tasks into one large\nlanguage model's (LLM) text decoder and achieves task-controllable video\nsummarization with temporal prompts and task instructions. Experiments show\nthat V2Xum-LLaMA outperforms strong baseline models on multiple video\nsummarization tasks. Furthermore, we propose an enhanced evaluation metric for\nV2V and V2VT summarization tasks.\n""]",Video Summarization Techniques,Video and Image Processing Techniques,Image and Video Processing,Image and Video Processing
413,16,413_climate_climateq_climatebert_nlp,"['climate', 'climateq', 'climatebert', 'nlp', 'climateli', 'climatepolicyradar', 'ecoverse', 'corpus', 'narratives', 'policymakers']","['climate', 'change', 'narratives', 'crisis', 'economic', 'social', 'wildfire', 'narrative', 'demand', 'economics']","[""  Understanding the multifaceted effects of climate change across diverse\ngeographic locations is crucial for timely adaptation and the development of\neffective mitigation strategies. As the volume of scientific literature on this\ntopic continues to grow exponentially, manually reviewing these documents has\nbecome an immensely challenging task. Utilizing Natural Language Processing\n(NLP) techniques to analyze this wealth of information presents an efficient\nand scalable solution. By gathering extensive amounts of peer-reviewed articles\nand studies, we can extract and process critical information about the effects\nof climate change in specific regions. We employ BERT (Bidirectional Encoder\nRepresentations from Transformers) for Named Entity Recognition (NER), which\nenables us to efficiently identify specific geographies within the climate\nliterature. This, in turn, facilitates location-specific analyses. We conduct\nregion-specific climate trend analyses to pinpoint the predominant themes or\nconcerns related to climate change within a particular area, trace the temporal\nprogression of these identified issues, and evaluate their frequency, severity,\nand potential development over time. These in-depth examinations of\nlocation-specific climate data enable the creation of more customized\npolicy-making, adaptation, and mitigation strategies, addressing each region's\nunique challenges and providing more effective solutions rooted in data-driven\ninsights. This approach, founded on a thorough exploration of scientific texts,\noffers actionable insights to a wide range of stakeholders, from policymakers\nto engineers to environmentalists. By proactively understanding these impacts,\nsocieties are better positioned to prepare, allocate resources wisely, and\ndesign tailored strategies to cope with future climate conditions, ensuring a\nmore resilient future for all.\n"", ""  Climate change poses grave challenges, demanding widespread understanding and\nlow-carbon lifestyle awareness. Large language models (LLMs) offer a powerful\ntool to address this crisis, yet comprehensive evaluations of their\nclimate-crisis knowledge are lacking. This paper proposes an automated\nevaluation framework to assess climate-crisis knowledge within LLMs. We adopt a\nhybrid approach for data acquisition, combining data synthesis and manual\ncollection, to compile a diverse set of questions encompassing various aspects\nof climate change. Utilizing prompt engineering based on the compiled\nquestions, we evaluate the model's knowledge by analyzing its generated\nanswers. Furthermore, we introduce a comprehensive set of metrics to assess\nclimate-crisis knowledge, encompassing indicators from 10 distinct\nperspectives. These metrics provide a multifaceted evaluation, enabling a\nnuanced understanding of the LLMs' climate crisis comprehension. The\nexperimental results demonstrate the efficacy of our proposed method. In our\nevaluation utilizing diverse high-performing LLMs, we discovered that while\nLLMs possess considerable climate-related knowledge, there are shortcomings in\nterms of timeliness, indicating a need for continuous updating and refinement\nof their climate-related content.\n"", ""  In this study, we propose a methodology to extract, index, and visualize\n``climate change narratives'' (stories about the connection between causal and\nconsequential events related to climate change). We use two natural language\nprocessing methods, BERT (Bidirectional Encoder Representations from\nTransformers) and causal extraction, to textually analyze newspaper articles on\nclimate change to extract ``climate change narratives.'' The novelty of the\nmethodology could extract and quantify the causal relationships assumed by the\nnewspaper's writers. Looking at the extracted climate change narratives over\ntime, we find that since 2018, an increasing number of narratives suggest the\nimpact of the development of climate change policy discussion and the\nimplementation of climate change-related policies on corporate behaviors,\nmacroeconomics, and price dynamics. We also observed the recent emergence of\nnarratives focusing on the linkages between climate change-related policies and\nmonetary policy. Furthermore, there is a growing awareness of the negative\nimpacts of natural disasters (e.g., abnormal weather and severe floods) related\nto climate change on economic activities, and this issue might be perceived as\na new challenge for companies and governments. The methodology of this study is\nexpected to be applied to a wide range of fields, as it can analyze causal\nrelationships among various economic topics, including analysis of inflation\nexpectation or monetary policy communication strategy.\n""]",Climate Change Analysis using NLP,Climate and Weather Modeling and Prediction using Machine Learning and NLP,Environmental Modeling and Prediction using Advanced Computing and AI,Environmental Modeling and Prediction using Advanced Computing and AI
414,16,414_copyright_plagiarism_copying_infringement,"['copyright', 'plagiarism', 'copying', 'infringement', 'infringing', 'copyrighted', 'watermarking', 'copybench', 'memorization', 'passphrases']","['copyright', 'literal', 'copying', 'verbatim', 'duplicates', 'content', 'traps', 'memorization', 'books', 'reproduction']","['  Memorization in large language models (LLMs) is a growing concern. LLMs have\nbeen shown to easily reproduce parts of their training data, including\ncopyrighted work. This is an important problem to solve, as it may violate\nexisting copyright laws as well as the European AI Act. In this work, we\npropose a systematic analysis to quantify the extent of potential copyright\ninfringements in LLMs using European law as an example. Unlike previous work,\nwe evaluate instruction-finetuned models in a realistic end-user scenario. Our\nanalysis builds on a proposed threshold of 160 characters, which we borrow from\nthe German Copyright Service Provider Act and a fuzzy text matching algorithm\nto identify potentially copyright-infringing textual reproductions. The\nspecificity of countermeasures against copyright infringement is analyzed by\ncomparing model behavior on copyrighted and public domain data. We investigate\nwhat behaviors models show instead of producing protected text (such as refusal\nor hallucination) and provide a first legal assessment of these behaviors. We\nfind that there are huge differences in copyright compliance, specificity, and\nappropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous\nperform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing\na particularly low absolute number of potential copyright violations. Code will\nbe published soon.\n', '  Evaluating the degree of reproduction of copyright-protected content by\nlanguage models (LMs) is of significant interest to the AI and legal\ncommunities. Although both literal and non-literal similarities are considered\nby courts when assessing the degree of reproduction, prior research has focused\nonly on literal similarities. To bridge this gap, we introduce CopyBench, a\nbenchmark designed to measure both literal and non-literal copying in LM\ngenerations. Using copyrighted fiction books as text sources, we provide\nautomatic evaluation protocols to assess literal and non-literal copying,\nbalanced against the model utility in terms of the ability to recall facts from\nthe copyrighted works and generate fluent completions. We find that, although\nliteral copying is relatively rare, two types of non-literal copying -- event\ncopying and character copying -- occur even in models as small as 7B\nparameters. Larger models demonstrate significantly more copying, with literal\ncopying rates increasing from 0.2% to 10.5% and non-literal copying from 2.3%\nto 6.9% when comparing Llama3-8B and 70B models, respectively. We further\nevaluate the effectiveness of current strategies for mitigating copying and\nshow that (1) training-time alignment can reduce literal copying but may\nincrease non-literal copying, and (2) current inference-time mitigation methods\nprimarily reduce literal but not non-literal copying.\n', '  Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating diverse and contextually rich text. However, concerns regarding\ncopyright infringement arise as LLMs may inadvertently produce copyrighted\nmaterial. In this paper, we first investigate the effectiveness of watermarking\nLLMs as a deterrent against the generation of copyrighted texts. Through\ntheoretical analysis and empirical evaluation, we demonstrate that\nincorporating watermarks into LLMs significantly reduces the likelihood of\ngenerating copyrighted content, thereby addressing a critical concern in the\ndeployment of LLMs. Additionally, we explore the impact of watermarking on\nMembership Inference Attacks (MIAs), which aim to discern whether a sample was\npart of the pretraining dataset and may be used to detect copyright violations.\nSurprisingly, we find that watermarking adversely affects the success rate of\nMIAs, complicating the task of detecting copyrighted text in the pretraining\ndataset. Finally, we propose an adaptive technique to improve the success rate\nof a recent MIA under watermarking. Our findings underscore the importance of\ndeveloping adaptive methods to study critical problems in LLMs with potential\nlegal implications.\n']",Copyright Infringement in Large Language Models,"Large Language Models: Safety, Security, and Ethics",Large Language Models,Large Language Models
414,16,414_copyright_plagiarism_copying_infringement,"['copyright', 'plagiarism', 'copying', 'infringement', 'infringing', 'copyrighted', 'watermarking', 'copybench', 'memorization', 'passphrases']","['copyright', 'literal', 'copying', 'verbatim', 'duplicates', 'content', 'traps', 'memorization', 'books', 'reproduction']","['  Memorization in large language models (LLMs) is a growing concern. LLMs have\nbeen shown to easily reproduce parts of their training data, including\ncopyrighted work. This is an important problem to solve, as it may violate\nexisting copyright laws as well as the European AI Act. In this work, we\npropose a systematic analysis to quantify the extent of potential copyright\ninfringements in LLMs using European law as an example. Unlike previous work,\nwe evaluate instruction-finetuned models in a realistic end-user scenario. Our\nanalysis builds on a proposed threshold of 160 characters, which we borrow from\nthe German Copyright Service Provider Act and a fuzzy text matching algorithm\nto identify potentially copyright-infringing textual reproductions. The\nspecificity of countermeasures against copyright infringement is analyzed by\ncomparing model behavior on copyrighted and public domain data. We investigate\nwhat behaviors models show instead of producing protected text (such as refusal\nor hallucination) and provide a first legal assessment of these behaviors. We\nfind that there are huge differences in copyright compliance, specificity, and\nappropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous\nperform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing\na particularly low absolute number of potential copyright violations. Code will\nbe published soon.\n', '  Evaluating the degree of reproduction of copyright-protected content by\nlanguage models (LMs) is of significant interest to the AI and legal\ncommunities. Although both literal and non-literal similarities are considered\nby courts when assessing the degree of reproduction, prior research has focused\nonly on literal similarities. To bridge this gap, we introduce CopyBench, a\nbenchmark designed to measure both literal and non-literal copying in LM\ngenerations. Using copyrighted fiction books as text sources, we provide\nautomatic evaluation protocols to assess literal and non-literal copying,\nbalanced against the model utility in terms of the ability to recall facts from\nthe copyrighted works and generate fluent completions. We find that, although\nliteral copying is relatively rare, two types of non-literal copying -- event\ncopying and character copying -- occur even in models as small as 7B\nparameters. Larger models demonstrate significantly more copying, with literal\ncopying rates increasing from 0.2% to 10.5% and non-literal copying from 2.3%\nto 6.9% when comparing Llama3-8B and 70B models, respectively. We further\nevaluate the effectiveness of current strategies for mitigating copying and\nshow that (1) training-time alignment can reduce literal copying but may\nincrease non-literal copying, and (2) current inference-time mitigation methods\nprimarily reduce literal but not non-literal copying.\n', '  Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating diverse and contextually rich text. However, concerns regarding\ncopyright infringement arise as LLMs may inadvertently produce copyrighted\nmaterial. In this paper, we first investigate the effectiveness of watermarking\nLLMs as a deterrent against the generation of copyrighted texts. Through\ntheoretical analysis and empirical evaluation, we demonstrate that\nincorporating watermarks into LLMs significantly reduces the likelihood of\ngenerating copyrighted content, thereby addressing a critical concern in the\ndeployment of LLMs. Additionally, we explore the impact of watermarking on\nMembership Inference Attacks (MIAs), which aim to discern whether a sample was\npart of the pretraining dataset and may be used to detect copyright violations.\nSurprisingly, we find that watermarking adversely affects the success rate of\nMIAs, complicating the task of detecting copyrighted text in the pretraining\ndataset. Finally, we propose an adaptive technique to improve the success rate\nof a recent MIA under watermarking. Our findings underscore the importance of\ndeveloping adaptive methods to study critical problems in LLMs with potential\nlegal implications.\n']",Copyright Infringement in Large Language Models,"Large Language Models: Safety, Security, and Ethics",Large Language Models,Large Language Models
415,16,415_fashionflow_garment_inception_clothing,"['fashionflow', 'garment', 'inception', 'clothing', 'apparel', 'clothes', 'garments', 'cloth', 'fashionformer', 'images']","['garment', 'try', 'garments', 'fashion', 'clothes', 'person', 'clothing', 'body', 'virtual', 'shopping']","['  Virtual try-on can significantly improve the garment shopping experiences in\nboth online and in-store scenarios, attracting broad interest in computer\nvision. However, to achieve high-fidelity try-on performance, most\nstate-of-the-art methods still rely on accurate segmentation masks, which are\noften produced by near-perfect parsers or manual labeling. To overcome the\nbottleneck, we propose a parser-free virtual try-on method based on the\ndiffusion model (PFDM). Given two images, PFDM can ""wear"" garments on the\ntarget person seamlessly by implicitly warping without any other information.\nTo learn the model effectively, we synthesize many pseudo-images and construct\nsample pairs by wearing various garments on persons. Supervised by the\nlarge-scale expanded dataset, we fuse the person and garment features using a\nproposed Garment Fusion Attention (GFA) mechanism. Experiments demonstrate that\nour proposed PFDM can successfully handle complex cases, synthesize\nhigh-fidelity images, and outperform both state-of-the-art parser-free and\nparser-based models.\n', '  Virtual Try-on (VTON) involves generating images of a person wearing selected\ngarments. Diffusion-based methods, in particular, can create high-quality\nimages, but they struggle to maintain the identities of the input garments. We\nidentified this problem stems from the specifics in the training formulation\nfor diffusion. To address this, we propose a unique training scheme that limits\nthe scope in which diffusion is trained. We use a control image that perfectly\naligns with the target image during training. In turn, this accurately\npreserves garment details during inference. We demonstrate our method not only\neffectively conserves garment details but also allows for layering, styling,\nand shoe try-on. Our method runs multi-garment try-on in a single inference\ncycle and can support high-quality zoomed-in generations without training in\nhigher resolutions. Finally, we show our method surpasses prior methods in\naccuracy and quality.\n', ""  Image-based virtual try-on is an increasingly important task for online\nshopping. It aims to synthesize images of a specific person wearing a specified\ngarment. Diffusion model-based approaches have recently become popular, as they\nare excellent at image synthesis tasks. However, these approaches usually\nemploy additional image encoders and rely on the cross-attention mechanism for\ntexture transfer from the garment to the person image, which affects the\ntry-on's efficiency and fidelity. To address these issues, we propose an\nTexture-Preserving Diffusion (TPD) model for virtual try-on, which enhances the\nfidelity of the results and introduces no additional image encoders.\nAccordingly, we make contributions from two aspects. First, we propose to\nconcatenate the masked person and reference garment images along the spatial\ndimension and utilize the resulting image as the input for the diffusion\nmodel's denoising UNet. This enables the original self-attention layers\ncontained in the diffusion model to achieve efficient and accurate texture\ntransfer. Second, we propose a novel diffusion-based method that predicts a\nprecise inpainting mask based on the person and reference garment images,\nfurther enhancing the reliability of the try-on results. In addition, we\nintegrate mask prediction and image synthesis into a single compact model. The\nexperimental results show that our approach can be applied to various try-on\ntasks, e.g., garment-to-person and person-to-person try-ons, and significantly\noutperforms state-of-the-art methods on popular VITON, VITON-HD databases.\n""]",Virtual Try-On with Diffusion Models,Diffusion Models for Generative Tasks,Diffusion Models for Generative Tasks,Diffusion Models for Generative Tasks
416,16,416_toxicity_frenchtoxicityprompts_toxic_corpus,"['toxicity', 'frenchtoxicityprompts', 'toxic', 'corpus', 'annotated', 'harmful', 'language', 'languages', 'harm', 'polyglotoxicityprompts']","['toxicity', 'toxic', 'moderation', 'mitigation', 'languages', 'bias', 'content', 'multilingual', 'harmful', 'detection']","[""  Large language models (LLMs) are increasingly popular but are also prone to\ngenerating bias, toxic or harmful language, which can have detrimental effects\non individuals and communities. Although most efforts is put to assess and\nmitigate toxicity in generated content, it is primarily concentrated on\nEnglish, while it's essential to consider other languages as well. For\naddressing this issue, we create and release FrenchToxicityPrompts, a dataset\nof 50K naturally occurring French prompts and their continuations, annotated\nwith toxicity scores from a widely used toxicity classifier. We evaluate 14\ndifferent models from four prevalent open-sourced families of LLMs against our\ndataset to assess their potential toxicity across various dimensions. We hope\nthat our contribution will foster future research on toxicity detection and\nmitigation beyond Englis\n"", ""  In the pursuit of developing Large Language Models (LLMs) that adhere to\nsocietal standards, it is imperative to detect the toxicity in the generated\ntext. The majority of existing toxicity metrics rely on encoder models trained\non specific toxicity datasets, which are susceptible to out-of-distribution\n(OOD) problems and depend on the dataset's definition of toxicity. In this\npaper, we introduce a robust metric grounded on LLMs to flexibly measure\ntoxicity according to the given definition. We first analyze the toxicity\nfactors, followed by an examination of the intrinsic toxic attributes of LLMs\nto ascertain their suitability as evaluators. Finally, we evaluate the\nperformance of our metric with detailed analysis. Our empirical results\ndemonstrate outstanding performance in measuring toxicity within verified\nfactors, improving on conventional metrics by 12 points in the F1 score. Our\nfindings also indicate that upstream toxicity significantly influences\ndownstream metrics, suggesting that LLMs are unsuitable for toxicity\nevaluations within unverified factors.\n"", '  Toxicity classification for voice heavily relies on the semantic content of\nspeech. We propose a novel framework that utilizes cross-modal learning to\nintegrate the semantic embedding of text into a multilabel speech toxicity\nclassifier during training. This enables us to incorporate textual information\nduring training while still requiring only audio during inference. We evaluate\nthis classifier on large-scale datasets with real-world characteristics to\nvalidate the effectiveness of this framework. Through ablation studies, we\ndemonstrate that general-purpose semantic text embeddings are rich and aligned\nwith speech for toxicity classification purposes. Conducting experiments across\nmultiple languages at scale, we show improvements in voice toxicity\nclassification across five languages and different toxicity categories.\n']",Toxicity Detection in Multilingual Text and Speech,Toxicity Detection and Mitigation in Natural Language Processing,Misbehavior and Toxicity in Online Content,Misbehavior and Toxicity in Online Content
417,16,417_submodularity_submodular_knapsack_optimization,"['submodularity', 'submodular', 'knapsack', 'optimization', 'maximizing', 'algorithms', 'maximization', 'subroutine', 'greedyml', 'complexity']","['submodular', 'approximation', 'monotone', 'maximization', 'constraint', 'greedy', 'algorithm', 'ratio', 'cover', 'cardinality']","['  Non-monotone constrained submodular maximization plays a crucial role in\nvarious machine learning applications. However, existing algorithms often\nstruggle with a trade-off between approximation guarantees and practical\nefficiency. The current state-of-the-art is a recent $0.401$-approximation\nalgorithm, but its computational complexity makes it highly impractical. The\nbest practical algorithms for the problem only guarantee $1/e$-approximation.\nIn this work, we present a novel algorithm for submodular maximization subject\nto a cardinality constraint that combines a guarantee of $0.385$-approximation\nwith a low and practical query complexity of $O(n+k^2)$. Furthermore, we\nevaluate the empirical performance of our algorithm in experiments based on\nvarious machine learning applications, including Movie Recommendation, Image\nSummarization, and more. These experiments demonstrate the efficacy of our\napproach.\n', '  Constrained submodular maximization problems encompass a wide variety of\napplications, including personalized recommendation, team formation, and\nrevenue maximization via viral marketing. The massive instances occurring in\nmodern day applications can render existing algorithms prohibitively slow,\nwhile frequently, those instances are also inherently stochastic. Focusing on\nthese challenges, we revisit the classic problem of maximizing a (possibly\nnon-monotone) submodular function subject to a knapsack constraint. We present\na simple randomized greedy algorithm that achieves a $5.83$ approximation and\nruns in $O(n \\log n)$ time, i.e., at least a factor $n$ faster than other\nstate-of-the-art algorithms. The robustness of our approach allows us to\nfurther transfer it to a stochastic version of the problem. There, we obtain a\n9-approximation to the best adaptive policy, which is the first constant\napproximation for non-monotone objectives. Experimental evaluation of our\nalgorithms showcases their improved performance on real and synthetic data.\n', '  Submodular optimization is a fundamental problem with many applications in\nmachine learning, often involving decision-making over datasets with sensitive\nattributes such as gender or age. In such settings, it is often desirable to\nproduce a diverse solution set that is fairly distributed with respect to these\nattributes. Motivated by this, we initiate the study of Fair Submodular Cover\n(FSC), where given a ground set $U$, a monotone submodular function\n$f:2^U\\to\\mathbb{R}_{\\ge 0}$, a threshold $\\tau$, the goal is to find a\nbalanced subset of $S$ with minimum cardinality such that $f(S)\\ge\\tau$. We\nfirst introduce discrete algorithms for FSC that achieve a bicriteria\napproximation ratio of $(\\frac{1}{\\epsilon}, 1-O(\\epsilon))$. We then present a\ncontinuous algorithm that achieves a $(\\ln\\frac{1}{\\epsilon},\n1-O(\\epsilon))$-bicriteria approximation ratio, which matches the best\napproximation guarantee of submodular cover without a fairness constraint.\nFinally, we complement our theoretical results with a number of empirical\nevaluations that demonstrate the effectiveness of our algorithms on instances\nof maximum coverage.\n']",Submodular Maximization and Optimization Algorithms,Optimization Methods and Algorithms,Optimization and Design,Optimization and Design
418,16,418_hopfield_memory_retrieval_sparse,"['hopfield', 'memory', 'retrieval', 'sparse', 'hop', '_softmax', 'efficient', 'sparsemap', 'storage', 'hop82']","['modern', 'prototype', 'memory', 'associative', 'replica', 'capacity', 'diagram', 'patterns', 'outlier', 'retrieval']","['  We investigate the computational limits of the memory retrieval dynamics of\nmodern Hopfield models from the fine-grained complexity analysis. Our key\ncontribution is the characterization of a phase transition behavior in the\nefficiency of all possible modern Hopfield models based on the norm of\npatterns. Specifically, we establish an upper bound criterion for the norm of\ninput query patterns and memory patterns. Only below this criterion,\nsub-quadratic (efficient) variants of the modern Hopfield model exist, assuming\nthe Strong Exponential Time Hypothesis (SETH). To showcase our theory, we\nprovide a formal example of efficient constructions of modern Hopfield models\nusing low-rank approximation when the efficient criterion holds. This includes\na derivation of a lower bound on the computational time, scaling linearly with\n$\\max\\{$# of stored memory patterns, length of input query sequence$\\}$. In\naddition, we prove its memory retrieval error bound and exponential memory\ncapacity.\n', '  We propose a two-stage memory retrieval dynamics for modern Hopfield models,\ntermed $\\mathtt{U\\text{-}Hop}$, with enhanced memory capacity. Our key\ncontribution is a learnable feature map $\\Phi$ which transforms the Hopfield\nenergy function into kernel space. This transformation ensures convergence\nbetween the local minima of energy and the fixed points of retrieval dynamics\nwithin the kernel space. Consequently, the kernel norm induced by $\\Phi$ serves\nas a novel similarity measure. It utilizes the stored memory patterns as\nlearning data to enhance memory capacity across all modern Hopfield models.\nSpecifically, we accomplish this by constructing a separation loss\n$\\mathcal{L}_\\Phi$ that separates the local minima of kernelized energy by\nseparating stored memory patterns in kernel space. Methodologically,\n$\\mathtt{U\\text{-}Hop}$ memory retrieval process consists of: (Stage I)\nminimizing separation loss for a more uniform memory (local minimum)\ndistribution, followed by (Stage II) standard Hopfield energy minimization for\nmemory retrieval. This results in a significant reduction of possible\nmetastable states in the Hopfield energy function, thus enhancing memory\ncapacity by preventing memory confusion. Empirically, with real-world datasets,\nwe demonstrate that $\\mathtt{U\\text{-}Hop}$ outperforms all existing modern\nHopfield models and state-of-the-art similarity measures, achieving substantial\nimprovements in both associative memory retrieval and deep learning tasks. Code\nis available at https://github.com/MAGICS-LAB/UHop ; future updates are on\narXiv:2404.03827\n', '  We present a nonparametric construction for deep learning compatible modern\nHopfield models and utilize this framework to debut an efficient variant. Our\nkey contribution stems from interpreting the memory storage and retrieval\nprocesses in modern Hopfield models as a nonparametric regression problem\nsubject to a set of query-memory pairs. Crucially, our framework not only\nrecovers the known results from the original dense modern Hopfield model but\nalso fills the void in the literature regarding efficient modern Hopfield\nmodels, by introducing \\textit{sparse-structured} modern Hopfield models with\nsub-quadratic complexity. We establish that this sparse model inherits the\nappealing theoretical properties of its dense analogue -- connection with\ntransformer attention, fixed point convergence and exponential memory capacity\n-- even without knowing details of the Hopfield energy function. Additionally,\nwe showcase the versatility of our framework by constructing a family of modern\nHopfield models as extensions, including linear, random masked, top-$K$ and\npositive random feature modern Hopfield models. Empirically, we validate the\nefficacy of our framework in both synthetic and realistic settings.\n']",Efficient Modern Hopfield Models for Memory Retrieval,Efficient Models for Information Retrieval,Information Retrieval and Knowledge Systems,Information Retrieval and Knowledge Systems
419,16,419_clicks_advertiser_recommender_ctr,"['clicks', 'advertiser', 'recommender', 'ctr', 'feature', 'advertisers', 'prediction', 'ads', 'advertising', 'click']","['advertising', 'click', 'conversions', 'auction', 'prediction', 'conversion', 'feature', 'online', 'loss', 'rate']","['  Click-through rate (CTR) prediction is widely used in academia and industry.\nMost CTR tasks fall into a feature embedding \\& feature interaction paradigm,\nwhere the accuracy of CTR prediction is mainly improved by designing practical\nfeature interaction structures. However, recent studies have argued that the\nfixed feature embedding learned only through the embedding layer limits the\nperformance of existing CTR models. Some works apply extra modules on top of\nthe embedding layer to dynamically refine feature representations in different\ninstances, making it effective and easy to integrate with existing CTR methods.\nDespite the promising results, there is a lack of a systematic review and\nsummarization of this new promising direction on the CTR task. To fill this\ngap, we comprehensively summarize and define a new module, namely\n\\textbf{feature refinement} (FR) module, that can be applied between feature\nembedding and interaction layers. We extract 14 FR modules from previous works,\nincluding instances where the FR module was proposed but not clearly defined or\nexplained. We fully assess the effectiveness and compatibility of existing FR\nmodules through comprehensive and extensive experiments with over 200 augmented\nmodels and over 4,000 runs for more than 15,000 GPU hours. The results offer\ninsightful guidelines for researchers, and all benchmarking code and\nexperimental results are open-sourced. In addition, we present a new\narchitecture of assigning independent FR modules to separate sub-networks for\nparallel CTR models, as opposed to the conventional method of inserting a\nshared FR module on top of the embedding layer. Our approach is also supported\nby comprehensive experiments demonstrating its effectiveness.\n', '  Multi-Task Learning (MTL) plays a crucial role in real-world advertising\napplications such as recommender systems, aiming to achieve robust\nrepresentations while minimizing resource consumption. MTL endeavors to\nsimultaneously optimize multiple tasks to construct a unified model serving\ndiverse objectives. In online advertising systems, tasks like Click-Through\nRate (CTR) and Conversion Rate (CVR) are often treated as MTL problems\nconcurrently. However, it has been overlooked that a conversion ($y_{cvr}=1$)\nnecessitates a preceding click ($y_{ctr}=1$). In other words, while certain CTR\ntasks are associated with corresponding conversions, others lack such\nassociations. Moreover, the likelihood of noise is significantly higher in CTR\ntasks where conversions do not occur compared to those where they do, and\nexisting methods lack the ability to differentiate between these two scenarios.\nIn this study, exposure labels corresponding to conversions are regarded as\ndefinitive indicators, and a novel task-specific loss is introduced by\ncalculating a \\textbf{p}air\\textbf{wise} \\textbf{r}anking (PWiseR) loss between\nmodel predictions, manifesting as pairwise ranking loss, to encourage the model\nto rely more on them. To demonstrate the effect of the proposed loss function,\nexperiments were conducted on different MTL and Single-Task Learning (STL)\nmodels using four distinct public MTL datasets, namely Alibaba FR, NL, US, and\nCCP, along with a proprietary industrial dataset. The results indicate that our\nproposed loss function outperforms the BCE loss function in most cases in terms\nof the AUC metric.\n', '  Click-Through Rate (CTR) prediction holds a pivotal place in online\nadvertising and recommender systems since CTR prediction performance directly\ninfluences the overall satisfaction of the users and the revenue generated by\ncompanies. Even so, CTR prediction is still an active area of research since it\ninvolves accurately modelling the preferences of users based on sparse and\nhigh-dimensional features where the higher-order interactions of multiple\nfeatures can lead to different outcomes.\n  Most CTR prediction models have relied on a single fusion and interaction\nlearning strategy. The few CTR prediction models that have utilized multiple\ninteraction modelling strategies have treated each interaction to be\nself-contained. In this paper, we propose a novel model named STEC that reaps\nthe benefits of multiple interaction learning approaches in a single unified\narchitecture. Additionally, our model introduces residual connections from\ndifferent orders of interactions which boosts the performance by allowing lower\nlevel interactions to directly affect the predictions. Through extensive\nexperiments on four real-world datasets, we demonstrate that STEC outperforms\nexisting state-of-the-art approaches for CTR prediction thanks to its greater\nexpressive capabilities.\n']",Click-Through Rate Prediction in Advertising,Online Advertising and E-commerce Search Optimization,Optimization and Decision Making in Complex Systems,Complex System Optimization and Management
420,16,420_fuzzy_sets_rough_soft,"['fuzzy', 'sets', 'rough', 'soft', 'vague', 'granular', 'approximations', 'granularly', 'quantification', 'quantifier']","['fuzzy', 'rough', 'hesitant', 'sets', 'induction', 'soft', 'rule', 'theory', 'pythagorean', 'interval']","[""  One of the weaknesses of classical (fuzzy) rough sets is their sensitivity to\nnoise, which is particularly undesirable for machine learning applications. One\napproach to solve this issue is by making use of fuzzy quantifiers, as done by\nthe vaguely quantified fuzzy rough set (VQFRS) model. While this idea is\nintuitive, the VQFRS model suffers from both theoretical flaws as well as from\nsuboptimal performance in applications. In this paper, we improve on VQFRS by\nintroducing fuzzy quantifier-based fuzzy rough sets (FQFRS), an intuitive\ngeneralization of fuzzy rough sets that makes use of general unary and binary\nquantification models. We show how several existing models fit in this\ngeneralization as well as how it inspires novel ones. Several binary\nquantification models are proposed to be used with FQFRS. We conduct a\ntheoretical study of their properties, and investigate their potential by\napplying them to classification problems. In particular, we highlight Yager's\nWeighted Implication-based (YWI) binary quantification model, which induces a\nfuzzy rough set model that is both a significant improvement on VQFRS, as well\nas a worthy competitor to the popular ordered weighted averaging based fuzzy\nrough set (OWAFRS) model.\n"", '  Fuzzy rough set theory can be used as a tool for dealing with inconsistent\ndata when there is a gradual notion of indiscernibility between objects. It\ndoes this by providing lower and upper approximations of concepts. In classical\nfuzzy rough sets, the lower and upper approximations are determined using the\nminimum and maximum operators, respectively. This is undesirable for machine\nlearning applications, since it makes these approximations sensitive to\noutlying samples. To mitigate this problem, ordered weighted average (OWA)\nbased fuzzy rough sets were introduced. In this paper, we show how the\nOWA-based approach can be interpreted intuitively in terms of vague\nquantification, and then generalize it to Choquet-based fuzzy rough sets\n(CFRS). This generalization maintains desirable theoretical properties, such as\nduality and monotonicity. Furthermore, it provides more flexibility for machine\nlearning applications. In particular, we show that it enables the seamless\nintegration of outlier detection algorithms, to enhance the robustness of\nmachine learning algorithms based on fuzzy rough sets.\n', '  Rough set theory is a well-known mathematical framework that can deal with\ninconsistent data by providing lower and upper approximations of concepts. A\nprominent property of these approximations is their granular representation:\nthat is, they can be written as unions of simple sets, called granules. The\nlatter can be identified with ""if. . . , then. . . "" rules, which form the\nbackbone of rough set rule induction. It has been shown previously that this\nproperty can be maintained for various fuzzy rough set models, including those\nbased on ordered weighted average (OWA) operators. In this paper, we will focus\non some instances of the general class of fuzzy quantifier-based fuzzy rough\nsets (FQFRS). In these models, the lower and upper approximations are evaluated\nusing binary and unary fuzzy quantifiers, respectively. One of the main targets\nof this study is to examine the granular representation of different models of\nFQFRS. The main findings reveal that Choquet-based fuzzy rough sets can be\nrepresented granularly under the same conditions as OWA-based fuzzy rough sets,\nwhereas Sugeno-based FRS can always be represented granularly. This observation\nhighlights the potential of these models for resolving data inconsistencies and\nmanaging noise.\n']",Fuzzy Rough Sets and Quantification Models,Fuzzy Rough Sets and Quantification Models for Handling Inconsistent Data,Handling Missing or Inconsistent Data,Handling Missing or Inconsistent Data
421,15,421_autism_autistic_asd_speech,"['autism', 'autistic', 'asd', 'speech', 'voice', 'neurodevelopment', 'diagnosing', 'developmental', 'linguistic', 'diagnostics']","['autism', 'children', 'disorder', 'spectrum', 'speech', 'developmental', 'autistic', 'diagnosis', 'early', 'behaviors']","['  Autism Spectrum Disorder (ASD) represents a multifaceted neurodevelopmental\ncondition marked by difficulties in social interaction, communication\nimpediments, and repetitive behaviors. Despite progress in understanding ASD,\nits diagnosis and treatment continue to pose significant challenges due to the\nvariability in symptomatology and the necessity for multidisciplinary care\napproaches. This paper investigates the potential of Artificial Intelligence\n(AI) to augment the capabilities of healthcare professionals and caregivers in\nmanaging ASD. We have developed a sophisticated algorithm designed to analyze\nfacial and bodily expressions during daily activities of both autistic and\nnon-autistic children, leading to the development of a powerful deep\nlearning-based autism detection system. Our study demonstrated that AI models,\nspecifically the Xception and ResNet50V2 architectures, achieved high accuracy\nin diagnosing Autism Spectrum Disorder (ASD). This research highlights the\ntransformative potential of AI in improving the diagnosis, treatment, and\ncomprehensive management of ASD. Our study revealed that AI models, notably the\nXception and ResNet50V2 architectures, demonstrated high accuracy in diagnosing\nASD.\n', ""  Purpose: Our study explored the use of artificial intelligence (AI) to\ndiagnose autism spectrum disorder (ASD). It focused on machine learning (ML)\nand deep learning (DL) to detect ASD from text inputs on social media,\naddressing challenges in traditional ASD diagnosis.\n  Methods: We used natural language processing (NLP), ML, and DL models\n(including decision trees, XGB, KNN, RNN, LSTM, Bi-LSTM, BERT, and BERTweet) to\nanalyze 404,627 tweets, classifying them based on ASD or non-ASD authors. A\nsubset of 90,000 tweets was used for model training and testing.\n  Results: Our AI models showed high accuracy, with an 88% success rate in\nidentifying texts from individuals with ASD.\n  Conclusion: The study demonstrates AI's potential in improving ASD diagnosis,\nespecially in children, highlighting the importance of early detection.\n"", ""  The diagnosis of autism spectrum disorder (ASD) is a complex, challenging\ntask as it depends on the analysis of interactional behaviors by psychologists\nrather than the use of biochemical diagnostics. In this paper, we present a\nmodeling approach to ASD diagnosis by analyzing acoustic/prosodic and\nlinguistic features extracted from diagnostic conversations between a\npsychologist and children who either are typically developing (TD) or have ASD.\nWe compare the contributions of different features across a range of\nconversation tasks. We focus on finding a minimal set of parameters that\ncharacterize conversational behaviors of children with ASD. Because ASD is\ndiagnosed through conversational interaction, in addition to analyzing the\nbehavior of the children, we also investigate whether the psychologist's\nconversational behaviors vary across diagnostic groups. Our results can\nfacilitate fine-grained analysis of conversation data for children with ASD to\nsupport diagnosis and intervention.\n""]",Autism Spectrum Disorder Diagnosis using AI,"Applications of Artificial Intelligence in Law, Patent Analysis, and Healthcare",Artificial Intelligence Applications and Implications,Artificial Intelligence Applications and Implications
422,15,422_urban_urbanvlp_urbangenai_neighborhoods,"['urban', 'urbanvlp', 'urbangenai', 'neighborhoods', 'landscape', 'visual', 'landscapes', 'planning', 'imagery', 'buildings']","['urban', 'street', 'socioeconomic', 'road', 'building', 'satellite', 'cities', 'images', 'imagery', 'facade']","[""  The visual appeal of urban environments significantly impacts residents'\nsatisfaction with their living spaces and their overall mood, which in turn,\naffects their health and well-being. Given the resource-intensive nature of\ngathering evaluations on urban visual appeal through surveys or inquiries from\nresidents, there is a constant quest for automated solutions to streamline this\nprocess and support spatial planning. In this study, we applied an\noff-the-shelf AI model to automate the analysis of urban visual appeal, using\nover 1,800 Google Street View images of Helsinki, Finland. By incorporating the\nGPT-4 model with specified criteria, we assessed these images. Simultaneously,\n24 participants were asked to rate the images. Our results demonstrated a\nstrong alignment between GPT-4 and participant ratings, although geographic\ndisparities were noted. Specifically, GPT-4 showed a preference for suburban\nareas with significant greenery, contrasting with participants who found these\nareas less appealing. Conversely, in the city centre and densely populated\nurban regions of Helsinki, GPT-4 assigned lower visual appeal scores than\nparticipant ratings. While there was general agreement between AI and human\nassessments across various locations, GPT-4 struggled to incorporate contextual\nnuances into its ratings, unlike participants, who considered both context and\nfeatures of the urban environment. The study suggests that leveraging AI models\nlike GPT-4 allows spatial planners to gather insights into the visual appeal of\ndifferent areas efficiently, aiding decisions that enhance residents' and\ntravellers' satisfaction and mental health. Although AI models provide valuable\ninsights, human perspectives are essential for a comprehensive understanding of\nurban visual appeal. This will ensure that planning and design decisions\npromote healthy living environments effectively.\n"", '  Urban region profiling aims to learn a low-dimensional representation of a\ngiven urban area while preserving its characteristics, such as demographics,\ninfrastructure, and economic activities, for urban planning and development.\nHowever, prevalent pretrained models, particularly those reliant on satellite\nimagery, face dual challenges. Firstly, concentrating solely on macro-level\npatterns from satellite data may introduce bias, lacking nuanced details at\nmicro levels, such as architectural details at a place.Secondly, the lack of\ninterpretability in pretrained models limits their utility in providing\ntransparent evidence for urban planning. In response to these issues, we devise\na novel framework entitled UrbanVLP based on Vision-Language Pretraining. Our\nUrbanVLP seamlessly integrates multi-granularity information from both macro\n(satellite) and micro (street-view) levels, overcoming the limitations of prior\npretrained models. Moreover, it introduces automatic text generation and\ncalibration, elevating interpretability in downstream applications by producing\nhigh-quality text descriptions of urban imagery. Rigorous experiments conducted\nacross six urban indicator prediction tasks underscore its superior\nperformance.\n', ""  Cities around the world face a critical shortage of affordable and decent\nhousing. Despite its critical importance for policy, our ability to effectively\nmonitor and track progress in urban housing is limited. Deep learning-based\ncomputer vision methods applied to street-level images have been successful in\nthe measurement of socioeconomic and environmental inequalities but did not\nfully utilize temporal images to track urban change as time-varying labels are\noften unavailable. We used self-supervised methods to measure change in London\nusing 15 million street images taken between 2008 and 2021. Our novel\nadaptation of Barlow Twins, Street2Vec, embeds urban structure while being\ninvariant to seasonal and daily changes without manual annotations. It\noutperformed generic embeddings, successfully identified point-level change in\nLondon's housing supply from street-level images, and distinguished between\nmajor and minor change. This capability can provide timely information for\nurban planning and policy decisions toward more liveable, equitable, and\nsustainable cities.\n""]",Urban Visual Analysis and Planning,Urban Planning and Analysis with AI and Data-Driven Approaches,AI and Data-Driven Approaches for Urban and Disaster Management,AI and Data-Driven Approaches for Urban and Disaster Management
423,15,423_multilingual_languages_answering_language,"['multilingual', 'languages', 'answering', 'language', 'translating', 'annotation', 'translation', 'linguistic', 'annotated', 'translated']","['languages', 'answering', 'question', 'questions', 'resource', 'dataset', 'native', 'speakers', 'extractive', 'low']","['  Question Answering (QA) datasets have been instrumental in developing and\nevaluating Large Language Model (LLM) capabilities. However, such datasets are\nscarce for languages other than English due to the cost and difficulties of\ncollection and manual annotation. This means that producing novel models and\nmeasuring the performance of multilingual LLMs in low-resource languages is\nchallenging. To mitigate this, we propose $\\textbf{S}$yn$\\textbf{DAR}$in, a\nmethod for generating and validating QA datasets for low-resource languages. We\nutilize parallel content mining to obtain $\\textit{human-curated}$ paragraphs\nbetween English and the target language. We use the English data as context to\n$\\textit{generate}$ synthetic multiple-choice (MC) question-answer pairs, which\nare automatically translated and further validated for quality. Combining these\nwith their designated non-English $\\textit{human-curated}$ paragraphs form the\nfinal QA dataset. The method allows to maintain the content quality, reduces\nthe likelihood of factual errors, and circumvents the need for costly\nannotation. To test the method, we created a QA dataset with $1.2$K samples for\nthe Armenian language. The human evaluation shows that $98\\%$ of the generated\nEnglish data maintains quality and diversity in the question types and topics,\nwhile the translation validation pipeline can filter out $\\sim70\\%$ of data\nwith poor quality. We use the dataset to benchmark state-of-the-art LLMs,\nshowing their inability to achieve human accuracy with some model performances\ncloser to random chance. This shows that the generated dataset is non-trivial\nand can be used to evaluate reasoning capabilities in low-resource language.\n', ""  Large Language Models (LLMs) have demonstrated remarkable zero-shot and\nfew-shot capabilities in unseen tasks, including context-grounded question\nanswering (QA) in English. However, the evaluation of LLMs' capabilities in\nnon-English languages for context-based QA is limited by the scarcity of\nbenchmarks in non-English languages. To address this gap, we introduce\nIndic-QA, the largest publicly available context-grounded question-answering\ndataset for 11 major Indian languages from two language families. The dataset\ncomprises both extractive and abstractive question-answering tasks and includes\nexisting datasets as well as English QA datasets translated into Indian\nlanguages. Additionally, we generate a synthetic dataset using the Gemini model\nto create question-answer pairs given a passage, which is then manually\nverified for quality assurance. We evaluate various multilingual Large Language\nModels and their instruction-fine-tuned variants on the benchmark and observe\nthat their performance is subpar, particularly for low-resource languages. We\nhope that the release of this dataset will stimulate further research on the\nquestion-answering abilities of LLMs for low-resource languages.\n"", '  Question answering (QA) is the task of answering questions posed in natural\nlanguage with free-form natural language answers extracted from a given\npassage. In the OpenQA variant, only a question text is given, and the system\nmust retrieve relevant passages from an unstructured knowledge source and use\nthem to provide answers, which is the case in the mainstream QA systems on the\nWeb. QA systems currently are mostly limited to the English language due to the\nlack of large-scale labeled QA datasets in non-English languages. In this\npaper, we show that effective, low-cost OpenQA systems can be developed for\nlow-resource contexts. The key ingredients are (1) weak supervision using\nmachine-translated labeled datasets and (2) a relevant unstructured knowledge\nsource in the target language context. Furthermore, we show that only a few\nhundred gold assessment examples are needed to reliably evaluate these systems.\nWe apply our method to Turkish as a challenging case study, since English and\nTurkish are typologically very distinct and Turkish has limited resources for\nQA. We present SQuAD-TR, a machine translation of SQuAD2.0, and we build our\nOpenQA system by adapting ColBERT-QA and retraining it over Turkish resources\nand SQuAD-TR using two versions of Wikipedia dumps spanning two years. We\nobtain a performance improvement of 24-32% in the Exact Match (EM) score and\n22-29% in the F1 score compared to the BM25-based and DPR-based baseline QA\nreader models. Our results show that SQuAD-TR makes OpenQA feasible for\nTurkish, which we hope encourages researchers to build OpenQA systems in other\nlow-resource languages. We make all the code, models, and the dataset publicly\navailable at https://github.com/boun-tabi/SQuAD-TR.\n']",Multilingual Question Answering Datasets,Multilingual Natural Language Processing,Natural Language Processing,Natural Language Processing
424,15,424_robust_minimizes_optimization_minimax,"['robust', 'minimizes', 'optimization', 'minimax', 'distributional', 'distributionally', 'stochastic', 'convex', 'regularized', 'wasserstein']","['risk', 'convex', 'robust', 'multistage', 'estimators', 'optimization', 'measurable', 'covariate', 'numerical', 'stochastic']","['  The goal of this paper is to develop distributionally robust optimization\n(DRO) estimators, specifically for multidimensional Extreme Value Theory (EVT)\nstatistics. EVT supports using semi-parametric models called max-stable\ndistributions built from spatial Poisson point processes. While powerful, these\nmodels are only asymptotically valid for large samples. However, since extreme\ndata is by definition scarce, the potential for model misspecification error is\ninherent to these applications, thus DRO estimators are natural. In order to\nmitigate over-conservative estimates while enhancing out-of-sample performance,\nwe study DRO estimators informed by semi-parametric max-stable constraints in\nthe space of point processes. We study both tractable convex formulations for\nsome problems of interest (e.g. CVaR) and more general neural network based\nestimators. Both approaches are validated using synthetically generated data,\nrecovering prescribed characteristics, and verifying the efficacy of the\nproposed techniques. Additionally, the proposed method is applied to a real\ndata set of financial returns for comparison to a previous analysis. We\nestablished the proposed model as a novel formulation in the multivariate EVT\ndomain, and innovative with respect to performance when compared to relevant\nalternate proposals.\n', '  We consider the penalized distributionally robust optimization (DRO) problem\nwith a closed, convex uncertainty set, a setting that encompasses the $f$-DRO,\nWasserstein-DRO, and spectral/$L$-risk formulations used in practice. We\npresent Drago, a stochastic primal-dual algorithm that achieves a\nstate-of-the-art linear convergence rate on strongly convex-strongly concave\nDRO problems. The method combines both randomized and cyclic components with\nmini-batching, which effectively handles the unique asymmetric nature of the\nprimal and dual problems in DRO. We support our theoretical results with\nnumerical benchmarks in classification and regression.\n', '  Distributionally robust optimization (DRO) is a powerful framework for\ntraining robust models against data distribution shifts. This paper focuses on\nconstrained DRO, which has an explicit characterization of the robustness\nlevel. Existing studies on constrained DRO mostly focus on convex loss\nfunction, and exclude the practical and challenging case with non-convex loss\nfunction, e.g., neural network. This paper develops a stochastic algorithm and\nits performance analysis for non-convex constrained DRO. The computational\ncomplexity of our stochastic algorithm at each iteration is independent of the\noverall dataset size, and thus is suitable for large-scale applications. We\nfocus on the general Cressie-Read family divergence defined uncertainty set\nwhich includes $\\chi^2$-divergences as a special case. We prove that our\nalgorithm finds an $\\epsilon$-stationary point with a computational complexity\nof $\\mathcal O(\\epsilon^{-3k_*-5})$, where $k_*$ is the parameter of the\nCressie-Read divergence. The numerical results indicate that our method\noutperforms existing methods.} Our method also applies to the smoothed\nconditional value at risk (CVaR) DRO.\n']",Distributionally Robust Optimization,Optimization Methods and Algorithms,Optimization and Design,Optimization and Design
425,15,425_vehicular_supervised_vehicles_vehicle,"['vehicular', 'supervised', 'vehicles', 'vehicle', 'driving', 'autonomous', 'cloud', 'federated', 'learning', 'networks']","['vehicular', 'vehicles', 'vehicle', 'driver', 'privacy', 'transportation', 'aggregation', 'driving', 'scoring', 'autonomous']","['  Roadside unit (RSU) can significantly improve the safety and robustness of\nautonomous vehicles through Vehicle-to-Everything (V2X) communication.\nCurrently, the usage of a single RSU mainly focuses on real-time inference and\nV2X collaboration, while neglecting the potential value of the high-quality\ndata collected by RSU sensors. Integrating the vast amounts of data from\nnumerous RSUs can provide a rich source of data for model training. However,\nthe absence of ground truth annotations and the difficulty of transmitting\nenormous volumes of data are two inevitable barriers to fully exploiting this\nhidden value. In this paper, we introduce FedRSU, an innovative federated\nlearning framework for self-supervised scene flow estimation. In FedRSU, we\npresent a recurrent self-supervision training paradigm, where for each RSU, the\nscene flow prediction of points at every timestamp can be supervised by its\nsubsequent future multi-modality observation. Another key component of FedRSU\nis federated learning, where multiple devices collaboratively train an ML model\nwhile keeping the training data local and private. With the power of the\nrecurrent self-supervised learning paradigm, FL is able to leverage innumerable\nunderutilized data from RSU. To verify the FedRSU framework, we construct a\nlarge-scale multi-modality dataset RSU-SF. The dataset consists of 17 RSU\nclients, covering various scenarios, modalities, and sensor settings. Based on\nRSU-SF, we show that FedRSU can greatly improve model performance in ITS and\nprovide a comprehensive benchmark under diverse FL scenarios. To the best of\nour knowledge, we provide the first real-world LiDAR-camera multi-modal dataset\nand benchmark for the FL community.\n', ""  Vehicular edge intelligence (VEI) is a promising paradigm for enabling future\nintelligent transportation systems by accommodating artificial intelligence\n(AI) at the vehicular edge computing (VEC) system. Federated learning (FL)\nstands as one of the fundamental technologies facilitating collaborative model\ntraining locally and aggregation, while safeguarding the privacy of vehicle\ndata in VEI. However, traditional FL faces challenges in adapting to vehicle\nheterogeneity, training large models on resource-constrained vehicles, and\nremaining susceptible to model weight privacy leakage. Meanwhile, split\nlearning (SL) is proposed as a promising collaborative learning framework which\ncan mitigate the risk of model wights leakage, and release the training\nworkload on vehicles. SL sequentially trains a model between a vehicle and an\nedge cloud (EC) by dividing the entire model into a vehicle-side model and an\nEC-side model at a given cut layer. In this work, we combine the advantages of\nSL and FL to develop an Adaptive Split Federated Learning scheme for Vehicular\nEdge Computing (ASFV). The ASFV scheme adaptively splits the model and\nparallelizes the training process, taking into account mobile vehicle selection\nand resource allocation. Our extensive simulations, conducted on\nnon-independent and identically distributed data, demonstrate that the proposed\nASFV solution significantly reduces training latency compared to existing\nbenchmarks, while adapting to network dynamics and vehicles' mobility.\n"", '  Federated Learning (FL) is an advanced distributed machine learning approach,\nthat protects the privacy of each vehicle by allowing the model to be trained\non multiple devices simultaneously without the need to upload all data to a\nroad side unit (RSU). This enables FL to handle scenarios with sensitive or\nwidely distributed data. However, in these fields, it is well known that the\nlabeling costs can be a significant expense, and models relying on labels are\nnot suitable for these rapidly evolving fields especially in vehicular\nnetworks, or mobile internet of things (MIoT), where new data emerges\nconstantly. To handle this issue, the self-supervised learning paves the way\nfor training without labels. Additionally, for vehicles with high velocity,\nowing to blurred images, simple aggregation not only impacts the accuracy of\nthe aggregated model but also reduces the convergence speed of FL. This paper\nproposes a FL algorithm based on image blur level to aggregation, called\nFLSimCo, which does not require labels and serves as a pre-training stage for\nself-supervised learning in the vehicular environment. Simulation results\ndemonstrate that the proposed algorithm exhibits fast and stable convergence.\n']",Federated Learning for Vehicular Networks,Federated Learning,Machine Learning and Data Privacy,Machine Learning and Data Privacy
426,15,426_tactics_uav_maneuvers_uavs,"['tactics', 'uav', 'maneuvers', 'uavs', 'evader', 'combat', 'drones', 'maneuver', 'maneuverability', 'drone']","['dogfight', 'aircraft', 'flight', 'combat', 'aerial', 'maneuvers', 'drones', 'evader', 'pursuit', 'agile']","['  This paper proposes a three-layer unmanned combat aerial vehicle (UCAV)\ndogfight frame where Deep reinforcement learning (DRL) is responsible for\nhigh-level maneuver decision. A four-channel low-level control law is firstly\nconstructed, followed by a library containing eight basic flight maneuvers\n(BFMs). Double deep Q network (DDQN) is applied for BFM selection in UCAV\ndogfight, where the opponent strategy during the training process is\nconstructed with DT. Our simulation result shows that, the agent can achieve a\nwin rate of 85.75% against the DT strategy, and positive results when facing\nvarious unseen opponents. Based on the proposed frame, interpretability of the\nDRL-based dogfight is significantly improved. The agent performs yo-yo to\nadjust its turn rate and gain higher maneuverability. Emergence of ""Dive and\nChase"" behavior also indicates the agent can generate a novel tactic that\nutilizes the drawback of its opponent.\n', ""  Unmanned Combat Aerial Vehicle (UCAV) dogfight, which refers to a fight\nbetween two or more UCAVs usually at close quarters, plays a decisive role on\nthe aerial battlefields. With the evolution of artificial intelligence,\ndogfight progressively transits towards intelligent and autonomous modes.\nHowever, the development of autonomous dogfight policy learning is hindered by\nchallenges such as weak exploration capabilities, low learning efficiency, and\nunrealistic simulated environments. To overcome these challenges, this paper\nproposes a novel imitative reinforcement learning framework, which efficiently\nleverages expert data while enabling autonomous exploration. The proposed\nframework not only enhances learning efficiency through expert imitation, but\nalso ensures adaptability to dynamic environments via autonomous exploration\nwith reinforcement learning. Therefore, the proposed framework can learn a\nsuccessful dogfight policy of 'pursuit-lock-launch' for UCAVs. To support\ndata-driven learning, we establish a dogfight environment based on the\nHarfang3D sandbox, where we conduct extensive experiments. The results indicate\nthat the proposed framework excels in multistage dogfight, significantly\noutperforms state-of-the-art reinforcement learning and imitation learning\nmethods. Thanks to the ability of imitating experts and autonomous exploration,\nour framework can quickly learn the critical knowledge in complex aerial combat\ntasks, achieving up to a 100% success rate and demonstrating excellent\nrobustness.\n"", '  Dogfighting is a challenging scenario in aerial applications that requires a\ncomprehensive understanding of both strategic maneuvers and the aerodynamics of\nagile aircraft. The aerial agent needs to not only understand tactically\nevolving maneuvers of fighter jets from a long-term perspective but also react\nto rapidly changing aerodynamics of aircraft from a short-term viewpoint. In\nthis paper, we introduce TempFuser, a novel long short-term temporal fusion\ntransformer architecture that can learn agile, tactical, and acrobatic flight\nmaneuvers in complex dogfight problems. Our approach integrates two distinct\ntemporal transition embeddings into a transformer-based network to\ncomprehensively capture both the long-term tactics and short-term agility of\naerial agents. By incorporating these perspectives, our policy network\ngenerates end-to-end flight commands that secure dominant positions over the\nlong term and effectively outmaneuver agile opponents. After training in a\nhigh-fidelity flight simulator, our model successfully learns to execute\nstrategic maneuvers, outperforming baseline policy models against various types\nof opponent aircraft. Notably, our model exhibits human-like acrobatic\nmaneuvers even when facing adversaries with superior specifications, all\nwithout relying on explicit prior knowledge. Moreover, it demonstrates robust\npursuit performance in challenging supersonic and low-altitude situations. Demo\nvideos are available at https://sites.google.com/view/tempfuser.\n']",UAV Dogfight Tactics and Maneuvers,Unmanned Aerial Vehicle (UAV) Systems and Technologies,Aerial Robotics and Agricultural Computer Vision,Aerial Robotics and Agricultural Computer Vision
427,15,427_ridesharing_taxis_passengers_taxi,"['ridesharing', 'taxis', 'passengers', 'taxi', 'intercity', 'passenger', 'routes', 'trips', 'dispatching', 'vehicles']","['demand', 'ride', 'vehicle', 'fleet', 'rebalancing', 'taxis', 'dispatching', 'taxi', 'mile', 'passenger']","[""  Large events such as conferences, concerts and sports games, often cause\nsurges in demand for ride services that are not captured in average demand\npatterns, posing unique challenges for routing algorithms. We propose a\nlearning framework for an autonomous fleet of taxis that leverages event data\nfrom the internet to predict demand surges and generate cooperative routing\npolicies. We achieve this through a combination of two major components: (i) a\ndemand prediction framework that uses textual event information in the form of\nevents' descriptions and reviews to predict event-driven demand surges over\nstreet intersections, and (ii) a scalable multiagent reinforcement learning\nframework that leverages demand predictions and uses one-agent-at-a-time\nrollout combined with limited sampling certainty equivalence to learn\nintersection-level routing policies. For our experimental results we consider\nreal NYC ride share data for the year 2022 and information for more than 2000\nevents across 300 unique venues in Manhattan. We test our approach with a fleet\nof 100 taxis on a map with 2235 street intersections. Our experimental results\ndemonstrate that our method learns routing policies that reduce wait time\noverhead per serviced request by 25% to 75%, while picking up 1% to 4% more\nrequests than other model-based RL frameworks and classical methods in\noperations research.\n"", '  Mobility-on-demand (MoD) systems consist of a fleet of shared vehicles that\ncan be hailed for one-way point-to-point trips. The total distance driven by\nthe vehicles and the fleet size can be reduced by employing ridesharing, i.e.,\nby assigning multiple passengers to one vehicle. However, finding the optimal\npassenger-vehicle assignment in an MoD system is a hard combinatorial problem.\nIn this work, we demonstrate how the VGA method, a recently proposed systematic\nmethod for ridesharing, can be used to compute the optimal passenger-vehicle\nassignments and corresponding vehicle routes in a massive-scale MoD system. In\ncontrast to existing works, we solve all passenger-vehicle assignment problems\nto optimality, regularly dealing with instances containing thousands of\nvehicles and passengers. Moreover, to examine the impact of using optimal\nridesharing assignments, we compare the performance of an MoD system that uses\noptimal assignments against an MoD system that uses assignments computed using\ninsertion heuristic and against an MoD system that uses no ridesharing. We\nfound that the system that uses optimal ridesharing assignments subject to the\nmaximum travel delay of 4 minutes reduces the vehicle distance driven by 57 %\ncompared to an MoD system without ridesharing. Furthermore, we found that the\noptimal assignments result in a 20 % reduction in vehicle distance driven and 5\n% lower average passenger travel delay compared to a system that uses insertion\nheuristic.\n', ""  The emergence of on-demand ride pooling services allows each vehicle to serve\nmultiple passengers at a time, thus increasing drivers' income and enabling\npassengers to travel at lower prices than taxi/car on-demand services (only one\npassenger can be assigned to a car at a time like UberX and Lyft). Although\non-demand ride pooling services can bring so many benefits, ride pooling\nservices need a well-defined matching strategy to maximize the benefits for all\nparties (passengers, drivers, aggregation companies and environment), in which\nthe regional dispatching of vehicles has a significant impact on the matching\nand revenue. Existing algorithms often only consider revenue maximization,\nwhich makes it difficult for requests with unusual distribution to get a ride.\nHow to increase revenue while ensuring a reasonable assignment of requests\nbrings a challenge to ride pooling service companies (aggregation companies).\nIn this paper, we propose a framework for vehicle dispatching for ride pooling\ntasks, which splits the city into discrete dispatching regions and uses the\nreinforcement learning (RL) algorithm to dispatch vehicles in these regions. We\nalso consider the mutual information (MI) between vehicle and order\ndistribution as the intrinsic reward of the RL algorithm to improve the\ncorrelation between their distributions, thus ensuring the possibility of\ngetting a ride for unusually distributed requests. In experimental results on a\nreal-world taxi dataset, we demonstrate that our framework can significantly\nincrease revenue up to an average of 3\\% over the existing best on-demand ride\npooling method.\n""]",Optimizing Ride-Sharing and Taxi Dispatch Systems,Optimization of Resource Allocation and Incentives in Dynamic Systems,Optimization and Management of Complex Systems,Optimization and Decision Making in Complex Systems
428,15,428_heuristics_heuristic_optimized_generating,"['heuristics', 'heuristic', 'optimized', 'generating', 'automate', 'evolutionary', 'algorithms', 'automated', 'generates', 'evolving']","['evolutionary', 'evolution', 'heuristics', 'heuristic', 'algorithms', 'optimization', 'parallels', 'genetic', 'fitness', 'prompts']","['  Pre-trained large language models (LLMs) have powerful capabilities for\ngenerating creative natural text. Evolutionary algorithms (EAs) can discover\ndiverse solutions to complex real-world problems. Motivated by the common\ncollective and directionality of text generation and evolution, this paper\nillustrates the parallels between LLMs and EAs, which includes multiple\none-to-one key characteristics: token representation and individual\nrepresentation, position encoding and fitness shaping, position embedding and\nselection, Transformers block and reproduction, and model training and\nparameter adaptation. By examining these parallels, we analyze existing\ninterdisciplinary research, with a specific focus on evolutionary fine-tuning\nand LLM-enhanced EAs. Drawing from these insights, valuable future directions\nare presented for advancing the integration of LLMs and EAs, while highlighting\nkey challenges along the way. These parallels not only reveal the evolution\nmechanism behind LLMs but also facilitate the development of evolved artificial\nagents that approach or surpass biological organisms.\n', '  Heuristics are widely used for dealing with complex search and optimization\nproblems. However, manual design of heuristics can be often very labour\nextensive and requires rich working experience and knowledge. This paper\nproposes Evolution of Heuristic (EoH), a novel evolutionary paradigm that\nleverages both Large Language Models (LLMs) and Evolutionary Computation (EC)\nmethods for Automatic Heuristic Design (AHD). EoH represents the ideas of\nheuristics in natural language, termed thoughts. They are then translated into\nexecutable codes by LLMs. The evolution of both thoughts and codes in an\nevolutionary search framework makes it very effective and efficient for\ngenerating high-performance heuristics. Experiments on three widely studied\ncombinatorial optimization benchmark problems demonstrate that EoH outperforms\ncommonly used handcrafted heuristics and other recent AHD methods including\nFunSearch. Particularly, the heuristic produced by EoH with a low computational\nbudget (in terms of the number of queries to LLMs) significantly outperforms\nwidely-used human hand-crafted baseline algorithms for the online bin packing\nproblem.\n', '  Large Language Models (LLMs) excel in various tasks, but they rely on\ncarefully crafted prompts that often demand substantial human effort. To\nautomate this process, in this paper, we propose a novel framework for discrete\nprompt optimization, called EvoPrompt, which borrows the idea of evolutionary\nalgorithms (EAs) as they exhibit good performance and fast convergence. To\nenable EAs to work on discrete prompts, which are natural language expressions\nthat need to be coherent and human-readable, we connect LLMs with EAs. This\napproach allows us to simultaneously leverage the powerful language processing\ncapabilities of LLMs and the efficient optimization performance of EAs.\nSpecifically, abstaining from any gradients or parameters, EvoPrompt starts\nfrom a population of prompts and iteratively generates new prompts with LLMs\nbased on the evolutionary operators, improving the population based on the\ndevelopment set. We optimize prompts for both closed- and open-source LLMs\nincluding GPT-3.5 and Alpaca, on 31 datasets covering language understanding,\ngeneration tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt\nsignificantly outperforms human-engineered prompts and existing methods for\nautomatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt\ndemonstrates that connecting LLMs with EAs creates synergies, which could\ninspire further research on the combination of LLMs and conventional\nalgorithms.\n']",Evolutionary Algorithms for Large Language Models,Optimization and Efficiency of Large Language Models,Large Language Models,Large Language Models
428,15,428_heuristics_heuristic_optimized_generating,"['heuristics', 'heuristic', 'optimized', 'generating', 'automate', 'evolutionary', 'algorithms', 'automated', 'generates', 'evolving']","['evolutionary', 'evolution', 'heuristics', 'heuristic', 'algorithms', 'optimization', 'parallels', 'genetic', 'fitness', 'prompts']","['  Pre-trained large language models (LLMs) have powerful capabilities for\ngenerating creative natural text. Evolutionary algorithms (EAs) can discover\ndiverse solutions to complex real-world problems. Motivated by the common\ncollective and directionality of text generation and evolution, this paper\nillustrates the parallels between LLMs and EAs, which includes multiple\none-to-one key characteristics: token representation and individual\nrepresentation, position encoding and fitness shaping, position embedding and\nselection, Transformers block and reproduction, and model training and\nparameter adaptation. By examining these parallels, we analyze existing\ninterdisciplinary research, with a specific focus on evolutionary fine-tuning\nand LLM-enhanced EAs. Drawing from these insights, valuable future directions\nare presented for advancing the integration of LLMs and EAs, while highlighting\nkey challenges along the way. These parallels not only reveal the evolution\nmechanism behind LLMs but also facilitate the development of evolved artificial\nagents that approach or surpass biological organisms.\n', '  Heuristics are widely used for dealing with complex search and optimization\nproblems. However, manual design of heuristics can be often very labour\nextensive and requires rich working experience and knowledge. This paper\nproposes Evolution of Heuristic (EoH), a novel evolutionary paradigm that\nleverages both Large Language Models (LLMs) and Evolutionary Computation (EC)\nmethods for Automatic Heuristic Design (AHD). EoH represents the ideas of\nheuristics in natural language, termed thoughts. They are then translated into\nexecutable codes by LLMs. The evolution of both thoughts and codes in an\nevolutionary search framework makes it very effective and efficient for\ngenerating high-performance heuristics. Experiments on three widely studied\ncombinatorial optimization benchmark problems demonstrate that EoH outperforms\ncommonly used handcrafted heuristics and other recent AHD methods including\nFunSearch. Particularly, the heuristic produced by EoH with a low computational\nbudget (in terms of the number of queries to LLMs) significantly outperforms\nwidely-used human hand-crafted baseline algorithms for the online bin packing\nproblem.\n', '  Large Language Models (LLMs) excel in various tasks, but they rely on\ncarefully crafted prompts that often demand substantial human effort. To\nautomate this process, in this paper, we propose a novel framework for discrete\nprompt optimization, called EvoPrompt, which borrows the idea of evolutionary\nalgorithms (EAs) as they exhibit good performance and fast convergence. To\nenable EAs to work on discrete prompts, which are natural language expressions\nthat need to be coherent and human-readable, we connect LLMs with EAs. This\napproach allows us to simultaneously leverage the powerful language processing\ncapabilities of LLMs and the efficient optimization performance of EAs.\nSpecifically, abstaining from any gradients or parameters, EvoPrompt starts\nfrom a population of prompts and iteratively generates new prompts with LLMs\nbased on the evolutionary operators, improving the population based on the\ndevelopment set. We optimize prompts for both closed- and open-source LLMs\nincluding GPT-3.5 and Alpaca, on 31 datasets covering language understanding,\ngeneration tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt\nsignificantly outperforms human-engineered prompts and existing methods for\nautomatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt\ndemonstrates that connecting LLMs with EAs creates synergies, which could\ninspire further research on the combination of LLMs and conventional\nalgorithms.\n']",Evolutionary Algorithms for Large Language Models,Optimization and Efficiency of Large Language Models,Large Language Models,Large Language Models
429,15,429_chemical_chemistry_molecular_molecule,"['chemical', 'chemistry', 'molecular', 'molecule', 'chemllm', 'molecules', 'synthesis', 'chemdfm', 'automating', 'chemdata']","['chemistry', 'chemical', 'synthesis', 'reaction', 'flowsheets', 'scientific', 'sieve', 'molecule', 'flowsheet', 'molecular']","['  In recent years, Large Language Models (LLMs) have achieved significant\nsuccess in natural language processing (NLP) and various interdisciplinary\nareas. However, applying LLMs to chemistry is a complex task that requires\nspecialized domain knowledge. This paper provides a thorough exploration of the\nnuanced methodologies employed in integrating LLMs into the field of chemistry,\ndelving into the complexities and innovations at this interdisciplinary\njuncture. Specifically, our analysis begins with examining how molecular\ninformation is fed into LLMs through various representation and tokenization\nmethods. We then categorize chemical LLMs into three distinct groups based on\nthe domain and modality of their input data, and discuss approaches for\nintegrating these inputs for LLMs. Furthermore, this paper delves into the\npretraining objectives with adaptations to chemical LLMs. After that, we\nexplore the diverse applications of LLMs in chemistry, including novel\nparadigms for their application in chemistry tasks. Finally, we identify\npromising research directions, including further integration with chemical\nknowledge, advancements in continual learning, and improvements in model\ninterpretability, paving the way for groundbreaking developments in the field.\n', ""  Large language models (LLMs) have made impressive progress in chemistry\napplications. However, the community lacks an LLM specifically designed for\nchemistry. The main challenges are two-fold: firstly, most chemical data and\nscientific knowledge are stored in structured databases, which limits the\nmodel's ability to sustain coherent dialogue when used directly. Secondly,\nthere is an absence of objective and fair benchmark that encompass most\nchemistry tasks. Here, we introduce ChemLLM, a comprehensive framework that\nfeatures the first LLM dedicated to chemistry. It also includes ChemData, a\ndataset specifically designed for instruction tuning, and ChemBench, a robust\nbenchmark covering nine essential chemistry tasks. ChemLLM is adept at\nperforming various tasks across chemical disciplines with fluid dialogue\ninteraction. Notably, ChemLLM achieves results comparable to GPT-4 on the core\nchemical tasks and demonstrates competitive performance with LLMs of similar\nsize in general scenarios. ChemLLM paves a new path for exploration in chemical\nstudies, and our method of incorporating structured chemical knowledge into\ndialogue systems sets a new standard for developing LLMs in various scientific\nfields. Codes, Datasets, and Model weights are publicly accessible at\nhttps://hf.co/AI4Chem\n"", ""  Chemical synthesis, which is crucial for advancing material synthesis and\ndrug discovery, impacts various sectors including environmental science and\nhealthcare. The rise of technology in chemistry has generated extensive\nchemical data, challenging researchers to discern patterns and refine synthesis\nprocesses. Artificial intelligence (AI) helps by analyzing data to optimize\nsynthesis and increase yields. However, AI faces challenges in processing\nliterature data due to the unstructured format and diverse writing style of\nchemical literature. To overcome these difficulties, we introduce an end-to-end\nAI agent framework capable of high-fidelity extraction from extensive chemical\nliterature. This AI agent employs large language models (LLMs) for prompt\ngeneration and iterative optimization. It functions as a chemistry assistant,\nautomating data collection and analysis, thereby saving manpower and enhancing\nperformance. Our framework's efficacy is evaluated using accuracy, recall, and\nF1 score of reaction condition data, and we compared our method with human\nexperts in terms of content correctness and time efficiency. The proposed\napproach marks a significant advancement in automating chemical literature\nextraction and demonstrates the potential for AI to revolutionize data\nmanagement and utilization in chemistry.\n""]",Large Language Models in Chemistry Applications,Applications of Large Language Models,Large Language Models,Large Language Models
429,15,429_chemical_chemistry_molecular_molecule,"['chemical', 'chemistry', 'molecular', 'molecule', 'chemllm', 'molecules', 'synthesis', 'chemdfm', 'automating', 'chemdata']","['chemistry', 'chemical', 'synthesis', 'reaction', 'flowsheets', 'scientific', 'sieve', 'molecule', 'flowsheet', 'molecular']","['  In recent years, Large Language Models (LLMs) have achieved significant\nsuccess in natural language processing (NLP) and various interdisciplinary\nareas. However, applying LLMs to chemistry is a complex task that requires\nspecialized domain knowledge. This paper provides a thorough exploration of the\nnuanced methodologies employed in integrating LLMs into the field of chemistry,\ndelving into the complexities and innovations at this interdisciplinary\njuncture. Specifically, our analysis begins with examining how molecular\ninformation is fed into LLMs through various representation and tokenization\nmethods. We then categorize chemical LLMs into three distinct groups based on\nthe domain and modality of their input data, and discuss approaches for\nintegrating these inputs for LLMs. Furthermore, this paper delves into the\npretraining objectives with adaptations to chemical LLMs. After that, we\nexplore the diverse applications of LLMs in chemistry, including novel\nparadigms for their application in chemistry tasks. Finally, we identify\npromising research directions, including further integration with chemical\nknowledge, advancements in continual learning, and improvements in model\ninterpretability, paving the way for groundbreaking developments in the field.\n', ""  Large language models (LLMs) have made impressive progress in chemistry\napplications. However, the community lacks an LLM specifically designed for\nchemistry. The main challenges are two-fold: firstly, most chemical data and\nscientific knowledge are stored in structured databases, which limits the\nmodel's ability to sustain coherent dialogue when used directly. Secondly,\nthere is an absence of objective and fair benchmark that encompass most\nchemistry tasks. Here, we introduce ChemLLM, a comprehensive framework that\nfeatures the first LLM dedicated to chemistry. It also includes ChemData, a\ndataset specifically designed for instruction tuning, and ChemBench, a robust\nbenchmark covering nine essential chemistry tasks. ChemLLM is adept at\nperforming various tasks across chemical disciplines with fluid dialogue\ninteraction. Notably, ChemLLM achieves results comparable to GPT-4 on the core\nchemical tasks and demonstrates competitive performance with LLMs of similar\nsize in general scenarios. ChemLLM paves a new path for exploration in chemical\nstudies, and our method of incorporating structured chemical knowledge into\ndialogue systems sets a new standard for developing LLMs in various scientific\nfields. Codes, Datasets, and Model weights are publicly accessible at\nhttps://hf.co/AI4Chem\n"", ""  Chemical synthesis, which is crucial for advancing material synthesis and\ndrug discovery, impacts various sectors including environmental science and\nhealthcare. The rise of technology in chemistry has generated extensive\nchemical data, challenging researchers to discern patterns and refine synthesis\nprocesses. Artificial intelligence (AI) helps by analyzing data to optimize\nsynthesis and increase yields. However, AI faces challenges in processing\nliterature data due to the unstructured format and diverse writing style of\nchemical literature. To overcome these difficulties, we introduce an end-to-end\nAI agent framework capable of high-fidelity extraction from extensive chemical\nliterature. This AI agent employs large language models (LLMs) for prompt\ngeneration and iterative optimization. It functions as a chemistry assistant,\nautomating data collection and analysis, thereby saving manpower and enhancing\nperformance. Our framework's efficacy is evaluated using accuracy, recall, and\nF1 score of reaction condition data, and we compared our method with human\nexperts in terms of content correctness and time efficiency. The proposed\napproach marks a significant advancement in automating chemical literature\nextraction and demonstrates the potential for AI to revolutionize data\nmanagement and utilization in chemistry.\n""]",Large Language Models in Chemistry Applications,Applications of Large Language Models,Large Language Models,Large Language Models
430,15,430_learns_imitation_reinforcement_learning,"['learns', 'imitation', 'reinforcement', 'learning', 'exploration', 'learned', 'actions', 'demonstrations', 'tasks', 'robotic']","['imitation', 'rewards', 'demonstrations', 'compositional', 'states', 'routing', 'task', 'offline', 'manipulation', 'reward']","[""  Unsupervised skill discovery is a learning paradigm that aims to acquire\ndiverse behaviors without explicit rewards. However, it faces challenges in\nlearning complex behaviors and often leads to learning unsafe or undesirable\nbehaviors. For instance, in various continuous control tasks, current\nunsupervised skill discovery methods succeed in learning basic locomotions like\nstanding but struggle with learning more complex movements such as walking and\nrunning. Moreover, they may acquire unsafe behaviors like tripping and rolling\nor navigate to undesirable locations such as pitfalls or hazardous areas. In\nresponse, we present DoDont (Do's and Don'ts), an instruction-based skill\ndiscovery algorithm composed of two stages. First, in an instruction learning\nstage, DoDont leverages action-free instruction videos to train an instruction\nnetwork to distinguish desirable transitions from undesirable ones. Then, in\nthe skill learning stage, the instruction network adjusts the reward function\nof the skill discovery algorithm to weight the desired behaviors. Specifically,\nwe integrate the instruction network into a distance-maximizing skill discovery\nalgorithm, where the instruction network serves as the distance function.\nEmpirically, with less than 8 instruction videos, DoDont effectively learns\ndesirable behaviors and avoids undesirable ones across complex continuous\ncontrol tasks. Code and videos are available at\nhttps://mynsng.github.io/dodont/\n"", '  Online Imitation Learning methods struggle with the gap between extensive\nonline exploration space and limited expert trajectories, which hinder\nefficient exploration due to inaccurate task-aware reward estimation. Inspired\nby the findings from cognitive neuroscience that task decomposition could\nfacilitate cognitive processing for efficient learning, we hypothesize that an\nagent could estimate precise task-aware imitation rewards for efficient online\nexploration by decomposing the target task into the objectives of ""what to do""\nand the mechanisms of ""how to do"". In this work, we introduce the hybrid\nKey-state guided Online Imitation (KOI) learning approach, which leverages the\nintegration of semantic and motion key states as guidance for task-aware reward\nestimation. Initially, we utilize the visual-language models to segment the\nexpert trajectory into semantic key states, indicating the objectives of ""what\nto do"". Within the intervals between semantic key states, optical flow is\nemployed to capture motion key states to understand the process of ""how to do"".\nBy integrating a thorough grasp of both semantic and motion key states, we\nrefine the trajectory-matching reward computation, encouraging task-aware\nexploration for efficient online imitation learning. Our experiment results\nprove that our method is more sample efficient in the Meta-World and LIBERO\nenvironments. We also conduct real-world robotic manipulation experiments to\nvalidate the efficacy of our method, demonstrating the practical applicability\nof our KOI method.\n', '  One-shot Imitation Learning~(OSIL) aims to imbue AI agents with the ability\nto learn a new task from a single demonstration. To supervise the learning,\nOSIL typically requires a prohibitively large number of paired expert\ndemonstrations -- i.e. trajectories corresponding to different variations of\nthe same semantic task. To overcome this limitation, we introduce the\nsemi-supervised OSIL problem setting, where the learning agent is presented\nwith a large dataset of trajectories with no task labels (i.e. an unpaired\ndataset), along with a small dataset of multiple demonstrations per semantic\ntask (i.e. a paired dataset). This presents a more realistic and practical\nembodiment of few-shot learning and requires the agent to effectively leverage\nweak supervision from a large dataset of trajectories. Subsequently, we develop\nan algorithm specifically applicable to this semi-supervised OSIL setting. Our\napproach first learns an embedding space where different tasks cluster\nuniquely. We utilize this embedding space and the clustering it supports to\nself-generate pairings between trajectories in the large unpaired dataset.\nThrough empirical results on simulated control tasks, we demonstrate that OSIL\nmodels trained on such self-generated pairings are competitive with OSIL models\ntrained with ground-truth labels, presenting a major advancement in the\nlabel-efficiency of OSIL.\n']",Imitation Learning and Reinforcement Exploration,Imitation Learning and Robotics,Robotics and Artificial Intelligence,Robotics and Artificial Intelligence
431,15,431_dimensionality_dimensional_multidimensional_dimension,"['dimensionality', 'dimensional', 'multidimensional', 'dimension', 'manifolds', 'manifold', 'clustering', 'eigenmaps', 'embedding', 'embeddings']","['dimensional', 'dimensionality', 'manifold', 'reduction', 'nonlinear', 'dimension', 'structures', 'distance', 'subsurface', 'spaces']","['  Dimensionality reduction techniques map values from a high dimensional space\nto one with a lower dimension. The result is a space which requires less\nphysical memory and has a faster distance calculation. These techniques are\nwidely used where required properties of the reduced-dimension space give an\nacceptable accuracy with respect to the original space. Many such transforms\nhave been described. They have been classified in two main groups: linear and\ntopological. Linear methods such as Principal Component Analysis (PCA) and\nRandom Projection (RP) define matrix-based transforms into a lower dimension of\nEuclidean space. Topological methods such as Multidimensional Scaling (MDS)\nattempt to preserve higher-level aspects such as the nearest-neighbour\nrelation, and some may be applied to non-Euclidean spaces. Here, we introduce\nnSimplex Zen, a novel topological method of reducing dimensionality. Like MDS,\nit relies only upon pairwise distances measured in the original space. The use\nof distances, rather than coordinates, allows the technique to be applied to\nboth Euclidean and other Hilbert spaces, including those governed by Cosine,\nJensen-Shannon and Quadratic Form distances. We show that in almost all cases,\ndue to geometric properties of high-dimensional spaces, our new technique gives\nbetter properties than others, especially with reduction to very low\ndimensions.\n', '  The characteristics of data like distribution and heterogeneity, become more\ncomplex and counterintuitive as the dimensionality increases. This phenomenon\nis known as curse of dimensionality, where common patterns and relationships\n(e.g., internal and boundary pattern) that hold in low-dimensional space may be\ninvalid in higher-dimensional space. It leads to a decreasing performance for\nthe regression, classification or clustering models or algorithms. Curse of\ndimensionality can be attributed to many causes. In this paper, we first\nsummarize five challenges associated with manipulating high-dimensional data,\nand explains the potential causes for the failure of regression, classification\nor clustering tasks. Subsequently, we delve into two major causes of the curse\nof dimensionality, distance concentration and manifold effect, by performing\ntheoretical and empirical analyses. The results demonstrate that nearest\nneighbor search (NNS) using three typical distance measurements, Minkowski\ndistance, Chebyshev distance, and cosine distance, becomes meaningless as the\ndimensionality increases. Meanwhile, the data incorporates more redundant\nfeatures, and the variance contribution of principal component analysis (PCA)\nis skewed towards a few dimensions. By interpreting the causes of the curse of\ndimensionality, we can better understand the limitations of current models and\nalgorithms, and drive to improve the performance of data analysis and machine\nlearning tasks in high-dimensional space.\n', ""  Dimensionality reduction methods are employed to decrease data\ndimensionality, either to enhance machine learning performance or to facilitate\ndata visualization in two or three-dimensional spaces. These methods typically\nfall into two categories: feature selection and feature transformation. Feature\nselection retains significant features, while feature transformation projects\ndata into a lower-dimensional space, with linear and nonlinear methods. While\nnonlinear methods excel in preserving local structures and capturing nonlinear\nrelationships, they may struggle with interpreting global structures and can be\ncomputationally intensive. Recent algorithms, such as the t-SNE, UMAP, TriMap,\nand PaCMAP prioritize preserving local structures, often at the expense of\naccurately representing global structures, leading to clusters being spread out\nmore in lower-dimensional spaces. Moreover, these methods heavily rely on\nhyperparameters, making their results sensitive to parameter settings. To\naddress these limitations, this study introduces a clustering-based approach,\nnamely CBMAP (Clustering-Based Manifold Approximation and Projection), for\ndimensionality reduction. CBMAP aims to preserve both global and local\nstructures, ensuring that clusters in lower-dimensional spaces closely resemble\nthose in high-dimensional spaces. Experimental evaluations on benchmark\ndatasets demonstrate CBMAP's efficacy, offering speed, scalability, and minimal\nreliance on hyperparameters. Importantly, CBMAP enables low-dimensional\nprojection of test data, addressing a critical need in machine learning\napplications. CBMAP is made freely available at\nhttps://github.com/doganlab/cbmap and can be installed from the Python Package\nDirectory (PyPI) software repository with the command pip install cbmap.\n""]",Dimensionality Reduction Techniques,Dimensionality Reduction and Data Visualization Techniques,Data Analysis and Visualization,Data Analysis and Visualization
432,15,432_adversarial_attacks_cyberattacks_attack,"['adversarial', 'attacks', 'cyberattacks', 'attack', 'security', 'supervised', 'mitigation', 'threats', 'vulnerability', 'grid']","['grid', 'grids', 'cyber', 'smart', 'attacks', 'power', 'attack', 'eventful', 'defense', 'proactive']","['  Detection of cyber attacks in smart power distribution grids with unbalanced\nconfigurations poses challenges due to the inherent nonlinear nature of these\nuncertain and stochastic systems. It originates from the intermittent\ncharacteristics of the distributed energy resources (DERs) generation and load\nvariations. Moreover, the unknown behavior of cyber attacks, especially false\ndata injection attacks (FDIAs) in the distribution grids with complex temporal\ncorrelations and the limited amount of labeled data increases the vulnerability\nof the grids and imposes a high risk in the secure and reliable operation of\nthe grids. To address these challenges, this paper proposes an unsupervised\nadversarial autoencoder (AAE) model to detect FDIAs in unbalanced power\ndistribution grids integrated with DERs, i.e., PV systems and wind generation.\nThe proposed method utilizes long short-term memory (LSTM) in the structure of\nthe autoencoder to capture the temporal dependencies in the time-series\nmeasurements and leverages the power of generative adversarial networks (GANs)\nfor better reconstruction of the input data. The advantage of the proposed\ndata-driven model is that it can detect anomalous points for the system\noperation without reliance on abstract models or mathematical representations.\nTo evaluate the efficacy of the approach, it is tested on IEEE 13-bus and\n123-bus systems with historical meteorological data (wind speed, ambient\ntemperature, and solar irradiance) as well as historical real-world load data\nunder three types of data falsification functions. The comparison of the\ndetection results of the proposed model with other unsupervised learning\nmethods verifies its superior performance in detecting cyber attacks in\nunbalanced power distribution grids.\n', '  In smart electrical grids, fault detection tasks may have a high impact on\nsociety due to their economic and critical implications. In the recent years,\nnumerous smart grid applications, such as defect detection and load\nforecasting, have embraced data-driven methodologies. The purpose of this study\nis to investigate the challenges associated with the security of machine\nlearning (ML) applications in the smart grid scenario. Indeed, the robustness\nand security of these data-driven algorithms have not been extensively studied\nin relation to all power grid applications. We demonstrate first that the deep\nneural network method used in the smart grid is susceptible to adversarial\nperturbation. Then, we highlight how studies on fault localization and type\nclassification illustrate the weaknesses of present ML algorithms in smart\ngrids to various adversarial attacks\n', '  In this study, we conduct a comprehensive review of smart grid security,\nexploring system architectures, attack methodologies, defense strategies, and\nfuture research opportunities. We provide an in-depth analysis of various\nattack vectors, focusing on new attack surfaces introduced by advanced\ncomponents in smart grids. The review particularly includes an extensive\nanalysis of coordinated attacks that incorporate multiple attack strategies and\nexploit vulnerabilities across various smart grid components to increase their\nadverse impact, demonstrating the complexity and potential severity of these\nthreats. Following this, we examine innovative detection and mitigation\nstrategies, including game theory, graph theory, blockchain, and machine\nlearning, discussing their advancements in counteracting evolving threats and\nassociated research challenges. In particular, our review covers a thorough\nexamination of widely used machine learning-based mitigation strategies,\nanalyzing their applications and research challenges spanning across\nsupervised, unsupervised, semi-supervised, ensemble, and reinforcement\nlearning. Further, we outline future research directions and explore new\ntechniques and concerns. We first discuss the research opportunities for\nexisting and emerging strategies, and then explore the potential role of new\ntechniques, such as large language models (LLMs), and the emerging threat of\nadversarial machine learning in the future of smart grid security.\n']",Smart Grid Security and Cyber Attack Detection,Cybersecurity and Artificial Intelligence in Emerging Technologies,Cybersecurity and Artificial Intelligence,Cybersecurity and Artificial Intelligence
433,15,433_analogies_analogical_analogy_analogykb,"['analogies', 'analogical', 'analogy', 'analogykb', 'semantic', 'cognition', 'analogous', 'cognitive', 'reasoning', 'thinking']","['analogies', 'analogical', 'analogy', 'reasoning', 'humans', 'cognition', 'cognitive', 'analogous', 'ability', 'distractors']","['  Analogical reasoning is a fundamental cognitive ability of humans. However,\ncurrent language models (LMs) still struggle to achieve human-like performance\nin analogical reasoning tasks due to a lack of resources for model training. In\nthis work, we address this gap by proposing ANALOGYKB, a million-scale analogy\nknowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB\nidentifies two types of analogies from the KGs: 1) analogies of the same\nrelations, which can be directly extracted from the KGs, and 2) analogies of\nanalogous relations, which are identified with a selection and filtering\npipeline enabled by large language models (LLMs), followed by minor human\nefforts for data quality control. Evaluations on a series of datasets of two\nanalogical reasoning tasks (analogy recognition and generation) demonstrate\nthat ANALOGYKB successfully enables both smaller LMs and LLMs to gain better\nanalogical reasoning capabilities.\n', '  As a core cognitive skill that enables the transferability of information\nacross domains, analogical reasoning has been extensively studied for both\nhumans and computational models. However, while cognitive theories of analogy\noften focus on narratives and study the distinction between surface,\nrelational, and system similarities, existing work in natural language\nprocessing has a narrower focus as far as relational analogies between word\npairs. This gap brings a natural question: can state-of-the-art large language\nmodels (LLMs) detect system analogies between narratives? To gain insight into\nthis question and extend word-based relational analogies to relational system\nanalogies, we devise a comprehensive computational framework that\noperationalizes dominant theories of analogy, using narrative elements to\ncreate surface and system mappings. Leveraging the interplay between these\nmappings, we create a binary task and benchmark for Analogical Reasoning on\nNarratives (ARN), covering four categories of far (cross-domain)/near\n(within-domain) analogies and disanalogies. We show that while all LLMs can\nlargely recognize near analogies, even the largest ones struggle with far\nanalogies in a zero-shot setting, with GPT4.0 scoring below random. Guiding the\nmodels through solved examples and chain-of-thought reasoning enhances their\nanalogical reasoning ability. Yet, since even in the few-shot setting, the best\nmodel only performs halfway between random and humans, ARN opens exciting\ndirections for computational analogical reasoners.\n', '  While analogies are a common way to evaluate word embeddings in NLP, it is\nalso of interest to investigate whether or not analogical reasoning is a task\nin itself that can be learned. In this paper, we test several ways to learn\nbasic analogical reasoning, specifically focusing on analogies that are more\ntypical of what is used to evaluate analogical reasoning in humans than those\nin commonly used NLP benchmarks. Our experiments find that models are able to\nlearn analogical reasoning, even with a small amount of data. We additionally\ncompare our models to a dataset with a human baseline, and find that after\ntraining, models approach human performance.\n']",Analogical Reasoning and Knowledge Bases,Analogical Reasoning and Knowledge Representation,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
433,15,433_analogies_analogical_analogy_analogykb,"['analogies', 'analogical', 'analogy', 'analogykb', 'semantic', 'cognition', 'analogous', 'cognitive', 'reasoning', 'thinking']","['analogies', 'analogical', 'analogy', 'reasoning', 'humans', 'cognition', 'cognitive', 'analogous', 'ability', 'distractors']","['  Analogical reasoning is a fundamental cognitive ability of humans. However,\ncurrent language models (LMs) still struggle to achieve human-like performance\nin analogical reasoning tasks due to a lack of resources for model training. In\nthis work, we address this gap by proposing ANALOGYKB, a million-scale analogy\nknowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB\nidentifies two types of analogies from the KGs: 1) analogies of the same\nrelations, which can be directly extracted from the KGs, and 2) analogies of\nanalogous relations, which are identified with a selection and filtering\npipeline enabled by large language models (LLMs), followed by minor human\nefforts for data quality control. Evaluations on a series of datasets of two\nanalogical reasoning tasks (analogy recognition and generation) demonstrate\nthat ANALOGYKB successfully enables both smaller LMs and LLMs to gain better\nanalogical reasoning capabilities.\n', '  As a core cognitive skill that enables the transferability of information\nacross domains, analogical reasoning has been extensively studied for both\nhumans and computational models. However, while cognitive theories of analogy\noften focus on narratives and study the distinction between surface,\nrelational, and system similarities, existing work in natural language\nprocessing has a narrower focus as far as relational analogies between word\npairs. This gap brings a natural question: can state-of-the-art large language\nmodels (LLMs) detect system analogies between narratives? To gain insight into\nthis question and extend word-based relational analogies to relational system\nanalogies, we devise a comprehensive computational framework that\noperationalizes dominant theories of analogy, using narrative elements to\ncreate surface and system mappings. Leveraging the interplay between these\nmappings, we create a binary task and benchmark for Analogical Reasoning on\nNarratives (ARN), covering four categories of far (cross-domain)/near\n(within-domain) analogies and disanalogies. We show that while all LLMs can\nlargely recognize near analogies, even the largest ones struggle with far\nanalogies in a zero-shot setting, with GPT4.0 scoring below random. Guiding the\nmodels through solved examples and chain-of-thought reasoning enhances their\nanalogical reasoning ability. Yet, since even in the few-shot setting, the best\nmodel only performs halfway between random and humans, ARN opens exciting\ndirections for computational analogical reasoners.\n', '  While analogies are a common way to evaluate word embeddings in NLP, it is\nalso of interest to investigate whether or not analogical reasoning is a task\nin itself that can be learned. In this paper, we test several ways to learn\nbasic analogical reasoning, specifically focusing on analogies that are more\ntypical of what is used to evaluate analogical reasoning in humans than those\nin commonly used NLP benchmarks. Our experiments find that models are able to\nlearn analogical reasoning, even with a small amount of data. We additionally\ncompare our models to a dataset with a human baseline, and find that after\ntraining, models approach human performance.\n']",Analogical Reasoning and Knowledge Bases,Analogical Reasoning and Knowledge Representation,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
434,15,434_bitrate_streaming_bitstream_videos,"['bitrate', 'streaming', 'bitstream', 'videos', 'encoder', 'compression', 'encoding', 'vvc', 'decoding', 'decoder']","['bitrate', 'video', 'codecs', 'transcoding', 'codec', 'compression', 'ladder', 'streaming', 'distortion', 'quality']","['  Volumetric video based on Neural Radiance Field (NeRF) holds vast potential\nfor various 3D applications, but its substantial data volume poses significant\nchallenges for compression and transmission. Current NeRF compression lacks the\nflexibility to adjust video quality and bitrate within a single model for\nvarious network and device capacities. To address these issues, we propose HPC,\na novel hierarchical progressive volumetric video coding framework achieving\nvariable bitrate using a single model. Specifically, HPC introduces a\nhierarchical representation with a multi-resolution residual radiance field to\nreduce temporal redundancy in long-duration sequences while simultaneously\ngenerating various levels of detail. Then, we propose an end-to-end progressive\nlearning approach with a multi-rate-distortion loss function to jointly\noptimize both hierarchical representation and compression. Our HPC trained only\nonce can realize multiple compression levels, while the current methods need to\ntrain multiple fixed-bitrate models for different rate-distortion (RD)\ntradeoffs. Extensive experiments demonstrate that HPC achieves flexible quality\nlevels with variable bitrate by a single model and exhibits competitive RD\nperformance, even outperforming fixed-bitrate models across various datasets.\n', '  Providing high-quality video with efficient bitrate is a main challenge in\nvideo industry. The traditional one-size-fits-all scheme for bitrate ladders is\ninefficient and reaching the best content-aware decision computationally\nimpractical due to extensive encodings required. To mitigate this, we propose a\nbitrate and complexity efficient bitrate ladder prediction method using\ntransfer learning and spatio-temporal features. We propose: (1) using feature\nmaps from well-known pre-trained DNNs to predict rate-quality behavior with\nlimited training data; and (2) improving highest quality rung efficiency by\npredicting minimum bitrate for top quality and using it for the top rung. The\nmethod tested on 102 video scenes demonstrates 94.1% reduction in complexity\nversus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning\nwas thoroughly studied through four networks and ablation studies.\n', '  Adaptive video streaming requires efficient bitrate ladder construction to\nmeet heterogeneous network conditions and end-user demands. Per-title optimized\nencoding typically traverses numerous encoding parameters to search the\nPareto-optimal operating points for each video. Recently, researchers have\nattempted to predict the content-optimized bitrate ladder for pre-encoding\noverhead reduction. However, existing methods commonly estimate the encoding\nparameters on the Pareto front and still require subsequent pre-encodings. In\nthis paper, we propose to directly predict the optimal transcoding resolution\nat each preset bitrate for efficient bitrate ladder construction. We adopt a\nTemporal Attentive Gated Recurrent Network to capture spatial-temporal features\nand predict transcoding resolutions as a multi-task classification problem. We\ndemonstrate that content-optimized bitrate ladders can thus be efficiently\ndetermined without any pre-encoding. Our method well approximates the\nground-truth bitrate-resolution pairs with a slight Bj{\\o}ntegaard Delta rate\nloss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.\n']",Video Compression and Bitrate Optimization,Image and Video Compression Techniques,Image and Video Processing,Image and Video Processing
435,15,435_valuation_valuing_shapley_value,"['valuation', 'valuing', 'shapley', 'value', 'data', 'values', 'ml', 'quantifying', 'usefulness', 'estimate']","['valuation', 'value', 'data', 'values', 'detrimental', 'ecoval', 'utility', 'deletions', 'asymptotic', 'solid']","['  As data plays an increasingly pivotal role in decision-making, the emergence\nof data markets underscores the growing importance of data valuation. Within\nthe machine learning landscape, Data Shapley stands out as a widely embraced\nmethod for data valuation. However, a limitation of Data Shapley is its\nassumption of a fixed dataset, contrasting with the dynamic nature of\nreal-world applications where data constantly evolves and expands. This paper\nestablishes the relationship between Data Shapley and infinite-order\nU-statistics and addresses this limitation by quantifying the uncertainty of\nData Shapley with changes in data distribution from the perspective of\nU-statistics. We make statistical inferences on data valuation to obtain\nconfidence intervals for the estimations. We construct two different algorithms\nto estimate this uncertainty and provide recommendations for their applicable\nsituations. We also conduct a series of experiments on various datasets to\nverify asymptotic normality and propose a practical trading scenario enabled by\nthis method.\n', '  Data valuation has garnered increasing attention in recent years, given the\ncritical role of high-quality data in various applications, particularly in\nmachine learning tasks. There are diverse technical avenues to quantify the\nvalue of data within a corpus. While Shapley value-based methods are among the\nmost widely used techniques in the literature due to their solid theoretical\nfoundation, the accurate calculation of Shapley values is often intractable,\nleading to the proposal of numerous approximated calculation methods. Despite\nsignificant progress, nearly all existing methods overlook the utilization of\ndistribution information of values within a data corpus. In this paper, we\ndemonstrate that both global and local statistical information of value\ndistributions hold significant potential for data valuation within the context\nof machine learning. Firstly, we explore the characteristics of both global and\nlocal value distributions across several simulated and real data corpora.\nUseful observations and clues are obtained. Secondly, we propose a new data\nvaluation method that estimates Shapley values by incorporating the explored\ndistribution characteristics into an existing method, AME. Thirdly, we present\na new path to address the dynamic data valuation problem by formulating an\noptimization problem that integrates information of both global and local value\ndistributions. Extensive experiments are conducted on Shapley value estimation,\nvalue-based data removal/adding, mislabeled data detection, and\nincremental/decremental data valuation. The results showcase the effectiveness\nand efficiency of our proposed methodologies, affirming the significant\npotential of global and local value distributions in data valuation.\n', '  Measuring the value of individual samples is critical for many data-driven\ntasks, e.g., the training of a deep learning model. Recent literature witnesses\nthe substantial efforts in developing data valuation methods. The primary data\nvaluation methodology is based on the Shapley value from game theory, and\nvarious methods are proposed along this path. {Even though Shapley value-based\nvaluation has solid theoretical basis, it is entirely an experiment-based\napproach and no valuation model has been constructed so far.} In addition,\ncurrent data valuation methods ignore the interpretability of the output\nvalues, despite an interptable data valuation method is of great helpful for\napplications such as data pricing. This study aims to answer an important\nquestion: is data valuation learnable and interpretable? A learned valuation\nmodel have several desirable merits such as fixed number of parameters and\nknowledge reusability. An intrepretable data valuation model can explain why a\nsample is valuable or invaluable. To this end, two new data value modeling\nframeworks are proposed, in which a multi-layer perception~(MLP) and a new\nregression tree are utilized as specific base models for model training and\ninterpretability, respectively. Extensive experiments are conducted on\nbenchmark datasets. {The experimental results provide a positive answer for the\nquestion.} Our study opens up a new technical path for the assessing of data\nvalues. Large data valuation models can be built across many different\ndata-driven tasks, which can promote the widespread application of data\nvaluation.\n']",Data Valuation with Machine Learning,Interpretable Machine Learning and Data Valuation,Explainable Artificial Intelligence,Artificial Intelligence and Machine Learning Interpretability and Explainability
436,15,436_regularization_inversions_inverse_inversion,"['regularization', 'inversions', 'inverse', 'inversion', 'neural', 'deep', 'cnn', 'invertible', 'encoder', 'imaging']","['inverse', 'inversion', 'scattering', 'geophysical', 'imaging', 'regularization', 'electromagnetic', 'invertible', 'inversions', 'aperture']","[""  Inverse scattering problems are inherently challenging, given the fact they\nare ill-posed and nonlinear. This paper presents a powerful deep learning-based\napproach that relies on generative adversarial networks to accurately and\nefficiently reconstruct randomly-shaped two-dimensional dielectric objects from\namplitudes of multi-frequency scattered electric fields. An adversarial\nautoencoder (AAE) is trained to learn to generate the scatterer's geometry from\na lower-dimensional latent representation constrained to adhere to the Gaussian\ndistribution. A cohesive inverse neural network (INN) framework is set up\ncomprising a sequence of appropriately designed dense layers, the\nalready-trained generator as well as a separately trained forward neural\nnetwork. The images reconstructed at the output of the inverse network are\nvalidated through comparison with outputs from the forward neural network,\naddressing the non-uniqueness challenge inherent to electromagnetic (EM)\nimaging problems. The trained INN demonstrates an enhanced robustness,\nevidenced by a mean binary cross-entropy (BCE) loss of $0.13$ and a structure\nsimilarity index (SSI) of $0.90$. The study not only demonstrates a significant\nreduction in computational load, but also marks a substantial improvement over\ntraditional objective-function-based methods. It contributes both to the fields\nof machine learning and EM imaging by offering a real-time quantitative imaging\napproach. The results obtained with the simulated data, for both training and\ntesting, yield promising results and may open new avenues for radio-frequency\ninverse imaging.\n"", '  Physics-Informed Neural Networks (PINNs) are a machine learning technique for\nsolving partial differential equations (PDEs) by incorporating PDEs as loss\nterms in neural networks and minimizing the loss function during training.\nTomographic imaging, a method to reconstruct internal properties from external\nmeasurement data, is highly complex and ill-posed, making it an inverse\nproblem. Recently, PINNs have shown significant potential in computational\nfluid dynamics (CFD) and have advantages in solving inverse problems. However,\nexisting research has primarily focused on semi-inverse Electrical Impedance\nTomography (EIT), where internal electric potentials are accessible. The\npractical full inverse EIT problem, where only boundary voltage measurements\nare available, remains challenging. To address this, we propose a two-stage\nhybrid learning framework combining Convolutional Neural Networks (CNNs) and\nPINNs to solve the full inverse EIT problem. This framework integrates\ndata-driven and model-driven approaches, combines supervised and unsupervised\nlearning, and decouples the forward and inverse problems within the PINN\nframework in EIT. Stage I: a U-Net constructs an end-to-end mapping from\nboundary voltage measurements to the internal potential distribution using\nsupervised learning. Stage II: a Multilayer Perceptron (MLP)-based PINN takes\nthe predicted internal potentials as input to solve for the conductivity\ndistribution through unsupervised learning.\n', '  Regularization is critical for solving ill-posed geophysical inverse\nproblems. Explicit regularization is often used, but there are opportunities to\nexplore the implicit regularization effects that are inherent in a Neural\nNetwork structure. Researchers have discovered that the Convolutional Neural\nNetwork (CNN) architecture inherently enforces a regularization that is\nadvantageous for addressing diverse inverse problems in computer vision,\nincluding de-noising and in-painting. In this study, we examine the\napplicability of this implicit regularization to geophysical inversions. The\nCNN maps an arbitrary vector to the model space. The predicted subsurface model\nis then fed into a forward numerical simulation to generate corresponding\npredicted measurements. Subsequently, the objective function value is computed\nby comparing these predicted measurements with the observed measurements. The\nbackpropagation algorithm is employed to update the trainable parameters of the\nCNN during the inversion. Note that the CNN in our proposed method does not\nrequire training before the inversion, rather, the CNN weights are estimated in\nthe inversion process, hence this is a test-time learning (TTL) approach. In\nthis study, we choose to focus on the Direct Current (DC) resistivity inverse\nproblem, which is representative of typical Tikhonov-style geophysical\ninversions (e.g. gravity, electromagnetic, etc.), to test our hypothesis. The\nexperimental results demonstrate that the implicit regularization can be useful\nin some DC resistivity inversions. We also provide a discussion of the\npotential sources of this implicit regularization introduced from the CNN\narchitecture and discuss some practical guides for applying the proposed method\nto other geophysical methods.\n']",Deep Learning for Inverse Problems and Imaging,Machine Learning for Inverse Problems,Machine Learning and Optimization,Machine Learning and Artificial Intelligence
437,15,437_recommender_federated_privacy_collaborative,"['recommender', 'federated', 'privacy', 'collaborative', 'personalization', 'personalized', 'collaboratively', 'shared', 'recommendation', 'sharing']","['recommendation', 'privacy', 'item', 'server', 'users', 'recommendations', 'user', 'recommender', 'protection', 'clients']","['  Sequential recommender systems have made significant progress. Recently, due\nto increasing concerns about user data privacy, some researchers have\nimplemented federated learning for sequential recommendation, a.k.a., Federated\nSequential Recommender Systems (FedSeqRecs), in which a public sequential\nrecommender model is shared and frequently transmitted between a central server\nand clients to achieve collaborative learning. Although these solutions\nmitigate user privacy to some extent, they present two significant limitations\nthat affect their practical usability: (1) They require a globally shared\nsequential recommendation model. However, in real-world scenarios, the\nrecommendation model constitutes a critical intellectual property for platform\nand service providers. Therefore, service providers may be reluctant to\ndisclose their meticulously developed models. (2) The communication costs are\nhigh as they correlate with the number of model parameters. This becomes\nparticularly problematic as the current FedSeqRec will be inapplicable when\nsequential recommendation marches into a large language model era.\n  To overcome the above challenges, this paper proposes a parameter\ntransmission-free federated sequential recommendation framework (PTF-FSR),\nwhich ensures both model and data privacy protection to meet the privacy needs\nof service providers and system users alike. Furthermore, since PTF-FSR only\ntransmits prediction results under privacy protection, which are independent of\nmodel sizes, this new federated learning architecture can accommodate more\ncomplex and larger sequential recommendation models. Extensive experiments\nconducted on three widely used recommendation datasets, employing various\nsequential recommendation models from both ID-based and ID-free paradigms,\ndemonstrate the effectiveness and generalization capability of our proposed\nframework.\n', ""  The federated recommendation system is an emerging AI service architecture\nthat provides recommendation services in a privacy-preserving manner. Using\nuser-relation graphs to enhance federated recommendations is a promising topic.\nHowever, it is still an open challenge to construct the user-relation graph\nwhile preserving data locality-based privacy protection in federated settings.\nInspired by a simple motivation, similar users share a similar vision\n(embeddings) to the same item set, this paper proposes a novel Graph-guided\nPersonalization for Federated Recommendation (GPFedRec). The proposed method\nconstructs a user-relation graph from user-specific personalized item\nembeddings at the server without accessing the users' interaction records. The\npersonalized item embedding is locally fine-tuned on each device, and then a\nuser-relation graph will be constructed by measuring the similarity among\nclient-specific item embeddings. Without accessing users' historical\ninteractions, we embody the data locality-based privacy protection of vanilla\nfederated learning. Furthermore, a graph-guided aggregation mechanism is\ndesigned to leverage the user-relation graph and federated optimization\nframework simultaneously. Extensive experiments on five benchmark datasets\ndemonstrate GPFedRec's superior performance. The in-depth study validates that\nGPFedRec can generally improve existing federated recommendation methods as a\nplugin while keeping user privacy safe. Code is available to ease\nreproducibility\n"", ""  With the growing concerns regarding user data privacy, Federated Recommender\nSystem (FedRec) has garnered significant attention recently due to its\nprivacy-preserving capabilities. Existing FedRecs generally adhere to a\nlearning protocol in which a central server shares a global recommendation\nmodel with clients, and participants achieve collaborative learning by\nfrequently communicating the model's public parameters. Nevertheless, this\nlearning framework has two drawbacks that limit its practical usability: (1) It\nnecessitates a global-sharing recommendation model; however, in real-world\nscenarios, information related to the recommender model, including its\nalgorithm and parameters, constitutes the platforms' intellectual property.\nHence, service providers are unlikely to release such information actively. (2)\nThe communication costs of model parameter transmission are expensive since the\nmodel parameters are usually high-dimensional matrices. With the model size\nincreasing, the communication burden will be the bottleneck for such\ntraditional FedRecs.\n  Given the above limitations, this paper introduces a novel parameter\ntransmission-free federated recommendation framework that balances the\nprotection between users' data privacy and platforms' model privacy, namely\nPTF-FedRec. Specifically, participants in PTF-FedRec collaboratively exchange\nknowledge by sharing their predictions within a privacy-preserving mechanism.\nThrough this way, the central server can learn a recommender model without\ndisclosing its model parameters or accessing clients' raw data, preserving both\nthe server's model privacy and users' data privacy. Besides, since clients and\nthe central server only need to communicate prediction scores which are just a\nfew real numbers, the overhead is significantly reduced compared to traditional\nFedRecs. The code is available\nat\\url{https://github.com/hi-weiyuan/PTF-FedRec}.\n""]",Federated Recommendation Systems with Privacy Protection,Federated Learning for Privacy-Preserving Machine Learning,Machine Learning and Data Privacy,Machine Learning and Data Privacy
438,15,438_safegen_unsafe_malicious_safety,"['safegen', 'unsafe', 'malicious', 'safety', 'protecting', 'prevent', 'redacts', 'images', 'videos', 'content']","['unsafe', 'safety', 'prompts', 'content', 'videos', 'text', 'safe', 'inappropriate', 'prompt', 'generation']","['  Large-scale vision-and-language models, such as CLIP, are typically trained\non web-scale data, which can introduce inappropriate content and lead to the\ndevelopment of unsafe and biased behavior. This, in turn, hampers their\napplicability in sensitive and trustworthy contexts and could raise significant\nconcerns in their adoption. Our research introduces a novel approach to\nenhancing the safety of vision-and-language models by diminishing their\nsensitivity to NSFW (not safe for work) inputs. In particular, our methodology\nseeks to sever ""toxic"" linguistic and visual concepts, unlearning the linkage\nbetween unsafe linguistic or visual items and unsafe regions of the embedding\nspace. We show how this can be done by fine-tuning a CLIP model on synthetic\ndata obtained from a large language model trained to convert between safe and\nunsafe sentences, and a text-to-image generator. We conduct extensive\nexperiments on the resulting embedding space for cross-modal retrieval,\ntext-to-image, and image-to-text generation, where we show that our model can\nbe remarkably employed with pre-trained generative models. Our source code and\ntrained models are available at: https://github.com/aimagelab/safe-clip.\n', ""  Text-to-image (T2I) models, such as Stable Diffusion, have exhibited\nremarkable performance in generating high-quality images from text descriptions\nin recent years. However, text-to-image models may be tricked into generating\nnot-safe-for-work (NSFW) content, particularly in sexual scenarios. Existing\ncountermeasures mostly focus on filtering inappropriate inputs and outputs, or\nsuppressing improper text embeddings, which can block explicit NSFW-related\ncontent (e.g., naked or sexy) but may still be vulnerable to adversarial\nprompts inputs that appear innocent but are ill-intended. In this paper, we\npresent SafeGen, a framework to mitigate unsafe content generation by\ntext-to-image models in a text-agnostic manner. The key idea is to eliminate\nunsafe visual representations from the model regardless of the text input. In\nthis way, the text-to-image model is resistant to adversarial prompts since\nunsafe visual representations are obstructed from within. Extensive experiments\nconducted on four datasets demonstrate SafeGen's effectiveness in mitigating\nunsafe content generation while preserving the high-fidelity of benign images.\nSafeGen outperforms eight state-of-the-art baseline methods and achieves 99.1%\nsexual content removal performance. Furthermore, our constructed benchmark of\nadversarial prompts provides a basis for future development and evaluation of\nanti-NSFW-generation methods.\n"", ""  Video generation models (VGMs) have demonstrated the capability to synthesize\nhigh-quality output. It is important to understand their potential to produce\nunsafe content, such as violent or terrifying videos. In this work, we provide\na comprehensive understanding of unsafe video generation.\n  First, to confirm the possibility that these models could indeed generate\nunsafe videos, we choose unsafe content generation prompts collected from 4chan\nand Lexica, and three open-source SOTA VGMs to generate unsafe videos. After\nfiltering out duplicates and poorly generated content, we created an initial\nset of 2112 unsafe videos from an original pool of 5607 videos. Through\nclustering and thematic coding analysis of these generated videos, we identify\n5 unsafe video categories: Distorted/Weird, Terrifying, Pornographic,\nViolent/Bloody, and Political. With IRB approval, we then recruit online\nparticipants to help label the generated videos. Based on the annotations\nsubmitted by 403 participants, we identified 937 unsafe videos from the initial\nvideo set. With the labeled information and the corresponding prompts, we\ncreated the first dataset of unsafe videos generated by VGMs.\n  We then study possible defense mechanisms to prevent the generation of unsafe\nvideos. Existing defense methods in image generation focus on filtering either\ninput prompt or output results. We propose a new approach called Latent\nVariable Defense (LVD), which works within the model's internal sampling\nprocess. LVD can achieve 0.90 defense accuracy while reducing time and\ncomputing resources by 10x when sampling a large number of unsafe prompts.\n""]",Mitigating Unsafe Content in AI-Generated Media,Artificial Intelligence in Creative Industries,Artificial Intelligence in Creative Industries,Artificial Intelligence in Creative Industries
439,14,439_erasing_unlearning_erasures_erasure,"['erasing', 'unlearning', 'erasures', 'erasure', 'relearning', 'unlearn', 'erase', 'memorization', 'eraser', 'erased']","['erasure', 'concepts', 'erasing', 'concept', 'undesirable', 'illegal', 'unlearning', 'diffusion', 'misuse', 'adaptations']","['  Large-scale diffusion models, known for their impressive image generation\ncapabilities, have raised concerns among researchers regarding social impacts,\nsuch as the imitation of copyrighted artistic styles. In response, existing\napproaches turn to machine unlearning techniques to eliminate unsafe concepts\nfrom pre-trained models. However, these methods compromise the generative\nperformance and neglect the coupling among multi-concept erasures, as well as\nthe concept restoration problem. To address these issues, we propose a\nSeparable Multi-concept Eraser (SepME), which mainly includes two parts: the\ngeneration of concept-irrelevant representations and the weight decoupling. The\nformer aims to avoid unlearning substantial information that is irrelevant to\nforgotten concepts. The latter separates optimizable model weights, making each\nweight increment correspond to a specific concept erasure without affecting\ngenerative performance on other concepts. Specifically, the weight increment\nfor erasing a specified concept is formulated as a linear combination of\nsolutions calculated based on other known undesirable concepts. Extensive\nexperiments indicate the efficacy of our approach in eliminating concepts,\npreserving model performance, and offering flexibility in the erasure or\nrecovery of various concepts.\n', '  The rapid expansion of large-scale text-to-image diffusion models has raised\ngrowing concerns regarding their potential misuse in creating harmful or\nmisleading content. In this paper, we introduce MACE, a finetuning framework\nfor the task of mass concept erasure. This task aims to prevent models from\ngenerating images that embody unwanted concepts when prompted. Existing concept\nerasure methods are typically restricted to handling fewer than five concepts\nsimultaneously and struggle to find a balance between erasing concept synonyms\n(generality) and maintaining unrelated concepts (specificity). In contrast,\nMACE differs by successfully scaling the erasure scope up to 100 concepts and\nby achieving an effective balance between generality and specificity. This is\nachieved by leveraging closed-form cross-attention refinement along with LoRA\nfinetuning, collectively eliminating the information of undesirable concepts.\nFurthermore, MACE integrates multiple LoRAs without mutual interference. We\nconduct extensive evaluations of MACE against prior methods across four\ndifferent tasks: object erasure, celebrity erasure, explicit content erasure,\nand artistic style erasure. Our results reveal that MACE surpasses prior\nmethods in all evaluated tasks. Code is available at\nhttps://github.com/Shilin-LU/MACE.\n', '  Generating images from text has become easier because of the scaling of\ndiffusion models and advancements in the field of vision and language. These\nmodels are trained using vast amounts of data from the Internet. Hence, they\noften contain undesirable content such as copyrighted material. As it is\nchallenging to remove such data and retrain the models, methods for erasing\nspecific concepts from pre-trained models have been investigated. We propose a\nnovel concept-erasure method that updates the text encoder using few-shot\nunlearning in which a few real images are used. The discussion regarding the\ngenerated images after erasing a concept has been lacking. While there are\nmethods for specifying the transition destination for concepts, the validity of\nthe specified concepts is unclear. Our method implicitly achieves this by\ntransitioning to the latent concepts inherent in the model or the images. Our\nmethod can erase a concept within 10 s, making concept erasure more accessible\nthan ever before. Implicitly transitioning to related concepts leads to more\nnatural concept erasure. We applied the proposed method to various concepts and\nconfirmed that concept erasure can be achieved tens to hundreds of times faster\nthan with current methods. By varying the parameters to be updated, we obtained\nresults suggesting that, like previous research, knowledge is primarily\naccumulated in the feed-forward networks of the text encoder.\n']",Concept Erasure in Diffusion Models,Diffusion Models for Generative Tasks,Diffusion Models for Generative Tasks,Diffusion Models for Generative Tasks
440,14,440_reidentification_matching_discriminative_pedestrians,"['reidentification', 'matching', 'discriminative', 'pedestrians', 'pedestrian', 'persons', 'feature', 'identification', 'infrared', 'cameras']","['person', 'modality', 'old', 'infrared', 'identification', 'camera', 'visible', 'pedestrian', 'cross', 'instance']","['  Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to\nmatch pedestrian images of the same identity from different modalities without\nannotations. Existing works mainly focus on alleviating the modality gap by\naligning instance-level features of the unlabeled samples. However, the\nrelationships between cross-modality clusters are not well explored. To this\nend, we propose a novel bilateral cluster matching-based learning framework to\nreduce the modality gap by matching cross-modality clusters. Specifically, we\ndesign a Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM)\nalgorithm through optimizing the maximum matching problem in a bipartite graph.\nThen, the matched pairwise clusters utilize shared visible and infrared\npseudo-labels during the model training. Under such a supervisory signal, a\nModality-Specific and Modality-Agnostic (MSMA) contrastive learning framework\nis proposed to align features jointly at a cluster-level. Meanwhile, the\ncross-modality Consistency Constraint (CC) is proposed to explicitly reduce the\nlarge modality discrepancy. Extensive experiments on the public SYSU-MM01 and\nRegDB datasets demonstrate the effectiveness of the proposed method, surpassing\nstate-of-the-art approaches by a large margin of 8.76% mAP on average.\n', '  The Visible-Infrared Person Re-identification (VI ReID) aims to match visible\nand infrared images of the same pedestrians across non-overlapped camera views.\nThese two input modalities contain both invariant information, such as shape,\nand modality-specific details, such as color. An ideal model should utilize\nvaluable information from both modalities during training for enhanced\nrepresentational capability. However, the gap caused by modality-specific\ninformation poses substantial challenges for the VI ReID model to handle\ndistinct modality inputs simultaneously. To address this, we introduce the\nModality-aware and Instance-aware Visual Prompts (MIP) network in our work,\ndesigned to effectively utilize both invariant and specific information for\nidentification. Specifically, our MIP model is built on the transformer\narchitecture. In this model, we have designed a series of modality-specific\nprompts, which could enable our model to adapt to and make use of the specific\ninformation inherent in different modality inputs, thereby reducing the\ninterference caused by the modality gap and achieving better identification.\nBesides, we also employ each pedestrian feature to construct a group of\ninstance-specific prompts. These customized prompts are responsible for guiding\nour model to adapt to each pedestrian instance dynamically, thereby capturing\nidentity-level discriminative clues for identification. Through extensive\nexperiments on SYSU-MM01 and RegDB datasets, the effectiveness of both our\ndesigned modules is evaluated. Additionally, our proposed MIP performs better\nthan most state-of-the-art methods.\n', '  Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)\naims at learning modality-invariant features from unlabeled cross-modality\ndataset, which is crucial for practical applications in video surveillance\nsystems. The key to essentially address the USL-VI-ReID task is to solve the\ncross-modality data association problem for further heterogeneous joint\nlearning. To address this issue, we propose a Dual Optimal Transport Label\nAssignment (DOTLA) framework to simultaneously assign the generated labels from\none modality to its counterpart modality. The proposed DOTLA mechanism\nformulates a mutual reinforcement and efficient solution to cross-modality data\nassociation, which could effectively reduce the side-effects of some\ninsufficient and noisy label associations. Besides, we further propose a\ncross-modality neighbor consistency guided label refinement and regularization\nmodule, to eliminate the negative effects brought by the inaccurate supervised\nsignals, under the assumption that the prediction or label distribution of each\nexample should be similar to its nearest neighbors. Extensive experimental\nresults on the public SYSU-MM01 and RegDB datasets demonstrate the\neffectiveness of the proposed method, surpassing existing state-of-the-art\napproach by a large margin of 7.76% mAP on average, which even surpasses some\nsupervised VI-ReID methods.\n']",Visible-Infrared Person Re-identification,Visible-Infrared Person Re-identification,Multimodal Person Identification,Multimodal Person Identification
441,14,441_3d_3dis_3dcompat_scenes,"['3d', '3dis', '3dcompat', 'scenes', 'opensun3d', '3ddet', 'visual', 'vision', 'any2point', 'scene']","['vocabulary', 'scene', 'object', 'point', 'grounding', 'segmentation', 'clouds', 'vision', 'open', 'masks']","['  Open-vocabulary 3D object detection (OV-3DDet) aims to localize and recognize\nboth seen and previously unseen object categories within any new 3D scene.\nWhile language and vision foundation models have achieved success in handling\nvarious open-vocabulary tasks with abundant training data, OV-3DDet faces a\nsignificant challenge due to the limited availability of training data.\nAlthough some pioneering efforts have integrated vision-language models (VLM)\nknowledge into OV-3DDet learning, the full potential of these foundational\nmodels has yet to be fully exploited. In this paper, we unlock the textual and\nvisual wisdom to tackle the open-vocabulary 3D detection task by leveraging the\nlanguage and vision foundation models. We leverage a vision foundation model to\nprovide image-wise guidance for discovering novel classes in 3D scenes.\nSpecifically, we utilize a object detection vision foundation model to enable\nthe zero-shot discovery of objects in images, which serves as the initial seeds\nand filtering guidance to identify novel 3D objects. Additionally, to align the\n3D space with the powerful vision-language space, we introduce a hierarchical\nalignment approach, where the 3D feature space is aligned with the\nvision-language feature space using a pre-trained VLM at the instance,\ncategory, and scene levels. Through extensive experimentation, we demonstrate\nsignificant improvements in accuracy and generalization, highlighting the\npotential of foundation models in advancing open-vocabulary 3D object detection\nin real-world scenarios.\n', ""  While 3D MLLMs have achieved significant progress, they are restricted to\nobject and scene understanding and struggle to understand 3D spatial structures\nat the part level. In this paper, we introduce Kestrel, representing a novel\napproach that empowers 3D MLLMs with part-aware understanding, enabling better\ninterpretation and segmentation grounding of 3D objects at the part level.\nDespite its significance, the current landscape lacks tasks and datasets that\nendow and assess this capability. Therefore, we propose two novel tasks: (1)\nPart-Aware Point Grounding, the model is tasked with directly predicting a\npart-level segmentation mask based on user instructions, and (2) Part-Aware\nPoint Grounded Captioning, the model provides a detailed caption that includes\npart-level descriptions and their corresponding masks. To support learning and\nevaluating for these tasks, we introduce 3DCoMPaT Grounded Instructions Dataset\n(3DCoMPaT-GRIN). 3DCoMPaT-GRIN Vanilla, comprising 789k part-aware point\ncloud-instruction-segmentation mask triplets, is used to evaluate MLLMs'\nability of part-aware segmentation grounding. 3DCoMPaT-GRIN Grounded Caption,\ncontaining 107k part-aware point cloud-instruction-grounded caption triplets,\nassesses both MLLMs' part-aware language comprehension and segmentation\ngrounding capabilities. Our introduced tasks, dataset, and Kestrel represent a\npreliminary effort to bridge the gap between human cognition and 3D MLLMs,\ni.e., the ability to perceive and engage with the environment at both global\nand part levels. Extensive experiments on the 3DCoMPaT-GRIN show that Kestrel\ncan generate user-specified segmentation masks, a capability not present in any\nexisting 3D MLLM. Kestrel thus established a benchmark for evaluating the\npart-aware language comprehension and segmentation grounding of 3D objects.\nProject page at https://feielysia.github.io/Kestrel.github.io/\n"", '  Large 2D vision-language models (2D-LLMs) have gained significant attention\nby bridging Large Language Models (LLMs) with images using a simple projector.\nInspired by their success, large 3D point cloud-language models (3D-LLMs) also\nintegrate point clouds into LLMs. However, directly aligning point clouds with\nLLM requires expensive training costs, typically in hundreds of GPU-hours on\nA100, which hinders the development of 3D-LLMs. In this paper, we introduce\nMiniGPT-3D, an efficient and powerful 3D-LLM that achieves multiple SOTA\nresults while training for only 27 hours on one RTX 3090. Specifically, we\npropose to align 3D point clouds with LLMs using 2D priors from 2D-LLMs, which\ncan leverage the similarity between 2D and 3D visual information. We introduce\na novel four-stage training strategy for modality alignment in a cascaded way,\nand a mixture of query experts module to adaptively aggregate features with\nhigh efficiency. Moreover, we utilize parameter-efficient fine-tuning methods\nLoRA and Norm fine-tuning, resulting in only 47.8M learnable parameters, which\nis up to 260x fewer than existing methods. Extensive experiments show that\nMiniGPT-3D achieves SOTA on 3D object classification and captioning tasks, with\nsignificantly cheaper training costs. Notably, MiniGPT-3D gains an 8.12\nincrease on GPT-4 evaluation score for the challenging object captioning task\ncompared to ShapeLLM-13B, while the latter costs 160 total GPU-hours on 8 A800.\nWe are the first to explore the efficient 3D-LLM, offering new insights to the\ncommunity. Code and weights are available at\nhttps://github.com/TangYuan96/MiniGPT-3D.\n']",3D Object Detection and Vision-Language Models,Vision-Language Models and Multimodal Learning,Multimodal Learning and Vision-Language Models,Multimodal Learning
442,14,442_imputations_imputation_missingness_data,"['imputations', 'imputation', 'missingness', 'data', 'personalized', 'ehrs', 'ehr', 'health', 'prediction', 'healthcare']","['missing', 'imputation', 'missingness', 'electronic', 'records', 'health', 'values', 'patient', 'patients', 'longitudinal']","['  Electronic health record (EHR) data has emerged as a valuable resource for\nanalyzing patient health status. However, the prevalence of missing data in EHR\nposes significant challenges to existing methods, leading to spurious\ncorrelations and suboptimal predictions. While various imputation techniques\nhave been developed to address this issue, they often obsess unnecessary\ndetails and may introduce additional noise when making clinical predictions. To\ntackle this problem, we propose SMART, a Self-Supervised Missing-Aware\nRepresenTation Learning approach for patient health status prediction, which\nencodes missing information via elaborated attentions and learns to impute\nmissing values through a novel self-supervised pre-training approach that\nreconstructs missing data representations in the latent space. By adopting\nmissing-aware attentions and focusing on learning higher-order representations,\nSMART promotes better generalization and robustness to missing data. We\nvalidate the effectiveness of SMART through extensive experiments on six EHR\ntasks, demonstrating its superiority over state-of-the-art methods.\n', ""  Anemia is a prevalent medical condition that typically requires invasive\nblood tests for diagnosis and monitoring. Electronic health records (EHRs) have\nemerged as valuable data sources for numerous medical studies. EHR-based\nhemoglobin level/anemia degree prediction is non-invasive and rapid but still\nfaces some challenges due to the fact that EHR data is typically an irregular\nmultivariate time series containing a significant number of missing values and\nirregular time intervals. To address these issues, we introduce HgbNet, a\nmachine learning-based prediction model that emulates clinicians'\ndecision-making processes for hemoglobin level/anemia degree prediction. The\nmodel incorporates a NanDense layer with a missing indicator to handle missing\nvalues and employs attention mechanisms to account for both local irregularity\nand global irregularity. We evaluate the proposed method using two real-world\ndatasets across two use cases. In our first use case, we predict hemoglobin\nlevel/anemia degree at moment T+1 by utilizing records from moments prior to\nT+1. In our second use case, we integrate all historical records with\nadditional selected test results at moment T+1 to predict hemoglobin\nlevel/anemia degree at the same moment, T+1. HgbNet outperforms the best\nbaseline results across all datasets and use cases. These findings demonstrate\nthe feasibility of estimating hemoglobin levels and anemia degree from EHR\ndata, positioning HgbNet as an effective non-invasive anemia diagnosis solution\nthat could potentially enhance the quality of life for millions of affected\nindividuals worldwide. To our knowledge, HgbNet is the first machine learning\nmodel leveraging EHR data for hemoglobin level/anemia degree prediction.\n"", ""  Electronic Health Records present a valuable modality for driving\npersonalized medicine, where treatment is tailored to fit individual-level\ndifferences. For this purpose, many data-driven machine learning and\nstatistical models rely on the wealth of longitudinal EHRs to study patients'\nphysiological and treatment effects. However, longitudinal EHRs tend to be\nsparse and highly missing, where missingness could also be informative and\nreflect the underlying patient's health status. Therefore, the success of\ndata-driven models for personalized medicine highly depends on how the EHR data\nis represented from physiological data, treatments, and the missing values in\nthe data. To this end, we propose a novel deep-learning model that learns the\nunderlying patient dynamics over time across multivariate data to generate\npersonalized realistic values conditioning on an individual's demographic\ncharacteristics and treatments. Our proposed model, IGNITE (Individualized\nGeNeration of Imputations in Time-series Electronic health records), utilises a\nconditional dual-variational autoencoder augmented with dual-stage attention to\ngenerate missing values for an individual. In IGNITE, we further propose a\nnovel individualized missingness mask (IMM), which helps our model generate\nvalues based on the individual's observed data and missingness patterns. We\nfurther extend the use of IGNITE from imputing missingness to a personalized\ndata synthesizer, where it generates missing EHRs that were never observed\nprior or even generates new patients for various applications. We validate our\nmodel on three large publicly available datasets and show that IGNITE\noutperforms state-of-the-art approaches in missing data reconstruction and task\nprediction.\n""]",Electronic Health Records Imputation and Prediction,Data Imputation and Missing Value Analysis,Handling Missing or Inconsistent Data,Handling Missing or Inconsistent Data
443,14,443_linearization_koopman_linearizations_observables,"['linearization', 'koopman', 'linearizations', 'observables', 'operators', 'eigenfunctions', 'dynamical', 'nonlinear', 'dynamics', 'spectral']","['operator', 'observables', 'dynamical', 'nonlinear', 'eigenfunctions', 'operators', 'decomposition', 'pseudospectra', 'mode', 'spectral']","['  Machine learning methods allow the prediction of nonlinear dynamical systems\nfrom data alone. The Koopman operator is one of them, which enables us to\nemploy linear analysis for nonlinear dynamical systems. The linear\ncharacteristics of the Koopman operator are hopeful to understand the nonlinear\ndynamics and perform rapid predictions. The extended dynamic mode decomposition\n(EDMD) is one of the methods to approximate the Koopman operator as a\nfinite-dimensional matrix. In this work, we propose a method to compress the\nKoopman matrix using hierarchical clustering. Numerical demonstrations for the\ncart-pole model and comparisons with the conventional singular value\ndecomposition (SVD) are shown; the results indicate that the hierarchical\nclustering performs better than the naive SVD compressions.\n', ""  Data-driven approximations of the Koopman operator are promising for\npredicting the time evolution of systems characterized by complex dynamics.\nAmong these methods, the approach known as extended dynamic mode decomposition\nwith dictionary learning (EDMD-DL) has garnered significant attention. Here we\npresent a modification of EDMD-DL that concurrently determines both the\ndictionary of observables and the corresponding approximation of the Koopman\noperator. This innovation leverages automatic differentiation to facilitate\ngradient descent computations through the pseudoinverse. We also address the\nperformance of several alternative methodologies. We assess a 'pure' Koopman\napproach, which involves the direct time-integration of a linear,\nhigh-dimensional system governing the dynamics within the space of observables.\nAdditionally, we explore a modified approach where the system alternates\nbetween spaces of states and observables at each time step -- this approach no\nlonger satisfies the linearity of the true Koopman operator representation. For\nfurther comparisons, we also apply a state space approach (neural ODEs). We\nconsider systems encompassing two and three-dimensional ordinary differential\nequation systems featuring steady, oscillatory, and chaotic attractors, as well\nas partial differential equations exhibiting increasingly complex and intricate\nbehaviors. Our framework significantly outperforms EDMD-DL. Furthermore, the\nstate space approach offers superior performance compared to the 'pure' Koopman\napproach where the entire time evolution occurs in the space of observables.\nWhen the temporal evolution of the Koopman approach alternates between states\nand observables at each time step, however, its predictions become comparable\nto those of the state space approach.\n"", '  The Koopman operator provides a linear perspective on non-linear dynamics by\nfocusing on the evolution of observables in an invariant subspace. Observables\nof interest are typically linearly reconstructed from the Koopman\neigenfunctions. Despite the broad use of Koopman operators over the past few\nyears, there exist some misconceptions about the applicability of Koopman\noperators to dynamical systems with more than one disjoint invariant sets\n(e.g., basins of attractions from isolated fixed points). In this work, we\nfirst provide a simple explanation for the mechanism of linear\nreconstruction-based Koopman operators of nonlinear systems with multiple\ndisjoint invariant sets. Next, we discuss the use of discrete symmetry among\nsuch invariant sets to construct Koopman eigenfunctions in a data efficient\nmanner. Finally, several numerical examples are provided to illustrate the\nbenefits of exploiting symmetry for learning the Koopman operator.\n']",Koopman Operator for Nonlinear Dynamics Analysis,Nonlinear Dynamics and Differential Equations Analysis,Machine Learning for Dynamical Systems and Differential Equations,Machine Learning for Dynamical Systems and Differential Equations
444,14,444_testing_bugs_apis_dllens,"['testing', 'bugs', 'apis', 'dllens', 'developers', 'deep', 'dl', 'reproducibility', 'projects', 'libraries']","['bugs', 'libraries', 'software', 'testing', 'projects', 'reproducibility', 'library', 'assumptions', 'developers', 'layer']","['  Recent deep learning (DL) applications are mostly built on top of DL\nlibraries. The quality assurance of these libraries is critical to the\ndependable deployment of DL applications. Techniques have been proposed to\ngenerate various DL models and apply them to test these libraries. However,\ntheir test effectiveness is constrained by the diversity of layer API calls in\ntheir generated DL models. Our study reveals that these techniques can cover at\nmost 34.1% layer inputs, 25.9% layer parameter values, and 15.6% layer\nsequences. As a result, we find that many bugs arising from specific layer API\ncalls (i.e., specific layer inputs, parameter values, or layer sequences) can\nbe missed by existing techniques. Because of this limitation, we propose COMET\nto effectively generate DL models with diverse layer API calls for DL library\ntesting. COMET: (1) designs a set of mutation operators and a coverage-based\nsearch algorithm to diversify layer inputs, layer parameter values, and layer\nsequences in DL models. (2) proposes a model synthesis method to boost the test\nefficiency without compromising the layer API call diversity. Our evaluation\nresult shows that COMET outperforms baselines by covering twice as many layer\ninputs (69.7% vs. 34.1%), layer parameter values (50.2% vs. 25.9%), and layer\nsequences (39.0% vs. 15.6%) as those by the state-of-the-art. Moreover, COMET\ncovers 3.4% more library branches than those by existing techniques. Finally,\nCOMET detects 32 new bugs in the latest version of eight popular DL libraries,\nincluding TensorFlow and MXNet, with 21 of them confirmed by DL library\ndevelopers and 7 of those confirmed bugs have been fixed by developers.\n', ""  Testing is a major approach to ensuring the quality of deep learning (DL)\nlibraries. Existing testing techniques commonly adopt differential testing to\nrelieve the need for test oracle construction. However, these techniques are\nlimited in finding implementations that offer the same functionality and\ngenerating diverse test inputs for differential testing. This paper introduces\nDLLens, a novel differential testing technique for DL library testing. Our\ninsight is that APIs in different DL libraries are commonly designed to\naccomplish various computations for the same set of published DL algorithms.\nAlthough the mapping of these APIs is not often one-to-one, we observe that\ntheir computations can be mutually simulated after proper composition and\nadaptation. The use of these simulation counterparts facilitates differential\ntesting for the detection of functional DL library bugs. Leveraging the\ninsight, we propose DLLens as a novel mechanism that utilizes a large language\nmodel (LLM) to synthesize valid counterparts of DL library APIs. To generate\ndiverse test inputs, DLLens incorporates a static analysis method aided by LLM\nto extract path constraints from all execution paths in each API and its\ncounterpart's implementations. These path constraints are then used to guide\nthe generation of diverse test inputs. We evaluate DLLens on two popular DL\nlibraries, TensorFlow and PyTorch. Our evaluation shows that DLLens can\nsynthesize counterparts for more than twice as many APIs found by\nstate-of-the-art techniques on these libraries. Moreover, DLLens can extract\n26.7% more constraints and detect 2.5 times as many bugs as state-of-the-art\ntechniques. DLLens has successfully found 56 bugs in recent TensorFlow and\nPyTorch libraries. Among them, 41 are previously unknown, 39 of which have been\nconfirmed by developers after reporting, and 19 of those confirmed bugs have\nbeen fixed by developers.\n"", ""  In recent years, software systems powered by deep learning (DL) techniques\nhave significantly facilitated people's lives in many aspects. As the backbone\nof these DL systems, various DL libraries undertake the underlying optimization\nand computation. However, like traditional software, DL libraries are not\nimmune to bugs, which can pose serious threats to users' personal property and\nsafety. Studying the characteristics of DL libraries, their associated bugs,\nand the corresponding testing methods is crucial for enhancing the security of\nDL systems and advancing the widespread application of DL technology. This\npaper provides an overview of the testing research related to various DL\nlibraries, discusses the strengths and weaknesses of existing methods, and\nprovides guidance and reference for the application of the DL library. This\npaper first introduces the workflow of DL underlying libraries and the\ncharacteristics of three kinds of DL libraries involved, namely DL framework,\nDL compiler, and DL hardware library. It then provides definitions for DL\nunderlying library bugs and testing. Additionally, this paper summarizes the\nexisting testing methods and tools tailored to these DL libraries separately\nand analyzes their effectiveness and limitations. It also discusses the\nexisting challenges of DL library testing and outlines potential directions for\nfuture research.\n""]",Deep Learning Library Testing and Bug Detection,Deep Learning for Defect Detection and Testing,Deep Learning Applications in Engineering and Computer Vision,Deep Learning Applications in Engineering and Computer Vision
445,14,445_classifiers_classification_classifier_classifying,"['classifiers', 'classification', 'classifier', 'classifying', 'misclassify', 'classes', 'recognition', 'softmax', 'openness', 'wiseopen']","['classes', 'set', 'open', 'unknown', 'recognition', 'closed', 'samples', 'class', 'classification', 'rejection']","['  Classifying patterns of known classes and rejecting ambiguous and novel (also\ncalled as out-of-distribution (OOD)) inputs are involved in open world pattern\nrecognition. Deep neural network models usually excel in closed-set\nclassification while performs poorly in rejecting OOD inputs. To tackle this\nproblem, numerous methods have been designed to perform open set recognition\n(OSR) or OOD rejection/detection tasks. Previous methods mostly take\npost-training score transformation or hybrid models to ensure low scores on OOD\ninputs while separating known classes. In this paper, we attempt to build a\nunified framework for building open set classifiers for both classification and\nOOD rejection. We formulate the open set recognition of $ K $-known-class as a\n$ (K+1) $-class classification problem with model trained on known-class\nsamples only. By decomposing the $ K $-class problem into $ K $ one-versus-all\n(OVA) binary classification tasks and binding some parameters, we show that\ncombining the scores of OVA classifiers can give $ (K+1) $-class posterior\nprobabilities, which enables classification and OOD rejection in a unified\nframework. To maintain the closed-set classification accuracy of the OVA\ntrained classifier, we propose a hybrid training strategy combining OVA loss\nand multi-class cross-entropy loss. We implement the OVA framework and hybrid\ntraining strategy on the recently proposed convolutional prototype network and\nprototype classifier on vision transformer (ViT) backbone. Experiments on\npopular OSR and OOD detection datasets demonstrate that the proposed framework,\nusing a single multi-class classifier, yields competitive performance in\nclosed-set classification, OOD detection, and misclassification detection.\n', ""  Open-set Semi-supervised Learning (OSSL) holds a realistic setting that\nunlabeled data may come from classes unseen in the labeled set, i.e.,\nout-of-distribution (OOD) data, which could cause performance degradation in\nconventional SSL models. To handle this issue, except for the traditional\nin-distribution (ID) classifier, some existing OSSL approaches employ an extra\nOOD detection module to avoid the potential negative impact of the OOD data.\nNevertheless, these approaches typically employ the entire set of open-set data\nduring their training process, which may contain data unfriendly to the OSSL\ntask that can negatively influence the model performance. This inspires us to\ndevelop a robust open-set data selection strategy for OSSL. Through a\ntheoretical understanding from the perspective of learning theory, we propose\nWise Open-set Semi-supervised Learning (WiseOpen), a generic OSSL framework\nthat selectively leverages the open-set data for training the model. By\napplying a gradient-variance-based selection mechanism, WiseOpen exploits a\nfriendly subset instead of the whole open-set dataset to enhance the model's\ncapability of ID classification. Moreover, to reduce the computational expense,\nwe also propose two practical variants of WiseOpen by adopting low-frequency\nupdate and loss-based selection respectively. Extensive experiments demonstrate\nthe effectiveness of WiseOpen in comparison with the state-of-the-art.\n"", ""  In open-set recognition, existing methods generally learn statically fixed\ndecision boundaries using known classes to reject unknown classes. Though they\nhave achieved promising results, such decision boundaries are evidently\ninsufficient for universal unknown classes in dynamic and open scenarios as\nthey can potentially appear at any position in the feature space. Moreover,\nthese methods just simply reject unknown class samples during testing without\nany effective utilization for them. In fact, such samples completely can\nconstitute the true instantiated representation of the unknown classes to\nfurther enhance the model's performance. To address these issues, this paper\nproposes a novel dynamic against dynamic idea, i.e., dynamic method against\ndynamic changing open-set world, where an open-set self-learning (OSSL)\nframework is correspondingly developed. OSSL starts with a good closed-set\nclassifier trained by known classes and utilizes available test samples for\nmodel adaptation during testing, thus gaining the adaptability to changing data\ndistributions. In particular, a novel self-matching module is designed for\nOSSL, which can achieve the adaptation in automatically identifying known class\nsamples while rejecting unknown class samples which are further utilized to\nenhance the discriminability of the model as the instantiated representation of\nunknown classes. Our method establishes new performance milestones respectively\nin almost all standard and cross-data benchmarks.\n""]",Open-Set Recognition and Classification,Computer Vision and Pattern Recognition,Computer Vision,Computer Vision
446,14,446_lasso_sparse_regularization_optimization,"['lasso', 'sparse', 'regularization', 'optimization', 'convexity', 'convex', 'regularized', 'sparsity', 'regularizer', 'solvers']","['skscope', 'convexity', 'sparse', 'convex', 'manifolds', 'recovery', 'norm', 'penalty', 'regularization', 'matrix']","['  Sparse linear regression (SLR) is a well-studied problem in statistics where\none is given a design matrix $X\\in\\mathbb{R}^{m\\times n}$ and a response vector\n$y=X\\theta^*+w$ for a $k$-sparse vector $\\theta^*$ (that is,\n$\\|\\theta^*\\|_0\\leq k$) and small, arbitrary noise $w$, and the goal is to find\na $k$-sparse $\\widehat{\\theta} \\in \\mathbb{R}^n$ that minimizes the mean\nsquared prediction error $\\frac{1}{m}\\|X\\widehat{\\theta}-X\\theta^*\\|^2_2$.\nWhile $\\ell_1$-relaxation methods such as basis pursuit, Lasso, and the Dantzig\nselector solve SLR when the design matrix is well-conditioned, no general\nalgorithm is known, nor is there any formal evidence of hardness in an\naverage-case setting with respect to all efficient algorithms.\n  We give evidence of average-case hardness of SLR w.r.t. all efficient\nalgorithms assuming the worst-case hardness of lattice problems. Specifically,\nwe give an instance-by-instance reduction from a variant of the bounded\ndistance decoding (BDD) problem on lattices to SLR, where the condition number\nof the lattice basis that defines the BDD instance is directly related to the\nrestricted eigenvalue condition of the design matrix, which characterizes some\nof the classical statistical-computational gaps for sparse linear regression.\nAlso, by appealing to worst-case to average-case reductions from the world of\nlattices, this shows hardness for a distribution of SLR instances; while the\ndesign matrices are ill-conditioned, the resulting SLR instances are in the\nidentifiable regime.\n  Furthermore, for well-conditioned (essentially) isotropic Gaussian design\nmatrices, where Lasso is known to behave well in the identifiable regime, we\nshow hardness of outputting any good solution in the unidentifiable regime\nwhere there are many solutions, assuming the worst-case hardness of standard\nand well-studied lattice problems.\n', '  Convex programming plays a fundamental role in machine learning, data\nscience, and engineering. Testing convexity structure in nonlinear programs\nrelies on verifying the convexity of objectives and constraints.\n\\citet{grant2006disciplined} introduced a framework, Disciplined Convex\nProgramming (DCP), for automating this verification task for a wide range of\nconvex functions that can be decomposed into basic convex functions (atoms)\nusing convexity-preserving compositions and transformations (rules). However,\nthe restriction to Euclidean convexity concepts can limit the applicability of\nthe framework. For instance, many notable instances of statistical estimators\nand matrix-valued (sub)routines in machine learning applications are Euclidean\nnon-convex, but exhibit geodesic convexity through a more general Riemannian\nlens. In this work, we extend disciplined programming to this setting by\nintroducing Disciplined Geodesically Convex Programming (DGCP). We determine\nconvexity-preserving compositions and transformations for geodesically convex\nfunctions on general Cartan-Hadamard manifolds, as well as for the special case\nof symmetric positive definite matrices, a common setting in matrix-valued\noptimization. For the latter, we also define a basic set of atoms. Our paper is\naccompanied by a Julia package SymbolicAnalysis.jl, which provides\nfunctionality for testing and certifying DGCP-compliant expressions. Our\nlibrary interfaces with manifold optimization software, which allows for\ndirectly solving verified geodesically convex programs.\n', '  The generalized minimax concave (GMC) penalty is a nonconvex sparse\nregularizer which can preserve the overall-convexity of the regularized\nleast-squares problem. In this paper, we focus on a significant instance of the\nGMC model termed scaled GMC (sGMC), and present various notable findings on its\nsolution-set geometry and regularization path. Our investigation indicates that\nwhile the sGMC penalty is a nonconvex extension of the LASSO penalty (i.e., the\n$\\ell_1$-norm), the sGMC model preserves many celebrated properties of the\nLASSO model, hence can serve as a less biased surrogate of LASSO without losing\nits advantages. Specifically, for a fixed regularization parameter $\\lambda$,\nwe show that the solution-set geometry, solution uniqueness and sparseness of\nthe sGMC model can be characterized in a similar elegant way to the LASSO model\n(see, e.g., Osborne et al. 2000, R. J. Tibshirani 2013). For a varying\n$\\lambda$, we prove that the sGMC solution set is a continuous polytope-valued\nmapping of $\\lambda$. Most noticeably, our study indicates that similar to\nLASSO, the minimum $\\ell_2$-norm regularization path of the sGMC model is\ncontinuous and piecewise linear in $\\lambda$. Based on these theoretical\nresults, an efficient regularization path algorithm is proposed for the sGMC\nmodel, extending the well-known least angle regression (LARS) algorithm for\nLASSO. We prove the correctness and finite termination of the proposed\nalgorithm under a mild assumption, and confirm its\ncorrectness-in-general-situation, efficiency, and practical utility through\nnumerical experiments. Many results in this study also contribute to the\ntheoretical research of LASSO.\n']",Sparse Linear Regression and Convex Optimization,Optimization Methods and Algorithms,Optimization and Design,Optimization and Design
447,14,447_phylogenies_protolanguage_phylogenetics_phylogenetic,"['phylogenies', 'protolanguage', 'phylogenetics', 'phylogenetic', 'linguistics', 'ancestral', 'linguists', 'ancestor', 'phonetic', 'linguistic']","['cognate', 'protoforms', 'sound', 'reflexes', 'languages', 'phylogenetic', 'synonyms', 'words', 'proto', 'reconstructed']","['  Protolanguage reconstruction is central to historical linguistics. The\ncomparative method, one of the most influential theoretical and methodological\nframeworks in the history of the language sciences, allows linguists to infer\nprotoforms (reconstructed ancestral words) from their reflexes (related modern\nwords) based on the assumption of regular sound change. Not surprisingly,\nnumerous computational linguists have attempted to operationalize comparative\nreconstruction through various computational models, the most successful of\nwhich have been supervised encoder-decoder models, which treat the problem of\npredicting protoforms given sets of reflexes as a sequence-to-sequence problem.\nWe argue that this framework ignores one of the most important aspects of the\ncomparative method: not only should protoforms be inferable from cognate sets\n(sets of related reflexes) but the reflexes should also be inferable from the\nprotoforms. Leveraging another line of research -- reflex prediction -- we\npropose a system in which candidate protoforms from a reconstruction model are\nreranked by a reflex prediction model. We show that this more complete\nimplementation of the comparative method allows us to surpass state-of-the-art\nprotoform reconstruction methods on three of four Chinese and Romance datasets.\n', '  In traditional studies on language evolution, scholars often emphasize the\nimportance of sound laws and sound correspondences for phylogenetic inference\nof language family trees. However, to date, computational approaches have\ntypically not taken this potential into account. Most computational studies\nstill rely on lexical cognates as major data source for phylogenetic\nreconstruction in linguistics, although there do exist a few studies in which\nauthors praise the benefits of comparing words at the level of sound sequences.\nBuilding on (a) ten diverse datasets from different language families, and (b)\nstate-of-the-art methods for automated cognate and sound correspondence\ndetection, we test, for the first time, the performance of sound-based versus\ncognate-based approaches to phylogenetic reconstruction. Our results show that\nphylogenies reconstructed from lexical cognates are topologically closer, by\napproximately one third with respect to the generalized quartet distance on\naverage, to the gold standard phylogenies than phylogenies reconstructed from\nsound correspondences.\n', '  Identification of cognates across related languages is one of the primary\nproblems in historical linguistics. Automated cognate identification is helpful\nfor several downstream tasks including identifying sound correspondences,\nproto-language reconstruction, phylogenetic classification, etc. Previous\nstate-of-the-art methods for cognate identification are mostly based on\ndistributions of phonemes computed across multilingual wordlists and make\nlittle use of the cognacy labels that define links among cognate clusters. In\nthis paper, we present a transformer-based architecture inspired by\ncomputational biology for the task of automated cognate detection. Beyond a\ncertain amount of supervision, this method performs better than the existing\nmethods, and shows steady improvement with further increase in supervision,\nthereby proving the efficacy of utilizing the labeled information. We also\ndemonstrate that accepting multiple sequence alignments as input and having an\nend-to-end architecture with link prediction head saves much computation time\nwhile simultaneously yielding superior performance.\n']",Phylogenetic Linguistics and Protolanguage Reconstruction,Computational Methods in Historical Linguistics and Phylogenetics,Computational Methods in Evolutionary Analysis,Computational Methods in Evolutionary Analysis
448,14,448_3d_referit3d_lv3d_scenes,"['3d', 'referit3d', 'lv3d', 'scenes', '3ddc', 'visual', 'cube', '3dvg', 'llmi3d', 'annotations']","['scene', 'grounding', 'object', 'objects', 'visual', 'scenes', 'queries', 'reasoning', 'modal', 'perception']","[""  Multi-modal large language models (MLLMs) have shown incredible capabilities\nin a variety of 2D vision and language tasks. We extend MLLMs' perceptual\ncapabilities to ground and reason about images in 3-dimensional space. To that\nend, we first develop a large-scale pre-training dataset for 2D and 3D called\nLV3D by combining multiple existing 2D and 3D recognition datasets under a\ncommon task formulation: as multi-turn question-answering. Next, we introduce a\nnew MLLM named Cube-LLM and pre-train it on LV3D. We show that pure data\nscaling makes a strong 3D perception capability without 3D specific\narchitectural design or training objective. Cube-LLM exhibits intriguing\nproperties similar to LLMs: (1) Cube-LLM can apply chain-of-thought prompting\nto improve 3D understanding from 2D context information. (2) Cube-LLM can\nfollow complex and diverse instructions and adapt to versatile input and output\nformats. (3) Cube-LLM can be visually prompted such as 2D box or a set of\ncandidate 3D boxes from specialists. Our experiments on outdoor benchmarks\ndemonstrate that Cube-LLM significantly outperforms existing baselines by 21.3\npoints of AP-BEV on the Talk2Car dataset for 3D grounded reasoning and 17.7\npoints on the DriveLM dataset for complex reasoning about driving scenarios,\nrespectively. Cube-LLM also shows competitive results in general MLLM\nbenchmarks such as refCOCO for 2D grounding with (87.0) average score, as well\nas visual question answering benchmarks such as VQAv2, GQA, SQA, POPE, etc. for\ncomplex reasoning. Our project is available at\nhttps://janghyuncho.github.io/Cube-LLM.\n"", '  In 3D Visual Question Answering (3D VQA), the scarcity of fully annotated\ndata and limited visual content diversity hampers the generalization to novel\nscenes and 3D concepts (e.g., only around 800 scenes are utilized in ScanQA and\nSQA dataset). Current approaches resort supplement 3D reasoning with 2D\ninformation. However, these methods face challenges: either they use top-down\n2D views that introduce overly complex and sometimes question-irrelevant visual\nclues, or they rely on globally aggregated scene/image-level representations\nfrom 2D VLMs, losing the fine-grained vision-language correlations. To overcome\nthese limitations, our approach utilizes question-conditional 2D view selection\nprocedure, pinpointing semantically relevant 2D inputs for crucial visual\nclues. We then integrate this 2D knowledge into the 3D-VQA system via a\ntwo-branch Transformer structure. This structure, featuring a Twin-Transformer\ndesign, compactly combines 2D and 3D modalities and captures fine-grained\ncorrelations between modalities, allowing them mutually augmenting each other.\nIntegrating proposed mechanisms above, we present BridgeQA, that offers a fresh\nperspective on multi-modal transformer-based architectures for 3D-VQA.\nExperiments validate that BridgeQA achieves state-of-the-art on 3D-VQA datasets\nand significantly outperforms existing solutions. Code is available at\n$\\href{https://github.com/matthewdm0816/BridgeQA}{\\text{this URL}}$.\n', '  Learning to ground natural language queries to target objects or regions in\n3D point clouds is quite essential for 3D scene understanding. Nevertheless,\nexisting 3D visual grounding approaches require a substantial number of\nbounding box annotations for text queries, which is time-consuming and\nlabor-intensive to obtain. In this paper, we propose \\textbf{3D-VLA}, a weakly\nsupervised approach for \\textbf{3D} visual grounding based on \\textbf{V}isual\n\\textbf{L}inguistic \\textbf{A}lignment. Our 3D-VLA exploits the superior\nability of current large-scale vision-language models (VLMs) on aligning the\nsemantics between texts and 2D images, as well as the naturally existing\ncorrespondences between 2D images and 3D point clouds, and thus implicitly\nconstructs correspondences between texts and 3D point clouds with no need for\nfine-grained box annotations in the training procedure. During the inference\nstage, the learned text-3D correspondence will help us ground the text queries\nto the 3D target objects even without 2D images. To the best of our knowledge,\nthis is the first work to investigate 3D visual grounding in a weakly\nsupervised manner by involving large scale vision-language models, and\nextensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our\n3D-VLA achieves comparable and even superior results over the fully supervised\nmethods.\n']",3D Visual Language Models and Grounding,Vision-Language Models and Multimodal Learning,Multimodal Learning and Vision-Language Models,Multimodal Learning
449,14,449_automl_automate_automated_automating,"['automl', 'automate', 'automated', 'automating', 'automation', 'autorecsys', 'autommlab', 'automatically', 'autorul', 'expertise']","['pipelines', 'pipeline', 'workflow', 'hyper', 'machine', 'expertise', 'engineering', 'tools', 'end', 'workflows']","['  Background. Due to the widespread adoption of Artificial Intelligence (AI)\nand Machine Learning (ML) for building software applications, companies are\nstruggling to recruit employees with a deep understanding of such technologies.\nIn this scenario, AutoML is soaring as a promising solution to fill the AI/ML\nskills gap since it promises to automate the building of end-to-end AI/ML\npipelines that would normally be engineered by specialized team members. Aims.\nDespite the growing interest and high expectations, there is a dearth of\ninformation about the extent to which AutoML is currently adopted by teams\ndeveloping AI/ML-enabled systems and how it is perceived by practitioners and\nresearchers. Method. To fill these gaps, in this paper, we present a\nmixed-method study comprising a benchmark of 12 end-to-end AutoML tools on two\nSE datasets and a user survey with follow-up interviews to further our\nunderstanding of AutoML adoption and perception. Results. We found that AutoML\nsolutions can generate models that outperform those trained and optimized by\nresearchers to perform classification tasks in the SE domain. Also, our\nfindings show that the currently available AutoML solutions do not live up to\ntheir names as they do not equally support automation across the stages of the\nML development workflow and for all the team members. Conclusions. We derive\ninsights to inform the SE research community on how AutoML can facilitate their\nactivities and tool builders on how to design the next generation of AutoML\ntechnologies.\n', ""  Automated machine learning (AutoML) was formed around the fundamental\nobjectives of automatically and efficiently configuring machine learning (ML)\nworkflows, aiding the research of new ML algorithms, and contributing to the\ndemocratization of ML by making it accessible to a broader audience. Over the\npast decade, commendable achievements in AutoML have primarily focused on\noptimizing predictive performance. This focused progress, while substantial,\nraises questions about how well AutoML has met its broader, original goals. In\nthis position paper, we argue that a key to unlocking AutoML's full potential\nlies in addressing the currently underexplored aspect of user interaction with\nAutoML systems, including their diverse roles, expectations, and expertise. We\nenvision a more human-centered approach in future AutoML research, promoting\nthe collaborative design of ML systems that tightly integrates the\ncomplementary strengths of human expertise and AutoML methodologies.\n"", '  Automated machine learning (AutoML) is envisioned to make ML techniques\naccessible to ordinary users. Recent work has investigated the role of humans\nin enhancing AutoML functionality throughout a standard ML workflow. However,\nit is also critical to understand how users adopt existing AutoML solutions in\ncomplex, real-world settings from a holistic perspective. To fill this gap,\nthis study conducted semi-structured interviews of AutoML users (N=19) focusing\non understanding (1) the limitations of AutoML encountered by users in their\nreal-world practices, (2) the strategies users adopt to cope with such\nlimitations, and (3) how the limitations and workarounds impact their use of\nAutoML. Our findings reveal that users actively exercise user agency to\novercome three major challenges arising from customizability, transparency, and\nprivacy. Furthermore, users make cautious decisions about whether and how to\napply AutoML on a case-by-case basis. Finally, we derive design implications\nfor developing future AutoML solutions.\n']",Automated Machine Learning (AutoML) Adoption and Perception,Technology Adoption and Acceptance in Various Domains,Human Interaction with Emerging Technologies,Human Interaction with Emerging Technologies
450,14,450_distributional_quantile_reinforcement_learns,"['distributional', 'quantile', 'reinforcement', 'learns', 'distributions', 'distribution', 'learning', 'dqn', 'reward', 'expectile']","['distributional', 'quantile', 'return', 'returns', 'reinforcement', 'distribution', 'difference', 'policy', 'distributions', 'value']","['  We propose a new algorithm for model-based distributional reinforcement\nlearning (RL), and prove that it is minimax-optimal for approximating return\ndistributions with a generative model (up to logarithmic factors), resolving an\nopen question of Zhang et al. (2023). Our analysis provides new theoretical\nresults on categorical approaches to distributional RL, and also introduces a\nnew distributional Bellman equation, the stochastic categorical CDF Bellman\nequation, which we expect to be of independent interest. We also provide an\nexperimental study comparing several model-based distributional RL algorithms,\nwith several takeaways for practitioners.\n', '  Distributional Reinforcement Learning (RL) estimates return distribution\nmainly by learning quantile values via minimizing the quantile Huber loss\nfunction, entailing a threshold parameter often selected heuristically or via\nhyperparameter search, which may not generalize well and can be suboptimal.\nThis paper introduces a generalized quantile Huber loss function derived from\nWasserstein distance (WD) calculation between Gaussian distributions, capturing\nnoise in predicted (current) and target (Bellman-updated) quantile values.\nCompared to the classical quantile Huber loss, this innovative loss function\nenhances robustness against outliers. Notably, the classical Huber loss\nfunction can be seen as an approximation of our proposed loss, enabling\nparameter adjustment by approximating the amount of noise in the data during\nthe learning process. Empirical tests on Atari games, a common application in\ndistributional RL, and a recent hedging strategy using distributional RL,\nvalidate the effectiveness of our proposed loss function and its potential for\nparameter adjustments in distributional RL. The implementation of the proposed\nloss function is available here.\n', '  Distributional reinforcement learning (RL) has proven useful in multiple\nbenchmarks as it enables approximating the full distribution of returns and\nmakes a better use of environment samples. The commonly used quantile\nregression approach to distributional RL -- based on asymmetric $L_1$ losses --\nprovides a flexible and effective way of learning arbitrary return\ndistributions. In practice, it is often improved by using a more efficient,\nhybrid asymmetric $L_1$-$L_2$ Huber loss for quantile regression. However, by\ndoing so, distributional estimation guarantees vanish, and we empirically\nobserve that the estimated distribution rapidly collapses to its mean. Indeed,\nasymmetric $L_2$ losses, corresponding to expectile regression, cannot be\nreadily used for distributional temporal difference learning. Motivated by the\nefficiency of $L_2$-based learning, we propose to jointly learn expectiles and\nquantiles of the return distribution in a way that allows efficient learning\nwhile keeping an estimate of the full distribution of returns. We prove that\nour approach approximately learns the correct return distribution, and we\nbenchmark a practical implementation on a toy example and at scale. On the\nAtari benchmark, our approach matches the performance of the Huber-based IQN-1\nbaseline after $200$M training frames but avoids distributional collapse and\nkeeps estimates of the full distribution of returns.\n']",Distributional Reinforcement Learning,Reinforcement Learning Methods and Applications,Reinforcement Learning,Reinforcement Learning
450,14,450_distributional_quantile_reinforcement_learns,"['distributional', 'quantile', 'reinforcement', 'learns', 'distributions', 'distribution', 'learning', 'dqn', 'reward', 'expectile']","['distributional', 'quantile', 'return', 'returns', 'reinforcement', 'distribution', 'difference', 'policy', 'distributions', 'value']","['  We propose a new algorithm for model-based distributional reinforcement\nlearning (RL), and prove that it is minimax-optimal for approximating return\ndistributions with a generative model (up to logarithmic factors), resolving an\nopen question of Zhang et al. (2023). Our analysis provides new theoretical\nresults on categorical approaches to distributional RL, and also introduces a\nnew distributional Bellman equation, the stochastic categorical CDF Bellman\nequation, which we expect to be of independent interest. We also provide an\nexperimental study comparing several model-based distributional RL algorithms,\nwith several takeaways for practitioners.\n', '  Distributional Reinforcement Learning (RL) estimates return distribution\nmainly by learning quantile values via minimizing the quantile Huber loss\nfunction, entailing a threshold parameter often selected heuristically or via\nhyperparameter search, which may not generalize well and can be suboptimal.\nThis paper introduces a generalized quantile Huber loss function derived from\nWasserstein distance (WD) calculation between Gaussian distributions, capturing\nnoise in predicted (current) and target (Bellman-updated) quantile values.\nCompared to the classical quantile Huber loss, this innovative loss function\nenhances robustness against outliers. Notably, the classical Huber loss\nfunction can be seen as an approximation of our proposed loss, enabling\nparameter adjustment by approximating the amount of noise in the data during\nthe learning process. Empirical tests on Atari games, a common application in\ndistributional RL, and a recent hedging strategy using distributional RL,\nvalidate the effectiveness of our proposed loss function and its potential for\nparameter adjustments in distributional RL. The implementation of the proposed\nloss function is available here.\n', '  Distributional reinforcement learning (RL) has proven useful in multiple\nbenchmarks as it enables approximating the full distribution of returns and\nmakes a better use of environment samples. The commonly used quantile\nregression approach to distributional RL -- based on asymmetric $L_1$ losses --\nprovides a flexible and effective way of learning arbitrary return\ndistributions. In practice, it is often improved by using a more efficient,\nhybrid asymmetric $L_1$-$L_2$ Huber loss for quantile regression. However, by\ndoing so, distributional estimation guarantees vanish, and we empirically\nobserve that the estimated distribution rapidly collapses to its mean. Indeed,\nasymmetric $L_2$ losses, corresponding to expectile regression, cannot be\nreadily used for distributional temporal difference learning. Motivated by the\nefficiency of $L_2$-based learning, we propose to jointly learn expectiles and\nquantiles of the return distribution in a way that allows efficient learning\nwhile keeping an estimate of the full distribution of returns. We prove that\nour approach approximately learns the correct return distribution, and we\nbenchmark a practical implementation on a toy example and at scale. On the\nAtari benchmark, our approach matches the performance of the Huber-based IQN-1\nbaseline after $200$M training frames but avoids distributional collapse and\nkeeps estimates of the full distribution of returns.\n']",Distributional Reinforcement Learning,Reinforcement Learning Methods and Applications,Reinforcement Learning,Reinforcement Learning
451,14,451_paraphrases_paraphrasing_paraphrastic_paraphrase,"['paraphrases', 'paraphrasing', 'paraphrastic', 'paraphrase', 'paraphrased', 'sentences', 'linguistic', 'nlg', 'semantic', 'corpora']","['paraphrase', 'paraphrases', 'paraphrasing', 'paraphrastic', 'diversity', 'syntactic', 'lexical', 'linguistic', 'generation', 'text']","['  Since paraphrasing is an ill-defined task, the term ""paraphrasing"" covers\ntext transformation tasks with different characteristics. Consequently,\nexisting paraphrasing studies have applied quite different (explicit and\nimplicit) criteria as to when a pair of texts is to be considered a paraphrase,\nall of which amount to postulating a certain level of semantic or lexical\nsimilarity. In this paper, we conduct a literature review and propose a\ntaxonomy to organize the 25~identified paraphrasing (sub-)tasks. Using\nclassifiers trained to identify the tasks that a given paraphrasing instance\nfits, we find that the distributions of task-specific instances in the known\nparaphrase corpora vary substantially. This means that the use of these\ncorpora, without the respective paraphrase conditions being clearly defined\n(which is the normal case), must lead to incomparable and misleading results.\n', ""  Paraphrases represent a human's intuitive ability to understand expressions\npresented in various different ways. Current paraphrase evaluations of language\nmodels primarily use binary approaches, offering limited interpretability of\nspecific text changes. Atomic paraphrase types (APT) decompose paraphrases into\ndifferent linguistic changes and offer a granular view of the flexibility in\nlinguistic expression (e.g., a shift in syntax or vocabulary used). In this\nstudy, we assess the human preferences towards ChatGPT in generating English\nparaphrases with ten APTs and five prompting techniques. We introduce APTY\n(Atomic Paraphrase TYpes), a dataset of 500 sentence-level and word-level\nannotations by 15 annotators. The dataset also provides a human preference\nranking of paraphrases with different types that can be used to fine-tune\nmodels with RLHF and DPO methods. Our results reveal that ChatGPT can generate\nsimple APTs, such as additions and deletions, but struggle with complex\nstructures (e.g., subordination changes). This study contributes to\nunderstanding which aspects of paraphrasing language models have already\nsucceeded at understanding and what remains elusive. In addition, our curated\ndatasets can be used to develop language models with specific linguistic\ncapabilities.\n"", '  Current approaches in paraphrase generation and detection heavily rely on a\nsingle general similarity score, ignoring the intricate linguistic properties\nof language. This paper introduces two new tasks to address this shortcoming by\nconsidering paraphrase types - specific linguistic perturbations at particular\ntext positions. We name these tasks Paraphrase Type Generation and Paraphrase\nType Detection. Our results suggest that while current techniques perform well\nin a binary classification scenario, i.e., paraphrased or not, the inclusion of\nfine-grained paraphrase types poses a significant challenge. While most\napproaches are good at generating and detecting general semantic similar\ncontent, they fail to understand the intrinsic linguistic variables they\nmanipulate. Models trained in generating and identifying paraphrase types also\nshow improvements in tasks without them. In addition, scaling these models\nfurther improves their ability to understand paraphrase types. We believe\nparaphrase types can unlock a new paradigm for developing paraphrase models and\nsolving tasks in the future.\n']",Paraphrasing and Paraphrase Generation,Natural Language Processing for Text Generation and Evaluation,Natural Language Processing,Natural Language Processing
452,14,452_surfaces_surface_curvature_curvatures,"['surfaces', 'surface', 'curvature', 'curvatures', 'shapes', 'mesh', '3d', 'neural', 'shape', 'meshes']","['surface', 'surfaces', 'shape', 'distance', 'geometry', 'implicit', 'clouds', 'geometric', 'occupancy', 'point']","['  Point clouds are popular 3D representations for real-life objects (such as in\nLiDAR and Kinect) due to their detailed and compact representation of\nsurface-based geometry. Recent approaches characterise the geometry of point\nclouds by bringing deep learning based techniques together with geometric\nfidelity metrics such as optimal transportation costs (e.g., Chamfer and\nWasserstein metrics). In this paper, we propose a new surface geometry\ncharacterisation within this realm, namely a neural varifold representation of\npoint clouds. Here the surface is represented as a measure/distribution over\nboth point positions and tangent spaces of point clouds. The varifold\nrepresentation quantifies not only the surface geometry of point clouds through\nthe manifold-based discrimination, but also subtle geometric consistencies on\nthe surface due to the combined product space. This study proposes neural\nvarifold algorithms to compute the varifold norm between two point clouds using\nneural networks on point clouds and their neural tangent kernel\nrepresentations. The proposed neural varifold is evaluated on three different\nsought-after tasks -- shape matching, few-shot shape classification and shape\nreconstruction. Detailed evaluation and comparison to the state-of-the-art\nmethods demonstrate that the proposed versatile neural varifold is superior in\nshape matching and few-shot shape classification, and is competitive for shape\nreconstruction.\n', '  Neural surfaces (e.g., neural map encoding, deep implicits and neural\nradiance fields) have recently gained popularity because of their generic\nstructure (e.g., multi-layer perceptron) and easy integration with modern\nlearning-based setups. Traditionally, we have a rich toolbox of geometry\nprocessing algorithms designed for polygonal meshes to analyze and operate on\nsurface geometry. However, neural representations are typically discretized and\nconverted into a mesh, before applying any geometry processing algorithm. This\nis unsatisfactory and, as we demonstrate, unnecessary. In this work, we propose\na spherical neural surface representation (a spherical parametrization) for\ngenus-0 surfaces and demonstrate how to compute core geometric operators\ndirectly on this representation. Namely, we show how to construct the normals\nand the first and second fundamental forms of the surface, and how to compute\nthe surface gradient, surface divergence and Laplace Beltrami operator on\nscalar/vector fields defined on the surface. These operators, in turn, enable\nus to create geometry processing tools that act directly on the neural\nrepresentations without any unnecessary meshing. We demonstrate illustrative\napplications in (neural) spectral analysis, heat flow and mean curvature flow,\nand our method shows robustness to isometric shape variations. We both propose\ntheoretical formulations and validate their numerical estimates. By\nsystematically linking neural surface representations with classical geometry\nprocessing algorithms, we believe this work can become a key ingredient in\nenabling neural geometry processing.\n', '  In the field of computer vision, the numerical encoding of 3D surfaces is\ncrucial. It is classical to represent surfaces with their Signed Distance\nFunctions (SDFs) or Unsigned Distance Functions (UDFs). For tasks like\nrepresentation learning, surface classification, or surface reconstruction,\nthis function can be learned by a neural network, called Neural Distance\nFunction. This network, and in particular its weights, may serve as a\nparametric and implicit representation for the surface. The network must\nrepresent the surface as accurately as possible. In this paper, we propose a\nmethod for learning UDFs that improves the fidelity of the obtained Neural UDF\nto the original 3D surface. The key idea of our method is to concentrate the\nlearning effort of the Neural UDF on surface edges. More precisely, we show\nthat sampling more training points around surface edges allows better local\naccuracy of the trained Neural UDF, and thus improves the global expressiveness\nof the Neural UDF in terms of Hausdorff distance. To detect surface edges, we\npropose a new statistical method based on the calculation of a $p$-value at\neach point on the surface. Our method is shown to detect surface edges more\naccurately than a commonly used local geometric descriptor.\n']",Neural Representations of 3D Surfaces,3D Geometry and Shape Representation Learning,Geometric and Equivariant Deep Learning,Geometric and Equivariant Deep Learning
453,14,453_captioning_embedding_captions_caption,"['captioning', 'embedding', 'captions', 'caption', 'clip', 'visual', 'recognition', 'modality', 'generatively', 'retrieval']","['contrastive', 'image', 'captions', 'shot', 'shortcut', 'vision', 'caption', 'captioning', 'text', 'matching']","[""  Image captioning aims at generating descriptive and meaningful textual\ndescriptions of images, enabling a broad range of vision-language applications.\nPrior works have demonstrated that harnessing the power of Contrastive Image\nLanguage Pre-training (CLIP) offers a promising approach to achieving zero-shot\ncaptioning, eliminating the need for expensive caption annotations. However,\nthe widely observed modality gap in the latent space of CLIP harms the\nperformance of zero-shot captioning by breaking the alignment between paired\nimage-text features. To address this issue, we conduct an analysis on the CLIP\nlatent space which leads to two findings. Firstly, we observe that the CLIP's\nvisual feature of image subregions can achieve closer proximity to the paired\ncaption due to the inherent information loss in text descriptions. In addition,\nwe show that the modality gap between a paired image-text can be empirically\nmodeled as a zero-mean Gaussian distribution. Motivated by the findings, we\npropose a novel zero-shot image captioning framework with text-only training to\nreduce the modality gap. In particular, we introduce a subregion feature\naggregation to leverage local region information, which produces a compact\nvisual representation for matching text representation. Moreover, we\nincorporate a noise injection and CLIP reranking strategy to boost captioning\nperformance. We also extend our framework to build a zero-shot VQA pipeline,\ndemonstrating its generality. Through extensive experiments on common\ncaptioning and VQA datasets such as MSCOCO, Flickr30k and VQAV2, we show that\nour method achieves remarkable performance improvements. Code is available at\nhttps://github.com/Artanic30/MacCap.\n"", '  In the field of vision-language contrastive learning, models such as CLIP\ncapitalize on matched image-caption pairs as positive examples and leverage\nwithin-batch non-matching pairs as negatives. This approach has led to\nremarkable outcomes in zero-shot image classification, cross-modal retrieval,\nand linear evaluation tasks. We show that the zero-shot classification and\nretrieval capabilities of CLIP-like models can be improved significantly\nthrough the introduction of semantically composite examples during pretraining.\nInspired by CutMix in vision categorization, we create semantically composite\nimage-caption pairs by merging elements from two distinct instances in the\ndataset via a novel procedure. Our method fuses the captions and blends 50% of\neach image to form a new composite sample. This simple technique (termed CLIP-C\nfor CLIP Compositions), devoid of any additional computational overhead or\nincrease in model parameters, significantly improves zero-shot image\nclassification and cross-modal retrieval. The benefits of CLIP-C are\nparticularly pronounced in settings with relatively limited pretraining data.\n', '  Multi-modal learning has become increasingly popular due to its ability to\nleverage information from different data sources (e.g., text and images) to\nimprove the model performance. Recently, CLIP has emerged as an effective\napproach that employs vision-language contrastive pretraining to learn joint\nimage and text representations and exhibits remarkable performance in zero-shot\nlearning and text-guided natural image generation. Despite the huge practical\nsuccess of CLIP, its theoretical understanding remains elusive. In this paper,\nwe formally study transferrable representation learning underlying CLIP and\ndemonstrate how features from different modalities get aligned. We also analyze\nits zero-shot transfer performance on the downstream tasks. Inspired by our\nanalysis, we propose a new CLIP-type approach, which achieves better\nperformance than CLIP and other state-of-the-art methods on benchmark datasets.\n']",Contrastive Image-Language Pre-training (CLIP) for Captioning,Multimodal Vision-Language Understanding and Generation,Multimodal Learning and Vision-Language Models,Multimodal Learning
454,13,454_rgb_camera_illumination_photography,"['rgb', 'camera', 'illumination', 'photography', 'dslr', 'cameras', 'srgb', 'brightness', 'dark', 'reflectance']","['dark', 'light', 'color', 'enhancement', 'images', 'camera', 'brightness', 'raw', 'cameras', 'compressed']","['  Modern smartphone camera quality heavily relies on the image signal processor\n(ISP) to enhance captured raw images, utilizing carefully designed modules to\nproduce final output images encoded in a standard color space (e.g., sRGB).\nNeural-based end-to-end learnable ISPs offer promising advancements,\npotentially replacing traditional ISPs with their ability to adapt without\nrequiring extensive tuning for each new camera model, as is often the case for\nnearly every module in traditional ISPs. However, the key challenge with the\nrecent learning-based ISPs is the urge to collect large paired datasets for\neach distinct camera model due to the influence of intrinsic camera\ncharacteristics on the formation of input raw images. This paper tackles this\nchallenge by introducing a novel method for unpaired learning of raw-to-raw\ntranslation across diverse cameras. Specifically, we propose Rawformer, an\nunsupervised Transformer-based encoder-decoder method for raw-to-raw\ntranslation. It accurately maps raw images captured by a certain camera to the\ntarget camera, facilitating the generalization of learnable ISPs to new unseen\ncameras. Our method demonstrates superior performance on real camera datasets,\nachieving higher accuracy compared to previous state-of-the-art techniques, and\npreserving a more robust correlation between the original and translated raw\nimages. The codes and the pretrained models are available at\nhttps://github.com/gosha20777/rawformer.\n', '  Dark image enhancement aims at converting dark images to normal-light images.\nExisting dark image enhancement methods take uncompressed dark images as inputs\nand achieve great performance. However, in practice, dark images are often\ncompressed before storage or transmission over the Internet. Current methods\nget poor performance when processing compressed dark images. Artifacts hidden\nin the dark regions are amplified by current methods, which results in\nuncomfortable visual effects for observers. Based on this observation, this\nstudy aims at enhancing compressed dark images while avoiding compression\nartifacts amplification. Since texture details intertwine with compression\nartifacts in compressed dark images, detail enhancement and blocking artifacts\nsuppression contradict each other in image space. Therefore, we handle the task\nin latent space. To this end, we propose a novel latent mapping network based\non variational auto-encoder (VAE). Firstly, different from previous VAE-based\nmethods with single-resolution features only, we exploit multiple latent spaces\nwith multi-resolution features, to reduce the detail blur and improve image\nfidelity. Specifically, we train two multi-level VAEs to project compressed\ndark images and normal-light images into their latent spaces respectively.\nSecondly, we leverage a latent mapping network to transform features from\ncompressed dark space to normal-light space. Specifically, since the\ndegradation models of darkness and compression are different from each other,\nthe latent mapping process is divided mapping into enlightening branch and\ndeblocking branch. Comprehensive experiments demonstrate that the proposed\nmethod achieves state-of-the-art performance in compressed dark image\nenhancement.\n', '  Low-Light Image Enhancement (LLIE) task tends to restore the details and\nvisual information from corrupted low-light images. Most existing methods learn\nthe mapping function between low/normal-light images by Deep Neural Networks\n(DNNs) on sRGB and HSV color space. Nevertheless, enhancement involves\namplifying image signals, and applying these color spaces to low-light images\nwith a low signal-to-noise ratio can introduce sensitivity and instability into\nthe enhancement process. Consequently, this results in the presence of color\nartifacts and brightness artifacts in the enhanced images. To alleviate this\nproblem, we propose a novel trainable color space, named\nHorizontal/Vertical-Intensity (HVI). It not only decouples brightness and color\nfrom RGB channels to mitigate the instability during enhancement but also\nadapts to low-light images in different illumination ranges due to the\ntrainable parameters. Further, we design a novel Color and Intensity Decoupling\nNetwork (CIDNet) with two branches dedicated to processing the decoupled image\nbrightness and color in the HVI space. Within CIDNet, we introduce the\nLightweight Cross-Attention (LCA) module to facilitate interaction between\nimage structure and content information in both branches, while also\nsuppressing noise in low-light images. Finally, we conducted 22 quantitative\nand qualitative experiments to show that the proposed CIDNet outperforms the\nstate-of-the-art methods on 11 datasets. The code is available at\nhttps://github.com/Fediory/HVI-CIDNet.\n']",Image Enhancement and Camera Processing,Image Processing and Enhancement,Image and Video Processing,Image and Video Processing
455,13,455_mixtures_mixture_clustering_likelihood,"['mixtures', 'mixture', 'clustering', 'likelihood', 'clusterability', 'gaussian', 'estimation', 'estimating', 'mixing', 'gmms']","['mixture', 'mixtures', 'components', 'covariances', 'exponential', 'g_0', 'likelihood', 'separation', 'clustering', 'generalized']","['  The purpose of this paper is twofold. First, we propose a novel algorithm for\nestimating parameters in one-dimensional Gaussian mixture models (GMMs). The\nalgorithm takes advantage of the Hankel structure inherent in the Fourier data\nobtained from independent and identically distributed (i.i.d) samples of the\nmixture. For GMMs with a unified variance, a singular value ratio functional\nusing the Fourier data is introduced and used to resolve the variance and\ncomponent number simultaneously. The consistency of the estimator is derived.\nCompared to classic algorithms such as the method of moments and the maximum\nlikelihood method, the proposed algorithm does not require prior knowledge of\nthe number of Gaussian components or good initial guesses. Numerical\nexperiments demonstrate its superior performance in estimation accuracy and\ncomputational cost. Second, we reveal that there exists a fundamental limit to\nthe problem of estimating the number of Gaussian components or model order in\nthe mixture model if the number of i.i.d samples is finite. For the case of a\nsingle variance, we show that the model order can be successfully estimated\nonly if the minimum separation distance between the component means exceeds a\ncertain threshold value and can fail if below. We derive a lower bound for this\nthreshold value, referred to as the computational resolution limit, in terms of\nthe number of i.i.d samples, the variance, and the number of Gaussian\ncomponents. Numerical experiments confirm this phase transition phenomenon in\nestimating the model order. Moreover, we demonstrate that our algorithm\nachieves better scores in likelihood, AIC, and BIC when compared to the EM\nalgorithm.\n', '  We investigate the landscape of the negative log-likelihood function of\nGaussian Mixture Models (GMMs) with a general number of components in the\npopulation limit. As the objective function is non-convex, there can be\nmultiple local minima that are not globally optimal, even for well-separated\nmixture models. Our study reveals that all local minima share a common\nstructure that partially identifies the cluster centers (i.e., means of the\nGaussian components) of the true location mixture. Specifically, each local\nminimum can be represented as a non-overlapping combination of two types of\nsub-configurations: fitting a single mean estimate to multiple Gaussian\ncomponents or fitting multiple estimates to a single true component. These\nresults apply to settings where the true mixture components satisfy a certain\nseparation condition, and are valid even when the number of components is over-\nor under-specified. We also present a more fine-grained analysis for the\nsetting of one-dimensional GMMs with three components, which provide sharper\napproximation error bounds with improved dependence on the separation.\n', '  We consider the parameter estimation problem in the deviated Gaussian mixture\nof experts in which the data are generated from $(1 - \\lambda^{\\ast}) g_0(Y|\nX)+ \\lambda^{\\ast} \\sum_{i = 1}^{k_{\\ast}} p_{i}^{\\ast}\nf(Y|(a_{i}^{\\ast})^{\\top}X+b_i^{\\ast},\\sigma_{i}^{\\ast})$, where $X, Y$ are\nrespectively a covariate vector and a response variable, $g_{0}(Y|X)$ is a\nknown function, $\\lambda^{\\ast} \\in [0, 1]$ is true but unknown mixing\nproportion, and $(p_{i}^{\\ast}, a_{i}^{\\ast}, b_{i}^{\\ast}, \\sigma_{i}^{\\ast})$\nfor $1 \\leq i \\leq k^{\\ast}$ are unknown parameters of the Gaussian mixture of\nexperts. This problem arises from the goodness-of-fit test when we would like\nto test whether the data are generated from $g_{0}(Y|X)$ (null hypothesis) or\nthey are generated from the whole mixture (alternative hypothesis). Based on\nthe algebraic structure of the expert functions and the distinguishability\nbetween $g_0$ and the mixture part, we construct novel Voronoi-based loss\nfunctions to capture the convergence rates of maximum likelihood estimation\n(MLE) for our models. We further demonstrate that our proposed loss functions\ncharacterize the local convergence rates of parameter estimation more\naccurately than the generalized Wasserstein, a loss function being commonly\nused for estimating parameters in the Gaussian mixture of experts.\n']",Gaussian Mixture Models Estimation,Gaussian Process and Mixture Models,Probabilistic Machine Learning,Probabilistic Machine Learning
456,13,456_stochastic_langevin_markovian_markovianity,"['stochastic', 'langevin', 'markovian', 'markovianity', 'dynamics', 'drift', 'trajectories', 'entropy', 'odinger', 'observations']","['dynamics', 'turbine', 'equation', 'wind', 'trajectories', 'trajectory', 'inferring', 'thermodynamic', 'snapshots', 'underdamped']","['  Trajectory inference seeks to recover the temporal dynamics of a population\nfrom snapshots of its (uncoupled) temporal marginals, i.e. where observed\nparticles are not tracked over time. Lavenant et al. arXiv:2102.09204 addressed\nthis challenging problem under a stochastic differential equation (SDE) model\nwith a gradient-driven drift in the observed space, introducing a minimum\nentropy estimator relative to the Wiener measure. Chizat et al.\narXiv:2205.07146 then provided a practical grid-free mean-field Langevin (MFL)\nalgorithm using Schr\\""odinger bridges. Motivated by the overwhelming success of\nobservable state space models in the traditional paired trajectory inference\nproblem (e.g. target tracking), we extend the above framework to a class of\nlatent SDEs in the form of observable state space models. In this setting, we\nuse partial observations to infer trajectories in the latent space under a\nspecified dynamics model (e.g. the constant velocity/acceleration models from\ntarget tracking). We introduce PO-MFL to solve this latent trajectory inference\nproblem and provide theoretical guarantees by extending the results of\narXiv:2102.09204 to the partially observed setting. We leverage the MFL\nframework of arXiv:2205.07146, yielding an algorithm based on entropic OT\nbetween dynamics-adjusted adjacent time marginals. Experiments validate the\nrobustness of our method and the exponential convergence of the MFL dynamics,\nand demonstrate significant outperformance over the latent-free method of\narXiv:2205.07146 in key scenarios.\n', ""  In the last few years, the dynamical characterization of the power output of\na wind turbine by means of a Langevin equation has been well established. For\nthis approach, temporally highly resolved measurements of wind speed and power\noutput are used to obtain the drift and diffusion coefficients of the energy\nconversion process. These coefficients fully determine a Langevin stochastic\ndifferential equation with Gaussian white noise. The drift term specifies the\ndeterministic behavior of the system whereas the diffusion term describes the\nstochastic behavior of the system. A precise estimation of these coefficients\nis essential to understand the dynamics of the power conversion process of the\nwind turbine. We show that the dynamics of the power output of a wind turbine\nhave a hidden dependency on turbine's different operational states. Here, we\nuse an approach based on clustering Pearson correlation matrices for different\nobservables on a moving time window to identify different operational states.\nWe have identified five operational states in total, for example the state of\nrated power. Those different operational states distinguish non-stationary\nbehavior in the mutual dependencies and represent different turbine control\nsettings. As a next step, we condition our Langevin analysis on these different\nstates to reveal distinctly different behaviors of the power conversion process\nfor each operational state. Moreover, in our new representation hysteresis\neffects which have typically appeared in the Langevin dynamics of wind turbines\nseem to be resolved. We assign these typically observed hysteresis effects\nclearly to the change of the wind energy system between our estimated different\noperational states. In this contribution, we discuss further consequences for\nthe meaning of hysteric switching and detection of malbehaviors in wind\nturbines.\n"", '  Pervasive across diverse domains, stochastic systems exhibit fluctuations in\nprocesses ranging from molecular dynamics to climate phenomena. The Langevin\nequation has served as a common mathematical model for studying such systems,\nenabling predictions of their temporal evolution and analyses of thermodynamic\nquantities, including absorbed heat, work done on the system, and entropy\nproduction. However, inferring the Langevin equation from observed trajectories\nremains challenging, particularly for nonlinear and high-dimensional systems.\nIn this study, we present a comprehensive framework that employs Bayesian\nneural networks for inferring Langevin equations in both overdamped and\nunderdamped regimes. Our framework first provides the drift force and diffusion\nmatrix separately and then combines them to construct the Langevin equation. By\nproviding a distribution of predictions instead of a single value, our approach\nallows us to assess prediction uncertainties, which can prevent potential\nmisunderstandings and erroneous decisions about the system. We demonstrate the\neffectiveness of our framework in inferring Langevin equations for various\nscenarios including a neuron model and microscopic engine, highlighting its\nversatility and potential impact.\n']",Stochastic Dynamics and Langevin Equations,Stochastic Methods for Sampling and Dynamics,Probabilistic Methods and Stochastic Processes,Probabilistic Methods and Stochastic Processes
457,13,457_networks_communities_social_influence,"['networks', 'communities', 'social', 'influence', 'network', 'community', 'socially', 'friends', 'cohesion', 'distrust']","['social', 'opinion', 'coordinated', 'opinions', 'media', 'influence', 'interactions', 'reciprocity', 'communities', 'transitive']","[""  This paper revises previous work and introduces changes in spatio-temporal\nscales. The paper presents a model that includes layers A and B with varying\ndegrees of forgetting and dependence over time. We also model changes in\ndependence and forgetting in layers A, A', B, and B' under certain conditions.\nIn addition, to discuss the formation of opinion clusters that have reinforcing\nor obstructive behaviors of forgetting and dependence and are conservative or\nbrainwashing or detoxifying and less prone to filter bubbling, new clusters C\nand D that recommend, obstruct, block, or incite forgetting and dependence over\ntime are Introduction. This introduction allows us to test hypotheses regarding\nthe expansion of opinions in two dimensions over time and space, the state of\ndevelopment of opinion space, and the expansion of public opinion. Challenges\nin consensus building will be highlighted, emphasizing the dynamic nature of\nopinions and the need to consider factors such as dissent, distrust, and media\ninfluence. The paper proposes an extended framework that incorporates trust,\ndistrust, and media influence into the consensus building model. We introduce\nnetwork analysis using dimerizing as a method to gain deeper insights. In this\ncontext, we discuss network clustering, media influence, and consensus\nbuilding. The location and distribution of dimers will be analyzed to gain\ninsight into the structure and dynamics of the network. Dimertiling has been\napplied in various fields other than network analysis, such as physics and\nsociology. The paper concludes by emphasizing the importance of diverse\nperspectives, network analysis, and influential entities in consensus building.\nIt also introduces torus-based visualizations that aid in understanding complex\nnetwork structures.\n"", '  This note considers an innovative interdisciplinary methodology that bridges\nthe gap between the fundamental principles of quantum mechanics applied to the\nstudy of materials such as tellurium nanoparticles (TeNPs) and graphene and the\ncomplex dynamics of social systems. The basis for this approach lies in the\nmetaphorical parallels drawn between the structural features of TeNPs and\ngraphene and the behavioral patterns of social groups in the face of\nmisinformation. TeNPs exhibit unique properties such as the strengthening of\ncovalent bonds within telluric chains and the disruption of secondary structure\nleading to the separation of these chains. This is analogous to increased\ncohesion within social groups and disruption of information flow between\ndifferent subgroups, respectively. . Similarly, the outstanding properties of\ngraphene, such as high electrical conductivity, strength, and flexibility,\nprovide additional aspects for understanding the resilience and adaptability of\nsocial structures in response to external stimuli such as fake news. This\nresearch note proposes a novel metaphorical framework for analyzing the spread\nof fake news within social groups, analogous to the structural features of\ntelluric nanoparticles (TeNPs). We investigate how the strengthening of\ncovalent bonds within TeNPs reflects the strengthening of social cohesion in\ngroups that share common beliefs and values. This paper is partially an attempt\nto utilize ""Generative AI"" and was written with educational intent. There are\ncurrently no plans for it to become a peer-reviewed paper.\n', ""  As people's opinions change, their social networks typically coevolve with\nthem. People are often more susceptible to influence by people with similar\nopinions than by people with dissimilar opinions. In a bounded-confidence model\n(BCM) of opinion dynamics, interacting individuals influence each other through\ndyadic influence if and only if their opinions are sufficiently similar to each\nother. We introduce `neighborhood BCMs' (NBCMs) that include both the usual\ndyadic influence and a transitive influence, which models the effect of friends\nof a friend when determining whether or not an interaction with a friend\ninfluences an individual. In this transitive influence, an individual's opinion\nis influenced by a neighbor when, on average, the opinions of the neighbor's\nneighbors are sufficiently similar to their own opinion. We formulate\nneighborhood Deffuant--Weisbuch (NDW) and neighborhood Hegselmann--Krause (NHK)\nBCMs. We simulate our NDW model on time-independent networks and observe\ninteresting opinion states that cannot occur in an associated baseline DW\nmodel. We also simulate our NDW model on adaptive networks that coevolve with\nopinions by changing its structure through `transitive homophily'. An\nindividual that breaks a tie to one of its neighbors and then rewires that tie\nto a new individual, with a preference for individuals with a mean neighbor\nopinion that is closer to that individual's opinion. We explore how the\nqualitative opinion dynamics and network properties of our time-independent and\nadaptive NDWM models change as we adjust the relative proportions of dyadic and\ntransitive influence. Finally, we study a two-layer opinion--disease model in\nwhich we couple our NDW model with disease spread through a shared adaptive\nnetwork that can change both on the opinion layer and on the disease layer and\nwe examine how the opinion dynamics affect disease spread.\n""]",Social Network Influence and Community Dynamics,Influence and Information Diffusion in Complex Networks,Information Dynamics and Network Influence,Information Dynamics and Network Influence
458,13,458_activations_sparse_activation_neuron,"['activations', 'sparse', 'activation', 'neuron', 'neurons', 'progressive_gradient_flow_nm_sparsity', 'relu', 'sparsely', 'sparsity', 'sparsification']","['activation', 'sparsity', 'sparse', 'neurons', 'sparsification', 'inference', 'magnitudes', 'recipes', 'activations', 'inactive']","['  In this work, we systematically investigate the efficacy of dynamic\nactivation mechanisms within the LLaMA family of language models. Despite the\npotential of dynamic activation methods to reduce computation and increase\nspeed in models using the ReLU activation function, our empirical findings have\nuncovered several inherent pitfalls in the current dynamic activation schemes.\nThrough extensive experiments across various dynamic activation strategies, we\ndemonstrate that LLaMA models usually underperform when compared to their ReLU\ncounterparts, particularly in scenarios demanding high sparsity ratio. We\nattribute these deficiencies to a combination of factors: 1) the inherent\ncomplexity of dynamically predicting activation heads and neurons; 2) the\ninadequate sparsity resulting from activation functions; 3) the insufficient\npreservation of information resulting from KV cache skipping. Our analysis not\nonly sheds light on the limitations of dynamic activation in the context of\nlarge-scale LLaMA models but also proposes roadmaps for enhancing the design of\nfuture sparsity schemes.\n', '  Sparse computation offers a compelling solution for the inference of Large\nLanguage Models (LLMs) in low-resource scenarios by dynamically skipping the\ncomputation of inactive neurons. While traditional approaches focus on\nReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of\nsparse LLMs beyond zero activation values. We introduce a general method that\ndefines neuron activation through neuron output magnitudes and a tailored\nmagnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse\nactivation. To find the most efficient activation function for sparse\ncomputation, we propose a systematic framework to examine the sparsity of LLMs\nfrom three aspects: the trade-off between sparsity and performance, the\npredictivity of sparsity, and the hardware affinity. We conduct thorough\nexperiments on LLMs utilizing different activation functions, including ReLU,\nSwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing\nReLU$^2$ excel across all three evaluation aspects, highlighting its potential\nas an efficient activation function for sparse LLMs. We will release the code\nto facilitate future research.\n', '  Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named ""ProSparse""\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup.\n']",Sparse Activations in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
458,13,458_activations_sparse_activation_neuron,"['activations', 'sparse', 'activation', 'neuron', 'neurons', 'progressive_gradient_flow_nm_sparsity', 'relu', 'sparsely', 'sparsity', 'sparsification']","['activation', 'sparsity', 'sparse', 'neurons', 'sparsification', 'inference', 'magnitudes', 'recipes', 'activations', 'inactive']","['  In this work, we systematically investigate the efficacy of dynamic\nactivation mechanisms within the LLaMA family of language models. Despite the\npotential of dynamic activation methods to reduce computation and increase\nspeed in models using the ReLU activation function, our empirical findings have\nuncovered several inherent pitfalls in the current dynamic activation schemes.\nThrough extensive experiments across various dynamic activation strategies, we\ndemonstrate that LLaMA models usually underperform when compared to their ReLU\ncounterparts, particularly in scenarios demanding high sparsity ratio. We\nattribute these deficiencies to a combination of factors: 1) the inherent\ncomplexity of dynamically predicting activation heads and neurons; 2) the\ninadequate sparsity resulting from activation functions; 3) the insufficient\npreservation of information resulting from KV cache skipping. Our analysis not\nonly sheds light on the limitations of dynamic activation in the context of\nlarge-scale LLaMA models but also proposes roadmaps for enhancing the design of\nfuture sparsity schemes.\n', '  Sparse computation offers a compelling solution for the inference of Large\nLanguage Models (LLMs) in low-resource scenarios by dynamically skipping the\ncomputation of inactive neurons. While traditional approaches focus on\nReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of\nsparse LLMs beyond zero activation values. We introduce a general method that\ndefines neuron activation through neuron output magnitudes and a tailored\nmagnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse\nactivation. To find the most efficient activation function for sparse\ncomputation, we propose a systematic framework to examine the sparsity of LLMs\nfrom three aspects: the trade-off between sparsity and performance, the\npredictivity of sparsity, and the hardware affinity. We conduct thorough\nexperiments on LLMs utilizing different activation functions, including ReLU,\nSwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing\nReLU$^2$ excel across all three evaluation aspects, highlighting its potential\nas an efficient activation function for sparse LLMs. We will release the code\nto facilitate future research.\n', '  Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named ""ProSparse""\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup.\n']",Sparse Activations in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
458,13,458_activations_sparse_activation_neuron,"['activations', 'sparse', 'activation', 'neuron', 'neurons', 'progressive_gradient_flow_nm_sparsity', 'relu', 'sparsely', 'sparsity', 'sparsification']","['activation', 'sparsity', 'sparse', 'neurons', 'sparsification', 'inference', 'magnitudes', 'recipes', 'activations', 'inactive']","['  In this work, we systematically investigate the efficacy of dynamic\nactivation mechanisms within the LLaMA family of language models. Despite the\npotential of dynamic activation methods to reduce computation and increase\nspeed in models using the ReLU activation function, our empirical findings have\nuncovered several inherent pitfalls in the current dynamic activation schemes.\nThrough extensive experiments across various dynamic activation strategies, we\ndemonstrate that LLaMA models usually underperform when compared to their ReLU\ncounterparts, particularly in scenarios demanding high sparsity ratio. We\nattribute these deficiencies to a combination of factors: 1) the inherent\ncomplexity of dynamically predicting activation heads and neurons; 2) the\ninadequate sparsity resulting from activation functions; 3) the insufficient\npreservation of information resulting from KV cache skipping. Our analysis not\nonly sheds light on the limitations of dynamic activation in the context of\nlarge-scale LLaMA models but also proposes roadmaps for enhancing the design of\nfuture sparsity schemes.\n', '  Sparse computation offers a compelling solution for the inference of Large\nLanguage Models (LLMs) in low-resource scenarios by dynamically skipping the\ncomputation of inactive neurons. While traditional approaches focus on\nReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of\nsparse LLMs beyond zero activation values. We introduce a general method that\ndefines neuron activation through neuron output magnitudes and a tailored\nmagnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse\nactivation. To find the most efficient activation function for sparse\ncomputation, we propose a systematic framework to examine the sparsity of LLMs\nfrom three aspects: the trade-off between sparsity and performance, the\npredictivity of sparsity, and the hardware affinity. We conduct thorough\nexperiments on LLMs utilizing different activation functions, including ReLU,\nSwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing\nReLU$^2$ excel across all three evaluation aspects, highlighting its potential\nas an efficient activation function for sparse LLMs. We will release the code\nto facilitate future research.\n', '  Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named ""ProSparse""\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup.\n']",Sparse Activations in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
458,13,458_activations_sparse_activation_neuron,"['activations', 'sparse', 'activation', 'neuron', 'neurons', 'progressive_gradient_flow_nm_sparsity', 'relu', 'sparsely', 'sparsity', 'sparsification']","['activation', 'sparsity', 'sparse', 'neurons', 'sparsification', 'inference', 'magnitudes', 'recipes', 'activations', 'inactive']","['  In this work, we systematically investigate the efficacy of dynamic\nactivation mechanisms within the LLaMA family of language models. Despite the\npotential of dynamic activation methods to reduce computation and increase\nspeed in models using the ReLU activation function, our empirical findings have\nuncovered several inherent pitfalls in the current dynamic activation schemes.\nThrough extensive experiments across various dynamic activation strategies, we\ndemonstrate that LLaMA models usually underperform when compared to their ReLU\ncounterparts, particularly in scenarios demanding high sparsity ratio. We\nattribute these deficiencies to a combination of factors: 1) the inherent\ncomplexity of dynamically predicting activation heads and neurons; 2) the\ninadequate sparsity resulting from activation functions; 3) the insufficient\npreservation of information resulting from KV cache skipping. Our analysis not\nonly sheds light on the limitations of dynamic activation in the context of\nlarge-scale LLaMA models but also proposes roadmaps for enhancing the design of\nfuture sparsity schemes.\n', '  Sparse computation offers a compelling solution for the inference of Large\nLanguage Models (LLMs) in low-resource scenarios by dynamically skipping the\ncomputation of inactive neurons. While traditional approaches focus on\nReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of\nsparse LLMs beyond zero activation values. We introduce a general method that\ndefines neuron activation through neuron output magnitudes and a tailored\nmagnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse\nactivation. To find the most efficient activation function for sparse\ncomputation, we propose a systematic framework to examine the sparsity of LLMs\nfrom three aspects: the trade-off between sparsity and performance, the\npredictivity of sparsity, and the hardware affinity. We conduct thorough\nexperiments on LLMs utilizing different activation functions, including ReLU,\nSwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing\nReLU$^2$ excel across all three evaluation aspects, highlighting its potential\nas an efficient activation function for sparse LLMs. We will release the code\nto facilitate future research.\n', '  Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named ""ProSparse""\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup.\n']",Sparse Activations in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
458,13,458_activations_sparse_activation_neuron,"['activations', 'sparse', 'activation', 'neuron', 'neurons', 'progressive_gradient_flow_nm_sparsity', 'relu', 'sparsely', 'sparsity', 'sparsification']","['activation', 'sparsity', 'sparse', 'neurons', 'sparsification', 'inference', 'magnitudes', 'recipes', 'activations', 'inactive']","['  In this work, we systematically investigate the efficacy of dynamic\nactivation mechanisms within the LLaMA family of language models. Despite the\npotential of dynamic activation methods to reduce computation and increase\nspeed in models using the ReLU activation function, our empirical findings have\nuncovered several inherent pitfalls in the current dynamic activation schemes.\nThrough extensive experiments across various dynamic activation strategies, we\ndemonstrate that LLaMA models usually underperform when compared to their ReLU\ncounterparts, particularly in scenarios demanding high sparsity ratio. We\nattribute these deficiencies to a combination of factors: 1) the inherent\ncomplexity of dynamically predicting activation heads and neurons; 2) the\ninadequate sparsity resulting from activation functions; 3) the insufficient\npreservation of information resulting from KV cache skipping. Our analysis not\nonly sheds light on the limitations of dynamic activation in the context of\nlarge-scale LLaMA models but also proposes roadmaps for enhancing the design of\nfuture sparsity schemes.\n', '  Sparse computation offers a compelling solution for the inference of Large\nLanguage Models (LLMs) in low-resource scenarios by dynamically skipping the\ncomputation of inactive neurons. While traditional approaches focus on\nReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of\nsparse LLMs beyond zero activation values. We introduce a general method that\ndefines neuron activation through neuron output magnitudes and a tailored\nmagnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse\nactivation. To find the most efficient activation function for sparse\ncomputation, we propose a systematic framework to examine the sparsity of LLMs\nfrom three aspects: the trade-off between sparsity and performance, the\npredictivity of sparsity, and the hardware affinity. We conduct thorough\nexperiments on LLMs utilizing different activation functions, including ReLU,\nSwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing\nReLU$^2$ excel across all three evaluation aspects, highlighting its potential\nas an efficient activation function for sparse LLMs. We will release the code\nto facilitate future research.\n', '  Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named ""ProSparse""\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup.\n']",Sparse Activations in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
458,13,458_activations_sparse_activation_neuron,"['activations', 'sparse', 'activation', 'neuron', 'neurons', 'progressive_gradient_flow_nm_sparsity', 'relu', 'sparsely', 'sparsity', 'sparsification']","['activation', 'sparsity', 'sparse', 'neurons', 'sparsification', 'inference', 'magnitudes', 'recipes', 'activations', 'inactive']","['  In this work, we systematically investigate the efficacy of dynamic\nactivation mechanisms within the LLaMA family of language models. Despite the\npotential of dynamic activation methods to reduce computation and increase\nspeed in models using the ReLU activation function, our empirical findings have\nuncovered several inherent pitfalls in the current dynamic activation schemes.\nThrough extensive experiments across various dynamic activation strategies, we\ndemonstrate that LLaMA models usually underperform when compared to their ReLU\ncounterparts, particularly in scenarios demanding high sparsity ratio. We\nattribute these deficiencies to a combination of factors: 1) the inherent\ncomplexity of dynamically predicting activation heads and neurons; 2) the\ninadequate sparsity resulting from activation functions; 3) the insufficient\npreservation of information resulting from KV cache skipping. Our analysis not\nonly sheds light on the limitations of dynamic activation in the context of\nlarge-scale LLaMA models but also proposes roadmaps for enhancing the design of\nfuture sparsity schemes.\n', '  Sparse computation offers a compelling solution for the inference of Large\nLanguage Models (LLMs) in low-resource scenarios by dynamically skipping the\ncomputation of inactive neurons. While traditional approaches focus on\nReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of\nsparse LLMs beyond zero activation values. We introduce a general method that\ndefines neuron activation through neuron output magnitudes and a tailored\nmagnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse\nactivation. To find the most efficient activation function for sparse\ncomputation, we propose a systematic framework to examine the sparsity of LLMs\nfrom three aspects: the trade-off between sparsity and performance, the\npredictivity of sparsity, and the hardware affinity. We conduct thorough\nexperiments on LLMs utilizing different activation functions, including ReLU,\nSwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing\nReLU$^2$ excel across all three evaluation aspects, highlighting its potential\nas an efficient activation function for sparse LLMs. We will release the code\nto facilitate future research.\n', '  Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, activation sparsity has been\nproven a promising paradigm to boost model inference efficiency. Nevertheless,\nmost large language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces a simple and effective sparsification method named ""ProSparse""\nto push LLMs for higher activation sparsity while maintaining comparable\nperformance. Specifically, after substituting the activation function of LLMs\nwith ReLU, ProSparse adopts progressive sparsity regularization with a factor\nsmoothly increasing along the multi-stage sine curves. This can enhance\nactivation sparsity and mitigate performance degradation by avoiding radical\nshifts in activation distributions. With ProSparse, we obtain high sparsity of\n89.32% for LLaMA2-7B, 88.80% for LLaMA2-13B, and 87.89% for end-size\nMiniCPM-1B, respectively, achieving comparable performance to their original\nSwish-activated versions. These present the most sparsely activated models\namong open-source LLaMA versions and competitive end-size models, considerably\nsurpassing ReluLLaMA-7B (66.98%) and ReluLLaMA-13B (71.56%). Our inference\nacceleration experiments further demonstrate the significant practical\nacceleration potential of LLMs with higher activation sparsity, obtaining up to\n4.52$\\times$ inference speedup.\n']",Sparse Activations in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
459,13,459_traffic_trafficgpt_packet_encryption,"['traffic', 'trafficgpt', 'packet', 'encryption', 'bytes', 'byte', 'classification', 'classifying', 'streams', 'encrypted']","['traffic', 'encrypted', 'packet', 'plaintext', 'classification', 'internet', 'byte', 'binnings', 'tangled', 'value']","['  With 95% of Internet traffic now encrypted, an effective approach to\nclassifying this traffic is crucial for network security and management. This\npaper introduces ECHO -- a novel optimization process for ML/DL-based encrypted\ntraffic classification. ECHO targets both classification time and memory\nutilization and incorporates two innovative techniques.\n  The first component, HO (Hyperparameter Optimization of binnings), aims at\ncreating efficient traffic representations. While previous research often uses\nrepresentations that map packet sizes and packet arrival times to fixed-sized\nbins, we show that non-uniform binnings are significantly more efficient. These\nnon-uniform binnings are derived by employing a hyperparameter optimization\nalgorithm in the training stage. HO significantly improves accuracy given a\nrequired representation size, or, equivalently, achieves comparable accuracy\nusing smaller representations.\n  Then, we introduce EC (Early Classification of traffic), which enables faster\nclassification using a cascade of classifiers adapted for different exit times,\nwhere classification is based on the level of confidence. EC reduces the\naverage classification latency by up to 90\\%. Remarkably, this method not only\nmaintains classification accuracy but also, in certain cases, improves it.\n  Using three publicly available datasets, we demonstrate that the combined\nmethod, Early Classification with Hyperparameter Optimization (ECHO), leads to\na significant improvement in classification efficiency.\n', '  Network traffic refers to the amount of data being sent and received over the\ninternet or any system that connects computers. Analyzing and understanding\nnetwork traffic is vital for improving network security and management.\nHowever, the analysis of network traffic is challenging due to the diverse\nnature of data packets, which often feature heterogeneous headers and encrypted\npayloads lacking semantics. To capture the latent semantics of traffic, a few\nstudies have adopted pre-training techniques based on the Transformer encoder\nor decoder to learn the representations from massive traffic data. However,\nthese methods typically excel in traffic understanding (classification) or\ntraffic generation tasks. To address this issue, we develop Lens, a foundation\nmodel for network traffic that leverages the T5 architecture to learn the\npre-trained representations from large-scale unlabeled data. Harnessing the\nstrength of the encoder-decoder framework, which captures the global\ninformation while preserving the generative ability, our model can better learn\nthe representations from raw data. To further enhance pre-training\neffectiveness, we design a novel loss that combines three distinct tasks:\nMasked Span Prediction (MSP), Packet Order Prediction (POP), and Homologous\nTraffic Prediction (HTP). Evaluation results across various benchmark datasets\ndemonstrate that the proposed Lens outperforms the baselines in most downstream\ntasks related to both traffic understanding and generation. Notably, it also\nrequires much less labeled data for fine-tuning compared to current methods.\n', ""  Encrypted traffic classification is the task of identifying the application\nor service associated with encrypted network traffic. One effective approach\nfor this task is to use deep learning methods to encode the raw traffic bytes\ndirectly and automatically extract features for classification (byte-based\nmodels). However, current byte-based models input raw traffic bytes, whether\nplaintext or encrypted text, for automated feature extraction, neglecting the\ndistinct impacts of plaintext and encrypted text on downstream tasks.\nAdditionally, these models primarily focus on improving classification\naccuracy, with little emphasis on the efficiency of models. In this paper, for\nthe first time, we analyze the impact of plaintext and encrypted text on the\nmodel's effectiveness and efficiency. Based on our observations and findings,\nwe propose a two-phase approach to balance the trade-off between plaintext and\nencrypted text in traffic classification. Specifically, Stage one is to\nDetermine whether the Plain text is enough to be accurately Classified (DPC)\nusing the proposed DPC Selector. This stage quickly identifies samples that can\nbe classified using plaintext, leveraging explicit byte features in plaintext\nto enhance model's efficiency. Stage two aims to adaptively make a\nclassification with the result from stage one. This stage incorporates\nencrypted text information for samples that cannot be classified using\nplaintext alone, ensuring the model's effectiveness on traffic classification\ntasks. Experiments on two datasets demonstrate that our proposed model achieves\nstate-of-the-art results in both effectiveness and efficiency.\n""]",Encrypted Traffic Classification,Machine Learning for Data Privacy and Security,Machine Learning and Data Privacy,Machine Learning and Data Privacy
460,13,460_extracting_materials_parsing_extraction,"['extracting', 'materials', 'parsing', 'extraction', 'structured', 'extract', 'material', 'extracted', 'tagging', 'automated']","['materials', 'science', 'material', 'creative', 'extraction', 'papers', 'smartname', 'structured', 'outline', 'metallic']","['  This study is dedicated to assessing the capabilities of large language\nmodels (LLMs) such as GPT-3.5-Turbo, GPT-4, and GPT-4-Turbo in extracting\nstructured information from scientific documents in materials science. To this\nend, we primarily focus on two critical tasks of information extraction: (i) a\nnamed entity recognition (NER) of studied materials and physical properties and\n(ii) a relation extraction (RE) between these entities. Due to the evident lack\nof datasets within Materials Informatics (MI), we evaluated using SuperMat,\nbased on superconductor research, and MeasEval, a generic measurement\nevaluation corpus. The performance of LLMs in executing these tasks is\nbenchmarked against traditional models based on the BERT architecture and\nrule-based approaches (baseline). We introduce a novel methodology for the\ncomparative analysis of intricate material expressions, emphasising the\nstandardisation of chemical formulas to tackle the complexities inherent in\nmaterials science information assessment. For NER, LLMs fail to outperform the\nbaseline with zero-shot prompting and exhibit only limited improvement with\nfew-shot prompting. However, a GPT-3.5-Turbo fine-tuned with the appropriate\nstrategy for RE outperforms all models, including the baseline. Without any\nfine-tuning, GPT-4 and GPT-4-Turbo display remarkable reasoning and\nrelationship extraction capabilities after being provided with merely a couple\nof examples, surpassing the baseline. Overall, the results suggest that\nalthough LLMs demonstrate relevant reasoning skills in connecting concepts,\nspecialised models are currently a better choice for tasks requiring extracting\ncomplex domain-specific entities like materials. These insights provide initial\nguidance applicable to other materials science sub-domains in future work.\n', '  The vast majority of materials science knowledge exists in unstructured\nnatural language, yet structured data is crucial for innovative and systematic\nmaterials design. Traditionally, the field has relied on manual curation and\npartial automation for data extraction for specific use cases. The advent of\nlarge language models (LLMs) represents a significant shift, potentially\nenabling efficient extraction of structured, actionable data from unstructured\ntext by non-experts. While applying LLMs to materials science data extraction\npresents unique challenges, domain knowledge offers opportunities to guide and\nvalidate LLM outputs. This review provides a comprehensive overview of\nLLM-based structured data extraction in materials science, synthesizing current\nknowledge and outlining future directions. We address the lack of standardized\nguidelines and present frameworks for leveraging the synergy between LLMs and\nmaterials science expertise. This work serves as a foundational resource for\nresearchers aiming to harness LLMs for data-driven materials research. The\ninsights presented here could significantly enhance how researchers across\ndisciplines access and utilize scientific information, potentially accelerating\nthe development of novel materials for critical societal needs.\n', '  Large Language Models (LLMs) create exciting possibilities for powerful\nlanguage processing tools to accelerate research in materials science. While\nLLMs have great potential to accelerate materials understanding and discovery,\nthey currently fall short in being practical materials science tools. In this\nposition paper, we show relevant failure cases of LLMs in materials science\nthat reveal current limitations of LLMs related to comprehending and reasoning\nover complex, interconnected materials science knowledge. Given those\nshortcomings, we outline a framework for developing Materials Science LLMs\n(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis\ngeneration followed by hypothesis testing. The path to attaining performant\nMatSci-LLMs rests in large part on building high-quality, multi-modal datasets\nsourced from scientific literature where various information extraction\nchallenges persist. As such, we describe key materials science information\nextraction challenges which need to be overcome in order to build large-scale,\nmulti-modal datasets that capture valuable materials science knowledge.\nFinally, we outline a roadmap for applying future MatSci-LLMs for real-world\nmaterials discovery via: 1. Automated Knowledge Base Generation; 2. Automated\nIn-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials\nLaboratories.\n']",Materials Science Information Extraction with LLMs,Information Extraction with Large Language Models,Information Extraction,Information Extraction
461,13,461_selfreflection_responses_corrections_self,"['selfreflection', 'responses', 'corrections', 'self', 'mistakes', 'revise', 'improve', 'feedback', 'refine', 'criticize']","['correction', 'self', 'reflection', 'intrinsic', 'feedback', 'guideline', 'responses', 'external', 'correct', 'stubborn']","['  Large Language Models (LLMs) can improve their responses when instructed to\ndo so, a capability known as self-correction. When these instructions lack\nspecific details about the issues in the response, this is referred to as\nleveraging the intrinsic self-correction capability. The empirical success of\nself-correction can be found in various applications, e.g., text detoxification\nand social bias mitigation. However, leveraging this self-correction capability\nmay not always be effective, as it has the potential to revise an initially\ncorrect response into an incorrect one. In this paper, we endeavor to\nunderstand how and why leveraging the self-correction capability is effective.\nWe identify that appropriate instructions can guide LLMs to a convergence\nstate, wherein additional self-correction steps do not yield further\nperformance improvements. We empirically demonstrate that model uncertainty and\nactivated latent concepts jointly characterize the effectiveness of\nself-correction. Furthermore, we provide a mathematical formulation indicating\nthat the activated latent concept drives the convergence of the model\nuncertainty and self-correction performance. Our analysis can also be\ngeneralized to the self-correction behaviors observed in Vision-Language Models\n(VLMs). Moreover, we highlight that task-agnostic debiasing can benefit from\nour principle in terms of selecting effective fine-tuning samples. Such initial\nsuccess demonstrates the potential extensibility for better instruction tuning\nand safety alignment.\n', ""  Large language models (LLMs) have attracted significant attention for their\nremarkable abilities in various natural language processing tasks, but they\nsuffer from hallucinations that will cause performance degradation. One\npromising solution to improve the LLMs' performance is to ask LLMs to revise\ntheir answer after generation, a technique known as self-correction. Among the\ntwo types of self-correction, intrinsic self-correction is considered a\npromising direction because it does not utilize external knowledge. However,\nrecent works doubt the validity of LLM's ability to conduct intrinsic\nself-correction. In this paper, we present a novel perspective on the intrinsic\nself-correction capabilities of LLMs through theoretical analyses and empirical\nexperiments. In addition, we identify two critical factors for successful\nself-correction: zero temperature and fair prompts. Leveraging these factors,\nwe demonstrate that intrinsic self-correction ability is exhibited across\nmultiple existing LLMs. Our findings offer insights into the fundamental\ntheories underlying the self-correction behavior of LLMs and remark on the\nimportance of unbiased prompts and zero temperature settings in harnessing\ntheir full potential.\n"", '  Self-correction is an approach to improving responses from large language\nmodels (LLMs) by refining the responses using LLMs during inference. Prior work\nhas proposed various self-correction frameworks using different sources of\nfeedback, including self-evaluation and external feedback. However, there is\nstill no consensus on the question of when LLMs can correct their own mistakes,\nas recent studies also report negative results. In this work, we critically\nsurvey broad papers and discuss the conditions required for successful\nself-correction. We first find that prior studies often do not define their\nresearch questions in detail and involve impractical frameworks or unfair\nevaluations that over-evaluate self-correction. To tackle these issues, we\ncategorize research questions in self-correction research and provide a\nchecklist for designing appropriate experiments. Our critical survey based on\nthe newly categorized research questions shows that (1) no prior work\ndemonstrates successful self-correction with feedback from prompted LLMs in\ngeneral tasks, (2) self-correction works well in tasks that can use reliable\nexternal feedback, and (3) large-scale fine-tuning enables self-correction.\n']",Self-Correction in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
461,13,461_selfreflection_responses_corrections_self,"['selfreflection', 'responses', 'corrections', 'self', 'mistakes', 'revise', 'improve', 'feedback', 'refine', 'criticize']","['correction', 'self', 'reflection', 'intrinsic', 'feedback', 'guideline', 'responses', 'external', 'correct', 'stubborn']","['  Large Language Models (LLMs) can improve their responses when instructed to\ndo so, a capability known as self-correction. When these instructions lack\nspecific details about the issues in the response, this is referred to as\nleveraging the intrinsic self-correction capability. The empirical success of\nself-correction can be found in various applications, e.g., text detoxification\nand social bias mitigation. However, leveraging this self-correction capability\nmay not always be effective, as it has the potential to revise an initially\ncorrect response into an incorrect one. In this paper, we endeavor to\nunderstand how and why leveraging the self-correction capability is effective.\nWe identify that appropriate instructions can guide LLMs to a convergence\nstate, wherein additional self-correction steps do not yield further\nperformance improvements. We empirically demonstrate that model uncertainty and\nactivated latent concepts jointly characterize the effectiveness of\nself-correction. Furthermore, we provide a mathematical formulation indicating\nthat the activated latent concept drives the convergence of the model\nuncertainty and self-correction performance. Our analysis can also be\ngeneralized to the self-correction behaviors observed in Vision-Language Models\n(VLMs). Moreover, we highlight that task-agnostic debiasing can benefit from\nour principle in terms of selecting effective fine-tuning samples. Such initial\nsuccess demonstrates the potential extensibility for better instruction tuning\nand safety alignment.\n', ""  Large language models (LLMs) have attracted significant attention for their\nremarkable abilities in various natural language processing tasks, but they\nsuffer from hallucinations that will cause performance degradation. One\npromising solution to improve the LLMs' performance is to ask LLMs to revise\ntheir answer after generation, a technique known as self-correction. Among the\ntwo types of self-correction, intrinsic self-correction is considered a\npromising direction because it does not utilize external knowledge. However,\nrecent works doubt the validity of LLM's ability to conduct intrinsic\nself-correction. In this paper, we present a novel perspective on the intrinsic\nself-correction capabilities of LLMs through theoretical analyses and empirical\nexperiments. In addition, we identify two critical factors for successful\nself-correction: zero temperature and fair prompts. Leveraging these factors,\nwe demonstrate that intrinsic self-correction ability is exhibited across\nmultiple existing LLMs. Our findings offer insights into the fundamental\ntheories underlying the self-correction behavior of LLMs and remark on the\nimportance of unbiased prompts and zero temperature settings in harnessing\ntheir full potential.\n"", '  Self-correction is an approach to improving responses from large language\nmodels (LLMs) by refining the responses using LLMs during inference. Prior work\nhas proposed various self-correction frameworks using different sources of\nfeedback, including self-evaluation and external feedback. However, there is\nstill no consensus on the question of when LLMs can correct their own mistakes,\nas recent studies also report negative results. In this work, we critically\nsurvey broad papers and discuss the conditions required for successful\nself-correction. We first find that prior studies often do not define their\nresearch questions in detail and involve impractical frameworks or unfair\nevaluations that over-evaluate self-correction. To tackle these issues, we\ncategorize research questions in self-correction research and provide a\nchecklist for designing appropriate experiments. Our critical survey based on\nthe newly categorized research questions shows that (1) no prior work\ndemonstrates successful self-correction with feedback from prompted LLMs in\ngeneral tasks, (2) self-correction works well in tasks that can use reliable\nexternal feedback, and (3) large-scale fine-tuning enables self-correction.\n']",Self-Correction in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
461,13,461_selfreflection_responses_corrections_self,"['selfreflection', 'responses', 'corrections', 'self', 'mistakes', 'revise', 'improve', 'feedback', 'refine', 'criticize']","['correction', 'self', 'reflection', 'intrinsic', 'feedback', 'guideline', 'responses', 'external', 'correct', 'stubborn']","['  Large Language Models (LLMs) can improve their responses when instructed to\ndo so, a capability known as self-correction. When these instructions lack\nspecific details about the issues in the response, this is referred to as\nleveraging the intrinsic self-correction capability. The empirical success of\nself-correction can be found in various applications, e.g., text detoxification\nand social bias mitigation. However, leveraging this self-correction capability\nmay not always be effective, as it has the potential to revise an initially\ncorrect response into an incorrect one. In this paper, we endeavor to\nunderstand how and why leveraging the self-correction capability is effective.\nWe identify that appropriate instructions can guide LLMs to a convergence\nstate, wherein additional self-correction steps do not yield further\nperformance improvements. We empirically demonstrate that model uncertainty and\nactivated latent concepts jointly characterize the effectiveness of\nself-correction. Furthermore, we provide a mathematical formulation indicating\nthat the activated latent concept drives the convergence of the model\nuncertainty and self-correction performance. Our analysis can also be\ngeneralized to the self-correction behaviors observed in Vision-Language Models\n(VLMs). Moreover, we highlight that task-agnostic debiasing can benefit from\nour principle in terms of selecting effective fine-tuning samples. Such initial\nsuccess demonstrates the potential extensibility for better instruction tuning\nand safety alignment.\n', ""  Large language models (LLMs) have attracted significant attention for their\nremarkable abilities in various natural language processing tasks, but they\nsuffer from hallucinations that will cause performance degradation. One\npromising solution to improve the LLMs' performance is to ask LLMs to revise\ntheir answer after generation, a technique known as self-correction. Among the\ntwo types of self-correction, intrinsic self-correction is considered a\npromising direction because it does not utilize external knowledge. However,\nrecent works doubt the validity of LLM's ability to conduct intrinsic\nself-correction. In this paper, we present a novel perspective on the intrinsic\nself-correction capabilities of LLMs through theoretical analyses and empirical\nexperiments. In addition, we identify two critical factors for successful\nself-correction: zero temperature and fair prompts. Leveraging these factors,\nwe demonstrate that intrinsic self-correction ability is exhibited across\nmultiple existing LLMs. Our findings offer insights into the fundamental\ntheories underlying the self-correction behavior of LLMs and remark on the\nimportance of unbiased prompts and zero temperature settings in harnessing\ntheir full potential.\n"", '  Self-correction is an approach to improving responses from large language\nmodels (LLMs) by refining the responses using LLMs during inference. Prior work\nhas proposed various self-correction frameworks using different sources of\nfeedback, including self-evaluation and external feedback. However, there is\nstill no consensus on the question of when LLMs can correct their own mistakes,\nas recent studies also report negative results. In this work, we critically\nsurvey broad papers and discuss the conditions required for successful\nself-correction. We first find that prior studies often do not define their\nresearch questions in detail and involve impractical frameworks or unfair\nevaluations that over-evaluate self-correction. To tackle these issues, we\ncategorize research questions in self-correction research and provide a\nchecklist for designing appropriate experiments. Our critical survey based on\nthe newly categorized research questions shows that (1) no prior work\ndemonstrates successful self-correction with feedback from prompted LLMs in\ngeneral tasks, (2) self-correction works well in tasks that can use reliable\nexternal feedback, and (3) large-scale fine-tuning enables self-correction.\n']",Self-Correction in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
461,13,461_selfreflection_responses_corrections_self,"['selfreflection', 'responses', 'corrections', 'self', 'mistakes', 'revise', 'improve', 'feedback', 'refine', 'criticize']","['correction', 'self', 'reflection', 'intrinsic', 'feedback', 'guideline', 'responses', 'external', 'correct', 'stubborn']","['  Large Language Models (LLMs) can improve their responses when instructed to\ndo so, a capability known as self-correction. When these instructions lack\nspecific details about the issues in the response, this is referred to as\nleveraging the intrinsic self-correction capability. The empirical success of\nself-correction can be found in various applications, e.g., text detoxification\nand social bias mitigation. However, leveraging this self-correction capability\nmay not always be effective, as it has the potential to revise an initially\ncorrect response into an incorrect one. In this paper, we endeavor to\nunderstand how and why leveraging the self-correction capability is effective.\nWe identify that appropriate instructions can guide LLMs to a convergence\nstate, wherein additional self-correction steps do not yield further\nperformance improvements. We empirically demonstrate that model uncertainty and\nactivated latent concepts jointly characterize the effectiveness of\nself-correction. Furthermore, we provide a mathematical formulation indicating\nthat the activated latent concept drives the convergence of the model\nuncertainty and self-correction performance. Our analysis can also be\ngeneralized to the self-correction behaviors observed in Vision-Language Models\n(VLMs). Moreover, we highlight that task-agnostic debiasing can benefit from\nour principle in terms of selecting effective fine-tuning samples. Such initial\nsuccess demonstrates the potential extensibility for better instruction tuning\nand safety alignment.\n', ""  Large language models (LLMs) have attracted significant attention for their\nremarkable abilities in various natural language processing tasks, but they\nsuffer from hallucinations that will cause performance degradation. One\npromising solution to improve the LLMs' performance is to ask LLMs to revise\ntheir answer after generation, a technique known as self-correction. Among the\ntwo types of self-correction, intrinsic self-correction is considered a\npromising direction because it does not utilize external knowledge. However,\nrecent works doubt the validity of LLM's ability to conduct intrinsic\nself-correction. In this paper, we present a novel perspective on the intrinsic\nself-correction capabilities of LLMs through theoretical analyses and empirical\nexperiments. In addition, we identify two critical factors for successful\nself-correction: zero temperature and fair prompts. Leveraging these factors,\nwe demonstrate that intrinsic self-correction ability is exhibited across\nmultiple existing LLMs. Our findings offer insights into the fundamental\ntheories underlying the self-correction behavior of LLMs and remark on the\nimportance of unbiased prompts and zero temperature settings in harnessing\ntheir full potential.\n"", '  Self-correction is an approach to improving responses from large language\nmodels (LLMs) by refining the responses using LLMs during inference. Prior work\nhas proposed various self-correction frameworks using different sources of\nfeedback, including self-evaluation and external feedback. However, there is\nstill no consensus on the question of when LLMs can correct their own mistakes,\nas recent studies also report negative results. In this work, we critically\nsurvey broad papers and discuss the conditions required for successful\nself-correction. We first find that prior studies often do not define their\nresearch questions in detail and involve impractical frameworks or unfair\nevaluations that over-evaluate self-correction. To tackle these issues, we\ncategorize research questions in self-correction research and provide a\nchecklist for designing appropriate experiments. Our critical survey based on\nthe newly categorized research questions shows that (1) no prior work\ndemonstrates successful self-correction with feedback from prompted LLMs in\ngeneral tasks, (2) self-correction works well in tasks that can use reliable\nexternal feedback, and (3) large-scale fine-tuning enables self-correction.\n']",Self-Correction in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
461,13,461_selfreflection_responses_corrections_self,"['selfreflection', 'responses', 'corrections', 'self', 'mistakes', 'revise', 'improve', 'feedback', 'refine', 'criticize']","['correction', 'self', 'reflection', 'intrinsic', 'feedback', 'guideline', 'responses', 'external', 'correct', 'stubborn']","['  Large Language Models (LLMs) can improve their responses when instructed to\ndo so, a capability known as self-correction. When these instructions lack\nspecific details about the issues in the response, this is referred to as\nleveraging the intrinsic self-correction capability. The empirical success of\nself-correction can be found in various applications, e.g., text detoxification\nand social bias mitigation. However, leveraging this self-correction capability\nmay not always be effective, as it has the potential to revise an initially\ncorrect response into an incorrect one. In this paper, we endeavor to\nunderstand how and why leveraging the self-correction capability is effective.\nWe identify that appropriate instructions can guide LLMs to a convergence\nstate, wherein additional self-correction steps do not yield further\nperformance improvements. We empirically demonstrate that model uncertainty and\nactivated latent concepts jointly characterize the effectiveness of\nself-correction. Furthermore, we provide a mathematical formulation indicating\nthat the activated latent concept drives the convergence of the model\nuncertainty and self-correction performance. Our analysis can also be\ngeneralized to the self-correction behaviors observed in Vision-Language Models\n(VLMs). Moreover, we highlight that task-agnostic debiasing can benefit from\nour principle in terms of selecting effective fine-tuning samples. Such initial\nsuccess demonstrates the potential extensibility for better instruction tuning\nand safety alignment.\n', ""  Large language models (LLMs) have attracted significant attention for their\nremarkable abilities in various natural language processing tasks, but they\nsuffer from hallucinations that will cause performance degradation. One\npromising solution to improve the LLMs' performance is to ask LLMs to revise\ntheir answer after generation, a technique known as self-correction. Among the\ntwo types of self-correction, intrinsic self-correction is considered a\npromising direction because it does not utilize external knowledge. However,\nrecent works doubt the validity of LLM's ability to conduct intrinsic\nself-correction. In this paper, we present a novel perspective on the intrinsic\nself-correction capabilities of LLMs through theoretical analyses and empirical\nexperiments. In addition, we identify two critical factors for successful\nself-correction: zero temperature and fair prompts. Leveraging these factors,\nwe demonstrate that intrinsic self-correction ability is exhibited across\nmultiple existing LLMs. Our findings offer insights into the fundamental\ntheories underlying the self-correction behavior of LLMs and remark on the\nimportance of unbiased prompts and zero temperature settings in harnessing\ntheir full potential.\n"", '  Self-correction is an approach to improving responses from large language\nmodels (LLMs) by refining the responses using LLMs during inference. Prior work\nhas proposed various self-correction frameworks using different sources of\nfeedback, including self-evaluation and external feedback. However, there is\nstill no consensus on the question of when LLMs can correct their own mistakes,\nas recent studies also report negative results. In this work, we critically\nsurvey broad papers and discuss the conditions required for successful\nself-correction. We first find that prior studies often do not define their\nresearch questions in detail and involve impractical frameworks or unfair\nevaluations that over-evaluate self-correction. To tackle these issues, we\ncategorize research questions in self-correction research and provide a\nchecklist for designing appropriate experiments. Our critical survey based on\nthe newly categorized research questions shows that (1) no prior work\ndemonstrates successful self-correction with feedback from prompted LLMs in\ngeneral tasks, (2) self-correction works well in tasks that can use reliable\nexternal feedback, and (3) large-scale fine-tuning enables self-correction.\n']",Self-Correction in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
461,13,461_selfreflection_responses_corrections_self,"['selfreflection', 'responses', 'corrections', 'self', 'mistakes', 'revise', 'improve', 'feedback', 'refine', 'criticize']","['correction', 'self', 'reflection', 'intrinsic', 'feedback', 'guideline', 'responses', 'external', 'correct', 'stubborn']","['  Large Language Models (LLMs) can improve their responses when instructed to\ndo so, a capability known as self-correction. When these instructions lack\nspecific details about the issues in the response, this is referred to as\nleveraging the intrinsic self-correction capability. The empirical success of\nself-correction can be found in various applications, e.g., text detoxification\nand social bias mitigation. However, leveraging this self-correction capability\nmay not always be effective, as it has the potential to revise an initially\ncorrect response into an incorrect one. In this paper, we endeavor to\nunderstand how and why leveraging the self-correction capability is effective.\nWe identify that appropriate instructions can guide LLMs to a convergence\nstate, wherein additional self-correction steps do not yield further\nperformance improvements. We empirically demonstrate that model uncertainty and\nactivated latent concepts jointly characterize the effectiveness of\nself-correction. Furthermore, we provide a mathematical formulation indicating\nthat the activated latent concept drives the convergence of the model\nuncertainty and self-correction performance. Our analysis can also be\ngeneralized to the self-correction behaviors observed in Vision-Language Models\n(VLMs). Moreover, we highlight that task-agnostic debiasing can benefit from\nour principle in terms of selecting effective fine-tuning samples. Such initial\nsuccess demonstrates the potential extensibility for better instruction tuning\nand safety alignment.\n', ""  Large language models (LLMs) have attracted significant attention for their\nremarkable abilities in various natural language processing tasks, but they\nsuffer from hallucinations that will cause performance degradation. One\npromising solution to improve the LLMs' performance is to ask LLMs to revise\ntheir answer after generation, a technique known as self-correction. Among the\ntwo types of self-correction, intrinsic self-correction is considered a\npromising direction because it does not utilize external knowledge. However,\nrecent works doubt the validity of LLM's ability to conduct intrinsic\nself-correction. In this paper, we present a novel perspective on the intrinsic\nself-correction capabilities of LLMs through theoretical analyses and empirical\nexperiments. In addition, we identify two critical factors for successful\nself-correction: zero temperature and fair prompts. Leveraging these factors,\nwe demonstrate that intrinsic self-correction ability is exhibited across\nmultiple existing LLMs. Our findings offer insights into the fundamental\ntheories underlying the self-correction behavior of LLMs and remark on the\nimportance of unbiased prompts and zero temperature settings in harnessing\ntheir full potential.\n"", '  Self-correction is an approach to improving responses from large language\nmodels (LLMs) by refining the responses using LLMs during inference. Prior work\nhas proposed various self-correction frameworks using different sources of\nfeedback, including self-evaluation and external feedback. However, there is\nstill no consensus on the question of when LLMs can correct their own mistakes,\nas recent studies also report negative results. In this work, we critically\nsurvey broad papers and discuss the conditions required for successful\nself-correction. We first find that prior studies often do not define their\nresearch questions in detail and involve impractical frameworks or unfair\nevaluations that over-evaluate self-correction. To tackle these issues, we\ncategorize research questions in self-correction research and provide a\nchecklist for designing appropriate experiments. Our critical survey based on\nthe newly categorized research questions shows that (1) no prior work\ndemonstrates successful self-correction with feedback from prompted LLMs in\ngeneral tasks, (2) self-correction works well in tasks that can use reliable\nexternal feedback, and (3) large-scale fine-tuning enables self-correction.\n']",Self-Correction in Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
462,13,462_drones_drone_unmanned_uavs,"['drones', 'drone', 'unmanned', 'uavs', 'nano', 'dronet', 'aerial', 'onboard', 'pose', 'chip']","['nano', 'drones', 'drone', 'ultra', '10cm', 'localization', 'onboard', 'flight', 'navigation', 'autonomous']","['  Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning\n(TinyML), such as nano-drones, are becoming an increasingly attractive\ntechnology. Their small form factor (i.e., ~10cm diameter) ensures vast\napplicability, ranging from the exploration of narrow disaster scenarios to\nsafe human-robot interaction. Simple electronics make these CPSes inexpensive,\nbut strongly limit the computational, memory, and sensing resources available\non board. In real-world applications, these limitations are further exacerbated\nby domain shift. This fundamental machine learning problem implies that model\nperception performance drops when moving from the training domain to a\ndifferent deployment one. To cope with and mitigate this general problem, we\npresent a novel on-device fine-tuning approach that relies only on the limited\nultra-low power resources available aboard nano-drones. Then, to overcome the\nlack of ground-truth training labels aboard our CPS, we also employ a\nself-supervised method based on ego-motion consistency. Albeit our work builds\non top of a specific real-world vision-based human pose estimation task, it is\nwidely applicable for many embedded TinyML use cases. Our 512-image on-device\ntraining procedure is fully deployed aboard an ultra-low power GWT GAP9\nSystem-on-Chip and requires only 1MB of memory while consuming as low as 19mW\nor running in just 510ms (at 38mW). Finally, we demonstrate the benefits of our\non-device learning approach by field-testing our closed-loop CPS, showing a\nreduction in horizontal position error of up to 26% vs. a non-fine-tuned\nstate-of-the-art baseline. In the most challenging never-seen-before\nenvironment, our on-device learning procedure makes the difference between\nsucceeding or failing the mission.\n', '  Sub-\\SI{50}{\\gram} nano-drones are gaining momentum in both academia and\nindustry. Their most compelling applications rely on onboard deep learning\nmodels for perception despite severe hardware constraints (\\ie\nsub-\\SI{100}{\\milli\\watt} processor). When deployed in unknown environments not\nrepresented in the training data, these models often underperform due to domain\nshift. To cope with this fundamental problem, we propose, for the first time,\non-device learning aboard nano-drones, where the first part of the in-field\nmission is dedicated to self-supervised fine-tuning of a pre-trained\nconvolutional neural network (CNN). Leveraging a real-world vision-based\nregression task, we thoroughly explore performance-cost trade-offs of the\nfine-tuning phase along three axes: \\textit{i}) dataset size (more data\nincreases the regression performance but requires more memory and longer\ncomputation); \\textit{ii}) methodologies (\\eg fine-tuning all model parameters\nvs. only a subset); and \\textit{iii}) self-supervision strategy. Our approach\ndemonstrates an improvement in mean absolute error up to 30\\% compared to the\npre-trained baseline, requiring only \\SI{22}{\\second} fine-tuning on an\nultra-low-power GWT GAP9 System-on-Chip. Addressing the domain shift problem\nvia on-device learning aboard nano-drones not only marks a novel result for\nhardware-limited robots but lays the ground for more general advancements for\nthe entire robotics community.\n', ""  Sub-10cm diameter nano-drones are gaining momentum thanks to their\napplicability in scenarios prevented to bigger flying drones, such as in narrow\nenvironments and close to humans. However, their tiny form factor also brings\ntheir major drawback: ultra-constrained memory and processors for the onboard\nexecution of their perception pipelines. Therefore, lightweight deep\nlearning-based approaches are becoming increasingly popular, stressing how\ncomputational efficiency and energy-saving are paramount as they can make the\ndifference between a fully working closed-loop system and a failing one. In\nthis work, to maximize the exploitation of the ultra-limited resources aboard\nnano-drones, we present a novel adaptive deep learning-based mechanism for the\nefficient execution of a vision-based human pose estimation task. We leverage\ntwo State-of-the-Art (SoA) convolutional neural networks (CNNs) with different\nregression performance vs. computational costs trade-offs. By combining these\nCNNs with three novel adaptation strategies based on the output's temporal\nconsistency and on auxiliary tasks to swap the CNN being executed proactively,\nwe present six different systems. On a real-world dataset and the actual\nnano-drone hardware, our best-performing system, compared to executing only the\nbigger and most accurate SoA model, shows 28% latency reduction while keeping\nthe same mean absolute error (MAE), 3% MAE reduction while being iso-latency,\nand the absolute peak performance, i.e., 6% better than SoA model.\n""]",On-Device Learning for Nano-Drones,Robotics and Autonomous Systems,Robotics and Artificial Intelligence,Robotics and Artificial Intelligence
463,13,463_documentunderstanding_layoutllm_text_doclaynet,"['documentunderstanding', 'layoutllm', 'text', 'doclaynet', 'layouts', 'layout', 'document', 'textual', 'ocr', 'structured']","['document', 'layout', 'documents', 'layouts', 'understanding', 'textual', 'structured', 'text', 'spatial', 'bounding']","[""  This paper proposes LayoutLLM, a more flexible document analysis method for\nunderstanding imaged documents. Visually Rich Document Understanding tasks,\nsuch as document image classification and information extraction, have gained\nsignificant attention due to their importance. Existing methods have been\ndeveloped to enhance document comprehension by incorporating pre-training\nawareness of images, text, and layout structure. However, these methods require\nfine-tuning for each task and dataset, and the models are expensive to train\nand operate. To overcome this limitation, we propose a new LayoutLLM that\nintegrates these with large-scale language models (LLMs). By leveraging the\nstrengths of existing research in document image understanding and LLMs'\nsuperior language understanding capabilities, the proposed model, fine-tuned\nwith multimodal instruction datasets, performs an understanding of document\nimages in a single model. Our experiments demonstrate improvement over the\nbaseline model in various document analysis tasks.\n"", '  Recently, leveraging large language models (LLMs) or multimodal large\nlanguage models (MLLMs) for document understanding has been proven very\npromising. However, previous works that employ LLMs/MLLMs for document\nunderstanding have not fully explored and utilized the document layout\ninformation, which is vital for precise document understanding. In this paper,\nwe propose LayoutLLM, an LLM/MLLM based method for document understanding. The\ncore of LayoutLLM is a layout instruction tuning strategy, which is specially\ndesigned to enhance the comprehension and utilization of document layouts. The\nproposed layout instruction tuning strategy consists of two components:\nLayout-aware Pre-training and Layout-aware Supervised Fine-tuning. To capture\nthe characteristics of document layout in Layout-aware Pre-training, three\ngroups of pre-training tasks, corresponding to document-level, region-level and\nsegment-level information, are introduced. Furthermore, a novel module called\nlayout chain-of-thought (LayoutCoT) is devised to enable LayoutLLM to focus on\nregions relevant to the question and generate accurate answers. LayoutCoT is\neffective for boosting the performance of document understanding. Meanwhile, it\nbrings a certain degree of interpretability, which could facilitate manual\ninspection and correction. Experiments on standard benchmarks show that the\nproposed LayoutLLM significantly outperforms existing methods that adopt\nopen-source 7B LLMs/MLLMs for document understanding. The training data of the\nLayoutLLM is publicly available at\nhttps://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/LayoutLLM\n', '  Recent advances in training large language models (LLMs) using massive\namounts of solely textual data lead to strong generalization across many\ndomains and tasks, including document-specific tasks. Opposed to that there is\na trend to train multi-modal transformer architectures tailored for document\nunderstanding that are designed specifically to fuse textual inputs with the\ncorresponding document layout. This involves a separate fine-tuning step for\nwhich additional training data is required. At present, no document\ntransformers with comparable generalization to LLMs are available That raises\nthe question which type of model is to be preferred for document understanding\ntasks. In this paper we investigate the possibility to use purely text-based\nLLMs for document-specific tasks by using layout enrichment. We explore drop-in\nmodifications and rule-based methods to enrich purely textual LLM prompts with\nlayout information. In our experiments we investigate the effects on the\ncommercial ChatGPT model and the open-source LLM Solar. We demonstrate that\nusing our approach both LLMs show improved performance on various standard\ndocument benchmarks. In addition, we study the impact of noisy OCR and layout\nerrors, as well as the limitations of LLMs when it comes to utilizing document\nlayout. Our results indicate that layout enrichment can improve the performance\nof purely text-based LLMs for document understanding by up to 15% compared to\njust using plain document text. In conclusion, this approach should be\nconsidered for the best model choice between text-based LLM or multi-modal\ndocument transformers.\n']",Document Understanding with LayoutLLM,Document Understanding with Multimodal and Layout-Aware Language Models,Multimodal Learning and Vision-Language Models,Multimodal Learning
464,13,464_nlp_classification_adapting_language,"['nlp', 'classification', 'adapting', 'language', 'tasks', 'examples', 'generalization', 'models', 'training', 'accuracy']","['curricula', 'priming', 'tuning', 'class', 'shot', 'specialised', 'fine', 'curriculum', 'downstream', 'black']","['  Parameter-efficient (PE) methods (like Prompts or Adapters) for adapting\npre-trained language models (PLM) to downstream tasks have been popular\nrecently. However, hindrances still prevent these methods from reaching their\nfull potential. For example, two significant challenges are few-shot adaptation\nand cross-task generalization. To tackle these issues, we propose a general PE\npriming framework to enhance and explore the few-shot adaptation and\ngeneralization ability of PE methods. In this framework, PLMs are primed with\nPE methods for rapidly adapting to various target tasks. To evaluate the\ngeneralization ability of these PE methods, we conduct experiments on a\nfew-shot cross-domain benchmark containing 160 diverse NLP tasks. Our\nexperiment not only reveals the best priming strategy but also verifies that\npriming facilitates the adaptation to target tasks.\n', '  Training or finetuning large-scale language models (LLMs) requires\nsubstantial computation resources, motivating recent efforts to explore\nparameter-efficient adaptation to downstream tasks. One approach is to treat\nthese models as black boxes and use forward passes (Inference APIs) to interact\nwith them. Current research focuses on adapting these black-box models to\ndownstream tasks using gradient-free prompt optimization, but this often\ninvolves an expensive process of searching task-specific prompts. Therefore, we\nare motivated to study black-box language model adaptation without prompt\nsearch. Specifically, we introduce a label-enhanced cross-attention network\ncalled CrossTune, which models the semantic relatedness between the input text\nsequence and task-specific label descriptions. Its effectiveness is examined in\nthe context of few-shot text classification. To improve the generalization of\nCrossTune, we utilize ChatGPT to generate additional training data through\nin-context learning. A switch mechanism is implemented to exclude low-quality\nChatGPT-generated data. Through extensive experiments on seven benchmark text\nclassification datasets, we demonstrate that our proposed approach outperforms\nthe previous state-of-the-art gradient-free black-box tuning method by 5.7% on\naverage. Even without using ChatGPT-augmented data, CrossTune performs better\nor comparably than previous black-box tuning methods, suggesting the\neffectiveness of our approach.\n', ""  For language model classification, would you prefer having only one workable\nclass or having every class working? The latter makes more practical uses.\nEspecially for large language models (LLMs), the fact that they achieve a fair\noverall accuracy by in-context learning (ICL) obscures a large difference in\nindividual class accuracies. In this work, we uncover and tackle language\nmodels' imbalance in per-class prediction accuracy by reconceptualizing it as\nthe Contextual Oddity Bias (COBias), and we are the first to engage nonlinear\ninteger programming (NIP) to debias it. Briefly, COBias refers to the\ndifference in accuracy by a class A compared to its ''odd'' class, which holds\nthe majority wrong predictions of class A. With the COBias metric, we reveal\nthat LLMs of varied scales and families exhibit large per-class accuracy\ndifferences. Then we propose Debiasing as Nonlinear Integer Programming (DNIP)\nto correct ICL per-class probabilities for lower bias and higher overall\naccuracy. Our optimization objective is directly based on the evaluation scores\nby COBias and accuracy metrics, solved by simulated annealing. Evaluations on\nthree LLMs across seven NLP classification tasks show that DNIP simultaneously\nachieves significant COBias reduction ($-27\\%$) and accuracy improvement\n($+12\\%$) over the conventional ICL approach, suggesting that modeling pairwise\nclass accuracy differences is a direction in pushing forward more accurate,\nmore reliable LLM predictions.\n""]",Adapting Language Models for NLP Tasks,Natural Language Processing (NLP) Techniques and Applications,Natural Language Processing,Natural Language Processing
465,13,465_detoxify_detoxifying_detoxified_detoxifier,"['detoxify', 'detoxifying', 'detoxified', 'detoxifier', 'detoxifies', 'detoxification', 'detox', 'detoxifiable', 'detoxifiability', 'corpus']","['detoxification', 'toxic', 'toxicity', 'detoxifier', 'detoxifying', 'detoxified', 'text', 'parallel', 'undesirable', 'corpus']","['  Prior works on detoxification are scattered in the sense that they do not\ncover all aspects of detoxification needed in a real-world scenario. Notably,\nprior works restrict the task of developing detoxification models to only a\nseen subset of platforms, leaving the question of how the models would perform\non unseen platforms unexplored. Additionally, these works do not address\nnon-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified\nwithout altering the meaning. We propose GreenLLaMA, the first comprehensive\nend-to-end detoxification framework, which attempts to alleviate the\naforementioned limitations. We first introduce a cross-platform pseudo-parallel\ncorpus applying multi-step data processing and generation strategies leveraging\nChatGPT. We then train a suite of detoxification models with our cross-platform\ncorpus. We show that our detoxification models outperform the SoTA model\ntrained with human-annotated parallel corpus. We further introduce explanation\nto promote transparency and trustworthiness. GreenLLaMA additionally offers a\nunique paraphrase detector especially dedicated for the detoxification task to\ntackle the non-detoxifiable cases. Through experimental analysis, we\ndemonstrate the effectiveness of our cross-platform corpus and the robustness\nof GreenLLaMA against adversarial toxicity.\n', '  Toxicity mitigation consists in rephrasing text in order to remove offensive\nor harmful meaning. Neural natural language processing (NLP) models have been\nwidely used to target and mitigate textual toxicity. However, existing methods\nfail to detoxify text while preserving the initial non-toxic meaning at the\nsame time. In this work, we propose to apply counterfactual generation methods\nfrom the eXplainable AI (XAI) field to target and mitigate textual toxicity. In\nparticular, we perform text detoxification by applying local feature importance\nand counterfactual generation methods to a toxicity classifier distinguishing\nbetween toxic and non-toxic texts. We carry out text detoxification through\ncounterfactual generation on three datasets and compare our approach to three\ncompetitors. Automatic and human evaluations show that recently developed NLP\ncounterfactual generators can mitigate toxicity accurately while better\npreserving the meaning of the initial text as compared to classical\ndetoxification methods. Finally, we take a step back from using automated\ndetoxification tools, and discuss how to manage the polysemous nature of\ntoxicity and the risk of malicious use of detoxification tools. This work is\nthe first to bridge the gap between counterfactual generation and text\ndetoxification and paves the way towards more practical application of XAI\nmethods.\n', '  Text detoxification aims to minimize the risk of language models producing\ntoxic content. Existing detoxification methods of directly constraining the\nmodel output or further training the model on the non-toxic corpus fail to\nachieve a decent balance between detoxification effectiveness and generation\nquality. This issue stems from the neglect of constrain imposed by the context\nsince language models are designed to generate output that closely matches the\ncontext while detoxification methods endeavor to ensure the safety of the\noutput even if it semantically deviates from the context. In view of this, we\nintroduce a Context-aware Model self-Detoxification~(CMD) framework that pays\nattention to both the context and the detoxification process, i.e., first\ndetoxifying the context and then making the language model generate along the\nsafe context. Specifically, CMD framework involves two phases: utilizing\nlanguage models to synthesize data and applying these data for training. We\nalso introduce a toxic contrastive loss that encourages the model generation\naway from the negative toxic samples. Experiments on various LLMs have verified\nthe effectiveness of our MSD framework, which can yield the best performance\ncompared to baselines.\n']",Text Detoxification Methods,Toxicity Detection and Mitigation in Natural Language Processing,Misbehavior and Toxicity in Online Content,Misbehavior and Toxicity in Online Content
466,13,466_counterfactual_interventions_counterfactuals_counterfactual_causality,"['counterfactual_interventions', 'counterfactuals', 'counterfactual', 'causality', 'causal', 'causation', 'inference', 'outcomes', 'consistency', 'interventions']","['counterfactual', 'counterfactuals', 'causal', 'structural', 'outcomes', 'curvature', 'causation', 'intervention', 'frameworks', 'degenerative']","['  The capacity to address counterfactual ""what if"" inquiries is crucial for\nunderstanding and making use of causal influences. Traditional counterfactual\ninference, under Pearls\' counterfactual framework, typically depends on having\naccess to or estimating a structural causal model. Yet, in practice, this\ncausal model is often unknown and might be challenging to identify. Hence, this\npaper aims to perform reliable counterfactual inference based solely on\nobservational data and the (learned) qualitative causal structure, without\nnecessitating a predefined causal model or even direct estimations of\nconditional distributions. To this end, we establish a novel connection between\ncounterfactual inference and quantile regression and show that counterfactual\ninference can be reframed as an extended quantile regression problem. Building\non this insight, we propose a practical framework for efficient and effective\ncounterfactual inference implemented with neural networks under a bi-level\noptimization scheme. The proposed approach enhances the capacity to generalize\nestimated counterfactual outcomes to unseen data, thereby providing an upper\nbound on the generalization error. Furthermore, empirical evidence demonstrates\nits superior statistical efficiency in comparison to existing methods.\nEmpirical results conducted on multiple datasets offer compelling support for\nour theoretical assertions.\n', ""  In the field of causal modeling, potential outcomes (PO) and structural\ncausal models (SCMs) stand as the predominant frameworks. However, these\nframeworks face notable challenges in practically modeling counterfactuals,\nformalized as parameters of the joint distribution of potential outcomes.\nCounterfactual reasoning holds paramount importance in contemporary\ndecision-making processes, especially in scenarios that demand personalized\nincentives based on the joint values of $(Y(0), Y(1))$. This paper begins with\nan investigation of the PO and SCM frameworks for modeling counterfactuals.\nThrough the analysis, we identify an inherent model capacity limitation, termed\nas the ``degenerative counterfactual problem'', emerging from the consistency\nrule that is the cornerstone of both frameworks. To address this limitation, we\nintroduce a novel \\textit{distribution-consistency} assumption, and in\nalignment with it, we propose the Distribution-consistency Structural Causal\nModels (DiscoSCMs) offering enhanced capabilities to model counterfactuals. To\nconcretely reveal the enhanced model capacity, we introduce a new identifiable\ncausal parameter, \\textit{the probability of consistency}, which holds\npractical significance within DiscoSCM alone, showcased with a personalized\nincentive example. Furthermore, we provide a comprehensive set of theoretical\nresults about the ``Ladder of Causation'' within the DiscoSCM framework. We\nhope it opens new avenues for future research of counterfactual modeling,\nultimately enhancing our understanding of causality and its real-world\napplications.\n"", '  Accurate estimation of counterfactual outcomes in high-dimensional data is\ncrucial for decision-making and understanding causal relationships and\nintervention outcomes in various domains, including healthcare, economics, and\nsocial sciences. However, existing methods often struggle to generate accurate\nand consistent counterfactuals, particularly when the causal relationships are\ncomplex. We propose a novel framework that incorporates causal mechanisms and\ndiffusion models to generate high-quality counterfactual samples guided by\ncausal representation. Our approach introduces a novel, theoretically grounded\ntraining and sampling process that enables the model to consistently generate\naccurate counterfactual high-dimensional data under multiple intervention\nsteps. Experimental results on various synthetic and real benchmarks\ndemonstrate the proposed approach outperforms state-of-the-art methods in\ngenerating accurate and high-quality counterfactuals, using different\nevaluation metrics.\n']",Counterfactual Inference and Causal Modeling,Causal Analysis and Counterfactual Reasoning,Causal Analysis and Reasoning,Causal Analysis and Reasoning
467,13,467_ctm_remote_sensing_change_detection_cnn_changes_changeanywhere,"['ctm_remote_sensing_change_detection', 'cnn', 'changes', 'changeanywhere', 'change', 'detection', 'features', 'sensing', 'imagery', 'mapchange']","['change', 'remote', 'sensing', 'land', 'temporal', 'bi', 'detection', 'changes', 'optical', 'cover']","[""  Remote sensing change detection is crucial for understanding the dynamics of\nour planet's surface, facilitating the monitoring of environmental changes,\nevaluating human impact, predicting future trends, and supporting\ndecision-making. In this work, we introduce a novel approach for change\ndetection that can leverage off-the-shelf, unlabeled remote sensing images in\nthe training process by pre-training a Denoising Diffusion Probabilistic Model\n(DDPM) - a class of generative models used in image synthesis. DDPMs learn the\ntraining data distribution by gradually converting training images into a\nGaussian distribution using a Markov chain. During inference (i.e., sampling),\nthey can generate a diverse set of samples closer to the training distribution,\nstarting from Gaussian noise, achieving state-of-the-art image synthesis\nresults. However, in this work, our focus is not on image synthesis but on\nutilizing it as a pre-trained feature extractor for the downstream application\nof change detection. Specifically, we fine-tune a lightweight change classifier\nutilizing the feature representations produced by the pre-trained DDPM\nalongside change labels. Experiments conducted on the LEVIR-CD, WHU-CD,\nDSIFN-CD, and CDD datasets demonstrate that the proposed DDPM-CD method\nsignificantly outperforms the existing state-of-the-art change detection\nmethods in terms of F1 score, IoU, and overall accuracy, highlighting the\npivotal role of pre-trained DDPM as a feature extractor for downstream\napplications. We have made both the code and pre-trained models available at\nhttps://github.com/wgcban/ddpm-cd\n"", '  Remote sensing change detection (CD) is a pivotal technique that pinpoints\nchanges on a global scale based on multi-temporal images. With the recent\nexpansion of deep learning, supervised deep learning-based CD models have shown\nsatisfactory performance. However, CD sample labeling is very time-consuming as\nit is densely labeled and requires expert knowledge. To alleviate this problem,\nwe introduce ChangeAnywhere, a novel CD sample generation method using the\nsemantic latent diffusion model and single-temporal images. Specifically,\nChangeAnywhere leverages the relative ease of acquiring large single-temporal\nsemantic datasets to generate large-scale, diverse, and semantically annotated\nbi-temporal CD datasets. ChangeAnywhere captures the two essentials of CD\nsamples, i.e., change implies semantically different, and non-change implies\nreasonable change under the same semantic constraints. We generated\nChangeAnywhere-100K, the largest synthesis CD dataset with 100,000 pairs of CD\nsamples based on the proposed method. The ChangeAnywhere-100K significantly\nimproved both zero-shot and few-shot performance on two CD benchmark datasets\nfor various deep learning-based CD models, as demonstrated by transfer\nexperiments. This paper delineates the enormous potential of ChangeAnywhere for\nCD sample generation and demonstrates the subsequent enhancement of model\nperformance. Therefore, ChangeAnywhere offers a potent tool for remote sensing\nCD. All codes and pre-trained models will be available at\nhttps://github.com/tangkai-RS/ChangeAnywhere.\n', '  The existing change detection(CD) methods can be summarized as the\nvisual-first change detection (ViFi-CD) paradigm, which first extracts change\nfeatures from visual differences and then assigns them specific semantic\ninformation. However, CD is essentially dependent on change regions of interest\n(CRoIs), meaning that the CD results are directly determined by the semantics\nchanges of interest, making its primary image factor semantic of interest\nrather than visual. The ViFi-CD paradigm can only assign specific semantics of\ninterest to specific change features extracted from visual differences, leading\nto the inevitable omission of potential CRoIs and the inability to adapt to\ndifferent CRoI CD tasks. In other words, changes in other CRoIs cannot be\ndetected by the ViFi-CD method without retraining the model or significantly\nmodifying the method. This paper introduces a new CD paradigm, the\nsemantic-first CD (SeFi-CD) paradigm. The core idea of SeFi-CD is to first\nperceive the dynamic semantics of interest and then visually search for change\nfeatures related to the semantics. Based on the SeFi-CD paradigm, we designed\nAnything You Want Change Detection (AUWCD). Experiments on public datasets\ndemonstrate that the AUWCD outperforms the current state-of-the-art CD methods,\nachieving an average F1 score 5.01\\% higher than that of these advanced\nsupervised baselines on the SECOND dataset, with a maximum increase of 13.17\\%.\nThe proposed SeFi-CD offers a novel CD perspective and approach.\n']",Remote Sensing Change Detection,Change Detection and Analysis in Remote Sensing and Time Series Data,Remote Sensing and Space Data Analysis,Remote Sensing and Space Data Analysis
468,13,468_networks_graphs_gnn_vertex,"['networks', 'graphs', 'gnn', 'vertex', 'gpus', 'graphscale', 'graph', 'bottleneck', 'gnns', 'gpu']","['partitioning', 'graph', 'mini', 'batch', 'sampling', 'server', 'dataloader', 'graphs', 'memory', 'communication']","['  Graph neural networks have been shown successful in recent years. While\ndifferent GNN architectures and training systems have been developed, GNN\ntraining on large-scale real-world graphs still remains challenging. Existing\ndistributed systems load the entire graph in memory for graph partitioning,\nrequiring a huge memory space to process large graphs and thus hindering GNN\ntraining on such large graphs using commodity workstations. In this paper, we\npropose CATGNN, a cost-efficient and scalable distributed GNN training system\nwhich focuses on scaling GNN training to billion-scale or larger graphs under\nlimited computational resources. Among other features, it takes a stream of\nedges as input, instead of loading the entire graph in memory, for\npartitioning. We also propose a novel streaming partitioning algorithm named\nSPRING for distributed GNN training. We verify the correctness and\neffectiveness of CATGNN with SPRING on 16 open datasets. In particular, we\ndemonstrate that CATGNN can handle the largest publicly available dataset with\nlimited memory, which would have been infeasible without increasing the memory\nspace. SPRING also outperforms state-of-the-art partitioning algorithms\nsignificantly, with a 50% reduction in replication factor on average.\n', '  Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training\n', '  Recently, graph neural networks (GNNs) have gained much attention as a\ngrowing area of deep learning capable of learning on graph-structured data.\nHowever, the computational and memory requirements for training GNNs on\nlarge-scale graphs make it necessary to distribute the training. A prerequisite\nfor distributed GNN training is to partition the input graph into smaller parts\nthat are distributed among multiple machines of a compute cluster. Although\ngraph partitioning has been studied with regard to graph analytics and graph\ndatabases, its effect on GNN training performance is largely unexplored. As a\nconsequence, it is unclear whether investing computational efforts into\nhigh-quality graph partitioning would pay off in GNN training scenarios.\n  In this paper, we study the effectiveness of graph partitioning for\ndistributed GNN training. Our study aims to understand how different factors\nsuch as GNN parameters, mini-batch size, graph type, features size, and\nscale-out factor influence the effectiveness of graph partitioning. We conduct\nexperiments with two different GNN systems using vertex and edge partitioning.\nWe found that high-quality graph partitioning is a very effective optimization\nto speed up GNN training and to reduce memory consumption. Furthermore, our\nresults show that invested partitioning time can quickly be amortized by\nreduced GNN training time, making it a relevant optimization for most GNN\nscenarios. Compared to research on distributed graph processing, our study\nreveals that graph partitioning plays an even more significant role in\ndistributed GNN training, which motivates further research on the graph\npartitioning problem.\n']",Scalable Graph Neural Networks (GNNs) Training Systems,Graph Neural Networks (GNNs) and Graph Data Analysis,Graph Representation Learning and Neural Networks,Graph Representation Learning and Neural Networks
469,13,469_bloodstream_nanodevice_nanodevices_microvascular,"['bloodstream', 'nanodevice', 'nanodevices', 'microvascular', 'bloodstreams', 'macrovascular', 'flow', 'nanotechnology', 'arterial', 'vivo']","['nanodevices', 'localization', 'blood', 'flow', 'bloodstreams', 'hemodynamic', 'bloodstream', 'nanoscale', 'body', 'carotid']","[""  Flow-guided localization using in-body nanodevices in the bloodstream is\nexpected to be beneficial for early disease detection, continuous monitoring of\nbiological conditions, and targeted treatment. The nanodevices face size and\npower constraints that produce erroneous raw data for localization purposes.\nOn-body anchors receive this data, and use it to derive the locations of\ndiagnostic events of interest. Different Machine Learning (ML) approaches have\nbeen recently proposed for this task, yet they are currently restricted to a\nreference bloodstream of a resting patient. As such, they are unable to deal\nwith the physical diversity of patients' bloodstreams and cannot provide\ncontinuous monitoring due to changes in individual patient's activities. Toward\naddressing these issues for the current State-of-the-Art (SotA) flow-guided\nlocalization approach based on Graph Neural Networks (GNNs), we propose a\npipeline for GNN adaptation based on individual physiological indicators\nincluding height, weight, and heart rate. Our results indicate that the\nproposed adaptions are beneficial in reconciling the individual differences\nbetween bloodstreams and activities.\n"", ""  Advancements in nanotechnology and material science are paving the way toward\nnanoscale devices that combine sensing, computing, data and energy storage, and\nwireless communication. In precision medicine, these nanodevices show promise\nfor disease diagnostics, treatment, and monitoring from within the patients'\nbloodstreams. Assigning the location of a sensed biological event with the\nevent itself, which is the main proposition of flow-guided in-body nanoscale\nlocalization, would be immensely beneficial from the perspective of precision\nmedicine. The nanoscale nature of the nanodevices and the challenging\nenvironment that the bloodstream represents, result in current flow-guided\nlocalization approaches being constrained in their communication and\nenergy-related capabilities. The communication and energy constraints of the\nnanodevices result in different features of raw data for flow-guided\nlocalization, in turn affecting its performance. An analytical modeling of the\neffects of imperfect communication and constrained energy causing intermittent\noperation of the nanodevices on the raw data produced by the nanodevices would\nbe beneficial. Hence, we propose an analytical model of raw data for\nflow-guided localization, where the raw data is modeled as a function of\ncommunication and energy-related capabilities of the nanodevice. We evaluate\nthe model by comparing its output with the one obtained through the utilization\nof a simulator for objective evaluation of flow-guided localization, featuring\ncomparably higher level of realism. Our results across a number of scenarios\nand heterogeneous performance metrics indicate high similarity between the\nmodel and simulator-generated raw datasets.\n"", ""  Contemporary research advances in nanotechnology and material science are\nrooted in the emergence of nanodevices as a versatile tool that harmonizes\nsensing, computing, wireless communication, data storage, and energy\nharvesting. These devices offer novel pathways for disease diagnostics,\ntreatment, and monitoring within the bloodstreams. Ensuring precise\nlocalization of events of diagnostic interest, which underpins the concept of\nflow-guided in-body nanoscale localization, would provide an added diagnostic\nvalue to the detected events. Raw data generated by the nanodevices is pivotal\nfor this localization and consist of an event detection indicator and the time\nelapsed since the last passage of a nanodevice through the heart. The energy\nconstraints of the nanodevices lead to intermittent operation and unreliable\ncommunication, intrinsically affecting this data. This posits a need for\ncomprehensively modelling the features of this data. These imperfections also\nhave profound implications for the viability of existing flow-guided\nlocalization approaches, which are ill-prepared to address the intricacies of\nthe environment. Our first contribution lies in an analytical model of raw data\nfor flow-guided localization, dissecting how communication and energy\ncapabilities influence the nanodevices' data output. This model acts as a vital\nbridge, reconciling idealized assumptions with practical challenges of\nflow-guided localization. Toward addressing these practical challenges, we also\npresent an integration of Graph Neural Networks (GNNs) into the flow-guided\nlocalization paradigm. GNNs excel in capturing complex dynamic interactions\ninherent to the localization of events sensed by the nanodevices. Our results\nhighlight the potential of GNNs not only to enhance localization accuracy but\nalso extend coverage to encompass the entire bloodstream.\n""]",Flow-Guided Localization in Bloodstreams using Nanodevices,Flow-Guided Localization of Nanodevices in Bloodstreams,Nanodevice Localization in Biological Systems,Nanodevice Localization in Biological Systems
470,13,470_influence_influencer_influential_influence_analysis_papers,"['influence', 'influencer', 'influential', 'influence_analysis_papers', 'annotated', 'attribution', 'predictions', 'evaluations', 'gradient', 'outlier']","['influence', 'attribution', 'functions', 'influential', 'samples', 'silver', 'synset', 'gradient', 'training', 'points']","['  Large-scale black-box models have become ubiquitous across numerous\napplications. Understanding the influence of individual training data sources\non predictions made by these models is crucial for improving their\ntrustworthiness. Current influence estimation techniques involve computing\ngradients for every training point or repeated training on different subsets.\nThese approaches face obvious computational challenges when scaled up to large\ndatasets and models.\n  In this paper, we introduce and explore the Mirrored Influence Hypothesis,\nhighlighting a reciprocal nature of influence between training and test data.\nSpecifically, it suggests that evaluating the influence of training data on\ntest predictions can be reformulated as an equivalent, yet inverse problem:\nassessing how the predictions for training samples would be altered if the\nmodel were trained on specific test samples. Through both empirical and\ntheoretical validations, we demonstrate the wide applicability of our\nhypothesis. Inspired by this, we introduce a new method for estimating the\ninfluence of training data, which requires calculating gradients for specific\ntest samples, paired with a forward pass for each training point. This approach\ncan capitalize on the common asymmetry in scenarios where the number of test\nsamples under concurrent examination is much smaller than the scale of the\ntraining dataset, thus gaining a significant improvement in efficiency compared\nto existing approaches.\n  We demonstrate the applicability of our method across a range of scenarios,\nincluding data attribution in diffusion models, data leakage detection,\nanalysis of memorization, mislabeled data detection, and tracing behavior in\nlanguage models. Our code will be made available at\nhttps://github.com/ruoxi-jia-group/Forward-INF.\n', ""  Good models require good training data. For overparameterized deep models,\nthe causal relationship between training data and model predictions is\nincreasingly opaque and poorly understood. Influence analysis partially\ndemystifies training's underlying interactions by quantifying the amount each\ntraining instance alters the final model. Measuring the training data's\ninfluence exactly can be provably hard in the worst case; this has led to the\ndevelopment and use of influence estimators, which only approximate the true\ninfluence. This paper provides the first comprehensive survey of training data\ninfluence analysis and estimation. We begin by formalizing the various, and in\nplaces orthogonal, definitions of training data influence. We then organize\nstate-of-the-art influence analysis methods into a taxonomy; we describe each\nof these methods in detail and compare their underlying assumptions, asymptotic\ncomplexities, and overall strengths and weaknesses. Finally, we propose future\nresearch directions to make influence analysis more useful in practice as well\nas more theoretically and empirically sound. A curated, up-to-date list of\nresources related to influence analysis is available at\nhttps://github.com/ZaydH/influence_analysis_papers.\n"", '  Influence functions serve as crucial tools for assessing sample influence in\nmodel interpretation, subset training set selection, noisy label detection, and\nmore. By employing the first-order Taylor extension, influence functions can\nestimate sample influence without the need for expensive model retraining.\nHowever, applying influence functions directly to deep models presents\nchallenges, primarily due to the non-convex nature of the loss function and the\nlarge size of model parameters. This difficulty not only makes computing the\ninverse of the Hessian matrix costly but also renders it non-existent in some\ncases. Various approaches, including matrix decomposition, have been explored\nto expedite and approximate the inversion of the Hessian matrix, with the aim\nof making influence functions applicable to deep models. In this paper, we\nrevisit a specific, albeit naive, yet effective approximation method known as\nTracIn. This method substitutes the inverse of the Hessian matrix with an\nidentity matrix. We provide deeper insights into why this simple approximation\nmethod performs well. Furthermore, we extend its applications beyond measuring\nmodel utility to include considerations of fairness and robustness. Finally, we\nenhance TracIn through an ensemble strategy. To validate its effectiveness, we\nconduct experiments on synthetic data and extensive evaluations on noisy label\ndetection, sample selection for large language model fine-tuning, and defense\nagainst adversarial attacks.\n']",Influence Estimation in Machine Learning,Influence and Information Diffusion in Complex Networks,Information Dynamics and Network Influence,Information Dynamics and Network Influence
471,13,471_steganography_steganographic_steganalysis_watermarking,"['steganography', 'steganographic', 'steganalysis', 'watermarking', 'stegano', 'stegotexts', 'stegotext', 'watermark', 'covert', 'hide']","['steganography', 'steganographic', 'watermark', 'stegos', 'watermarking', 'stegotext', 'secret', 'covertext', 'secure', 'attribution']","['  This study discusses a new method combining image steganography technology\nwith Natural Language Processing (NLP) large models, aimed at improving the\naccuracy and robustness of extracting steganographic text. Traditional Least\nSignificant Bit (LSB) steganography techniques face challenges in accuracy and\nrobustness of information extraction when dealing with complex character\nencoding, such as Chinese characters. To address this issue, this study\nproposes an innovative LSB-NLP hybrid framework. This framework integrates the\nadvanced capabilities of NLP large models, such as error detection, correction,\nand semantic consistency analysis, as well as information reconstruction\ntechniques, thereby significantly enhancing the robustness of steganographic\ntext extraction. Experimental results show that the LSB-NLP hybrid framework\nexcels in improving the extraction accuracy of steganographic text, especially\nin handling Chinese characters. The findings of this study not only confirm the\neffectiveness of combining image steganography technology and NLP large models\nbut also propose new ideas for research and application in the field of\ninformation hiding. The successful implementation of this interdisciplinary\napproach demonstrates the great potential of integrating image steganography\ntechnology with natural language processing technology in solving complex\ninformation processing problems.\n', '  To detect stego (steganographic text) in complex scenarios, linguistic\nsteganalysis (LS) with various motivations has been proposed and achieved\nexcellent performance. However, with the development of generative\nsteganography, some stegos have strong concealment, especially after the\nemergence of LLMs-based steganography, the existing LS has low detection or\ncannot detect them. We designed a novel LS with two modes called LSGC. In the\ngeneration mode, we created an LS-task ""description"" and used the generation\nability of LLM to explain whether texts to be detected are stegos. On this\nbasis, we rethought the principle of LS and LLMs, and proposed the\nclassification mode. In this mode, LSGC deleted the LS-task ""description"" and\nused the ""causalLM"" LLMs to extract steganographic features. The LS features\ncan be extracted by only one pass of the model, and a linear layer with\ninitialization weights is added to obtain the classification probability.\nExperiments on strongly concealed stegos show that LSGC significantly improves\ndetection and reaches SOTA performance. Additionally, LSGC in classification\nmode greatly reduces training time while maintaining high performance.\n', '  Linguistic steganography (LS) tasks aim to generate steganographic text\n(stego) based on secret. Only authorized receivers can perceive and extract\nsecrets, thereby protecting privacy. However, existing generative LS schemes\noften do not consider the conditional probability of tokens in the candidate\npool, and allocate one or the same number of codings to all tokens. The tokens\nwith lower probabilities are selected to embed secrets that will affect the\nquality of stegos. As a result, the stegos are easy to perceive and detect.\nThis paper proposes the LS scheme based on dynamically allocated intervals,\ncalled DAIRstega. DAIRstega uses the idea of the roulette wheel and takes the\nconditional probabilities of tokens as the main basis for allocating the\nroulette area (i.e., the interval length). Thus, the token with a larger\nconditional probability is allocated more. The secret will be more likely to\nselect the tokens with larger probabilities. In the allocation process, we\ndesign some functions between probability and allocated interval length. Based\non the invisible characteristics of LS, we give three constraints that need to\nbe met to design the function. To simplify the form, the expression of the\nallocation function is condensed. Furthermore, DAIRstega can receive additional\ninstruction and controllably generate stegos. Rich experiments show that the\nproposed embedding way and DAIRstega perform superior to the existing ways and\nLS schemes, which shows strong perceptual, statistical, and semantic\nconcealment and anti-steganalysis ability. This scheme can also generate\nhigh-quality longer stegos, improving the deficiencies in this task. The\nexperiment also verified that DAIRstega can be used as a secure watermarking\nscheme, providing some ideas for its development.\n']",Steganography and Steganalysis Techniques,Information Hiding Techniques,Information Protection and Concealment,Information Protection and Concealment
472,13,472_rhythms_synchronization_oscillators_oscillating,"['rhythms', 'synchronization', 'oscillators', 'oscillating', 'synchronize', 'oscillator', 'bifurcation', 'fluctuations', 'dynamics', 'synchronized']","['synchronization', 'oscillators', 'dynamical', 'phase', 'dynamics', 'series', 'timescales', 'transitions', 'amplitude', 'noise']","['  Many real-world systems undergo abrupt changes in dynamics as they move\nacross critical points, often with dramatic consequences. Much existing theory\non identifying the time-series signatures of nearby critical points -- such as\nincreased variance and slower timescales -- is derived for the case of fixed,\nlow-amplitude noise. However, real-world systems are often corrupted by unknown\nlevels of noise that can distort these temporal signatures. Here we aimed to\ndevelop noise-robust indicators of the distance to criticality (DTC) for\nsystems affected by dynamical noise in two cases: when the noise amplitude is\nfixed, or is unknown and variable across recordings. To approach this problem,\nwe compare the ability of over 7000 candidate time-series features to track the\nDTC in the vicinity of a supercritical Hopf bifurcation. We recover existing\ntheory in the fixed-noise case, highlighting conventional time-series features\nthat accurately track the DTC. But in the variable-noise setting, where these\nconventional indicators perform poorly, we highlight new types of\nhigh-performing time-series features and show that their success is\naccomplished by capturing the shape of the invariant density (which depends on\nboth the DTC and the noise amplitude) relative to the spread of fast\nfluctuations (which depends on the noise amplitude). We introduce a new\nhigh-performing time-series statistic, the Rescaled Auto-Density (RAD), that\ncombines these two algorithmic components. We then use RAD to provide new\nevidence that brain regions higher in the visual hierarchy are positioned\ncloser to criticality, supporting existing hypotheses about patterns of brain\norganization that are not detected using conventional metrics of the DTC. Our\nresults demonstrate how large-scale algorithmic comparison can yield\ntheoretical insights that can motivate new theory and interpretable algorithms\nfor real-world problems.\n', '  In this study a new method for analyzing synchronization in oscillator\nsystems is proposed using the example of modeling the dynamics of a circuit of\ntwo resistively coupled pulse oscillators. The dynamic characteristic of\nsynchronization is fuzzy entropy (FuzzyEn) calculated a time series composed of\nthe ratios of the number of pulse periods (subharmonic ratio, SHR) during\nphase-locking intervals. Low entropy values indicate strong synchronization,\nwhereas high entropy values suggest weak synchronization between the two\noscillators. This method effectively visualizes synchronized modes of the\ncircuit using entropy maps of synchronization states. Additionally, a\nclassification of synchronization states is proposed based on the dependencies\nof FuzzyEn on the length of embedding vectors of SHR time series. An extension\nof this method for analyzing non-relaxation (non-spike) type signals is\nillustrated using the example of phase-phase coupling rhythms of local field\npotential of rat hippocampus. The entropy-statistical approach using rational\nfractions and pulse signal forms makes this method promising for analyzing\nbiosignal synchronization and implementing the algorithm in mobile digital\nplatforms.\n', ""  The synchronization analysis of limit-cycle oscillators is prevalent in many\nfields, including physics, chemistry, and life sciences. It relies on the phase\ncalculation that utilizes measurements. However, the synchronization of\nspatiotemporal dynamics cannot be analyzed because a standardized method for\ncalculating the phase has not been established. The presence of spatial\nstructure complicates the determination of which measurements should be used\nfor accurate phase calculation. To address this, we explore a method for\ncalculating the phase from the time series of measurements taken at a single\nspatial grid point. The phase is calculated to increase linearly between event\ntimes when the measurement time series intersects the Poincar\\'e section. The\ndifference between the calculated phase and the isochron-based phase, resulting\nfrom the discrepancy between the isochron and the Poincar\\'e section, is\nevaluated using a linear approximation near the limit-cycle solution. We found\nthat the difference is small when measurements are taken from regions that\ndominate the rhythms of the entire spatiotemporal dynamics. Furthermore, we\ninvestigate an alternative method where the Poincar\\'e section is applied to\nthe time series obtained through orthogonal decomposition of the entire\nspatiotemporal dynamics. We present two decomposition schemes that utilize the\nprincipal component analysis. For illustration, the phase is calculated from\nthe measurements of spatiotemporal dynamics exhibiting target waves or\noscillating spots, simulated by weakly coupled FitzHugh-Nagumo\nreaction-diffusion models.\n""]",Synchronization and Oscillations in Complex Systems,Complex Systems Dynamics and Synchronization,Complex Systems Analysis and Optimization,Optimization and Decision Making in Complex Systems
473,13,473_dysarthric_dysarthria_speech_impaired,"['dysarthric', 'dysarthria', 'speech', 'impaired', 'speaker', 'dsr', 'wav2vec2', 'classification', 'disability', 'assessment']","['dysarthria', 'dysarthric', 'speech', 'intelligibility', 'speaker', 'therapy', 'recognition', 'pathological', 'severity', 'mode']","['  Disordered speech recognition profound implications for improving the quality\nof life for individuals afflicted with, for example, dysarthria. Dysarthric\nspeech recognition encounters challenges including limited data, substantial\ndissimilarities between dysarthric and non-dysarthric speakers, and significant\nspeaker variations stemming from the disorder. This paper introduces\nPerceiver-Prompt, a method for speaker adaptation that utilizes P-Tuning on the\nWhisper large-scale model. We first fine-tune Whisper using LoRA and then\nintegrate a trainable Perceiver to generate fixed-length speaker prompts from\nvariable-length inputs, to improve model recognition of Chinese dysarthric\nspeech. Experimental results from our Chinese dysarthric speech dataset\ndemonstrate consistent improvements in recognition performance with\nPerceiver-Prompt. Relative reduction up to 13.04% in CER is obtained over the\nfine-tuned Whisper.\n', ""  Dysarthria is a speech disorder that hinders communication due to\ndifficulties in articulating words. Detection of dysarthria is important for\nseveral reasons as it can be used to develop a treatment plan and help improve\na person's quality of life and ability to communicate effectively. Much of the\nliterature focused on improving ASR systems for dysarthric speech. The\nobjective of the current work is to develop models that can accurately classify\nthe presence of dysarthria and also give information about the intelligibility\nlevel using limited data by employing a few-shot approach using a transformer\nmodel. This work also aims to tackle the data leakage that is present in\nprevious studies. Our whisper-large-v2 transformer model trained on a subset of\nthe UASpeech dataset containing medium intelligibility level patients achieved\nan accuracy of 85%, precision of 0.92, recall of 0.8 F1-score of 0.85, and\nspecificity of 0.91. Experimental results also demonstrate that the model\ntrained using the 'words' dataset performed better compared to the model\ntrained on the 'letters' and 'digits' dataset. Moreover, the multiclass model\nachieved an accuracy of 67%.\n"", '  Automating dysarthria assessments offers the opportunity to develop\npractical, low-cost tools that address the current limitations of manual and\nsubjective assessments. Nonetheless, the small size of most dysarthria datasets\nmakes it challenging to develop automated assessment. Recent research showed\nthat speech representations from models pre-trained on large unlabelled data\ncan enhance Automatic Speech Recognition (ASR) performance for dysarthric\nspeech. We are the first to evaluate the representations from pre-trained\nstate-of-the-art Self-Supervised models across three downstream tasks on\ndysarthric speech: disease classification, word recognition and intelligibility\nclassification, and under three noise scenarios on the UA-Speech dataset. We\nshow that HuBERT is the most versatile feature extractor across dysarthria\nclassification, word recognition, and intelligibility classification, achieving\nrespectively $+24.7\\%, +61\\%, \\text{and} +7.2\\%$ accuracy compared to classical\nacoustic features.\n']",Dysarthric Speech Recognition and Assessment,Speech Processing and Recognition Systems,Speech and Audio Processing,Speech and Audio Processing
474,13,474_twinning_twin_twinlab_twins,"['twinning', 'twin', 'twinlab', 'twins', 'modelization', 'modeling', 'neural', 'modelling', 'simulations', 'digital']","['digital', 'twins', 'twin', 'reduced', 'physical', 'twinning', 'noninvasive', 'fidelity', 'analogue', 'brain']","[""  A digital twin is a virtual replica of a real-world physical phenomena that\nuses mathematical modeling to characterize and simulate its defining features.\nBy constructing digital twins for disease processes, we can perform in-silico\nsimulations that mimic patients' health conditions and counterfactual outcomes\nunder hypothetical interventions in a virtual setting. This eliminates the need\nfor invasive procedures or uncertain treatment decisions. In this paper, we\npropose a method to identify digital twin model parameters using only\nnoninvasive patient health data. We approach the digital twin modeling as a\ncomposite inverse problem, and observe that its structure resembles pretraining\nand finetuning in self-supervised learning (SSL). Leveraging this, we introduce\na physics-informed SSL algorithm that initially pretrains a neural network on\nthe pretext task of learning a differentiable simulator of a physiological\nprocess. Subsequently, the model is trained to reconstruct physiological\nmeasurements from noninvasive modalities while being constrained by the\nphysical equations learned in pretraining. We apply our method to identify\ndigital twins of cardiac hemodynamics using noninvasive echocardiogram videos,\nand demonstrate its utility in unsupervised disease detection and in-silico\nclinical trials.\n"", '  Digital twins, the cornerstone of Industry 4.0, replicate real-world entities\nthrough computer models, revolutionising fields such as manufacturing\nmanagement and industrial automation. Recent advances in machine learning\nprovide data-driven methods for developing digital twins using discrete-time\ndata and finite-depth models on digital computers. However, this approach fails\nto capture the underlying continuous dynamics and struggles with modelling\ncomplex system behaviour. Additionally, the architecture of digital computers,\nwith separate storage and processing units, necessitates frequent data\ntransfers and Analogue-Digital (A/D) conversion, thereby significantly\nincreasing both time and energy costs. Here, we introduce a memristive neural\nordinary differential equation (ODE) solver for digital twins, which is capable\nof capturing continuous-time dynamics and facilitates the modelling of complex\nsystems using an infinite-depth model. By integrating storage and computation\nwithin analogue memristor arrays, we circumvent the von Neumann bottleneck,\nthus enhancing both speed and energy efficiency. We experimentally validate our\napproach by developing a digital twin of the HP memristor, which accurately\nextrapolates its nonlinear dynamics, achieving a 4.2-fold projected speedup and\na 41.4-fold projected decrease in energy consumption compared to\nstate-of-the-art digital hardware, while maintaining an acceptable error\nmargin. Additionally, we demonstrate scalability through experimentally\ngrounded simulations of Lorenz96 dynamics, exhibiting projected performance\nimprovements of 12.6-fold in speed and 189.7-fold in energy efficiency relative\nto traditional digital approaches. By harnessing the capabilities of fully\nanalogue computing, our breakthrough accelerates the development of digital\ntwins, offering an efficient and rapid solution to meet the demands of Industry\n4.0.\n', ""  A patient's digital twin is a computational model that describes the\nevolution of their health over time. Digital twins have the potential to\nrevolutionize medicine by enabling individual-level computer simulations of\nhuman health, which can be used to conduct more efficient clinical trials or to\nrecommend personalized treatment options. Due to the overwhelming complexity of\nhuman biology, machine learning approaches that leverage large datasets of\nhistorical patients' longitudinal health records to generate patients' digital\ntwins are more tractable than potential mechanistic models. In this manuscript,\nwe describe a neural network architecture that can learn conditional generative\nmodels of clinical trajectories, which we call Digital Twin Generators (DTGs),\nthat can create digital twins of individual patients. We show that the same\nneural network architecture can be trained to generate accurate digital twins\nfor patients across 13 different indications simply by changing the training\nset and tuning hyperparameters. By introducing a general purpose architecture,\nwe aim to unlock the ability to scale machine learning approaches to larger\ndatasets and across more indications so that a digital twin could be created\nfor any patient in the world.\n""]",Digital Twin Modeling and Simulation,Digital Twin Technology and Applications,"Advanced Technologies for Simulation, Sensing, and Fabrication","Advanced Technologies for Simulation, Sensing, and Fabrication"
474,13,474_twinning_twin_twinlab_twins,"['twinning', 'twin', 'twinlab', 'twins', 'modelization', 'modeling', 'neural', 'modelling', 'simulations', 'digital']","['digital', 'twins', 'twin', 'reduced', 'physical', 'twinning', 'noninvasive', 'fidelity', 'analogue', 'brain']","[""  A digital twin is a virtual replica of a real-world physical phenomena that\nuses mathematical modeling to characterize and simulate its defining features.\nBy constructing digital twins for disease processes, we can perform in-silico\nsimulations that mimic patients' health conditions and counterfactual outcomes\nunder hypothetical interventions in a virtual setting. This eliminates the need\nfor invasive procedures or uncertain treatment decisions. In this paper, we\npropose a method to identify digital twin model parameters using only\nnoninvasive patient health data. We approach the digital twin modeling as a\ncomposite inverse problem, and observe that its structure resembles pretraining\nand finetuning in self-supervised learning (SSL). Leveraging this, we introduce\na physics-informed SSL algorithm that initially pretrains a neural network on\nthe pretext task of learning a differentiable simulator of a physiological\nprocess. Subsequently, the model is trained to reconstruct physiological\nmeasurements from noninvasive modalities while being constrained by the\nphysical equations learned in pretraining. We apply our method to identify\ndigital twins of cardiac hemodynamics using noninvasive echocardiogram videos,\nand demonstrate its utility in unsupervised disease detection and in-silico\nclinical trials.\n"", '  Digital twins, the cornerstone of Industry 4.0, replicate real-world entities\nthrough computer models, revolutionising fields such as manufacturing\nmanagement and industrial automation. Recent advances in machine learning\nprovide data-driven methods for developing digital twins using discrete-time\ndata and finite-depth models on digital computers. However, this approach fails\nto capture the underlying continuous dynamics and struggles with modelling\ncomplex system behaviour. Additionally, the architecture of digital computers,\nwith separate storage and processing units, necessitates frequent data\ntransfers and Analogue-Digital (A/D) conversion, thereby significantly\nincreasing both time and energy costs. Here, we introduce a memristive neural\nordinary differential equation (ODE) solver for digital twins, which is capable\nof capturing continuous-time dynamics and facilitates the modelling of complex\nsystems using an infinite-depth model. By integrating storage and computation\nwithin analogue memristor arrays, we circumvent the von Neumann bottleneck,\nthus enhancing both speed and energy efficiency. We experimentally validate our\napproach by developing a digital twin of the HP memristor, which accurately\nextrapolates its nonlinear dynamics, achieving a 4.2-fold projected speedup and\na 41.4-fold projected decrease in energy consumption compared to\nstate-of-the-art digital hardware, while maintaining an acceptable error\nmargin. Additionally, we demonstrate scalability through experimentally\ngrounded simulations of Lorenz96 dynamics, exhibiting projected performance\nimprovements of 12.6-fold in speed and 189.7-fold in energy efficiency relative\nto traditional digital approaches. By harnessing the capabilities of fully\nanalogue computing, our breakthrough accelerates the development of digital\ntwins, offering an efficient and rapid solution to meet the demands of Industry\n4.0.\n', ""  A patient's digital twin is a computational model that describes the\nevolution of their health over time. Digital twins have the potential to\nrevolutionize medicine by enabling individual-level computer simulations of\nhuman health, which can be used to conduct more efficient clinical trials or to\nrecommend personalized treatment options. Due to the overwhelming complexity of\nhuman biology, machine learning approaches that leverage large datasets of\nhistorical patients' longitudinal health records to generate patients' digital\ntwins are more tractable than potential mechanistic models. In this manuscript,\nwe describe a neural network architecture that can learn conditional generative\nmodels of clinical trajectories, which we call Digital Twin Generators (DTGs),\nthat can create digital twins of individual patients. We show that the same\nneural network architecture can be trained to generate accurate digital twins\nfor patients across 13 different indications simply by changing the training\nset and tuning hyperparameters. By introducing a general purpose architecture,\nwe aim to unlock the ability to scale machine learning approaches to larger\ndatasets and across more indications so that a digital twin could be created\nfor any patient in the world.\n""]",Digital Twin Modeling and Simulation,Digital Twin Technology and Applications,"Advanced Technologies for Simulation, Sensing, and Fabrication","Advanced Technologies for Simulation, Sensing, and Fabrication"
474,13,474_twinning_twin_twinlab_twins,"['twinning', 'twin', 'twinlab', 'twins', 'modelization', 'modeling', 'neural', 'modelling', 'simulations', 'digital']","['digital', 'twins', 'twin', 'reduced', 'physical', 'twinning', 'noninvasive', 'fidelity', 'analogue', 'brain']","[""  A digital twin is a virtual replica of a real-world physical phenomena that\nuses mathematical modeling to characterize and simulate its defining features.\nBy constructing digital twins for disease processes, we can perform in-silico\nsimulations that mimic patients' health conditions and counterfactual outcomes\nunder hypothetical interventions in a virtual setting. This eliminates the need\nfor invasive procedures or uncertain treatment decisions. In this paper, we\npropose a method to identify digital twin model parameters using only\nnoninvasive patient health data. We approach the digital twin modeling as a\ncomposite inverse problem, and observe that its structure resembles pretraining\nand finetuning in self-supervised learning (SSL). Leveraging this, we introduce\na physics-informed SSL algorithm that initially pretrains a neural network on\nthe pretext task of learning a differentiable simulator of a physiological\nprocess. Subsequently, the model is trained to reconstruct physiological\nmeasurements from noninvasive modalities while being constrained by the\nphysical equations learned in pretraining. We apply our method to identify\ndigital twins of cardiac hemodynamics using noninvasive echocardiogram videos,\nand demonstrate its utility in unsupervised disease detection and in-silico\nclinical trials.\n"", '  Digital twins, the cornerstone of Industry 4.0, replicate real-world entities\nthrough computer models, revolutionising fields such as manufacturing\nmanagement and industrial automation. Recent advances in machine learning\nprovide data-driven methods for developing digital twins using discrete-time\ndata and finite-depth models on digital computers. However, this approach fails\nto capture the underlying continuous dynamics and struggles with modelling\ncomplex system behaviour. Additionally, the architecture of digital computers,\nwith separate storage and processing units, necessitates frequent data\ntransfers and Analogue-Digital (A/D) conversion, thereby significantly\nincreasing both time and energy costs. Here, we introduce a memristive neural\nordinary differential equation (ODE) solver for digital twins, which is capable\nof capturing continuous-time dynamics and facilitates the modelling of complex\nsystems using an infinite-depth model. By integrating storage and computation\nwithin analogue memristor arrays, we circumvent the von Neumann bottleneck,\nthus enhancing both speed and energy efficiency. We experimentally validate our\napproach by developing a digital twin of the HP memristor, which accurately\nextrapolates its nonlinear dynamics, achieving a 4.2-fold projected speedup and\na 41.4-fold projected decrease in energy consumption compared to\nstate-of-the-art digital hardware, while maintaining an acceptable error\nmargin. Additionally, we demonstrate scalability through experimentally\ngrounded simulations of Lorenz96 dynamics, exhibiting projected performance\nimprovements of 12.6-fold in speed and 189.7-fold in energy efficiency relative\nto traditional digital approaches. By harnessing the capabilities of fully\nanalogue computing, our breakthrough accelerates the development of digital\ntwins, offering an efficient and rapid solution to meet the demands of Industry\n4.0.\n', ""  A patient's digital twin is a computational model that describes the\nevolution of their health over time. Digital twins have the potential to\nrevolutionize medicine by enabling individual-level computer simulations of\nhuman health, which can be used to conduct more efficient clinical trials or to\nrecommend personalized treatment options. Due to the overwhelming complexity of\nhuman biology, machine learning approaches that leverage large datasets of\nhistorical patients' longitudinal health records to generate patients' digital\ntwins are more tractable than potential mechanistic models. In this manuscript,\nwe describe a neural network architecture that can learn conditional generative\nmodels of clinical trajectories, which we call Digital Twin Generators (DTGs),\nthat can create digital twins of individual patients. We show that the same\nneural network architecture can be trained to generate accurate digital twins\nfor patients across 13 different indications simply by changing the training\nset and tuning hyperparameters. By introducing a general purpose architecture,\nwe aim to unlock the ability to scale machine learning approaches to larger\ndatasets and across more indications so that a digital twin could be created\nfor any patient in the world.\n""]",Digital Twin Modeling and Simulation,Digital Twin Technology and Applications,"Advanced Technologies for Simulation, Sensing, and Fabrication","Advanced Technologies for Simulation, Sensing, and Fabrication"
475,13,475_recommender_recommenders_reinforcement_rewards,"['recommender', 'recommenders', 'reinforcement', 'rewards', 'reward', 'recommendation', 'planning', 'rl', 'offline', 'agent']","['recommendation', 'recommender', 'offline', 'reward', 'item', 'user', 'items', 'term', 'reinforcement', 'request']","[""  In recent years, there has been a growing interest in utilizing reinforcement\nlearning (RL) to optimize long-term rewards in recommender systems. Since\nindustrial recommender systems are typically designed as multi-stage systems,\nRL methods with a single agent face challenges when optimizing multiple stages\nsimultaneously. The reason is that different stages have different observation\nspaces, and thus cannot be modeled by a single agent. To address this issue, we\npropose a novel UNidirectional-EXecution-based multi-agent Reinforcement\nLearning (UNEX-RL) framework to reinforce the long-term rewards in multi-stage\nrecommender systems. We show that the unidirectional execution is a key feature\nof multi-stage recommender systems, bringing new challenges to the applications\nof multi-agent reinforcement learning (MARL), namely the observation dependency\nand the cascading effect. To tackle these challenges, we provide a cascading\ninformation chain (CIC) method to separate the independent observations from\naction-dependent observations and use CIC to train UNEX-RL effectively. We also\ndiscuss practical variance reduction techniques for UNEX-RL. Finally, we show\nthe effectiveness of UNEX-RL on both public datasets and an online recommender\nsystem with over 100 million users. Specifically, UNEX-RL reveals a 0.558%\nincrease in users' usage time compared with single-agent RL algorithms in\nonline A/B experiments, highlighting the effectiveness of UNEX-RL in industrial\nrecommender systems.\n"", ""  As the last critical stage of RSs, Multi-Task Fusion (MTF) is responsible for\ncombining multiple scores outputted by Multi-Task Learning (MTL) into a final\nscore to maximize user satisfaction, which determines the ultimate\nrecommendation results. Recently, to optimize long-term user satisfaction\nwithin a recommendation session, Reinforcement Learning (RL) is used for MTF in\nthe industry. However, the off-policy RL algorithms used for MTF so far have\nthe following severe problems: 1) to avoid out-of-distribution (OOD) problem,\ntheir constraints are overly strict, which seriously damage their performance;\n2) they are unaware of the exploration policy used for producing training data\nand never interact with real environment, so only suboptimal policy can be\nlearned; 3) the traditional exploration policies are inefficient and hurt user\nexperience. To solve the above problems, we propose a novel method named\nIntegratedRL-MTF customized for MTF in large-scale RSs. IntegratedRL-MTF\nintegrates off-policy RL model with our online exploration policy to relax\noverstrict and complicated constraints, which significantly improves its\nperformance. We also design an extremely efficient exploration policy, which\neliminates low-value exploration space and focuses on exploring potential\nhigh-value state-action pairs. Moreover, we adopt progressive training mode to\nfurther enhance our model's performance with the help of our exploration\npolicy. We conduct extensive offline and online experiments in the short video\nchannel of Tencent News. The results demonstrate that our model outperforms\nother models remarkably. IntegratedRL-MTF has been fully deployed in our RS and\nother large-scale RSs in Tencent, which have achieved significant improvements.\n"", ""  Sequential recommendation, where user preference is dynamically inferred from\nsequential historical behaviors, is a critical task in recommender systems\n(RSs). To further optimize long-term user engagement, offline\nreinforcement-learning-based RSs have become a mainstream technique as they\nprovide an additional advantage in avoiding global explorations that may harm\nonline users' experiences. However, previous studies mainly focus on discrete\naction and policy spaces, which might have difficulties in handling\ndramatically growing items efficiently.\n  To mitigate this issue, in this paper, we aim to design an algorithmic\nframework applicable to continuous policies. To facilitate the control in the\nlow-dimensional but dense user preference space, we propose an\n\\underline{\\textbf{E}}fficient \\underline{\\textbf{Co}}ntinuous\n\\underline{\\textbf{C}}ontrol framework (ECoC). Based on a statistically tested\nassumption, we first propose the novel unified action representation abstracted\nfrom normalized user and item spaces. Then, we develop the corresponding policy\nevaluation and policy improvement procedures. During this process, strategic\nexploration and directional control in terms of unified actions are carefully\ndesigned and crucial to final recommendation decisions. Moreover, beneficial\nfrom unified actions, the conservatism regularization for policies and value\nfunctions are combined and perfectly compatible with the continuous framework.\nThe resulting dual regularization ensures the successful offline training of\nRL-based recommendation policies. Finally, we conduct extensive experiments to\nvalidate the effectiveness of our framework. The results show that compared to\nthe discrete baselines, our ECoC is trained far more efficiently. Meanwhile,\nthe final policies outperform baselines in both capturing the offline data and\ngaining long-term rewards.\n""]",Reinforcement Learning in Recommender Systems,Reinforcement Learning Methods and Applications,Reinforcement Learning,Reinforcement Learning
475,13,475_recommender_recommenders_reinforcement_rewards,"['recommender', 'recommenders', 'reinforcement', 'rewards', 'reward', 'recommendation', 'planning', 'rl', 'offline', 'agent']","['recommendation', 'recommender', 'offline', 'reward', 'item', 'user', 'items', 'term', 'reinforcement', 'request']","[""  In recent years, there has been a growing interest in utilizing reinforcement\nlearning (RL) to optimize long-term rewards in recommender systems. Since\nindustrial recommender systems are typically designed as multi-stage systems,\nRL methods with a single agent face challenges when optimizing multiple stages\nsimultaneously. The reason is that different stages have different observation\nspaces, and thus cannot be modeled by a single agent. To address this issue, we\npropose a novel UNidirectional-EXecution-based multi-agent Reinforcement\nLearning (UNEX-RL) framework to reinforce the long-term rewards in multi-stage\nrecommender systems. We show that the unidirectional execution is a key feature\nof multi-stage recommender systems, bringing new challenges to the applications\nof multi-agent reinforcement learning (MARL), namely the observation dependency\nand the cascading effect. To tackle these challenges, we provide a cascading\ninformation chain (CIC) method to separate the independent observations from\naction-dependent observations and use CIC to train UNEX-RL effectively. We also\ndiscuss practical variance reduction techniques for UNEX-RL. Finally, we show\nthe effectiveness of UNEX-RL on both public datasets and an online recommender\nsystem with over 100 million users. Specifically, UNEX-RL reveals a 0.558%\nincrease in users' usage time compared with single-agent RL algorithms in\nonline A/B experiments, highlighting the effectiveness of UNEX-RL in industrial\nrecommender systems.\n"", ""  As the last critical stage of RSs, Multi-Task Fusion (MTF) is responsible for\ncombining multiple scores outputted by Multi-Task Learning (MTL) into a final\nscore to maximize user satisfaction, which determines the ultimate\nrecommendation results. Recently, to optimize long-term user satisfaction\nwithin a recommendation session, Reinforcement Learning (RL) is used for MTF in\nthe industry. However, the off-policy RL algorithms used for MTF so far have\nthe following severe problems: 1) to avoid out-of-distribution (OOD) problem,\ntheir constraints are overly strict, which seriously damage their performance;\n2) they are unaware of the exploration policy used for producing training data\nand never interact with real environment, so only suboptimal policy can be\nlearned; 3) the traditional exploration policies are inefficient and hurt user\nexperience. To solve the above problems, we propose a novel method named\nIntegratedRL-MTF customized for MTF in large-scale RSs. IntegratedRL-MTF\nintegrates off-policy RL model with our online exploration policy to relax\noverstrict and complicated constraints, which significantly improves its\nperformance. We also design an extremely efficient exploration policy, which\neliminates low-value exploration space and focuses on exploring potential\nhigh-value state-action pairs. Moreover, we adopt progressive training mode to\nfurther enhance our model's performance with the help of our exploration\npolicy. We conduct extensive offline and online experiments in the short video\nchannel of Tencent News. The results demonstrate that our model outperforms\nother models remarkably. IntegratedRL-MTF has been fully deployed in our RS and\nother large-scale RSs in Tencent, which have achieved significant improvements.\n"", ""  Sequential recommendation, where user preference is dynamically inferred from\nsequential historical behaviors, is a critical task in recommender systems\n(RSs). To further optimize long-term user engagement, offline\nreinforcement-learning-based RSs have become a mainstream technique as they\nprovide an additional advantage in avoiding global explorations that may harm\nonline users' experiences. However, previous studies mainly focus on discrete\naction and policy spaces, which might have difficulties in handling\ndramatically growing items efficiently.\n  To mitigate this issue, in this paper, we aim to design an algorithmic\nframework applicable to continuous policies. To facilitate the control in the\nlow-dimensional but dense user preference space, we propose an\n\\underline{\\textbf{E}}fficient \\underline{\\textbf{Co}}ntinuous\n\\underline{\\textbf{C}}ontrol framework (ECoC). Based on a statistically tested\nassumption, we first propose the novel unified action representation abstracted\nfrom normalized user and item spaces. Then, we develop the corresponding policy\nevaluation and policy improvement procedures. During this process, strategic\nexploration and directional control in terms of unified actions are carefully\ndesigned and crucial to final recommendation decisions. Moreover, beneficial\nfrom unified actions, the conservatism regularization for policies and value\nfunctions are combined and perfectly compatible with the continuous framework.\nThe resulting dual regularization ensures the successful offline training of\nRL-based recommendation policies. Finally, we conduct extensive experiments to\nvalidate the effectiveness of our framework. The results show that compared to\nthe discrete baselines, our ECoC is trained far more efficiently. Meanwhile,\nthe final policies outperform baselines in both capturing the offline data and\ngaining long-term rewards.\n""]",Reinforcement Learning in Recommender Systems,Reinforcement Learning Methods and Applications,Reinforcement Learning,Reinforcement Learning
476,12,476_confounders_confounder_recommender_recommending,"['confounders', 'confounder', 'recommender', 'recommending', 'bias', 'confounding', 'biases', 'recommendation', 'recommendations', 'causald']","['confounders', 'confounder', 'propensity', 'unmeasured', 'causal', 'recommendation', 'recommender', 'effect', 'item', 'bias']","['  Recommender models aim to capture user preferences from historical feedback\nand then predict user-specific feedback on candidate items. However, the\npresence of various unmeasured confounders causes deviations between the user\npreferences in the historical feedback and the true preferences, resulting in\nmodels not meeting their expected performance. Existing debias models either\n(1) specific to solving one particular bias or (2) directly obtain auxiliary\ninformation from user historical feedback, which cannot identify whether the\nlearned preferences are true user preferences or mixed with unmeasured\nconfounders. Moreover, we find that the former recommender system is not only a\nsuccessor to unmeasured confounders but also acts as an unmeasured confounder\naffecting user preference modeling, which has always been neglected in previous\nstudies. To this end, we incorporate the effect of the former recommender\nsystem and treat it as a proxy for all unmeasured confounders. We propose a\nnovel framework, Separating and Learning Latent Confounders For Recommendation\n(SLFR), which obtains the representation of unmeasured confounders to identify\nthe counterfactual feedback by disentangling user preferences and unmeasured\nconfounders, then guides the target model to capture the true preferences of\nusers. Extensive experiments in five real-world datasets validate the\nadvantages of our method.\n', ""  In recent years, dual-target Cross-Domain Recommendation (CDR) has been\nproposed to capture comprehensive user preferences in order to ultimately\nenhance the recommendation accuracy in both data-richer and data-sparser\ndomains simultaneously. However, in addition to users' true preferences, the\nuser-item interactions might also be affected by confounders (e.g., free\nshipping, sales promotion). As a result, dual-target CDR has to meet two\nchallenges: (1) how to effectively decouple observed confounders, including\nsingle-domain confounders and cross-domain confounders, and (2) how to preserve\nthe positive effects of observed confounders on predicted interactions, while\neliminating their negative effects on capturing comprehensive user preferences.\nTo address the above two challenges, we propose a Causal Deconfounding\nframework via Confounder Disentanglement for dual-target Cross-Domain\nRecommendation, called CD2CDR. In CD2CDR, we first propose a confounder\ndisentanglement module to effectively decouple observed single-domain and\ncross-domain confounders. We then propose a causal deconfounding module to\npreserve the positive effects of such observed confounders and eliminate their\nnegative effects via backdoor adjustment, thereby enhancing the recommendation\naccuracy in each domain. Extensive experiments conducted on five real-world\ndatasets demonstrate that CD2CDR significantly outperforms the state-of-the-art\nmethods.\n"", ""  Recommender systems suffer from confounding biases when there exist\nconfounders affecting both item features and user feedback (e.g., like or not).\nExisting causal recommendation methods typically assume confounders are fully\nobserved and measured, forgoing the possible existence of hidden confounders in\nreal applications. For instance, product quality is a confounder since\naffecting both item prices and user ratings, but is hidden for the third-party\ne-commerce platform due to the difficulty of large-scale quality inspection;\nignoring it could result in the bias effect of over-recommending high-price\nitems. This work analyzes and addresses the problem from a causal perspective.\nThe key lies in modeling the causal effect of item features on a user's\nfeedback. To mitigate hidden confounding effects, it is compulsory but\nchallenging to estimate the causal effect without measuring the confounder.\nTowards this goal, we propose a Hidden Confounder Removal (HCR) framework that\nleverages front-door adjustment to decompose the causal effect into two partial\neffects, according to the mediators between item features and user feedback.\nThe partial effects are independent from the hidden confounder and\nidentifiable. During training, HCR performs multi-task learning to infer the\npartial effects from historical interactions. We instantiate HCR for two\nscenarios and conduct experiments on three real-world datasets. Empirical\nresults show that the HCR framework provides more accurate recommendations,\nespecially for less-active users. We will release the code once accepted.\n""]",Deconfounding in Recommender Systems,Advances in Recommender Systems,Recommender Systems and Personalization,Recommender Systems and Personalization
477,12,477_uav_unmanned_uavs_swarm,"['uav', 'unmanned', 'uavs', 'swarm', 'swarms', 'autonomous', 'flying', 'planning', 'aircraft', 'aerial']","['swarms', 'unmanned', 'flight', 'jamming', 'vehicle', 'vehicles', 'aerial', 'swarm', 'vertiport', 'path']","['  With the impact of artificial intelligence on the traditional UAV industry,\nautonomous UAV flight has become a current hot research field. Based on the\ndemand for research on critical technologies for autonomous flying UAVs, this\npaper addresses the field of flight state recognition and trajectory prediction\nof UAVs. This paper proposes a method to improve the accuracy of UAV trajectory\nprediction based on UAV flight state recognition and verifies it using two\nprediction models. Firstly, UAV flight data acquisition and data preprocessing\nare carried out; secondly, UAV flight trajectory features are extracted based\non data fusion and a UAV flight state recognition model based on PCA-DAGSVM\nmodel is established; finally, two UAV flight trajectory prediction models are\nestablished and the trajectory prediction errors of the two prediction models\nare compared and analyzed after flight state recognition. The results show\nthat: 1) the UAV flight state recognition model based on PCA-DAGSVM has good\nrecognition effect. 2) compared with the traditional UAV trajectory prediction\nmodel, the prediction model based on flight state recognition can effectively\nreduce the prediction error.\n', '  As the demands for immediate and effective responses increase in both\ncivilian and military domains, the unmanned aerial vehicle (UAV) swarms emerge\nas effective solutions, in which multiple cooperative UAVs can work together to\nachieve specific goals. However, how to manage such complex systems to ensure\nreal-time adaptability lack sufficient researches. Hence, in this paper, we\npropose the cooperative cognitive dynamic system (CCDS), to optimize the\nmanagement for UAV swarms. CCDS leverages a hierarchical and cooperative\ncontrol structure that enables real-time data processing and decision.\nAccordingly, CCDS optimizes the UAV swarm management via dynamic\nreconfigurability and adaptive intelligent optimization. In addition, CCDS can\nbe integrated with the biomimetic mechanism to efficiently allocate tasks for\nUAV swarms. Further, the distributed coordination of CCDS ensures reliable and\nresilient control, thus enhancing the adaptability and robustness. Finally, the\npotential challenges and future directions are analyzed, to provide insights\ninto managing UAV swarms in dynamic heterogeneous networking.\n', ""  This paper addresses the increasing significance of UAVs (Unmanned Aerial\nVehicles) and the emergence of UAV swarms for collaborative operations in\nvarious domains. However, the effectiveness of UAV swarms can be severely\ncompromised by jamming technology, necessitating robust antijamming strategies.\nWhile existing methods such as frequency hopping and physical path planning\nhave been explored, there remains a gap in research on path planning for UAV\nswarms when the jammer's location is unknown. To address this, a novel\napproach, where UAV swarms leverage collective intelligence to predict jamming\nareas, evade them, and efficiently reach target destinations, is proposed. This\napproach utilizes Graph Convolutional Networks (GCN) to predict the location\nand intensity of jamming areas based on information gathered from each UAV. A\nmulti-agent control algorithm is then employed to disperse the UAV swarm, avoid\njamming, and regroup upon reaching the target. Through simulations, the\neffectiveness of the proposed method is demonstrated, showcasing accurate\nprediction of jamming areas and successful evasion through obstacle avoidance\nalgorithms, ultimately achieving the mission objective. Proposed method offers\nrobustness, scalability, and computational efficiency, making it applicable\nacross various scenarios where UAV swarms operate in potentially hostile\nenvironments.\n""]",Autonomous UAV Swarms and Trajectory Planning,Unmanned Aerial Vehicle (UAV) Systems and Technologies,Aerial Robotics and Agricultural Computer Vision,Aerial Robotics and Agricultural Computer Vision
478,12,478_prediction_inferences_predict_predictions,"['prediction', 'inferences', 'predict', 'predictions', 'inference', 'predicted', 'estimates', 'statistical', 'labeled', 'confidence']","['ale', 'statistical', 'inference', 'unobserved', 'intervals', 'prediction', 'valid', 'confidence', 'autoraters', 'covariates']","['  We present PPI++: a computationally lightweight methodology for estimation\nand inference based on a small labeled dataset and a typically much larger\ndataset of machine-learning predictions. The methods automatically adapt to the\nquality of available predictions, yielding easy-to-compute confidence sets --\nfor parameters of any dimensionality -- that always improve on classical\nintervals using only the labeled data. PPI++ builds on prediction-powered\ninference (PPI), which targets the same problem setting, improving its\ncomputational and statistical efficiency. Real and synthetic experiments\ndemonstrate the benefits of the proposed adaptations.\n', ""  Prediction-powered inference (PPI) is a method that improves statistical\nestimates based on limited human-labeled data. Specifically, PPI methods\nprovide tighter confidence intervals by combining small amounts of\nhuman-labeled data with larger amounts of data labeled by a reasonably\naccurate, but potentially biased, automatic system. We propose a framework for\nPPI based on Bayesian inference that allows researchers to develop new\ntask-appropriate PPI methods easily. Exploiting the ease with which we can\ndesign new metrics, we propose improved PPI methods for several importantcases,\nsuch as autoraters that give discrete responses (e.g., prompted LLM ``judges'')\nand autoraters with scores that have a non-linear relationship to human scores.\n"", '  Prediction-powered inference (PPI) is a method that improves statistical\nestimates based on limited human-labeled data. PPI achieves this by combining\nsmall amounts of human-labeled data with larger amounts of data labeled by a\nreasonably accurate -- but potentially biased -- automatic system, in a way\nthat results in tighter confidence intervals for certain parameters of interest\n(e.g., the mean performance of a language model). In this paper, we propose a\nmethod called Stratified Prediction-Powered Inference (StratPPI), in which we\nshow that the basic PPI estimates can be considerably improved by employing\nsimple data stratification strategies. Without making any assumptions on the\nunderlying automatic labeling system or data distribution, we derive an\nalgorithm for computing provably valid confidence intervals for population\nparameters (such as averages) that is based on stratified sampling. In\nparticular, we show both theoretically and empirically that, with appropriate\nchoices of stratification and sample allocation, our approach can provide\nsubstantially tighter confidence intervals than unstratified approaches.\nSpecifically, StratPPI is expected to improve in cases where the performance of\nthe autorater varies across different conditional distributions of the target\ndata.\n']",Prediction-Powered Statistical Inference,Machine Learning for Data Analysis and Modeling,Machine Learning and Artificial Intelligence,Machine Learning and Intelligent Systems
479,12,479_artistic_artworks_artwork_creativity,"['artistic', 'artworks', 'artwork', 'creativity', 'aesthetics', 'aesthetic', 'typography', 'wordart', 'calligraphy', 'aesthetically']","['artistic', 'calligraphy', 'typography', 'aesthetic', 'typographic', 'body', 'legibility', 'artists', 'artworks', 'aesthetics']","['  As a communication channel, body movements have been widely explored in\nbehavioral studies and kinesics. Performing and visual arts share the same\ninterests but focus on documenting and representing human body movements, such\nas for dance notation and visual work creation. This paper investigates body\nmovements in oriental calligraphy and how to apply calligraphy principles to\nstimulate and archive body movements. Through an artwork (Wushu), the authors\nexperiment with an interactive and generative approach to engage the audience\'s\nbodily participation and archive the body movements as a compendium of\ngenerated calligraphy. The audience assumes the role of both writers and\nreaders; creating (""writing"") and appreciating (""reading"") the generated\ncalligraphy becomes a cyclical process within this infinite ""Book,"" which can\nmotivate further attention and discussions concerning Chinese characters and\ncalligraphy.\n', '  This paper introduces the WordArt Designer API, a novel framework for\nuser-driven artistic typography synthesis utilizing Large Language Models\n(LLMs) on ModelScope. We address the challenge of simplifying artistic\ntypography for non-professionals by offering a dynamic, adaptive, and\ncomputationally efficient alternative to traditional rigid templates. Our\napproach leverages the power of LLMs to understand and interpret user input,\nfacilitating a more intuitive design process. We demonstrate through various\ncase studies how users can articulate their aesthetic preferences and\nfunctional requirements, which the system then translates into unique and\ncreative typographic designs. Our evaluations indicate significant improvements\nin user satisfaction, design flexibility, and creative expression over existing\nsystems. The WordArt Designer API not only democratizes the art of typography\nbut also opens up new possibilities for personalized digital communication and\ndesign.\n', ""  MetaDesigner revolutionizes artistic typography synthesis by leveraging the\nstrengths of Large Language Models (LLMs) to drive a design paradigm centered\naround user engagement. At the core of this framework lies a multi-agent system\ncomprising the Pipeline, Glyph, and Texture agents, which collectively enable\nthe creation of customized WordArt, ranging from semantic enhancements to the\nimposition of complex textures. MetaDesigner incorporates a comprehensive\nfeedback mechanism that harnesses insights from multimodal models and user\nevaluations to refine and enhance the design process iteratively. Through this\nfeedback loop, the system adeptly tunes hyperparameters to align with\nuser-defined stylistic and thematic preferences, generating WordArt that not\nonly meets but exceeds user expectations of visual appeal and contextual\nrelevance. Empirical validations highlight MetaDesigner's capability to\neffectively serve diverse WordArt applications, consistently producing\naesthetically appealing and context-sensitive results.\n""]",Artistic Typography and Calligraphy,Artistic and Creative AI Applications,Artificial Intelligence in Creative Industries,Artificial Intelligence in Creative Industries
480,12,480_tokamaks_tokamak_plasma_plasmas,"['tokamaks', 'tokamak', 'plasma', 'plasmas', 'fusion', 'simulations', 'modeling', 'magnetohydrodynamics', 'modelling', 'dynamics']","['plasma', 'tokamak', 'plasmas', 'kinetic', 'dynamics', 'tokamaks', 'evolution', 'fusion', 'surrogate', 'divertor']","['  We explore the possibility of fully replacing a plasma physics kinetic\nsimulator with a graph neural network-based simulator. We focus on this class\nof surrogate models given the similarity between their message-passing update\nmechanism and the traditional physics solver update, and the possibility of\nenforcing known physical priors into the graph construction and update. We show\nthat our model learns the kinetic plasma dynamics of the one-dimensional plasma\nmodel, a predecessor of contemporary kinetic plasma simulation codes, and\nrecovers a wide range of well-known kinetic plasma processes, including plasma\nthermalization, electrostatic fluctuations about thermal equilibrium, and the\ndrag on a fast sheet and Landau damping. We compare the performance against the\noriginal plasma model in terms of run-time, conservation laws, and temporal\nevolution of key physical quantities. The limitations of the model are\npresented and possible directions for higher-dimensional surrogate models for\nkinetic plasmas are discussed.\n', '  Reduced-order plasma models that can efficiently predict plasma behavior\nacross various settings and configurations are highly sought after yet elusive.\nThe demand for such models has surged in the past decade due to their potential\nto facilitate scientific research and expedite the development of plasma\ntechnologies. In line with the advancements in computational power and\ndata-driven methods, we introduce the ""Phi Method"" in this two-part article.\nPart I presents this novel algorithm, which employs constrained regression on a\ncandidate term library informed by numerical discretization schemes to discover\ndiscretized systems of differential equations. We demonstrate Phi Method\'s\nefficacy in deriving reliable and robust reduced-order models (ROMs) for three\ntest cases: the Lorenz attractor, flow past a cylinder, and a 1D\nHall-thruster-representative plasma. Part II will delve into the method\'s\napplication for parametric dynamics discovery. Our results show that ROMs\nderived from the Phi Method provide remarkably accurate predictions of systems\'\nbehavior, whether derived from steady-state or transient-state data. This\nunderscores the method\'s potential for transforming plasma system modeling.\n', '  Predicting plasma evolution within a Tokamak reactor is crucial to realizing\nthe goal of sustainable fusion. Capabilities in forecasting the spatio-temporal\nevolution of plasma rapidly and accurately allow us to quickly iterate over\ndesign and control strategies on current Tokamak devices and future reactors.\nModelling plasma evolution using numerical solvers is often expensive,\nconsuming many hours on supercomputers, and hence, we need alternative\ninexpensive surrogate models. We demonstrate accurate predictions of plasma\nevolution both in simulation and experimental domains using deep learning-based\nsurrogate modelling tools, viz., Fourier Neural Operators (FNO). We show that\nFNO has a speedup of six orders of magnitude over traditional solvers in\npredicting the plasma dynamics simulated from magnetohydrodynamic models, while\nmaintaining a high accuracy (MSE in the normalised domain $\\approx$ $10^{-5}$).\nOur modified version of the FNO is capable of solving multi-variable Partial\nDifferential Equations (PDE), and can capture the dependence among the\ndifferent variables in a single model. FNOs can also predict plasma evolution\non real-world experimental data observed by the cameras positioned within the\nMAST Tokamak, i.e., cameras looking across the central solenoid and the\ndivertor in the Tokamak. We show that FNOs are able to accurately forecast the\nevolution of plasma and have the potential to be deployed for real-time\nmonitoring. We also illustrate their capability in forecasting the plasma\nshape, the locations of interactions of the plasma with the central solenoid\nand the divertor for the full (available) duration of the plasma shot within\nMAST. The FNO offers a viable alternative for surrogate modelling as it is\nquick to train and infer, and requires fewer data points, while being able to\ndo zero-shot super-resolution and getting high-fidelity solutions.\n']",Plasma Simulation and Modeling in Tokamaks,Fluid Dynamics and Plasma Simulation,Fluid and Plasma Simulation and Modeling,Fluid and Plasma Simulation and Modeling
481,12,481_pca_eigenvector_eigenspace_eigen,"['pca', 'eigenvector', 'eigenspace', 'eigen', 'cpca', 'sparse', 'algorithms', 'spectral', 'matrix', 'eigenvalue']","['principal', 'deflation', 'subspace', 'matrix', 'loadings', 'component', 'spiked', 'approximation', 'singular', 'eigenvector']","[""  In this paper we analyze the behavior of the Oja's algorithm for\nonline/streaming principal component subspace estimation. It is proved that\nwith high probability it performs an efficient, gap-free, global convergence\nrate to approximate an principal component subspace for any sub-Gaussian\ndistribution. Moreover, it is the first time to show that the convergence rate,\nnamely the upper bound of the approximation, exactly matches the lower bound of\nan approximation obtained by the offline/classical PCA up to a constant factor.\n"", '  The $k$-principal component analysis ($k$-PCA) problem is a fundamental\nalgorithmic primitive that is widely-used in data analysis and dimensionality\nreduction applications. In statistical settings, the goal of $k$-PCA is to\nidentify a top eigenspace of the covariance matrix of a distribution, which we\nonly have black-box access to via samples. Motivated by these settings, we\nanalyze black-box deflation methods as a framework for designing $k$-PCA\nalgorithms, where we model access to the unknown target matrix via a black-box\n$1$-PCA oracle which returns an approximate top eigenvector, under two popular\nnotions of approximation. Despite being arguably the most natural\nreduction-based approach to $k$-PCA algorithm design, such black-box methods,\nwhich recursively call a $1$-PCA oracle $k$ times, were previously\npoorly-understood.\n  Our main contribution is significantly sharper bounds on the approximation\nparameter degradation of deflation methods for $k$-PCA. For a quadratic form\nnotion of approximation we term ePCA (energy PCA), we show deflation methods\nsuffer no parameter loss. For an alternative well-studied approximation notion\nwe term cPCA (correlation PCA), we tightly characterize the parameter regimes\nwhere deflation methods are feasible. Moreover, we show that in all feasible\nregimes, $k$-cPCA deflation algorithms suffer no asymptotic parameter loss for\nany constant $k$. We apply our framework to obtain state-of-the-art $k$-PCA\nalgorithms robust to dataset contamination, improving prior work in sample\ncomplexity by a $\\mathsf{poly}(k)$ factor.\n', ""  Oja's algorithm for streaming Principal Component Analysis (PCA) for $n$\ndatapoints in a $d$ dimensional space achieves the same sin-squared error\n$O(r_\\mathsf{eff}/n)$ as the offline algorithm in $O(d)$ space and $O(nd)$ time\nand a single pass through the datapoints. Here $r_\\mathsf{eff}$ is the\neffective rank (ratio of the trace and the principal eigenvalue of the\npopulation covariance matrix $\\Sigma$). Under this computational budget, we\nconsider the problem of sparse PCA, where the principal eigenvector of $\\Sigma$\nis $s$-sparse, and $r_\\mathsf{eff}$ can be large. In this setting, to our\nknowledge, \\textit{there are no known single-pass algorithms} that achieve the\nminimax error bound in $O(d)$ space and $O(nd)$ time without either requiring\nstrong initialization conditions or assuming further structure (e.g., spiked)\nof the covariance matrix. We show that a simple single-pass procedure that\nthresholds the output of Oja's algorithm (the Oja vector) can achieve the\nminimax error bound under some regularity conditions in $O(d)$ space and\n$O(nd)$ time as long as $r_\\mathsf{eff}=O(n/\\log n)$. We present a nontrivial\nand novel analysis of the entries of the unnormalized Oja vector, which\ninvolves the projection of a product of independent random matrices on a random\ninitial vector. This is completely different from previous analyses of Oja's\nalgorithm and matrix products, which have been done when the $r_\\mathsf{eff}$\nis bounded.\n""]",Principal Component Analysis Algorithms,Dimensionality Reduction and Data Visualization Techniques,Data Analysis and Visualization,Data Analysis and Visualization
482,12,482_translations_decoding_translation_mbr,"['translations', 'decoding', 'translation', 'mbr', 'mbrs', 'bayes', 'mbmbr', 'text', 'cbmbr', 'risk']","['decoding', 'translation', 'translations', 'hypotheses', 'utility', 'mbrs', 'risk', 'quality', 'reranking', 'probability']","['  Minimum Bayes Risk (MBR) decoding has been shown to be a powerful alternative\nto beam search decoding in a variety of text generation tasks. MBR decoding\nselects a hypothesis from a pool of hypotheses that has the least expected risk\nunder a probability model according to a given utility function. Since it is\nimpractical to compute the expected risk exactly over all possible hypotheses,\ntwo approximations are commonly used in MBR. First, it integrates over a\nsampled set of hypotheses rather than over all possible hypotheses. Second, it\nestimates the probability of each hypothesis using a Monte Carlo estimator.\nWhile the first approximation is necessary to make it computationally feasible,\nthe second is not essential since we typically have access to the model\nprobability at inference time. We propose Model-Based MBR (MBMBR), a variant of\nMBR that uses the model probability itself as the estimate of the probability\ndistribution instead of the Monte Carlo estimate. We show analytically and\nempirically that the model-based estimate is more promising than the Monte\nCarlo estimate in text generation tasks. Our experiments show that MBMBR\noutperforms MBR in several text generation tasks, both with encoder-decoder\nmodels and with large language models.\n', '  This paper explores Minimum Bayes Risk (MBR) decoding for self-improvement in\nmachine translation (MT), particularly for domain adaptation and low-resource\nlanguages. We implement the self-improvement process by fine-tuning the model\non its MBR-decoded forward translations. By employing COMET as the MBR utility\nmetric, we aim to achieve the reranking of translations that better aligns with\nhuman preferences. The paper explores the iterative application of this\napproach and the potential need for language-specific MBR utility metrics. The\nresults demonstrate significant enhancements in translation quality for all\nexamined language pairs, including successful application to domain-adapted\nmodels and generalisation to low-resource settings. This highlights the\npotential of COMET-guided MBR for efficient MT self-improvement in various\nscenarios.\n', '  Minimum Bayes Risk (MBR) decoding is a powerful decoding strategy widely used\nfor text generation tasks, but its quadratic computational complexity limits\nits practical application. This paper presents a novel approach for\napproximating MBR decoding using matrix completion techniques, focusing on the\ntask of machine translation. We formulate MBR decoding as a matrix completion\nproblem, where the utility metric scores between candidate hypotheses and\npseudo-reference translations form a low-rank matrix. First, we empirically\nshow that the scores matrices indeed have a low-rank structure. Then, we\nexploit this by only computing a random subset of the scores and efficiently\nrecover the missing entries in the matrix by applying the Alternating Least\nSquares (ALS) algorithm, thereby enabling a fast approximation of the MBR\ndecoding process. Our experimental results on machine translation tasks\ndemonstrate that the proposed method requires 1/16 utility metric computations\ncompared to vanilla MBR decoding while achieving equal translation quality\nmeasured by COMET22 on the WMT22 dataset (en<>de and en<>ru). We also benchmark\nour method against other approximation methods and we show gains in quality\nwhen comparing to them.\n']",Minimum Bayes Risk Decoding for Text Generation,Natural Language Processing for Text Generation and Evaluation,Natural Language Processing,Natural Language Processing
483,12,483_gerrymandering_redistricting_gerrymanderer_candidates,"['gerrymandering', 'redistricting', 'gerrymanderer', 'candidates', 'elections', 'voting', 'candidate', 'electoral', 'ensemble', 'voters']","['voting', 'redistricting', 'ordinal', 'candidates', 'gerrymanderer', 'voters', 'planar', 'gerrymandering', 'partisan', 'manipulable']","['  By classic results in social choice theory, any reasonable preferential\nvoting method sometimes gives individuals an incentive to report an insincere\npreference. The extent to which different voting methods are more or less\nresistant to such strategic manipulation has become a key consideration for\ncomparing voting methods. Here we measure resistance to manipulation by whether\nneural networks of varying sizes can learn to profitably manipulate a given\nvoting method in expectation, given different types of limited information\nabout how other voters will vote. We trained over 70,000 neural networks of 26\nsizes to manipulate against 8 different voting methods, under 6 types of\nlimited information, in committee-sized elections with 5-21 voters and 3-6\ncandidates. We find that some voting methods, such as Borda, are highly\nmanipulable by networks with limited information, while others, such as Instant\nRunoff, are not, despite being quite profitably manipulated by an ideal\nmanipulator with full information. For the two probability models for elections\nthat we use, the overall least manipulable of the 8 methods we study are\nCondorcet methods, namely Minimax and Split Cycle.\n', '  We study the computational complexity of the map redistricting problem\n(gerrymandering). Mathematically, the electoral district designer\n(gerrymanderer) attempts to partition a weighted graph into $k$ connected\ncomponents (districts) such that its candidate (party) wins as many districts\nas possible. Prior work has principally concerned the special cases where the\ngraph is a path or a tree. Our focus concerns the realistic case where the\ngraph is planar. We prove that the gerrymandering problem is solvable in\npolynomial time in $\\lambda$-outerplanar graphs, when the number of candidates\nand $\\lambda$ are constants and the vertex weights (voting weights) are\npolynomially bounded. In contrast, the problem is NP-complete in general planar\ngraphs even with just two candidates. This motivates the study of approximation\nalgorithms for gerrymandering planar graphs. However, when the number of\ncandidates is large, we prove it is hard to distinguish between instances where\nthe gerrymanderer cannot win a single district and instances where the\ngerrymanderer can win at least one district. This immediately implies that the\nredistricting problem is inapproximable in polynomial time in planar graphs,\nunless P=NP. This conclusion appears terminal for the design of good\napproximation algorithms -- but it is not. The inapproximability bound can be\ncircumvented as it only applies when the maximum number of districts the\ngerrymanderer can win is extremely small, say one. Indeed, for a fixed number\nof candidates, our main result is that there is a constant factor approximation\nalgorithm for redistricting unweighted planar graphs, provided the optimal\nvalue is a large enough constant.\n', ""  Role mining is a technique used to derive a role-based authorization policy\nfrom an existing policy. Given a set of users $U$, a set of permissions $P$ and\na user-permission authorization relation $\\mahtit{UPA}\\subseteq U\\times P$, a\nrole mining algorithm seeks to compute a set of roles $R$, a user-role\nauthorization relation $\\mathit{UA}\\subseteq U\\times R$ and a permission-role\nauthorization relation $\\mathit{PA}\\subseteq R\\times P$, such that the\ncomposition of $\\mathit{UA}$ and $\\mathit{PA}$ is close (in some appropriate\nsense) to $\\mathit{UPA}$.\n  In this paper, we first introduce the Generalized Noise Role Mining problem\n(GNRM) -- a generalization of the MinNoise Role Mining problem -- which we\nbelieve has considerable practical relevance. Extending work of Fomin et al.,\nwe show that GNRM is fixed parameter tractable, with parameter $r + k$, where\n$r$ is the number of roles in the solution and $k$ is the number of\ndiscrepancies between $\\mathit{UPA}$ and the relation defined by the\ncomposition of $\\mathit{UA}$ and $\\mathit{PA}$. We further introduce a\nbi-objective optimization variant of GNRM, where we wish to minimize both $r$\nand $k$ subject to upper bounds $r\\le \\bar{r}$ and $k\\le \\bar{k}$, where\n$\\bar{r}$ and $\\bar{k}$ are constants. We show that the Pareto front of this\nbi-objective optimization problem (BO-GNRM) can be computed in fixed-parameter\ntractable time with parameter $\\bar{r}+\\bar{k}$.\n  We then report the results of our experimental work using the integer\nprogramming solver Gurobi to solve instances of BO-GNRM. Our key findings are\nthat (a) we obtained strong support that Gurobi's performance is\nfixed-parameter tractable, (b) our results suggest that our techniques may be\nuseful for role mining in practice, based on our experiments in the context of\nthree well-known real-world authorization policies.\n""]",Voting Systems and Electoral Manipulation,Mechanisms and Strategies in Auctions and Voting Systems,Optimization and Decision Making in Complex Systems,Complex System Optimization and Management
484,12,484_imputations_imputation__imputation_missingness,"['imputations', 'imputation', '_imputation', 'missingness', 'imputed', 'imputing', 'forecasting', 'forecasts', 'impute', 'incomplete']","['imputation', 'missing', 'series', 'hinge', 'imputed', 'values', 'multivariate', 'time', 'forecasting', 'impute']","['  Time series imputation is one of the most fundamental tasks for time series.\nReal-world time series datasets are frequently incomplete (or irregular with\nmissing observations), in which case imputation is strongly required. Many\ndifferent time series imputation methods have been proposed. Recent\nself-attention-based methods show the state-of-the-art imputation performance.\nHowever, it has been overlooked for a long time to design an imputation method\nbased on continuous-time recurrent neural networks (RNNs), i.e., neural\ncontrolled differential equations (NCDEs). To this end, we redesign time series\n(variational) autoencoders based on NCDEs. Our method, called continuous-time\nautoencoder (CTA), encodes an input time series sample into a continuous hidden\npath (rather than a hidden vector) and decodes it to reconstruct and impute the\ninput. In our experiments with 4 datasets and 19 baselines, our method shows\nthe best imputation performance in almost all cases.\n', ""  Time series imputation plays a crucial role in various real-world systems and\nhas been extensively explored. Models for time series imputation often require\nspecialization, necessitating distinct designs for different domains and\nmissing patterns. In this study, we introduce NuwaTS, a framework to repurpose\nPre-trained Language Model (PLM) for general time series imputation. Once\ntrained, this model can be applied to imputation tasks on incomplete time\nseries from any domain with any missing patterns. We begin by devising specific\nembeddings for each sub-series patch of the incomplete time series. These\nembeddings encapsulate information about the patch itself, the missing data\npatterns within the patch, and the patch's statistical characteristics. To\nenhance the model's adaptability to different missing patterns, we propose a\ncontrastive learning approach to make representations of the same patch more\nsimilar across different missing patterns. By combining this contrastive loss\nwith the missing data imputation task, we train PLMs to obtain a one-for-all\nimputation model. Furthermore, we utilize a plug-and-play layer-wise\nfine-tuning approach to train domain-specific models. Experimental results\ndemonstrate that leveraging a dataset of over seventeen million time series\nfrom diverse domains, we obtain a one-for-all imputation model which\noutperforms existing domain-specific models across various datasets and missing\npatterns. Additionally, we find that NuwaTS can be generalized to other time\nseries tasks such as forecasting. Our codes are available at\nhttps://github.com/Chengyui/NuwaTS.\n"", '  Time series classification with missing data is a prevalent issue in time\nseries analysis, as temporal data often contain missing values in practical\napplications. The traditional two-stage approach, which handles imputation and\nclassification separately, can result in sub-optimal performance as label\ninformation is not utilized in the imputation process. On the other hand, a\none-stage approach can learn features under missing information, but feature\nrepresentation is limited as imputed errors are propagated in the\nclassification process. To overcome these challenges, this study proposes an\nend-to-end neural network that unifies data imputation and representation\nlearning within a single framework, allowing the imputation process to take\nadvantage of label information. Differing from previous methods, our approach\nplaces less emphasis on the accuracy of imputation data and instead prioritizes\nclassification performance. A specifically designed multi-scale feature\nlearning module is implemented to extract useful information from the\nnoise-imputation data. The proposed model is evaluated on 68 univariate time\nseries datasets from the UCR archive, as well as a multivariate time series\ndataset with various missing data ratios and 4 real-world datasets with missing\ninformation. The results indicate that the proposed model outperforms\nstate-of-the-art approaches for incomplete time series classification,\nparticularly in scenarios with high levels of missing data.\n']",Time Series Imputation and Analysis,Data Imputation and Missing Value Analysis,Handling Missing or Inconsistent Data,Handling Missing or Inconsistent Data
485,12,485_gps_accelerometer_gyroscope_sensor,"['gps', 'accelerometer', 'gyroscope', 'sensor', 'sensors', 'tracking', 'inertial', 'magnetometer', 'magnetometers', 'satellite']","['inertial', 'magnetic', 'navigation', 'heading', 'angle', 'gyroscope', 'aircraft', 'tracking', 'accelerometer', 'sensor']","['  Many Internet of Things applications utilize low-cost, micro,\nelectro-mechanical inertial sensors. A common task is orientation estimation.\nTo tackle such a task, attitude and heading reference system algorithms are\napplied. Relying on the gyroscope readings, the accelerometer readings are used\nto update the attitude angles, and magnetometer measurements are utilized to\nupdate the heading angle. In indoor environments, magnetometers suffer from\ninterference that degrades their performance. This mainly influences\napplications focused on estimating the heading angle like finding the heading\nangle of a closet or fridge door. To circumvent such situations, we propose\nDoorINet, an end-to-end deep-learning framework to calculate the heading angle\nfrom door-mounted, low-cost inertial sensors without using magnetometers. To\nevaluate our approach, we record a unique dataset containing 391 minutes of\naccelerometer and gyroscope measurements and corresponding ground-truth heading\nangle. We show that our proposed approach outperforms commonly used, model\nbased approaches and data-driven methods.\n', '  Accurate alignment of a fixed mobile device equipped with inertial sensors\ninside a moving vehicle is important for navigation, activity recognition, and\nother applications. Accurate estimation of the device mounting angle is\nrequired to rotate the inertial measurement from the sensor frame to the moving\nplatform frame to standardize measurements and improve the performance of the\ntarget task. In this work, a data-driven approach using deep neural networks\n(DNNs) is proposed to learn the yaw mounting angle of a smartphone equipped\nwith an inertial measurement unit (IMU) and strapped to a car. The proposed\nmodel uses only the accelerometer and gyroscope readings from an IMU as input\nand, in contrast to existing solutions, does not require global position inputs\nfrom global navigation satellite systems (GNSS). To train the model in a\nsupervised manner, IMU data is collected for training and validation with the\nsensor mounted at a known yaw mounting angle, and a range of ground truth\nlabels is generated by applying a random rotation in a bounded range to the\nmeasurements. The trained model is tested on data with real rotations showing\nsimilar performance as with synthetic rotations. The trained model is deployed\non an Android device and evaluated in real-time to test the accuracy of the\nestimated yaw mounting angle. The model is shown to find the mounting angle at\nan accuracy of 8 degrees within 5 seconds, and 4 degrees within 27 seconds. An\nexperiment is conducted to compare the proposed model with an existing\noff-the-shelf solution.\n', ""  In this paper, we validate the performance of the a sensor fusion-based\nGlobal Navigation Satellite System (GNSS) spoofing attack detection framework\nfor Autonomous Vehicles (AVs). To collect data, a vehicle equipped with a GNSS\nreceiver, along with Inertial Measurement Unit (IMU) is used. The detection\nframework incorporates two strategies: The first strategy involves comparing\nthe predicted location shift, which is the distance traveled between two\nconsecutive timestamps, with the inertial sensor-based location shift. For this\npurpose, data from low-cost in-vehicle inertial sensors such as the\naccelerometer and gyroscope sensor are fused and fed into a long short-term\nmemory (LSTM) neural network. The second strategy employs a Random-Forest\nsupervised machine learning model to detect and classify turns, distinguishing\nbetween left and right turns using the output from the steering angle sensor.\nIn experiments, two types of spoofing attack models: turn-by-turn and wrong\nturn are simulated. These spoofing attacks are modeled as SQL injection\nattacks, where, upon successful implementation, the navigation system perceives\ninjected spoofed location information as legitimate while being unable to\ndetect legitimate GNSS signals. Importantly, the IMU data remains uncompromised\nthroughout the spoofing attack. To test the effectiveness of the detection\nframework, experiments are conducted in Tuscaloosa, AL, mimicking urban road\nstructures. The results demonstrate the framework's ability to detect various\nsophisticated GNSS spoofing attacks, even including slow position drifting\nattacks. Overall, the experimental results showcase the robustness and efficacy\nof the sensor fusion-based spoofing attack detection approach in safeguarding\nAVs against GNSS spoofing threats.\n""]",Inertial Sensor-Based Navigation and Tracking,Indoor Navigation and Tracking Technologies,Wireless Technologies and Sensing Systems,Wireless Technologies and Sensing Systems
486,12,486_poetry2image_poetry_poems_poets,"['poetry2image', 'poetry', 'poems', 'poets', 'rhyme', 'rhyming', 'poem', 'syllables', 'literary', 'poetic']","['poetry', 'poems', 'poetic', 'poem', 'rhyme', 'lyric', 'luc', 'meter', 'translation', 'genres']","[""  Large language models (LLMs) can now generate and recognize text in a wide\nrange of styles and genres, including highly specialized, creative genres like\npoetry. But what do LLMs really know about poetry? What can they know about\npoetry? We develop a task to evaluate how well LLMs recognize a specific aspect\nof poetry, poetic form, for more than 20 forms and formal elements in the\nEnglish language. Poetic form captures many different poetic features,\nincluding rhyme scheme, meter, and word or line repetition. We use this task to\nreflect on LLMs' current poetic capabilities, as well as the challenges and\npitfalls of creating NLP benchmarks for poetry and for other creative tasks. In\nparticular, we use this task to audit and reflect on the poems included in\npopular pretraining datasets. Our findings have implications for NLP\nresearchers interested in model evaluation, digital humanities and cultural\nanalytics scholars, and cultural heritage professionals.\n"", '  Text-to-image generation models often struggle with key element loss or\nsemantic confusion in tasks involving Chinese classical poetry.Addressing this\nissue through fine-tuning models needs considerable training costs.\nAdditionally, manual prompts for re-diffusion adjustments need professional\nknowledge. To solve this problem, we propose Poetry2Image, an iterative\ncorrection framework for images generated from Chinese classical poetry.\nUtilizing an external poetry dataset, Poetry2Image establishes an automated\nfeedback and correction loop, which enhances the alignment between poetry and\nimage through image generation models and subsequent re-diffusion modifications\nsuggested by large language models (LLM). Using a test set of 200 sentences of\nChinese classical poetry, the proposed method--when integrated with five\npopular image generation models--achieves an average element completeness of\n70.63%, representing an improvement of 25.56% over direct image generation. In\ntests of semantic correctness, our method attains an average semantic\nconsistency of 80.09%. The study not only promotes the dissemination of ancient\npoetry culture but also offers a reference for similar non-fine-tuning methods\nto enhance LLM generation.\n', '  Natural Language Generation (NLG), and more generally generative AI, are\namong the currently most impactful research fields. Creative NLG, such as\nautomatic poetry generation, is a fascinating niche in this area. While most\nprevious research has focused on forms of the Turing test when evaluating\nautomatic poetry generation - can humans distinguish between automatic and\nhuman generated poetry - we evaluate the diversity of automatically generated\npoetry, by comparing distributions of generated poetry to distributions of\nhuman poetry along structural, lexical, semantic and stylistic dimensions,\nassessing different model types (word vs. character-level, general purpose LLMs\nvs. poetry-specific models), including the very recent LLaMA3, and types of\nfine-tuning (conditioned vs. unconditioned). We find that current automatic\npoetry systems are considerably underdiverse along multiple dimensions - they\noften do not rhyme sufficiently, are semantically too uniform and even do not\nmatch the length distribution of human poetry. Our experiments reveal, however,\nthat style-conditioning and character-level modeling clearly increases\ndiversity across virtually all dimensions we explore. Our identified\nlimitations may serve as the basis for more genuinely diverse future poetry\ngeneration models.\n']",Poetry Generation and Analysis,Artistic and Creative AI Applications,Artificial Intelligence in Creative Industries,Artificial Intelligence in Creative Industries
487,12,487_coreference_annotation_ontonotes_texts,"['coreference', 'annotation', 'ontonotes', 'texts', 'corpora', 'translations', 'multilingual', 'nlp', 'referring', 'mentions']","['coreference', 'resolution', 'mentions', 'mention', 'literary', 'singleton', 'entity', 'discourse', 'rhetorical', 'singletons']","[""  In this paper, we present KoCoNovel, a novel character coreference dataset\nderived from Korean literary texts, complete with detailed annotation\nguidelines. Comprising 178K tokens from 50 modern and contemporary novels,\nKoCoNovel stands as one of the largest public coreference resolution corpora in\nKorean, and the first to be based on literary texts. KoCoNovel offers four\ndistinct versions to accommodate a wide range of literary coreference analysis\nneeds. These versions are designed to support perspectives of the omniscient\nauthor or readers, and to manage multiple entities as either separate or\noverlapping, thereby broadening its applicability. One of KoCoNovel's\ndistinctive features is that 24% of all character mentions are single common\nnouns, lacking possessive markers or articles. This feature is particularly\ninfluenced by the nuances of Korean address term culture, which favors the use\nof terms denoting social relationships and kinship over personal names. In\nexperiments with a BERT-based coreference model, we observe notable performance\nenhancements with KoCoNovel in character coreference tasks within literary\ntexts, compared to a larger non-literary coreference dataset. Such findings\nunderscore KoCoNovel's potential to significantly enhance coreference\nresolution models through the integration of Korean cultural and linguistic\ndynamics.\n"", '  Coreference resolution involves the task of identifying text spans within a\ndiscourse that pertain to the same real-world entity. While this task has been\nextensively explored in the English language, there has been a notable scarcity\nof publicly accessible resources and models for coreference resolution in South\nAsian languages. We introduce a Translated dataset for Multilingual Coreference\nResolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools\nfor translation and word-alignment. Nearly all of the predicted translations\nsuccessfully pass a sanity check, and 75% of English references align with\ntheir predicted translations. Using multilingual encoders, two off-the-shelf\ncoreference resolution models were trained on a concatenation of TransMuCoRes\nand a Hindi coreference resolution dataset with manual annotations. The best\nperforming model achieved a score of 64 and 68 for LEA F1 and CoNLL F1,\nrespectively, on our test-split of Hindi golden set. This study is the first to\nevaluate an end-to-end coreference resolution model on a Hindi golden set.\nFurthermore, this work underscores the limitations of current coreference\nevaluation metrics when applied to datasets with split antecedents, advocating\nfor the development of more suitable evaluation metrics.\n', '  Coreference resolution is the task of identifying and grouping mentions\nreferring to the same real-world entity. Previous neural models have mainly\nfocused on learning span representations and pairwise scores for coreference\ndecisions. However, current methods do not explicitly capture the referential\nchoice in the hierarchical discourse, an important factor in coreference\nresolution. In this study, we propose a new approach that incorporates\nrhetorical information into neural coreference resolution models. We collect\nrhetorical features from automated discourse parses and examine their impact.\nAs a base model, we implement an end-to-end span-based coreference resolver\nusing a partially fine-tuned multilingual entity-aware language model LUKE. We\nevaluate our method on the RuCoCo-23 Shared Task for coreference resolution in\nRussian. Our best model employing rhetorical distance between mentions has\nranked 1st on the development set (74.6% F1) and 2nd on the test set (73.3% F1)\nof the Shared Task. We hope that our work will inspire further research on\nincorporating discourse information in neural coreference resolution models.\n']",Coreference Resolution in Multilingual Texts,Multilingual Natural Language Processing,Natural Language Processing,Natural Language Processing
488,12,488_ai_automation_software_developers,"['ai', 'automation', 'software', 'developers', 'programmers', 'development', 'architectural', 'automl', 'engineering', 'prototyping']","['software', 'development', 'prototyping', 'engineering', 'programmers', 'requirements', 'copilots', 'conceptual', 'phases', 'partnership']","['  A paradigm shift is underway in Software Engineering, with AI systems such as\nLLMs gaining increasing importance for improving software development\nproductivity. This trend is anticipated to persist. In the next five years, we\nwill likely see an increasing symbiotic partnership between human developers\nand AI. The Software Engineering research community cannot afford to overlook\nthis trend; we must address the key research challenges posed by the\nintegration of AI into the software development process. In this paper, we\npresent our vision of the future of software development in an AI-Driven world\nand explore the key challenges that our research community should address to\nrealize this vision.\n', ""  Background:Technical systems are growing in complexity with more components\nand functions across various disciplines. Model-Driven Engineering (MDE) helps\nmanage this complexity by using models as key artifacts. Domain-Specific\nLanguages (DSL) supported by MDE facilitate modeling. As data generation in\nproduct development increases, there's a growing demand for AI algorithms,\nwhich can be challenging to implement. Integrating AI algorithms with DSL and\nMDE can streamline this process. Objective:This study aims to investigate the\nexisting model-driven approaches relying on DSL in support of the engineering\nof AI software systems to sharpen future research further and define the\ncurrent state of the art. Method:We conducted a Systemic Literature Review\n(SLR), collecting papers from five major databases resulting in 1335 candidate\nstudies, eventually retaining 18 primary studies. Each primary study will be\nevaluated and discussed with respect to the adoption of MDE principles and\npractices and the phases of AI development support aligned with the stages of\nthe CRISP-DM methodology. Results:The study's findings show that language\nworkbenches are of paramount importance in dealing with all aspects of modeling\nlanguage development and are leveraged to define DSL explicitly addressing AI\nconcerns. The most prominent AI-related concerns are training and modeling of\nthe AI algorithm, while minor emphasis is given to the time-consuming\npreparation of the data. Early project phases that support interdisciplinary\ncommunication of requirements, e.g., CRISP-DM Business Understanding phase, are\nrarely reflected. Conclusion:The study found that the use of MDE for AI is\nstill in its early stages, and there is no single tool or method that is widely\nused. Additionally, current approaches tend to focus on specific stages of\ndevelopment rather than providing support for the entire development process.\n"", '  Across the dynamic business landscape today, enterprises face an\never-increasing range of challenges. These include the constantly evolving\nregulatory environment, the growing demand for personalization within software\napplications, and the heightened emphasis on governance. In response to these\nmultifaceted demands, large enterprises have been adopting automation that\nspans from the optimization of core business processes to the enhancement of\ncustomer experiences. Indeed, Artificial Intelligence (AI) has emerged as a\npivotal element of modern software systems. In this context, data plays an\nindispensable role. AI-centric software systems based on supervised learning\nand operating at an industrial scale require large volumes of training data to\nperform effectively. Moreover, the incorporation of generative AI has led to a\ngrowing demand for adequate evaluation benchmarks. Our experience in this field\nhas revealed that the requirement for large datasets for training and\nevaluation introduces a host of intricate challenges. This book chapter\nexplores the evolving landscape of Software Engineering (SE) in general, and\nRequirements Engineering (RE) in particular, in this era marked by AI\nintegration. We discuss challenges that arise while integrating Natural\nLanguage Processing (NLP) and generative AI into enterprise-critical software\nsystems. The chapter provides practical insights, solutions, and examples to\nequip readers with the knowledge and tools necessary for effectively building\nsolutions with NLP at their cores. We also reflect on how these text\ndata-centric tasks sit together with the traditional RE process. We also\nhighlight new RE tasks that may be necessary for handling the increasingly\nimportant text data-centricity involved in developing software systems.\n']",AI-Driven Software Development and Engineering,Artificial Intelligence in Technology and Engineering,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
488,12,488_ai_automation_software_developers,"['ai', 'automation', 'software', 'developers', 'programmers', 'development', 'architectural', 'automl', 'engineering', 'prototyping']","['software', 'development', 'prototyping', 'engineering', 'programmers', 'requirements', 'copilots', 'conceptual', 'phases', 'partnership']","['  A paradigm shift is underway in Software Engineering, with AI systems such as\nLLMs gaining increasing importance for improving software development\nproductivity. This trend is anticipated to persist. In the next five years, we\nwill likely see an increasing symbiotic partnership between human developers\nand AI. The Software Engineering research community cannot afford to overlook\nthis trend; we must address the key research challenges posed by the\nintegration of AI into the software development process. In this paper, we\npresent our vision of the future of software development in an AI-Driven world\nand explore the key challenges that our research community should address to\nrealize this vision.\n', ""  Background:Technical systems are growing in complexity with more components\nand functions across various disciplines. Model-Driven Engineering (MDE) helps\nmanage this complexity by using models as key artifacts. Domain-Specific\nLanguages (DSL) supported by MDE facilitate modeling. As data generation in\nproduct development increases, there's a growing demand for AI algorithms,\nwhich can be challenging to implement. Integrating AI algorithms with DSL and\nMDE can streamline this process. Objective:This study aims to investigate the\nexisting model-driven approaches relying on DSL in support of the engineering\nof AI software systems to sharpen future research further and define the\ncurrent state of the art. Method:We conducted a Systemic Literature Review\n(SLR), collecting papers from five major databases resulting in 1335 candidate\nstudies, eventually retaining 18 primary studies. Each primary study will be\nevaluated and discussed with respect to the adoption of MDE principles and\npractices and the phases of AI development support aligned with the stages of\nthe CRISP-DM methodology. Results:The study's findings show that language\nworkbenches are of paramount importance in dealing with all aspects of modeling\nlanguage development and are leveraged to define DSL explicitly addressing AI\nconcerns. The most prominent AI-related concerns are training and modeling of\nthe AI algorithm, while minor emphasis is given to the time-consuming\npreparation of the data. Early project phases that support interdisciplinary\ncommunication of requirements, e.g., CRISP-DM Business Understanding phase, are\nrarely reflected. Conclusion:The study found that the use of MDE for AI is\nstill in its early stages, and there is no single tool or method that is widely\nused. Additionally, current approaches tend to focus on specific stages of\ndevelopment rather than providing support for the entire development process.\n"", '  Across the dynamic business landscape today, enterprises face an\never-increasing range of challenges. These include the constantly evolving\nregulatory environment, the growing demand for personalization within software\napplications, and the heightened emphasis on governance. In response to these\nmultifaceted demands, large enterprises have been adopting automation that\nspans from the optimization of core business processes to the enhancement of\ncustomer experiences. Indeed, Artificial Intelligence (AI) has emerged as a\npivotal element of modern software systems. In this context, data plays an\nindispensable role. AI-centric software systems based on supervised learning\nand operating at an industrial scale require large volumes of training data to\nperform effectively. Moreover, the incorporation of generative AI has led to a\ngrowing demand for adequate evaluation benchmarks. Our experience in this field\nhas revealed that the requirement for large datasets for training and\nevaluation introduces a host of intricate challenges. This book chapter\nexplores the evolving landscape of Software Engineering (SE) in general, and\nRequirements Engineering (RE) in particular, in this era marked by AI\nintegration. We discuss challenges that arise while integrating Natural\nLanguage Processing (NLP) and generative AI into enterprise-critical software\nsystems. The chapter provides practical insights, solutions, and examples to\nequip readers with the knowledge and tools necessary for effectively building\nsolutions with NLP at their cores. We also reflect on how these text\ndata-centric tasks sit together with the traditional RE process. We also\nhighlight new RE tasks that may be necessary for handling the increasingly\nimportant text data-centricity involved in developing software systems.\n']",AI-Driven Software Development and Engineering,Artificial Intelligence in Technology and Engineering,Artificial Intelligence and Cognitive Systems,Artificial Intelligence and Reasoning Systems
489,12,489_gradients_differential_pdes_nonlinear,"['gradients', 'differential', 'pdes', 'nonlinear', 'pde', 'solvers', 'neural', 'parameterized', 'dde', 'nonlinearity']","['equations', 'differential', 'adjoint', 'probabilities', 'nonlinear', 'risk', 'physics', 'operator', 'gradients', 'term']","['  Machine learning techniques have recently been of great interest for solving\ndifferential equations. Training these models is classically a data-fitting\ntask, but knowledge of the expression of the differential equation can be used\nto supplement the training objective, leading to the development of\nphysics-informed scientific machine learning. In this article, we focus on one\nclass of models called nonlinear vector autoregression (NVAR) to solve ordinary\ndifferential equations (ODEs). Motivated by connections to numerical\nintegration and physics-informed neural networks, we explicitly derive the\nphysics-informed NVAR (piNVAR) which enforces the right-hand side of the\nunderlying differential equation regardless of NVAR construction. Because NVAR\nand piNVAR completely share their learned parameters, we propose an augmented\nprocedure to jointly train the two models. Then, using both data-driven and\nODE-driven metrics, we evaluate the ability of the piNVAR model to predict\nsolutions to various ODE systems, such as the undamped spring, a Lotka-Volterra\npredator-prey nonlinear model, and the chaotic Lorenz system.\n', '  In this work, we present an adjoint-based method for discovering the\nunderlying governing partial differential equations (PDEs) given data. The idea\nis to consider a parameterized PDE in a general form and formulate a\nPDE-constrained optimization problem aimed at minimizing the error of the PDE\nsolution from data. Using variational calculus, we obtain an evolution equation\nfor the Lagrange multipliers (adjoint equations) allowing us to compute the\ngradient of the objective function with respect to the parameters of PDEs given\ndata in a straightforward manner. In particular, we consider a family of\nparameterized PDEs encompassing linear, nonlinear, and spatial derivative\ncandidate terms, and elegantly derive the corresponding adjoint equations. We\nshow the efficacy of the proposed approach in identifying the form of the PDE\nup to machine accuracy, enabling the accurate discovery of PDEs from data. We\nalso compare its performance with the famous PDE Functional Identification of\nNonlinear Dynamics method known as PDE-FIND (Rudy et al., 2017), on both smooth\nand noisy data sets. Even though the proposed adjoint method relies on\nforward/backward solvers, it outperforms PDE-FIND for large data sets thanks to\nthe analytic expressions for gradients of the cost function with respect to\neach PDE parameter.\n', ""  Delay Differential Equations (DDEs) are a class of differential equations\nthat can model diverse scientific phenomena. However, identifying the\nparameters, especially the time delay, that make a DDE's predictions match\nexperimental results can be challenging. We introduce DDE-Find, a data-driven\nframework for learning a DDE's parameters, time delay, and initial condition\nfunction. DDE-Find uses an adjoint-based approach to efficiently compute the\ngradient of a loss function with respect to the model parameters. We motivate\nand rigorously prove an expression for the gradients of the loss using the\nadjoint. DDE-Find builds upon recent developments in learning DDEs from data\nand delivers the first complete framework for learning DDEs from data. Through\na series of numerical experiments, we demonstrate that DDE-Find can learn DDEs\nfrom noisy, limited data.\n""]",Physics-Informed Machine Learning for Differential Equations,Physics-Informed Machine Learning for Differential Equations and Fluid Dynamics,Machine Learning for Dynamical Systems and Differential Equations,Machine Learning for Dynamical Systems and Differential Equations
490,12,490_networks_graphs_nodes_prediction,"['networks', 'graphs', 'nodes', 'prediction', 'graph', 'classification', 'node', 'neural', 'gnns', 'predictions']","['conformal', 'uncertainty', 'degree', 'graph', 'node', 'estimates', 'prediction', 'sets', 'nodes', 'hypotheses']","['  Graph Neural Networks have achieved remarkable accuracy in semi-supervised\nnode classification tasks. However, these results lack reliable uncertainty\nestimates. Conformal prediction methods provide a theoretical guarantee for\nnode classification tasks, ensuring that the conformal prediction set contains\nthe ground-truth label with a desired probability (e.g., 95%). In this paper,\nwe empirically show that for each node, aggregating the non-conformity scores\nof nodes with the same label can improve the efficiency of conformal prediction\nsets. This observation motivates us to propose a novel algorithm named\nSimilarity-Navigated Adaptive Prediction Sets (SNAPS), which aggregates the\nnon-conformity scores based on feature similarity and structural neighborhood.\nThe key idea behind SNAPS is that nodes with high feature similarity or direct\nconnections tend to have the same label. By incorporating adaptive similar\nnodes information, SNAPS can generate compact prediction sets and increase the\nsingleton hit ratio (correct prediction sets of size one). Moreover, we\ntheoretically provide a finite-sample coverage guarantee of SNAPS. Extensive\nexperiments demonstrate the superiority of SNAPS, improving the efficiency of\nprediction sets and singleton hit ratio while maintaining valid coverage.\n', '  While graph neural networks (GNNs) are widely used for node and graph\nrepresentation learning tasks, the reliability of GNN uncertainty estimates\nunder distribution shifts remains relatively under-explored. Indeed, while\npost-hoc calibration strategies can be used to improve in-distribution\ncalibration, they need not also improve calibration under distribution shift.\nHowever, techniques which produce GNNs with better intrinsic uncertainty\nestimates are particularly valuable, as they can always be combined with\npost-hoc strategies later. Therefore, in this work, we propose G-$\\Delta$UQ, a\nnovel training framework designed to improve intrinsic GNN uncertainty\nestimates. Our framework adapts the principle of stochastic data centering to\ngraph data through novel graph anchoring strategies, and is able to support\npartially stochastic GNNs. While, the prevalent wisdom is that fully stochastic\nnetworks are necessary to obtain reliable estimates, we find that the\nfunctional diversity induced by our anchoring strategies when sampling\nhypotheses renders this unnecessary and allows us to support G-$\\Delta$UQ on\npretrained models. Indeed, through extensive evaluation under covariate,\nconcept and graph size shifts, we show that G-$\\Delta$UQ leads to better\ncalibrated GNNs for node and graph classification. Further, it also improves\nperformance on the uncertainty-based tasks of out-of-distribution detection and\ngeneralization gap estimation. Overall, our work provides insights into\nuncertainty estimation for GNNs, and demonstrates the utility of G-$\\Delta$UQ\nin obtaining reliable estimates.\n', '  Graph Neural Networks (GNNs) have emerged as potent tools for predicting\noutcomes in graph-structured data. Despite their efficacy, a significant\ndrawback of GNNs lies in their limited ability to provide robust uncertainty\nestimates, posing challenges to their reliability in contexts where errors\ncarry significant consequences. Moreover, GNNs typically excel in\nin-distribution settings, assuming that training and test data follow identical\ndistributions a condition often unmet in real world graph data scenarios. In\nthis article, we leverage conformal prediction, a widely recognized statistical\ntechnique for quantifying uncertainty by transforming predictive model outputs\ninto prediction sets, to address uncertainty quantification in GNN predictions\namidst conditional shift\\footnote{Representing the change in conditional\nprobability distribution \\(P(label|input)\\) from source domain to target\ndomain.} in graph-based semi-supervised learning (SSL). Additionally, we\npropose a novel loss function aimed at refining model predictions by minimizing\nconditional shift in latent stages. Termed Conditional Shift Robust (CondSR)\nconformal prediction for GNNs, our approach CondSR is model-agnostic and\nadaptable to various classification models. We validate the effectiveness of\nour method on standard graph benchmark datasets, integrating it with\nstate-of-the-art GNNs in node classification tasks. Comprehensive evaluations\ndemonstrate that our approach consistently achieves any predefined target\nmarginal coverage, enhances the accuracy of state of the art GNN models by up\nto 12\\% under conditional shift, and reduces the prediction set size by up to\n48\\%. The code implementation is publicly available for further exploration and\nexperimentation.\n']",Graph Neural Networks for Node Classification,Graph Neural Networks and Deep Learning on Graph-Structured Data,Graph Representation Learning and Neural Networks,Graph Representation Learning and Neural Networks
491,12,491_promptcompressor_compression_prompts_decoder,"['promptcompressor', 'compression', 'prompts', 'decoder', 'compressed', 'prompt', 'compressing', 'encoder', 'compress', 'adacoder']","['compression', 'gist', 'prompt', 'prompts', 'compressed', 'compressing', 'lengthy', 'transferability', 'ratios', 'natural']","[""  Large language models (LLMs) have shown exceptional abilities for multiple\ndifferent natural language processing tasks. While prompting is a crucial tool\nfor LLM inference, we observe that there is a significant cost associated with\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\nto sub-standard results in terms of readability and interpretability of the\ncompressed prompt, with a detrimental impact on prompt utility. To address\nthis, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an\neffective strategy for prompt compression over task-agnostic and task-aware\nprompts. PROMPT-SAW uses the prompt's textual information to build a graph,\nlater extracts key information elements in the graph to come up with the\ncompressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the\nexisting GSM8k benchmark for task-agnostic prompts in order to provide a\ncomprehensive evaluation platform. Experimental evaluation using benchmark\ndatasets shows that prompts compressed by PROMPT-SAW are not only better in\nterms of readability, but they also outperform the best-performing baseline\nmodels by up to 14.3 and 13.7 respectively for task-aware and task-agnostic\nsettings while compressing the original prompt text by 33.0 and 56.7.\n"", '  Large language models (LLMs) require lengthy prompts as the input context to\nproduce output aligned with user intentions, a process that incurs extra costs\nduring inference. In this paper, we propose the Gist COnditioned deCOding\n(Gist-COCO) model, introducing a novel method for compressing prompts which\nalso can assist the prompt interpretation and engineering. Gist-COCO employs an\nencoder-decoder based language model and then incorporates an additional\nencoder as a plugin module to compress prompts with inputs using gist tokens.\nIt finetunes the compression plugin module and uses the representations of gist\ntokens to emulate the raw prompts in the vanilla language model. By verbalizing\nthe representations of gist tokens into gist prompts, the compression ability\nof Gist-COCO can be generalized to different LLMs with high compression rates.\nOur experiments demonstrate that Gist-COCO outperforms previous prompt\ncompression models in both passage and instruction compression tasks. Further\nanalysis on gist verbalization results suggests that our gist prompts serve\ndifferent functions in aiding language models. They may directly provide\npotential answers, generate the chain-of-thought, or simply repeat the inputs.\nAll data and codes are available at https://github.com/OpenMatch/Gist-COCO .\n', '  This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x. Our code is available at\nhttps://aka.ms/LLMLingua-2.\n']",Prompt Compression for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
491,12,491_promptcompressor_compression_prompts_decoder,"['promptcompressor', 'compression', 'prompts', 'decoder', 'compressed', 'prompt', 'compressing', 'encoder', 'compress', 'adacoder']","['compression', 'gist', 'prompt', 'prompts', 'compressed', 'compressing', 'lengthy', 'transferability', 'ratios', 'natural']","[""  Large language models (LLMs) have shown exceptional abilities for multiple\ndifferent natural language processing tasks. While prompting is a crucial tool\nfor LLM inference, we observe that there is a significant cost associated with\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\nto sub-standard results in terms of readability and interpretability of the\ncompressed prompt, with a detrimental impact on prompt utility. To address\nthis, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an\neffective strategy for prompt compression over task-agnostic and task-aware\nprompts. PROMPT-SAW uses the prompt's textual information to build a graph,\nlater extracts key information elements in the graph to come up with the\ncompressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the\nexisting GSM8k benchmark for task-agnostic prompts in order to provide a\ncomprehensive evaluation platform. Experimental evaluation using benchmark\ndatasets shows that prompts compressed by PROMPT-SAW are not only better in\nterms of readability, but they also outperform the best-performing baseline\nmodels by up to 14.3 and 13.7 respectively for task-aware and task-agnostic\nsettings while compressing the original prompt text by 33.0 and 56.7.\n"", '  Large language models (LLMs) require lengthy prompts as the input context to\nproduce output aligned with user intentions, a process that incurs extra costs\nduring inference. In this paper, we propose the Gist COnditioned deCOding\n(Gist-COCO) model, introducing a novel method for compressing prompts which\nalso can assist the prompt interpretation and engineering. Gist-COCO employs an\nencoder-decoder based language model and then incorporates an additional\nencoder as a plugin module to compress prompts with inputs using gist tokens.\nIt finetunes the compression plugin module and uses the representations of gist\ntokens to emulate the raw prompts in the vanilla language model. By verbalizing\nthe representations of gist tokens into gist prompts, the compression ability\nof Gist-COCO can be generalized to different LLMs with high compression rates.\nOur experiments demonstrate that Gist-COCO outperforms previous prompt\ncompression models in both passage and instruction compression tasks. Further\nanalysis on gist verbalization results suggests that our gist prompts serve\ndifferent functions in aiding language models. They may directly provide\npotential answers, generate the chain-of-thought, or simply repeat the inputs.\nAll data and codes are available at https://github.com/OpenMatch/Gist-COCO .\n', '  This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x. Our code is available at\nhttps://aka.ms/LLMLingua-2.\n']",Prompt Compression for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
491,12,491_promptcompressor_compression_prompts_decoder,"['promptcompressor', 'compression', 'prompts', 'decoder', 'compressed', 'prompt', 'compressing', 'encoder', 'compress', 'adacoder']","['compression', 'gist', 'prompt', 'prompts', 'compressed', 'compressing', 'lengthy', 'transferability', 'ratios', 'natural']","[""  Large language models (LLMs) have shown exceptional abilities for multiple\ndifferent natural language processing tasks. While prompting is a crucial tool\nfor LLM inference, we observe that there is a significant cost associated with\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\nto sub-standard results in terms of readability and interpretability of the\ncompressed prompt, with a detrimental impact on prompt utility. To address\nthis, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an\neffective strategy for prompt compression over task-agnostic and task-aware\nprompts. PROMPT-SAW uses the prompt's textual information to build a graph,\nlater extracts key information elements in the graph to come up with the\ncompressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the\nexisting GSM8k benchmark for task-agnostic prompts in order to provide a\ncomprehensive evaluation platform. Experimental evaluation using benchmark\ndatasets shows that prompts compressed by PROMPT-SAW are not only better in\nterms of readability, but they also outperform the best-performing baseline\nmodels by up to 14.3 and 13.7 respectively for task-aware and task-agnostic\nsettings while compressing the original prompt text by 33.0 and 56.7.\n"", '  Large language models (LLMs) require lengthy prompts as the input context to\nproduce output aligned with user intentions, a process that incurs extra costs\nduring inference. In this paper, we propose the Gist COnditioned deCOding\n(Gist-COCO) model, introducing a novel method for compressing prompts which\nalso can assist the prompt interpretation and engineering. Gist-COCO employs an\nencoder-decoder based language model and then incorporates an additional\nencoder as a plugin module to compress prompts with inputs using gist tokens.\nIt finetunes the compression plugin module and uses the representations of gist\ntokens to emulate the raw prompts in the vanilla language model. By verbalizing\nthe representations of gist tokens into gist prompts, the compression ability\nof Gist-COCO can be generalized to different LLMs with high compression rates.\nOur experiments demonstrate that Gist-COCO outperforms previous prompt\ncompression models in both passage and instruction compression tasks. Further\nanalysis on gist verbalization results suggests that our gist prompts serve\ndifferent functions in aiding language models. They may directly provide\npotential answers, generate the chain-of-thought, or simply repeat the inputs.\nAll data and codes are available at https://github.com/OpenMatch/Gist-COCO .\n', '  This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x. Our code is available at\nhttps://aka.ms/LLMLingua-2.\n']",Prompt Compression for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
491,12,491_promptcompressor_compression_prompts_decoder,"['promptcompressor', 'compression', 'prompts', 'decoder', 'compressed', 'prompt', 'compressing', 'encoder', 'compress', 'adacoder']","['compression', 'gist', 'prompt', 'prompts', 'compressed', 'compressing', 'lengthy', 'transferability', 'ratios', 'natural']","[""  Large language models (LLMs) have shown exceptional abilities for multiple\ndifferent natural language processing tasks. While prompting is a crucial tool\nfor LLM inference, we observe that there is a significant cost associated with\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\nto sub-standard results in terms of readability and interpretability of the\ncompressed prompt, with a detrimental impact on prompt utility. To address\nthis, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an\neffective strategy for prompt compression over task-agnostic and task-aware\nprompts. PROMPT-SAW uses the prompt's textual information to build a graph,\nlater extracts key information elements in the graph to come up with the\ncompressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the\nexisting GSM8k benchmark for task-agnostic prompts in order to provide a\ncomprehensive evaluation platform. Experimental evaluation using benchmark\ndatasets shows that prompts compressed by PROMPT-SAW are not only better in\nterms of readability, but they also outperform the best-performing baseline\nmodels by up to 14.3 and 13.7 respectively for task-aware and task-agnostic\nsettings while compressing the original prompt text by 33.0 and 56.7.\n"", '  Large language models (LLMs) require lengthy prompts as the input context to\nproduce output aligned with user intentions, a process that incurs extra costs\nduring inference. In this paper, we propose the Gist COnditioned deCOding\n(Gist-COCO) model, introducing a novel method for compressing prompts which\nalso can assist the prompt interpretation and engineering. Gist-COCO employs an\nencoder-decoder based language model and then incorporates an additional\nencoder as a plugin module to compress prompts with inputs using gist tokens.\nIt finetunes the compression plugin module and uses the representations of gist\ntokens to emulate the raw prompts in the vanilla language model. By verbalizing\nthe representations of gist tokens into gist prompts, the compression ability\nof Gist-COCO can be generalized to different LLMs with high compression rates.\nOur experiments demonstrate that Gist-COCO outperforms previous prompt\ncompression models in both passage and instruction compression tasks. Further\nanalysis on gist verbalization results suggests that our gist prompts serve\ndifferent functions in aiding language models. They may directly provide\npotential answers, generate the chain-of-thought, or simply repeat the inputs.\nAll data and codes are available at https://github.com/OpenMatch/Gist-COCO .\n', '  This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x. Our code is available at\nhttps://aka.ms/LLMLingua-2.\n']",Prompt Compression for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
491,12,491_promptcompressor_compression_prompts_decoder,"['promptcompressor', 'compression', 'prompts', 'decoder', 'compressed', 'prompt', 'compressing', 'encoder', 'compress', 'adacoder']","['compression', 'gist', 'prompt', 'prompts', 'compressed', 'compressing', 'lengthy', 'transferability', 'ratios', 'natural']","[""  Large language models (LLMs) have shown exceptional abilities for multiple\ndifferent natural language processing tasks. While prompting is a crucial tool\nfor LLM inference, we observe that there is a significant cost associated with\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\nto sub-standard results in terms of readability and interpretability of the\ncompressed prompt, with a detrimental impact on prompt utility. To address\nthis, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an\neffective strategy for prompt compression over task-agnostic and task-aware\nprompts. PROMPT-SAW uses the prompt's textual information to build a graph,\nlater extracts key information elements in the graph to come up with the\ncompressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the\nexisting GSM8k benchmark for task-agnostic prompts in order to provide a\ncomprehensive evaluation platform. Experimental evaluation using benchmark\ndatasets shows that prompts compressed by PROMPT-SAW are not only better in\nterms of readability, but they also outperform the best-performing baseline\nmodels by up to 14.3 and 13.7 respectively for task-aware and task-agnostic\nsettings while compressing the original prompt text by 33.0 and 56.7.\n"", '  Large language models (LLMs) require lengthy prompts as the input context to\nproduce output aligned with user intentions, a process that incurs extra costs\nduring inference. In this paper, we propose the Gist COnditioned deCOding\n(Gist-COCO) model, introducing a novel method for compressing prompts which\nalso can assist the prompt interpretation and engineering. Gist-COCO employs an\nencoder-decoder based language model and then incorporates an additional\nencoder as a plugin module to compress prompts with inputs using gist tokens.\nIt finetunes the compression plugin module and uses the representations of gist\ntokens to emulate the raw prompts in the vanilla language model. By verbalizing\nthe representations of gist tokens into gist prompts, the compression ability\nof Gist-COCO can be generalized to different LLMs with high compression rates.\nOur experiments demonstrate that Gist-COCO outperforms previous prompt\ncompression models in both passage and instruction compression tasks. Further\nanalysis on gist verbalization results suggests that our gist prompts serve\ndifferent functions in aiding language models. They may directly provide\npotential answers, generate the chain-of-thought, or simply repeat the inputs.\nAll data and codes are available at https://github.com/OpenMatch/Gist-COCO .\n', '  This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x. Our code is available at\nhttps://aka.ms/LLMLingua-2.\n']",Prompt Compression for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
491,12,491_promptcompressor_compression_prompts_decoder,"['promptcompressor', 'compression', 'prompts', 'decoder', 'compressed', 'prompt', 'compressing', 'encoder', 'compress', 'adacoder']","['compression', 'gist', 'prompt', 'prompts', 'compressed', 'compressing', 'lengthy', 'transferability', 'ratios', 'natural']","[""  Large language models (LLMs) have shown exceptional abilities for multiple\ndifferent natural language processing tasks. While prompting is a crucial tool\nfor LLM inference, we observe that there is a significant cost associated with\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\nto sub-standard results in terms of readability and interpretability of the\ncompressed prompt, with a detrimental impact on prompt utility. To address\nthis, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an\neffective strategy for prompt compression over task-agnostic and task-aware\nprompts. PROMPT-SAW uses the prompt's textual information to build a graph,\nlater extracts key information elements in the graph to come up with the\ncompressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the\nexisting GSM8k benchmark for task-agnostic prompts in order to provide a\ncomprehensive evaluation platform. Experimental evaluation using benchmark\ndatasets shows that prompts compressed by PROMPT-SAW are not only better in\nterms of readability, but they also outperform the best-performing baseline\nmodels by up to 14.3 and 13.7 respectively for task-aware and task-agnostic\nsettings while compressing the original prompt text by 33.0 and 56.7.\n"", '  Large language models (LLMs) require lengthy prompts as the input context to\nproduce output aligned with user intentions, a process that incurs extra costs\nduring inference. In this paper, we propose the Gist COnditioned deCOding\n(Gist-COCO) model, introducing a novel method for compressing prompts which\nalso can assist the prompt interpretation and engineering. Gist-COCO employs an\nencoder-decoder based language model and then incorporates an additional\nencoder as a plugin module to compress prompts with inputs using gist tokens.\nIt finetunes the compression plugin module and uses the representations of gist\ntokens to emulate the raw prompts in the vanilla language model. By verbalizing\nthe representations of gist tokens into gist prompts, the compression ability\nof Gist-COCO can be generalized to different LLMs with high compression rates.\nOur experiments demonstrate that Gist-COCO outperforms previous prompt\ncompression models in both passage and instruction compression tasks. Further\nanalysis on gist verbalization results suggests that our gist prompts serve\ndifferent functions in aiding language models. They may directly provide\npotential answers, generate the chain-of-thought, or simply repeat the inputs.\nAll data and codes are available at https://github.com/OpenMatch/Gist-COCO .\n', '  This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x. Our code is available at\nhttps://aka.ms/LLMLingua-2.\n']",Prompt Compression for Large Language Models,Advances in Large Language Models,Large Language Models,Large Language Models
492,12,492_autoencoder_accelerator_neuralized_autoencoded,"['autoencoder', 'accelerator', 'neuralized', 'autoencoded', 'accelerators', 'lstm', 'particle', 'beam', 'reactor', 'particles']","['reactor', 'particle', 'projections', 'accelerator', 'beam', 'nuclear', 'accelerators', 'phase', 'spill', 'fuel']","['  Particle accelerators are complex systems that focus, guide, and accelerate\nintense charged particle beams to high energy. Beam diagnostics present a\nchallenging problem due to limited non-destructive measurements,\ncomputationally demanding simulations, and inherent uncertainties in the\nsystem. We propose a two-step unsupervised deep learning framework named as\nConditional Latent Autoregressive Recurrent Model (CLARM) for learning the\nspatiotemporal dynamics of charged particles in accelerators. CLARM consists of\na Conditional Variational Autoencoder (CVAE) transforming six-dimensional phase\nspace into a lower-dimensional latent distribution and a Long Short-Term Memory\n(LSTM) network capturing temporal dynamics in an autoregressive manner. The\nCLARM can generate projections at various accelerator modules by sampling and\ndecoding the latent space representation. The model also forecasts future\nstates (downstream locations) of charged particles from past states (upstream\nlocations). The results demonstrate that the generative and forecasting ability\nof the proposed approach is promising when tested against a variety of\nevaluation metrics.\n', '  Charged particle dynamics under the influence of electromagnetic fields is a\nchallenging spatiotemporal problem. Many high performance physics-based\nsimulators for predicting behavior in a charged particle beam are\ncomputationally expensive, limiting their utility for solving inverse problems\nonline. The problem of estimating upstream six-dimensional phase space given\ndownstream measurements of charged particles in an accelerator is an inverse\nproblem of growing importance. This paper introduces a reverse Latent Evolution\nModel (rLEM) designed for temporal inversion of forward beam dynamics. In this\ntwo-step self-supervised deep learning framework, we utilize a Conditional\nVariational Autoencoder (CVAE) to project 6D phase space projections of a\ncharged particle beam into a lower-dimensional latent distribution.\nSubsequently, we autoregressively learn the inverse temporal dynamics in the\nlatent space using a Long Short-Term Memory (LSTM) network. The coupled\nCVAE-LSTM framework can predict 6D phase space projections across all upstream\naccelerating sections based on single or multiple downstream phase space\nmeasurements as inputs. The proposed model also captures the aleatoric\nuncertainty of the high-dimensional input data within the latent space. This\nuncertainty, which reflects potential uncertain measurements at a given module,\nis propagated through the LSTM to estimate uncertainty bounds for all upstream\npredictions, demonstrating the robustness of the LSTM against in-distribution\nvariations in the input data.\n', '  Addressing the charged particle beam diagnostics in accelerators poses a\nformidable challenge, demanding high-fidelity simulations in limited\ncomputational time. Machine learning (ML) based surrogate models have emerged\nas a promising tool for non-invasive charged particle beam diagnostics. Trained\nML models can make predictions much faster than computationally expensive\nphysics simulations. In this work, we have proposed a temporally structured\nvariational autoencoder model to autoregressively forecast the spatiotemporal\ndynamics of the 15 unique 2D projections of 6D phase space of charged particle\nbeam as it travels through the LANSCE linear accelerator. In the model, VAE\nembeds the phase space projections into a lower dimensional latent space. A\nlong-short-term memory network then learns the temporal correlations in the\nlatent space. The trained network can evolve the phase space projections across\nfurther modules provided the first few modules as inputs. The model predicts\nall the projections across different modules with low mean squared error and\nhigh structural similarity index.\n']",Particle Beam Dynamics in Accelerators,Particle Physics and Accelerator Technologies,Particle Physics and Accelerator Technologies,Particle Physics and Accelerator Technologies
493,12,493_dialogues_dialogue_conversational_personas,"['dialogues', 'dialogue', 'conversational', 'personas', 'persona', 'personachat', 'conversation', 'conversations', 'personalizing', 'profiles']","['persona', 'dialogue', 'personas', 'personalized', 'commonsense', 'roles', 'dialogues', 'round', 'conversations', 'extraction']","['  Persona-based dialogue systems aim to generate consistent responses based on\nhistorical context and predefined persona. Unlike conventional dialogue\ngeneration, the persona-based dialogue needs to consider both dialogue context\nand persona, posing a challenge for coherent training. Specifically, this\nrequires a delicate weight balance between context and persona. To achieve\nthat, in this paper, we propose an effective framework with Persona-Adaptive\nAttention (PAA), which adaptively integrates the weights from the persona and\ncontext information via our designed attention. In addition, a dynamic masking\nmechanism is applied to the PAA to not only drop redundant information in\ncontext and persona but also serve as a regularization mechanism to avoid\noverfitting. Experimental results demonstrate the superiority of the proposed\nPAA framework compared to the strong baselines in both automatic and human\nevaluation. Moreover, the proposed PAA approach can perform equivalently well\nin a low-resource regime compared to models trained in a full-data setting,\nwhich achieve a similar result with only 20% to 30% of data compared to the\nlarger models trained in the full-data setting. To fully exploit the\neffectiveness of our design, we designed several variants for handling the\nweighted information in different ways, showing the necessity and sufficiency\nof our weighting and masking designs.\n', ""  Providing emotional support through dialogue systems is becoming increasingly\nimportant in today's world, as it can support both mental health and social\ninteractions in many conversation scenarios. Previous works have shown that\nusing persona is effective for generating empathetic and supportive responses.\nThey have often relied on pre-provided persona rather than inferring them\nduring conversations. However, it is not always possible to obtain a user\npersona before the conversation begins. To address this challenge, we propose\nPESS (Persona Extraction through Semantic Similarity), a novel framework that\ncan automatically infer informative and consistent persona from dialogues. We\ndevise completeness loss and consistency loss based on semantic similarity\nscores. The completeness loss encourages the model to generate missing persona\ninformation, and the consistency loss guides the model to distinguish between\nconsistent and inconsistent persona. Our experimental results demonstrate that\nhigh-quality persona information inferred by PESS is effective in generating\nemotionally supportive responses.\n"", '  While valuable datasets such as PersonaChat provide a foundation for training\npersona-grounded dialogue agents, they lack diversity in conversational and\nnarrative settings, primarily existing in the ""real"" world. To develop dialogue\nagents with unique personas, models are trained to converse given a specific\npersona, but hand-crafting these persona can be time-consuming, thus methods\nexist to automatically extract persona information from existing\ncharacter-specific dialogue. However, these persona-extraction models are also\ntrained on datasets derived from PersonaChat and struggle to provide\nhigh-quality persona information from conversational settings that do not take\nplace in the real world, such as the fantasy-focused dataset, LIGHT. Creating\nnew data to train models on a specific setting is human-intensive, thus\nprohibitively expensive. To address both these issues, we introduce a natural\nlanguage inference method for post-hoc adapting a trained persona extraction\nmodel to a new setting. We draw inspiration from the literature of dialog\nnatural language inference (NLI), and devise NLI-reranking methods to extract\nstructured persona information from dialogue. Compared to existing persona\nextraction models, our method returns higher-quality extracted persona and\nrequires less human annotation.\n']",Persona-Based Dialogue Systems,Persona-Based Natural Language Processing,Natural Language Processing,Natural Language Processing
494,12,494_videos_video_attention_memory,"['videos', 'video', 'attention', 'memory', 'vid', 'diffusion', 'webvid', 'frames', 'pixels', 'conditioned']","['video', 'diffusion', 'videos', 'frames', 'generation', 'consistency', 'image', 'motion', 'conditional', 'frame']","['  Diffusion models have obtained substantial progress in image-to-video (I2V)\ngeneration. However, such models are not fully understood. In this paper, we\nreport a significant but previously overlooked issue in I2V diffusion models\n(I2V-DMs), namely, conditional image leakage. I2V-DMs tend to over-rely on the\nconditional image at large time steps, neglecting the crucial task of\npredicting the clean video from noisy inputs, which results in videos lacking\ndynamic and vivid motion. We further address this challenge from both inference\nand training aspects by presenting plug-and-play strategies accordingly. First,\nwe introduce a training-free inference strategy that starts the generation\nprocess from an earlier time step to avoid the unreliable late-time steps of\nI2V-DMs, as well as an initial noise distribution with optimal analytic\nexpressions (Analytic-Init) by minimizing the KL divergence between it and the\nactual marginal distribution to effectively bridge the training-inference gap.\nSecond, to mitigate conditional image leakage during training, we design a\ntime-dependent noise distribution for the conditional image, which favors high\nnoise levels at large time steps to sufficiently interfere with the conditional\nimage. We validate these strategies on various I2V-DMs using our collected\nopen-domain image benchmark and the UCF101 dataset. Extensive results\ndemonstrate that our methods outperform baselines by producing videos with more\ndynamic and natural motion without compromising image alignment and temporal\nconsistency. The project page: \\url{https://cond-image-leak.github.io/}.\n', '  Video diffusion models has been gaining increasing attention for its ability\nto produce videos that are both coherent and of high fidelity. However, the\niterative denoising process makes it computationally intensive and\ntime-consuming, thus limiting its applications. Inspired by the Consistency\nModel (CM) that distills pretrained image diffusion models to accelerate the\nsampling with minimal steps and its successful extension Latent Consistency\nModel (LCM) on conditional image generation, we propose AnimateLCM, allowing\nfor high-fidelity video generation within minimal steps. Instead of directly\nconducting consistency learning on the raw video dataset, we propose a\ndecoupled consistency learning strategy that decouples the distillation of\nimage generation priors and motion generation priors, which improves the\ntraining efficiency and enhance the generation visual quality. Additionally, to\nenable the combination of plug-and-play adapters in stable diffusion community\nto achieve various functions (e.g., ControlNet for controllable generation). we\npropose an efficient strategy to adapt existing adapters to our distilled\ntext-conditioned video consistency model or train adapters from scratch without\nharming the sampling speed. We validate the proposed strategy in\nimage-conditioned video generation and layout-conditioned video generation, all\nachieving top-performing results. Experimental results validate the\neffectiveness of our proposed method. Code and weights will be made public.\nMore details are available at https://github.com/G-U-N/AnimateLCM.\n', '  Video diffusion models have recently made great progress in generation\nquality, but are still limited by the high memory and computational\nrequirements. This is because current video diffusion models often attempt to\nprocess high-dimensional videos directly. To tackle this issue, we propose\ncontent-motion latent diffusion model (CMD), a novel efficient extension of\npretrained image diffusion models for video generation. Specifically, we\npropose an autoencoder that succinctly encodes a video as a combination of a\ncontent frame (like an image) and a low-dimensional motion latent\nrepresentation. The former represents the common content, and the latter\nrepresents the underlying motion in the video, respectively. We generate the\ncontent frame by fine-tuning a pretrained image diffusion model, and we\ngenerate the motion latent representation by training a new lightweight\ndiffusion model. A key innovation here is the design of a compact latent space\nthat can directly utilizes a pretrained image diffusion model, which has not\nbeen done in previous latent video diffusion models. This leads to considerably\nbetter quality generation and reduced computational costs. For instance, CMD\ncan sample a video 7.7$\\times$ faster than prior approaches by generating a\nvideo of 512$\\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMD\nachieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previous\nstate-of-the-art of 292.4.\n']",Video Diffusion Models for Generation,Diffusion Models for Image and Video Generation and Manipulation,Diffusion Models for Generative Tasks,Diffusion Models for Generative Tasks
495,12,495_reward_rewards_reinforcement_backpropagation,"['reward', 'rewards', 'reinforcement', 'backpropagation', 'gradient', 'backpropagate', 'generative', 'diffusion', 'bias', 'optimizing']","['reward', 'diffusion', 'denoising', 'overoptimization', 'image', 'rewards', 'objectives', 'primacy', 'preference', 'compressibility']","[""  Diffusion models are a class of flexible generative models trained with an\napproximation to the log-likelihood objective. However, most use cases of\ndiffusion models are not concerned with likelihoods, but instead with\ndownstream objectives such as human-perceived image quality or drug\neffectiveness. In this paper, we investigate reinforcement learning methods for\ndirectly optimizing diffusion models for such objectives. We describe how\nposing denoising as a multi-step decision-making problem enables a class of\npolicy gradient algorithms, which we refer to as denoising diffusion policy\noptimization (DDPO), that are more effective than alternative reward-weighted\nlikelihood approaches. Empirically, DDPO is able to adapt text-to-image\ndiffusion models to objectives that are difficult to express via prompting,\nsuch as image compressibility, and those derived from human feedback, such as\naesthetic quality. Finally, we show that DDPO can improve prompt-image\nalignment using feedback from a vision-language model without the need for\nadditional data collection or human annotation. The project's website can be\nfound at http://rl-diffusion.github.io .\n"", '  Reward finetuning has emerged as a promising approach to aligning foundation\nmodels with downstream objectives. Remarkable success has been achieved in the\nlanguage domain by using reinforcement learning (RL) to maximize rewards that\nreflect human preference. However, in the vision domain, existing RL-based\nreward finetuning methods are limited by their instability in large-scale\ntraining, rendering them incapable of generalizing to complex, unseen prompts.\nIn this paper, we propose Proximal Reward Difference Prediction (PRDP),\nenabling stable black-box reward finetuning for diffusion models for the first\ntime on large-scale prompt datasets with over 100K prompts. Our key innovation\nis the Reward Difference Prediction (RDP) objective that has the same optimal\nsolution as the RL objective while enjoying better training stability.\nSpecifically, the RDP objective is a supervised regression objective that tasks\nthe diffusion model with predicting the reward difference of generated image\npairs from their denoising trajectories. We theoretically prove that the\ndiffusion model that obtains perfect reward difference prediction is exactly\nthe maximizer of the RL objective. We further develop an online algorithm with\nproximal updates to stably optimize the RDP objective. In experiments, we\ndemonstrate that PRDP can match the reward maximization ability of\nwell-established RL-based methods in small-scale training. Furthermore, through\nlarge-scale training on text prompts from the Human Preference Dataset v2 and\nthe Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a\ndiverse set of complex, unseen prompts whereas RL-based methods completely\nfail.\n', ""  Using reinforcement learning with human feedback (RLHF) has shown significant\npromise in fine-tuning diffusion models. Previous methods start by training a\nreward model that aligns with human preferences, then leverage RL techniques to\nfine-tune the underlying models. However, crafting an efficient reward model\ndemands extensive datasets, optimal architecture, and manual hyperparameter\ntuning, making the process both time and cost-intensive. The direct preference\noptimization (DPO) method, effective in fine-tuning large language models,\neliminates the necessity for a reward model. However, the extensive GPU memory\nrequirement of the diffusion model's denoising process hinders the direct\napplication of the DPO method. To address this issue, we introduce the Direct\nPreference for Denoising Diffusion Policy Optimization (D3PO) method to\ndirectly fine-tune diffusion models. The theoretical analysis demonstrates that\nalthough D3PO omits training a reward model, it effectively functions as the\noptimal reward model trained using human feedback data to guide the learning\nprocess. This approach requires no training of a reward model, proving to be\nmore direct, cost-effective, and minimizing computational overhead. In\nexperiments, our method uses the relative scale of objectives as a proxy for\nhuman preference, delivering comparable results to methods using ground-truth\nrewards. Moreover, D3PO demonstrates the ability to reduce image distortion\nrates and generate safer images, overcoming challenges lacking robust reward\nmodels. Our code is publicly available at https://github.com/yk7333/D3PO.\n""]",Optimizing Diffusion Models with Reinforcement Learning,Reinforcement Learning Applications and Methodologies,Reinforcement Learning,Reinforcement Learning
496,12,496_robot_robots_robotic_planning,"['robot', 'robots', 'robotic', 'planning', 'autonomous', 'navigate', 'navigating', 'pedestrians', 'navigation', 'agent']","['navigation', 'robot', 'social', 'planner', 'pedestrians', 'crowded', 'norms', 'robots', 'pedestrian', 'assistive']","[""  Recent research on mobile robot navigation has focused on socially aware\nnavigation in crowded environments. However, existing methods do not adequately\naccount for human robot interactions and demand accurate location information\nfrom omnidirectional sensors, rendering them unsuitable for practical\napplications. In response to this need, this study introduces a novel\nalgorithm, BNBRL+, predicated on the partially observable Markov decision\nprocess framework to assess risks in unobservable areas and formulate movement\nstrategies under uncertainty. BNBRL+ consolidates belief algorithms with\nBayesian neural networks to probabilistically infer beliefs based on the\npositional data of humans. It further integrates the dynamics between the\nrobot, humans, and inferred beliefs to determine the navigation paths and\nembeds social norms within the reward function, thereby facilitating socially\naware navigation. Through experiments in various risk laden scenarios, this\nstudy validates the effectiveness of BNBRL+ in navigating crowded environments\nwith blind spots. The model's ability to navigate effectively in spaces with\nlimited visibility and avoid obstacles dynamically can significantly improve\nthe safety and reliability of autonomous vehicles.\n"", '  Learning robot navigation strategies among pedestrian is crucial for domain\nbased applications. Combining perception, planning and prediction allows us to\nmodel the interactions between robots and pedestrians, resulting in impressive\noutcomes especially with recent approaches based on deep reinforcement learning\n(RL). However, these works do not consider multi-robot scenarios. In this\npaper, we present MultiSoc, a new method for learning multi-agent socially\naware navigation strategies using RL. Inspired by recent works on multi-agent\ndeep RL, our method leverages graph-based representation of agent interactions,\ncombining the positions and fields of view of entities (pedestrians and\nagents). Each agent uses a model based on two Graph Neural Network combined\nwith attention mechanisms. First an edge-selector produces a sparse graph, then\na crowd coordinator applies node attention to produce a graph representing the\ninfluence of each entity on the others. This is incorporated into a model-free\nRL framework to learn multi-agent policies. We evaluate our approach on\nsimulation and provide a series of experiments in a set of various conditions\n(number of agents / pedestrians). Empirical results show that our method learns\nfaster than social navigation deep RL mono-agent techniques, and enables\nefficient multi-agent implicit coordination in challenging crowd navigation\nwith multiple heterogeneous humans. Furthermore, by incorporating customizable\nmeta-parameters, we can adjust the neighborhood density to take into account in\nour navigation strategy.\n', ""  Mobile robots are being used on a large scale in various crowded situations\nand become part of our society. The socially acceptable navigation behavior of\na mobile robot with individual human consideration is an essential requirement\nfor scalable applications and human acceptance. Deep Reinforcement Learning\n(DRL) approaches are recently used to learn a robot's navigation policy and to\nmodel the complex interactions between robots and humans. We propose to divide\nexisting DRL-based navigation approaches based on the robot's exhibited social\nbehavior and distinguish between social collision avoidance with a lack of\nsocial behavior and socially aware approaches with explicit predefined social\nbehavior. In addition, we propose a novel socially integrated navigation\napproach where the robot's social behavior is adaptive and emerges from the\ninteraction with humans. The formulation of our approach is derived from a\nsociological definition, which states that social acting is oriented toward the\nacting of others. The DRL policy is trained in an environment where other\nagents interact socially integrated and reward the robot's behavior\nindividually. The simulation results indicate that the proposed socially\nintegrated navigation approach outperforms a socially aware approach in terms\nof ego navigation performance while significantly reducing the negative impact\non all agents within the environment.\n""]",Robot Navigation in Crowded Environments,Robot Navigation and Locomotion,Robotics and Artificial Intelligence,Robotics and Artificial Intelligence
497,12,497_denoising_restoration_imaging_diffusion,"['denoising', 'restoration', 'imaging', 'diffusion', 'inpainting', 'reconstructions', 'generative', 'residual', 'jpeg', 'restored']","['restoration', 'denoising', 'diffusion', 'image', 'degraded', 'perceptual', 'residual', 'reverse', 'noise', 'sky']","['  Diffusion models have demonstrated remarkable efficacy in generating\nhigh-quality samples. Existing diffusion-based image restoration algorithms\nexploit pre-trained diffusion models to leverage data priors, yet they still\npreserve elements inherited from the unconditional generation paradigm. These\nstrategies initiate the denoising process with pure white noise and incorporate\nrandom noise at each generative step, leading to over-smoothed results. In this\npaper, we present a refined paradigm for diffusion-based image restoration.\nSpecifically, we opt for a sample consistent with the measurement identity at\neach generative step, exploiting the sampling selection as an avenue for output\nstability and enhancement. The number of candidate samples used for selection\nis adaptively determined based on the signal-to-noise ratio of the timestep.\nAdditionally, we start the restoration process with an initialization combined\nwith the measurement signal, providing supplementary information to better\nalign the generative process. Extensive experimental results and analyses\nvalidate that our proposed method significantly enhances image restoration\nperformance while consuming negligible additional computational resources.\n', '  We propose residual denoising diffusion models (RDDM), a novel dual diffusion\nprocess that decouples the traditional single denoising diffusion process into\nresidual diffusion and noise diffusion. This dual diffusion framework expands\nthe denoising-based diffusion models, initially uninterpretable for image\nrestoration, into a unified and interpretable model for both image generation\nand restoration by introducing residuals. Specifically, our residual diffusion\nrepresents directional diffusion from the target image to the degraded input\nimage and explicitly guides the reverse generation process for image\nrestoration, while noise diffusion represents random perturbations in the\ndiffusion process. The residual prioritizes certainty, while the noise\nemphasizes diversity, enabling RDDM to effectively unify tasks with varying\ncertainty or diversity requirements, such as image generation and restoration.\nWe demonstrate that our sampling process is consistent with that of DDPM and\nDDIM through coefficient transformation, and propose a partially\npath-independent generation process to better understand the reverse process.\nNotably, our RDDM enables a generic UNet, trained with only an L1 loss and a\nbatch size of 1, to compete with state-of-the-art image restoration methods. We\nprovide code and pre-trained models to encourage further exploration,\napplication, and development of our innovative framework\n(https://github.com/nachifur/RDDM).\n', '  Inversion by Direct Iteration (InDI) is a new formulation for supervised\nimage restoration that avoids the so-called ""regression to the mean"" effect and\nproduces more realistic and detailed images than existing regression-based\nmethods. It does this by gradually improving image quality in small steps,\nsimilar to generative denoising diffusion models. Image restoration is an\nill-posed problem where multiple high-quality images are plausible\nreconstructions of a given low-quality input. Therefore, the outcome of a\nsingle step regression model is typically an aggregate of all possible\nexplanations, therefore lacking details and realism. The main advantage of InDI\nis that it does not try to predict the clean target image in a single step but\ninstead gradually improves the image in small steps, resulting in better\nperceptual quality. While generative denoising diffusion models also work in\nsmall steps, our formulation is distinct in that it does not require knowledge\nof any analytic form of the degradation process. Instead, we directly learn an\niterative restoration process from low-quality and high-quality paired\nexamples. InDI can be applied to virtually any image degradation, given paired\ntraining data. In conditional denoising diffusion image restoration the\ndenoising network generates the restored image by repeatedly denoising an\ninitial image of pure noise, conditioned on the degraded input. Contrary to\nconditional denoising formulations, InDI directly proceeds by iteratively\nrestoring the input low-quality image, producing high-quality results on a\nvariety of image restoration tasks, including motion and out-of-focus\ndeblurring, super-resolution, compression artifact removal, and denoising.\n']",Diffusion-based Image Restoration,Image Restoration and Denoising Techniques,Image and Video Processing,Image and Video Processing
498,12,498_gans_generative_gan_autoencoders,"['gans', 'generative', 'gan', 'autoencoders', 'adversarial', 'data', 'ehrs', 'healthcare', 'ehr', 'generate']","['healthcare', 'synthetic', 'records', 'electronic', 'generative', 'health', 'clinical', 'underrepresented', 'visit', 'generation']","['  Synthesizing electronic health records (EHR) data has become a preferred\nstrategy to address data scarcity, improve data quality, and model fairness in\nhealthcare. However, existing approaches for EHR data generation predominantly\nrely on state-of-the-art generative techniques like generative adversarial\nnetworks, variational autoencoders, and language models. These methods\ntypically replicate input visits, resulting in inadequate modeling of temporal\ndependencies between visits and overlooking the generation of time information,\na crucial element in EHR data. Moreover, their ability to learn visit\nrepresentations is limited due to simple linear mapping functions, thus\ncompromising generation quality. To address these limitations, we propose a\nnovel EHR data generation model called EHRPD. It is a diffusion-based model\ndesigned to predict the next visit based on the current one while also\nincorporating time interval estimation. To enhance generation quality and\ndiversity, we introduce a novel time-aware visit embedding module and a\npioneering predictive denoising diffusion probabilistic model (PDDPM).\nAdditionally, we devise a predictive U-Net (PU-Net) to optimize P-DDPM.We\nconduct experiments on two public datasets and evaluate EHRPD from fidelity,\nprivacy, and utility perspectives. The experimental results demonstrate the\nefficacy and utility of the proposed EHRPD in addressing the aforementioned\nlimitations and advancing EHR data generation.\n', '  Electronic health records (EHRs) are a pivotal data source that enables\nnumerous applications in computational medicine, e.g., disease progression\nprediction, clinical trial design, and health economics and outcomes research.\nDespite wide usability, their sensitive nature raises privacy and\nconfidentially concerns, which limit potential use cases. To tackle these\nchallenges, we explore the use of generative models to synthesize artificial,\nyet realistic EHRs. While diffusion-based methods have recently demonstrated\nstate-of-the-art performance in generating other data modalities and overcome\nthe training instability and mode collapse issues that plague previous\nGAN-based approaches, their applications in EHR generation remain\nunderexplored. The discrete nature of tabular medical code data in EHRs poses\nchallenges for high-quality data generation, especially for continuous\ndiffusion models. To this end, we introduce a novel tabular EHR generation\nmethod, EHR-D3PM, which enables both unconditional and conditional generation\nusing the discrete diffusion model. Our experiments demonstrate that EHR-D3PM\nsignificantly outperforms existing generative baselines on comprehensive\nfidelity and utility metrics while maintaining less attribute and membership\nvulnerability risks. Furthermore, we show EHR-D3PM is effective as a data\naugmentation method and enhances performance on downstream tasks when combined\nwith real data.\n', '  Electronic health records (EHR) contain a wealth of biomedical information,\nserving as valuable resources for the development of precision medicine\nsystems. However, privacy concerns have resulted in limited access to\nhigh-quality and large-scale EHR data for researchers, impeding progress in\nmethodological development. Recent research has delved into synthesizing\nrealistic EHR data through generative modeling techniques, where a majority of\nproposed methods relied on generative adversarial networks (GAN) and their\nvariants for EHR synthesis. Despite GAN-based methods attaining\nstate-of-the-art performance in generating EHR data, these approaches are\ndifficult to train and prone to mode collapse. Recently introduced in\ngenerative modeling, diffusion models have established cutting-edge performance\nin image generation, but their efficacy in EHR data synthesis remains largely\nunexplored. In this study, we investigate the potential of diffusion models for\nEHR data synthesis and introduce a novel method, EHRDiff. Through extensive\nexperiments, EHRDiff establishes new state-of-the-art quality for synthetic EHR\ndata, protecting private information in the meanwhile.\n']",Synthetic Electronic Health Records Generation,Synthetic Data Generation and Applications,Artificial Intelligence in Data Generation and Chemical Synthesis,Artificial Intelligence in Data Generation and Chemical Synthesis
499,11,499_cnns_cracknet_convolutional_pavement,"['cnns', 'cracknet', 'convolutional', 'pavement', 'cracks', 'segmentation', 'efficientnetv2', 'pavements', 'yolov5', 'images']","['crack', 'pavement', 'cracks', 'cracking', 'road', 'segmentation', 'damage', 'defect', 'severity', 'asphalt']","[""  Anomalous crack region detection is a typical binary semantic segmentation\ntask, which aims to detect pixels representing cracks on pavement surface\nimages automatically by algorithms. Although existing deep learning-based\nmethods have achieved outcoming results on specific public pavement datasets,\nthe performance would deteriorate dramatically on imbalanced datasets. The\ninput datasets used in such tasks suffer from severely between-class imbalanced\nproblems, hence, it is a core challenge to obtain a robust performance on\ndiverse pavement datasets with generic deep learning models. To address this\nproblem, in this work, we propose a deep learning framework based on\nconditional Generative Adversarial Networks (cGANs) for the anomalous crack\nregion detection tasks at the pixel level. In particular, the proposed\nframework containing a cGANs and a novel auxiliary network is developed to\nenhance and stabilize the generator's performance under two alternative\ntraining stages, when estimating a multiscale probability feature map from\nheterogeneous and imbalanced inputs iteratively. Moreover, several attention\nmechanisms and entropy strategies are incorporated into the cGANs architecture\nand the auxiliary network separately to mitigate further the performance\ndeterioration of model training on severely imbalanced datasets. We implement\nextensive experiments on six accessible pavement datasets. The experimental\nresults from both visual and quantitative evaluation show that the proposed\nframework can achieve state-of-the-art results on these datasets efficiently\nand robustly without acceleration of computation complexity.\n"", '  Due to the varying intensity of pavement cracks, the complexity of\ntopological structure, and the noise of texture background, image\nclassification for asphalt pavement cracking has proven to be a challenging\nproblem. Fatigue cracking, also known as alligator cracking, is one of the\ncommon distresses of asphalt pavement. It is thus important to detect and\nmonitor the condition of alligator cracking on roadway pavements. Most research\nin this area has typically focused on pixel-level detection of cracking using\nlimited datasets. A novel deep convolutional neural network that can achieve\ntwo objectives is proposed. The first objective of the proposed neural network\nis to classify presence of fatigue cracking based on pavement surface images.\nThe second objective is to classify the fatigue cracking severity level based\non the Distress Identification Manual (DIM) standard. In this paper, a databank\nof 4484 high-resolution pavement surface images is established in which images\nare taken locally in the Town of Blacksburg, Virginia, USA. In the data\npre-preparation, over 4000 images are labeled into 4 categories manually\naccording to DIM standards. A four-layer convolutional neural network model is\nthen built to achieve the goal of classification of images by pavement crack\nseverity category. The trained model reached the highest accuracy among all\nexisting methods. After only 30 epochs of training, the model achieved a crack\nexistence classification accuracy of 96.23% and a severity level classification\naccuracy of 96.74%. After 20 epochs of training, the model achieved a pavement\nmarking presence classification accuracy of 97.64%.\n', ""  Cracks pose safety risks to infrastructure and cannot be overlooked. The\nprevailing structures in existing crack segmentation networks predominantly\nconsist of CNNs or Transformers. However, CNNs exhibit a deficiency in global\nmodeling capability, hindering the representation to entire crack features.\nTransformers can capture long-range dependencies but suffer from high and\nquadratic complexity. Recently, Mamba has garnered extensive attention due to\nits linear spatial and computational complexity and its powerful global\nperception. This study explores the representation capabilities of Mamba to\ncrack features. Specifically, this paper uncovers the connection between Mamba\nand the attention mechanism, providing a profound insight, an attention\nperspective, into interpreting Mamba and devising a novel Mamba module\nfollowing the principles of attention blocks, namely CrackMamba. We compare\nCrackMamba with the most prominent visual Mamba modules, Vim and Vmamba, on two\ndatasets comprising asphalt pavement and concrete pavement cracks, and steel\ncracks, respectively. The quantitative results show that CrackMamba stands out\nas the sole Mamba block consistently enhancing the baseline model's performance\nacross all evaluation measures, while reducing its parameters and computational\ncosts. Moreover, this paper substantiates that Mamba can achieve global\nreceptive fields through both theoretical analysis and visual interpretability.\nThe discoveries of this study offer a dual contribution. First, as a\nplug-and-play and simple yet effective Mamba module, CrackMamba exhibits\nimmense potential for integration into various crack segmentation models.\nSecond, the proposed innovative Mamba design concept, integrating Mamba with\nthe attention mechanism, holds significant reference value for all Mamba-based\ncomputer vision models, not limited to crack segmentation networks, as\ninvestigated in this study.\n""]",Pavement Crack Detection and Segmentation,Pavement Crack Detection and Analysis using Deep Learning Techniques,Deep Learning Applications in Engineering and Computer Vision,Deep Learning Applications in Engineering and Computer Vision
500,11,500_ai_artmaking_deviantart_creators,"['ai', 'artmaking', 'deviantart', 'creators', 'artistic', 'artists', 'arts', 'copyright', 'imagery', 'media']","['media', 'intermediality', 'content', 'artists', 'platforms', 'artistic', 'generative', 'labour', 'creative', 'theft']","[""  Text-to-video generative AI models such as Sora OpenAI have the potential to\ndisrupt multiple industries. In this paper, we report a qualitative social\nmedia analysis aiming to uncover people's perceived impact of and concerns\nabout Sora's integration. We collected and analyzed comments (N=292) under\npopular posts about Sora-generated videos, comparison between Sora videos and\nMidjourney images, and artists' complaints about copyright infringement by\nGenerative AI. We found that people were most concerned about Sora's impact on\ncontent creation-related industries. Emerging governance challenges included\nthe for-profit nature of OpenAI, the blurred boundaries between real and fake\ncontent, human autonomy, data privacy, copyright issues, and environmental\nimpact. Potential regulatory solutions proposed by people included law-enforced\nlabeling of AI content and AI literacy education for the public. Based on the\nfindings, we discuss the importance of gauging people's tech perceptions early\nand propose policy recommendations to regulate Sora before its public release.\n"", ""  2023 was the year the world woke up to generative AI, and 2024 is the year\npolicymakers are responding more firmly. Importantly, this policy momentum is\ntaking place alongside real world creation and distribution of synthetic media.\nSocial media platforms, news organizations, dating apps, image generation\ncompanies, and more are already navigating a world of AI-generated visuals and\nsounds, already changing hearts and minds, as policymakers try to catch up.\nHow, then, can AI governance capture the complexity of the synthetic media\nlandscape? How can it attend to synthetic media's myriad uses, ranging from\nstorytelling to privacy preservation, to deception, fraud, and defamation,\ntaking into account the many stakeholders involved in its development,\ncreation, and distribution? And what might it mean to govern synthetic media in\na manner that upholds the truth while bolstering freedom of expression? What\nfollows is the first known collection of diverse examples of the implementation\nof synthetic media governance that responds to these questions, specifically\nthrough Partnership on AI's (PAI) Responsible Practices for Synthetic Media - a\nvoluntary, normative Framework for creating, distributing, and building\ntechnology for synthetic media responsibly, launched in February 2023. In this\npaper, we present a case bank of real world examples that help operationalize\nthe Framework - highlighting areas synthetic media governance can be applied,\naugmented, expanded, and refined for use, in practice. Read together, the cases\nemphasize distinct elements of AI policymaking and seven emergent best\npractices supporting transparency, safety, expression, and digital dignity\nonline: consent, disclosure, and differentiation between harmful and creative\nuse cases.\n"", '  The article presents some current observations (as of April 10, 2024) on the\nintegration of AI-generated images within processes of media convergence. It\ndraws on two different concepts of intermediality. Primary intermediality\nconcepts are motivated by the object when a new type of technology develops the\npotential to become socially relevant as a media form and thus a socially,\npolitically, or culturally important communicative factor. Due to their\nuncertain \'measurements\' within the wider media ecology, however, the new,\nstill potential media form appears hybrid. The ""inter-"" or ""between-"" of this\ninitial intermediality moment thus refers to the questionable ""site"" and the\nquestionable description of the potential media form between already existing\ntechnologies and cultural forms and their conceptual measurements. For\nsecondary concepts of intermediality, in contrast, it can be assumed that the\nboundaries of media forms and their application have already been drawn and are\nreasonably undisputed. This then raises the question of intentional and staged\nreferences to AI imagery within other media forms and pictures. The article\ndiscusses indicators of both intermediality moments using current examples and\ncontroversies surrounding AI images. The thesis is that there can be no talk of\na seamless \'integration\' of AI images into the wider media landscape at the\nmoment (within films, comic books, or video games, for example) - as one of\ncountless other image production techniques - and that the medial \'site\' of AI\nimage circulation - at least where it is not a matter of deception, but rather\ntheir conscious use as AI images - especially in social media communication and\nin fan cultures, but with repercussions for the more general media ecology and\nimage interpretation, insofar as the suspicion that an image could be\nAI-generated is now increasingly present as a ""hermeneutics of suspicion"".\n']",AI-Generated Media and Its Societal Impact,Artificial Intelligence in Creative Industries,Artificial Intelligence in Creative Industries,Artificial Intelligence in Creative Industries
501,11,501_changepoints_changepoint_observations_detection,"['changepoints', 'changepoint', 'observations', 'detection', 'detecting', 'cusum', 'detect', 'outliers', 'changes', 'cumulative']","['change', 'point', 'changes', 'detection', 'changepoint', 'abrupt', 'streams', 'points', 'changepoints', 'cumulative']","[""  The objective of change point detection is to identify abrupt changes at\npotentially multiple points within a data sequence. This task is particularly\nchallenging in the online setting where various types of changes can occur,\nincluding shifts in both the marginal and joint distributions of the data. This\npaper tackles these challenges by sequentially tracking correlation matrices on\nthe Riemannian geometry, where the geodesic distances accurately capture the\ndevelopment of correlations. We propose Rio-CPD, a non-parametric\ncorrelation-aware online change point detection framework that combines the\nRiemannian geometry of the manifold of symmetric positive definite matrices and\nthe cumulative sum statistic (CUSUM) for detecting change points. Rio-CPD\nenhances CUSUM by computing the geodesic distance from present observations to\nthe Fr\\'echet mean of previous observations. With careful choice of metrics\nequipped to the Riemannian geometry, Rio-CPD is simple and computationally\nefficient. Experimental results on both synthetic and real-world datasets\ndemonstrate that Rio-CPD outperforms existing methods in detection accuracy and\nefficiency.\n"", '  In the problem of quickest change detection (QCD), a change occurs at some\nunknown time in the distribution of a sequence of independent observations.\nThis work studies a QCD problem where the change is either a bad change, which\nwe aim to detect, or a confusing change, which is not of our interest. Our\nobjective is to detect a bad change as quickly as possible while avoiding\nraising a false alarm for pre-change or a confusing change. We identify a\nspecific set of pre-change, bad change, and confusing change distributions that\npose challenges beyond the capabilities of standard Cumulative Sum (CuSum)\nprocedures. Proposing novel CuSum-based detection procedures, S-CuSum and\nJ-CuSum, leveraging two CuSum statistics, we offer solutions applicable across\nall kinds of pre-change, bad change, and confusing change distributions. For\nboth S-CuSum and J-CuSum, we provide analytical performance guarantees and\nvalidate them by numerical results. Furthermore, both procedures are\ncomputationally efficient as they only require simple recursive updates.\n', '  Change-point detection, detecting an abrupt change in the data distribution\nfrom sequential data, is a fundamental problem in statistics and machine\nlearning. CUSUM is a popular statistical method for online change-point\ndetection due to its efficiency from recursive computation and constant memory\nrequirement, and it enjoys statistical optimality. CUSUM requires knowing the\nprecise pre- and post-change distribution. However, post-change distribution is\nusually unknown a priori since it represents anomaly and novelty. Classic CUSUM\ncan perform poorly when there is a model mismatch with actual data. While\nlikelihood ratio-based methods encounter challenges facing high dimensional\ndata, neural networks have become an emerging tool for change-point detection\nwith computational efficiency and scalability. In this paper, we introduce a\nneural network CUSUM (NN-CUSUM) for online change-point detection. We also\npresent a general theoretical condition when the trained neural networks can\nperform change-point detection and what losses can achieve our goal. We further\nextend our analysis by combining it with the Neural Tangent Kernel theory to\nestablish learning guarantees for the standard performance metrics, including\nthe average run length (ARL) and expected detection delay (EDD). The strong\nperformance of NN-CUSUM is demonstrated in detecting change-point in\nhigh-dimensional data using both synthetic and real-world data.\n']",Change Point Detection Methods,Change Detection and Analysis in Remote Sensing and Time Series Data,Remote Sensing and Space Data Analysis,Remote Sensing and Space Data Analysis
502,11,502_depressive_depression_depressed_speech,"['depressive', 'depression', 'depressed', 'speech', 'audio', 'distress', 'suicide', 'recordings', 'features', 'psychiatric']","['depression', 'suicide', 'speech', 'acoustic', 'mental', 'detection', 'symptoms', 'health', 'risk', 'depressed']","[""  Depression can significantly impact many aspects of an individual's life,\nincluding their personal and social functioning, academic and work performance,\nand overall quality of life. Many researchers within the field of affective\ncomputing are adopting deep learning technology to explore potential patterns\nrelated to the detection of depression. However, because of subjects' privacy\nprotection concerns, that data in this area is still scarce, presenting a\nchallenge for the deep discriminative models used in detecting depression. To\nnavigate these obstacles, a large-scale multimodal vlog dataset (LMVD), for\ndepression recognition in the wild is built. In LMVD, which has 1823 samples\nwith 214 hours of the 1475 participants captured from four multimedia platforms\n(Sina Weibo, Bilibili, Tiktok, and YouTube). A novel architecture termed\nMDDformer to learn the non-verbal behaviors of individuals is proposed.\nExtensive validations are performed on the LMVD dataset, demonstrating superior\nperformance for depression detection. We anticipate that the LMVD will\ncontribute a valuable function to the depression detection community. The data\nand code will released at the link: https://github.com/helang818/LMVD/.\n"", '  Depression is a critical concern in global mental health, prompting extensive\nresearch into AI-based detection methods. Among various AI technologies, Large\nLanguage Models (LLMs) stand out for their versatility in mental healthcare\napplications. However, their primary limitation arises from their exclusive\ndependence on textual input, which constrains their overall capabilities.\nFurthermore, the utilization of LLMs in identifying and analyzing depressive\nstates is still relatively untapped. In this paper, we present an innovative\napproach to integrating acoustic speech information into the LLMs framework for\nmultimodal depression detection. We investigate an efficient method for\ndepression detection by integrating speech signals into LLMs utilizing Acoustic\nLandmarks. By incorporating acoustic landmarks, which are specific to the\npronunciation of spoken words, our method adds critical dimensions to text\ntranscripts. This integration also provides insights into the unique speech\npatterns of individuals, revealing the potential mental states of individuals.\nEvaluations of the proposed approach on the DAIC-WOZ dataset reveal\nstate-of-the-art results when compared with existing Audio-Text baselines. In\naddition, this approach is not only valuable for the detection of depression\nbut also represents a new perspective in enhancing the ability of LLMs to\ncomprehend and process speech signals.\n', '  Current automatic depression detection systems provide predictions directly\nwithout relying on the individual symptoms/items of depression as denoted in\nthe clinical depression rating scales. In contrast, clinicians assess each item\nin the depression rating scale in a clinical setting, thus implicitly providing\na more detailed rationale for a depression diagnosis. In this work, we make a\nfirst step towards using the acoustic features of speech to predict individual\nitems of the depression rating scale before obtaining the final depression\nprediction. For this, we use convolutional (CNN) and recurrent (long short-term\nmemory (LSTM)) neural networks. We consider different approaches to learning\nthe temporal context of speech. Further, we analyze two variants of voting\nschemes for individual item prediction and depression detection. We also\ninclude an animated visualization that shows an example of item prediction over\ntime as the speech progresses.\n']",Depression Detection using Multimodal Data and AI,Depression Detection using AI and Multimodal Data,Human Behavior Analysis with AI and Multimodal Data,Human Behavior Analysis with AI and Multimodal Data
503,11,503_throughput_streaming_chatbot_batching,"['throughput', 'streaming', 'chatbot', 'batching', 'workloads', 'chatbots', 'serving', 'scheduling', 'concurrent', 'sglang']","['request', 'throughput', 'latency', 'requests', 'token', 'eloquent', 'streaming', 'batching', 'prefill', 'stalls']","['  The advent of large language models (LLMs) has transformed text-based\nservices, enabling capabilities ranging from real-time translation to AI-driven\nchatbots. However, existing serving systems primarily focus on optimizing\nserver-side aggregate metrics like token generation throughput, ignoring\nindividual user experience with streamed text. As a result, under high and/or\nbursty load, a significant number of users can receive unfavorable service\nquality or poor Quality-of-Experience (QoE). In this paper, we first formally\ndefine QoE of text streaming services, where text is delivered incrementally\nand interactively to users, by considering the end-to-end token delivery\nprocess throughout the entire interaction with the user. Thereafter, we propose\nAndes, a QoE-aware serving system that enhances user experience for LLM-enabled\ntext streaming services. At its core, Andes strategically allocates contended\nGPU resources among multiple requests over time to optimize their QoE. Our\nevaluations demonstrate that, compared to the state-of-the-art LLM serving\nsystems like vLLM, Andes improves the average QoE by up to 3.2$\\times$ under\nhigh request rate, or alternatively, it attains up to 1.6$\\times$ higher\nrequest rate while preserving high QoE.\n', '  High-demand LLM inference services (e.g., ChatGPT and BARD) support a wide\nrange of requests from short chat conversations to long document reading. To\nensure that all client requests are processed fairly, most major LLM inference\nservices have request rate limits, to ensure that no client can dominate the\nrequest queue. However, this rudimentary notion of fairness also results in\nunder-utilization of the resources and poor client experience when there is\nspare capacity. While there is a rich literature on fair scheduling, serving\nLLMs presents new challenges due to their unpredictable request lengths and\ntheir unique batching characteristics on parallel accelerators. This paper\nintroduces the definition of LLM serving fairness based on a cost function that\naccounts for the number of input and output tokens processed. To achieve\nfairness in serving, we propose a novel scheduling algorithm, the Virtual Token\nCounter (VTC), a fair scheduler based on the continuous batching mechanism. We\nprove a 2x tight upper bound on the service difference between two backlogged\nclients, adhering to the requirement of work-conserving. Through extensive\nexperiments, we demonstrate the superior performance of VTC in ensuring\nfairness, especially in contrast to other baseline methods, which exhibit\nshortcomings under various conditions. The reproducible code is available at\nhttps://github.com/Ying1123/VTC-artifact\n', '  Each LLM serving request goes through two phases. The first is prefill which\nprocesses the entire input prompt and produces the first output token and the\nsecond is decode which generates the rest of output tokens, one-at-a-time.\nPrefill iterations have high latency but saturate GPU compute due to parallel\nprocessing of the input prompt. In contrast, decode iterations have low latency\nbut also low compute utilization because a decode iteration processes only a\nsingle token per request. This makes batching highly effective for decodes and\nconsequently for overall throughput. However, batching multiple requests leads\nto an interleaving of prefill and decode iterations which makes it challenging\nto achieve both high throughput and low latency.\n  We introduce an efficient LLM inference scheduler, Sarathi-Serve, to address\nthis throughput-latency tradeoff. Sarathi-Serve introduces chunked-prefills\nwhich splits a prefill request into near equal sized chunks and creates\nstall-free schedules that adds new requests in a batch without pausing ongoing\ndecodes. Stall-free scheduling unlocks the opportunity to improve throughput\nwith large batch sizes while minimizing the effect of batching on latency.\nFurthermore, uniform batches in Sarathi-Serve ameliorate the imbalance between\niterations resulting in minimal pipeline bubbles.\n  Our techniques yield significant improvements in inference performance across\nmodels and hardware under tail latency constraints. For Mistral-7B on single\nA100 GPUs, we achieve 2.6x higher serving capacity and up to 3.7x higher\nserving capacity for the Yi-34B model on two A100 GPUs as compared to vLLM.\nWhen used with pipeline parallelism on Falcon-180B, Sarathi-Serve provides up\nto 5.6x gain in the end-to-end serving capacity. The source code for\nSarathi-Serve is available at https://github.com/microsoft/sarathi-serve.\n']",Optimizing Large Language Model Serving Systems,Optimization and Efficiency of Large Language Models,Large Language Models,Large Language Models
503,11,503_throughput_streaming_chatbot_batching,"['throughput', 'streaming', 'chatbot', 'batching', 'workloads', 'chatbots', 'serving', 'scheduling', 'concurrent', 'sglang']","['request', 'throughput', 'latency', 'requests', 'token', 'eloquent', 'streaming', 'batching', 'prefill', 'stalls']","['  The advent of large language models (LLMs) has transformed text-based\nservices, enabling capabilities ranging from real-time translation to AI-driven\nchatbots. However, existing serving systems primarily focus on optimizing\nserver-side aggregate metrics like token generation throughput, ignoring\nindividual user experience with streamed text. As a result, under high and/or\nbursty load, a significant number of users can receive unfavorable service\nquality or poor Quality-of-Experience (QoE). In this paper, we first formally\ndefine QoE of text streaming services, where text is delivered incrementally\nand interactively to users, by considering the end-to-end token delivery\nprocess throughout the entire interaction with the user. Thereafter, we propose\nAndes, a QoE-aware serving system that enhances user experience for LLM-enabled\ntext streaming services. At its core, Andes strategically allocates contended\nGPU resources among multiple requests over time to optimize their QoE. Our\nevaluations demonstrate that, compared to the state-of-the-art LLM serving\nsystems like vLLM, Andes improves the average QoE by up to 3.2$\\times$ under\nhigh request rate, or alternatively, it attains up to 1.6$\\times$ higher\nrequest rate while preserving high QoE.\n', '  High-demand LLM inference services (e.g., ChatGPT and BARD) support a wide\nrange of requests from short chat conversations to long document reading. To\nensure that all client requests are processed fairly, most major LLM inference\nservices have request rate limits, to ensure that no client can dominate the\nrequest queue. However, this rudimentary notion of fairness also results in\nunder-utilization of the resources and poor client experience when there is\nspare capacity. While there is a rich literature on fair scheduling, serving\nLLMs presents new challenges due to their unpredictable request lengths and\ntheir unique batching characteristics on parallel accelerators. This paper\nintroduces the definition of LLM serving fairness based on a cost function that\naccounts for the number of input and output tokens processed. To achieve\nfairness in serving, we propose a novel scheduling algorithm, the Virtual Token\nCounter (VTC), a fair scheduler based on the continuous batching mechanism. We\nprove a 2x tight upper bound on the service difference between two backlogged\nclients, adhering to the requirement of work-conserving. Through extensive\nexperiments, we demonstrate the superior performance of VTC in ensuring\nfairness, especially in contrast to other baseline methods, which exhibit\nshortcomings under various conditions. The reproducible code is available at\nhttps://github.com/Ying1123/VTC-artifact\n', '  Each LLM serving request goes through two phases. The first is prefill which\nprocesses the entire input prompt and produces the first output token and the\nsecond is decode which generates the rest of output tokens, one-at-a-time.\nPrefill iterations have high latency but saturate GPU compute due to parallel\nprocessing of the input prompt. In contrast, decode iterations have low latency\nbut also low compute utilization because a decode iteration processes only a\nsingle token per request. This makes batching highly effective for decodes and\nconsequently for overall throughput. However, batching multiple requests leads\nto an interleaving of prefill and decode iterations which makes it challenging\nto achieve both high throughput and low latency.\n  We introduce an efficient LLM inference scheduler, Sarathi-Serve, to address\nthis throughput-latency tradeoff. Sarathi-Serve introduces chunked-prefills\nwhich splits a prefill request into near equal sized chunks and creates\nstall-free schedules that adds new requests in a batch without pausing ongoing\ndecodes. Stall-free scheduling unlocks the opportunity to improve throughput\nwith large batch sizes while minimizing the effect of batching on latency.\nFurthermore, uniform batches in Sarathi-Serve ameliorate the imbalance between\niterations resulting in minimal pipeline bubbles.\n  Our techniques yield significant improvements in inference performance across\nmodels and hardware under tail latency constraints. For Mistral-7B on single\nA100 GPUs, we achieve 2.6x higher serving capacity and up to 3.7x higher\nserving capacity for the Yi-34B model on two A100 GPUs as compared to vLLM.\nWhen used with pipeline parallelism on Falcon-180B, Sarathi-Serve provides up\nto 5.6x gain in the end-to-end serving capacity. The source code for\nSarathi-Serve is available at https://github.com/microsoft/sarathi-serve.\n']",Optimizing Large Language Model Serving Systems,Optimization and Efficiency of Large Language Models,Large Language Models,Large Language Models
504,11,504_policies_policy_estimators_unbiasedness,"['policies', 'policy', 'estimators', 'unbiasedness', 'estimation', 'unbiasedly', 'unbiased', 'evaluation', 'estimate', 'estimator']","['estimator', 'logged', 'policy', 'estimators', 'policies', 'variance', 'risk', 'propensity', 'counterfactual', 'evaluation']","[""  Offline policy evaluation (OPE) allows us to evaluate and estimate a new\nsequential decision-making policy's performance by leveraging historical\ninteraction data collected from other policies. Evaluating a new policy online\nwithout a confident estimate of its performance can lead to costly, unsafe, or\nhazardous outcomes, especially in education and healthcare. Several OPE\nestimators have been proposed in the last decade, many of which have\nhyperparameters and require training. Unfortunately, choosing the best OPE\nalgorithm for each task and domain is still unclear. In this paper, we propose\na new algorithm that adaptively blends a set of OPE estimators given a dataset\nwithout relying on an explicit selection using a statistical procedure. We\nprove that our estimator is consistent and satisfies several desirable\nproperties for policy evaluation. Additionally, we demonstrate that when\ncompared to alternative approaches, our estimator can be used to select\nhigher-performing policies in healthcare and robotics. Our work contributes to\nimproving ease of use for a general-purpose, estimator-agnostic, off-policy\nevaluation framework for offline RL.\n"", '  Off-Policy Evaluation (OPE) aims to assess the effectiveness of\ncounterfactual policies using only offline logged data and is often used to\nidentify the top-k promising policies for deployment in online A/B tests.\nExisting evaluation metrics for OPE estimators primarily focus on the\n""accuracy"" of OPE or that of downstream policy selection, neglecting\nrisk-return tradeoff in the subsequent online policy deployment. To address\nthis issue, we draw inspiration from portfolio evaluation in finance and\ndevelop a new metric, called SharpeRatio@k, which measures the risk-return\ntradeoff of policy portfolios formed by an OPE estimator under varying online\nevaluation budgets (k). We validate our metric in two example scenarios,\ndemonstrating its ability to effectively distinguish between low-risk and\nhigh-risk estimators and to accurately identify the most efficient one.\nEfficiency of an estimator is characterized by its capability to form the most\nadvantageous policy portfolios, maximizing returns while minimizing risks\nduring online deployment, a nuance that existing metrics typically overlook. To\nfacilitate a quick, accurate, and consistent evaluation of OPE via\nSharpeRatio@k, we have also integrated this metric into an open-source\nsoftware, SCOPE-RL (https://github.com/hakuhodo-technologies/scope-rl).\nEmploying SharpeRatio@k and SCOPE-RL, we conduct comprehensive benchmarking\nexperiments on various estimators and RL tasks, focusing on their risk-return\ntradeoff. These experiments offer several interesting directions and\nsuggestions for future OPE research.\n', '  The Off-Policy Evaluation (OPE) problem consists of evaluating the\nperformance of counterfactual policies with data collected by another one. This\nproblem is of utmost importance for various application domains, e.g.,\nrecommendation systems, medical treatments, and many others. To solve the OPE\nproblem, we resort to estimators, which aim to estimate in the most accurate\nway possible the performance that the counterfactual policies would have had if\nthey were deployed in place of the logging policy. In the literature, several\nestimators have been developed, all with different characteristics and\ntheoretical guarantees. Therefore, there is no dominant estimator, and each\nestimator may be the best one for different OPE problems, depending on the\ncharacteristics of the dataset at hand. While the selection of the estimator is\na crucial choice for an accurate OPE, this problem has been widely overlooked\nin the literature. We propose an automated data-driven OPE estimator selection\nmethod based on machine learning. In particular, the core idea we propose in\nthis paper is to create several synthetic OPE tasks and use a machine learning\nmodel trained to predict the best estimator for those synthetic tasks. We\nempirically show how our method is able to generalize to unseen tasks and make\na better estimator selection compared to a baseline method on several\nreal-world datasets, with a computational cost significantly lower than the one\nof the baseline.\n']",Off-Policy Evaluation Estimators,State Estimation and System Identification,Industrial Automation and Control Systems,Industrial Automation and Control Systems
505,11,505_privacy_private_privately_memorization,"['privacy', 'private', 'privately', 'memorization', 'secure', 'leak', 'protect', 'serializing', 'examples', 'context']","['private', 'privacy', 'instructions', 'tabular', 'secure', 'differential', 'prompts', 'tuning', 'prompt', 'untrusted']","['  We study the problem of in-context learning (ICL) with large language models\n(LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leak\nor regurgitate the private examples demonstrated in the prompt. We propose a\nnovel algorithm that generates synthetic few-shot demonstrations from the\nprivate dataset with formal differential privacy (DP) guarantees, and show\nempirically that it can achieve effective ICL. We conduct extensive experiments\non standard benchmarks and compare our algorithm with non-private ICL and\nzero-shot solutions. Our results demonstrate that our algorithm can achieve\ncompetitive performance with strong privacy levels. These results open up new\npossibilities for ICL with privacy protection for a broad range of\napplications.\n', ""  Large Language Models (LLMs) have emerged as dominant tools for various\ntasks, particularly when tailored for a specific target by prompt tuning.\nNevertheless, concerns surrounding data privacy present obstacles due to the\ntuned prompts' dependency on sensitive private information. A practical\nsolution is to host a local LLM and optimize a soft prompt privately using\ndata. Yet, hosting a local model becomes problematic when model ownership is\nprotected. Alternative methods, like sending data to the model's provider for\ntraining, intensify these privacy issues facing an untrusted provider. In this\npaper, we present a novel solution called Differentially-Private Offsite Prompt\nTuning (DP-OPT) to address this challenge. Our approach involves tuning a\ndiscrete prompt on the client side and then applying it to the desired cloud\nmodels. We demonstrate that prompts suggested by LLMs themselves can be\ntransferred without compromising performance significantly. To ensure that the\nprompts do not leak private information, we introduce the first private prompt\ngeneration mechanism, by a differentially-private (DP) ensemble of in-context\nlearning with private demonstrations. With DP-OPT, generating\nprivacy-preserving prompts by Vicuna-7b can yield competitive performance\ncompared to non-private in-context learning on GPT3.5 or local private prompt\ntuning. Codes are available at https://github.com/VITA-Group/DP-OPT .\n"", '  In-context learning (ICL) enables large language models (LLMs) to adapt to\nnew tasks by conditioning on demonstrations of question-answer pairs and it has\nbeen shown to have comparable performance to costly model retraining and\nfine-tuning. Recently, ICL has been extended to allow tabular data to be used\nas demonstration examples by serializing individual records into natural\nlanguage formats. However, it has been shown that LLMs can leak information\ncontained in prompts, and since tabular data often contain sensitive\ninformation, understanding how to protect the underlying tabular data used in\nICL is a critical area of research. This work serves as an initial\ninvestigation into how to use differential privacy (DP) -- the long-established\ngold standard for data privacy and anonymization -- to protect tabular data\nused in ICL. Specifically, we investigate the application of DP mechanisms for\nprivate tabular ICL via data privatization prior to serialization and\nprompting. We formulate two private ICL frameworks with provable privacy\nguarantees in both the local (LDP-TabICL) and global (GDP-TabICL) DP scenarios\nvia injecting noise into individual records or group statistics, respectively.\nWe evaluate our DP-based frameworks on eight real-world tabular datasets and\nacross multiple ICL and DP settings. Our evaluations show that DP-based ICL can\nprotect the privacy of the underlying tabular data while achieving comparable\nperformance to non-LLM baselines, especially under high privacy regimes.\n']",Private In-Context Learning with Differential Privacy,Differential Privacy in Machine Learning,Machine Learning and Data Privacy,Machine Learning and Data Privacy
506,11,506_ecommerce_retailing_retail_customers,"['ecommerce', 'retailing', 'retail', 'customers', 'purchases', 'marketing', 'promotions', 'customer', 'willingness', 'categories']","['customer', 'customers', 'ecommerce', 'marketing', 'choice', 'utility', 'utilities', 'retail', 'telecallers', 'business']","[""  Predicting human decision-making under risk and uncertainty represents a\nquintessential challenge that spans economics, psychology, and related\ndisciplines. Despite decades of research effort, no model can be said to\naccurately describe and predict human choice even for the most stylized tasks\nlike choice between lotteries. Here, we introduce BEAST Gradient Boosting\n(BEAST-GB), a novel hybrid model that synergizes behavioral theories,\nspecifically the model BEAST, with machine learning techniques. First, we show\nthe effectiveness of BEAST-GB by describing CPC18, an open competition for\nprediction of human decision making under risk and uncertainty, in which\nBEAST-GB won. Second, we show that it achieves state-of-the-art performance on\nthe largest publicly available dataset of human risky choice, outperforming\npurely data-driven neural networks, indicating the continued relevance of BEAST\ntheoretical insights in the presence of large data. Third, we demonstrate\nBEAST-GB's superior predictive power in an ensemble of choice experiments in\nwhich the BEAST model alone falters, underscoring the indispensable role of\nmachine learning in interpreting complex idiosyncratic behavioral data.\nFinally, we show BEAST-GB also displays robust domain generalization\ncapabilities as it effectively predicts choice behavior in new experimental\ncontexts that it was not trained on. These results confirm the potency of\ncombining domain-specific theoretical frameworks with machine learning,\nunderscoring a methodological advance with broad implications for modeling\ndecisions in diverse environments.\n"", '  Recently, peoples awareness of online purchases has significantly risen. This\nhas given rise to online retail platforms and the need for a better\nunderstanding of customer purchasing behaviour. Retail companies are pressed\nwith the need to deal with a high volume of customer purchases, which requires\nsophisticated approaches to perform more accurate and efficient customer\nsegmentation. Customer segmentation is a marketing analytical tool that aids\ncustomer-centric service and thus enhances profitability. In this paper, we aim\nto develop a customer segmentation model to improve decision-making processes\nin the retail market industry. To achieve this, we employed a UK-based online\nretail dataset obtained from the UCI machine learning repository. The retail\ndataset consists of 541,909 customer records and eight features. Our study\nadopted the RFM (recency, frequency, and monetary) framework to quantify\ncustomer values. Thereafter, we compared several state-of-the-art (SOTA)\nclustering algorithms, namely, K-means clustering, the Gaussian mixture model\n(GMM), density-based spatial clustering of applications with noise (DBSCAN),\nagglomerative clustering, and balanced iterative reducing and clustering using\nhierarchies (BIRCH). The results showed the GMM outperformed other approaches,\nwith a Silhouette Score of 0.80.\n', ""  Problem definition. In retailing, discrete choice models (DCMs) are commonly\nused to capture the choice behavior of customers when offered an assortment of\nproducts. When estimating DCMs using transaction data, flexible models (such as\nmachine learning models or nonparametric models) are typically not\ninterpretable and hard to estimate, while tractable models (such as the\nmultinomial logit model) tend to misspecify the complex behavior represeted in\nthe data. Methodology/results. In this study, we use a forest of binary\ndecision trees to represent DCMs. This approach is based on random forests, a\npopular machine learning algorithm. The resulting model is interpretable: the\ndecision trees can explain the decision-making process of customers during the\npurchase. We show that our approach can predict the choice probability of any\nDCM consistently and thus never suffers from misspecification. Moreover, our\nalgorithm predicts assortments unseen in the training data. The mechanism and\nerrors can be theoretically analyzed. We also prove that the random forest can\nrecover preference rankings of customers thanks to the splitting criterion such\nas the Gini index and information gain ratio. Managerial implications. The\nframework has unique practical advantages. It can capture customers' behavioral\npatterns such as irrationality or sequential searches when purchasing a\nproduct. It handles nonstandard formats of training data that result from\naggregation. It can measure product importance based on how frequently a random\ncustomer would make decisions depending on the presence of the product. It can\nalso incorporate price information and customer features. Our numerical\nexperiments using synthetic and real data show that using random forests to\nestimate customer choices can outperform existing methods.\n""]",Customer Behavior in Retail and E-commerce,Business and Marketing Analytics,Business and Marketing Analytics,Business and Marketing Analytics
507,11,507_cultural_cultures_culturally_culturalvqa,"['cultural', 'cultures', 'culturally', 'culturalvqa', 'culture', 'cultureadapt', 'multicultural', 'recognizing', 'captioning', 'multilingual']","['cultural', 'culture', 'cultures', 'visual', 'vision', 'food', 'captions', 'understanding', 'countries', 'elements']","[""  To create culturally inclusive vision-language models (VLMs), the foremost\nrequirement is developing a test benchmark that can diagnose the models'\nability to respond to questions reflecting cultural elements. This paper\naddresses the necessity for such benchmarks, noting that existing research has\nrelied on human annotators' manual efforts, which impedes diversity and\nefficiency. We propose a semi-automated pipeline for constructing cultural VLM\nbenchmarks to enhance diversity and efficiency. This pipeline leverages\nhuman-VLM collaboration, where VLMs generate questions based on guidelines,\nhuman-annotated examples, and image-wise relevant knowledge, which are then\nreviewed by native speakers for quality and cultural relevance. The\neffectiveness of our adaptable pipeline is demonstrated through a specific\napplication: creating a dataset tailored to Korean culture, dubbed K-Viscuit.\nThe resulting benchmark features two types of questions: Type 1 questions\nmeasure visual recognition abilities, while Type 2 assess fine-grained visual\nreasoning skills. This ensures a thorough diagnosis of VLM models across\nvarious aspects. Our evaluation using K-Viscuit revealed that open-source\nmodels notably lag behind proprietary models in understanding Korean culture,\nhighlighting areas for improvement. We provided diverse analyses of VLM\nperformance across different cultural aspects. Besides, we explored the\npotential of incorporating external knowledge retrieval to enhance the\ngeneration process, suggesting future directions for improving cultural\ninterpretation ability of VLMs. Our dataset and code will be made publicly\navailable.\n"", ""  Foundation models and vision-language pre-training have notably advanced\nVision Language Models (VLMs), enabling multimodal processing of visual and\nlinguistic data. However, their performance has been typically assessed on\ngeneral scene understanding - recognizing objects, attributes, and actions -\nrather than cultural comprehension. This study introduces CulturalVQA, a visual\nquestion-answering benchmark aimed at assessing VLM's geo-diverse cultural\nunderstanding. We curate a collection of 2,378 image-question pairs with 1-5\nanswers per question representing cultures from 11 countries across 5\ncontinents. The questions probe understanding of various facets of culture such\nas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on\nCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of\ncultural understanding across regions, with strong cultural understanding\ncapabilities for North America while significantly lower performance for\nAfrica. We observe disparity in their performance across cultural facets too,\nwith clothing, rituals, and traditions seeing higher performances than food and\ndrink. These disparities help us identify areas where VLMs lack cultural\nunderstanding and demonstrate the potential of CulturalVQA as a comprehensive\nevaluation set for gauging VLM progress in understanding diverse cultures.\n"", '  Image Captioning generates descriptive sentences from images using\nVision-Language Pre-trained models (VLPs) such as BLIP, which has improved\ngreatly. However, current methods lack the generation of detailed descriptive\ncaptions for the cultural elements depicted in the images, such as the\ntraditional clothing worn by people from Asian cultural groups. In this paper,\nwe propose a new framework, \\textbf{Culturally-aware Image Captioning (CIC)},\nthat generates captions and describes cultural elements extracted from cultural\nvisual elements in images representing cultures. Inspired by methods combining\nvisual modality and Large Language Models (LLMs) through appropriate prompts,\nour framework (1) generates questions based on cultural categories from images,\n(2) extracts cultural visual elements from Visual Question Answering (VQA)\nusing generated questions, and (3) generates culturally-aware captions using\nLLMs with the prompts. Our human evaluation conducted on 45 participants from 4\ndifferent cultural groups with a high understanding of the corresponding\nculture shows that our proposed framework generates more culturally descriptive\ncaptions when compared to the image captioning baseline based on VLPs. Our code\nand dataset will be made publicly available upon acceptance.\n']",Cultural Understanding in Vision-Language Models,Cultural Understanding in AI Models,Artificial Intelligence and Machine Learning Interpretability and Explainability,Explainable AI and Machine Learning
508,11,508_denoising_denoisers_denoiser_denoised,"['denoising', 'denoisers', 'denoiser', 'denoised', 'noise2noise', 'pixels', 'noisy', 'supervised', 'noise', 'images']","['denoising', 'noise', 'image', 'denoisers', 'supervised', 'filter', 'denoiser', 'pixels', 'corners', 'self']","['  Recently, the mainstream practice for training low-light raw image denoising\nmethods has shifted towards employing synthetic data. Noise modeling, which\nfocuses on characterizing the noise distribution of real-world sensors,\nprofoundly influences the effectiveness and practicality of synthetic data.\nCurrently, physics-based noise modeling struggles to characterize the entire\nreal noise distribution, while learning-based noise modeling impractically\ndepends on paired real data. In this paper, we propose a novel strategy:\nlearning the noise model from dark frames instead of paired real data, to break\ndown the data dependency. Based on this strategy, we introduce an efficient\nphysics-guided noise neural proxy (PNNP) to approximate the real-world sensor\nnoise model. Specifically, we integrate physical priors into neural proxies and\nintroduce three efficient techniques: physics-guided noise decoupling (PND),\nphysics-guided proxy model (PPM), and differentiable distribution loss (DDL).\nPND decouples the dark frame into different components and handles different\nlevels of noise flexibly, which reduces the complexity of noise modeling. PPM\nincorporates physical priors to constrain the generated noise, which promotes\nthe accuracy of noise modeling. DDL provides explicit and reliable supervision\nfor noise distribution, which promotes the precision of noise modeling. PNNP\nexhibits powerful potential in characterizing the real noise distribution.\nExtensive experiments on public datasets demonstrate superior performance in\npractical low-light raw image denoising. The code will be available at\n\\url{https://github.com/fenghansen/PNNP}.\n', '  Due to the high flexibility and remarkable performance, low-rank\napproximation methods has been widely studied for color image denoising.\nHowever, those methods mostly ignore either the cross-channel difference or the\nspatial variation of noise, which limits their capacity in real world color\nimage denoising. To overcome those drawbacks, this paper is proposed to denoise\ncolor images with a double-weighted truncated nuclear norm minus truncated\nFrobenius norm minimization (DtNFM) method. Through exploiting the nonlocal\nself-similarity of the noisy image, the similar structures are gathered and a\nseries of similar patch matrices are constructed. For each group, the DtNFM\nmodel is conducted for estimating its denoised version. The denoised image\nwould be obtained by concatenating all the denoised patch matrices. The\nproposed DtNFM model has two merits. First, it models and utilizes both the\ncross-channel difference and the spatial variation of noise. This provides\nsufficient flexibility for handling the complex distribution of noise in real\nworld images. Second, the proposed DtNFM model provides a close approximation\nto the underlying clean matrix since it can treat different rank components\nflexibly. To solve the problem resulted from DtNFM model, an accurate and\neffective algorithm is proposed by exploiting the framework of the alternating\ndirection method of multipliers (ADMM). The generated subproblems are discussed\nin detail. And their global optima can be easily obtained in closed-form.\nRigorous mathematical derivation proves that the solution sequences generated\nby the algorithm converge to a single critical point. Extensive experiments on\nsynthetic and real noise datasets demonstrate that the proposed method\noutperforms many state-of-the-art color image denoising methods.\n', ""  Noise is ubiquitous during image acquisition. Sufficient denoising is often\nan important first step for image processing. In recent decades, deep neural\nnetworks (DNNs) have been widely used for image denoising. Most DNN-based image\ndenoising methods require a large-scale dataset or focus on supervised\nsettings, in which single/pairs of clean images or a set of noisy images are\nrequired. This poses a significant burden on the image acquisition process.\nMoreover, denoisers trained on datasets of limited scale may incur\nover-fitting. To mitigate these issues, we introduce a new self-supervised\nframework for image denoising based on the Tucker low-rank tensor\napproximation. With the proposed design, we are able to characterize our\ndenoiser with fewer parameters and train it based on a single image, which\nconsiderably improves the model's generalizability and reduces the cost of data\nacquisition. Extensive experiments on both synthetic and real-world noisy\nimages have been conducted. Empirical results show that our proposed method\noutperforms existing non-learning-based methods (e.g., low-pass filter,\nnon-local mean), single-image unsupervised denoisers (e.g., DIP, NN+BM3D)\nevaluated on both in-sample and out-sample datasets. The proposed method even\nachieves comparable performances with some supervised methods (e.g., DnCNN).\n""]",Image Denoising Methods,Image Restoration and Denoising Techniques,Image and Video Processing,Image and Video Processing
509,11,509_learning_shot_classification_dataset,"['learning', 'shot', 'classification', 'dataset', 'overfitting', 'trained', 'training', 'features', 'adaptation', 'labeled']","['shot', 'outlier', 'target', 'examples', 'meta', 'combiner', 'classes', 'adaptation', 'query', 'margin']","[""  Few-shot learning (FSL) is a challenging machine learning problem due to a\nscarcity of labeled data. The ability to generalize effectively on both novel\nand training tasks is a significant barrier to FSL. This paper proposes a novel\nsolution that can generalize to both training and novel tasks while also\nutilizing unlabeled samples. The method refines the embedding model before\nupdating the outer loop using unsupervised techniques as ``meta-tasks''. The\nexperimental results show that our proposed method performs well on novel and\ntraining tasks, with faster and better convergence, lower generalization, and\nstandard deviation error, indicating its potential for practical applications\nin FSL. The experimental results show that the proposed method outperforms\nprototypical networks by 3.9%.\n"", '  In Few-Shot Learning (FSL), models are trained to recognise unseen objects\nfrom a query set, given a few labelled examples from a support set. In standard\nFSL, models are evaluated on query instances sampled from the same class\ndistribution of the support set. In this work, we explore the more nuanced and\npractical challenge of Open-Set Few-Shot Recognition (OSFSL). Unlike standard\nFSL, OSFSL incorporates unknown classes into the query set, thereby requiring\nthe model not only to classify known classes but also to identify outliers.\nBuilding on the groundwork laid by previous studies, we define a novel\ntransductive inference technique that leverages the InfoMax principle to\nexploit the unlabelled query set. We called our approach the Enhanced Outlier\nLogit (EOL) method. EOL refines class prototype representations through model\ncalibration, effectively balancing the inlier-outlier ratio. This calibration\nenhances pseudo-label accuracy for the query set and improves the optimisation\nobjective within the transductive inference process. We provide a comprehensive\nempirical evaluation demonstrating that EOL consistently surpasses traditional\nmethods, recording performance improvements ranging from approximately $+1.3%$\nto $+6.3%$ across a variety of classification and outlier detection metrics and\nbenchmarks, even in the presence of inlier-outlier imbalance.\n', '  Few-shot-learning (FSL) commonly requires a model to identify images\n(queries) that belong to classes unseen during training, based on a few labeled\nsamples of the new classes (support set) as reference. So far, plenty of\nalgorithms involve training data augmentation to improve the generalization\ncapability of FSL models, but outlier queries or support images during\ninference can still pose great generalization challenges. In this work, to\nreduce the bias caused by the outlier samples, we generate additional\ntest-class samples by combining original samples with suitable train-class\nsamples via a generative image combiner. Then, we obtain averaged features via\nan augmentor, which leads to more typical representations through the\naveraging. We experimentally and theoretically demonstrate the effectiveness of\nour method, e.g., obtaining a test accuracy improvement proportion of around\n10% (e.g., from 46.86% to 53.28%) for trained FSL models. Importantly, given\npretrained image combiner, our method is training-free for off-the-shelf FSL\nmodels, whose performance can be improved without extra datasets nor further\ntraining of the models themselves.\n']",Few-Shot Learning Methods,Meta-Learning and Few-Shot Learning in Machine Learning and NLP,Machine Learning Methodologies,Machine Learning Methodologies
510,11,510_multimodal_modality_crossmodal_federated,"['multimodal', 'modality', 'crossmodal', 'federated', 'learning', 'fedmml', 'modal', 'modalities', 'regularization', 'distributed']","['modality', 'modalities', 'clients', 'multimodal', 'client', 'missing', 'incongruity', 'modal', 'unimodal', 'local']","['  In real-world scenarios, multimodal federated learning often faces the\npractical challenge of intricate modality missing, which poses constraints on\nbuilding federated frameworks and significantly degrades model inference\naccuracy. Existing solutions for addressing missing modalities generally\ninvolve developing modality-specific encoders on clients and training modality\nfusion modules on servers. However, these methods are primarily constrained to\nspecific scenarios with either unimodal clients or complete multimodal clients,\nstruggling to generalize effectively in the intricate modality missing\nscenarios. In this paper, we introduce a prototype library into the\nFedAvg-based Federated Learning framework, thereby empowering the framework\nwith the capability to alleviate the global model performance degradation\nresulting from modality missing during both training and testing. The proposed\nmethod utilizes prototypes as masks representing missing modalities to\nformulate a task-calibrated training loss and a model-agnostic uni-modality\ninference strategy. In addition, a proximal term based on prototypes is\nconstructed to enhance local training. Experimental results demonstrate the\nstate-of-the-art performance of our approach. Compared to the baselines, our\nmethod improved inference accuracy by 3.7\\% with 50\\% modality missing during\ntraining and by 23.8\\% during uni-modality inference. Code is available at\nhttps://github.com/BaoGuangYin/PmcmFL.\n', '  Federated learning (FL) underpins advancements in privacy-preserving\ndistributed computing by collaboratively training neural networks without\nexposing clients\' raw data. Current FL paradigms primarily focus on uni-modal\ndata, while exploiting the knowledge from distributed multimodal data remains\nlargely unexplored. Existing multimodal FL (MFL) solutions are mainly designed\nfor statistical or modality heterogeneity from the input side, however, have\nyet to solve the fundamental issue,""modality imbalance"", in distributed\nconditions, which can lead to inadequate information exploitation and\nheterogeneous knowledge aggregation on different modalities.In this paper, we\npropose a novel Cross-Modal Infiltration Federated Learning (FedCMI) framework\nthat effectively alleviates modality imbalance and knowledge heterogeneity via\nknowledge transfer from the global dominant modality. To avoid the loss of\ninformation in the weak modality due to merely imitating the behavior of\ndominant modality, we design the two-projector module to integrate the\nknowledge from dominant modality while still promoting the local feature\nexploitation of weak modality. In addition, we introduce a class-wise\ntemperature adaptation scheme to achieve fair performance across different\nclasses. Extensive experiments over popular datasets are conducted and give us\na gratifying confirmation of the proposed framework for fully exploring the\ninformation of each modality in MFL.\n', '  Multimodal federated learning (MFL) has emerged as a decentralized machine\nlearning paradigm, allowing multiple clients with different modalities to\ncollaborate on training a machine learning model across diverse data sources\nwithout sharing their private data. However, challenges, such as data\nheterogeneity and severely missing modalities, pose crucial hindrances to the\nrobustness of MFL, significantly impacting the performance of global model. The\nabsence of a modality introduces misalignment during the local training phase,\nstemming from zero-filling in the case of clients with missing modalities.\nConsequently, achieving robust generalization in global model becomes\nimperative, especially when dealing with clients that have incomplete data. In\nthis paper, we propose Multimodal Federated Cross Prototype Learning (MFCPL), a\nnovel approach for MFL under severely missing modalities by conducting the\ncomplete prototypes to provide diverse modality knowledge in modality-shared\nlevel with the cross-modal regularization and modality-specific level with\ncross-modal contrastive mechanism. Additionally, our approach introduces the\ncross-modal alignment to provide regularization for modality-specific features,\nthereby enhancing overall performance, particularly in scenarios involving\nseverely missing modalities. Through extensive experiments on three multimodal\ndatasets, we demonstrate the effectiveness of MFCPL in mitigating these\nchallenges and improving the overall performance.\n']",Multimodal Federated Learning with Missing Modalities,Federated Learning,Machine Learning and Data Privacy,Machine Learning and Data Privacy
511,11,511_byzantine_distributed_adversarial_mlmc,"['byzantine', 'distributed', 'adversarial', 'mlmc', 'decentralized', 'robustscgmm', 'robust', 'federated', 'aggregators', 'adversaries']","['byzantine', 'tolerant', 'workers', 'resilient', 'fault', 'aggregation', 'clipping', 'tolerance', 'consensus', 'decentralized']","['  This paper proposes two split-and-conquer (SC) learning estimators for finite\nmixture models that are tolerant to Byzantine failures. In SC learning,\nindividual machines obtain local estimates, which are then transmitted to a\ncentral server for aggregation. During this communication, the server may\nreceive malicious or incorrect information from some local machines, a scenario\nknown as Byzantine failures. While SC learning approaches have been devised to\nmitigate Byzantine failures in statistical models with Euclidean parameters,\ndeveloping Byzantine-tolerant methods for finite mixture models with\nnon-Euclidean parameters requires a distinct strategy. Our proposed\ndistance-based methods are hyperparameter tuning free, unlike existing methods,\nand are resilient to Byzantine failures while achieving high statistical\nefficiency. We validate the effectiveness of our methods both theoretically and\nempirically via experiments on simulated and real data from machine learning\napplications for digit recognition. The code for the experiment can be found at\nhttps://github.com/SarahQiong/RobustSCGMM.\n', '  In Federated Reinforcement Learning (FRL), agents aim to collaboratively\nlearn a common task, while each agent is acting in its local environment\nwithout exchanging raw trajectories. Existing approaches for FRL either (a) do\nnot provide any fault-tolerance guarantees (against misbehaving agents), or (b)\nrely on a trusted central agent (a single point of failure) for aggregating\nupdates. We provide the first decentralized Byzantine fault-tolerant FRL\nmethod. Towards this end, we first propose a new centralized Byzantine\nfault-tolerant policy gradient (PG) algorithm that improves over existing\nmethods by relying only on assumptions standard for non-fault-tolerant PG.\nThen, as our main contribution, we show how a combination of robust aggregation\nand Byzantine-resilient agreement methods can be leveraged in order to\neliminate the need for a trusted central entity. Since our results represent\nthe first sample complexity analysis for Byzantine fault-tolerant decentralized\nfederated non-convex optimization, our technical contributions may be of\nindependent interest. Finally, we corroborate our theoretical results\nexperimentally for common RL environments, demonstrating the speed-up of\ndecentralized federations w.r.t. the number of participating agents and\nresilience against various Byzantine attacks.\n', '  Distributed learning has emerged as a leading paradigm for training large\nmachine learning models. However, in real-world scenarios, participants may be\nunreliable or malicious, posing a significant challenge to the integrity and\naccuracy of the trained models. Byzantine fault tolerance mechanisms have been\nproposed to address these issues, but they often assume full participation from\nall clients, which is not always practical due to the unavailability of some\nclients or communication constraints. In our work, we propose the first\ndistributed method with client sampling and provable tolerance to Byzantine\nworkers. The key idea behind the developed method is the use of gradient\nclipping to control stochastic gradient differences in recursive variance\nreduction. This allows us to bound the potential harm caused by Byzantine\nworkers, even during iterations when all sampled clients are Byzantine.\nFurthermore, we incorporate communication compression into the method to\nenhance communication efficiency. Under general assumptions, we prove\nconvergence rates for the proposed method that match the existing\nstate-of-the-art (SOTA) theoretical results. We also propose a heuristic on\nadjusting any Byzantine-robust method to a partial participation scenario via\nclipping.\n']",Byzantine Fault Tolerance in Distributed ML,Byzantine Fault Tolerance in Distributed Machine Learning,Machine Learning and Data Privacy,Machine Learning and Data Privacy
512,11,512_graphslim_graphs_graph_tinygraph,"['graphslim', 'graphs', 'graph', 'tinygraph', 'gnn', 'nodes', 'condense', 'gnns', 'node', 'condenses']","['condensation', 'graph', 'original', 'graphs', 'matching', 'nodes', 'node', 'structure', 'storage', 'redundancy']","['  Graph condensation (GC) is an emerging technique designed to learn a\nsignificantly smaller graph that retains the essential information of the\noriginal graph. This condensed graph has shown promise in accelerating graph\nneural networks while preserving performance comparable to those achieved with\nthe original, larger graphs. Additionally, this technique facilitates\ndownstream applications such as neural architecture search and enhances our\nunderstanding of redundancy in large graphs. Despite the rapid development of\nGC methods, a systematic evaluation framework remains absent, which is\nnecessary to clarify the critical designs for particular evaluative aspects.\nFurthermore, several meaningful questions have not been investigated, such as\nwhether GC inherently preserves certain graph properties and offers robustness\neven without targeted design efforts. In this paper, we introduce GC-Bench, a\ncomprehensive framework to evaluate recent GC methods across multiple\ndimensions and to generate new insights. Our experimental findings provide a\ndeeper insights into the GC process and the characteristics of condensed\ngraphs, guiding future efforts in enhancing performance and exploring new\napplications. Our code is available at\n\\url{https://github.com/Emory-Melody/GraphSlim/tree/main/benchmark}.\n', '  The increasing prevalence of large-scale graphs poses a significant challenge\nfor graph neural network training, attributed to their substantial\ncomputational requirements. In response, graph condensation (GC) emerges as a\npromising data-centric solution aiming to substitute the large graph with a\nsmall yet informative condensed graph to facilitate data-efficient GNN\ntraining. However, existing GC methods suffer from intricate optimization\nprocesses, necessitating excessive computing resources. In this paper, we\nrevisit existing GC optimization strategies and identify two pervasive issues:\n1. various GC optimization strategies converge to class-level node feature\nmatching between the original and condensed graphs, making the optimization\ntarget coarse-grained despite the complex computations; 2. to bridge the\noriginal and condensed graphs, existing GC methods rely on a Siamese graph\nnetwork architecture that requires time-consuming bi-level optimization with\niterative gradient computations. To overcome these issues, we propose a\ntraining-free GC framework termed Class-partitioned Graph Condensation (CGC),\nwhich refines the node feature matching from the class-to-class paradigm into a\nnovel class-to-node paradigm. Remarkably, this refinement also simplifies the\nGC optimization as a class partition problem, which can be efficiently solved\nby any clustering methods. Moreover, CGC incorporates a pre-defined graph\nstructure to enable a closed-form solution for condensed node features,\neliminating the back-and-forth gradient descent in existing GC approaches\nwithout sacrificing accuracy. Extensive experiments demonstrate that CGC\nachieves state-of-the-art performance with a more efficient condensation\nprocess. For instance, compared with the seminal GC method (i.e., GCond), CGC\ncondenses the largest Reddit graph within 10 seconds, achieving a 2,680X\nspeedup and a 1.4% accuracy increase.\n', '  The burgeoning volume of graph data presents significant computational\nchallenges in training graph neural networks (GNNs), critically impeding their\nefficiency in various applications. To tackle this challenge, graph\ncondensation (GC) has emerged as a promising acceleration solution, focusing on\nthe synthesis of a compact yet representative graph for efficiently training\nGNNs while retaining performance. Despite the potential to promote scalable use\nof GNNs, existing GC methods are limited to aligning the condensed graph with\nmerely the observed static graph distribution. This limitation significantly\nrestricts the generalization capacity of condensed graphs, particularly in\nadapting to dynamic distribution changes. In real-world scenarios, however,\ngraphs are dynamic and constantly evolving, with new nodes and edges being\ncontinually integrated. Consequently, due to the limited generalization\ncapacity of condensed graphs, applications that employ GC for efficient GNN\ntraining end up with sub-optimal GNNs when confronted with evolving graph\nstructures and distributions in dynamic real-world situations. To overcome this\nissue, we propose open-world graph condensation (OpenGC), a robust GC framework\nthat integrates structure-aware distribution shift to simulate evolving graph\npatterns and exploit the temporal environments for invariance condensation.\nThis approach is designed to extract temporal invariant patterns from the\noriginal graph, thereby enhancing the generalization capabilities of the\ncondensed graph and, subsequently, the GNNs trained on it. Extensive\nexperiments on both real-world and synthetic evolving graphs demonstrate that\nOpenGC outperforms state-of-the-art (SOTA) GC methods in adapting to dynamic\nchanges in open-world graph environments.\n']",Graph Condensation for Efficient GNN Training,Graph Neural Networks (GNNs) and Graph Data Analysis,Graph Representation Learning and Neural Networks,Graph Representation Learning and Neural Networks
513,10,513_detection_detecting_outliers_outlier,"['detection', 'detecting', 'outliers', 'outlier', 'classifier', 'inlier', 'ood', 'classified', 'novelty', 'od']","['detection', 'outlier', 'interclass', 'distribution', 'outliers', 'generalized', 'classifier', 'boundaries', 'unfamiliar', 'samples']","['  Out-of-distribution (OOD) detection is crucial for deploying robust machine\nlearning models, especially in areas where security is critical. However,\ntraditional OOD detection methods often fail to capture complex data\ndistributions from large scale date. In this paper, we present a novel approach\nfor OOD detection that leverages the generative ability of diffusion models and\nthe powerful feature extraction capabilities of CLIP. By using these features\nas conditional inputs to a diffusion model, we can reconstruct the images after\nencoding them with CLIP. The difference between the original and reconstructed\nimages is used as a signal for OOD identification. The practicality and\nscalability of our method is increased by the fact that it does not require\nclass-specific labeled ID data, as is the case with many other methods.\nExtensive experiments on several benchmark datasets demonstrates the robustness\nand effectiveness of our method, which have significantly improved the\ndetection accuracy.\n', ""  Few-shot OOD detection focuses on recognizing out-of-distribution (OOD)\nimages that belong to classes unseen during training, with the use of only a\nsmall number of labeled in-distribution (ID) images. Up to now, a mainstream\nstrategy is based on large-scale vision-language models, such as CLIP. However,\nthese methods overlook a crucial issue: the lack of reliable OOD supervision\ninformation, which can lead to biased boundaries between in-distribution (ID)\nand OOD. To tackle this problem, we propose CLIP-driven Outliers\nSynthesis~(CLIP-OS). Firstly, CLIP-OS enhances patch-level features' perception\nby newly proposed patch uniform convolution, and adaptively obtains the\nproportion of ID-relevant information by employing CLIP-surgery-discrepancy,\nthus achieving separation between ID-relevant and ID-irrelevant. Next, CLIP-OS\nsynthesizes reliable OOD data by mixing up ID-relevant features from different\nclasses to provide OOD supervision information. Afterward, CLIP-OS leverages\nsynthetic OOD samples by unknown-aware prompt learning to enhance the\nseparability of ID and OOD. Extensive experiments across multiple benchmarks\ndemonstrate that CLIP-OS achieves superior few-shot OOD detection capability.\n"", '  Detecting out-of-distribution (OOD) samples is crucial for ensuring the\nsafety of machine learning systems and has shaped the field of OOD detection.\nMeanwhile, several other problems are closely related to OOD detection,\nincluding anomaly detection (AD), novelty detection (ND), open set recognition\n(OSR), and outlier detection (OD). To unify these problems, a generalized OOD\ndetection framework was proposed, taxonomically categorizing these five\nproblems. However, Vision Language Models (VLMs) such as CLIP have\nsignificantly changed the paradigm and blurred the boundaries between these\nfields, again confusing researchers. In this survey, we first present a\ngeneralized OOD detection v2, encapsulating the evolution of AD, ND, OSR, OOD\ndetection, and OD in the VLM era. Our framework reveals that, with some field\ninactivity and integration, the demanding challenges have become OOD detection\nand AD. In addition, we also highlight the significant shift in the definition,\nproblem settings, and benchmarks; we thus feature a comprehensive review of the\nmethodology for OOD detection, including the discussion over other related\ntasks to clarify their relationship to OOD detection. Finally, we explore the\nadvancements in the emerging Large Vision Language Model (LVLM) era, such as\nGPT-4V. We conclude this survey with open challenges and future directions.\n']",Out-of-Distribution Detection Methods,Deep Learning for Out-of-Distribution Detection and Robustness,Deep Learning Optimization and Security,Deep Learning Methodologies
514,10,514_attention_tokenizers_tokenizer_tokenization,"['attention', 'tokenizers', 'tokenizer', 'tokenization', 'representations', 'recognition', 'imagenet', 'vision', 'visual', 'representation']","['slots', 'subobjects', 'object', 'subobject', 'primitives', 'patches', 'tokenizer', 'patch', 'visual', 'tokens']","['  The extraction of modular object-centric representations for downstream tasks\nis an emerging area of research. Learning grounded representations of objects\nthat are guaranteed to be stable and invariant promises robust performance\nacross different tasks and environments. Slot Attention (SA) learns\nobject-centric representations by assigning objects to \\textit{slots}, but\npresupposes a \\textit{single} distribution from which all slots are randomly\ninitialised. This results in an inability to learn \\textit{specialized} slots\nwhich bind to specific object types and remain invariant to identity-preserving\nchanges in object appearance. To address this, we present\n\\emph{\\textsc{Co}nditional \\textsc{S}lot \\textsc{A}ttention} (\\textsc{CoSA})\nusing a novel concept of \\emph{Grounded Slot Dictionary} (GSD) inspired by\nvector quantization. Our proposed GSD comprises (i) canonical object-level\nproperty vectors and (ii) parametric Gaussian distributions, which define a\nprior over the slots. We demonstrate the benefits of our method in multiple\ndownstream tasks such as scene generation, composition, and task adaptation,\nwhilst remaining competitive with SA in popular object discovery benchmarks.\n', '  Object-centric methods have seen significant progress in unsupervised\ndecomposition of raw perception into rich object-like abstractions. However,\nlimited ability to ground object semantics of the real world into the learned\nabstractions has hindered their adoption in downstream understanding\napplications. We present the Neural Slot Interpreter (NSI) that learns to\nground and generate object semantics via slot representations. At the core of\nNSI is an XML-like programming language that uses simple syntax rules to\norganize the object semantics of a scene into object-centric program\nprimitives. Then, an alignment model learns to ground program primitives into\nslots through a bi-level contrastive learning objective over a shared embedding\nspace. Finally, we formulate the NSI program generator model to use the dense\nassociations inferred from the alignment model to generate object-centric\nprograms from slots. Experiments on bi-modal retrieval tasks demonstrate the\nefficacy of the learned alignments, surpassing set-matching-based predictors by\na significant margin. Moreover, learning the program generator from grounded\nassociations enhances the predictive power of slots. NSI generated programs\ndemonstrate improved performance of object-centric learners on property\nprediction and object detection, and scale with real-world scene complexity.\n', '  The tokenizer, as one of the fundamental components of large models, has long\nbeen overlooked or even misunderstood in visual tasks. One key factor of the\ngreat comprehension power of the large language model is that natural language\ntokenizers utilize meaningful words or subwords as the basic elements of\nlanguage. In contrast, mainstream visual tokenizers, represented by patch-based\nmethods such as Patch Embed, rely on meaningless rectangular patches as basic\nelements of vision, which cannot serve as effectively as words or subwords in\nlanguage. Starting from the essence of the tokenizer, we defined semantically\nindependent regions (SIRs) for vision. We designed a simple HOmogeneous visual\ntOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception\nModule (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity,\nthe OPM splits the image into 4*4 pixel seeds and then utilizes the attention\nmechanism to perceive SIRs. The OVM employs cross-attention to merge seeds\nwithin the same SIR. To achieve adaptability, the OVM defines a variable number\nof learnable vectors as cross-attention queries, allowing for the adjustment of\ntoken quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19\nclassification dataset, and GID5 segmentation dataset for sparse and dense\ntasks. The results demonstrate that the visual tokens obtained by HOOK\ncorrespond to individual objects, which demonstrates homogeneity. HOOK\noutperformed Patch Embed by 6\\% and 10\\% in the two tasks and achieved\nstate-of-the-art performance compared to the baselines used for comparison.\nCompared to Patch Embed, which requires more than one hundred tokens for one\nimage, HOOK requires only 6 and 8 tokens for sparse and dense tasks,\nrespectively, resulting in efficiency improvements of 1.5 to 2.8 times. The\ncode is available at https://github.com/GeoX-Lab/Hook.\n']",Object-Centric Representations and Visual Tokenization,Computer Vision and Object Recognition,Computer Vision,Computer Vision
515,10,515_ranking_rankings_rank_fairness,"['ranking', 'rankings', 'rank', 'fairness', 'bias', 'merit', 'unfair', 'utility', 'predictors', 'redistribution']","['ranking', 'fairness', 'rankings', 'tax', 'fair', 'group', 'utility', 'items', 'groups', 'stochastic']","['  Learning to Rank (LTR) is one of the most widely used machine learning\napplications. It is a key component in platforms with profound societal\nimpacts, including job search, healthcare information retrieval, and social\nmedia content feeds. Conventional LTR models have been shown to produce biases\nresults, stimulating a discourse on how to address the disparities introduced\nby ranking systems that solely prioritize user relevance. However, while\nseveral models of fair learning to rank have been proposed, they suffer from\ndeficiencies either in accuracy or efficiency, thus limiting their\napplicability to real-world ranking platforms. This paper shows how\nefficiently-solvable fair ranking models, based on the optimization of Ordered\nWeighted Average (OWA) functions, can be integrated into the training loop of\nan LTR model to achieve favorable balances between fairness, user utility, and\nruntime efficiency. In particular, this paper is the first to show how to\nbackpropagate through constrained optimizations of OWA objectives, enabling\ntheir use in integrated prediction and decision models.\n', '  Stochastic learning to rank (LTR) is a recent branch in the LTR field that\nconcerns the optimization of probabilistic ranking models. Their probabilistic\nbehavior enables certain ranking qualities that are impossible with\ndeterministic models. For example, they can increase the diversity of displayed\ndocuments, increase fairness of exposure over documents, and better balance\nexploitation and exploration through randomization. A core difficulty in LTR is\ngradient estimation, for this reason, existing stochastic LTR methods have been\nlimited to differentiable ranking models (e.g., neural networks). This is in\nstark contrast with the general field of LTR where Gradient Boosted Decision\nTrees (GBDTs) have long been considered the state-of-the-art. In this work, we\naddress this gap by introducing the first stochastic LTR method for GBDTs. Our\nmain contribution is a novel estimator for the second-order derivatives, i.e.,\nthe Hessian matrix, which is a requirement for effective GBDTs. To efficiently\ncompute both the first and second-order derivatives simultaneously, we\nincorporate our estimator into the existing PL-Rank framework, which was\noriginally designed for first-order derivatives only. Our experimental results\nindicate that stochastic LTR without the Hessian has extremely poor\nperformance, whilst the performance is competitive with the current\nstate-of-the-art with our estimated Hessian. Thus, through the contribution of\nour novel Hessian estimation method, we have successfully introduced GBDTs to\nstochastic LTR.\n', '  Learning to Rank (LTR) methods are vital in online economies, affecting users\nand item providers. Fairness in LTR models is crucial to allocate exposure\nproportionally to item relevance. Widely used deterministic LTR models can lead\nto unfair exposure distribution, especially when items with the same relevance\nreceive slightly different ranking scores. Stochastic LTR models, incorporating\nthe Plackett-Luce (PL) ranking model, address fairness issues but suffer from\nhigh training cost. In addition, they cannot provide guarantees on the utility\nor fairness, which can lead to dramatic degraded utility when optimized for\nfairness. To overcome these limitations, we propose Inference-time Stochastic\nRanking with Risk Control (ISRR), a novel method that performs stochastic\nranking at inference time with guanranteed utility or fairness given pretrained\nscoring functions from deterministic or stochastic LTR models. Comprehensive\nexperimental results on three widely adopted datasets demonstrate that our\nproposed method achieves utility and fairness comparable to existing stochastic\nranking methods with much lower computational cost. In addition, results verify\nthat our method provides finite-sample guarantee on utility and fairness. This\nadvancement represents a significant contribution to the field of stochastic\nranking and fair LTR with promising real-world applications.\n']",Fairness in Learning to Rank Models,Fairness in Artificial Intelligence and Machine Learning,Fairness and Bias in Artificial Intelligence,Fairness and Bias in Artificial Intelligence
516,10,516_voices_voice_vocal_vocals,"['voices', 'voice', 'vocal', 'vocals', 'singing', 'singers', 'karaoker', 'singer', 'wav2vec', 'transcription']","['singing', 'singer', 'voice', 'voices', 'pitch', 'vocal', 'singers', 'lyrics', 'style', 'synthesis']","[""  Singing voice conversion (SVC) automates song covers by converting one\nsinger's singing voice into another target singer's singing voice with the\noriginal lyrics and melody. However, it raises serious concerns about copyright\nand civil right infringements to multiple entities. This work proposes\nSongBsAb, the first proactive approach to mitigate unauthorized SVC-based\nillegal song covers. SongBsAb introduces human-imperceptible perturbations to\nsinging voices before releasing them, so that when they are used, the\ngeneration process of SVC will be interfered, resulting in unexpected singing\nvoices. SongBsAb features a dual prevention effect by causing both (singer)\nidentity disruption and lyric disruption, namely, the SVC-covered singing voice\nneither imitates the target singer nor preserves the original lyrics. To\nimprove the imperceptibility of perturbations, we refine a psychoacoustic\nmodel-based loss with the backing track as an additional masker, a unique\naccompanying element for singing voices compared to ordinary speech voices. To\nenhance the transferability, we propose to utilize a frame-level interaction\nreduction-based loss. We demonstrate the prevention effectiveness, utility, and\nrobustness of SongBsAb on three SVC models and two datasets using both\nobjective and human study-based subjective metrics. Our work fosters an\nemerging research direction for mitigating illegal automated song covers.\n"", '  Significant strides have been made in creating voice identity representations\nusing speech data. However, the same level of progress has not been achieved\nfor singing voices. To bridge this gap, we suggest a framework for training\nsinger identity encoders to extract representations suitable for various\nsinging-related tasks, such as singing voice similarity and synthesis. We\nexplore different self-supervised learning techniques on a large collection of\nisolated vocal tracks and apply data augmentations during training to ensure\nthat the representations are invariant to pitch and content variations. We\nevaluate the quality of the resulting representations on singer similarity and\nidentification tasks across multiple datasets, with a particular emphasis on\nout-of-domain generalization. Our proposed framework produces high-quality\nembeddings that outperform both speaker verification and wav2vec 2.0\npre-trained baselines on singing voice while operating at 44.1 kHz. We release\nour code and trained models to facilitate further research on singing voice and\nrelated areas.\n', '  Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses\non generating high-quality singing voices with unseen styles (such as timbre,\nemotion, pronunciation, and articulation skills) derived from reference singing\nvoice samples. However, the endeavor to model the intricate nuances of singing\nvoice styles is an arduous task, as singing voices possess a remarkable degree\nof expressiveness. Moreover, existing SVS methods encounter a decline in the\nquality of synthesized singing voices in OOD scenarios, as they rest upon the\nassumption that the target vocal attributes are discernible during the training\nphase. To overcome these challenges, we propose StyleSinger, the first singing\nvoice synthesis model for zero-shot style transfer of out-of-domain reference\nsinging voice samples. StyleSinger incorporates two critical approaches for\nenhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a\nresidual quantization module to capture diverse style characteristics in\nsinging voices, and 2) the Uncertainty Modeling Layer Normalization (UMLN) to\nperturb the style attributes within the content representation during the\ntraining phase and thus improve the model generalization. Our extensive\nevaluations in zero-shot style transfer undeniably establish that StyleSinger\noutperforms baseline models in both audio quality and similarity to the\nreference singing voice samples. Access to singing voice samples can be found\nat https://stylesinger.github.io/.\n']",Singing Voice Conversion and Synthesis,Speech and Voice Synthesis,Speech and Audio Processing,Speech and Audio Processing
517,10,517_cultures_cultural_culture_nationalities,"['cultures', 'cultural', 'culture', 'nationalities', 'culturally', 'multicultural', 'nationality', 'diverse', 'attitudes', 'anthropology']","['culture', 'cultural', 'cultures', 'values', 'country', 'alignment', 'dominance', 'countries', 'fairy', 'anthropological']","['  This paper identifies a cultural dominance issue within large language models\n(LLMs) due to the predominant use of English data in model training (e.g.,\nChatGPT). LLMs often provide inappropriate English-culture-related answers that\nare not relevant to the expected culture when users ask in non-English\nlanguages. To systematically evaluate the cultural dominance issue, we build a\nbenchmark of concrete (e.g., holidays and songs) and abstract (e.g., values and\nopinions) cultural objects. Empirical results show that the representative GPT\nmodels suffer from the culture dominance problem, where GPT-4 is the most\naffected while text-davinci-003 suffers the least from this problem. Our study\nemphasizes the need to critically examine cultural dominance and ethical\nconsideration in their development and deployment. We show that two\nstraightforward methods in model development (i.e., pretraining on more diverse\ndata) and deployment (e.g., culture-aware prompting) can significantly mitigate\nthe cultural dominance issue in LLMs.\n', '  The intricate relationship between language and culture has long been a\nsubject of exploration within the realm of linguistic anthropology. Large\nLanguage Models (LLMs), promoted as repositories of collective human knowledge,\nraise a pivotal question: do these models genuinely encapsulate the diverse\nknowledge adopted by different cultures? Our study reveals that these models\ndemonstrate greater cultural alignment along two dimensions -- firstly, when\nprompted with the dominant language of a specific culture, and secondly, when\npretrained with a refined mixture of languages employed by that culture. We\nquantify cultural alignment by simulating sociological surveys, comparing model\nresponses to those of actual survey participants as references. Specifically,\nwe replicate a survey conducted in various regions of Egypt and the United\nStates through prompting LLMs with different pretraining data mixtures in both\nArabic and English with the personas of the real respondents and the survey\nquestions. Further analysis reveals that misalignment becomes more pronounced\nfor underrepresented personas and for culturally sensitive topics, such as\nthose probing social values. Finally, we introduce Anthropological Prompting, a\nnovel method leveraging anthropological reasoning to enhance cultural\nalignment. Our study emphasizes the necessity for a more balanced multilingual\npretraining dataset to better represent the diversity of human experience and\nthe plurality of different cultures with many implications on the topic of\ncross-lingual transfer.\n', '  As the utilization of large language models (LLMs) has proliferated\nworld-wide, it is crucial for them to have adequate knowledge and fair\nrepresentation for diverse global cultures. In this work, we uncover culture\nperceptions of three SOTA models on 110 countries and regions on 8\nculture-related topics through culture-conditioned generations, and extract\nsymbols from these generations that are associated to each culture by the LLM.\nWe discover that culture-conditioned generation consist of linguistic ""markers""\nthat distinguish marginalized cultures apart from default cultures. We also\ndiscover that LLMs have an uneven degree of diversity in the culture symbols,\nand that cultures from different geographic regions have different presence in\nLLMs\' culture-agnostic generation. Our findings promote further research in\nstudying the knowledge and fairness of global culture perception in LLMs. Code\nand Data can be found here: https://github.com/huihanlhh/Culture-Gen/\n']",Cultural Bias in Large Language Models,Bias and Fairness in Large Language Models,Fairness and Bias in Artificial Intelligence,Fairness and Bias in Artificial Intelligence
518,10,518_logs_parsers_log_openlogparser,"['logs', 'parsers', 'log', 'openlogparser', 'parsing', 'parser', 'parses', 'logprompt', 'loghub', 'logelectra']","['log', 'logs', 'parsing', 'parsers', 'templates', 'software', 'anomalies', 'parser', 'messages', 'maintenance']","['  Logs are important in modern software development with runtime information.\nLog parsing is the first step in many log-based analyses, that involve\nextracting structured information from unstructured log data. Traditional log\nparsers face challenges in accurately parsing logs due to the diversity of log\nformats, which directly impacts the performance of downstream log-analysis\ntasks. In this paper, we explore the potential of using Large Language Models\n(LLMs) for log parsing and propose LLMParser, an LLM-based log parser based on\ngenerative LLMs and few-shot tuning. We leverage four LLMs, Flan-T5-small,\nFlan-T5-base, LLaMA-7B, and ChatGLM-6B in LLMParsers. Our evaluation of 16\nopen-source systems shows that LLMParser achieves statistically significantly\nhigher parsing accuracy than state-of-the-art parsers (a 96% average parsing\naccuracy). We further conduct a comprehensive empirical analysis on the effect\nof training size, model size, and pre-training LLM on log parsing accuracy. We\nfind that smaller LLMs may be more effective than more complex LLMs; for\ninstance where Flan-T5-base achieves comparable results as LLaMA-7B with a\nshorter inference time. We also find that using LLMs pre-trained using logs\nfrom other systems does not always improve parsing accuracy. While using\npre-trained Flan-T5-base shows an improvement in accuracy, pre-trained LLaMA\nresults in a decrease (decrease by almost 55% in group accuracy). In short, our\nstudy provides empirical evidence for using LLMs for log parsing and highlights\nthe limitations and future research direction of LLM-based log parsers.\n', '  Log parsing is a critical step that transforms unstructured log data into\nstructured formats, facilitating subsequent log-based analysis. Traditional\nsyntax-based log parsers are efficient and effective, but they often experience\ndecreased accuracy when processing logs that deviate from the predefined rules.\nRecently, large language models (LLM) based log parsers have shown superior\nparsing accuracy. However, existing LLM-based parsers face three main\nchallenges: 1)time-consuming and labor-intensive manual labeling for\nfine-tuning or in-context learning, 2)increased parsing costs due to the vast\nvolume of log data and limited context size of LLMs, and 3)privacy risks from\nusing commercial models like ChatGPT with sensitive log information. To\novercome these limitations, this paper introduces OpenLogParser, an\nunsupervised log parsing approach that leverages open-source LLMs (i.e.,\nLlama3-8B) to enhance privacy and reduce operational costs while achieving\nstate-of-the-art parsing accuracy. OpenLogParser first groups logs with similar\nstatic text but varying dynamic variables using a fixed-depth grouping tree. It\nthen parses logs within these groups using three components: i)similarity\nscoring-based retrieval augmented generation: selects diverse logs within each\ngroup based on Jaccard similarity, helping the LLM distinguish between static\ntext and dynamic variables; ii)self-reflection: iteratively query LLMs to\nrefine log templates to improve parsing accuracy; and iii) log template memory:\nstores parsed templates to reduce LLM queries for improved parsing efficiency.\nOur evaluation on LogHub-2.0 shows that OpenLogParser achieves 25% higher\nparsing accuracy and processes logs 2.7 times faster compared to\nstate-of-the-art LLM-based parsers. In short, OpenLogParser addresses privacy\nand cost concerns of using commercial LLMs while achieving state-of-the-arts\nparsing efficiency and accuracy.\n', ""  Logs are a first-hand source of information for software maintenance and\nfailure diagnosis. Log parsing, which converts semi-structured log messages\ninto structured templates, is a prerequisite for automated log analysis tasks\nsuch as anomaly detection, troubleshooting, and root cause analysis. However,\nexisting log parsers fail in real-world systems for three main reasons. First,\ntraditional heuristics-based parsers require handcrafted features and domain\nknowledge, which are difficult to generalize at scale. Second, existing large\nlanguage model-based parsers rely on periodic offline processing, limiting\ntheir effectiveness in real-time use cases. Third, existing online parsing\nalgorithms are susceptible to log drift, where slight log changes create false\npositives that drown out real anomalies. To address these challenges, we\npropose HELP, a Hierarchical Embeddings-based Log Parser. HELP is the first\nonline semantic-based parser to leverage LLMs for performant and cost-effective\nlog parsing. We achieve this through a novel hierarchical embeddings module,\nwhich fine-tunes a text embedding model to cluster logs before parsing,\nreducing querying costs by multiple orders of magnitude. To combat log drift,\nwe also develop an iterative rebalancing module, which periodically updates\nexisting log groupings. We evaluate HELP extensively on 14 public large-scale\ndatasets, showing that HELP achieves significantly higher F1-weighted grouping\nand parsing accuracy than current state-of-the-art online log parsers. We also\nimplement HELP into Iudex's production observability platform, confirming\nHELP's practicality in a production environment. Our results show that HELP is\neffective and efficient for high-throughput real-world log parsing.\n""]",Log Parsing with Large Language Models,Optimization and Efficiency of Large Language Models,Large Language Models,Large Language Models
518,10,518_logs_parsers_log_openlogparser,"['logs', 'parsers', 'log', 'openlogparser', 'parsing', 'parser', 'parses', 'logprompt', 'loghub', 'logelectra']","['log', 'logs', 'parsing', 'parsers', 'templates', 'software', 'anomalies', 'parser', 'messages', 'maintenance']","['  Logs are important in modern software development with runtime information.\nLog parsing is the first step in many log-based analyses, that involve\nextracting structured information from unstructured log data. Traditional log\nparsers face challenges in accurately parsing logs due to the diversity of log\nformats, which directly impacts the performance of downstream log-analysis\ntasks. In this paper, we explore the potential of using Large Language Models\n(LLMs) for log parsing and propose LLMParser, an LLM-based log parser based on\ngenerative LLMs and few-shot tuning. We leverage four LLMs, Flan-T5-small,\nFlan-T5-base, LLaMA-7B, and ChatGLM-6B in LLMParsers. Our evaluation of 16\nopen-source systems shows that LLMParser achieves statistically significantly\nhigher parsing accuracy than state-of-the-art parsers (a 96% average parsing\naccuracy). We further conduct a comprehensive empirical analysis on the effect\nof training size, model size, and pre-training LLM on log parsing accuracy. We\nfind that smaller LLMs may be more effective than more complex LLMs; for\ninstance where Flan-T5-base achieves comparable results as LLaMA-7B with a\nshorter inference time. We also find that using LLMs pre-trained using logs\nfrom other systems does not always improve parsing accuracy. While using\npre-trained Flan-T5-base shows an improvement in accuracy, pre-trained LLaMA\nresults in a decrease (decrease by almost 55% in group accuracy). In short, our\nstudy provides empirical evidence for using LLMs for log parsing and highlights\nthe limitations and future research direction of LLM-based log parsers.\n', '  Log parsing is a critical step that transforms unstructured log data into\nstructured formats, facilitating subsequent log-based analysis. Traditional\nsyntax-based log parsers are efficient and effective, but they often experience\ndecreased accuracy when processing logs that deviate from the predefined rules.\nRecently, large language models (LLM) based log parsers have shown superior\nparsing accuracy. However, existing LLM-based parsers face three main\nchallenges: 1)time-consuming and labor-intensive manual labeling for\nfine-tuning or in-context learning, 2)increased parsing costs due to the vast\nvolume of log data and limited context size of LLMs, and 3)privacy risks from\nusing commercial models like ChatGPT with sensitive log information. To\novercome these limitations, this paper introduces OpenLogParser, an\nunsupervised log parsing approach that leverages open-source LLMs (i.e.,\nLlama3-8B) to enhance privacy and reduce operational costs while achieving\nstate-of-the-art parsing accuracy. OpenLogParser first groups logs with similar\nstatic text but varying dynamic variables using a fixed-depth grouping tree. It\nthen parses logs within these groups using three components: i)similarity\nscoring-based retrieval augmented generation: selects diverse logs within each\ngroup based on Jaccard similarity, helping the LLM distinguish between static\ntext and dynamic variables; ii)self-reflection: iteratively query LLMs to\nrefine log templates to improve parsing accuracy; and iii) log template memory:\nstores parsed templates to reduce LLM queries for improved parsing efficiency.\nOur evaluation on LogHub-2.0 shows that OpenLogParser achieves 25% higher\nparsing accuracy and processes logs 2.7 times faster compared to\nstate-of-the-art LLM-based parsers. In short, OpenLogParser addresses privacy\nand cost concerns of using commercial LLMs while achieving state-of-the-arts\nparsing efficiency and accuracy.\n', ""  Logs are a first-hand source of information for software maintenance and\nfailure diagnosis. Log parsing, which converts semi-structured log messages\ninto structured templates, is a prerequisite for automated log analysis tasks\nsuch as anomaly detection, troubleshooting, and root cause analysis. However,\nexisting log parsers fail in real-world systems for three main reasons. First,\ntraditional heuristics-based parsers require handcrafted features and domain\nknowledge, which are difficult to generalize at scale. Second, existing large\nlanguage model-based parsers rely on periodic offline processing, limiting\ntheir effectiveness in real-time use cases. Third, existing online parsing\nalgorithms are susceptible to log drift, where slight log changes create false\npositives that drown out real anomalies. To address these challenges, we\npropose HELP, a Hierarchical Embeddings-based Log Parser. HELP is the first\nonline semantic-based parser to leverage LLMs for performant and cost-effective\nlog parsing. We achieve this through a novel hierarchical embeddings module,\nwhich fine-tunes a text embedding model to cluster logs before parsing,\nreducing querying costs by multiple orders of magnitude. To combat log drift,\nwe also develop an iterative rebalancing module, which periodically updates\nexisting log groupings. We evaluate HELP extensively on 14 public large-scale\ndatasets, showing that HELP achieves significantly higher F1-weighted grouping\nand parsing accuracy than current state-of-the-art online log parsers. We also\nimplement HELP into Iudex's production observability platform, confirming\nHELP's practicality in a production environment. Our results show that HELP is\neffective and efficient for high-throughput real-world log parsing.\n""]",Log Parsing with Large Language Models,Optimization and Efficiency of Large Language Models,Large Language Models,Large Language Models
519,10,519_rumors_tweets_rumor_news,"['rumors', 'tweets', 'rumor', 'news', 'blogging', 'credibility', 'virality', 'viral', 'textual', 'debunk']","['rumor', 'rumors', 'propagation', 'virality', 'detection', 'media', 'spread', 'early', 'social', 'comments']","[""  The wide spread of rumors on social media has caused a negative impact on\npeople's daily life, leading to potential panic, fear, and mental health\nproblems for the public. How to debunk rumors as early as possible remains a\nchallenging problem. Existing studies mainly leverage information propagation\nstructure to detect rumors, while very few works focus on correlation among\nusers that they may coordinate to spread rumors in order to gain large\npopularity. In this paper, we propose a new detection model, that jointly\nlearns both the representations of user correlation and information propagation\nto detect rumors on social media. Specifically, we leverage graph neural\nnetworks to learn the representations of user correlation from a bipartite\ngraph that describes the correlations between users and source tweets, and the\nrepresentations of information propagation with a tree structure. Then we\ncombine the learned representations from these two modules to classify the\nrumors. Since malicious users intend to subvert our model after deployment, we\nfurther develop a greedy attack scheme to analyze the cost of three adversarial\nattacks: graph attack, comment attack, and joint attack. Evaluation results on\ntwo public datasets illustrate that the proposed MODEL outperforms the\nstate-of-the-art rumor detection models. We also demonstrate our method\nperforms well for early rumor detection. Moreover, the proposed detection\nmethod is more robust to adversarial attacks compared to the best existing\nmethod. Importantly, we show that it requires a high cost for attackers to\nsubvert user correlation pattern, demonstrating the importance of considering\nuser correlation for rumor detection.\n"", '  Recently a lot of progress has been made in rumor modeling and rumor\ndetection for micro-blogging streams. However, existing automated methods do\nnot perform very well for early rumor detection, which is crucial in many\nsettings, e.g., in crisis situations. One reason for this is that aggregated\nrumor features such as propagation features, which work well on the long run,\nare - due to their accumulating characteristic - not very helpful in the early\nphase of a rumor. In this work, we present an approach for early rumor\ndetection, which leverages Convolutional Neural Networks for learning the\nhidden representations of individual rumor-related tweets to gain insights on\nthe credibility of each tweets. We then aggregate the predictions from the very\nbeginning of a rumor to obtain the overall event credits (so-called wisdom),\nand finally combine it with a time series based rumor classification model. Our\nextensive experiments show a clearly improved classification performance within\nthe critical very first hours of a rumor. For a better understanding, we also\nconduct an extensive feature evaluation that emphasized on the early stage and\nshows that the low-level credibility has best predictability at all phases of\nthe rumor lifetime.\n', ""  A crucial aspect of a rumor detection model is its ability to generalize,\nparticularly its ability to detect emerging, previously unknown rumors. Past\nresearch has indicated that content-based (i.e., using solely source posts as\ninput) rumor detection models tend to perform less effectively on unseen\nrumors. At the same time, the potential of context-based models remains largely\nuntapped. The main contribution of this paper is in the in-depth evaluation of\nthe performance gap between content and context-based models specifically on\ndetecting new, unseen rumors. Our empirical findings demonstrate that\ncontext-based models are still overly dependent on the information derived from\nthe rumors' source post and tend to overlook the significant role that\ncontextual information can play. We also study the effect of data split\nstrategies on classifier performance. Based on our experimental results, the\npaper also offers practical suggestions on how to minimize the effects of\ntemporal concept drift in static datasets during the training of rumor\ndetection methods.\n""]",Rumor Detection on Social Media,Misinformation and Disinformation Detection,Information Verification and Validation,Information Verification and Validation
520,10,520_inceptionv3_dnns_dnn_deepfault,"['inceptionv3', 'dnns', 'dnn', 'deepfault', 'tensorflow', 'deep', 'testability', 'deepgd', 'faults', 'neuron']","['fault', 'conversion', 'testing', 'test', 'inputs', 'converters', 'faults', 'coverage', 'failures', 'interoperability']","['  When deploying Deep Neural Networks (DNNs), developers often convert models\nfrom one deep learning framework to another (e.g., TensorFlow to PyTorch).\nHowever, this process is error-prone and can impact target model accuracy. To\nidentify the extent of such impact, we perform and briefly present a\ndifferential analysis against three DNNs widely used for image recognition\n(MobileNetV2, ResNet101, and InceptionV3) converted across four well-known deep\nlearning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which\nrevealed numerous model crashes and output label discrepancies of up to 100%.\nTo mitigate such errors, we present a novel approach towards fault localization\nand repair of buggy deep learning framework conversions, focusing on\npre-trained image recognition models. Our technique consists of four stages of\nanalysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters,\nand 4) graph representation. In addition, we propose various strategies towards\nfault repair of the faults detected. We implement our technique on top of the\nApache TVM deep learning compiler, and we test it by conducting a preliminary\nfault localization analysis for the conversion of InceptionV3 from TF to\nTFLite. Our approach detected a fault in a common DNN converter tool, which\nintroduced precision errors in weights, reducing model accuracy. After our\nfault localization, we repaired the issue, reducing our conversion error to\nzero.\n', '  Deep neural networks (DNNs) are widely used in various application domains\nsuch as image processing, speech recognition, and natural language processing.\nHowever, testing DNN models may be challenging due to the complexity and size\nof their input domain. Particularly, testing DNN models often requires\ngenerating or exploring large unlabeled datasets. In practice, DNN test\noracles, which identify the correct outputs for inputs, often require expensive\nmanual effort to label test data, possibly involving multiple experts to ensure\nlabeling correctness. In this paper, we propose DeepGD, a black-box\nmulti-objective test selection approach for DNN models. It reduces the cost of\nlabeling by prioritizing the selection of test inputs with high fault revealing\npower from large unlabeled datasets. DeepGD not only selects test inputs with\nhigh uncertainty scores to trigger as many mispredicted inputs as possible but\nalso maximizes the probability of revealing distinct faults in the DNN model by\nselecting diverse mispredicted inputs. The experimental results conducted on\nfour widely used datasets and five DNN models show that in terms of\nfault-revealing ability: (1) White-box, coverage-based approaches fare poorly,\n(2) DeepGD outperforms existing black-box test selection approaches in terms of\nfault detection, and (3) DeepGD also leads to better guidance for DNN model\nretraining when using selected inputs to augment the training set.\n', ""  Despite deep learning's transformative impact on various domains, the\nreliability of Deep Neural Networks (DNNs) is still a pressing concern due to\ntheir complexity and data dependency. Traditional software fault localization\ntechniques, such as Spectrum-based Fault Localization (SBFL), have been adapted\nto DNNs with limited success. Existing methods like DeepFault utilize SBFL\nmeasures but fail to account for fault propagation across neural pathways,\nleading to suboptimal fault detection. Addressing this gap, we propose the\nNP-SBFL method, leveraging Layer-wise Relevance Propagation (LRP) to identify\nand verify critical neural pathways. Our innovative multi-stage gradient ascent\n(MGA) technique, an extension of gradient ascent (GA), activates neurons\nsequentially, enhancing fault detection efficacy. We evaluated the\neffectiveness of our method, i.e. NP-SBFL-MGA, on two commonly used datasets,\nMNIST and CIFAR-10, two baselines DeepFault and NP- SBFL-GA, and three\nsuspicious neuron measures, Tarantula, Ochiai, and Barinel. The empirical\nresults showed that NP-SBFL-MGA is statistically more effective than the\nbaselines at identifying suspicious paths and synthesizing adversarial inputs.\nParticularly, Tarantula on NP-SBFL-MGA had the highest fault detection rate at\n96.75%, surpassing DeepFault on Ochiai (89.90%) and NP-SBFL-GA on Ochiai\n(60.61%). Our approach also yielded results comparable to those of the\nbaselines in synthesizing naturalness inputs, and we found a positive\ncorrelation between the coverage of critical paths and the number of failed\ntests in DNN fault localization.\n""]",Deep Neural Network Testing and Fault Localization,Deep Learning for Defect Detection and Testing,Deep Learning Applications in Engineering and Computer Vision,Deep Learning Applications in Engineering and Computer Vision
521,10,521_shap_shapley_ranking_feature,"['shap', 'shapley', 'ranking', 'feature', 'generalising', 'probabilistic', 'kernelshap', 'attributions', 'rank', 'complexity']","['axiomatic', 'attribution', 'characteristic', 'scores', 'values', 'game', 'value', 'rank', 'axioms', 'feature']","['  Recent work uncovered examples of classifiers for which SHAP scores yield\nmisleading feature attributions. While such examples might be perceived as\nsuggesting the inadequacy of Shapley values for explainability, this paper\nshows that the source of the identified shortcomings of SHAP scores resides\nelsewhere. Concretely, the paper makes the case that the failings of SHAP\nscores result from the characteristic functions used in earlier works.\nFurthermore, the paper identifies a number of properties that characteristic\nfunctions ought to respect, and proposes several novel characteristic\nfunctions, each exhibiting one or more of the desired properties. More\nimportantly, some of the characteristic functions proposed in this paper are\nguaranteed not to exhibit any of the shortcomings uncovered by earlier work.\nThe paper also investigates the impact of the new characteristic functions on\nthe complexity of computing SHAP scores. Finally, the paper proposes\nmodifications to the tool SHAP to use instead one of our novel characteristic\nfunctions, thereby eliminating some of the limitations reported for SHAP\nscores.\n', '  Attribution scores reflect how important the feature values in an input\nentity are for the output of a machine learning model. One of the most popular\nattribution scores is the SHAP score, which is an instantiation of the general\nShapley value used in coalition game theory. The definition of this score\nrelies on a probability distribution on the entity population. Since the exact\ndistribution is generally unknown, it needs to be assigned subjectively or be\nestimated from data, which may lead to misleading feature scores. In this\npaper, we propose a principled framework for reasoning on SHAP scores under\nunknown entity population distributions. In our framework, we consider an\nuncertainty region that contains the potential distributions, and the SHAP\nscore of a feature becomes a function defined over this region. We study the\nbasic problems of finding maxima and minima of this function, which allows us\nto determine tight ranges for the SHAP scores of all features. In particular,\nwe pinpoint the complexity of these problems, and other related ones, showing\nthem to be NP-complete. Finally, we present experiments on a real-world\ndataset, showing that our framework may contribute to a more robust feature\nscoring.\n', '  Several works propose various post-hoc, model-agnostic explanations for the\ntask of ranking, i.e. the task of ordering a set of documents, via feature\nattribution methods. However, these attributions are seen to weakly correlate\nand sometimes contradict each other. In classification/regression, several\nworks focus on \\emph{axiomatic characterization} of feature attribution\nmethods, showing that a certain method uniquely satisfies a set of desirable\nproperties. However, no such efforts have been taken in the space of feature\nattributions for the task of ranking. We take an axiomatic game-theoretic\napproach, popular in the feature attribution community, to identify candidate\nattribution methods for ranking tasks. We first define desirable axioms:\nRank-Efficiency, Rank-Missingness, Rank-Symmetry and Rank-Monotonicity, all\nvariants of the classical Shapley axioms. Next, we introduce Rank-SHAP, a\nfeature attribution algorithm for the general ranking task, which is an\nextension to classical Shapley values. We identify a polynomial-time algorithm\nfor computing approximate Rank-SHAP values and evaluate the computational\nefficiency and accuracy of our algorithm under various scenarios. We also\nevaluate its alignment with human intuition with a user study. Lastly, we\ntheoretically examine popular rank attribution algorithms, EXS and Rank-LIME,\nand evaluate their capacity to satisfy the classical Shapley axioms.\n']",SHAP Scores and Feature Attributions for Machine Learning,Interpretable Machine Learning and Data Valuation,Explainable Artificial Intelligence,Artificial Intelligence and Machine Learning Interpretability and Explainability
522,10,522_adversarial_exploitability_attacker_malicious,"['adversarial', 'exploitability', 'attacker', 'malicious', 'vulnerability', 'attacks', 'retrieval', 'security', 'attackers', 'adversary']","['passages', 'attacker', 'database', 'retrieval', 'attack', 'attacks', 'documents', 'malicious', 'queries', 'denial']","[""  Retrieval Augmented Generation (RAG) expands the capabilities of modern large\nlanguage models (LLMs) in chatbot applications, enabling developers to adapt\nand personalize the LLM output without expensive training or fine-tuning. RAG\nsystems use an external knowledge database to retrieve the most relevant\ndocuments for a given query, providing this context to the LLM generator. While\nRAG achieves impressive utility in many applications, its adoption to enable\npersonalized generative models introduces new security risks. In this work, we\npropose new attack surfaces for an adversary to compromise a victim's RAG\nsystem, by injecting a single malicious document in its knowledge database. We\ndesign Phantom, general two-step attack framework against RAG augmented LLMs.\nThe first step involves crafting a poisoned document designed to be retrieved\nby the RAG system within the top-k results only when an adversarial trigger, a\nspecific sequence of words acting as backdoor, is present in the victim's\nqueries. In the second step, a specially crafted adversarial string within the\npoisoned document triggers various adversarial attacks in the LLM generator,\nincluding denial of service, reputation damage, privacy violations, and harmful\nbehaviors. We demonstrate our attacks on multiple LLM architectures, including\nGemma, Vicuna, and Llama.\n"", '  Large Language Models (LLMs) are constrained by outdated information and a\ntendency to generate incorrect data, commonly referred to as ""hallucinations.""\nRetrieval-Augmented Generation (RAG) addresses these limitations by combining\nthe strengths of retrieval-based methods and generative models. This approach\ninvolves retrieving relevant information from a large, up-to-date dataset and\nusing it to enhance the generation process, leading to more accurate and\ncontextually appropriate responses. Despite its benefits, RAG introduces a new\nattack surface for LLMs, particularly because RAG databases are often sourced\nfrom public data, such as the web. In this paper, we propose \\TrojRAG{} to\nidentify the vulnerabilities and attacks on retrieval parts (RAG database) and\ntheir indirect attacks on generative parts (LLMs). Specifically, we identify\nthat poisoning several customized content passages could achieve a retrieval\nbackdoor, where the retrieval works well for clean queries but always returns\ncustomized poisoned adversarial queries. Triggers and poisoned passages can be\nhighly customized to implement various attacks. For example, a trigger could be\na semantic group like ""The Republican Party, Donald Trump, etc."" Adversarial\npassages can be tailored to different contents, not only linked to the triggers\nbut also used to indirectly attack generative LLMs without modifying them.\nThese attacks can include denial-of-service attacks on RAG and semantic\nsteering attacks on LLM generations conditioned by the triggers. Our\nexperiments demonstrate that by just poisoning 10 adversarial passages can\ninduce 98.2\\% success rate to retrieve the adversarial passages. Then, these\npassages can increase the reject ratio of RAG-based GPT-4 from 0.01\\% to 74.6\\%\nor increase the rate of negative responses from 0.22\\% to 72\\% for targeted\nqueries.\n', '  Large language models (LLMs) have achieved remarkable success due to their\nexceptional generative capabilities. Despite their success, they also have\ninherent limitations such as a lack of up-to-date knowledge and hallucination.\nRetrieval-Augmented Generation (RAG) is a state-of-the-art technique to\nmitigate these limitations. The key idea of RAG is to ground the answer\ngeneration of an LLM on external knowledge retrieved from a knowledge database.\nExisting studies mainly focus on improving the accuracy or efficiency of RAG,\nleaving its security largely unexplored. We aim to bridge the gap in this work.\nWe find that the knowledge database in a RAG system introduces a new and\npractical attack surface. Based on this attack surface, we propose PoisonedRAG,\nthe first knowledge corruption attack to RAG, where an attacker could inject a\nfew malicious texts into the knowledge database of a RAG system to induce an\nLLM to generate an attacker-chosen target answer for an attacker-chosen target\nquestion. We formulate knowledge corruption attacks as an optimization problem,\nwhose solution is a set of malicious texts. Depending on the background\nknowledge (e.g., black-box and white-box settings) of an attacker on a RAG\nsystem, we propose two solutions to solve the optimization problem,\nrespectively. Our results show PoisonedRAG could achieve a 90% attack success\nrate when injecting five malicious texts for each target question into a\nknowledge database with millions of texts. We also evaluate several defenses\nand our results show they are insufficient to defend against PoisonedRAG,\nhighlighting the need for new defenses.\n']",Adversarial Attacks on Retrieval-Augmented Language Models,Adversarial Attacks and Defenses in AI and Machine Learning Models,Adversarial Attacks and Vulnerabilities in AI and Machine Learning,Adversarial Attacks and Vulnerabilities in AI and Machine Learning
