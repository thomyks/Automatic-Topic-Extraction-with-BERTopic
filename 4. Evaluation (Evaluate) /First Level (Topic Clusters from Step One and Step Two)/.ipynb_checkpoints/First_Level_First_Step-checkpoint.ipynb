{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a7dff6-1614-45e3-b677-5693928f8b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Diversity\n",
    "# Describe the details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b86edb-8bf5-4738-abf0-22310fd2979b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yc656703/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yc656703/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Diversity: 0.429073482428115\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load your dataset\n",
    "file_path = 'Second_step_clustering_BERTopic_results.csv'  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Example: Assuming you have a separate list of documents\n",
    "documents = data['text']\n",
    "\n",
    "# Preprocess documents\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "processed_docs = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Extract topics after filtering\n",
    "topic_representations = data.groupby('Topic Label')['Second_Step_Topic_Keywords'].apply(lambda x: x.iloc[0].split(', '))\n",
    "topics = topic_representations.tolist()\n",
    "\n",
    "# Calculate Topic Diversity\n",
    "def calculate_topic_diversity(topics):\n",
    "    unique_words = set()\n",
    "    total_words = 0\n",
    "\n",
    "    for topic in topics:\n",
    "        unique_words.update(topic)  # Add words to the unique set\n",
    "        total_words += len(topic)   # Count total words in all topics\n",
    "\n",
    "    # Topic diversity is the proportion of unique words to total words\n",
    "    topic_diversity = len(unique_words) / total_words\n",
    "    return topic_diversity\n",
    "\n",
    "# Calculate topic diversity\n",
    "topic_diversity = calculate_topic_diversity(topics)\n",
    "\n",
    "# Display the topic diversity score\n",
    "print(f\"\\nTopic Diversity: {topic_diversity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb8e24-02a3-4bad-bc42-c9588875dfa2",
   "metadata": {},
   "source": [
    "Topic Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb9d146-c0f0-4baa-ac51-2d477359eae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hypergraph', 'subgraph', 'graphs', 'hypergraphs', 'graph', 'adjacency', 'networks', 'nodes', 'vertices', 'embeddings']]\n",
      "0.57424560033956\n",
      "[['federated', 'privacy', 'learning', 'datasets', 'collaborative', 'distributed', 'trained', 'sharing', 'fedllm', 'collaboratively']]\n",
      "0.5736654091434796\n",
      "[['graphs', 'subgraph', 'nodes', 'networks', 'graph', 'subgraphs', 'supervised', 'node', 'learning', 'labeled']]\n",
      "0.6255834867442949\n",
      "[['captioning', 'multimodal', 'embeddings', 'recognition', 'captions', 'encoder', 'embedding', 'retrieval', 'visual', 'text']]\n",
      "0.6520743704244423\n",
      "[['learns', 'reinforcement', 'robotic', 'robotics', 'robot', 'learning', 'robots', 'exploration', 'learned', 'imitation']]\n",
      "0.6901123007785468\n",
      "[['mri', 'imaging', 'neuroimaging', 'tomography', 'fmri', 'segmentation', 'dataset', 'deep', 'images', 'brain']]\n",
      "0.5866121210747726\n",
      "[['attention', 'rnns', 'memory', 'nlp', 'learn', 'recurrent', 'examples', 'context', 'language', 'transformers']]\n",
      "0.4572251851231385\n",
      "[['audio', 'microphone', 'encoder', 'recording', 'acoustic', 'supervised', 'acoustics', 'wavlm', 'recognition', 'auditory']]\n",
      "0.5093620848200476\n",
      "[['adversarial', 'malicious', 'vulnerabilities', 'attacker', 'threats', 'vulnerability', 'security', 'attacks', 'attackers', 'vulnerable']]\n",
      "0.893062211538694\n",
      "[['utterances', 'utterance', 'transcription', 'speech', 'dialogue', 'voice', 'transcripts', 'multilingual', 'conversational', 'decoder']]\n",
      "0.6223952725024909\n",
      "[['reinforcement', 'reward', 'rewards', 'learning', 'agents', 'agent', 'learned', 'automaton', 'autonomous', 'tasks']]\n",
      "0.6226387660245961\n",
      "[['sparse', 'lasso', 'optimal', 'iterative', 'optimization', 'algorithms', 'matrix', 'matrices', 'spectral', 'complexity']]\n",
      "0.5109648528342654\n",
      "[['forecast', 'forecasts', 'forecasting', 'meteorological', 'predicting', 'spatiotemporal', 'predict', 'climate', 'prediction', 'weather']]\n",
      "0.806511666042215\n",
      "[['reinforcement', 'planning', 'learning', 'exploration', 'actions', 'rewards', 'learned', 'reward', 'critic', 'control']]\n",
      "0.6747447339135065\n",
      "[['reinforcement', 'optimality', 'learning', 'optimal', 'critic', 'policies', 'policy', 'markovian', 'reward', 'exploration']]\n",
      "0.6840010068280248\n",
      "[['learns', 'reinforcement', 'learning', 'planning', 'supervised', 'learned', 'trained', 'reward', 'rewards', 'learn']]\n",
      "0.582686499438803\n",
      "[['bias', 'biases', 'stereotypes', 'attitudes', 'stereotyping', 'perceptions', 'stereotypical', 'language', 'demographics', 'culturally']]\n",
      "0.4767092015661568\n",
      "[['privacy', 'adversarial', 'private', 'confidential', 'obfuscation', 'datasets', 'unlearning', 'deep', 'secure', 'learning']]\n",
      "0.4215601998148699\n",
      "[['graphs', 'networks', 'graph', 'graphgpt', 'adjacency', 'neural', 'nodes', 'node', 'edges', 'network']]\n",
      "0.6778989483417048\n",
      "[['visual', 'multimodal', 'recognition', 'vision', 'embedding', 'language', 'linguistic', 'concepts', 'semantic', 'vlm']]\n",
      "0.5688168435607891\n",
      "[['federated', 'privacy', 'distributed', 'cloud', 'collaborative', 'shared', 'cluster', 'sharing', 'networks', 'fedmap']]\n",
      "0.5594227651617444\n",
      "[['graphnas', 'networks', 'graphs', 'subgraph', 'graphstorm', 'nodes', 'graph', 'gnn', 'gnns', 'neural']]\n",
      "0.5585220983367701\n",
      "[['kernels', 'kernel', 'approximations', 'rkhs', 'gaussian', 'hessian', 'approximation', 'neural', 'spectral', 'dimensional']]\n",
      "0.5844312668341587\n",
      "[['parallelization', 'fpgas', 'cnns', 'fpga', 'throughput', 'accelerator', 'accelerators', 'hardware', 'memory', 'microarchitecture']]\n",
      "0.6457557331872021\n",
      "[['cloud', 'edge', 'federated', 'networks', 'servers', 'bandwidth', 'iot', 'serverless', 'network', 'distributed']]\n",
      "0.5975547329894593\n",
      "[['federated', 'distributed', 'learning', 'synchronization', 'adaptive', 'sgd', 'stochastic', 'centralized', 'asynchronous', 'gradient']]\n",
      "0.6279535228060577\n",
      "[['prognosis', 'lung', 'cancer', 'ai', 'cnn', 'predict', 'radiology', 'oncology', 'prediction', 'melanoma']]\n",
      "0.37034002175930564\n",
      "[['sentiment', 'corpus', 'tweets', 'linguistic', 'annotators', 'annotations', 'twitter', 'lexicon', 'annotation', 'textual']]\n",
      "0.6018601906581919\n",
      "[['ranking', 'evaluations', 'rankings', 'evaluation', 'language', 'assessment', 'responses', 'bias', 'questioner', 'prompts']]\n",
      "0.40233641360656414\n",
      "[['adversarial', 'teaming', 'threat', 'unsafe', 'attacker', 'attackers', 'attacking', 'vulnerability', 'vulnerabilities', 'attacks']]\n",
      "0.6610684848343811\n",
      "[['pretraining', 'trained', 'unlearning', 'pretrained', 'training', 'benchmarks', 'tuning', 'datasets', 'dataset', 'tuned']]\n",
      "0.47550070744908696\n",
      "[['modeling', 'simulations', 'learning', 'neural', 'dynamics', 'networks', 'nonlinear', 'pdes', 'simulation', 'computational']]\n",
      "0.6042479975588739\n",
      "[['dynamics', 'learning', 'dynamical', 'neural', 'modeling', 'odes', 'networks', 'dynamic', 'nonlinear', 'hamiltonian']]\n",
      "0.6228515057315229\n",
      "[['editing', 'gans', 'generative', 'gan', 'blur', 'denoising', 'attention', 'diffusion', 'images', 'pixel']]\n",
      "0.4586409529173262\n",
      "[['factorization', 'dimensionality', 'supervised', 'autoencoders', 'regularization', 'features', 'discriminative', 'dimensional', 'matrix', 'feature']]\n",
      "0.5224058932387785\n",
      "[['recommender', 'recommendation', 'recommendations', 'personalized', 'popularity', 'preference', 'spotify', 'preferences', 'playlists', 'rankings']]\n",
      "0.4839662911252419\n",
      "[['transportation', 'traffic', 'routes', 'ridesharing', 'travelers', 'travel', 'trips', 'transport', 'mobility', 'passengers']]\n",
      "0.6011799423206865\n",
      "[['optimality', 'reinforcement', 'optimal', 'bandit', 'mdps', 'learning', 'mdp', 'policies', 'exploration', 'policy']]\n",
      "0.7609694065485028\n",
      "[['diffusion', 'models', 'denoising', 'generative', 'stochastic', 'denoisers', 'denoiser', 'priors', 'brownian', 'sampling']]\n",
      "0.5338525238975892\n",
      "[['bert', 'encoder', 'encoders', 'encode', 'embeddings', 'tokenizer', 'tokenization', 'decoder', 'embedding', 'representations']]\n",
      "0.6409189059228149\n",
      "[['privacy', 'adversary', 'security', 'attacks', 'secure', 'federated', 'confidentiality', 'malicious', 'attacker', 'protect']]\n",
      "0.7439415730026335\n",
      "[['answerability', 'answering', 'questions', 'annotators', 'retrieval', 'comprehension', 'responses', 'answers', 'questionnaires', 'texts']]\n",
      "0.5924581566800334\n",
      "[['tweets', 'twitter', 'sentiment', 'reddit', 'hashtags', 'communities', 'sentiments', 'news', 'epidemic', 'content']]\n",
      "0.6187680166348878\n",
      "[['captioning', 'captions', 'multimodal', 'caption', 'multilingual', 'visual', 'translation', 'textual', 'text', 'benchmark']]\n",
      "0.6395354799304405\n",
      "[['speechtokenizer', 'voice', 'vocalizations', 'voices', 'utterances', 'audio', 'speaker', 'speech', 'vocal', 'acoustic']]\n",
      "0.7052786353764221\n",
      "[['reinforcement', 'reward', 'rewards', 'learning', 'rl', 'offline', 'learned', 'learner', 'optimal', 'exploration']]\n",
      "0.7046511120695412\n",
      "[['variational', 'dynamical', 'neural', 'stochastic', 'dynamics', 'kalman', 'nonlinear', 'ensemble', 'spatiotemporal', 'simulations']]\n",
      "0.6682045246231554\n",
      "[['diffusion', 'denoising', 'denoiser', 'gans', 'denoised', 'generative', 'imagenet', 'images', 'modeling', 'gan']]\n",
      "0.5848109638969549\n",
      "[['segmentation', 'imaging', 'supervised', 'mri', 'microscopy', 'tomography', 'images', 'datasets', 'unsupervised', 'detection']]\n",
      "0.5557133355338986\n",
      "[['attention', 'memory', 'sparse', 'recurrent', 'recall', 'tasks', 'efficient', 'language', 'chunk', 'longlora']]\n",
      "0.3312199206995437\n",
      "[['utterances', 'phonetics', 'phonetic', 'speech', 'phonetically', 'voice', 'corpus', 'asr', 'pronunciation', 'multilingual']]\n",
      "0.62860088925881\n",
      "[['multimodal', 'embedding', 'supervised', 'modality', 'ehr', 'ehrs', 'health', 'medical', 'structured', 'unstructured']]\n",
      "0.5982948494737623\n",
      "[['privacy', 'federated', 'private', 'adversarial', 'distributed', 'sharing', 'learning', 'secret', 'security', 'decentralized']]\n",
      "0.711697326677502\n",
      "[['seismic', 'waveform', 'deep', 'inversion', 'predicting', 'forecasting', 'neural', 'prediction', 'deeponet', 'wave']]\n",
      "0.45785813822637395\n",
      "[['caching', 'cache', 'caches', 'memory', 'cachedattention', 'quantization', 'storage', 'attention', 'compression', 'efficient']]\n",
      "0.4757688704696217\n",
      "[['speaker', 'utterances', 'diarization', 'voice', 'voices', 'speech', 'audio', 'microphone', 'corpus', 'transcription']]\n",
      "0.7145629378456411\n",
      "[['optimal', 'allocation', 'equilibrium', 'incentive', 'bandits', 'equilibria', 'auctions', 'games', 'markets', 'bandit']]\n",
      "0.40894143297528096\n",
      "[['corpus', 'zeroshot', 'annotation', 'annotate', 'annotated', 'nlp', 'classification', 'shot', 'nlg', 'sentences']]\n",
      "0.4005141636817079\n",
      "[['wasserstein', 'gradient', 'variational', 'divergence', 'divergences', 'generative', 'flow', 'minimization', 'stochastic', 'langevin']]\n",
      "0.5747738241392282\n",
      "[['biomarkers', 'ai', 'diagnoses', 'diagnosis', 'diagnostic', 'predicting', 'classification', 'predictive', 'interpretability', 'prediction']]\n",
      "0.5005822707209258\n",
      "[['boltzmann', 'molecule', 'molecules', 'molecular', 'simulations', 'sampling', 'kinetic', 'atomnet', 'particles', 'flow']]\n",
      "0.41160550047858246\n",
      "[['annotated', 'instruction', 'responses', 'language', 'metacognitive', 'feedback', 'tasks', 'examples', 'reinforcement', 'text']]\n",
      "0.37780477912980504\n",
      "[['pose', 'poses', 'camera', 'cameras', 'scenes', '3d', 'articulated', 'vision', 'scene', 'slam']]\n",
      "0.6043217508400683\n",
      "[['visual', 'supervised', 'learning', 'multimodal', 'prompts', 'captioning', 'embeddings', 'trained', 'captions', 'prompt']]\n",
      "0.4957250493332735\n",
      "[['bandits', 'bandit', 'optimal', 'regret', 'optimization', 'reward', 'games', 'guarantees', 'exploration', 'rewards']]\n",
      "0.7471854620519426\n",
      "[['graphs', 'graphon', 'graph', 'adjacency', 'spectral', 'nodes', 'networks', 'vertex', 'edges', 'clustering']]\n",
      "0.6818154772016267\n",
      "[['reinforcement', 'agents', 'games', 'agent', 'rewards', 'cooperation', 'reward', 'opponents', 'strategies', 'cooperative']]\n",
      "0.734550783273643\n",
      "[['adversarial', 'gans', 'watermarking', 'steganalysis', 'copyright', 'steganography', 'deepfacegen', 'watermark', 'spoofing', 'impersonation']]\n",
      "0.2555940878347858\n",
      "[['posterior', 'bayesian', 'generative', 'phylogenetic', 'likelihood', 'inference', 'models', 'estimating', 'markov', 'monte']]\n",
      "0.615663633375027\n",
      "[['dialogues', 'dialogue', 'dialog', 'conversation', 'conversational', 'chatbots', 'conversations', 'chatbot', 'chat', 'planning']]\n",
      "0.7833439945874182\n",
      "[['traffic', 'accidentgpt', 'driving', 'accidents', 'roads', 'lanes', 'crash', 'vehicles', 'predicting', 'prediction']]\n",
      "0.6430297116272601\n",
      "[['cnn', 'cnns', 'efficientnetv2b1', 'memory', 'optimized', 'networks', 'neural', 'deep', 'dnns', 'dnn']]\n",
      "0.6460324009171957\n",
      "[['supervised', 'classification', 'learning', 'adversarial', 'labeling', 'deep', 'unlearning', 'trained', 'labeled', 'labels']]\n",
      "0.42914515130516084\n",
      "[['normalization', 'regularization', 'neural', 'generalization', 'backpropagation', 'convolutional', 'neuron', 'learning', 'networks', 'deep']]\n",
      "0.5831028056316273\n",
      "[['classifiers', 'learning', 'classification', 'classifier', 'generalization', 'regularization', 'optimal', 'memorization', 'minimax', 'distributionally']]\n",
      "0.33083837572630836\n",
      "[['planning', 'robotics', 'planner', 'robotic', 'robot', 'planners', 'robots', 'motions', 'motion', 'trajectory']]\n",
      "0.7629548132575179\n",
      "[['reservoir', 'deep', 'flow', 'flows', 'predicting', 'modeling', 'predict', 'simulations', 'neural', 'prediction']]\n",
      "0.5414759597664193\n",
      "[['saliency', 'neural', 'attention', 'recognition', 'visual', 'attentions', 'vision', 'representations', 'images', 'perception']]\n",
      "0.4413850849177832\n",
      "[['stochastic', 'brownian', 'discretization', 'diffusion', 'gaussian', 'sde', 'drift', 'probabilistic', 'sdes', 'mcmc']]\n",
      "0.5379160385172506\n",
      "[['hippocampal', 'planning', 'hippocampus', 'exploration', 'cortex', 'cognitive', 'environments', 'reinforcement', 'behaviors', 'adaptive']]\n",
      "0.34273998223464497\n",
      "[['reinforcement', 'objectives', 'optimal', 'critic', 'learning', 'reward', 'objective', 'rewards', 'optimization', 'control']]\n",
      "0.6073144625657866\n",
      "[['textual', 'nlp', 'summarization', 'linguistic', 'texts', 'evaluation', 'text', 'generated', 'language', 'lingual']]\n",
      "0.5748768502257013\n",
      "[['predicting', 'biomarkers', 'prediction', 'personalized', 'learning', 'lstm', 'biomedical', 'cancer', 'genomic', 'genome']]\n",
      "0.3660681222840737\n",
      "[['adversarial', 'privacy', 'security', 'anonymization', 'protected', 'adversary', 'secure', 'confidentiality', 'protection', 'attacks']]\n",
      "0.656086298885316\n",
      "[['compression', 'compressed', 'memory', 'softmax', 'decoding', 'tokenizers', 'decoder', 'tokenization', 'tokenizer', 'attention']]\n",
      "0.39175637294752885\n",
      "[['mcmc', 'bayesian', 'probabilistic', 'posteriors', 'markov', 'posterior', 'stochastic', 'bayesmbar', 'monte', 'dmc']]\n",
      "0.56104711338069\n",
      "[['knowledge', 'contexts', 'language', 'entailment', 'comprehension', 'memory', 'paraphrases', 'nlp', 'facts', 'retrieval']]\n",
      "0.438722410908036\n",
      "[['opioids', 'opioid', 'nlp', 'annotated', 'tweets', 'drug', 'classification', 'text', 'addiction', 'posts']]\n",
      "0.4618433198693972\n",
      "[['multimodal', 'modality', 'dialogues', 'visual', 'models', 'mllm', 'interactive', 'conversations', 'conversational', 'dialog']]\n",
      "0.5158504646917875\n",
      "[['retrieval', 'embeddings', 'embedding', 'embed', 'textual', 'multilingual', 'search', 'encoder', 'lingual', 'relevance']]\n",
      "0.4161396768780299\n",
      "[['forgetting', 'continual', 'learning', 'memory', 'learned', 'forgotten', 'adaptive', 'regularization', 'incremental', 'trained']]\n",
      "0.46392124610761537\n",
      "[['reinforcement', 'learning', 'optimizing', 'exploration', 'reward', 'rewards', 'autonomous', 'optimization', 'optimisation', 'strategies']]\n",
      "0.5198844583884875\n",
      "[['memorizing', 'memorization', 'context', 'examples', 'skills', 'exemplars', 'answering', 'knowledge', 'language', 'reasoning']]\n",
      "0.42830458206119265\n",
      "[['bias', 'biases', 'debiasing', 'biased', 'unbiased', 'debiased', 'debias', 'annotators', 'biasalert', 'nlp']]\n",
      "0.5130729713220139\n",
      "[['cnn', 'neural', 'backpropagation', 'imagenet', 'layers', 'layer', 'convolutional', 'neurons', 'networks', 'memory']]\n",
      "0.6630450980043937\n",
      "[['workflows', 'workflow', 'automate', 'automation', 'apis', 'tools', 'interpreter', 'flowmind', 'software', 'pipelines']]\n",
      "0.4942374080087858\n",
      "[['quantization', 'compression', 'softmax', 'compressed', 'quantize', 'compressing', 'quantized', 'memory', 'pruning', 'efficient']]\n",
      "0.6592971686040026\n",
      "[['reinforcement', 'optimal', 'regret', 'adaptive', 'learning', 'optimization', 'guarantees', 'exploration', 'control', 'reward']]\n",
      "0.6212826256550781\n",
      "[['recognition', 'vision', 'detection', 'supervised', 'detectors', 'objectness', 'attention', 'yolov8', 'objects', '3d']]\n",
      "0.4495356375368771\n",
      "[['inferences', 'answering', 'reasoning', 'questions', 'retrieval', 'knowledge', 'answers', 'hop', 'language', 'contexts']]\n",
      "0.5393760988019146\n",
      "[['judgments', 'evaluations', 'bias', 'evaluation', 'annotators', 'biases', 'evaluating', 'judge', 'annotations', 'assessing']]\n",
      "0.6425216178183477\n",
      "[['navigate', 'navigating', 'navigation', 'planning', 'robotics', 'robot', 'terrain', 'slam', 'obstacle', 'exploration']]\n",
      "0.6440148713353469\n",
      "[['recourses', 'recourse', 'algorithmic', 'unfairness', 'algorithms', 'decisions', 'predictive', 'bias', 'allocation', 'fairness']]\n",
      "0.4679408474574004\n",
      "[['biases', 'bias', 'biased', 'partisan', 'debates', 'sentiment', 'propaganda', 'biasscanner', 'politically', 'political']]\n",
      "0.41033465086656057\n",
      "[['headlines', 'corpus', 'headline', 'nlp', 'texts', 'newswire', 'newsserow', 'annotated', 'multilingual', 'corpora']]\n",
      "0.3834640591685352\n",
      "[['planning', 'planner', 'pomdps', 'pomdp', 'reinforcement', 'rpomdps', 'markov', 'stochastic', 'optimal', 'pomcp']]\n",
      "0.5059523096663172\n",
      "[['memorization', 'pretrained', 'tuning', 'trained', 'language', 'tuned', 'training', 'generating', 'tasks', 'examples']]\n",
      "0.41878749914315894\n",
      "[['ai', 'generative', 'openai', 'aied', 'creativity', 'intelligence', 'artificial', 'creators', 'software', 'openness']]\n",
      "0.4325880859453246\n",
      "[['agents', 'reinforcement', 'agent', 'cooperative', 'cooperation', 'learning', 'coordination', 'cooperate', 'collaborative', 'centralized']]\n",
      "0.7194537516440109\n",
      "[['estimation', 'estimators', 'optimal', 'estimator', 'lasso', 'robust', 'outliers', 'outlier', 'unbiased', 'empirical']]\n",
      "0.46708970640656694\n",
      "[['energy', 'buildings', 'forecasting', 'buildingsbench', 'predicting', 'forecast', 'prediction', 'hvac', 'predict', 'forecasts']]\n",
      "0.5118521930216046\n",
      "[['optimizers', 'optimization', 'optimize', 'hyperparameters', 'minimization', 'hyperparameter', 'minimizing', 'optimal', 'objectives', 'constraints']]\n",
      "0.5668527264653311\n",
      "[['multimodal', 'modality', 'attention', 'semantic', 'modal', 'linking', 'semantics', 'visual', 'retrieval', 'linkage']]\n",
      "0.4179736041056044\n",
      "[['cnn', 'segmentation', 'objects', 'instance', 'detection', 'vision', 'images', 'masks', 'detectors', 'mask']]\n",
      "0.6095307510481973\n",
      "[['chatbots', 'chatbot', 'ai', 'conversational', 'intelligence', 'bots', 'communication', 'social', 'human', 'intentions']]\n",
      "0.46131807764775534\n",
      "[['nonlinear', 'fractional', 'sparse', 'solvers', 'odes', 'differential', 'pdes', 'rnns', 'solver', 'numerical']]\n",
      "0.5406120856964431\n",
      "[['planning', 'reinforcement', 'planner', 'maze', 'robotics', 'exploration', 'robot', 'autonomous', 'robotic', 'learning']]\n",
      "0.6449121755213932\n",
      "[['retrieval', 'conversational', 'conversation', 'chatbots', 'conversations', 'dialogue', 'chatbot', 'dialogues', 'dialog', 'assistant']]\n",
      "0.8091479220611898\n",
      "[['dehazing', 'haze', 'blur', 'convolutional', 'encoder', 'vision', 'deblur', 'deep', 'hazy', 'res2net']]\n",
      "0.2942918564265769\n",
      "[['programming', 'program', 'generate', 'pseudocode', 'abstractions', 'synthesis', 'algorithmic', 'programs', 'automata', 'solvers']]\n",
      "0.416642726907465\n",
      "[['wildfires', 'wildfire', 'disasters', 'flood', 'fires', 'floodwater', 'disaster', 'rescue', 'emergency', 'emergencies']]\n",
      "0.4656025832169708\n",
      "[['corpus', 'nlp', 'texts', 'sentences', 'linguistic', 'language', 'structured', 'languages', 'text', 'writing']]\n",
      "0.7279534664382916\n",
      "[['urban', 'cities', 'city', 'infrastructure', 'spatial', 'data', 'features', 'geoshapley', 'areas', 'geographical']]\n",
      "0.5013055529627366\n",
      "[['regularization', 'overfitting', 'gradients', 'minimization', 'minimizing', 'overparameterization', 'generalization', 'gradient', 'learning', 'overparameterized']]\n",
      "0.5883165187066333\n",
      "[['learnability', 'classification', 'prediction', 'optimal', 'deterministic', 'classifier', 'adversary', 'randomized', 'bandit', 'complexity']]\n",
      "0.3581527027950864\n",
      "[['forecasting', 'inventory', 'forecast', 'forecasts', 'predicting', 'prediction', 'supply', 'planning', 'analytics', 'suppliers']]\n",
      "0.485905933998921\n",
      "[['scaling', 'generalization', 'neural', 'regularization', 'sparse', 'sgd', 'networks', 'learned', 'small', 'empirically']]\n",
      "0.5157935487861535\n",
      "[['visual', '3d', 'multimodal', 'vision', 'spatial', 'interactive', 'reasoning', 'robot', 'tasks', 'robots']]\n",
      "0.5230671711442119\n",
      "[['attention', 'memory', 'softmax', 'decoder', 'layers', 'transformers', 'throughput', 'efficient', 'pruning', 'faster']]\n",
      "0.49453424479315633\n",
      "[['iot', 'iotds', 'sensor', 'rscnet', 'cloud', 'sensing', 'sensors', 'networks', 'network', 'classification']]\n",
      "0.45631422680121814\n",
      "[['vision', 'camera', 'cameras', '3d', 'rgb', 'recognition', 'scenes', 'view', 'views', 'frames']]\n",
      "0.6732745202008121\n",
      "[['chatbots', 'chatbot', 'conversational', 'conversations', 'conversation', 'ai', 'dialogue', 'messages', 'dialogues', 'chat']]\n",
      "0.7623543921275143\n",
      "[['automation', 'ai', 'architecture', 'agent', 'agents', 'software', 'industrial', 'engineering', 'intelligence', 'artificial']]\n",
      "0.5615784119308646\n",
      "[['artworks', 'artists', 'artistic', 'stylistic', 'styles', 'artness', 'artwork', 'generative', 'stylization', 'artist']]\n",
      "0.5725014648674771\n",
      "[['multimodal', 'videos', 'visual', 'language', 'tasks', 'answering', 'mllm', 'benchmark', 'mind', 'modal']]\n",
      "0.5554935844405503\n",
      "[['articulated', 'pose', 'poses', 'portrait', '3d', 'animation', 'avatars', 'vr', 'avatar', 'camera']]\n",
      "0.6195316723655279\n",
      "[['programming', 'programmers', 'programmer', 'coding', 'code', 'program', 'developers', 'software', 'agilecoder', 'prompts']]\n",
      "0.5940119169040952\n",
      "[['cognition', 'biases', 'bias', 'judgments', 'cognitive', 'behavioral', 'behavior', 'intelligence', 'decisions', 'language']]\n",
      "0.5542845814557202\n",
      "[['language', 'models', 'lms', 'decoding', 'llms', 'deepseek', 'llama2', 'llm', 'throughput', 'computational']]\n",
      "0.4275228267192981\n",
      "[['regularization', 'kernels', 'lasso', 'classification', 'learning', 'classifier', 'regularized', 'kernel', 'generalization', 'svm']]\n",
      "0.3683227028159105\n",
      "[['supervised', 'labeling', 'classification', 'imagenet', 'labeled', 'labels', 'embeddings', 'learning', 'embedding', 'label']]\n",
      "0.5544177230854992\n",
      "[['disinformation', 'misinformation', 'debunking', 'persuasive', 'deception', 'fallacy', 'biases', 'fallacies', 'bias', 'headlines']]\n",
      "0.287872847810286\n",
      "[['annotating', 'annotations', 'annotators', 'annotation', 'annotate', 'annotated', 'annotator', 'labeling', 'nlp', 'crowdsourcing']]\n",
      "0.8298440413025837\n",
      "[['hashing', 'imagenet', 'hash', 'cnn', 'cnns', 'retrieval', 'convolutional', 'images', 'features', 'embedding']]\n",
      "0.5800581088577441\n",
      "[['llms', 'language', 'modelizer', 'llm', 'benchmarks', 'expertise', 'models', 'knowledge', 'training', 'optimizers']]\n",
      "0.4232397518095393\n",
      "[['aesthetics', 'aesthetic', 'multimodal', 'visual', 'attributes', 'generative', 'creative', 'ratings', 'colors', 'perception']]\n",
      "0.5194949544209713\n",
      "[['tokenizers', 'language', 'datasets', 'nlp', 'models', 'feature', 'dataset', 'generating', 'trained', 'tokens']]\n",
      "0.45841150105626954\n",
      "[['nlg', 'text', 'generate', 'texts', 'sentences', 'generation', 'generated', 'constraints', 'language', 'lexical']]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'processed_docs' is a list of tokenized documents\n",
    "# Create a Gensim dictionary from the processed documents\n",
    "dictionary = Dictionary(processed_docs)\n",
    "\n",
    "# Initialize a list to store c_v coherence scores\n",
    "per_topic_coherence_cv = []\n",
    "\n",
    "for topic in topics:\n",
    "    # Create a list containing just the current topic\n",
    "    current_topic = [topic]\n",
    "    \n",
    "    print(current_topic)\n",
    "    # print(processed_docs)\n",
    "    # print(dictionary)\n",
    "    \n",
    "    # Initialize the CoherenceModel for the current topic using 'c_v'\n",
    "    coherence_model_cv = CoherenceModel(topics=current_topic, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "    \n",
    "    # Compute the c_v coherence score\n",
    "    coherence_cv = coherence_model_cv.get_coherence()\n",
    "    print(coherence_cv)\n",
    "    # Append the c_v score to the list\n",
    "    per_topic_coherence_cv.append(coherence_cv)\n",
    "    \n",
    "    \n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results = pd.DataFrame({\n",
    "    \"Topic\": topic_ids,\n",
    "    \"Keywords\": topic_representations.values,\n",
    "    \"Coherence c_v\": per_topic_coherence_cv,\n",
    "})\n",
    "\n",
    "\n",
    "# Display overall and per-topic coherence scores\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f4842-0774-4cc1-9c6d-82ace238c2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
