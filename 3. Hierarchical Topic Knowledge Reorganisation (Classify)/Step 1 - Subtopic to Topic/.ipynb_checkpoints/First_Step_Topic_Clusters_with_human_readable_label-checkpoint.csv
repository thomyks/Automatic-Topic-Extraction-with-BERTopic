Unnamed: 0,Topic,Count,Name,Representation,Aspect1,Aspect2,Representative_Docs,Human_Readable_Topic
0,-1,20415,-1_learning_knowledge_trained_tasks,"['learning', 'knowledge', 'trained', 'tasks', 'ai', 'deep', 'models', 'training', 'neural', 'context']","['data', 'models', 'learning', 'model', 'language', 'training', 'tasks', 'large', 'human', 'performance']","['knowledge', 'trained', 'tasks', 'ai', 'models', 'datasets', 'capabilities', 'agent', 'accuracy', 'speech']","[""  In meta reinforcement learning (meta RL), an agent learns from a set of\ntraining tasks how to quickly solve a new task, drawn from the same task\ndistribution. The optimal meta RL policy, a.k.a. the Bayes-optimal behavior, is\nwell defined, and guarantees optimal reward in expectation, taken with respect\nto the task distribution. The question we explore in this work is how many\ntraining tasks are required to guarantee approximately optimal behavior with\nhigh probability. Recent work provided the first such PAC analysis for a\nmodel-free setting, where a history-dependent policy was learned from the\ntraining tasks. In this work, we propose a different approach: directly learn\nthe task distribution, using density estimation techniques, and then train a\npolicy on the learned task distribution. We show that our approach leads to\nbounds that depend on the dimension of the task distribution. In particular, in\nsettings where the task distribution lies in a low-dimensional manifold, we\nextend our analysis to use dimensionality reduction techniques and account for\nsuch structure, obtaining significantly better bounds than previous work, which\nstrictly depend on the number of states and actions. The key of our approach is\nthe regularization implied by the kernel density estimation method. We further\ndemonstrate that this regularization is useful in practice, when `plugged in'\nthe state-of-the-art VariBAD meta RL algorithm.\n"", '  Large language models (LLMs) and vision-language models (VLMs) have\ndemonstrated remarkable performance across a wide range of tasks and domains.\nDespite this promise, spatial understanding and reasoning -- a fundamental\ncomponent of human cognition -- remains under-explored. We develop novel\nbenchmarks that cover diverse aspects of spatial reasoning such as relationship\nunderstanding, navigation, and counting. We conduct a comprehensive evaluation\nof competitive language and vision-language models. Our findings reveal several\ncounter-intuitive insights that have been overlooked in the literature: (1)\nSpatial reasoning poses significant challenges where competitive models can\nfall behind random guessing; (2) Despite additional visual input, VLMs often\nunder-perform compared to their LLM counterparts; (3) When both textual and\nvisual information is available, multi-modal language models become less\nreliant on visual information if sufficient textual clues are provided.\nAdditionally, we demonstrate that leveraging redundancy between vision and text\ncan significantly enhance model performance. We hope our study will inform the\ndevelopment of multimodal models to improve spatial intelligence and further\nclose the gap with human intelligence.\n', '  One highly promising direction for enabling flexible real-time on-device\nimage editing is utilizing data distillation by leveraging large-scale\ntext-to-image diffusion models to generate paired datasets used for training\ngenerative adversarial networks (GANs). This approach notably alleviates the\nstringent requirements typically imposed by high-end commercial GPUs for\nperforming image editing with diffusion models. However, unlike text-to-image\ndiffusion models, each distilled GAN is specialized for a specific image\nediting task, necessitating costly training efforts to obtain models for\nvarious concepts. In this work, we introduce and address a novel research\ndirection: can the process of distilling GANs from diffusion models be made\nsignificantly more efficient? To achieve this goal, we propose a series of\ninnovative techniques. First, we construct a base GAN model with generalized\nfeatures, adaptable to different concepts through fine-tuning, eliminating the\nneed for training from scratch. Second, we identify crucial layers within the\nbase GAN model and employ Low-Rank Adaptation (LoRA) with a simple yet\neffective rank search process, rather than fine-tuning the entire base model.\nThird, we investigate the minimal amount of data necessary for fine-tuning,\nfurther reducing the overall training time. Extensive experiments show that we\ncan efficiently empower GANs with the ability to perform real-time high-quality\nimage editing on mobile devices with remarkably reduced training and storage\ncosts for each concept.\n']",Artificial Intelligence and Machine Learning Models
1,0,1000,0_molecular_molecule_molecules_ligands,"['molecular', 'molecule', 'molecules', 'ligands', 'proteins', 'ligand', 'protein', 'models', 'modeling', 'discovery']","['molecular', 'protein', 'drug', 'molecules', 'chemical', 'molecule', 'materials', 'proteins', 'discovery', 'structures']","['molecular', 'ligands', 'modeling', 'discovery', 'amino', 'novo', 'drugs', 'synthesis', 'affinity', 'conformations']","['  Generating molecules that bind to specific proteins is an important but\nchallenging task in drug discovery. Previous works usually generate atoms in an\nauto-regressive way, where element types and 3D coordinates of atoms are\ngenerated one by one. However, in real-world molecular systems, the\ninteractions among atoms in an entire molecule are global, leading to the\nenergy function pair-coupled among atoms. With such energy-based consideration,\nthe modeling of probability should be based on joint distributions, rather than\nsequentially conditional ones. Thus, the unnatural sequentially auto-regressive\nmodeling of molecule generation is likely to violate the physical rules, thus\nresulting in poor properties of the generated molecules. In this work, a\ngenerative diffusion model for molecular 3D structures based on target proteins\nas contextual constraints is established, at a full-atom level in a\nnon-autoregressive way. Given a designated 3D protein binding site, our model\nlearns the generative process that denoises both element types and 3D\ncoordinates of an entire molecule, with an equivariant network. Experimentally,\nthe proposed method shows competitive performance compared with prevailing\nworks in terms of high affinity with proteins and appropriate molecule sizes as\nwell as other drug properties such as drug-likeness of the generated molecules.\n', ""  Diffusion models have emerged as powerful tools for molecular generation,\nparticularly in the context of 3D molecular structures. Inspired by\nnon-equilibrium statistical physics, these models can generate 3D molecular\nstructures with specific properties or requirements crucial to drug discovery.\nDiffusion models were particularly successful at learning 3D molecular\ngeometries' complex probability distributions and their corresponding chemical\nand physical properties through forward and reverse diffusion processes. This\nreview focuses on the technical implementation of diffusion models tailored for\n3D molecular generation. It compares the performance, evaluation methods, and\nimplementation details of various diffusion models used for molecular\ngeneration tasks. We cover strategies for atom and bond representation,\narchitectures of reverse diffusion denoising networks, and challenges\nassociated with generating stable 3D molecular structures. This review also\nexplores the applications of diffusion models in $\\textit{de novo}$ drug design\nand related areas of computational chemistry, such as structure-based drug\ndesign, including target-specific molecular generation, molecular docking, and\nmolecular dynamics of protein-ligand complexes. We also cover conditional\ngeneration on physical properties, conformation generation, and fragment-based\ndrug design. By summarizing the state-of-the-art diffusion models for 3D\nmolecular generation, this review sheds light on their role in advancing drug\ndiscovery as well as their current limitations.\n"", ""  Molecular representation learning is pivotal for various molecular property\nprediction tasks related to drug discovery. Robust and accurate benchmarks are\nessential for refining and validating current methods. Existing molecular\nproperty benchmarks derived from wet experiments, however, face limitations\nsuch as data volume constraints, unbalanced label distribution, and noisy\nlabels. To address these issues, we construct a large-scale and precise\nmolecular representation dataset of approximately 140,000 small molecules,\nmeticulously designed to capture an extensive array of chemical, physical, and\nbiological properties, derived through a robust computational ligand-target\nbinding analysis pipeline. We conduct extensive experiments on various deep\nlearning models, demonstrating that our dataset offers significant\nphysicochemical interpretability to guide model development and design.\nNotably, the dataset's properties are linked to binding affinity metrics,\nproviding additional insights into model performance in drug-target interaction\ntasks. We believe this dataset will serve as a more accurate and reliable\nbenchmark for molecular representation learning, thereby expediting progress in\nthe field of artificial intelligence-driven drug discovery.\n""]",Molecular Generation and Modeling for Drug Discovery
2,1,622,1_recommender_recommenders_personalized_recommendation,"['recommender', 'recommenders', 'personalized', 'recommendation', 'recommendations', 'factorization', 'collaborative', 'embeddings', 'attention', 'embedding']","['recommendation', 'recommender', 'item', 'user', 'items', 'recommendations', 'users', 'preferences', 'collaborative', 'sequential']","['recommenders', 'personalized', 'factorization', 'collaborative', 'embedding', 'conversational', 'rating', 'items', 'interests', 'popularity']","['  Contemporary recommender systems predominantly rely on collaborative\nfiltering techniques, employing ID-embedding to capture latent associations\namong users and items. However, this approach overlooks the wealth of semantic\ninformation embedded within textual descriptions of items, leading to\nsuboptimal performance in cold-start scenarios and long-tail user\nrecommendations. Leveraging the capabilities of Large Language Models (LLMs)\npretrained on massive text corpus presents a promising avenue for enhancing\nrecommender systems by integrating open-world domain knowledge. In this paper,\nwe propose an Llm-driven knowlEdge Adaptive RecommeNdation (LEARN) framework\nthat synergizes open-world knowledge with collaborative knowledge. We address\ncomputational complexity concerns by utilizing pretrained LLMs as item encoders\nand freezing LLM parameters to avoid catastrophic forgetting and preserve\nopen-world knowledge. To bridge the gap between the open-world and\ncollaborative domains, we design a twin-tower structure supervised by the\nrecommendation task and tailored for practical industrial application. Through\noffline experiments on the large-scale industrial dataset and online\nexperiments on A/B tests, we demonstrate the efficacy of our approach.\n', '  Many existing industrial recommender systems are sensitive to the patterns of\nuser-item engagement. Light users, who interact less frequently, correspond to\na data sparsity problem, making it difficult for the system to accurately learn\nand represent their preferences. On the other hand, heavy users with rich\ninteraction history often demonstrate a variety of niche interests that are\nhard to be precisely captured under the standard ""user-item"" similarity\nmeasurement. Moreover, implementing these systems in an industrial environment\nnecessitates that they are resource-efficient and scalable to process web-scale\ndata under strict latency constraints. In this paper, we address these\nchallenges by introducing an intermediate ""interest"" layer between users and\nitems. We propose a novel approach that efficiently constructs user interest\nand facilitates low computational cost inference by clustering engagement\ngraphs and incorporating user-interest attention. This method enhances the\nunderstanding of light users\' preferences by linking them with heavy users. By\nintegrating user-interest attention, our approach allows a more personalized\nsimilarity metric, adept at capturing the complex dynamics of user-item\ninteractions. The use of interest as an intermediary layer fosters a balance\nbetween scalability and expressiveness in the model. Evaluations on two public\ndatasets reveal that our method not only achieves improved recommendation\nperformance but also demonstrates enhanced computational efficiency compared to\nitem-level attention models. Our approach has also been deployed in multiple\nproducts at Meta, facilitating short-form video related recommendation.\n', '  The explainability of recommendation systems is crucial for enhancing user\ntrust and satisfaction. Leveraging large language models (LLMs) offers new\nopportunities for comprehensive recommendation logic generation. However, in\nexisting related studies, fine-tuning LLM models for recommendation tasks\nincurs high computational costs and alignment issues with existing systems,\nlimiting the application potential of proven proprietary/closed-source LLM\nmodels, such as GPT-4. In this work, our proposed effective strategy LANE\naligns LLMs with online recommendation systems without additional LLMs tuning,\nreducing costs and improving explainability. This innovative approach addresses\nkey challenges in integrating language models with recommendation systems while\nfully utilizing the capabilities of powerful proprietary models. Specifically,\nour strategy operates through several key components: semantic embedding, user\nmulti-preference extraction using zero-shot prompting, semantic alignment, and\nexplainable recommendation generation using Chain of Thought (CoT) prompting.\nBy embedding item titles instead of IDs and utilizing multi-head attention\nmechanisms, our approach aligns the semantic features of user preferences with\nthose of candidate items, ensuring coherent and user-aligned recommendations.\nSufficient experimental results including performance comparison, questionnaire\nvoting, and visualization cases prove that our method can not only ensure\nrecommendation performance, but also provide easy-to-understand and reasonable\nrecommendation logic.\n']",Personalized Recommendation Systems
3,2,602,2_nlp_text_annotated_clinical,"['nlp', 'text', 'annotated', 'clinical', 'medical', 'patients', 'medicine', 'hospital', 'evaluation', 'clinicians']","['medical', 'clinical', 'biomedical', 'patient', 'healthcare', 'notes', 'health', 'patients', 'records', 'extraction']","['nlp', 'text', 'annotated', 'hospital', 'retrieval', 'physicians', 'biomedical', 'reports', 'health', 'ehr']","['  In studies that rely on data from electronic health records (EHRs),\nunstructured text data such as clinical progress notes offer a rich source of\ninformation about patient characteristics and care that may be missing from\nstructured data. Despite the prevalence of text in clinical research, these\ndata are often ignored for the purposes of quantitative analysis due their\ncomplexity. This paper presents a unified framework for leveraging text data to\nsupport causal inference with electronic health data at multiple stages of\nanalysis. In particular, we consider how natural language processing and\nstatistical text analysis can be combined with standard inferential techniques\nto address common challenges due to missing data, confounding bias, and\ntreatment effect heterogeneity. Through an application to a recent EHR study\ninvestigating the effects of a non-randomized medical intervention on patient\noutcomes, we show how incorporating text data in a traditional matching\nanalysis can help strengthen the validity of an estimated treatment effect and\nidentify patient subgroups that may benefit most from treatment. We believe\nthese methods have the potential to expand the scope of secondary analysis of\nclinical data to domains where structured EHR data is limited, such as in\ndeveloping countries. To this end, we provide code and open-source replication\nmaterials to encourage adoption and broader exploration of these techniques in\nclinical research.\n', '  Analyzing vast textual data and summarizing key information from electronic\nhealth records imposes a substantial burden on how clinicians allocate their\ntime. Although large language models (LLMs) have shown promise in natural\nlanguage processing (NLP), their effectiveness on a diverse range of clinical\nsummarization tasks remains unproven. In this study, we apply adaptation\nmethods to eight LLMs, spanning four distinct clinical summarization tasks:\nradiology reports, patient questions, progress notes, and doctor-patient\ndialogue. Quantitative assessments with syntactic, semantic, and conceptual NLP\nmetrics reveal trade-offs between models and adaptation methods. A clinical\nreader study with ten physicians evaluates summary completeness, correctness,\nand conciseness; in a majority of cases, summaries from our best adapted LLMs\nare either equivalent (45%) or superior (36%) compared to summaries from\nmedical experts. The ensuing safety analysis highlights challenges faced by\nboth LLMs and medical experts, as we connect errors to potential medical harm\nand categorize types of fabricated information. Our research provides evidence\nof LLMs outperforming medical experts in clinical text summarization across\nmultiple tasks. This suggests that integrating LLMs into clinical workflows\ncould alleviate documentation burden, allowing clinicians to focus more on\npatient care.\n', '  Large language models (LLMs) have emerged as powerful tools with\ntransformative potential across numerous domains, including healthcare and\nmedicine. In the medical domain, LLMs hold promise for tasks ranging from\nclinical decision support to patient education. However, evaluating the\nperformance of LLMs in medical contexts presents unique challenges due to the\ncomplex and critical nature of medical information. This paper provides a\ncomprehensive overview of the landscape of medical LLM evaluation, synthesizing\ninsights from existing studies and highlighting evaluation data sources, task\nscenarios, and evaluation methods. Additionally, it identifies key challenges\nand opportunities in medical LLM evaluation, emphasizing the need for continued\nresearch and innovation to ensure the responsible integration of LLMs into\nclinical practice.\n']",Natural Language Processing in Clinical Text Analysis
4,3,583,3_retrieval_search_relevance_recall,"['retrieval', 'search', 'relevance', 'recall', 'retrieved', 'queries', 'semantic', 'ranking', 'corpus', 'indexing']","['retrieval', 'documents', 'query', 'document', 'ranking', 'relevance', 'queries', 'answering', 'question', 'search']","['retrieval', 'recall', 'semantic', 'retriever', 'answering', 'generative', 'benchmark', 'docid', 'rag', 'reranking']","['  Large Language Models (LLMs) excel in various language tasks but they often\ngenerate incorrect information, a phenomenon known as ""hallucinations"".\nRetrieval-Augmented Generation (RAG) aims to mitigate this by using document\nretrieval for accurate responses. However, RAG still faces hallucinations due\nto vague queries. This study aims to improve RAG by optimizing query generation\nwith a query-document alignment score, refining queries using LLMs for better\nprecision and efficiency of document retrieval. Experiments have shown that our\napproach improves document retrieval, resulting in an average accuracy gain of\n1.6%.\n', ""  Retrieval-Augmented Generation (RAG) has recently emerged as a method to\nextend beyond the pre-trained knowledge of Large Language Models by augmenting\nthe original prompt with relevant passages or documents retrieved by an\nInformation Retrieval (IR) system. RAG has become increasingly important for\nGenerative AI solutions, especially in enterprise settings or in any domain in\nwhich knowledge is constantly refreshed and cannot be memorized in the LLM. We\nargue here that the retrieval component of RAG systems, be it dense or sparse,\ndeserves increased attention from the research community, and accordingly, we\nconduct the first comprehensive and systematic examination of the retrieval\nstrategy of RAG systems. We focus, in particular, on the type of passages IR\nsystems within a RAG solution should retrieve. Our analysis considers multiple\nfactors, such as the relevance of the passages included in the prompt context,\ntheir position, and their number. One counter-intuitive finding of this work is\nthat the retriever's highest-scoring documents that are not directly relevant\nto the query (e.g., do not contain the answer) negatively impact the\neffectiveness of the LLM. Even more surprising, we discovered that adding\nrandom documents in the prompt improves the LLM accuracy by up to 35%. These\nresults highlight the need to investigate the appropriate strategies when\nintegrating retrieval with LLMs, thereby laying the groundwork for future\nresearch in this area.\n"", '  Retrieval-Augmented Generation (RAG) has recently demonstrated the\nperformance of Large Language Models (LLMs) in the knowledge-intensive tasks\nsuch as Question-Answering (QA). RAG expands the query context by incorporating\nexternal knowledge bases to enhance the response accuracy. However, it would be\ninefficient to access LLMs multiple times for each query and unreliable to\nretrieve all the relevant documents by a single query. We have found that even\nthough there is low relevance between some critical documents and query, it is\npossible to retrieve the remaining documents by combining parts of the\ndocuments with the query. To mine the relevance, a two-stage retrieval\nframework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is\nproposed to improve document retrieval recall and the accuracy of answers while\nmaintaining efficiency. Additionally, a compact classifier is applied to two\ndifferent selection strategies to determine the contribution of the retrieved\ndocuments to answering the query and retrieve the relatively relevant\ndocuments. Meanwhile, DR-RAG call the LLMs only once, which significantly\nimproves the efficiency of the experiment. The experimental results on\nmulti-hop QA datasets show that DR-RAG can significantly improve the accuracy\nof the answers and achieve new progress in QA systems.\n']",Improving Document Retrieval for Large Language Models
5,4,560,4_quantum_qubits_qubit_qcnns,"['quantum', 'qubits', 'qubit', 'qcnns', 'qcnn', 'qnns', 'grover', 'qiskit', 'qnn', 'entanglement']","['quantum', 'classical', 'circuit', 'qubit', 'qubits', 'computers', 'computing', 'states', 'gates', 'entanglement']","['quantum', 'qubits', 'qcnns', 'qiskit', 'qml', 'pqcs', 'hamiltonian', 'pauli', 'circuits', 'trainability']","['  Our primary objective is to conduct a brief survey of various classical and\nquantum neural net sequence models, which includes self-attention and recurrent\nneural networks, with a focus on recent quantum approaches proposed to work\nwith near-term quantum devices, while exploring some basic enhancements for\nthese quantum models. We re-implement a key representative set of these\nexisting methods, adapting an image classification approach using quantum\nself-attention to create a quantum hybrid transformer that works for text and\nimage classification, and applying quantum self-attention and quantum recurrent\nneural networks to natural language processing tasks. We also explore different\nencoding techniques and introduce positional encoding into quantum\nself-attention neural networks leading to improved accuracy and faster\nconvergence in text and image classification experiments. This paper also\nperforms a comparative analysis of classical self-attention models and their\nquantum counterparts, helping shed light on the differences in these models and\ntheir performance.\n', '  In this work, quantum transformers are designed and analysed in detail by\nextending the state-of-the-art classical transformer neural network\narchitectures known to be very performant in natural language processing and\nimage analysis. Building upon the previous work, which uses parametrised\nquantum circuits for data loading and orthogonal neural layers, we introduce\nthree types of quantum transformers for training and inference, including a\nquantum transformer based on compound matrices, which guarantees a theoretical\nadvantage of the quantum attention mechanism compared to their classical\ncounterpart both in terms of asymptotic run time and the number of model\nparameters. These quantum architectures can be built using shallow quantum\ncircuits and produce qualitatively different classification models. The three\nproposed quantum attention layers vary on the spectrum between closely\nfollowing the classical transformers and exhibiting more quantum\ncharacteristics. As building blocks of the quantum transformer, we propose a\nnovel method for loading a matrix as quantum states as well as two new\ntrainable quantum orthogonal layers adaptable to different levels of\nconnectivity and quality of quantum computers. We performed extensive\nsimulations of the quantum transformers on standard medical image datasets that\nshowed competitively, and at times better performance compared to the classical\nbenchmarks, including the best-in-class classical vision transformers. The\nquantum transformers we trained on these small-scale datasets require fewer\nparameters compared to standard classical benchmarks. Finally, we implemented\nour quantum transformers on superconducting quantum computers and obtained\nencouraging results for up to six qubit experiments.\n', '  In the processing of quantum computation, analyzing and learning the pattern\nof the quantum data are essential for many tasks. Quantum machine learning\nalgorithms can not only deal with the quantum states generated in the preceding\nquantum procedures, but also the quantum registers encoding classical problems.\nIn this work, we experimentally demonstrate the anomaly detection of quantum\nstates encoding audio samples with a three-qubit quantum processor consisting\nof solid-state spins in diamond. By training the quantum machine with a few\nnormal samples, the quantum machine can detect the anomaly samples with a\nminimum error rate of 15.4%. These results show the power of quantum anomaly\ndetection in dealing with machine learning tasks and the potential to detect\nabnormal output of quantum devices.\n']",Quantum Machine Learning and Neural Networks
6,5,539,5_robotic_grasping_robot_robotics,"['robotic', 'grasping', 'robot', 'robotics', 'robots', 'grasp', 'teleoperation', 'scenes', 'actions', 'pose']","['robot', 'manipulation', 'robotic', 'robots', 'objects', 'grasping', 'object', 'navigation', 'tactile', 'demonstrations']","['grasping', 'robotics', 'teleoperation', 'pose', 'videos', 'manipulator', '3d', 'tasks', 'imitation', 'motions']","['  We present a new reproducible benchmark for evaluating robot manipulation in\nthe real world, specifically focusing on pick-and-place. Our benchmark uses the\nYCB objects, a commonly used dataset in the robotics community, to ensure that\nour results are comparable to other studies. Additionally, the benchmark is\ndesigned to be easily reproducible in the real world, making it accessible to\nresearchers and practitioners. We also provide our experimental results and\nanalyzes for model-based and model-free 6D robotic grasping on the benchmark,\nwhere representative algorithms are evaluated for object perception, grasping\nplanning, and motion planning. We believe that our benchmark will be a valuable\ntool for advancing the field of robot manipulation. By providing a standardized\nevaluation framework, researchers can more easily compare different techniques\nand algorithms, leading to faster progress in developing robot manipulation\nmethods.\n', '  Robot learning of manipulation skills is hindered by the scarcity of diverse,\nunbiased datasets. While curated datasets can help, challenges remain in\ngeneralizability and real-world transfer. Meanwhile, large-scale ""in-the-wild""\nvideo datasets have driven progress in computer vision through self-supervised\ntechniques. Translating this to robotics, recent works have explored learning\nmanipulation skills by passively watching abundant videos sourced online.\nShowing promising results, such video-based learning paradigms provide scalable\nsupervision while reducing dataset bias. This survey reviews foundations such\nas video feature representation learning techniques, object affordance\nunderstanding, 3D hand/body modeling, and large-scale robot resources, as well\nas emerging techniques for acquiring robot manipulation skills from\nuncontrolled video demonstrations. We discuss how learning only from observing\nlarge-scale human videos can enhance generalization and sample efficiency for\nrobotic manipulation. The survey summarizes video-based learning approaches,\nanalyses their benefits over standard datasets, survey metrics, and benchmarks,\nand discusses open challenges and future directions in this nascent domain at\nthe intersection of computer vision, natural language processing, and robot\nlearning.\n', ""  In collaborative human-robot manipulation, a robot must predict human intents\nand adapt its actions accordingly to smoothly execute tasks. However, the\nhuman's intent in turn depends on actions the robot takes, creating a\nchicken-or-egg problem. Prior methods ignore such inter-dependency and instead\ntrain marginal intent prediction models independent of robot actions. This is\nbecause training conditional models is hard given a lack of paired human-robot\ninteraction datasets. Can we instead leverage large-scale human-human\ninteraction data that is more easily accessible? Our key insight is to exploit\na correspondence between human and robot actions that enables transfer learning\nfrom human-human to human-robot data. We propose a novel architecture,\nInteRACT, that pre-trains a conditional intent prediction model on large\nhuman-human datasets and fine-tunes on a small human-robot dataset. We evaluate\non a set of real-world collaborative human-robot manipulation tasks and show\nthat our conditional model improves over various marginal baselines. We also\nintroduce new techniques to tele-operate a 7-DoF robot arm and collect a\ndiverse range of human-robot collaborative manipulation data, which we\nopen-source.\n""]",Robot Manipulation and Grasping
7,6,513,6_reasoning_thinking_prompting_deductive,"['reasoning', 'thinking', 'prompting', 'deductive', 'inference', 'prompts', 'tasks', 'steps', 'task', 'logic']","['reasoning', 'math', 'thought', 'mathematical', 'logical', 'prompting', 'chain', 'abilities', 'step', 'solving']","['thinking', 'deductive', 'inference', 'prompts', 'steps', 'puzzles', 'skills', 'questions', 'lms', 'symbolic']","['  Chain-of-thought (CoT) prompting can guide language models to engage in\ncomplex multi-step reasoning. The quality of provided demonstrations\nsignificantly impacts the success of downstream inference tasks. While existing\nautomated methods prioritize accuracy and semantics in these demonstrations, we\nshow that the underlying reasoning patterns play a more crucial role in such\ntasks. In this paper, we propose Pattern-Aware CoT, a prompting method that\nconsiders the diversity of demonstration patterns. By incorporating patterns\nsuch as step length and reasoning process within intermediate steps, PA-CoT\neffectively mitigates the issue of bias induced by demonstrations and enables\nbetter generalization to diverse scenarios. We conduct experiments on nine\nreasoning benchmark tasks using two open-source LLMs. The results show that our\nmethod substantially enhances reasoning performance and exhibits robustness to\nerrors. The code will be made publicly available.\n', ""  Chain of Thought (CoT) is significant in improving the reasoning abilities of\nlarge language models (LLMs). However, the correlation between the\neffectiveness of CoT and the length of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we have conducted several empirical\nexperiments to explore the relations. Specifically, we design experiments that\nexpand and compress the rationale reasoning steps within CoT demonstrations\nwhile keeping all other factors constant. We have the following key findings.\nFirst, the results indicate that lengthening the reasoning steps in prompts,\neven without adding new information into the prompt, considerably enhances\nLLMs' reasoning abilities across multiple datasets. Alternatively, shortening\nthe reasoning steps, even while preserving the key information, significantly\ndiminishes the reasoning abilities of models. This finding highlights the\nimportance of the number of steps in CoT prompts and provides practical\nguidance to make better use of LLMs' potential in complex problem-solving\nscenarios. Second, we also investigated the relationship between the\nperformance of CoT and the rationales used in demonstrations. Surprisingly, the\nresult shows that even incorrect rationales can yield favorable outcomes if\nthey maintain the requisite length of inference. Third, we observed that the\nadvantages of increasing reasoning steps are task-dependent: simpler tasks\nrequire fewer steps, whereas complex tasks gain significantly from longer\ninference sequences. The code is available at\nhttps://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models\n"", ""  Large Language Models (LLMs) have showcased impressive reasoning\ncapabilities, particularly when guided by specifically designed prompts in\ncomplex reasoning tasks such as math word problems. These models typically\nsolve tasks using a chain-of-thought approach, which not only bolsters their\nreasoning abilities but also provides valuable insights into their\nproblem-solving process. However, there is still significant room for enhancing\nthe reasoning abilities of LLMs. Some studies suggest that the integration of\nan LLM output verifier can boost reasoning accuracy without necessitating\nadditional model training. In this paper, we follow these studies and introduce\na novel graph-based method to further augment the reasoning capabilities of\nLLMs. We posit that multiple solutions to a reasoning task, generated by an\nLLM, can be represented as a reasoning graph due to the logical connections\nbetween intermediate steps from different reasoning paths. Therefore, we\npropose the Reasoning Graph Verifier (GraphReason) to analyze and verify the\nsolutions generated by LLMs. By evaluating these graphs, models can yield more\naccurate and reliable results.Our experimental results show that our\ngraph-based verification method not only significantly enhances the reasoning\nabilities of LLMs but also outperforms existing verifier methods in terms of\nimproving these models' reasoning performance.\n""]",Reasoning and Prompting in Language Models
8,7,482,7_cnn_cnns_segmentation_segmentations,"['cnn', 'cnns', 'segmentation', 'segmentations', 'segmenting', 'supervised', 'mri', 'imaging', 'deep', 'classification']","['segmentation', 'cancer', 'tumor', 'medical', 'imaging', 'breast', 'images', 'tumors', 'diagnosis', 'tissue']","['cnns', 'mri', 'deep', 'mammography', 'dataset', 'tumour', 'convolutional', 'scans', 'segment', 'organ']","['  Brain tumors remain a critical global health challenge, necessitating\nadvancements in diagnostic techniques and treatment methodologies. A tumor or\nits recurrence often needs to be identified in imaging studies and\ndifferentiated from normal brain tissue. In response to the growing need for\nage-specific segmentation models, particularly for pediatric patients, this\nstudy explores the deployment of deep learning techniques using magnetic\nresonance imaging (MRI) modalities. By introducing a novel ensemble approach\nusing ONet and modified versions of UNet, coupled with innovative loss\nfunctions, this study achieves a precise segmentation model for the BraTS-PEDs\n2023 Challenge. Data augmentation, including both single and composite\ntransformations, ensures model robustness and accuracy across different\nscanning protocols. The ensemble strategy, integrating the ONet and UNet\nmodels, shows greater effectiveness in capturing specific features and modeling\ndiverse aspects of the MRI images which result in lesion wise Dice scores of\n0.52, 0.72 and 0.78 on unseen validation data and scores of 0.55, 0.70, 0.79 on\nfinal testing data for the ""enhancing tumor"", ""tumor core"" and ""whole tumor""\nlabels respectively. Visual comparisons further confirm the superiority of the\nensemble method in accurate tumor region coverage. The results indicate that\nthis advanced ensemble approach, building upon the unique strengths of\nindividual models, offers promising prospects for enhanced diagnostic accuracy\nand effective treatment planning and monitoring for brain tumors in pediatric\nbrains.\n', '  Brain tumor is a life-threatening problem and hampers the normal functioning\nof the human body. The average five-year relative survival rate for malignant\nbrain tumors is 35.6 percent. For proper diagnosis and efficient treatment\nplanning, it is necessary to detect the brain tumor in early stages. Due to\nadvancement in medical imaging technology, the brain images are taken in\ndifferent modalities. The ability to extract relevant characteristics from\nmagnetic resonance imaging (MRI) scans is a crucial step for brain tumor\nclassifiers. Several studies have proposed various strategies to extract\nrelevant features from different modalities of MRI to predict the growth of\nabnormal tumors. Most techniques used conventional methods of image processing\nfor feature extraction and machine learning for classification. More recently,\nthe use of deep learning algorithms in medical imaging has resulted in\nsignificant improvements in the classification and diagnosis of brain tumors.\nSince tumors are located at different regions of the brain, localizing the\ntumor and classifying it to a particular category is a challenging task. The\nobjective of this project is to develop a predictive system for brain tumor\ndetection using machine learning(ensembling).\n', '  Medical image segmentation of anatomical structures and pathology is crucial\nin modern clinical diagnosis, disease study, and treatment planning. To date,\ngreat progress has been made in deep learning-based segmentation techniques,\nbut most methods still lack data efficiency, generalizability, and\ninteractability. Consequently, the development of new, precise segmentation\nmethods that demand fewer labeled datasets is of utmost importance in medical\nimage analysis. Recently, the emergence of foundation models, such as CLIP and\nSegment-Anything-Model (SAM), with comprehensive cross-domain representation\nopened the door for interactive and universal image segmentation. However,\nexploration of these models for data-efficient medical image segmentation is\nstill limited, but is highly necessary. In this paper, we propose a novel\nframework, called MedCLIP-SAM that combines CLIP and SAM models to generate\nsegmentation of clinical scans using text prompts in both zero-shot and weakly\nsupervised settings. To achieve this, we employed a new Decoupled Hard Negative\nNoise Contrastive Estimation (DHN-NCE) loss to fine-tune the BiomedCLIP model\nand the recent gScoreCAM to generate prompts to obtain segmentation masks from\nSAM in a zero-shot setting. Additionally, we explored the use of zero-shot\nsegmentation labels in a weakly supervised paradigm to improve the segmentation\nquality further. By extensively testing three diverse segmentation tasks and\nmedical image modalities (breast tumor ultrasound, brain tumor MRI, and lung\nX-ray), our proposed framework has demonstrated excellent accuracy. Code is\navailable at https://github.com/HealthX-Lab/MedCLIP-SAM.\n']",Brain Tumor Segmentation using Deep Learning
9,8,437,8_causal_causality_causally_inference,"['causal', 'causality', 'causally', 'inference', 'confounders', 'observational', 'discovery', 'identifiability', 'identifiable', 'unobserved']","['causal', 'variables', 'treatment', 'observational', 'causality', 'discovery', 'effects', 'effect', 'identifiability', 'interventions']","['causal', 'confounders', 'discovery', 'identifiability', 'unobserved', 'counterfactual', 'estimating', 'covariates', 'interventions', 'propensity']","['  Dynamic structural causal models (SCMs) are a powerful framework for\nreasoning in dynamic systems about direct effects which measure how a change in\none variable affects another variable while holding all other variables\nconstant. The causal relations in a dynamic structural causal model can be\nqualitatively represented with an acyclic full-time causal graph. Assuming\nlinearity and no hidden confounding and given the full-time causal graph, the\ndirect causal effect is always identifiable. However, in many application such\na graph is not available for various reasons but nevertheless experts have\naccess to the summary causal graph of the full-time causal graph which\nrepresents causal relations between time series while omitting temporal\ninformation and allowing cycles. This paper presents a complete identifiability\nresult which characterizes all cases for which the direct effect is graphically\nidentifiable from a summary causal graph and gives two sound finite adjustment\nsets that can be used to estimate the direct effect whenever it is\nidentifiable.\n', '  The ability to understand causality from data is one of the major milestones\nof human-level intelligence. Causal Discovery (CD) algorithms can identify the\ncause-effect relationships among the variables of a system from related\nobservational data with certain assumptions. Over the years, several methods\nhave been developed primarily based on the statistical properties of data to\nuncover the underlying causal mechanism. In this study, we present an extensive\ndiscussion on the methods designed to perform causal discovery from both\nindependent and identically distributed (I.I.D.) data and time series data. For\nthis purpose, we first introduce the common terminologies used in causal\ndiscovery literature and then provide a comprehensive discussion of the\nalgorithms designed to identify causal relations in different settings. We\nfurther discuss some of the benchmark datasets available for evaluating the\nalgorithmic performance, off-the-shelf tools or software packages to perform\ncausal discovery readily, and the common metrics used to evaluate these\nmethods. We also evaluate some widely used causal discovery algorithms on\nmultiple benchmark datasets and compare their performances. Finally, we\nconclude by discussing the research challenges and the applications of causal\ndiscovery algorithms in multiple areas of interest.\n', ""  Causal discovery from observational data holds great promise, but existing\nmethods rely on strong assumptions about the underlying causal structure, often\nrequiring full observability of all relevant variables. We tackle these\nchallenges by leveraging the score function $\\nabla \\log p(X)$ of observed\nvariables for causal discovery and propose the following contributions. First,\nwe generalize the existing results of identifiability with the score to\nadditive noise models with minimal requirements on the causal mechanisms.\nSecond, we establish conditions for inferring causal relations from the score\neven in the presence of hidden variables; this result is two-faced: we\ndemonstrate the score's potential as an alternative to conditional independence\ntests to infer the equivalence class of causal graphs with hidden variables,\nand we provide the necessary conditions for identifying direct causes in latent\nvariable models. Building on these insights, we propose a flexible algorithm\nfor causal discovery across linear, nonlinear, and latent variable models,\nwhich we empirically validate.\n""]",Causal Inference and Discovery
10,9,401,9_sgd_optimizers_optimizer_gradient,"['sgd', 'optimizers', 'optimizer', 'gradient', 'gradients', 'stochastic', 'optimization', 'adaptive', 'hessian', 'minimization']","['gradient', 'convergence', 'stochastic', 'convex', 'descent', 'nonconvex', 'optimization', 'momentum', 'rate', 'order']","['sgd', 'optimizers', 'gradients', 'stochastic', 'subgradient', 'minimax', 'adam', 'convexity', 'nonconvex', 'iteration']","['  While stochastic gradient descent (SGD) can use various learning rates, such\nas constant or diminishing rates, the previous numerical results showed that\nSGD performs better than other deep learning optimizers using when it uses\nlearning rates given by line search methods. In this paper, we perform a\nconvergence analysis on SGD with a learning rate given by an Armijo line search\nfor nonconvex optimization indicating that the upper bound of the expectation\nof the squared norm of the full gradient becomes small when the number of steps\nand the batch size are large. Next, we show that, for SGD with the\nArmijo-line-search learning rate, the number of steps needed for nonconvex\noptimization is a monotone decreasing convex function of the batch size; that\nis, the number of steps needed for nonconvex optimization decreases as the\nbatch size increases. Furthermore, we show that the stochastic first-order\noracle (SFO) complexity, which is the stochastic gradient computation cost, is\na convex function of the batch size; that is, there exists a critical batch\nsize that minimizes the SFO complexity. Finally, we provide numerical results\nthat support our theoretical results. The numerical results indicate that the\nnumber of steps needed for training deep neural networks decreases as the batch\nsize increases and that there exist the critical batch sizes that can be\nestimated from the theoretical results.\n', '  It is known that the standard stochastic gradient descent (SGD) optimization\nmethod, as well as accelerated and adaptive SGD optimization methods such as\nthe Adam optimizer fail to converge if the learning rates do not converge to\nzero (as, for example, in the situation of constant learning rates). Numerical\nsimulations often use human-tuned deterministic learning rate schedules or\nsmall constant learning rates. The default learning rate schedules for SGD\noptimization methods in machine learning implementation frameworks such as\nTensorFlow and Pytorch are constant learning rates. In this work we propose and\nstudy a learning-rate-adaptive approach for SGD optimization methods in which\nthe learning rate is adjusted based on empirical estimates for the values of\nthe objective function of the considered optimization problem (the function\nthat one intends to minimize). In particular, we propose a\nlearning-rate-adaptive variant of the Adam optimizer and implement it in case\nof several neural network learning problems, particularly, in the context of\ndeep learning approximation methods for partial differential equations such as\ndeep Kolmogorov methods, physics-informed neural networks, and deep Ritz\nmethods. In each of the presented learning problems the proposed\nlearning-rate-adaptive variant of the Adam optimizer faster reduces the value\nof the objective function than the Adam optimizer with the default learning\nrate. For a simple class of quadratic minimization problems we also rigorously\nprove that a learning-rate-adaptive variant of the SGD optimization method\nconverges to the minimizer of the considered minimization problem. Our\nconvergence proof is based on an analysis of the laws of invariant measures of\nthe SGD method as well as on a more general convergence analysis for SGD with\nrandom but predictable learning rates which we develop in this work.\n', '  For nonconvex objective functions, including deep neural networks, stochastic\ngradient descent (SGD) with momentum has fast convergence and excellent\ngeneralizability, but a theoretical explanation for this is lacking. In\ncontrast to previous studies that defined the stochastic noise that occurs\nduring optimization as the variance of the stochastic gradient, we define it as\nthe gap between the search direction of the optimizer and the steepest descent\ndirection and show that its level dominates generalizability of the model. We\nalso show that the stochastic noise in SGD with momentum smoothes the objective\nfunction, the degree of which is determined by the learning rate, the batch\nsize, the momentum factor, the variance of the stochastic gradient, and the\nupper bound of the gradient norm. By numerically deriving the stochastic noise\nlevel in SGD and SGD with momentum, we provide theoretical findings that help\nexplain the training dynamics of SGD with momentum, which were not explained by\nprevious studies on convergence and stability. We also provide experimental\nresults supporting our assertion that model generalizability depends on the\nstochastic noise level.\n']",Stochastic Gradient Descent Optimization
11,10,401,10_networks_neural_neurons_regularization,"['networks', 'neural', 'neurons', 'regularization', 'approximation', 'gradient', 'relu', 'layers', 'network', 'layer']","['relu', 'activation', 'networks', 'neural', 'functions', 'layer', 'network', 'function', 'descent', 'linear']","['neurons', 'regularization', 'gradient', 'relu', 'layers', 'dnns', 'shallow', 'activation', 'sgd', 'dimensional']","['  We prove a large deviation principle for deep neural networks with Gaussian\nweights and (at most linearly growing) activation functions. This generalises\nearlier work, in which bounded and continuous activation functions were\nconsidered. In practice, linearly growing activation functions such as ReLU are\nmost commonly used. We furthermore simplify previous expressions for the rate\nfunction and a give power-series expansions for the ReLU case.\n', '  We study the approximation capacity of some variation spaces corresponding to\nshallow ReLU$^k$ neural networks. It is shown that sufficiently smooth\nfunctions are contained in these spaces with finite variation norms. For\nfunctions with less smoothness, the approximation rates in terms of the\nvariation norm are established. Using these results, we are able to prove the\noptimal approximation rates in terms of the number of neurons for shallow\nReLU$^k$ neural networks. It is also shown how these results can be used to\nderive approximation bounds for deep neural networks and convolutional neural\nnetworks (CNNs). As applications, we study convergence rates for nonparametric\nregression using three ReLU neural network models: shallow neural network,\nover-parameterized neural network, and CNN. In particular, we show that shallow\nneural networks can achieve the minimax optimal rates for learning H\\""older\nfunctions, which complements recent results for deep neural networks. It is\nalso proven that over-parameterized (deep or shallow) neural networks can\nachieve nearly optimal rates for nonparametric regression.\n', '  We investigate the expressivity and learning dynamics of bias-free ReLU\nnetworks. We firstly show that two-layer bias-free ReLU networks have limited\nexpressivity: the only odd function two-layer bias-free ReLU networks can\nexpress is a linear one. We then show that, under symmetry conditions on the\ndata, these networks have the same learning dynamics as linear networks. This\nallows us to give closed-form time-course solutions to certain two-layer\nbias-free ReLU networks, which has not been done for nonlinear networks outside\nthe lazy learning regime. While deep bias-free ReLU networks are more\nexpressive than their two-layer counterparts, they still share a number of\nsimilarities with deep linear networks. These similarities enable us to\nleverage insights from linear networks, leading to a novel understanding of\nbias-free ReLU networks. Overall, our results show that some properties\nestablished for bias-free ReLU networks arise due to equivalence to linear\nnetworks, and suggest that including bias or considering asymmetric data are\navenues to engage with nonlinear behaviors.\n']",Neural Network Theory and Approximation Properties
12,11,397,11_forecasting_forecasts_forecast_lstm,"['forecasting', 'forecasts', 'forecast', 'lstm', 'prediction', 'predictive', 'future', 'datasets', 'models', 'seasonal']","['series', 'forecasting', 'time', 'multivariate', 'temporal', 'dependencies', 'term', 'long', 'forecast', 'attention']","['forecasts', 'lstm', 'datasets', 'seasonal', 'sequences', 'tasks', 'representations', 'trend', 'transformer', 'wavelet']","['  Time series forecasting is an important task in many fields ranging from\nsupply chain management to weather forecasting. Recently, Transformer neural\nnetwork architectures have shown promising results in forecasting on common\ntime series benchmark datasets. However, application to supply chain demand\nforecasting, which can have challenging characteristics such as sparsity and\ncross-series effects, has been limited.\n  In this work, we explore the application of Transformer-based models to\nsupply chain demand forecasting. In particular, we develop a new\nTransformer-based forecasting approach using a shared, multi-task per-time\nseries network with an initial component applying attention across time series,\nto capture interactions and help address sparsity. We provide a case study\napplying our approach to successfully improve demand prediction for a medical\ndevice manufacturing company. To further validate our approach, we also apply\nit to public demand forecasting datasets as well and demonstrate competitive to\nsuperior performance compared to a variety of baseline and state-of-the-art\nforecast methods across the private and public datasets.\n', '  The rapid development of time series forecasting research has brought many\ndeep learning-based modules in this field. However, despite the increasing\namount of new forecasting architectures, it is still unclear if we have\nleveraged the full potential of these existing modules within a properly\ndesigned architecture. In this work, we propose a novel hierarchical neural\narchitecture search approach for time series forecasting tasks. With the design\nof a hierarchical search space, we incorporate many architecture types designed\nfor forecasting tasks and allow for the efficient combination of different\nforecasting architecture modules. Results on long-term-time-series-forecasting\ntasks show that our approach can search for lightweight high-performing\nforecasting architectures across different forecasting tasks.\n', '  Large language models (LLMs) are being applied to time series tasks,\nparticularly time series forecasting. However, are language models actually\nuseful for time series? After a series of ablation studies on three recent and\npopular LLM-based time series forecasting methods, we find that removing the\nLLM component or replacing it with a basic attention layer does not degrade the\nforecasting results -- in most cases the results even improved. We also find\nthat despite their significant computational cost, pretrained LLMs do no better\nthan models trained from scratch, do not represent the sequential dependencies\nin time series, and do not assist in few-shot settings. Additionally, we\nexplore time series encoders and reveal that patching and attention structures\nperform similarly to state-of-the-art LLM-based forecasters.\n']",Time Series Forecasting with Deep Learning
13,12,364,12_editing_attention_images_videos,"['editing', 'attention', 'images', 'videos', 'generative', 'visual', 'scene', 'image', 'inpainting', 'stylization']","['diffusion', 'image', 'video', 'editing', 'text', 'images', 'generation', 'style', 'prompts', 'videos']","['editing', 'attention', 'generative', 'visual', 'inpainting', 'stylization', 'frames', 'sketches', 'grained', 'textual']","['  With recent advances in image and video diffusion models for content\ncreation, a plethora of techniques have been proposed for customizing their\ngenerated content. In particular, manipulating the cross-attention layers of\nText-to-Image (T2I) diffusion models has shown great promise in controlling the\nshape and location of objects in the scene. Transferring image-editing\ntechniques to the video domain, however, is extremely challenging as object\nmotion and temporal consistency are difficult to capture accurately. In this\nwork, we take a first look at the role of cross-attention in Text-to-Video\n(T2V) diffusion models for zero-shot video editing. While one-shot models have\nshown potential in controlling motion and camera movement, we demonstrate\nzero-shot control over object shape, position and movement in T2V models. We\nshow that despite the limitations of current T2V models, cross-attention\nguidance can be a promising approach for editing videos.\n', '  While text-to-image models have achieved impressive capabilities in image\ngeneration and editing, their application across various modalities often\nnecessitates training separate models. Inspired by existing method of single\nimage editing with self attention injection and video editing with shared\nattention, we propose a novel unified editing framework that combines the\nstrengths of both approaches by utilizing only a basic 2D image text-to-image\n(T2I) diffusion model. Specifically, we design a sampling method that\nfacilitates editing consecutive images while maintaining semantic consistency\nutilizing shared self-attention features during both reference and consecutive\nimage sampling processes. Experimental results confirm that our method enables\nediting across diverse modalities including 3D scenes, videos, and panorama\nimages.\n', '  Diffusion models have made tremendous progress in text-driven image and video\ngeneration. Now text-to-image foundation models are widely applied to various\ndownstream image synthesis tasks, such as controllable image generation and\nimage editing, while downstream video synthesis tasks are less explored for\nseveral reasons. First, it requires huge memory and computation overhead to\ntrain a video generation foundation model. Even with video foundation models,\nadditional costly training is still required for downstream video synthesis\ntasks. Second, although some works extend image diffusion models into videos in\na training-free manner, temporal consistency cannot be well preserved. Finally,\nthese adaption methods are specifically designed for one task and fail to\ngeneralize to different tasks. To mitigate these issues, we propose a\ntraining-free general-purpose video synthesis framework, coined as {\\bf\nBIVDiff}, via bridging specific image diffusion models and general\ntext-to-video foundation diffusion models. Specifically, we first use a\nspecific image diffusion model (e.g., ControlNet and Instruct Pix2Pix) for\nframe-wise video generation, then perform Mixed Inversion on the generated\nvideo, and finally input the inverted latents into the video diffusion models\n(e.g., VidRD and ZeroScope) for temporal smoothing. This decoupled framework\nenables flexible image model selection for different purposes with strong task\ngeneralization and high efficiency. To validate the effectiveness and general\nuse of BIVDiff, we perform a wide range of video synthesis tasks, including\ncontrollable video generation, video editing, video inpainting, and\noutpainting.\n']",Video and Image Editing with Generative Models
14,13,362,13_multimodal_captioning_visual_mllm,"['multimodal', 'captioning', 'visual', 'mllm', 'modality', 'text', 'mllms', 'textual', 'language', 'comprehension']","['visual', 'multimodal', 'reasoning', 'vision', 'image', 'question', 'answering', 'modal', 'images', 'instruction']","['multimodal', 'captioning', 'mllm', 'modality', 'text', 'comprehension', 'vision', 'skills', 'prompting', 'benchmark']","[""  Multimodal Large Language Models (MLLMs) have recently achieved promising\nzero-shot accuracy on visual question answering (VQA) -- a fundamental task\naffecting various downstream applications and domains. Given the great\npotential for the broad use of these models, it is important to investigate\ntheir limitations in dealing with different image and question properties. In\nthis work, we investigate whether MLLMs can perceive small details as well as\nlarge details in images. In particular, we show that their zero-shot accuracy\nin answering visual questions is very sensitive to the size of the visual\nsubject of the question, declining up to 46% with size. Furthermore, we show\nthat this effect is causal by observing that human visual cropping can\nsignificantly mitigate their sensitivity to size. Inspired by the usefulness of\nhuman cropping, we then propose five automatic visual cropping methods --\nleveraging either external localization models or the decision process of the\ngiven MLLM itself -- as inference time mechanisms to improve the zero-shot\nperformance of MLLMs. We study their effectiveness on four popular VQA\ndatasets, and a subset of the VQAv2 dataset tailored towards fine visual\ndetails. Our findings suggest that MLLMs should be used with caution in\ndetail-sensitive VQA applications, and that visual cropping is a promising\ndirection to improve their zero-shot performance. To facilitate further\ninvestigation of MLLMs' behaviors, our code and data are publicly released.\n"", '  Recent advancements in Chain-of-Thought (CoT) and related rationale-based\nworks have significantly improved the performance of Large Language Models\n(LLMs) in complex reasoning tasks. With the evolution of Multimodal Large\nLanguage Models (MLLMs), enhancing their capability to tackle complex\nmultimodal reasoning problems is a crucial frontier. However, incorporating\nmultimodal rationales in CoT has yet to be thoroughly investigated. We propose\nthe Image-of-Thought (IoT) prompting method, which helps MLLMs to extract\nvisual rationales step-by-step. Specifically, IoT prompting can automatically\ndesign critical visual information extraction operations based on the input\nimages and questions. Each step of visual information refinement identifies\nspecific visual rationales that support answers to complex visual reasoning\nquestions. Beyond the textual CoT, IoT simultaneously utilizes visual and\ntextual rationales to help MLLMs understand complex multimodal information. IoT\nprompting has improved zero-shot visual reasoning performance across various\nvisual understanding tasks in different MLLMs. Moreover, the step-by-step\nvisual feature explanations generated by IoT prompting elucidate the visual\nreasoning process, aiding in analyzing the cognitive processes of large\nmultimodal models\n', '  Connecting text and visual modalities plays an essential role in generative\nintelligence. For this reason, inspired by the success of large language\nmodels, significant research efforts are being devoted to the development of\nMultimodal Large Language Models (MLLMs). These models can seamlessly integrate\nvisual and textual modalities, while providing a dialogue-based interface and\ninstruction-following capabilities. In this paper, we provide a comprehensive\nreview of recent visual-based MLLMs, analyzing their architectural choices,\nmultimodal alignment strategies, and training techniques. We also conduct a\ndetailed analysis of these models across a wide range of tasks, including\nvisual grounding, image generation and editing, visual understanding, and\ndomain-specific applications. Additionally, we compile and describe training\ndatasets and evaluation benchmarks, conducting comparisons among existing\nmodels in terms of performance and computational requirements. Overall, this\nsurvey offers a comprehensive overview of the current state of the art, laying\nthe groundwork for future MLLMs.\n']",Multimodal Large Language Models for Visual Understanding
15,14,347,14_highway_driving_autonomous_traffic,"['highway', 'driving', 'autonomous', 'traffic', 'vehicles', 'lane', 'road', 'vehicle', 'planning', 'cars']","['driving', 'autonomous', 'traffic', 'vehicles', 'vehicle', 'lane', 'safety', 'road', 'scenarios', 'car']","['traffic', 'vehicles', 'lane', 'planning', 'safety', 'prediction', 'simulators', 'trajectory', 'pedestrian', 'collisions']","[""  In recent years, the expansion of internet technology and advancements in\nautomation have brought significant attention to autonomous driving technology.\nMajor automobile manufacturers, including Volvo, Mercedes-Benz, and Tesla, have\nprogressively introduced products ranging from assisted-driving vehicles to\nsemi-autonomous vehicles. However, this period has also witnessed several\ntraffic safety incidents involving self-driving vehicles. For instance, in\nMarch 2016, a Google self-driving car was involved in a minor collision with a\nbus. At the time of the accident, the autonomous vehicle was attempting to\nmerge into the right lane but failed to dynamically respond to the real-time\nenvironmental information during the lane change. It incorrectly assumed that\nthe approaching bus would slow down to avoid it, leading to a low-speed\ncollision with the bus. This incident highlights the current technological\nshortcomings and safety concerns associated with autonomous lane-changing\nbehavior, despite the rapid advancements in autonomous driving technology.\nLane-changing is among the most common and hazardous behaviors in highway\ndriving, significantly impacting traffic safety and flow. Therefore,\nlane-changing is crucial for traffic safety, and accurately predicting drivers'\nlane change intentions can markedly enhance driving safety. This paper\nintroduces a deep learning-based prediction method for autonomous driving lane\nchange behavior, aiming to facilitate safe lane changes and thereby improve\nroad safety.\n"", '  Autonomous driving technology can improve traffic safety and reduce traffic\naccidents. In addition, it improves traffic flow, reduces congestion, saves\nenergy and increases travel efficiency. In the relatively mature automatic\ndriving technology, the automatic driving function is divided into several\nmodules: perception, decision-making, planning and control, and a reasonable\ndivision of labor can improve the stability of the system. Therefore,\nautonomous vehicles need to have the ability to predict the trajectory of\nsurrounding vehicles in order to make reasonable decision planning and safety\nmeasures to improve driving safety. By using deep learning method, a\nsafety-sensitive deep learning model based on short term memory (LSTM) network\nis proposed. This model can alleviate the shortcomings of current automatic\ndriving trajectory planning, and the output trajectory not only ensures high\naccuracy but also improves safety. The cell state simulation algorithm\nsimulates the trackability of the trajectory generated by this model. The\nresearch results show that compared with the traditional model-based method,\nthe trajectory prediction method based on LSTM network has obvious advantages\nin predicting the trajectory in the long time domain. The intention recognition\nmodule considering interactive information has higher prediction and accuracy,\nand the algorithm results show that the trajectory is very smooth based on the\npremise of safe prediction and efficient lane change. And autonomous vehicles\ncan efficiently and safely complete lane changes.\n', '  Autonomous vehicles (AVs) have the potential to prevent accidents caused by\ndrivers errors and reduce road traffic risks. Due to the nature of heavy\nvehicles, whose collisions cause more serious crashes, the weights of vehicles\nneed to be considered when making driving strategies aimed at reducing the\npotential risks and their consequences in the context of autonomous driving.\nThis study develops an autonomous driving strategy based on risk anticipation,\nconsidering the weights of surrounding vehicles and using hierarchical deep\nreinforcement learning. A risk indicator integrating surrounding vehicles\nweights, based on the risk field theory, is proposed and incorporated into\nautonomous driving decisions. A hybrid action space is designed to allow for\nleft lane changes, right lane changes and car-following, which enables AVs to\nact more freely and realistically whenever possible. To solve the above hybrid\ndecision-making problem, a hierarchical proximal policy optimization (HPPO)\nalgorithm with an attention mechanism (AT-HPPO) is developed, providing great\nadvantages in maintaining stable performance with high robustness and\ngeneralization. An indicator, potential collision energy in conflicts (PCEC),\nis newly proposed to evaluate the performance of the developed AV driving\nstrategy from the perspective of the consequences of potential accidents. The\nperformance evaluation results in simulation and dataset demonstrate that our\nmodel provides driving strategies that reduce both the likelihood and\nconsequences of potential accidents, at the same time maintaining driving\nefficiency. The developed method is especially meaningful for AVs driving on\nhighways, where heavy vehicles make up a high proportion of the traffic.\n']",Autonomous Highway Driving Safety
16,15,326,15_adversarial_adversarially_adversary_imagenet,"['adversarial', 'adversarially', 'adversary', 'imagenet', 'robustness', 'trained', 'robust', 'overfitting', 'defenses', 'attacks']","['adversarial', 'robustness', 'attacks', 'perturbations', 'attack', 'robust', 'examples', 'defense', 'perturbation', 'clean']","['adversarial', 'adversary', 'imagenet', 'robustness', 'overfitting', 'defenses', 'dnns', 'training', 'ensemble', 'accuracy']","['  Deep neural networks have been successfully applied in various machine\nlearning tasks. However, studies show that neural networks are susceptible to\nadversarial attacks. This exposes a potential threat to neural network-based\nintelligent systems. We observe that the probability of the correct result\noutputted by the neural network increases by applying small first-order\nperturbations generated for non-predicted class labels to adversarial examples.\nBased on this observation, we propose a method for counteracting adversarial\nperturbations to improve adversarial robustness. In the proposed method, we\nrandomly select a number of class labels and generate small first-order\nperturbations for these selected labels. The generated perturbations are added\ntogether and then clamped onto a specified space. The obtained perturbation is\nfinally added to the adversarial example to counteract the adversarial\nperturbation contained in the example. The proposed method is applied at\ninference time and does not require retraining or finetuning the model. We\nexperimentally validate the proposed method on CIFAR-10 and CIFAR-100. The\nresults demonstrate that our method effectively improves the defense\nperformance of several transformation-based defense methods, especially against\nstrong adversarial examples generated using more iterations.\n', ""  Deep neural networks (DNNs) are easily fooled by adversarial perturbations\nthat are imperceptible to humans. Adversarial training, a process where\nadversarial examples are added to the training set, is the current\nstate-of-the-art defense against adversarial attacks, but it lowers the model's\naccuracy on clean inputs, is computationally expensive, and offers less\nrobustness to natural noise. In contrast, energy-based models (EBMs), which\nwere designed for efficient implementation in neuromorphic hardware and\nphysical systems, incorporate feedback connections from each layer to the\nprevious layer, yielding a recurrent, deep-attractor architecture which we\nhypothesize should make them naturally robust. Our work is the first to explore\nthe robustness of EBMs to both natural corruptions and adversarial attacks,\nwhich we do using the CIFAR-10 and CIFAR-100 datasets. We demonstrate that EBMs\nare more robust than transformers and display comparable robustness to\nadversarially-trained DNNs on gradient-based (white-box) attacks, query-based\n(black-box) attacks, and natural perturbations without sacrificing clean\naccuracy, and without the need for adversarial training or additional training\ntechniques.\n"", '  As deep learning (DL) models are increasingly being integrated into our\neveryday lives, ensuring their safety by making them robust against adversarial\nattacks has become increasingly critical. DL models have been found to be\nsusceptible to adversarial attacks which can be achieved by introducing small,\ntargeted perturbations to disrupt the input data. Adversarial training has been\npresented as a mitigation strategy which can result in more robust models. This\nadversarial robustness comes with additional computational costs required to\ndesign adversarial attacks during training. The two objectives -- adversarial\nrobustness and computational efficiency -- then appear to be in conflict of\neach other. In this work, we explore the effects of two different model\ncompression methods -- structured weight pruning and quantization -- on\nadversarial robustness. We specifically explore the effects of fine-tuning on\ncompressed models, and present the trade-off between standard fine-tuning and\nadversarial fine-tuning. Our results show that compression does not inherently\nlead to loss in model robustness and adversarial fine-tuning of a compressed\nmodel can yield large improvement to the robustness performance of models. We\npresent experiments on two benchmark datasets showing that adversarial\nfine-tuning of compressed models can achieve robustness performance comparable\nto adversarially trained models, while also improving computational efficiency.\n']",Adversarial Attacks and Defenses in Deep Learning
17,16,325,16_reward_rewards_reinforcement_supervised,"['reward', 'rewards', 'reinforcement', 'supervised', 'learning', 'trained', 'language', 'preference', 'sampling', 'regularization']","['preference', 'reward', 'alignment', 'preferences', 'feedback', 'human', 'reinforcement', 'policy', 'responses', 'optimization']","['reinforcement', 'supervised', 'language', 'preference', 'sampling', 'regularization', 'tuning', 'rl', 'verbosity', 'textit']","['  Language model (LM) post-training (or alignment) involves maximizing a reward\nfunction that is derived from preference annotations. Direct Preference\nOptimization (DPO) is a popular offline alignment method that trains a policy\ndirectly on preference data without the need to train a reward model or apply\nreinforcement learning. However, typical preference datasets have only a\nsingle, or at most a few, annotation per preference pair, which causes DPO to\noverconfidently assign rewards that trend towards infinite magnitude. This\nfrequently leads to degenerate policies, sometimes causing even the\nprobabilities of the preferred generations to go to zero. In this work, we\nanalyze this phenomenon and propose distillation to get a better proxy for the\ntrue preference distribution over generation pairs: we train the LM to produce\nprobabilities that match the distribution induced by a reward model trained on\nthe preference data. Moreover, to account for uncertainty in the reward model\nwe are distilling from, we optimize against a family of reward models that, as\na whole, is likely to include at least one reasonable proxy for the preference\ndistribution. Our results show that distilling from such a family of reward\nmodels leads to improved robustness to distribution shift in preference\nannotations, while preserving the simple supervised nature of DPO.\n', '  RLHF has emerged as a pivotal step in aligning language models with human\nobjectives and values. It typically involves learning a reward model from human\npreference data and then using reinforcement learning to update the generative\nmodel accordingly. Conversely, Direct Preference Optimization (DPO) directly\noptimizes the generative model with preference data, skipping reinforcement\nlearning. However, both RLHF and DPO assume uniform preferences, overlooking\nthe reality of diverse human annotators. This paper presents a new method to\nalign generative models with varied human preferences. We propose an\nExpectation-Maximization adaptation to DPO, generating a mixture of models\nbased on latent preference types of the annotators. We then introduce a min-max\nregret ensemble learning model to produce a single generative method to\nminimize worst-case regret among annotator subgroups with similar latent\nfactors. Our algorithms leverage the simplicity of DPO while accommodating\ndiverse preferences. Experimental results validate the effectiveness of our\napproach in producing equitable generative policies.\n', '  The success of reinforcement learning from human feedback (RLHF) in language\nmodel alignment is strongly dependent on the quality of the underlying reward\nmodel. In this paper, we present a novel approach to improve reward model\nquality by generating synthetic preference data, thereby augmenting the\ntraining dataset with on-policy, high-quality preference pairs. Motivated by\nthe promising results of Best-of-N sampling strategies in language model\ntraining, we extend their application to reward model training. This results in\na self-training strategy to generate preference pairs by selecting the best and\nworst candidates in a pool of responses to a given query. Empirically, we find\nthat this approach improves the performance of any reward model, with an effect\ncomparable to the addition of a similar quantity of human preference data. This\nwork opens up new avenues of research for improving RLHF for language model\nalignment, by offering synthetic preference generation as a solution to reward\nmodeling challenges.\n']",Language Model Alignment with Human Preferences
18,17,322,17_bandit_bandits_optimal_regret,"['bandit', 'bandits', 'optimal', 'regret', 'reward', 'optimality', 'exploitation', 'rewards', 'exploration', 'incentive']","['regret', 'bandit', 'bandits', 'arm', 'armed', 'arms', 'bounds', 'reward', 'algorithm', 'contextual']","['bandit', 'optimality', 'rewards', 'sampling', 'guarantees', 'stochastic', 'thompson', 'adversarial', 'learner', 'near']","['  Fast changing states or volatile environments pose a significant challenge to\nonline optimization, which needs to perform rapid adaptation under limited\nobservation. In this paper, we give query and regret optimal bandit algorithms\nunder the strict notion of strongly adaptive regret, which measures the maximum\nregret over any contiguous interval $I$. Due to its worst-case nature, there is\nan almost-linear $\\Omega(|I|^{1-\\epsilon})$ regret lower bound, when only one\nquery per round is allowed [Daniely el al, ICML 2015]. Surprisingly, with just\ntwo queries per round, we give Strongly Adaptive Bandit Learner (StABL) that\nachieves $\\tilde{O}(\\sqrt{n|I|})$ adaptive regret for multi-armed bandits with\n$n$ arms. The bound is tight and cannot be improved in general. Our algorithm\nleverages a multiplicative update scheme of varying stepsizes and a carefully\nchosen observation distribution to control the variance. Furthermore, we extend\nour results and provide optimal algorithms in the bandit convex optimization\nsetting. Finally, we empirically demonstrate the superior performance of our\nalgorithms under volatile environments and for downstream tasks, such as\nalgorithm selection for hyperparameter optimization.\n', ""  Contextual dueling bandit is used to model the bandit problems, where a\nlearner's goal is to find the best arm for a given context using observed noisy\npreference feedback over the selected arms for the past contexts. However,\nexisting algorithms assume the reward function is linear, which can be complex\nand non-linear in many real-life applications like online recommendations or\nranking web search results. To overcome this challenge, we use a neural network\nto estimate the reward function using preference feedback for the previously\nselected arms. We propose upper confidence bound- and Thompson sampling-based\nalgorithms with sub-linear regret guarantees that efficiently select arms in\neach round. We then extend our theoretical results to contextual bandit\nproblems with binary feedback, which is in itself a non-trivial contribution.\nExperimental results on the problem instances derived from synthetic datasets\ncorroborate our theoretical results.\n"", '  We present substantial evidence demonstrating the benefits of integrating\nLarge Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.\nContextual bandits have been widely used in recommendation systems to generate\npersonalized suggestions based on user-specific contexts. We show that LLMs,\npre-trained on extensive corpora rich in human knowledge and preferences, can\nsimulate human behaviours well enough to jump-start contextual multi-armed\nbandits to reduce online learning regret. We propose an initialization\nalgorithm for contextual bandits by prompting LLMs to produce a pre-training\ndataset of approximate human preferences for the bandit. This significantly\nreduces online learning regret and data-gathering costs for training such\nmodels. Our approach is validated empirically through two sets of experiments\nwith different bandit setups: one which utilizes LLMs to serve as an oracle and\na real-world experiment utilizing data from a conjoint survey experiment.\n']",Optimal Bandit Algorithms
19,18,287,18_pdes_pde_learning_neural,"['pdes', 'pde', 'learning', 'neural', 'solvers', 'pinn', 'nonlinear', 'pinns', 'solver', 'networks']","['equations', 'physics', 'differential', 'operator', 'partial', 'equation', 'operators', 'boundary', 'neural', 'numerical']","['pdes', 'solvers', 'pinn', 'nonlinear', 'discretization', 'galerkin', 'convection', 'deeponets', 'dde', 'navier']","['  Learning and solving governing equations of a physical system, represented by\npartial differential equations (PDEs), from data is a central challenge in a\nvariety of areas of science and engineering. Traditional numerical methods for\nsolving PDEs can be computationally expensive for complex systems and require\nthe complete PDEs of the physical system. On the other hand, current\ndata-driven machine learning methods require a large amount of data to learn a\nsurrogate model of the PDE solution operator, which could be impractical. Here,\nwe propose the first solution operator learning method that only requires one\nPDE solution, i.e., one-shot learning. By leveraging the principle of locality\nof PDEs, we consider small local domains instead of the entire computational\ndomain and define a local solution operator. The local solution operator is\nthen trained using a neural network, and utilized to predict the solution of a\nnew input function via mesh-based fixed-point iteration (FPI), meshfree\nlocal-solution-operator informed neural network (LOINN) or\nlocal-solution-operator informed neural network with correction (cLOINN). We\ntest our method on diverse PDEs, including linear or nonlinear PDEs, PDEs\ndefined on complex geometries, and PDE systems, demonstrating the effectiveness\nand generalization capabilities of our method across these varied scenarios.\n', '  Physics-informed neural networks (PINNs) have attracted significant attention\nfor solving partial differential equations (PDEs) in recent years because they\nalleviate the curse of dimensionality that appears in traditional methods.\nHowever, the most disadvantage of PINNs is that one neural network corresponds\nto one PDE. In practice, we usually need to solve a class of PDEs, not just\none. With the explosive growth of deep learning, many useful techniques in\ngeneral deep learning tasks are also suitable for PINNs. Transfer learning\nmethods may reduce the cost for PINNs in solving a class of PDEs. In this\npaper, we proposed a transfer learning method of PINNs via keeping singular\nvectors and optimizing singular values (namely SVD-PINNs). Numerical\nexperiments on high dimensional PDEs (10-d linear parabolic equations and 10-d\nAllen-Cahn equations) show that SVD-PINNs work for solving a class of PDEs with\ndifferent but close right-hand-side functions.\n', '  Physics-informed neural networks (PINNs) have recently emerged as a promising\nway to compute the solutions of partial differential equations (PDEs) using\ndeep neural networks. However, despite their significant success in various\nfields, it remains unclear in many aspects how to effectively train PINNs if\nthe solutions of PDEs exhibit stiff behaviors or high frequencies. In this\npaper, we propose a new method for training PINNs using variable-scaling\ntechniques. This method is simple and it can be applied to a wide range of\nproblems including PDEs with rapidly-varying solutions. Throughout various\nnumerical experiments, we will demonstrate the effectiveness of the proposed\nmethod for these problems and confirm that it can significantly improve the\ntraining efficiency and performance of PINNs. Furthermore, based on the\nanalysis of the neural tangent kernel (NTK), we will provide theoretical\nevidence for this phenomenon and show that our methods can indeed improve the\nperformance of PINNs.\n']",Physics-Informed Neural Networks for Solving Partial Differential Equations
20,19,286,19_graphs_subgraph_networks_graph,"['graphs', 'subgraph', 'networks', 'graph', 'subgraphs', 'nodes', 'node', 'neural', 'gnn', 'gnns']","['graph', 'node', 'message', 'nodes', 'passing', 'graphs', 'homophily', 'heterophilic', 'subgraph', 'networks']","['subgraph', 'networks', 'node', 'embeddings', 'gne', 'mpnn', 'gcns', 'attention', 'messages', 'generalization']","['  Graph neural networks (GNNs) have become the \\textit{de facto} standard for\nrepresentational learning in graphs, and have achieved state-of-the-art\nperformance in many graph-related tasks; however, it has been shown that the\nexpressive power of standard GNNs are equivalent maximally to 1-dimensional\nWeisfeiler-Lehman (1-WL) Test. Recently, there is a line of works aiming to\nenhance the expressive power of graph neural networks. One line of such works\naim at developing $K$-hop message-passing GNNs where node representation is\nupdated by aggregating information from not only direct neighbors but all\nneighbors within $K$-hop of the node. Another line of works leverages subgraph\ninformation to enhance the expressive power which is proven to be strictly more\npowerful than 1-WL test. In this work, we discuss the limitation of $K$-hop\nmessage-passing GNNs and propose \\textit{substructure encoding function} to\nuplift the expressive power of any $K$-hop message-passing GNN. We further\ninject contextualized substructure information to enhance the expressiveness of\n$K$-hop message-passing GNNs. Our method is provably more powerful than\nprevious works on $K$-hop graph neural networks and 1-WL subgraph GNNs, which\nis a specific type of subgraph based GNN models, and not less powerful than\n3-WL. Empirically, our proposed method set new state-of-the-art performance or\nachieves comparable performance for a variety of datasets. Our code is\navailable at \\url{https://github.com/tianyao-aka/Expresive_K_hop_GNNs}.\n', ""  Graph Neural Network (GNN), with the main idea of encoding graph structure\ninformation of graphs by propagation and aggregation, has developed rapidly. It\nachieved excellent performance in representation learning of multiple types of\ngraphs such as homogeneous graphs, heterogeneous graphs, and more complex\ngraphs like knowledge graphs. However, merely stacking GNN layers may not\nimprove the model's performance and can even be detrimental. For the phenomenon\nof performance degradation in deep GNNs, we propose a new perspective. Unlike\nthe popular explanations of over-smoothing or over-squashing, we think the\nissue arises from the interference of low-quality node representations during\nmessage propagation. We introduce a simple and general method, SF-GNN, to\naddress this problem. In SF-GNN, we define two representations for each node,\none is the node representation that represents the feature of the node itself,\nand the other is the message representation specifically for propagating\nmessages to neighbor nodes. A self-filter module evaluates the quality of the\nnode representation and decides whether to integrate it into the message\npropagation based on this quality assessment. Experiments on node\nclassification tasks for both homogeneous and heterogeneous graphs, as well as\nlink prediction tasks on knowledge graphs, demonstrate that our method can be\napplied to various GNN models and outperforms state-of-the-art baseline methods\nin addressing deep GNN degradation.\n"", '  Graph neural networks (GNNs) have achieved state-of-the-art performance in\ngraph representation learning. Message passing neural networks, which learn\nrepresentations through recursively aggregating information from each node and\nits neighbors, are among the most commonly-used GNNs. However, a wealth of\nstructural information of individual nodes and full graphs is often ignored in\nsuch process, which restricts the expressive power of GNNs. Various graph data\naugmentation methods that enable the message passing with richer structure\nknowledge have been introduced as one main way to tackle this issue, but they\nare often focused on individual structure features and difficult to scale up\nwith more structure features. In this work we propose a novel approach, namely\ncollective structure knowledge-augmented graph neural network (CoS-GNN), in\nwhich a new message passing method is introduced to allow GNNs to harness a\ndiverse set of node- and graph-level structure features, together with original\nnode features/attributes, in augmented graphs. In doing so, our approach\nlargely improves the structural knowledge modeling of GNNs in both node and\ngraph levels, resulting in substantially improved graph representations. This\nis justified by extensive empirical results where CoS-GNN outperforms\nstate-of-the-art models in various graph-level learning tasks, including graph\nclassification, anomaly detection, and out-of-distribution generalization.\n']",Graph Neural Networks
21,20,285,20_federated_distributed_collaborative_clustered,"['federated', 'distributed', 'collaborative', 'clustered', 'collaboratively', 'shared', 'learning', 'aggregated', 'centralized', 'client']","['clients', 'federated', 'client', 'heterogeneity', 'local', 'server', 'global', 'communication', 'convergence', 'heterogeneous']","['federated', 'collaborative', 'clustered', 'datasets', 'generalization', 'privacy', 'decentralized', 'personalized', 'locally', 'synchronous']","['  Federated learning (FL), as an emerging collaborative learning paradigm, has\ngarnered significant attention due to its capacity to preserve privacy within\ndistributed learning systems. In these systems, clients collaboratively train a\nunified neural network model using their local datasets and share model\nparameters rather than raw data, enhancing privacy. Predominantly, FL systems\nare designed for mobile and edge computing environments where training\ntypically occurs over wireless networks. Consequently, as model sizes increase,\nthe conventional FL frameworks increasingly consume substantial communication\nresources. To address this challenge and improve communication efficiency, this\npaper introduces a novel hierarchical FL framework that integrates the benefits\nof clustered FL and model compression. We present an adaptive clustering\nalgorithm that identifies a core client and dynamically organizes clients into\nclusters. Furthermore, to enhance transmission efficiency, each core client\nimplements a local aggregation with compression (LC aggregation) algorithm\nafter collecting compressed models from other clients within the same cluster.\nSimulation results affirm that our proposed algorithms not only maintain\ncomparable predictive accuracy but also significantly reduce energy consumption\nrelative to existing FL mechanisms.\n', '  Federated Learning (FL) is a distributed machine learning paradigm that\nallows clients to train models on their data while preserving their privacy. FL\nalgorithms, such as Federated Averaging (FedAvg) and its variants, have been\nshown to converge well in many scenarios. However, these methods require\nclients to upload their local updates to the server in a synchronous manner,\nwhich can be slow and unreliable in realistic FL settings. To address this\nissue, researchers have developed asynchronous FL methods that allow clients to\ncontinue training on their local data using a stale global model. However, most\nof these methods simply aggregate all of the received updates without\nconsidering their relative contributions, which can slow down convergence. In\nthis paper, we propose a contribution-aware asynchronous FL method that takes\ninto account the staleness and statistical heterogeneity of the received\nupdates. Our method dynamically adjusts the contribution of each update based\non these factors, which can speed up convergence compared to existing methods.\n', ""  Federated Learning (FL) allows several clients to construct a common global\nmachine-learning model without having to share their data. FL, however, faces\nthe challenge of statistical heterogeneity between the client's data, which\ndegrades performance and slows down the convergence toward the global model. In\nthis paper, we provide theoretical proof that minimizing heterogeneity between\nclients facilitates the convergence of a global model for every single client.\nThis becomes particularly important under empirical concept shifts among\nclients, rather than merely considering imbalanced classes, which have been\nstudied until now. Therefore, we propose a method for knowledge transfer\nbetween clients where the server trains client-specific generators. Each\ngenerator generates samples for the corresponding client to remove the conflict\nwith other clients' models. Experiments conducted on synthetic and real data,\nalong with a theoretical study, support the effectiveness of our method in\nconstructing a well-generalizable global model by reducing the conflict between\nlocal models.\n""]",Federated Learning Systems
22,21,265,21_coding_programming_compiler_code,"['coding', 'programming', 'compiler', 'code', 'coder', 'programmers', 'programmer', 'snippets', 'runtime', 'program']","['code', 'programming', 'software', 'generation', 'coding', 'developers', 'programs', 'execution', 'source', 'program']","['coding', 'compiler', 'snippets', 'developers', 'benchmarks', 'codellama', 'starcoder', 'java', 'python', 'debugging']","['  Context. Nowadays, 83% of software developers use Large Language Models\n(LLMs) to generate code. LLMs recently became essential to increase the\nproductivity of software developers and decrease the time and cost of software\ndevelopment. Developers ranging from novices to experts use LLM tools not only\nto detect and patch bugs, but also to integrate generated code into their\nsoftware. However, as of today there is no objective assessment of the energy\nefficiency of the source code generated by LLM tools. Released in August 2023,\nCode Llama is one of the most recent LLM tools.\n  Goal. In this paper, we present an empirical study that assesses the energy\nefficiency of Code Llama with respect to human-written source code.\n  Method. We design an experiment involving three human-written benchmarks\nimplemented in C++, JavaScript, and Python. We ask Code Llama to generate the\ncode of the benchmarks using different prompts and temperatures. Therefore, we\nexecute both implementations and profile their energy efficiency.\n  Results. Our study shows that the energy efficiency of code generated by Code\nLlama is heavily-dependent on the chosen programming language and the specific\ncode problem at hand. Also, human implementations tend to be more energy\nefficient overall, with generated JavaScript code outperforming its human\ncounterpart. Moreover, explicitly asking Code Llama to generate\nenergy-efficient code results in an equal or worse energy efficiency, as well\nas using different temperatures seems not to affect the energy efficiency of\ngenerated code.\n  Conclusions. According to our results, code generated using Code Llama does\nnot guarantee energy efficiency, even when prompted to do so. Therefore,\nsoftware developers should evaluate the energy efficiency of generated code\nbefore integrating it into the software system under development.\n', ""  Code Large Language Models (Code LLMs) have demonstrated outstanding\nperformance in code-related tasks. Several instruction tuning approaches have\nbeen proposed to boost the code generation performance of pre-trained Code\nLLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with\nself-evaluating for code generation. It learns diverse instruction targets and\ncombines a code evaluation objective to enhance its code generation ability.\nOur model achieves superior performance on the HumanEval and MBPP benchmarks,\ndemonstrating new insights for future code instruction tuning work. Our key\nfindings are: (1) Augmenting more diverse responses with distinct reasoning\npaths increases the code capability of LLMs. (2) Improving one's ability to\nevaluate the correctness of code solutions also enhances their ability to\ncreate it.\n"", '  Large language models for code (i.e., code LLMs) have shown strong code\nunderstanding and generation capabilities. To evaluate the capabilities of code\nLLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval\nand ClassEval). Code reasoning is one of the most essential abilities of code\nLLMs, but existing benchmarks for code reasoning are not sufficient. Typically,\nthey focus on predicting the input and output of a program, ignoring the\nevaluation of the intermediate behavior during program execution, as well as\nthe logical consistency (e.g., the model should not give the correct output if\nthe prediction of execution path is wrong) when performing the reasoning. To\naddress these problems, in this paper, we propose a framework, namely REval,\nfor evaluating code reasoning abilities and consistency of code LLMs with\nprogram execution. We utilize existing code benchmarks and adapt them to new\nbenchmarks within our framework. A large-scale empirical study is conducted and\nmost LLMs show unsatisfactory performance on both Runtime Behavior Reasoning\n(i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation\n(i.e., an average IC score of 10.3). Evaluation results of current code LLMs\nreflect the urgent need for the community to strengthen the code reasoning\ncapability of code LLMs. Our code, data, and \\newname leaderboard are available\nat https://r-eval.github.io.\n']",Code Generation and Efficiency with Large Language Models
23,22,258,22_forgetting_continual_learning_memory,"['forgetting', 'continual', 'learning', 'memory', 'learned', 'retention', 'forget', 'retaining', 'learn', 'continually']","['continual', 'forgetting', 'catastrophic', 'incremental', 'old', 'replay', 'classes', 'class', 'stream', 'new']","['forgetting', 'lifelong', 'regularization', 'overfitting', 'training', 'classifier', 'representations', 'continuously', 'storing', 'imagenet']","['  Continual learning strives to ensure stability in solving previously seen\ntasks while demonstrating plasticity in a novel domain. Recent advances in\ncontinual learning are mostly confined to a supervised learning setting,\nespecially in NLP domain. In this work, we consider a few-shot continual active\nlearning setting where labeled data are inadequate, and unlabeled data are\nabundant but with a limited annotation budget. We exploit meta-learning and\npropose a method, called Meta-Continual Active Learning. This method\nsequentially queries the most informative examples from a pool of unlabeled\ndata for annotation to enhance task-specific performance and tackle continual\nlearning problems through meta-objective. Specifically, we employ meta-learning\nand experience replay to address inter-task confusion and catastrophic\nforgetting. We further incorporate textual augmentations to avoid memory\nover-fitting caused by experience replay and sample queries, thereby ensuring\ngeneralization. We conduct extensive experiments on benchmark text\nclassification datasets from diverse domains to validate the feasibility and\neffectiveness of meta-continual active learning. We also analyze the impact of\ndifferent active learning strategies on various meta continual learning models.\nThe experimental results demonstrate that introducing randomness into sample\nselection is the best default strategy for maintaining generalization in\nmeta-continual learning framework.\n', '  A key challenge for machine intelligence is to learn new visual concepts\nwithout forgetting the previously acquired knowledge. Continual learning is\naimed towards addressing this challenge. However, there is a gap between\nexisting supervised continual learning and human-like intelligence, where human\nis able to learn from both labeled and unlabeled data. How unlabeled data\naffects learning and catastrophic forgetting in the continual learning process\nremains unknown. To explore these issues, we formulate a new semi-supervised\ncontinual learning method, which can be generically applied to existing\ncontinual learning models. Specifically, a novel gradient learner learns from\nlabeled data to predict gradients on unlabeled data. Hence, the unlabeled data\ncould fit into the supervised continual learning method. Different from\nconventional semi-supervised settings, we do not hypothesize that the\nunderlying classes, which are associated to the unlabeled data, are known to\nthe learning process. In other words, the unlabeled data could be very distinct\nfrom the labeled data. We evaluate the proposed method on mainstream continual\nlearning, adversarial continual learning, and semi-supervised learning tasks.\nThe proposed method achieves state-of-the-art performance on classification\naccuracy and backward transfer in the continual learning setting while\nachieving desired performance on classification accuracy in the semi-supervised\nlearning setting. This implies that the unlabeled images can enhance the\ngeneralizability of continual learning models on the predictive ability on\nunseen data and significantly alleviate catastrophic forgetting. The code is\navailable at \\url{https://github.com/luoyan407/grad_prediction.git}.\n', ""  Continual learning (CL) aims to incrementally learn different tasks (such as\nclassification) in a non-stationary data stream without forgetting old ones.\nMost CL works focus on tackling catastrophic forgetting under a\nlearning-from-scratch paradigm. However, with the increasing prominence of\nfoundation models, pre-trained models equipped with informative representations\nhave become available for various downstream requirements. Several CL methods\nbased on pre-trained models have been explored, either utilizing pre-extracted\nfeatures directly (which makes bridging distribution gaps challenging) or\nincorporating adaptors (which may be subject to forgetting). In this paper, we\npropose a concise and effective approach for CL with pre-trained models. Given\nthat forgetting occurs during parameter updating, we contemplate an alternative\napproach that exploits training-free random projectors and class-prototype\naccumulation, which thus bypasses the issue. Specifically, we inject a frozen\nRandom Projection layer with nonlinear activation between the pre-trained\nmodel's feature representations and output head, which captures interactions\nbetween features with expanded dimensionality, providing enhanced linear\nseparability for class-prototype-based CL. We also demonstrate the importance\nof decorrelating the class-prototypes to reduce the distribution disparity when\nusing pre-trained representations. These techniques prove to be effective and\ncircumvent the problem of forgetting for both class- and domain-incremental\ncontinual learning. Compared to previous methods applied to pre-trained\nViT-B/16 models, we reduce final error rates by between 20% and 62% on seven\nclass-incremental benchmarks, despite not using any rehearsal memory. We\nconclude that the full potential of pre-trained models for simple, effective,\nand fast CL has not hitherto been fully tapped. Code is at\ngithub.com/RanPAC/RanPAC.\n""]",Continual Learning and Forgetting in Artificial Intelligence
24,23,251,23_optimality_mdps_optimal_reinforcement,"['optimality', 'mdps', 'optimal', 'reinforcement', 'markovian', 'mdp', 'reward', 'rewards', 'markov', 'discount']","['policy', 'horizon', 'reinforcement', 'approximation', 'value', 'convergence', 'regret', 'algorithm', 'sample', 'gradient']","['optimality', 'mdps', 'reinforcement', 'markovian', 'discount', 'policies', 'critic', 'exploration', 'optimism', 'iteration']","['  We study reward-free reinforcement learning (RL) with linear function\napproximation, where the agent works in two phases: (1) in the exploration\nphase, the agent interacts with the environment but cannot access the reward;\nand (2) in the planning phase, the agent is given a reward function and is\nexpected to find a near-optimal policy based on samples collected in the\nexploration phase. The sample complexities of existing reward-free algorithms\nhave a polynomial dependence on the planning horizon, which makes them\nintractable for long planning horizon RL problems. In this paper, we propose a\nnew reward-free algorithm for learning linear mixture Markov decision processes\n(MDPs), where the transition probability can be parameterized as a linear\ncombination of known feature mappings. At the core of our algorithm is\nuncertainty-weighted value-targeted regression with exploration-driven\npseudo-reward and a high-order moment estimator for the aleatoric and epistemic\nuncertainties. When the total reward is bounded by $1$, we show that our\nalgorithm only needs to explore $\\tilde O( d^2\\varepsilon^{-2})$ episodes to\nfind an $\\varepsilon$-optimal policy, where $d$ is the dimension of the feature\nmapping. The sample complexity of our algorithm only has a polylogarithmic\ndependence on the planning horizon and therefore is ""horizon-free"". In\naddition, we provide an $\\Omega(d^2\\varepsilon^{-2})$ sample complexity lower\nbound, which matches the sample complexity of our algorithm up to logarithmic\nfactors, suggesting that our algorithm is optimal.\n', '  We study offline reinforcement learning (RL) with linear MDPs under the\ninfinite-horizon discounted setting which aims to learn a policy that maximizes\nthe expected discounted cumulative reward using a pre-collected dataset.\nExisting algorithms for this setting either require a uniform data coverage\nassumptions or are computationally inefficient for finding an\n$\\epsilon$-optimal policy with $O(\\epsilon^{-2})$ sample complexity. In this\npaper, we propose a primal dual algorithm for offline RL with linear MDPs in\nthe infinite-horizon discounted setting. Our algorithm is the first\ncomputationally efficient algorithm in this setting that achieves sample\ncomplexity of $O(\\epsilon^{-2})$ with partial data coverage assumption. Our\nwork is an improvement upon a recent work that requires $O(\\epsilon^{-4})$\nsamples. Moreover, we extend our algorithm to work in the offline constrained\nRL setting that enforces constraints on additional reward signals.\n', '  We present the first finite time global convergence analysis of policy\ngradient in the context of infinite horizon average reward Markov decision\nprocesses (MDPs). Specifically, we focus on ergodic tabular MDPs with finite\nstate and action spaces. Our analysis shows that the policy gradient iterates\nconverge to the optimal policy at a sublinear rate of\n$O\\left({\\frac{1}{T}}\\right),$ which translates to $O\\left({\\log(T)}\\right)$\nregret, where $T$ represents the number of iterations. Prior work on\nperformance bounds for discounted reward MDPs cannot be extended to average\nreward MDPs because the bounds grow proportional to the fifth power of the\neffective horizon. Thus, our primary contribution is in proving that the policy\ngradient algorithm converges for average-reward MDPs and in obtaining\nfinite-time performance guarantees. In contrast to the existing discounted\nreward performance bounds, our performance bounds have an explicit dependence\non constants that capture the complexity of the underlying MDP. Motivated by\nthis observation, we reexamine and improve the existing performance bounds for\ndiscounted reward MDPs. We also present simulations to empirically evaluate the\nperformance of average reward policy gradient algorithm.\n']",Optimal Reinforcement Learning in Markov Decision Processes
25,24,245,24_ai_accountability_intelligence_ethics,"['ai', 'accountability', 'intelligence', 'ethics', 'ethical', 'governance', 'aia', 'oversight', 'governments', 'responsibility']","['governance', 'trust', 'risks', 'regulatory', 'ethical', 'safety', 'ethics', 'regulation', 'systems', 'risk']","['ai', 'ethical', 'governance', 'oversight', 'eu', 'initiatives', 'assurance', 'audits', 'compliance', 'trustworthiness']","['  The evolution of AI is set to profoundly reshape the future. The European\nUnion, recognizing this impending prominence, has enacted the AI Act,\nregulating market access for AI-based systems. A salient feature of the Act is\nto guard democratic and humanistic values by focusing regulation on\ntransparency, explainability, and the human ability to understand and control\nAI systems. Hereby, the EU AI Act does not merely specify technological\nrequirements for AI systems. The EU issues a democratic call for human-centered\nAI systems and, in turn, an interdisciplinary research agenda for\nhuman-centered innovation in AI development. Without robust methods to assess\nAI systems and their effect on individuals and society, the EU AI Act may lead\nto repeating the mistakes of the General Data Protection Regulation of the EU\nand to rushed, chaotic, ad-hoc, and ambiguous implementation, causing more\nconfusion than lending guidance. Moreover, determined research activities in\nHuman-AI interaction will be pivotal for both regulatory compliance and the\nadvancement of AI in a manner that is both ethical and effective. Such an\napproach will ensure that AI development aligns with human values and needs,\nfostering a technology landscape that is innovative, responsible, and an\nintegral part of our society.\n', '  Our research endeavors to advance the concept of responsible artificial\nintelligence (AI), a topic of increasing importance within EU policy\ndiscussions. The EU has recently issued several publications emphasizing the\nnecessity of trust in AI, underscoring the dual nature of AI as both a\nbeneficial tool and a potential weapon. This dichotomy highlights the urgent\nneed for international regulation. Concurrently, there is a need for frameworks\nthat guide companies in AI development, ensuring compliance with such\nregulations. Our research aims to assist lawmakers and machine learning\npractitioners in navigating the evolving landscape of AI regulation,\nidentifying focal areas for future attention. This paper introduces a\ncomprehensive and, to our knowledge, the first unified definition of\nresponsible AI. Through a structured literature review, we elucidate the\ncurrent understanding of responsible AI. Drawing from this analysis, we propose\nan approach for developing a future framework centered around this concept. Our\nfindings advocate for a human-centric approach to Responsible AI. This approach\nencompasses the implementation of AI methods with a strong emphasis on ethics,\nmodel explainability, and the pillars of privacy, security, and trust.\n', ""  As AI systems become increasingly prevalent and impactful, the need for\neffective AI governance and accountability measures is paramount. This paper\nexamines the AI governance landscape, focusing on Anthropic's Claude, a\nfoundational AI model. We analyze Claude through the lens of the NIST AI Risk\nManagement Framework and the EU AI Act, identifying potential threats and\nproposing mitigation strategies. The paper highlights the importance of\ntransparency, rigorous benchmarking, and comprehensive data handling processes\nin ensuring the responsible development and deployment of AI systems. We\nconclude by discussing the social impact of AI governance and the ethical\nconsiderations surrounding AI accountability.\n""]","""AI Ethics and Governance"""
26,25,245,25_3d_scenes_scene_voxel,"['3d', 'scenes', 'scene', 'voxel', 'depth', 'viewpoints', 'rendering', 'view', '3dgs', 'shading']","['scene', 'view', 'scenes', 'depth', 'rendering', 'radiance', 'reconstruction', 'geometry', 'views', 'camera']","['3d', 'voxel', 'depth', 'views', 'blur', 'rgb', 'flow', 'realistic', 'nerfs', 'rendered']","['  Editing a local region or a specific object in a 3D scene represented by a\nNeRF or consistently blending a new realistic object into the scene is\nchallenging, mainly due to the implicit nature of the scene representation. We\npresent Blended-NeRF, a robust and flexible framework for editing a specific\nregion of interest in an existing NeRF scene, based on text prompts, along with\na 3D ROI box. Our method leverages a pretrained language-image model to steer\nthe synthesis towards a user-provided text prompt, along with a 3D MLP model\ninitialized on an existing NeRF scene to generate the object and blend it into\na specified region in the original scene. We allow local editing by localizing\na 3D ROI box in the input scene, and blend the content synthesized inside the\nROI with the existing scene using a novel volumetric blending technique. To\nobtain natural looking and view-consistent results, we leverage existing and\nnew geometric priors and 3D augmentations for improving the visual fidelity of\nthe final result. We test our framework both qualitatively and quantitatively\non a variety of real 3D scenes and text prompts, demonstrating realistic\nmulti-view consistent results with much flexibility and diversity compared to\nthe baselines. Finally, we show the applicability of our framework for several\n3D editing applications, including adding new objects to a scene,\nremoving/replacing/altering existing objects, and texture conversion.\n', '  Text-driven 3D scene generation techniques have made rapid progress in recent\nyears. Their success is mainly attributed to using existing generative models\nto iteratively perform image warping and inpainting to generate 3D scenes.\nHowever, these methods heavily rely on the outputs of existing models, leading\nto error accumulation in geometry and appearance that prevent the models from\nbeing used in various scenarios (e.g., outdoor and unreal scenarios). To\naddress this limitation, we generatively refine the newly generated local views\nby querying and aggregating global 3D information, and then progressively\ngenerate the 3D scene. Specifically, we employ a tri-plane features-based NeRF\nas a unified representation of the 3D scene to constrain global 3D consistency,\nand propose a generative refinement network to synthesize new contents with\nhigher quality by exploiting the natural image prior from 2D diffusion model as\nwell as the global 3D information of the current scene. Our extensive\nexperiments demonstrate that, in comparison to previous methods, our approach\nsupports wide variety of scene generation and arbitrary camera trajectories\nwith improved visual quality and 3D consistency.\n', '  Generating 3D scenes is a challenging open problem, which requires\nsynthesizing plausible content that is fully consistent in 3D space. While\nrecent methods such as neural radiance fields excel at view synthesis and 3D\nreconstruction, they cannot synthesize plausible details in unobserved regions\nsince they lack a generative capability. Conversely, existing generative\nmethods are typically not capable of reconstructing detailed, large-scale\nscenes in the wild, as they use limited-capacity 3D scene representations,\nrequire aligned camera poses, or rely on additional regularizers. In this work,\nwe introduce the first diffusion model able to perform fast, detailed\nreconstruction and generation of real-world 3D scenes. To achieve this, we make\nthree contributions. First, we introduce a new neural scene representation,\nIB-planes, that can efficiently and accurately represent large 3D scenes,\ndynamically allocating more capacity as needed to capture details visible in\neach image. Second, we propose a denoising-diffusion framework to learn a prior\nover this novel 3D scene representation, using only 2D images without the need\nfor any additional supervision signal such as masks or depths. This supports 3D\nreconstruction and generation in a unified architecture. Third, we develop a\nprincipled approach to avoid trivial 3D solutions when integrating the\nimage-based rendering with the diffusion model, by dropping out representations\nof some images. We evaluate the model on several challenging datasets of real\nand synthetic images, and demonstrate superior results on generation, novel\nview synthesis and 3D reconstruction.\n']",3D Scene Generation and Editing
27,26,242,26_anomaly_anomalies_anomalous_supervised,"['anomaly', 'anomalies', 'anomalous', 'supervised', 'autoencoders', 'detection', 'detecting', 'detect', 'autoencoder', 'datasets']","['anomaly', 'anomalies', 'detection', 'normal', 'anomalous', 'series', 'unsupervised', 'reconstruction', 'autoencoder', 'time']","['anomalous', 'autoencoders', 'datasets', 'lstm', 'detector', 'unsupervised', 'logs', 'patterns', 'attention', 'events']","['  Unsupervised Anomaly Detection (UAD) plays a crucial role in identifying\nabnormal patterns within data without labeled examples, holding significant\npractical implications across various domains. Although the individual\ncontributions of representation learning and clustering to anomaly detection\nare well-established, their interdependencies remain under-explored due to the\nabsence of a unified theoretical framework. Consequently, their collective\npotential to enhance anomaly detection performance remains largely untapped. To\nbridge this gap, in this paper, we propose a novel probabilistic mixture model\nfor anomaly detection to establish a theoretical connection among\nrepresentation learning, clustering, and anomaly detection. By maximizing a\nnovel anomaly-aware data likelihood, representation learning and clustering can\neffectively reduce the adverse impact of anomalous data and collaboratively\nbenefit anomaly detection. Meanwhile, a theoretically substantiated anomaly\nscore is naturally derived from this framework. Lastly, drawing inspiration\nfrom gravitational analysis in physics, we have devised an improved anomaly\nscore that more effectively harnesses the combined power of representation\nlearning and clustering. Extensive experiments, involving 17 baseline methods\nacross 30 diverse datasets, validate the effectiveness and generalization\ncapability of the proposed method, surpassing state-of-the-art methods.\n', '  Transformer, as one of the most advanced neural network models in Natural\nLanguage Processing (NLP), exhibits diverse applications in the field of\nanomaly detection. To inspire research on Transformer-based anomaly detection,\nthis review offers a fresh perspective on the concept of anomaly detection. We\nexplore the current challenges of anomaly detection and provide detailed\ninsights into the operating principles of Transformer and its variants in\nanomaly detection tasks. Additionally, we delineate various application\nscenarios for Transformer-based anomaly detection models and discuss the\ndatasets and evaluation metrics employed. Furthermore, this review highlights\nthe key challenges in Transformer-based anomaly detection research and conducts\na comprehensive analysis of future research trends in this domain. The review\nincludes an extensive compilation of over 100 core references related to\nTransformer-based anomaly detection. To the best of our knowledge, this is the\nfirst comprehensive review that focuses on the research related to Transformer\nin the context of anomaly detection. We hope that this paper can provide\ndetailed technical information to researchers interested in Transformer-based\nanomaly detection tasks.\n', '  Semi-supervised anomaly detection, which aims to improve the performance of\nthe anomaly detector by using a small amount of anomaly data in addition to\nunlabeled data, has attracted attention. Existing semi-supervised approaches\nassume that unlabeled data are mostly normal. They train the anomaly detector\nto minimize the anomaly scores for the unlabeled data, and to maximize those\nfor the anomaly data. However, in practice, the unlabeled data are often\ncontaminated with anomalies. This weakens the effect of maximizing the anomaly\nscores for anomalies, and prevents us from improving the detection performance.\nTo solve this problem, we propose the positive-unlabeled autoencoder, which is\nbased on positive-unlabeled learning and the anomaly detector such as the\nautoencoder. With our approach, we can approximate the anomaly scores for\nnormal data using the unlabeled and anomaly data. Therefore, without the\nlabeled normal data, we can train the anomaly detector to minimize the anomaly\nscores for normal data, and to maximize those for the anomaly data. In\naddition, our approach is applicable to various anomaly detectors such as the\nDeepSVDD. Experiments on various datasets show that our approach achieves\nbetter detection performance than existing approaches.\n']",Anomaly Detection Methods
28,27,236,27_fairness_bias_discrimination_unfairness,"['fairness', 'bias', 'discrimination', 'unfairness', 'discriminatory', 'unfair', 'biases', 'equalized', 'classifiers', 'classifier']","['fairness', 'fair', 'demographic', 'groups', 'sensitive', 'discrimination', 'group', 'attributes', 'unfairness', 'bias']","['fairness', 'discrimination', 'equalized', 'classifier', 'biased', 'mitigation', 'equitable', 'outcomes', 'datasets', 'disparity']","['  While significant advancements have been made in the field of fair machine\nlearning, the majority of studies focus on scenarios where the decision model\noperates on a static population. In this paper, we study fairness in dynamic\nsystems where sequential decisions are made. Each decision may shift the\nunderlying distribution of features or user behavior. We model the dynamic\nsystem through a Markov Decision Process (MDP). By acknowledging that\ntraditional fairness notions and long-term fairness are distinct requirements\nthat may not necessarily align with one another, we propose an algorithmic\nframework to integrate various fairness considerations with reinforcement\nlearning using both pre-processing and in-processing approaches. Three case\nstudies show that our method can strike a balance between traditional fairness\nnotions, long-term fairness, and utility.\n', '  Training supervised machine learning systems with a fairness loss can improve\nprediction fairness across different demographic groups. However, doing so\nrequires demographic annotations for training data, without which we cannot\nproduce debiased classifiers for most tasks. Drawing inspiration from transfer\nlearning methods, we investigate whether we can utilize demographic data from a\nrelated task to improve the fairness of a target task. We adapt a single-task\nfairness loss to a multi-task setting to exploit demographic labels from a\nrelated task in debiasing a target task and demonstrate that demographic\nfairness objectives transfer fairness within a multi-task framework.\nAdditionally, we show that this approach enables intersectional fairness by\ntransferring between two datasets with different single-axis demographics. We\nexplore different data domains to show how our loss can improve fairness\ndomains and tasks.\n', '  With the introduction of machine learning in high-stakes decision making,\nensuring algorithmic fairness has become an increasingly important problem to\nsolve. In response to this, many mathematical definitions of fairness have been\nproposed, and a variety of optimisation techniques have been developed, all\ndesigned to maximise a defined notion of fairness. However, fair solutions are\nreliant on the quality of the training data, and can be highly sensitive to\nnoise. Recent studies have shown that robustness (the ability for a model to\nperform well on unseen data) plays a significant role in the type of strategy\nthat should be used when approaching a new problem and, hence, measuring the\nrobustness of these strategies has become a fundamental problem. In this work,\nwe therefore propose a new criterion to measure the robustness of various\nfairness optimisation strategies - the robustness ratio. We conduct multiple\nextensive experiments on five bench mark fairness data sets using three of the\nmost popular fairness strategies with respect to four of the most popular\ndefinitions of fairness. Our experiments empirically show that fairness methods\nthat rely on threshold optimisation are very sensitive to noise in all the\nevaluated data sets, despite mostly outperforming other methods. This is in\ncontrast to the other two methods, which are less fair for low noise scenarios\nbut fairer for high noise ones. To the best of our knowledge, we are the first\nto quantitatively evaluate the robustness of fairness optimisation strategies.\nThis can potentially can serve as a guideline in choosing the most suitable\nfairness strategy for various data sets.\n']",Fairness in Machine Learning
29,28,233,28_voice_utterances_transcription_asr,"['voice', 'utterances', 'transcription', 'asr', 'corpus', 'talker', 'wav2vec', 'encoder', 'speech', 'transcriptions']","['speech', 'recognition', 'automatic', 'word', 'languages', 'acoustic', 'phoneme', 'error', 'encoder', 'audio']","['utterances', 'asr', 'talker', 'wav2vec', 'encoder', 'transcriptions', 'accents', 'acoustic', 'stuttering', 'transducer']","['  Recent advancements in supervised automatic speech recognition (ASR) have\nachieved remarkable performance, largely due to the growing availability of\nlarge transcribed speech corpora. However, most languages lack sufficient\npaired speech and text data to effectively train these systems. In this\narticle, we tackle the challenge of developing ASR systems without paired\nspeech and text corpora by proposing the removal of reliance on a phoneme\nlexicon. We explore a new research direction: word-level unsupervised ASR.\nUsing a curated speech corpus containing only high-frequency English words, our\nsystem achieves a word error rate of nearly 20% without parallel transcripts or\noracle word boundaries. Furthermore, we experimentally demonstrate that an\nunsupervised speech recognizer can emerge from joint speech-to-speech and\ntext-to-text masked token-infilling. This innovative model surpasses the\nperformance of previous unsupervised ASR models trained with direct\ndistribution matching.\n', '  In this paper, we focus on solving one of the most important tasks in the\nfield of speech processing, i.e., automatic speech recognition (ASR), with\nspeech foundation encoders and large language models (LLM). Recent works have\ncomplex designs such as compressing the output temporally for the speech\nencoder, tackling modal alignment for the projector, and utilizing\nparameter-efficient fine-tuning for the LLM. We found that delicate designs are\nnot necessary, while an embarrassingly simple composition of off-the-shelf\nspeech encoder, LLM, and the only trainable linear projector is competent for\nthe ASR task. To be more specific, we benchmark and explore various\ncombinations of LLMs and speech encoders, leading to the optimal LLM-based ASR\nsystem, which we call SLAM-ASR. The proposed SLAM-ASR provides a clean setup\nand little task-specific design, where only the linear projector is trained. To\nthe best of our knowledge, SLAM-ASR achieves the best performance on the\nLibrispeech benchmark among LLM-based ASR models and even outperforms the\nlatest LLM-based audio-universal model trained on massive pair data. Finally,\nwe explore the capability emergence of LLM-based ASR in the process of modal\nalignment. We hope that our study can facilitate the research on extending LLM\nwith cross-modality capacity and shed light on the LLM-based ASR community.\n', ""  In the realm of automatic speech recognition (ASR), robustness in noisy\nenvironments remains a significant challenge. Recent ASR models, such as\nWhisper, have shown promise, but their efficacy in noisy conditions can be\nfurther enhanced. This study is focused on recovering from packet loss to\nimprove the word error rate (WER) of ASR models. We propose using a front-end\nadaptation network connected to a frozen ASR model. The adaptation network is\ntrained to modify the corrupted input spectrum by minimizing the criteria of\nthe ASR model in addition to an enhancement loss function. Our experiments\ndemonstrate that the adaptation network, trained on Whisper's criteria, notably\nreduces word error rates across domains and languages in packet-loss scenarios.\nThis improvement is achieved with minimal affect to Whisper model's\nfoundational performance, underscoring our method's practicality and potential\nin enhancing ASR models in challenging acoustic environments.\n""]",Automatic Speech Recognition (ASR) Advances
30,29,223,29_privacy_privately_private_sgd,"['privacy', 'privately', 'private', 'sgd', 'unlearning', 'leakage', 'public', 'differentially', 'gradients', 'protection']","['privacy', 'private', 'differential', 'utility', 'guarantees', 'bounds', 'noise', 'mechanism', 'gradient', 'tight']","['privately', 'sgd', 'leakage', 'dpsgd', 'minimization', 'differential', 'ldp', 'auditing', 'risk', 'empirical']","['  Differentially private stochastic gradient descent (DP-SGD) is the standard\nalgorithm for training machine learning models under differential privacy (DP).\nThe major drawback of DP-SGD is the drop in utility which prior work has\ncomprehensively studied. However, in practice another major drawback that\nhinders the large-scale deployment is the significantly higher computational\ncost. We conduct a comprehensive empirical study to quantify the computational\ncost of training deep learning models under DP and benchmark methods that aim\nat reducing the cost. Among these are more efficient implementations of DP-SGD\nand training with lower precision. Finally, we study the scaling behaviour\nusing up to 80 GPUs.\n', '  Differentially Private Stochastic Gradient Descent (DP-SGD) is a popular\niterative algorithm used to train machine learning models while formally\nguaranteeing the privacy of users. However the privacy analysis of DP-SGD makes\nthe unrealistic assumption that all intermediate iterates (aka internal state)\nof the algorithm are released since in practice, only the final trained model,\ni.e., the final iterate of the algorithm is released. In this hidden state\nsetting, prior work has provided tighter analyses, albeit only when the loss\nfunction is constrained, e.g., strongly convex and smooth or linear. On the\nother hand, the privacy leakage observed empirically from hidden state DP-SGD,\neven when using non-convex loss functions suggest that there is in fact a gap\nbetween the theoretical privacy analysis and the privacy guarantees achieved in\npractice. Therefore, it remains an open question whether privacy amplification\nfor DP-SGD is possible in the hidden state setting for general loss functions.\n  Unfortunately, this work answers the aforementioned research question\nnegatively. By carefully constructing a loss function for DP-SGD, we show that\nfor specific loss functions, the final iterate of DP-SGD alone leaks as much\ninformation as the sequence of all iterates combined. Furthermore, we\nempirically verify this result by evaluating the privacy leakage from the final\niterate of DP-SGD with our loss function and show that this matches the\ntheoretical upper bound guaranteed by DP exactly. Therefore, we show that the\ncurrent privacy analysis fo DP-SGD is tight for general loss functions and\nconclude that no privacy amplification is possible for DP-SGD in general for\nall (possibly non-convex) loss functions.\n', ""  Machine learning (ML) models have been shown to leak private information from\ntheir training datasets. Differential Privacy (DP), typically implemented\nthrough the differential private stochastic gradient descent algorithm\n(DP-SGD), has become the standard solution to bound leakage from the models.\nDespite recent improvements, DP-SGD-based approaches for private learning still\nusually struggle in the high privacy ($\\varepsilon\\le1)$ and low data regimes,\nand when the private training datasets are imbalanced. To overcome these\nlimitations, we propose Differentially Private Prototype Learning (DPPL) as a\nnew paradigm for private transfer learning. DPPL leverages publicly pre-trained\nencoders to extract features from private data and generates DP prototypes that\nrepresent each private class in the embedding space and can be publicly\nreleased for inference. Since our DP prototypes can be obtained from only a few\nprivate training data points and without iterative noise addition, they offer\nhigh-utility predictions and strong privacy guarantees even under the notion of\npure DP. We additionally show that privacy-utility trade-offs can be further\nimproved when leveraging the public data beyond pre-training of the encoder: in\nparticular, we can privately sample our DP prototypes from the publicly\navailable data points used to train the encoder. Our experimental evaluation\nwith four state-of-the-art encoders, four vision datasets, and under different\ndata and imbalancedness regimes demonstrate DPPL's high performance under\nstrong privacy guarantees in challenging private learning setups.\n""]",Differential Privacy in Machine Learning
31,30,217,30_musicgen_music_midi_musical,"['musicgen', 'music', 'midi', 'musical', 'songs', 'musicrl', 'genres', 'musicians', 'artists', 'composers']","['music', 'musical', 'audio', 'melody', 'song', 'symbolic', 'pitch', 'sound', 'genre', 'songs']","['musicgen', 'midi', 'genres', 'composers', 'synthesizers', 'polyphonic', 'melody', 'generative', 'timbre', 'recordings']","['  Recent advances in text-to-music generation models have opened new avenues in\nmusical creativity. However, music generation usually involves iterative\nrefinements, and how to edit the generated music remains a significant\nchallenge. This paper introduces a novel approach to the editing of music\ngenerated by such models, enabling the modification of specific attributes,\nsuch as genre, mood and instrument, while maintaining other aspects unchanged.\nOur method transforms text editing to \\textit{latent space manipulation} while\nadding an extra constraint to enforce consistency. It seamlessly integrates\nwith existing pretrained text-to-music diffusion models without requiring\nadditional training. Experimental results demonstrate superior performance over\nboth zero-shot and certain supervised baselines in style and timbre transfer\nevaluations. Additionally, we showcase the practical applicability of our\napproach in real-world music editing scenarios.\n', ""  Symbolic Music, akin to language, can be encoded in discrete symbols. Recent\nresearch has extended the application of large language models (LLMs) such as\nGPT-4 and Llama2 to the symbolic music domain including understanding and\ngeneration. Yet scant research explores the details of how these LLMs perform\non advanced music understanding and conditioned generation, especially from the\nmulti-step reasoning perspective, which is a critical aspect in the\nconditioned, editable, and interactive human-computer co-creation process. This\nstudy conducts a thorough investigation of LLMs' capability and limitations in\nsymbolic music processing. We identify that current LLMs exhibit poor\nperformance in song-level multi-step music reasoning, and typically fail to\nleverage learned music knowledge when addressing complex musical tasks. An\nanalysis of LLMs' responses highlights distinctly their pros and cons. Our\nfindings suggest achieving advanced musical capability is not intrinsically\nobtained by LLMs, and future research should focus more on bridging the gap\nbetween music knowledge and reasoning, to improve the co-creation experience\nfor musicians.\n"", '  Controllable music generation plays a vital role in human-AI music\nco-creation. While Large Language Models (LLMs) have shown promise in\ngenerating high-quality music, their focus on autoregressive generation limits\ntheir utility in music editing tasks. To address this gap, we propose a novel\napproach leveraging a parameter-efficient heterogeneous adapter combined with a\nmasking training scheme. This approach enables autoregressive language models\nto seamlessly address music inpainting tasks. Additionally, our method\nintegrates frame-level content-based controls, facilitating track-conditioned\nmusic refinement and score-conditioned music arrangement. We apply this method\nto fine-tune MusicGen, a leading autoregressive music generation model. Our\nexperiments demonstrate promising results across multiple music editing tasks,\noffering more flexible controls for future AI-driven music editing tools. The\nsource codes and a demo page showcasing our work are available at\nhttps://kikyo-16.github.io/AIR.\n']",Music Generation and Editing
32,31,212,31_citations_citing_citation_cited,"['citations', 'citing', 'citation', 'cited', 'bibliometric', 'cite', 'scholarly', 'references', 'researchers', 'publications']","['citation', 'papers', 'scientific', 'citations', 'academic', 'publications', 'scholarly', 'literature', 'reviews', 'research']","['citations', 'bibliometric', 'scholarly', 'publications', 'corpus', 'abstracts', 'scientometrics', 'collaborations', 'surveys', 'metadata']","['  Due to the rapid pace of research publications, keeping up to date with all\nthe latest related papers is very time-consuming, even with daily feed tools.\nThere is a need for automatically generated, short, customized literature\nreviews of sets of papers to help researchers decide what to read. While\nseveral works in the last decade have addressed the task of explaining a single\nresearch paper, usually in the context of another paper citing it, the\nrelationship among multiple papers has been ignored; prior works have focused\non generating a single citation sentence in isolation, without addressing the\nexpository and transition sentences needed to connect multiple papers in a\ncoherent story. In this work, we explore a feature-based, LLM-prompting\napproach to generate richer citation texts, as well as generating multiple\ncitations at once to capture the complex relationships among research papers.\nWe perform an expert evaluation to investigate the impact of our proposed\nfeatures on the quality of the generated paragraphs and find a strong\ncorrelation between human preference and integrative writing style, suggesting\nthat humans prefer high-level, abstract citations, with transition sentences\nbetween them to provide an overall story.\n', '  Abstractive citation text generation is usually framed as an infilling task,\nwhere a sequence-to-sequence model is trained to generate a citation given a\nreference paper and the context window around the target; the generated\ncitation should be a brief discussion of the reference paper as it relates to\nthe citing context. However, examining a recent LED-based citation generation\nsystem, we find that many of the generated citations are generic summaries of\nthe reference papers main contribution, ignoring the citation contexts focus on\na different topic. To address this problem, we propose a simple modification to\nthe citation text generation task: the generation target is not only the\ncitation itself, but the entire context window, including the target citation.\nThis approach can be easily applied to any abstractive citation generation\nsystem, and our experimental results show that training in this way is\npreferred by human readers and allows the generation model to make use of\ncontextual clues about what topic to discuss and what stance to take.\n', '  In this contribution, we deal with seed-based information retrieval in\nnetworks of research publications. Using systematic reviews as a baseline, and\npublication data from the NIH Open Citation Collection, we compare the\nperformance of the three citation-based approaches direct citation,\nco-citation, and bibliographic coupling with respect to recall and precision\nmeasures. In addition, we include the PubMed Related Article score as well as\ncombined approaches in the comparison. We also provide a fairly comprehensive\nreview of earlier research in which citation relations have been used for\ninformation retrieval purposes. The results show an advantage for co-citation\nover bibliographic coupling and direct citation. However, combining the three\napproaches outperforms the exclusive use of co-citation in the study. The\nresults further indicate, in line with previous research, that combining\ncitation-based approaches with textual approaches enhances the performance of\nseed-based information retrieval. The results from the study may guide\napproaches combining citation-based and textual approaches in their choice of\ncitation similarity measures. We suggest that future research use more\nstructured approaches to evaluate methods for seed-based retrieval of\npublications, including comparative approaches as well as the elaboration of\ncommon data sets and baselines for evaluation.\n']",Citation Analysis and Generation in Research Publications
33,32,212,32_forecast_forecasts_forecasting_meteorological,"['forecast', 'forecasts', 'forecasting', 'meteorological', 'weather', 'climate', 'ensembles', 'prediction', 'predictions', 'precipitation']","['weather', 'climate', 'precipitation', 'forecasts', 'forecast', 'forecasting', 'resolution', 'downscaling', 'atmospheric', 'ensemble']","['forecasts', 'meteorological', 'ensembles', 'precipitation', 'cyclone', 'simulations', 'nwp', 'convection', 'radar', 'tropical']","['  General circulation models (GCMs) are the foundation of weather and climate\nprediction. GCMs are physics-based simulators which combine a numerical solver\nfor large-scale dynamics with tuned representations for small-scale processes\nsuch as cloud formation. Recently, machine learning (ML) models trained on\nreanalysis data achieved comparable or better skill than GCMs for deterministic\nweather forecasting. However, these models have not demonstrated improved\nensemble forecasts, or shown sufficient stability for long-term weather and\nclimate simulations. Here we present the first GCM that combines a\ndifferentiable solver for atmospheric dynamics with ML components, and show\nthat it can generate forecasts of deterministic weather, ensemble weather and\nclimate on par with the best ML and physics-based methods. NeuralGCM is\ncompetitive with ML models for 1-10 day forecasts, and with the European Centre\nfor Medium-Range Weather Forecasts ensemble prediction for 1-15 day forecasts.\nWith prescribed sea surface temperature, NeuralGCM can accurately track climate\nmetrics such as global mean temperature for multiple decades, and climate\nforecasts with 140 km resolution exhibit emergent phenomena such as realistic\nfrequency and trajectories of tropical cyclones. For both weather and climate,\nour approach offers orders of magnitude computational savings over conventional\nGCMs. Our results show that end-to-end deep learning is compatible with tasks\nperformed by conventional GCMs, and can enhance the large-scale physical\nsimulations that are essential for understanding and predicting the Earth\nsystem.\n', ""  Weather forecasts are fundamentally uncertain, so predicting the range of\nprobable weather scenarios is crucial for important decisions, from warning the\npublic about hazardous weather, to planning renewable energy use. Here, we\nintroduce GenCast, a probabilistic weather model with greater skill and speed\nthan the top operational medium-range weather forecast in the world, the\nEuropean Centre for Medium-Range Forecasts (ECMWF)'s ensemble forecast, ENS.\nUnlike traditional approaches, which are based on numerical weather prediction\n(NWP), GenCast is a machine learning weather prediction (MLWP) method, trained\non decades of reanalysis data. GenCast generates an ensemble of stochastic\n15-day global forecasts, at 12-hour steps and 0.25 degree latitude-longitude\nresolution, for over 80 surface and atmospheric variables, in 8 minutes. It has\ngreater skill than ENS on 97.4% of 1320 targets we evaluated, and better\npredicts extreme weather, tropical cyclones, and wind power production. This\nwork helps open the next chapter in operational weather forecasting, where\ncritical weather-dependent decisions are made with greater accuracy and\nefficiency.\n"", ""  Operational numerical weather prediction systems consist of three fundamental\ncomponents: the global observing system for data collection, data assimilation\nfor generating initial conditions, and the forecasting model to predict future\nweather conditions. While NWP have undergone a quiet revolution, with forecast\nskills progressively improving over the past few decades, their advancement has\nslowed due to challenges such as high computational costs and the complexities\nassociated with assimilating an increasing volume of observational data and\nmanaging finer spatial grids. Advances in machine learning offer an alternative\npath towards more efficient and accurate weather forecasts. The rise of machine\nlearning based weather forecasting models has also spurred the development of\nmachine learning based DA models or even purely machine learning based weather\nforecasting systems. This paper introduces FuXi Weather, an end-to-end machine\nlearning based weather forecasting system. FuXi Weather employs specialized\ndata preprocessing and multi-modal data fusion techniques to integrate\ninformation from diverse sources under all-sky conditions, including microwave\nsounders from 3 polar-orbiting satellites and radio occultation data from\nGlobal Navigation Satellite System. Operating on a 6-hourly DA and forecasting\ncycle, FuXi Weather independently generates robust and accurate 10-day global\nweather forecasts at a spatial resolution of 0.25\\textdegree. It surpasses the\nEuropean Centre for Medium-range Weather Forecasts high-resolution forecasts in\nterms of predictability, extending the skillful forecast lead times for several\nkey weather variables such as the geopotential height at 500 hPa from 9.25 days\nto 9.5 days. The system's high computational efficiency and robust performance,\neven with limited observations, demonstrates its potential as a promising\nalternative to traditional NWP systems.\n""]",Weather Forecasting and Climate Modeling
34,33,204,33_spiking_neuron_neurons_neuromorphic,"['spiking', 'neuron', 'neurons', 'neuromorphic', 'neural', 'spike', 'neuronal', 'spikes', 'snn', 'rsnns']","['spiking', 'neuromorphic', 'spike', 'neurons', 'event', 'energy', 'neuron', 'spikes', 'hardware', 'networks']","['spiking', 'neurons', 'neuromorphic', 'rsnns', 'synapses', 'backpropagation', 'ann', 'chip', 'spikformer', 'efficient']","['  Spiking neural networks (SNNs) provide an energy-efficient alternative to a\nvariety of artificial neural network (ANN) based AI applications. As the\nprogress in neuromorphic computing with SNNs expands their use in applications,\nthe problem of adversarial robustness of SNNs becomes more pronounced. To the\ncontrary of the widely explored end-to-end adversarial training based\nsolutions, we address the limited progress in scalable robust SNN training\nmethods by proposing an adversarially robust ANN-to-SNN conversion algorithm.\nOur method provides an efficient approach to embrace various computationally\ndemanding robust learning objectives that have been proposed for ANNs. During a\npost-conversion robust finetuning phase, our method adversarially optimizes\nboth layer-wise firing thresholds and synaptic connectivity weights of the SNN\nto maintain transferred robustness gains from the pre-trained ANN. We perform\nexperimental evaluations in a novel setting proposed to rigorously assess the\nrobustness of SNNs, where numerous adaptive adversarial attacks that account\nfor the spike-based operation dynamics are considered. Results show that our\napproach yields a scalable state-of-the-art solution for adversarially robust\ndeep SNNs with low-latency.\n', '  Brain-inspired Spiking Neural Networks (SNNs) have bio-plausibility and\nlow-power advantages over Artificial Neural Networks (ANNs). Applications of\nSNNs are currently limited to simple classification tasks because of their poor\nperformance. In this work, we focus on bridging the performance gap between\nANNs and SNNs on object detection. Our design revolves around network\narchitecture and spiking neuron. First, the overly complex module design causes\nspike degradation when the YOLO series is converted to the corresponding\nspiking version. We design a SpikeYOLO architecture to solve this problem by\nsimplifying the vanilla YOLO and incorporating meta SNN blocks. Second, object\ndetection is more sensitive to quantization errors in the conversion of\nmembrane potentials into binary spikes by spiking neurons. To address this\nchallenge, we design a new spiking neuron that activates Integer values during\ntraining while maintaining spike-driven by extending virtual timesteps during\ninference. The proposed method is validated on both static and neuromorphic\nobject detection datasets. On the static COCO dataset, we obtain 66.2% mAP@50\nand 48.9% mAP@50:95, which is +15.0% and +18.7% higher than the prior\nstate-of-the-art SNN, respectively. On the neuromorphic Gen1 dataset, we\nachieve 67.2% mAP@50, which is +2.5% greater than the ANN with equivalent\narchitecture, and the energy efficiency is improved by 5.7*. Code:\nhttps://github.com/BICLab/SpikeYOLO\n', '  Spiking neural networks (SNNs) have low power consumption and\nbio-interpretable characteristics, and are considered to have tremendous\npotential for energy-efficient computing. However, the exploration of SNNs on\nimage generation tasks remains very limited, and a unified and effective\nstructure for SNN-based generative models has yet to be proposed. In this\npaper, we explore a novel diffusion model architecture within spiking neural\nnetworks. We utilize transformer to replace the commonly used U-net structure\nin mainstream diffusion models. It can generate higher quality images with\nrelatively lower computational cost and shorter sampling time. It aims to\nprovide an empirical baseline for research of generative models based on SNNs.\nExperiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets demonstrate that our\nwork is highly competitive compared to existing SNN generative models.\n']",Spiking Neural Networks
35,34,202,34_optimization_optimisation_optimizing_optimizer,"['optimization', 'optimisation', 'optimizing', 'optimizer', 'pareto', 'optimal', 'metaheuristics', 'objectives', 'metaheuristic', 'objective']","['evolutionary', 'optimization', 'problems', 'objective', 'combinatorial', 'solutions', 'search', 'solver', 'integer', 'solvers']","['optimizer', 'pareto', 'metaheuristics', 'objectives', 'multiobjective', 'constraint', 'scheduling', 'mp', 'swarm', 'crossover']","['  Pareto set learning (PSL) is an emerging approach for acquiring the complete\nPareto set of a multi-objective optimization problem. Existing methods\nprimarily rely on the mapping of preference vectors in the objective space to\nPareto optimal solutions in the decision space. However, the sampling of\npreference vectors theoretically requires prior knowledge of the Pareto front\nshape to ensure high performance of the PSL methods. Designing a sampling\nstrategy of preference vectors is difficult since the Pareto front shape cannot\nbe known in advance. To make Pareto set learning work effectively in any Pareto\nfront shape, we propose a Pareto front shape-agnostic Pareto Set Learning\n(GPSL) that does not require the prior information about the Pareto front. The\nfundamental concept behind GPSL is to treat the learning of the Pareto set as a\ndistribution transformation problem. Specifically, GPSL can transform an\narbitrary distribution into the Pareto set distribution. We demonstrate that\ntraining a neural network by maximizing hypervolume enables the process of\ndistribution transformation. Our proposed method can handle any shape of the\nPareto front and learn the Pareto set without requiring prior knowledge.\nExperimental results show the high performance of our proposed method on\ndiverse test problems compared with recent Pareto set learning algorithms.\n', '  Real-world scenarios frequently involve multi-objective data-driven\noptimization problems, characterized by unknown problem coefficients and\nmultiple conflicting objectives. Traditional two-stage methods independently\napply a machine learning model to estimate problem coefficients, followed by\ninvoking a solver to tackle the predicted optimization problem. The independent\nuse of optimization solvers and prediction models may lead to suboptimal\nperformance due to mismatches between their objectives. Recent efforts have\nfocused on end-to-end training of predictive models that use decision loss\nderived from the downstream optimization problem. However, these methods have\nprimarily focused on single-objective optimization problems, thus limiting\ntheir applicability. We aim to propose a multi-objective decision-focused\napproach to address this gap. In order to better align with the inherent\nproperties of multi-objective optimization problems, we propose a set of novel\nloss functions. These loss functions are designed to capture the discrepancies\nbetween predicted and true decision problems, considering solution space,\nobjective space, and decision quality, named landscape loss, Pareto set loss,\nand decision loss, respectively. Our experimental results demonstrate that our\nproposed method significantly outperforms traditional two-stage methods and\nmost current decision-focused methods.\n', '  Multi-objective optimization problems can be found in many real-world\napplications, where the objectives often conflict each other and cannot be\noptimized by a single solution. In the past few decades, numerous methods have\nbeen proposed to find Pareto solutions that represent optimal trade-offs among\nthe objectives for a given problem. However, these existing methods could have\nhigh computational complexity or may not have good theoretical properties for\nsolving a general differentiable multi-objective optimization problem. In this\nwork, by leveraging the smooth optimization technique, we propose a lightweight\nand efficient smooth Tchebycheff scalarization approach for gradient-based\nmulti-objective optimization. It has good theoretical properties for finding\nall Pareto solutions with valid trade-off preferences, while enjoying\nsignificantly lower computational complexity compared to other methods.\nExperimental results on various real-world application problems fully\ndemonstrate the effectiveness of our proposed method.\n']",Multi-Objective Optimization Techniques
36,35,202,35_dialogue_dialogs_dialogues_dialog,"['dialogue', 'dialogs', 'dialogues', 'dialog', 'conversational', 'conversation', 'conversations', 'utterances', 'utterance', 'assistant']","['dialogue', 'dialogues', 'intent', 'conversational', 'slot', 'dialog', 'turn', 'conversations', 'conversation', 'responses']","['dialogues', 'conversational', 'utterance', 'annotation', 'assistants', 'prompting', 'datasets', 'intents', 'api', 'response']","['  Although there have been remarkable advances in dialogue systems through the\ndialogue systems technology competition (DSTC), it remains one of the key\nchallenges to building a robust task-oriented dialogue system with a speech\ninterface. Most of the progress has been made for text-based dialogue systems\nsince there are abundant datasets with written corpora while those with spoken\ndialogues are very scarce. However, as can be seen from voice assistant systems\nsuch as Siri and Alexa, it is of practical importance to transfer the success\nto spoken dialogues. In this paper, we describe our engineering effort in\nbuilding a highly successful model that participated in the speech-aware\ndialogue systems technology challenge track in DSTC11. Our model consists of\nthree major modules: (1) automatic speech recognition error correction to\nbridge the gap between the spoken and the text utterances, (2) text-based\ndialogue system (D3ST) for estimating the slots and values using slot\ndescriptions, and (3) post-processing for recovering the error of the estimated\nslot value. Our experiments show that it is important to use an explicit\nautomatic speech recognition error correction module, post-processing, and data\naugmentation to adapt a text-based dialogue state tracker for spoken dialogue\ncorpora.\n', '  Dialogue State Tracking (DST) is designed to monitor the evolving dialogue\nstate in the conversations and plays a pivotal role in developing task-oriented\ndialogue systems. However, obtaining the annotated data for the DST task is\nusually a costly endeavor. In this paper, we focus on employing LLMs to\ngenerate dialogue data to reduce dialogue collection and annotation costs.\nSpecifically, GPT-4 is used to simulate the user and agent interaction,\ngenerating thousands of dialogues annotated with DST labels. Then a two-stage\nfine-tuning on LLaMA 2 is performed on the generated data and the real data for\nthe DST prediction. Experimental results on two public DST benchmarks show that\nwith the generated dialogue data, our model performs better than the baseline\ntrained solely on real data. In addition, our approach is also capable of\nadapting to the dynamic demands in real-world scenarios, generating dialogues\nin new domains swiftly. After replacing dialogue segments in any domain with\nthe corresponding generated ones, the model achieves comparable performance to\nthe model trained on real data.\n', '  This survey provides a comprehensive review of research on multi-turn\ndialogue systems, with a particular focus on multi-turn dialogue systems based\non large language models (LLMs). This paper aims to (a) give a summary of\nexisting LLMs and approaches for adapting LLMs to downstream tasks; (b)\nelaborate recent advances in multi-turn dialogue systems, covering both\nLLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems,\nalong with datasets and evaluation metrics; (c) discuss some future emphasis\nand recent research problems arising from the development of LLMs and the\nincreasing demands on multi-turn dialogue systems.\n']",Dialogue Systems and Conversational AI
37,36,202,36_rnns_lstm_lstms_memory,"['rnns', 'lstm', 'lstms', 'memory', 'softmax', 'attention', 'rnn', 'neural', 'transformers', 'recurrent']","['transformers', 'attention', 'transformer', 'sequence', 'recurrent', 'linear', 'softmax', 'layer', 'sequences', 'layers']","['rnns', 'lstm', 'memory', 'softmax', 'transformer', 'ssms', 'tasks', 'sequences', 'depth', 'kernels']","['  Deep neural networks based on state space models (SSMs) are attracting much\nattention in sequence modeling since their computational cost is significantly\nsmaller than that of Transformers. While the capabilities of SSMs have been\nprimarily investigated through experimental comparisons, theoretical\nunderstanding of SSMs is still limited. In particular, there is a lack of\nstatistical and quantitative evaluation of whether SSM can replace\nTransformers. In this paper, we theoretically explore in which tasks SSMs can\nbe alternatives of Transformers from the perspective of estimating\nsequence-to-sequence functions. We consider the setting where the target\nfunction has direction-dependent smoothness and prove that SSMs can estimate\nsuch functions with the same convergence rate as Transformers. Additionally, we\nprove that SSMs can estimate the target function, even if the smoothness\nchanges depending on the input sequence, as well as Transformers. Our results\nshow the possibility that SSMs can replace Transformers when estimating the\nfunctions in certain classes that appear in practice.\n', ""  Selective state-space models (SSMs) like Mamba overcome some of the\nshortcomings of Transformers, such as quadratic computational complexity with\nsequence length and large inference-time memory requirements from the key-value\ncache. Moreover, recent studies have shown that SSMs can match or exceed the\nlanguage modeling capabilities of Transformers, making them an attractive\nalternative. In a controlled setting (e.g., same data), however, studies so far\nhave only presented small scale experiments comparing SSMs to Transformers. To\nunderstand the strengths and weaknesses of these architectures at larger\nscales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and\nTransformer models trained on the same datasets of up to 3.5T tokens. We also\ncompare these models to a hybrid architecture consisting of 43% Mamba-2, 7%\nattention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks,\nwe answer the question of whether Mamba models can match Transformers at larger\ntraining budgets. Our results show that while pure SSMs match or exceed\nTransformers on many tasks, they lag behind Transformers on tasks which require\nstrong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook)\nor long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid\nexceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points\non average) and is predicted to be up to 8x faster when generating tokens at\ninference time. To validate long-context capabilities, we provide additional\nexperiments evaluating variants of the Mamba-2-Hybrid and Transformer extended\nto support 16K, 32K, and 128K sequences. On an additional 23 long-context\ntasks, the hybrid model continues to closely match or exceed the Transformer on\naverage. To enable further study, we release the checkpoints as well as the\ncode used to train our models as part of NVIDIA's Megatron-LM project.\n"", ""  Foundation models, now powering most of the exciting applications in deep\nlearning, are almost universally based on the Transformer architecture and its\ncore attention module. Many subquadratic-time architectures such as linear\nattention, gated convolution and recurrent models, and structured state space\nmodels (SSMs) have been developed to address Transformers' computational\ninefficiency on long sequences, but they have not performed as well as\nattention on important modalities such as language. We identify that a key\nweakness of such models is their inability to perform content-based reasoning,\nand make several improvements. First, simply letting the SSM parameters be\nfunctions of the input addresses their weakness with discrete modalities,\nallowing the model to selectively propagate or forget information along the\nsequence length dimension depending on the current token. Second, even though\nthis change prevents the use of efficient convolutions, we design a\nhardware-aware parallel algorithm in recurrent mode. We integrate these\nselective SSMs into a simplified end-to-end neural network architecture without\nattention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$\nhigher throughput than Transformers) and linear scaling in sequence length, and\nits performance improves on real data up to million-length sequences. As a\ngeneral sequence model backbone, Mamba achieves state-of-the-art performance\nacross several modalities such as language, audio, and genomics. On language\nmodeling, our Mamba-3B model outperforms Transformers of the same size and\nmatches Transformers twice its size, both in pretraining and downstream\nevaluation.\n""]",State Space Models vs Transformers for Sequence Modeling
38,37,194,37_qcd_particles_lhc_collider,"['qcd', 'particles', 'lhc', 'collider', 'particle', 'quarks', 'quark', 'collisions', 'detectors', 'lhcb']","['particle', 'physics', 'detector', 'jet', 'collider', 'calorimeter', 'energy', 'particles', 'jets', 'events']","['qcd', 'lhc', 'particle', 'quarks', 'collisions', 'detectors', 'neutrinos', 'higgs', 'luminosity', 'events']","['  In modern collider experiments, the quest to explore fundamental interactions\nbetween elementary particles has reached unparalleled levels of precision.\nSignatures from particle physics detectors are low-level objects (such as\nenergy depositions or tracks) encoding the physics of collisions (the final\nstate particles of hard scattering interactions). The complete simulation of\nthem in a detector is a computational and storage-intensive task. To address\nthis computational bottleneck in particle physics, alternative approaches have\nbeen developed, introducing additional assumptions and trade off accuracy for\nspeed.The field has seen a surge in interest in surrogate modeling the detector\nsimulation, fueled by the advancements in deep generative models. These models\naim to generate responses that are statistically identical to the observed\ndata. In this paper, we conduct a comprehensive and exhaustive taxonomic review\nof the existing literature on the simulation of detector signatures from both\nmethodological and application-wise perspectives. Initially, we formulate the\nproblem of detector signature simulation and discuss its different variations\nthat can be unified. Next, we classify the state-of-the-art methods into five\ndistinct categories based on their underlying model architectures, summarizing\ntheir respective generation strategies. Finally, we shed light on the\nchallenges and opportunities that lie ahead in detector signature simulation,\nsetting the stage for future research and development.\n', '  The detection of out-of-distribution data points is a common task in particle\nphysics. It is used for monitoring complex particle detectors or for\nidentifying rare and unexpected events that may be indicative of new phenomena\nor physics beyond the Standard Model. Recent advances in Machine Learning for\nanomaly detection have encouraged the utilization of such techniques on\nparticle physics problems. This review article provides an overview of the\nstate-of-the-art techniques for anomaly detection in particle physics using\nmachine learning. We discuss the challenges associated with anomaly detection\nin large and complex data sets, such as those produced by high-energy particle\ncolliders, and highlight some of the successful applications of anomaly\ndetection in particle physics experiments.\n', '  Machine learning algorithms are heavily relied on to understand the vast\namounts of data from high-energy particle collisions at the CERN Large Hadron\nCollider (LHC). The data from such collision events can naturally be\nrepresented with graph structures. Therefore, deep geometric methods, such as\ngraph neural networks (GNNs), have been leveraged for various data analysis\ntasks in high-energy physics. One typical task is jet tagging, where jets are\nviewed as point clouds with distinct features and edge connections between\ntheir constituent particles. The increasing size and complexity of the LHC\nparticle datasets, as well as the computational models used for their analysis,\ngreatly motivate the development of alternative fast and efficient\ncomputational paradigms such as quantum computation. In addition, to enhance\nthe validity and robustness of deep networks, one can leverage the fundamental\nsymmetries present in the data through the use of invariant inputs and\nequivariant layers. In this paper, we perform a fair and comprehensive\ncomparison between classical graph neural networks (GNNs) and equivariant graph\nneural networks (EGNNs) and their quantum counterparts: quantum graph neural\nnetworks (QGNNs) and equivariant quantum graph neural networks (EQGNN). The\nfour architectures were benchmarked on a binary classification task to classify\nthe parton-level particle initiating the jet. Based on their AUC scores, the\nquantum networks were shown to outperform the classical networks. However,\nseeing the computational advantage of the quantum networks in practice may have\nto wait for the further development of quantum technology and its associated\nAPIs.\n']",Particle Physics and Collider Research
39,38,193,38_reinforcement_reward_rewards_learning,"['reinforcement', 'reward', 'rewards', 'learning', 'learned', 'planning', 'exploration', 'deepmind', 'learn', 'tasks']","['reinforcement', 'exploration', 'rewards', 'reward', 'environments', 'environment', 'agent', 'agents', 'policies', 'intrinsic']","['reinforcement', 'planning', 'deepmind', 'agent', 'skills', 'conditioned', 'rl', 'exploitation', 'goals', 'sparse']","['  Improving sample efficiency is central to Reinforcement Learning (RL),\nespecially in environments where the rewards are sparse. Some recent approaches\nhave proposed to specify reward functions as manually designed or learned\nreward structures whose integrations in the RL algorithms are claimed to\nsignificantly improve the learning efficiency. Manually designed reward\nstructures can suffer from inaccuracy and existing automatically learning\nmethods are often computationally intractable for complex tasks. The\nintegration of inaccurate or partial reward structures in RL algorithms fail to\nlearn optimal policies. In this work, we propose an RL algorithm that can\nautomatically structure the reward function for sample efficiency, given a set\nof labels that signify subtasks. Given such minimal knowledge about the task,\nwe train a high-level policy that selects optimal sub-tasks in each state\ntogether with a low-level policy that efficiently learns to complete each\nsub-task. We evaluate our algorithm in a variety of sparse-reward environments.\nThe experiment results show that our approach significantly outperforms the\nstate-of-art baselines as the difficulty of the task increases.\n', ""  Fast adaptation to new tasks is extremely important for embodied agents in\nthe real world. Meta-reinforcement learning (meta-RL) has emerged as an\neffective method to enable fast adaptation in unknown environments. Compared to\non-policy meta-RL algorithms, off-policy algorithms rely heavily on efficient\ndata sampling strategies to extract and represent the historical trajectories.\nHowever, little is known about how different data sampling methods impact the\nability of meta-RL agents to represent unknown environments. Here, we\ninvestigate the impact of data sampling strategies on the exploration and\nadaptability of meta-RL agents. Specifically, we conducted experiments with two\ntypes of off-policy meta-RL algorithms based on Thompson sampling and\nBayes-optimality theories in continuous control tasks within the MuJoCo\nenvironment and sparse reward navigation tasks. Our analysis revealed the\nlong-memory and short-memory sequence sampling strategies affect the\nrepresentation and adaptive capabilities of meta-RL agents. We found that the\nalgorithm based on Bayes-optimality theory exhibited more robust and better\nadaptability than the algorithm based on Thompson sampling, highlighting the\nimportance of appropriate data sampling strategies for the agent's\nrepresentation of an unknown environment, especially in the case of sparse\nrewards.\n"", '  Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as\npromising approaches for learning data-efficient RL algorithms tailored to a\ngiven task distribution. However, they show poor asymptotic performance and\nstruggle with out-of-distribution tasks because they rely on sequence models,\nsuch as recurrent neural networks or transformers, to process experiences\nrather than summarize them using general-purpose RL components such as value\nfunctions. In contrast, traditional RL algorithms are data-inefficient as they\ndo not use domain knowledge, but they do converge to an optimal policy in the\nlimit. We propose RL$^3$, a principled hybrid approach that incorporates\naction-values, learned per task through traditional RL, in the inputs to\nmeta-RL. We show that RL$^3$ earns greater cumulative reward in the long term,\ncompared to RL$^2$, while maintaining data-efficiency in the short term, and\ngeneralizes better to out-of-distribution tasks. Experiments are conducted on\nboth custom and benchmark discrete domains from the meta-RL literature that\nexhibit a range of short-term, long-term, and complex dependencies.\n']",Reinforcement Learning with Efficient Reward Structures
40,39,188,39_translations_translators_translator_translating,"['translations', 'translators', 'translator', 'translating', 'translation', 'translated', 'multilingual', 'translate', 'bilingual', 'lingual']","['translation', 'translations', 'parallel', 'languages', 'machine', 'sentence', 'resource', 'quality', 'pairs', 'translators']","['translations', 'monolingual', 'corpus', 'finetuning', 'performance', 'decoder', 'korean', 'resource', 'mnmt', 'levt']","['  Fine-tuning large language models (LLMs) for machine translation has shown\nimprovements in overall translation quality. However, it is unclear what is the\nimpact of fine-tuning on desirable LLM behaviors that are not present in neural\nmachine translation models, such as steerability, inherent document-level\ntranslation abilities, and the ability to produce less literal translations. We\nperform an extensive translation evaluation on the LLaMA and Falcon family of\nmodels with model size ranging from 7 billion up to 65 billion parameters. Our\nresults show that while fine-tuning improves the general translation quality of\nLLMs, several abilities degrade. In particular, we observe a decline in the\nability to perform formality steering, to produce technical translations\nthrough few-shot examples, and to perform document-level translation. On the\nother hand, we observe that the model produces less literal translations after\nfine-tuning on parallel data. We show that by including monolingual data as\npart of the fine-tuning data we can maintain the abilities while simultaneously\nenhancing overall translation quality. Our findings emphasize the need for\nfine-tuning strategies that preserve the benefits of LLMs for machine\ntranslation.\n', ""  Translation-tailored Large language models (LLMs) exhibit remarkable\ntranslation capabilities, even competing with supervised-trained commercial\ntranslation systems. However, off-target translation remains an unsolved\nproblem, especially for low-resource languages, hindering us from developing\naccurate LLMs-based translation models. To mitigate the off-target translation\nproblem and enhance the performance of LLMs on translation, recent works have\neither designed advanced prompting strategies to highlight the functionality of\ntranslation instructions or exploited the in-context learning ability of LLMs\nby feeding few-shot demonstrations. However, these methods essentially do not\nimprove LLM's ability to follow translation instructions, especially the\nlanguage direction information. In this work, we design a two-stage fine-tuning\nalgorithm to improve the instruction-following ability (especially the\ntranslation direction) of LLMs. Specifically, we first tune LLMs with the\nmaximum likelihood estimation loss on the translation dataset to elicit the\nbasic translation capabilities. In the second stage, we construct\ninstruction-conflicting samples by randomly replacing the translation\ndirections with a wrong one within the instruction, and then introduce an extra\nunlikelihood loss to learn those samples. Experiments on IWSLT and WMT\nbenchmarks upon the LLaMA model spanning 16 zero-shot directions show that,\ncompared to the competitive baseline -- translation-finetuned LLama, our method\ncould effectively reduce the off-target translation ratio (averagely -53.3\\%),\nthus improving translation quality with average +5.7 SacreBLEU and +16.4\nBLEURT. Analysis shows that our method could preserve the model's general task\nperformance on AlpacaEval. Code and models will be released at\n\\url{https://github.com/alphadl/LanguageAware_Tuning}.\n"", '  Machine translation (MT) encompasses a variety of methodologies aimed at\nenhancing the accuracy of translations. In contrast, the process of\nhuman-generated translation relies on a wide range of translation techniques,\nwhich are crucial for ensuring linguistic adequacy and fluency. This study\nsuggests that these translation techniques could further optimize machine\ntranslation if they are automatically identified before being applied to guide\nthe translation process effectively. The study differentiates between two\nscenarios of the translation process: from-scratch translation and\npost-editing. For each scenario, a specific set of experiments has been\ndesigned to forecast the most appropriate translation techniques. The findings\nindicate that the predictive accuracy for from-scratch translation reaches 82%,\nwhile the post-editing process exhibits even greater potential, achieving an\naccuracy rate of 93%.\n']",Machine Translation Evaluation and Optimization
41,40,185,40_explainability_explanations_interpretability_ai,"['explainability', 'explanations', 'interpretability', 'ai', 'explainers', 'classifiers', 'explaining', 'explainer', 'interpretable', 'predictive']","['explanations', 'explanation', 'attribution', 'explainable', 'explainability', 'interpretability', 'black', 'box', 'hoc', 'feature']","['explainability', 'interpretability', 'ai', 'explainers', 'classifiers', 'predictions', 'regression', 'trustworthy', 'xai', 'insights']","['  eXplainable Artificial Intelligence (XAI) aims at providing understandable\nexplanations of black box models. In this paper, we evaluate current XAI\nmethods by scoring them based on ground truth simulations and sensitivity\nanalysis. To this end, we used an Electric Arc Furnace (EAF) model to better\nunderstand the limits and robustness characteristics of XAI methods such as\nSHapley Additive exPlanations (SHAP), Local Interpretable Model-agnostic\nExplanations (LIME), as well as Averaged Local Effects (ALE) or Smooth\nGradients (SG) in a highly topical setting. These XAI methods were applied to\nvarious types of black-box models and then scored based on their correctness\ncompared to the ground-truth sensitivity of the data-generating processes using\na novel scoring evaluation methodology over a range of simulated additive\nnoise. The resulting evaluation shows that the capability of the Machine\nLearning (ML) models to capture the process accurately is, indeed, coupled with\nthe correctness of the explainability of the underlying data-generating\nprocess. We furthermore show the differences between XAI methods in their\nability to correctly predict the true sensitivity of the modeled industrial\nprocess.\n', ""  Artificial Intelligence (AI) is often an integral part of modern decision\nsupport systems. The best-performing predictive models used in AI-based\ndecision support systems lack transparency. Explainable Artificial Intelligence\n(XAI) aims to create AI systems that can explain their rationale to human\nusers. Local explanations in XAI can provide information about the causes of\nindividual predictions in terms of feature importance. However, a critical\ndrawback of existing local explanation methods is their inability to quantify\nthe uncertainty associated with a feature's importance. This paper introduces\nan extension of a feature importance explanation method, Calibrated\nExplanations, previously only supporting classification, with support for\nstandard regression and probabilistic regression, i.e., the probability that\nthe target is above an arbitrary threshold. The extension for regression keeps\nall the benefits of Calibrated Explanations, such as calibration of the\nprediction from the underlying model with confidence intervals, uncertainty\nquantification of feature importance, and allows both factual and\ncounterfactual explanations. Calibrated Explanations for standard regression\nprovides fast, reliable, stable, and robust explanations. Calibrated\nExplanations for probabilistic regression provides an entirely new way of\ncreating probabilistic explanations from any ordinary regression model,\nallowing dynamic selection of thresholds. The method is model agnostic with\neasily understood conditional rules. An implementation in Python is freely\navailable on GitHub and for installation using both pip and conda, making the\nresults in this paper easily replicable.\n"", '  Strategies based on Explainable Artificial Intelligence (XAI) have promoted\nbetter human interpretability of the results of black box models. This opens up\nthe possibility of questioning whether explanations created by XAI methods meet\nhuman expectations. The XAI methods being currently used (Ciu, Dalex, Eli5,\nLofo, Shap, and Skater) provide various forms of explanations, including global\nrankings of relevance of features, which allow for an overview of how the model\nis explained as a result of its inputs and outputs. These methods provide for\nan increase in the explainability of the model and a greater interpretability\ngrounded on the context of the problem. Intending to shed light on the\nexplanations generated by XAI methods and their interpretations, this research\naddresses a real-world classification problem related to homicide prediction,\nalready peer-validated, replicated its proposed black box model and used 6\ndifferent XAI methods to generate explanations and 6 different human experts.\nThe results were generated through calculations of correlations, comparative\nanalysis and identification of relationships between all ranks of features\nproduced. It was found that even though it is a model that is difficult to\nexplain, 75\\% of the expectations of human experts were met, with approximately\n48\\% agreement between results from XAI methods and human experts. The results\nallow for answering questions such as: ""Are the Expectation of Interpretation\ngenerated among different human experts similar?"", ""Do the different XAI\nmethods generate similar explanations for the proposed problem?"", ""Can\nexplanations generated by XAI methods meet human expectation of\nInterpretations?"", and ""Can Explanations and Expectations of Interpretation\nwork together?"".\n']",Explainable Artificial Intelligence (XAI)
42,41,182,41_summarizers_summarizer_summarizing_summarization,"['summarizers', 'summarizer', 'summarizing', 'summarization', 'summarisation', 'summaries', 'sentences', 'summary', 'paragraph', 'text']","['summarization', 'summaries', 'summary', 'abstractive', 'extractive', 'document', 'factual', 'summarisation', 'opinion', 'reviews']","['summarizers', 'text', 'informativeness', 'transcript', 'evaluation', 'extractive', 'reports', 'nli', 'discourse', 'salient']","['  Factual consistency is an important quality in dialogue summarization. Large\nlanguage model (LLM)-based automatic text summarization models generate more\nfactually consistent summaries compared to those by smaller pretrained language\nmodels, but they face deployment challenges in real-world applications due to\nprivacy or resource constraints. In this paper, we investigate the use of\nsymbolic knowledge distillation to improve the factual consistency of smaller\npretrained models for dialogue summarization. We employ zero-shot learning to\nextract symbolic knowledge from LLMs, generating both factually consistent\n(positive) and inconsistent (negative) summaries. We then apply two contrastive\nlearning objectives on these summaries to enhance smaller summarization models.\nExperiments with BART, PEGASUS, and Flan-T5 indicate that our approach\nsurpasses strong baselines that rely on complex data augmentation strategies.\nOur approach achieves better factual consistency while maintaining coherence,\nfluency, and relevance, as confirmed by various automatic evaluation metrics.\nWe also provide access to the data and code to facilitate future research.\n', ""  Recent studies have found that summaries generated by large language models\n(LLMs) are favored by human annotators over the original reference summaries in\ncommonly used summarization datasets. Therefore, we study an LLM-as-reference\nlearning setting for smaller text summarization models to investigate whether\ntheir performance can be substantially improved. To this end, we use LLMs as\nboth oracle summary generators for standard supervised fine-tuning and oracle\nsummary evaluators for efficient contrastive learning that leverages the LLMs'\nsupervision signals. We conduct comprehensive experiments with source news\narticles and find that (1) summarization models trained under the\nLLM-as-reference setting achieve significant performance improvement in both\nLLM and human evaluations; (2) contrastive learning outperforms standard\nsupervised fine-tuning under both low and high resource settings. Our\nexperimental results also enable a meta-analysis of LLMs' summary evaluation\ncapacities under a challenging setting, showing that LLMs are not well-aligned\nwith human evaluators. Particularly, our expert human evaluation reveals\nremaining nuanced performance gaps between LLMs and our fine-tuned models,\nwhich LLMs fail to capture. Thus, we call for further studies into both the\npotential and challenges of using LLMs in summarization model development.\n"", '  While large language models (LLMs) can already achieve strong performance on\nstandard generic summarization benchmarks, their performance on more complex\nsummarization task settings is less studied. Therefore, we benchmark LLMs on\ninstruction controllable text summarization, where the model input consists of\nboth a source article and a natural language requirement for desired summary\ncharacteristics. To this end, we curate an evaluation-only dataset for this\ntask setting and conduct human evaluations of five LLM-based systems to assess\ntheir instruction-following capabilities in controllable summarization. We then\nbenchmark LLM-based automatic evaluation for this task with 4 different\nevaluation protocols and 11 LLMs, resulting in 40 evaluation methods. Our study\nreveals that instruction controllable text summarization remains a challenging\ntask for LLMs, since (1) all LLMs evaluated still make factual and other types\nof errors in their summaries; (2) no LLM-based evaluation methods can achieve a\nstrong alignment with human annotators when judging the quality of candidate\nsummaries; (3) different LLMs show large performance gaps in summary generation\nand evaluation capabilities. We make our collected benchmark InstruSum publicly\navailable to facilitate future research in this direction.\n']",Text Summarization Models
43,42,177,42_offloading_vehicular_edge_congestion,"['offloading', 'vehicular', 'edge', 'congestion', 'scheduling', 'traffic', 'networks', 'vehicles', 'throughput', 'network']","['offloading', 'allocation', 'wireless', 'communication', 'edge', 'service', 'vehicular', 'network', 'latency', 'scheduling']","['offloading', 'vehicular', 'edge', 'congestion', 'iot', 'caching', 'qos', 'reinforcement', '6g', 'v2i']","['  Computational offloading has become an enabling component for edge\nintelligence in mobile and smart devices. Existing offloading schemes mainly\nfocus on mobile devices and servers, while ignoring the potential network\ncongestion caused by tasks from multiple mobile devices, especially in wireless\nmulti-hop networks. To fill this gap, we propose a low-overhead,\ncongestion-aware distributed task offloading scheme by augmenting a distributed\ngreedy framework with graph-based machine learning. In simulated wireless\nmulti-hop networks with 20-110 nodes and a resource allocation scheme based on\nshortest path routing and contention-based link scheduling, our approach is\ndemonstrated to be effective in reducing congestion or unstable queues under\nthe context-agnostic baseline, while improving the execution latency over local\ncomputing.\n', '  With the increasing demand for multiple applications on internet of vehicles.\nIt requires vehicles to carry out multiple computing tasks in real time.\nHowever, due to the insufficient computing capability of vehicles themselves,\noffloading tasks to vehicular edge computing (VEC) servers and allocating\ncomputing resources to tasks becomes a challenge. In this paper, a multi task\ndigital twin (DT) VEC network is established. By using DT to develop offloading\nstrategies and resource allocation strategies for multiple tasks of each\nvehicle in a single slot, an optimization problem is constructed. To solve it,\nwe propose a multi-agent reinforcement learning method on the task offloading\nand resource allocation. Numerous experiments demonstrate that our method is\neffective compared to other benchmark algorithms.\n', '  Vehicular edge computing (VEC) is an emerging technology that enables\nvehicles to perform high-intensity tasks by executing tasks locally or\noffloading them to nearby edge devices. However, obstacles such as buildings\nmay degrade the communications and incur communication interruptions, and thus\nthe vehicle may not meet the requirement for task offloading. Reconfigurable\nintelligent surfaces (RIS) is introduced to support vehicle communication and\nprovide an alternative communication path. The system performance can be\nimproved by flexibly adjusting the phase-shift of the RIS. For RIS-assisted VEC\nsystem where tasks arrive randomly, we design a control scheme that considers\noffloading power, local power allocation and phase-shift optimization. To solve\nthis non-convex problem, we propose a new deep reinforcement learning (DRL)\nframework that employs modified multi-agent deep deterministic policy gradient\n(MADDPG) approach to optimize the power allocation for vehicle users (VUs) and\nblock coordinate descent (BCD) algorithm to optimize the phase-shift of the\nRIS. Simulation results show that our proposed scheme outperforms the\ncentralized deep deterministic policy gradient (DDPG) scheme and random scheme.\n']",Vehicular Edge Computing and Task Offloading Optimization
44,43,174,43_safety_reinforcement_unsafe_safely,"['safety', 'reinforcement', 'unsafe', 'safely', 'safe', 'autonomous', 'control', 'learning', 'barrier', 'constraints']","['safety', 'safe', 'control', 'controllers', 'constraints', 'controller', 'reinforcement', 'policy', 'barrier', 'guarantees']","['safety', 'reinforcement', 'barrier', 'constraints', 'policies', 'avoidance', 'controllers', 'robot', 'rl', 'lagrangian']","[""  Recently, safe reinforcement learning (RL) with the actor-critic structure\nfor continuous control tasks has received increasing attention. It is still\nchallenging to learn a near-optimal control policy with safety and convergence\nguarantees. Also, few works have addressed the safe RL algorithm design under\ntime-varying safety constraints. This paper proposes a safe RL algorithm for\noptimal control of nonlinear systems with time-varying state and control\nconstraints. In the proposed approach, we construct a novel barrier force-based\ncontrol policy structure to guarantee control safety. A multi-step policy\nevaluation mechanism is proposed to predict the policy's safety risk under\ntime-varying safety constraints and guide the policy to update safely.\nTheoretical results on stability and robustness are proven. Also, the\nconvergence of the actor-critic implementation is analyzed. The performance of\nthe proposed algorithm outperforms several state-of-the-art RL algorithms in\nthe simulated Safety Gym environment. Furthermore, the approach is applied to\nthe integrated path following and collision avoidance problem for two\nreal-world intelligent vehicles. A differential-drive vehicle and an\nAckermann-drive one are used to verify offline deployment and online learning\nperformance, respectively. Our approach shows an impressive sim-to-real\ntransfer capability and a satisfactory online control performance in the\nexperiment.\n"", '  Deep reinforcement learning (DRL) has demonstrated remarkable performance in\nmany continuous control tasks. However, a significant obstacle to the\nreal-world application of DRL is the lack of safety guarantees. Although DRL\nagents can satisfy system safety in expectation through reward shaping,\ndesigning agents to consistently meet hard constraints (e.g., safety\nspecifications) at every time step remains a formidable challenge. In contrast,\nexisting work in the field of safe control provides guarantees on persistent\nsatisfaction of hard safety constraints. However, these methods require\nexplicit analytical system dynamics models to synthesize safe control, which\nare typically inaccessible in DRL settings. In this paper, we present a\nmodel-free safe control algorithm, the implicit safe set algorithm, for\nsynthesizing safeguards for DRL agents that ensure provable safety throughout\ntraining. The proposed algorithm synthesizes a safety index (barrier\ncertificate) and a subsequent safe control law solely by querying a black-box\ndynamic function (e.g., a digital twin simulator). Moreover, we theoretically\nprove that the implicit safe set algorithm guarantees finite time convergence\nto the safe set and forward invariance for both continuous-time and\ndiscrete-time systems. We validate the proposed algorithm on the\nstate-of-the-art Safety Gym benchmark, where it achieves zero safety violations\nwhile gaining $95\\% \\pm 9\\%$ cumulative reward compared to state-of-the-art\nsafe DRL methods. Furthermore, the resulting algorithm scales well to\nhigh-dimensional systems with parallel computing.\n', '  We develop provably safe and convergent reinforcement learning (RL)\nalgorithms for control of nonlinear dynamical systems, bridging the gap between\nthe hard safety guarantees of control theory and the convergence guarantees of\nRL theory. Recent advances at the intersection of control and RL follow a\ntwo-stage, safety filter approach to enforcing hard safety constraints:\nmodel-free RL is used to learn a potentially unsafe controller, whose actions\nare projected onto safe sets prescribed, for example, by a control barrier\nfunction. Though safe, such approaches lose any convergence guarantees enjoyed\nby the underlying RL methods. In this paper, we develop a single-stage,\nsampling-based approach to hard constraint satisfaction that learns RL\ncontrollers enjoying classical convergence guarantees while satisfying hard\nsafety constraints throughout training and deployment. We validate the efficacy\nof our approach in simulation, including safe control of a quadcopter in a\nchallenging obstacle avoidance problem, and demonstrate that it outperforms\nexisting benchmarks.\n']",Safe Reinforcement Learning for Autonomous Control Systems
45,44,173,44_finance_financial_textual_nlp,"['finance', 'financial', 'textual', 'nlp', 'text', 'lexicon', 'investor', 'forecasting', 'finllms', 'investment']","['financial', 'stock', 'market', 'sentiment', 'finance', 'news', 'investment', 'investors', 'companies', 'corporate']","['finance', 'textual', 'nlp', 'finllms', 'predicting', 'documents', 'traders', 'analyst', 'portfolio', 'reports']","[""  Natural language processing (NLP) has recently gained relevance within\nfinancial institutions by providing highly valuable insights into companies and\nmarkets' financial documents. However, the landscape of the financial domain\npresents extra challenges for NLP, due to the complexity of the texts and the\nuse of specific terminology. Generalist language models tend to fall short in\ntasks specifically tailored for finance, even when using large language models\n(LLMs) with great natural language understanding and generative capabilities.\nThis paper presents a study on LLM adaptation methods targeted at the financial\ndomain and with high emphasis on financial sentiment analysis. To this purpose,\ntwo foundation models with less than 1.5B parameters have been adapted using a\nwide range of strategies. We show that through careful fine-tuning on both\nfinancial documents and instructions, these foundation models can be adapted to\nthe target domain. Moreover, we observe that small LLMs have comparable\nperformance to larger scale models, while being more efficient in terms of\nparameters and data. In addition to the models, we show how to generate\nartificial instructions through LLMs to augment the number of samples of the\ninstruction dataset.\n"", '  The task of financial analysis primarily encompasses two key areas: stock\ntrend prediction and the corresponding financial question answering. Currently,\nmachine learning and deep learning algorithms (ML&DL) have been widely applied\nfor stock trend predictions, leading to significant progress. However, these\nmethods fail to provide reasons for predictions, lacking interpretability and\nreasoning processes. Also, they can not integrate textual information such as\nfinancial news or reports. Meanwhile, large language models (LLMs) have\nremarkable textual understanding and generation ability. But due to the\nscarcity of financial training datasets and limited integration with real-time\nknowledge, LLMs still suffer from hallucinations and are unable to keep up with\nthe latest information. To tackle these challenges, we first release AlphaFin\ndatasets, combining traditional research datasets, real-time financial data,\nand handwritten chain-of-thought (CoT) data. It has a positive impact on\ntraining LLMs for completing financial analysis. We then use AlphaFin datasets\nto benchmark a state-of-the-art method, called Stock-Chain, for effectively\ntackling the financial analysis task, which integrates retrieval-augmented\ngeneration (RAG) techniques. Extensive experiments are conducted to demonstrate\nthe effectiveness of our framework on financial analysis.\n', ""  In recent years, Large Language Models (LLMs) like ChatGPT have seen\nconsiderable advancements and have been applied in diverse fields. Built on the\nTransformer architecture, these models are trained on extensive datasets,\nenabling them to understand and generate human language effectively. In the\nfinancial domain, the deployment of LLMs is gaining momentum. These models are\nbeing utilized for automating financial report generation, forecasting market\ntrends, analyzing investor sentiment, and offering personalized financial\nadvice. Leveraging their natural language processing capabilities, LLMs can\ndistill key insights from vast financial data, aiding institutions in making\ninformed investment choices and enhancing both operational efficiency and\ncustomer satisfaction. In this study, we provide a comprehensive overview of\nthe emerging integration of LLMs into various financial tasks. Additionally, we\nconducted holistic tests on multiple financial tasks through the combination of\nnatural language instructions. Our findings show that GPT-4 effectively follow\nprompt instructions across various financial tasks. This survey and evaluation\nof LLMs in the financial domain aim to deepen the understanding of LLMs'\ncurrent role in finance for both financial practitioners and LLM researchers,\nidentify new research and application prospects, and highlight how these\ntechnologies can be leveraged to solve practical challenges in the finance\nindustry.\n""]",Financial Natural Language Processing
46,45,169,45_videos_captioning_captions_multimodal,"['videos', 'captioning', 'captions', 'multimodal', 'vid', 'clips', 'video', 'scenes', 'textual', 'dialogue']","['video', 'videos', 'frames', 'movie', 'summarization', 'visual', 'clips', 'temporal', 'frame', 'long']","['captions', 'multimodal', 'vid', 'clips', 'videollama', 'summarizer', 'transcripts', 'retrieval', 'dialog', 'vcmr']","['  Video-text Large Language Models (video-text LLMs) have shown remarkable\nperformance in answering questions and holding conversations on simple videos.\nHowever, they perform almost the same as random on grounding text queries in\nlong and complicated videos, having little ability to understand and reason\nabout temporal information, which is the most fundamental difference between\nvideos and images. In this paper, we propose HawkEye, one of the first\nvideo-text LLMs that can perform temporal video grounding in a fully\ntext-to-text manner. To collect training data that is applicable for temporal\nvideo grounding, we construct InternVid-G, a large-scale video-text corpus with\nsegment-level captions and negative spans, with which we introduce two new\ntime-aware training objectives to video-text LLMs. We also propose a\ncoarse-grained method of representing segments in videos, which is more robust\nand easier for LLMs to learn and follow than other alternatives. Extensive\nexperiments show that HawkEye is better at temporal video grounding and\ncomparable on other video-text tasks with existing video-text LLMs, which\nverifies its superior video-text multi-modal understanding abilities.\n', '  Recent advancements in image understanding have benefited from the extensive\nuse of web image-text pairs. However, video understanding remains a challenge\ndespite the availability of substantial web video-text data. This difficulty\nprimarily arises from the inherent complexity of videos and the inefficient\nlanguage supervision in recent web-collected video-text datasets. In this\npaper, we introduce Text-Only Pre-Alignment (TOPA), a novel approach to extend\nlarge language models (LLMs) for video understanding, without the need for\npre-training on real video data. Specifically, we first employ an advanced LLM\nto automatically generate Textual Videos comprising continuous textual frames,\nalong with corresponding annotations to simulate real video-text data. Then,\nthese annotated textual videos are used to pre-align a language-only LLM with\nthe video modality. To bridge the gap between textual and real videos, we\nemploy the CLIP model as the feature extractor to align image and text\nmodalities. During text-only pre-alignment, the continuous textual frames,\nencoded as a sequence of CLIP text features, are analogous to continuous CLIP\nimage features, thus aligning the LLM with real video representation. Extensive\nexperiments, including zero-shot evaluation and finetuning on various video\nunderstanding tasks, demonstrate that TOPA is an effective and efficient\nframework for aligning video content with LLMs. In particular, without training\non any video data, the TOPA-Llama2-13B model achieves a Top-1 accuracy of 51.0%\non the challenging long-form video understanding benchmark, Egoschema. This\nperformance surpasses previous video-text pre-training approaches and proves\ncompetitive with recent GPT-3.5-based video agents.\n', ""  A more robust and holistic language-video representation is the key to\npushing video understanding forward. Despite the improvement in training\nstrategies, the quality of the language-video dataset is less attention to. The\ncurrent plain and simple text descriptions and the visual-only focus for the\nlanguage-video tasks result in a limited capacity in real-world natural\nlanguage video retrieval tasks where queries are much more complex. This paper\nintroduces a method to automatically enhance video-language datasets, making\nthem more modality and context-aware for more sophisticated representation\nlearning needs, hence helping all downstream tasks. Our multifaceted video\ncaptioning method captures entities, actions, speech transcripts, aesthetics,\nand emotional cues, providing detailed and correlating information from the\ntext side to the video side for training. We also develop an agent-like\nstrategy using language models to generate high-quality, factual textual\ndescriptions, reducing human intervention and enabling scalability. The\nmethod's effectiveness in improving language-video representation is evaluated\nthrough text-video retrieval using the MSR-VTT dataset and several multi-modal\nretrieval models.\n""]",Video Multimodal Understanding and Captioning
47,46,164,46_cropland_imagery_land_satellite,"['cropland', 'imagery', 'land', 'satellite', 'crop', 'vegetation', 'cnn', 'earth', 'sensing', 'dataset']","['satellite', 'crop', 'imagery', 'land', 'remote', 'soil', 'sensing', 'cover', 'vegetation', 'segmentation']","['cropland', 'imagery', 'satellite', 'vegetation', 'sensing', 'dataset', 'sentinel', 'climate', 'multispectral', 'geospatial']","[""  Earth observation (EO) satellite missions have been providing detailed images\nabout the state of the Earth and its land cover for over 50 years. Long term\nmissions, such as NASA's Landsat, Terra, and Aqua satellites, and more\nrecently, the ESA's Sentinel missions, record images of the entire world every\nfew days. Although single images provide point-in-time data, repeated images of\nthe same area, or satellite image time series (SITS) provide information about\nthe changing state of vegetation and land use. These SITS are useful for\nmodeling dynamic processes and seasonal changes such as plant phenology. They\nhave potential benefits for many aspects of land and natural resource\nmanagement, including applications in agricultural, forest, water, and disaster\nmanagement, urban planning, and mining. However, the resulting satellite image\ntime series (SITS) are complex, incorporating information from the temporal,\nspatial, and spectral dimensions. Therefore, deep learning methods are often\ndeployed as they can analyze these complex relationships. This review presents\na summary of the state-of-the-art methods of modelling environmental,\nagricultural, and other Earth observation variables from SITS data using deep\nlearning methods. We aim to provide a resource for remote sensing experts\ninterested in using deep learning techniques to enhance Earth observation\nmodels with temporal information.\n"", ""  Satellites equipped with optical sensors capture high-resolution imagery,\nproviding valuable insights into various environmental phenomena. In recent\nyears, there has been a surge of research focused on addressing some challenges\nin remote sensing, ranging from water detection in diverse landscapes to the\nsegmentation of mountainous and terrains. Ongoing investigations goals to\nenhance the precision and efficiency of satellite imagery analysis. Especially,\nthere is a growing emphasis on developing methodologies for accurate water body\ndetection, snow and clouds, important for environmental monitoring, resource\nmanagement, and disaster response. Within this context, this paper focus on the\ncloud segmentation from remote sensing imagery. Accurate remote sensing data\nanalysis can be challenging due to the presence of clouds in optical\nsensor-based applications. The quality of resulting products such as\napplications and research is directly impacted by cloud detection, which plays\na key role in the remote sensing data processing pipeline. This paper examines\nseven cutting-edge semantic segmentation and detection algorithms applied to\nclouds identification, conducting a benchmark analysis to evaluate their\narchitectural approaches and identify the most performing ones. To increase the\nmodel's adaptability, critical elements including the type of imagery and the\namount of spectral bands used during training are analyzed. Additionally, this\nresearch tries to produce machine learning algorithms that can perform cloud\nsegmentation using only a few spectral bands, including RGB and RGBN-IR\ncombinations. The model's flexibility for a variety of applications and user\nscenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as\ndatasets. This benchmark can be reproduced using the material from this github\nlink: https://github.com/toelt-llc/cloud_segmentation_comparative.\n"", '  We introduce a simple yet effective early fusion method for crop yield\nprediction that handles multiple input modalities with different temporal and\nspatial resolutions. We use high-resolution crop yield maps as ground truth\ndata to train crop and machine learning model agnostic methods at the sub-field\nlevel. We use Sentinel-2 satellite imagery as the primary modality for input\ndata with other complementary modalities, including weather, soil, and DEM\ndata. The proposed method uses input modalities available with global coverage,\nmaking the framework globally scalable. We explicitly highlight the importance\nof input modalities for crop yield prediction and emphasize that the\nbest-performing combination of input modalities depends on region, crop, and\nchosen model.\n']",Satellite Imagery for Land and Crop Monitoring
48,47,160,47_courts_judicial_court_legalsemi,"['courts', 'judicial', 'court', 'legalsemi', 'lawyers', 'lawyer', 'law', 'legalai', 'retrieval', 'jurisdiction']","['legal', 'court', 'case', 'law', 'documents', 'judicial', 'cases', 'lawyers', 'judgments', 'retrieval']","['courts', 'legalai', 'jurisdictions', 'precedents', 'documents', 'corpus', 'judgments', 'relevance', 'casegnn', 'annotated']","['  Legal case retrieval aims to help legal workers find relevant cases related\nto their cases at hand, which is important for the guarantee of fairness and\njustice in legal judgments. While recent advances in neural retrieval methods\nhave significantly improved the performance of open-domain retrieval tasks\n(e.g., Web search), their advantages have not been observed in legal case\nretrieval due to their thirst for annotated data. As annotating large-scale\ntraining data in legal domains is prohibitive due to the need for domain\nexpertise, traditional search techniques based on lexical matching such as\nTF-IDF, BM25, and Query Likelihood are still prevalent in legal case retrieval\nsystems. While previous studies have designed several pre-training methods for\nIR models in open-domain tasks, these methods are usually suboptimal in legal\ncase retrieval because they cannot understand and capture the key knowledge and\ndata structures in the legal corpus. To this end, we propose a novel\npre-training framework named Caseformer that enables the pre-trained models to\nlearn legal knowledge and domain-specific relevance information in legal case\nretrieval without any human-labeled data. Through three unsupervised learning\ntasks, Caseformer is able to capture the special language, document structure,\nand relevance patterns of legal case documents, making it a strong backbone for\ndownstream legal case retrieval tasks. Experimental results show that our model\nhas achieved state-of-the-art performance in both zero-shot and full-data\nfine-tuning settings. Also, experiments on both Chinese and English legal\ndatasets demonstrate that the effectiveness of Caseformer is\nlanguage-independent in legal case retrieval.\n', '  General and legal domain LLMs have demonstrated strong performance in various\ntasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are\ndefined by the experts of computer science, lacking consistency with the logic\nof legal practice, making it difficult to judge their practical capabilities.\nTo address this challenge, we are the first to build the Chinese legal LLMs\nbenchmark LAiW, based on the logic of legal practice. To align with the\nthinking process of legal experts and legal practice (syllogism), we divide the\nlegal capabilities of LLMs from easy to difficult into three levels: basic\ninformation retrieval, legal foundation inference, and complex legal\napplication. Each level contains multiple tasks to ensure a comprehensive\nevaluation. Through automated evaluation of current general and legal domain\nLLMs on our benchmark, we indicate that these LLMs may not align with the logic\nof legal practice. LLMs seem to be able to directly acquire complex legal\napplication capabilities but perform poorly in some basic tasks, which may pose\nobstacles to their practical application and acceptance by legal experts. To\nfurther confirm the complex legal application capabilities of current LLMs in\nlegal application scenarios, we also incorporate human evaluation with legal\nexperts. The results indicate that while LLMs may demonstrate strong\nperformance, they still require reinforcement of legal logic.\n', '  Recent advances in Large Language Models (LLMs) have significantly shaped the\napplications of AI in multiple fields, including the studies of legal\nintelligence. Trained on extensive legal texts, including statutes and legal\ndocuments, the legal LLMs can capture important legal knowledge/concepts\neffectively and provide important support for downstream legal applications\nsuch as legal consultancy. Yet, the dynamic nature of legal statutes and\ninterpretations also poses new challenges to the use of LLMs in legal\napplications. Particularly, how to update the legal knowledge of LLMs\neffectively and efficiently has become an important research problem in\npractice. Existing benchmarks for evaluating knowledge update methods are\nmostly designed for the open domain and cannot address the specific challenges\nof the legal domain, such as the nuanced application of new legal knowledge,\nthe complexity and lengthiness of legal regulations, and the intricate nature\nof legal reasoning. To address this gap, we introduce the Legal Knowledge\nUpdate BEnchmark, i.e. LeKUBE, which evaluates knowledge update methods for\nlegal LLMs across five dimensions. Specifically, we categorize the needs of\nknowledge updates in the legal domain with the help of legal professionals, and\nthen hire annotators from law schools to create synthetic updates to the\nChinese Criminal and Civil Code as well as sets of questions of which the\nanswers would change after the updates. Through a comprehensive evaluation of\nstate-of-the-art knowledge update methods, we reveal a notable gap between\nexisting knowledge update methods and the unique needs of the legal domain,\nemphasizing the need for further research and development of knowledge update\nmechanisms tailored for legal LLMs.\n']",Legal Information Retrieval and AI Applications
49,48,155,48_bias_biases_genders_gender,"['bias', 'biases', 'genders', 'gender', 'gendered', 'biased', 'stereotypes', 'languages', 'language', 'demographics']","['gender', 'bias', 'biases', 'stereotypes', 'fairness', 'stereotypical', 'stereotype', 'social', 'names', 'groups']","['biases', 'demographics', 'discriminatory', 'stereotype', 'masculine', 'nlp', 'pronouns', 'dialect', 'hiring', 'hispanic']","['  Large Language Models (LLMs) can generate biased responses. Yet previous\ndirect probing techniques contain either gender mentions or predefined gender\nstereotypes, which are challenging to comprehensively collect. Hence, we\npropose an indirect probing framework based on conditional generation. This\napproach aims to induce LLMs to disclose their gender bias even without\nexplicit gender or stereotype mentions. We explore three distinct strategies to\ndisclose explicit and implicit gender bias in LLMs. Our experiments demonstrate\nthat all tested LLMs exhibit explicit and/or implicit gender bias, even when\ngender stereotypes are not present in the inputs. In addition, an increased\nmodel size or model alignment amplifies bias in most cases. Furthermore, we\ninvestigate three methods to mitigate bias in LLMs via Hyperparameter Tuning,\nInstruction Guiding, and Debias Tuning. Remarkably, these methods prove\neffective even in the absence of explicit genders or stereotypes.\n', ""  Gender bias research has been pivotal in revealing undesirable behaviors in\nlarge language models, exposing serious gender stereotypes associated with\noccupations, and emotions. A key observation in prior work is that models\nreinforce stereotypes as a consequence of the gendered correlations that are\npresent in the training data. In this paper, we focus on bias where the effect\nfrom training data is unclear, and instead address the question: Do language\nmodels still exhibit gender bias in non-stereotypical settings? To do so, we\nintroduce UnStereoEval (USE), a novel framework tailored for investigating\ngender bias in stereotype-free scenarios. USE defines a sentence-level score\nbased on pretraining data statistics to determine if the sentence contain\nminimal word-gender associations. To systematically benchmark the fairness of\npopular language models in stereotype-free scenarios, we utilize USE to\nautomatically generate benchmarks without any gender-related language. By\nleveraging USE's sentence-level score, we also repurpose prior gender bias\nbenchmarks (Winobias and Winogender) for non-stereotypical evaluation.\nSurprisingly, we find low fairness across all 28 tested models. Concretely,\nmodels demonstrate fair behavior in only 9%-41% of stereotype-free sentences,\nsuggesting that bias does not solely stem from the presence of gender-related\nwords. These results raise important questions about where underlying model\nbiases come from and highlight the need for more systematic and comprehensive\nbias evaluation. We release the full dataset and code at\nhttps://ucinlp.github.io/unstereo-eval.\n"", '  With the growing deployment of large language models (LLMs) across various\napplications, assessing the influence of gender biases embedded in LLMs becomes\ncrucial. The topic of gender bias within the realm of natural language\nprocessing (NLP) has gained considerable focus, particularly in the context of\nEnglish. Nonetheless, the investigation of gender bias in languages other than\nEnglish is still relatively under-explored and insufficiently analyzed. In this\nwork, We examine gender bias in LLMs-generated outputs for different languages.\nWe use three measurements: 1) gender bias in selecting descriptive words given\nthe gender-related context. 2) gender bias in selecting gender-related pronouns\n(she/he) given the descriptive words. 3) gender bias in the topics of\nLLM-generated dialogues. We investigate the outputs of the GPT series of LLMs\nin various languages using our three measurement methods. Our findings revealed\nsignificant gender biases across all the languages we examined.\n']",Gender Bias in Language Models
50,49,154,49_electroencephalogram_electroencephalographic_eeg_electroencephalography,"['electroencephalogram', 'electroencephalographic', 'eeg', 'electroencephalography', 'eeg2rep', 'bci', 'neural', 'bcis', 'evoked', 'electrode']","['brain', 'seizure', 'signals', 'electroencephalography', 'subject', 'decoding', 'subjects', 'electroencephalogram', 'motor', 'computer']","['electroencephalogram', 'eeg2rep', 'bcis', 'evoked', 'electrodes', 'recordings', 'eegnet', 'classifier', 'seizures', 'decode']","['  Electroencephalogram (EEG) is a non-invasive technique to record\nbioelectrical signals. Integrating supervised deep learning techniques with EEG\nsignals has recently facilitated automatic analysis across diverse EEG-based\ntasks. However, the label issues of EEG signals have constrained the\ndevelopment of EEG-based deep models. Obtaining EEG annotations is difficult\nthat requires domain experts to guide collection and labeling, and the\nvariability of EEG signals among different subjects causes significant label\nshifts. To solve the above challenges, self-supervised learning (SSL) has been\nproposed to extract representations from unlabeled samples through\nwell-designed pretext tasks. This paper concentrates on integrating SSL\nframeworks with temporal EEG signals to achieve efficient representation and\nproposes a systematic review of the SSL for EEG signals. In this paper, 1) we\nintroduce the concept and theory of self-supervised learning and typical SSL\nframeworks. 2) We provide a comprehensive review of SSL for EEG analysis,\nincluding taxonomy, methodology, and technique details of the existing\nEEG-based SSL frameworks, and discuss the difference between these methods. 3)\nWe investigate the adaptation of the SSL approach to various downstream tasks,\nincluding the task description and related benchmark datasets. 4) Finally, we\ndiscuss the potential directions for future SSL-EEG research.\n', ""  Decoding linguistic information from non-invasive brain signals using EEG has\ngained increasing research attention due to its vast applicational potential.\nRecently, a number of works have adopted a generative-based framework to decode\nelectroencephalogram (EEG) signals into sentences by utilizing the power\ngenerative capacity of pretrained large language models (LLMs). However, this\napproach has several drawbacks that hinder the further development of\nlinguistic applications for brain-computer interfaces (BCIs). Specifically, the\nability of the EEG encoder to learn semantic information from EEG data remains\nquestionable, and the LLM decoder's tendency to generate sentences based on its\ntraining memory can be hard to avoid. These issues necessitate a novel approach\nfor converting EEG signals into sentences. In this paper, we propose a novel\ntwo-step pipeline that addresses these limitations and enhances the validity of\nlinguistic EEG decoding research. We first confirm that word-level semantic\ninformation can be learned from EEG data recorded during natural reading by\ntraining a Conformer encoder via a masked contrastive objective for word-level\nclassification. To achieve sentence decoding results, we employ a training-free\nretrieval method to retrieve sentences based on the predictions from the EEG\nencoder. Extensive experiments and ablation studies were conducted in this\npaper for a comprehensive evaluation of the proposed approach. Visualization of\nthe top prediction candidates reveals that our model effectively groups EEG\nsegments into semantic categories with similar meanings, thereby validating its\nability to learn patterns from unspoken EEG recordings. Despite the exploratory\nnature of this work, these results suggest that our method holds promise for\nproviding more reliable solutions for converting EEG signals into text.\n"", '  In recent years, with the development of deep learning, electroencephalogram\n(EEG) classification networks have achieved certain progress. Transformer-based\nmodels can perform well in capturing long-term dependencies in EEG signals.\nHowever, their quadratic computational complexity leads to significant\ncomputational overhead. Moreover, most EEG classification models are only\nsuitable for single tasks, showing poor generalization capabilities across\ndifferent tasks and further unable to handle EEG data from various tasks\nsimultaneously due to variations in signal length and the number of channels.\nIn this paper, we introduce a universal EEG classification network named\nEEGMamba, which seamlessly integrates the Spatio-Temporal-Adaptive\n(ST-Adaptive) module, Bidirectional Mamba, and Mixture of Experts (MoE) into a\nunified framework for multiple tasks. The proposed ST-Adaptive module performs\nunified feature extraction on EEG signals of different lengths and channel\ncounts through spatio-adaptive convolution and incorporates a class token to\nachieve temporal-adaptability. Moreover, we design a bidirectional Mamba\nparticularly suitable for EEG signals for further feature extraction, balancing\nhigh accuracy and fast inference speed in processing long EEG signals. In order\nto better process EEG data for different tasks, we introduce Task-aware MoE\nwith a universal expert, achieving the capture of both differences and\ncommonalities between EEG data from different tasks. We test our model on eight\npublicly available EEG datasets, and experimental results demonstrate its\nsuperior performance in four types of tasks: seizure detection, emotion\nrecognition, sleep stage classification, and motor imagery. The code is set to\nbe released soon.\n']",Electroencephalogram Analysis and Applications
51,50,150,50_multilinguality_multilingual_crosslingual_lingual,"['multilinguality', 'multilingual', 'crosslingual', 'lingual', 'languages', 'bilingual', 'monolingual', 'language', 'translations', 'linguistic']","['languages', 'lingual', 'multilingual', 'cross', 'transfer', 'resource', 'language', 'pretraining', 'instruction', 'monolingual']","['multilinguality', 'translations', 'm2lingual', 'pretraining', 'creole', 'vocabulary', 'cross', 'syntactic', 'embeddings', 'chinese']","['  Despite their strong ability to retrieve knowledge in English, current large\nlanguage models show imbalance abilities in different languages. Two approaches\nare proposed to address this, i.e., multilingual pretraining and multilingual\ninstruction tuning. However, whether and how do such methods contribute to the\ncross-lingual knowledge alignment inside the models is unknown. In this paper,\nwe propose CLiKA, a systematic framework to assess the cross-lingual knowledge\nalignment of LLMs in the Performance, Consistency and Conductivity levels, and\nexplored the effect of multilingual pretraining and instruction tuning on the\ndegree of alignment. Results show that: while both multilingual pretraining and\ninstruction tuning are beneficial for cross-lingual knowledge alignment, the\ntraining strategy needs to be carefully designed. Namely, continued pretraining\nimproves the alignment of the target language at the cost of other languages,\nwhile mixed pretraining affect other languages less. Also, the overall\ncross-lingual knowledge alignment, especially in the conductivity level, is\nunsatisfactory for all tested LLMs, and neither multilingual pretraining nor\ninstruction tuning can substantially improve the cross-lingual knowledge\nconductivity.\n', '  Recent advances in training multilingual language models on large datasets\nseem to have shown promising results in knowledge transfer across languages and\nachieve high performance on downstream tasks. However, we question to what\nextent the current evaluation benchmarks and setups accurately measure\nzero-shot cross-lingual knowledge transfer. In this work, we challenge the\nassumption that high zero-shot performance on target tasks reflects high\ncross-lingual ability by introducing more challenging setups involving\ninstances with multiple languages. Through extensive experiments and analysis,\nwe show that the observed high performance of multilingual models can be\nlargely attributed to factors not requiring the transfer of actual linguistic\nknowledge, such as task- and surface-level knowledge. More specifically, we\nobserve what has been transferred across languages is mostly data artifacts and\nbiases, especially for low-resource languages. Our findings highlight the\noverlooked drawbacks of existing cross-lingual test data and evaluation setups,\ncalling for a more nuanced understanding of the cross-lingual capabilities of\nmultilingual models.\n', '  As instruction-tuned large language models (LLMs) gain global adoption, their\nability to follow instructions in multiple languages becomes increasingly\ncrucial. In this work, we investigate how multilinguality during instruction\ntuning of a multilingual LLM affects instruction-following across languages\nfrom the pre-training corpus. We first show that many languages transfer some\ninstruction-following capabilities to other languages from even monolingual\ntuning. Furthermore, we find that only 40 multilingual examples integrated in\nan English tuning set substantially improve multilingual instruction-following,\nboth in seen and unseen languages during tuning. In general, we observe that\nmodels tuned on multilingual mixtures exhibit comparable or superior\nperformance in multiple languages compared to monolingually tuned models,\ndespite training on 10x fewer examples in those languages. Finally, we find\nthat diversifying the instruction tuning set with even just 2-4 languages\nsignificantly improves cross-lingual generalization. Our results suggest that\nbuilding massively multilingual instruction-tuned models can be done with only\na very small set of multilingual instruction-responses.\n']",Multilingual Language Models and Cross-Lingual Knowledge Transfer
52,51,149,51_denoising_denoiser_restoration_diffusion,"['denoising', 'denoiser', 'restoration', 'diffusion', 'imagenet', 'generative', 'rectified', 'images', 'quantization', 'models']","['diffusion', 'denoising', 'sampling', 'noise', 'restoration', 'steps', 'image', 'generative', 'quality', 'timesteps']","['denoising', 'imagenet', 'generative', 'quantization', 'degraded', 'sampler', 'resolution', 'dit', 'ddpm', 'likelihood']","['  Diffusion models have demonstrated remarkable efficacy in generating\nhigh-quality samples. Existing diffusion-based image restoration algorithms\nexploit pre-trained diffusion models to leverage data priors, yet they still\npreserve elements inherited from the unconditional generation paradigm. These\nstrategies initiate the denoising process with pure white noise and incorporate\nrandom noise at each generative step, leading to over-smoothed results. In this\npaper, we present a refined paradigm for diffusion-based image restoration.\nSpecifically, we opt for a sample consistent with the measurement identity at\neach generative step, exploiting the sampling selection as an avenue for output\nstability and enhancement. The number of candidate samples used for selection\nis adaptively determined based on the signal-to-noise ratio of the timestep.\nAdditionally, we start the restoration process with an initialization combined\nwith the measurement signal, providing supplementary information to better\nalign the generative process. Extensive experimental results and analyses\nvalidate that our proposed method significantly enhances image restoration\nperformance while consuming negligible additional computational resources.\n', '  Deep generative models have garnered significant attention in low-level\nvision tasks due to their generative capabilities. Among them, diffusion\nmodel-based solutions, characterized by a forward diffusion process and a\nreverse denoising process, have emerged as widely acclaimed for their ability\nto produce samples of superior quality and diversity. This ensures the\ngeneration of visually compelling results with intricate texture information.\nDespite their remarkable success, a noticeable gap exists in a comprehensive\nsurvey that amalgamates these pioneering diffusion model-based works and\norganizes the corresponding threads. This paper proposes the comprehensive\nreview of diffusion model-based techniques. We present three generic diffusion\nmodeling frameworks and explore their correlations with other deep generative\nmodels, establishing the theoretical foundation. Following this, we introduce a\nmulti-perspective categorization of diffusion models, considering both the\nunderlying framework and the target task. Additionally, we summarize extended\ndiffusion models applied in other tasks, including medical, remote sensing, and\nvideo scenarios. Moreover, we provide an overview of commonly used benchmarks\nand evaluation metrics. We conduct a thorough evaluation, encompassing both\nperformance and efficiency, of diffusion model-based techniques in three\nprominent tasks. Finally, we elucidate the limitations of current diffusion\nmodels and propose seven intriguing directions for future research. This\ncomprehensive examination aims to facilitate a profound understanding of the\nlandscape surrounding denoising diffusion models in the context of low-level\nvision tasks. A curated list of diffusion model-based techniques in over 20\nlow-level vision tasks can be found at\nhttps://github.com/ChunmingHe/awesome-diffusion-models-in-low-level-vision.\n', '  Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.\n']",Image Restoration with Diffusion Models
53,52,145,52_labeling_supervised_labeled_classification,"['labeling', 'supervised', 'labeled', 'classification', 'labels', 'unlabeled', 'label', 'classifier', 'regularization', 'learning']","['label', 'labels', 'pseudo', 'unlabeled', 'noisy', 'labeled', 'labeling', 'semi', 'supervised', 'class']","['labeling', 'unlabeled', 'classifier', 'regularization', 'training', 'convnets', 'instance', 'semi', 'ssoc', 'mll']","['  Real-world datasets usually are class-imbalanced and corrupted by label\nnoise. To solve the joint issue of long-tailed distribution and label noise,\nmost previous works usually aim to design a noise detector to distinguish the\nnoisy and clean samples. Despite their effectiveness, they may be limited in\nhandling the joint issue effectively in a unified way. In this work, we develop\na novel pseudo labeling method using class prototypes from the perspective of\ndistribution matching, which can be solved with optimal transport (OT). By\nsetting a manually-specific probability measure and using a learned transport\nplan to pseudo-label the training samples, the proposed method can reduce the\nside-effects of noisy and long-tailed data simultaneously. Then we introduce a\nsimple yet effective filter criteria by combining the observed labels and\npseudo labels to obtain a more balanced and less noisy subset for a robust\nmodel training. Extensive experiments demonstrate that our method can extract\nthis class-balanced subset with clean labels, which brings effective\nperformance gains for long-tailed classification with label noise.\n', '  Self-supervised pretraining on unlabeled data followed by supervised\nfine-tuning on labeled data is a popular paradigm for learning from limited\nlabeled examples. We extend this paradigm to the classical positive unlabeled\n(PU) setting, where the task is to learn a binary classifier given only a few\nlabeled positive samples, and (often) a large amount of unlabeled samples\n(which could be positive or negative).\n  We first propose a simple extension of standard infoNCE family of contrastive\nlosses, to the PU setting; and show that this learns superior representations,\nas compared to existing unsupervised and supervised approaches. We then develop\na simple methodology to pseudo-label the unlabeled samples using a new\nPU-specific clustering scheme; these pseudo-labels can then be used to train\nthe final (positive vs. negative) classifier. Our method handily outperforms\nstate-of-the-art PU methods over several standard PU benchmark datasets, while\nnot requiring a-priori knowledge of any class prior (which is a common\nassumption in other PU methods). We also provide a simple theoretical analysis\nthat motivates our methods.\n', '  Deep neural models have achieved state of the art performance on a wide range\nof problems in computer science, especially in computer vision. However, deep\nneural networks often require large datasets of labeled samples to generalize\neffectively, and an important area of active research is semi-supervised\nlearning, which attempts to instead utilize large quantities of (easily\nacquired) unlabeled samples. One family of methods in this space is\npseudo-labeling, a class of algorithms that use model outputs to assign labels\nto unlabeled samples which are then used as labeled samples during training.\nSuch assigned labels, called pseudo-labels, are most commonly associated with\nthe field of semi-supervised learning. In this work we explore a broader\ninterpretation of pseudo-labels within both self-supervised and unsupervised\nmethods. By drawing the connection between these areas we identify new\ndirections when advancements in one area would likely benefit others, such as\ncurriculum learning and self-supervised regularization.\n']",Semi-Supervised Learning with Noisy Labels
54,53,144,53_programming_tutoring_tutor_tutors,"['programming', 'tutoring', 'tutor', 'tutors', 'chatbots', 'learners', 'students', 'pedagogical', 'educational', 'student']","['students', 'education', 'student', 'programming', 'tutoring', 'educational', 'tutors', 'teaching', 'course', 'tutor']","['tutoring', 'chatbots', 'students', 'curriculum', 'programs', 'chatgpt', 'introductory', 'courseassist', 'pedagogy', 'conversational']","[""  The application of Artificial intelligence for teaching and learning in the\nacademic sphere is a trending subject of interest in the computing education.\nChatGPT, as an AI-based tool, provides various advantages, such as heightened\nstudent involvement, cooperation, accessibility and availability. This paper\naddresses the prospects and obstacles associated with utilizing ChatGPT as a\ntool for learning and assessment in undergraduate Computer Science curriculum\nin particular to teaching and learning fundamental programming courses.\nStudents having completed the course work for a Data Structures and Algorithms\n(a sophomore level course) participated in this study. Two groups of students\nwere given programming challenges to solve within a short period of time. The\ncontrol group (group A) had access to text books and notes of programming\ncourses, however no Internet access was provided. Group B students were given\naccess to ChatGPT and were encouraged to use it to help solve the programming\nchallenges. The challenge was conducted in a computer lab environment using PC2\nenvironment. Each team of students address the problem by writing executable\ncode that satisfies certain number of test cases. Student teams were scored\nbased on their performance in terms of number of successful passed testcases.\nResults show that students using ChatGPT had an advantage in terms of earned\nscores, however there were inconsistencies and inaccuracies in the submitted\ncode consequently affecting the overall performance. After a thorough analysis,\nthe paper's findings indicate that incorporating AI in higher education brings\nabout various opportunities and challenges.\n"", ""  The integration of ChatGPT as a supportive tool in education, notably in\nprogramming courses, addresses the unique challenges of programming education\nby providing assistance with debugging, code generation, and explanations.\nDespite existing research validating ChatGPT's effectiveness, its application\nin university-level programming education and a detailed understanding of\nstudent interactions and perspectives remain limited. This paper explores\nChatGPT's impact on learning in a Python programming course tailored for\nfirst-year students over eight weeks. By analyzing responses from surveys,\nopen-ended questions, and student-ChatGPT dialog data, we aim to provide a\ncomprehensive view of ChatGPT's utility and identify both its advantages and\nlimitations as perceived by students. Our study uncovers a generally positive\nreception toward ChatGPT and offers insights into its role in enhancing the\nprogramming education experience. These findings contribute to the broader\ndiscourse on AI's potential in education, suggesting paths for future research\nand application.\n"", ""  This research paper contributes to the computing education research\ncommunity's understanding of Generative AI (GenAI) in the context of\nintroductory programming, and specifically, how students utilize related tools,\nsuch as ChatGPT. An increased understanding of students' use is mandatory for\neducators and higher education institutions, as GenAI is here to stay, and its\nperformance is likely to improve rapidly in the near future. Learning about\nstudents' use patterns is not only crucial to support their learning, but to\ndevelop adequate forms of instruction and assessment. With the rapid\nadvancement of AI, its broad availability, and ubiquitous presence in\neducational environments, elaborating how AI can enhance learning experiences,\nespecially in courses such as introductory programming is important. To date,\nmost studies have focused on the educator's perspective on GenAI, its\nperformance, characteristics, and limitations. However, the student\nperspective, and how they actually use GenAI tools in course contexts, has not\nbeen subject to a great number of studies. Therefore, this study is guided by\nthe following research questions: (1) What do students report on their use\npattern of ChatGPT in the context of introductory programming exercises? and\n(2) How do students perceive ChatGPT in the context of introductory programming\nexercises? To address these questions, computing students at a large German\nuniversity were asked to solve programming tasks with the assistance of ChatGPT\nas part of their introductory programming course. Students (n=298) provided\ninformation regarding the use of ChatGPT, and their evaluation of the tool via\nan online survey. This research provides a comprehensive evaluation of\nChatGPT-3.5's application by novice programmers in a higher education\ncontext...\n""]","""AI-powered tools in programming education"""
55,54,142,54_optimizing_optimize_optimization_optimisation,"['optimizing', 'optimize', 'optimization', 'optimisation', 'optimal', 'hyperparameters', 'optimum', 'hyperparameter', 'algorithms', 'prior']","['optimization', 'acquisition', 'black', 'box', 'function', 'functions', 'surrogate', 'expensive', 'design', 'objective']","['optimisation', 'hyperparameters', 'monte', 'bayesian', 'surrogate', 'search', 'exploration', 'sampling', 'objective', 'dbo']","[""  Gaussian process (GP) based Bayesian optimization (BO) is a powerful method\nfor optimizing black-box functions efficiently. The practical performance and\ntheoretical guarantees of this approach depend on having the correct GP\nhyperparameter values, which are usually unknown in advance and need to be\nestimated from the observed data. However, in practice, these estimations could\nbe incorrect due to biased data sampling strategies used in BO. This can lead\nto degraded performance and break the sub-linear global convergence guarantee\nof BO. To address this issue, we propose a new BO method that can sub-linearly\nconverge to the objective function's global optimum even when the true GP\nhyperparameters are unknown in advance and need to be estimated from the\nobserved data. Our method uses a multi-armed bandit technique (EXP3) to add\nrandom data points to the BO process, and employs a novel training loss\nfunction for the GP hyperparameter estimation process that ensures consistent\nestimation. We further provide theoretical analysis of our proposed method.\nFinally, we demonstrate empirically that our method outperforms existing\napproaches on various synthetic and real-world problems.\n"", '  In this paper, we address the problem of cost-sensitive multi-fidelity\nBayesian Optimization (BO) for efficient hyperparameter optimization (HPO).\nSpecifically, we assume a scenario where users want to early-stop the BO when\nthe performance improvement is not satisfactory with respect to the required\ncomputational cost. Motivated by this scenario, we introduce utility, which is\na function predefined by each user and describes the trade-off between cost and\nperformance of BO. This utility function, combined with our novel acquisition\nfunction and stopping criterion, allows us to dynamically choose for each BO\nstep the best configuration that we expect to maximally improve the utility in\nfuture, and also automatically stop the BO around the maximum utility. Further,\nwe improve the sample efficiency of existing learning curve (LC) extrapolation\nmethods with transfer learning, while successfully capturing the correlations\nbetween different configurations to develop a sensible surrogate function for\nmulti-fidelity BO. We validate our algorithm on various LC datasets and found\nit outperform all the previous multi-fidelity BO and transfer-BO baselines we\nconsider, achieving significantly better trade-off between cost and performance\nof BO.\n', '  Bayesian Optimization (BO) is a method for globally optimizing black-box\nfunctions. While BO has been successfully applied to many scenarios, developing\neffective BO algorithms that scale to functions with high-dimensional domains\nis still a challenge. Optimizing such functions by vanilla BO is extremely\ntime-consuming. Alternative strategies for high-dimensional BO that are based\non the idea of embedding the high-dimensional space to the one with low\ndimension are sensitive to the choice of the embedding dimension, which needs\nto be pre-specified. We develop a new computationally efficient\nhigh-dimensional BO method that exploits variable selection. Our method is able\nto automatically learn axis-aligned sub-spaces, i.e. spaces containing selected\nvariables, without the demand of any pre-specified hyperparameters. We\ntheoretically analyze the computational complexity of our algorithm and derive\nthe regret bound. We empirically show the efficacy of our method on several\nsynthetic and real problems.\n']",Bayesian Optimization Methods
56,55,142,55_ecg_electrocardiogram_electrocardiograms_ecgs,"['ecg', 'electrocardiogram', 'electrocardiograms', 'ecgs', 'arrhythmia', 'electrocardiography', 'heartbeat', 'arrhythmias', 'cardiac', 'recordings']","['heart', 'signals', 'electrocardiogram', 'cardiovascular', 'signal', 'arrhythmia', 'cardiac', 'monitoring', 'atrial', 'wearable']","['electrocardiograms', 'ecgs', 'heartbeat', 'arrhythmias', 'recordings', 'dataset', 'waveform', 'photoplethysmography', 'cvds', 'wearable']","['  Within cardiovascular disease detection using deep learning applied to ECG\nsignals, the complexities of handling physiological signals have sparked\ngrowing interest in leveraging deep generative models for effective data\naugmentation. In this paper, we introduce a novel versatile approach based on\ndenoising diffusion probabilistic models for ECG synthesis, addressing three\nscenarios: (i) heartbeat generation, (ii) partial signal imputation, and (iii)\nfull heartbeat forecasting. Our approach presents the first generalized\nconditional approach for ECG synthesis, and our experimental results\ndemonstrate its effectiveness for various ECG-related tasks. Moreover, we show\nthat our approach outperforms other state-of-the-art ECG generative models and\ncan enhance the performance of state-of-the-art classifiers.\n', '  Cardiovascular disease is a major life-threatening condition that is commonly\nmonitored using electrocardiogram (ECG) signals. However, these signals are\noften contaminated by various types of noise at different intensities,\nsignificantly interfering with downstream tasks. Therefore, denoising ECG\nsignals and increasing the signal-to-noise ratio is crucial for cardiovascular\nmonitoring. In this paper, we propose a deep learning method that combines a\none-dimensional convolutional layer with transformer architecture for denoising\nECG signals. The convolutional layer processes the ECG signal by various\nkernel/patch sizes and generates an embedding called multi-scale patch\nembedding. The embedding then is used as the input of a transformer network and\nenhances the capability of the transformer for denoising the ECG signal.\n', ""  The electrocardiogram (ECG) is a ubiquitous diagnostic test. Conventional\ntask-specific ECG analysis models require large numbers of expensive ECG\nannotations or associated labels to train. Transfer learning techniques have\nbeen shown to improve generalization and reduce reliance on labeled data. We\npresent ECG-FM, an open foundation model for ECG analysis, and conduct a\ncomprehensive study performed on a dataset of 1.66 million ECGs sourced from\nboth publicly available and private institutional sources. ECG-FM adopts a\ntransformer-based architecture and is pretrained on 2.5 million samples using\nECG-specific augmentations and contrastive learning, as well as a continuous\nsignal masking objective. Our transparent evaluation includes a diverse range\nof downstream tasks, where we predict ECG interpretation labels, reduced left\nventricular ejection fraction, and abnormal cardiac troponin. Affirming\nECG-FM's effectiveness as a foundation model, we demonstrate how its command of\ncontextual information results in strong performance, rich pretrained\nembeddings, and reliable interpretability. Due to a lack of open-weight\npractices, we highlight how ECG analysis is lagging behind other medical\nmachine learning subfields in terms of foundation model adoption. Our code is\navailable at https://github.com/bowang-lab/ECG-FM/.\n""]",Electrocardiogram Analysis and Applications
57,56,142,56_hatecheck_hatred_hateful_offensiveness,"['hatecheck', 'hatred', 'hateful', 'offensiveness', 'hate', 'bullying', 'dehumanization', 'profanity', 'cyberbullying', 'speech']","['hate', 'speech', 'offensive', 'hateful', 'media', 'cyberbullying', 'content', 'social', 'moderation', 'counterspeech']","['hatecheck', 'offensiveness', 'dehumanization', 'cyberbullying', 'speech', 'nlp', 'twitter', 'annotators', 'targeted', 'slurs']","['  Despite the extensive communication benefits offered by social media\nplatforms, numerous challenges must be addressed to ensure user safety. One of\nthe most significant risks faced by users on these platforms is targeted hate\nspeech. Social media platforms are widely utilised for generating datasets\nemployed in training and evaluating machine learning algorithms for hate speech\ndetection. However, existing public datasets exhibit numerous limitations,\nhindering the effective training of these algorithms and leading to inaccurate\nhate speech classification. This study provides a comprehensive empirical\nevaluation of several public datasets commonly used in automated hate speech\nclassification. Through rigorous analysis, we present compelling evidence\nhighlighting the limitations of current hate speech datasets. Additionally, we\nconduct a range of statistical analyses to elucidate the strengths and\nweaknesses inherent in these datasets. This work aims to advance the\ndevelopment of more accurate and reliable machine learning models for hate\nspeech detection by addressing the dataset limitations identified.\n', ""  The growth of social networks makes toxic content spread rapidly. Hate speech\ndetection is a task to help decrease the number of harmful comments. With the\ndiversity in the hate speech created by users, it is necessary to interpret the\nhate speech besides detecting it. Hence, we propose a methodology to construct\na system for targeted hate speech detection from online streaming texts from\nsocial media. We first introduce the ViTHSD - a targeted hate speech detection\ndataset for Vietnamese Social Media Texts. The dataset contains 10K comments,\neach comment is labeled to specific targets with three levels: clean,\noffensive, and hate. There are 5 targets in the dataset, and each target is\nlabeled with the corresponding level manually by humans with strict annotation\nguidelines. The inter-annotator agreement obtained from the dataset is 0.45 by\nCohen's Kappa index, which is indicated as a moderate level. Then, we construct\na baseline for this task by combining the Bi-GRU-LSTM-CNN with the pre-trained\nlanguage model to leverage the power of text representation of BERTology.\nFinally, we suggest a methodology to integrate the baseline model for targeted\nhate speech detection into the online streaming system for practical\napplication in preventing hateful and offensive content on social media.\n"", ""  Hate speech on social media threatens the mental and physical well-being of\nindividuals and contributes to real-world violence. Resharing is an important\ndriver behind the spread of hate speech on social media. Yet, little is known\nabout who reshares hate speech and what their characteristics are. In this\npaper, we analyze the role of user characteristics in hate speech resharing\nacross different types of hate speech (e.g., political hate). For this, we\nproceed as follows: First, we cluster hate speech posts using large language\nmodels to identify different types of hate speech. Then we model the effects of\nuser attributes on users' probability to reshare hate speech using an\nexplainable machine learning model. To do so, we apply debiasing to control for\nselection bias in our observational social media data and further control for\nthe latent vulnerability of users to hate speech. We find that, all else equal,\nusers with fewer followers, fewer friends, fewer posts, and older accounts\nshare more hate speech. This shows that users with little social influence tend\nto share more hate speech. Further, we find substantial heterogeneity across\ndifferent types of hate speech. For example, racist and misogynistic hate is\nspread mostly by users with little social influence. In contrast, political\nanti-Trump and anti-right-wing hate is reshared by users with larger social\ninfluence. Overall, understanding the factors that drive users to share hate\nspeech is crucial for detecting individuals at risk of engaging in harmful\nbehavior and for designing effective mitigation strategies.\n""]",Hate Speech Detection and Analysis
58,57,140,57_intrusions_intrusion_cybersecurity_iot,"['intrusions', 'intrusion', 'cybersecurity', 'iot', 'botnet', 'attacks', 'ddos', 'ids2018', 'security', 'attack']","['intrusion', 'traffic', 'detection', 'attacks', 'security', 'cyber', 'attack', 'threats', 'network', 'cybersecurity']","['intrusion', 'cybersecurity', 'iot', 'botnet', 'attacks', 'ddos', 'ids', 'detect', 'packets', 'lstm']","['  The large number of sensors and actuators that make up the Internet of Things\nobliges these systems to use diverse technologies and protocols. This means\nthat IoT networks are more heterogeneous than traditional networks. This gives\nrise to new challenges in cybersecurity to protect these systems and devices\nwhich are characterized by being connected continuously to the Internet.\nIntrusion detection systems (IDS) are used to protect IoT systems from the\nvarious anomalies and attacks at the network level. Intrusion Detection Systems\n(IDS) can be improved through machine learning techniques. Our work focuses on\ncreating classification models that can feed an IDS using a dataset containing\nframes under attacks of an IoT system that uses the MQTT protocol. We have\naddressed two types of method for classifying the attacks, ensemble methods and\ndeep learning models, more specifically recurrent networks with very\nsatisfactory results.\n', '  The integration of Internet of Things (IoT) applications in our daily lives\nhas led to a surge in data traffic, posing significant security challenges. IoT\napplications using cloud and edge computing are at higher risk of cyberattacks\nbecause of the expanded attack surface from distributed edge and cloud\nservices, the vulnerability of IoT devices, and challenges in managing security\nacross interconnected systems leading to oversights. This led to the rise of\nML-based solutions for intrusion detection systems (IDSs), which have proven\neffective in enhancing network security and defending against diverse threats.\nHowever, ML-based IDS in IoT systems encounters challenges, particularly from\nnoisy, redundant, and irrelevant features in varied IoT datasets, potentially\nimpacting its performance. Therefore, reducing such features becomes crucial to\nenhance system performance and minimize computational costs. This paper focuses\non improving the effectiveness of ML-based IDS at the edge level by introducing\na novel method to find a balanced trade-off between cost and accuracy through\nthe creation of informative features in a two-tier edge-user IoT environment. A\nhybrid Binary Quantum-inspired Artificial Bee Colony and Genetic Programming\nalgorithm is utilized for this purpose. Three IoT intrusion detection datasets,\nnamely NSL-KDD, UNSW-NB15, and BoT-IoT, are used for the evaluation of the\nproposed approach.\n', '  Intrusion Detection Systems (IDS) play a crucial role in ensuring the\nsecurity of computer networks. Machine learning has emerged as a popular\napproach for intrusion detection due to its ability to analyze and detect\npatterns in large volumes of data. However, current ML-based IDS solutions\noften struggle to keep pace with the ever-changing nature of attack patterns\nand the emergence of new attack types. Additionally, these solutions face\nchallenges related to class imbalance, where the number of instances belonging\nto different classes (normal and intrusions) is significantly imbalanced, which\nhinders their ability to effectively detect minor classes. In this paper, we\npropose a novel multi-agent reinforcement learning (RL) architecture, enabling\nautomatic, efficient, and robust network intrusion detection. To enhance the\ncapabilities of the proposed model, we have improved the DQN algorithm by\nimplementing the weighted mean square loss function and employing\ncost-sensitive learning techniques. Our solution introduces a resilient\narchitecture designed to accommodate the addition of new attacks and\neffectively adapt to changes in existing attack patterns. Experimental results\nrealized using CIC-IDS-2017 dataset, demonstrate that our approach can\neffectively handle the class imbalance problem and provide a fine grained\nclassification of attacks with a very low false positive rate. In comparison to\nthe current state-of-the-art works, our solution demonstrates a significant\nsuperiority in both detection rate and false positive rate.\n']",Intrusion Detection in IoT Systems
59,58,140,58_activity_activities_sensing_recognition,"['activity', 'activities', 'sensing', 'recognition', 'wearable', 'recognizing', 'tracking', 'feature', 'sensor', 'datasets']","['activity', 'sensor', 'activities', 'recognition', 'sensors', 'wearable', 'rehabilitation', 'monitoring', 'home', 'human']","['activities', 'sensing', 'wearable', 'tracking', 'datasets', 'ambient', 'walking', 'fitness', 'accelerometer', 'rehabilitation']","['  Human Activity Recognition (HAR) is a well-studied field with research dating\nback to the 1980s. Over time, HAR technologies have evolved significantly from\nmanual feature extraction, rule-based algorithms, and simple machine learning\nmodels to powerful deep learning models, from one sensor type to a diverse\narray of sensing modalities. The scope has also expanded from recognising a\nlimited set of activities to encompassing a larger variety of both simple and\ncomplex activities. However, there still exist many challenges that hinder\nadvancement in complex activity recognition using modern deep learning methods.\nIn this paper, we comprehensively systematise factors leading to inaccuracy in\ncomplex HAR, such as data variety and model capacity. Among many sensor types,\nwe give more attention to wearable and camera due to their prevalence. Through\nthis Systematisation of Knowledge (SoK) paper, readers can gain a solid\nunderstanding of the development history and existing challenges of HAR,\ndifferent categorisations of activities, obstacles in deep learning-based\ncomplex HAR that impact accuracy, and potential research directions.\n', '  Sensor-based human activity recognition (HAR) has been an active research\narea, owing to its applications in smart environments, assisted living,\nfitness, healthcare, etc. Recently, deep learning based end-to-end training has\nresulted in state-of-the-art performance in domains such as computer vision and\nnatural language, where large amounts of annotated data are available. However,\nlarge quantities of annotated data are not available for sensor-based HAR.\nMoreover, the real-world settings on which the HAR is performed differ in terms\nof sensor modalities, classification tasks, and target users. To address this\nproblem, transfer learning has been employed extensively. In this survey, we\nfocus on these transfer learning methods in the application domains of smart\nhome and wearables-based HAR. In particular, we provide a problem-solution\nperspective by categorizing and presenting the works in terms of their\ncontributions and the challenges they address. We also present an updated view\nof the state-of-the-art for both application domains. Based on our analysis of\n205 papers, we highlight the gaps in the literature and provide a roadmap for\naddressing them. This survey provides a reference to the HAR community, by\nsummarizing the existing works and providing a promising research agenda.\n', '  Human activity recognition (HAR) from on-body sensors is a core functionality\nin many AI applications: from personal health, through sports and wellness to\nIndustry 4.0. A key problem holding up progress in wearable sensor-based HAR,\ncompared to other ML areas, such as computer vision, is the unavailability of\ndiverse and labeled training data. Particularly, while there are innumerable\nannotated images available in online repositories, freely available sensor data\nis sparse and mostly unlabeled. We propose an unsupervised statistical\nfeature-guided diffusion model specifically optimized for wearable sensor-based\nhuman activity recognition with devices such as inertial measurement unit (IMU)\nsensors. The method generates synthetic labeled time-series sensor data without\nrelying on annotated training data. Thereby, it addresses the scarcity and\nannotation difficulties associated with real-world sensor data. By conditioning\nthe diffusion model on statistical information such as mean, standard\ndeviation, Z-score, and skewness, we generate diverse and representative\nsynthetic sensor data. We conducted experiments on public human activity\nrecognition datasets and compared the method to conventional oversampling and\nstate-of-the-art generative adversarial network methods. Experimental results\ndemonstrate that this can improve the performance of human activity recognition\nand outperform existing techniques.\n']",Human Activity Recognition
60,59,137,59_galaxies_galactic_galaxy_astronomical,"['galaxies', 'galactic', 'galaxy', 'astronomical', 'astronomy', 'astrophysical', 'astrophysics', 'cosmology', 'cosmic', 'cosmological']","['galaxy', 'cosmological', 'gravitational', 'galaxies', 'spectra', 'radio', 'dark', 'astrophysical', 'mass', 'redshift']","['galaxies', 'astrophysical', 'cosmological', 'redshift', 'stars', 'telescope', 'dark', 'hubble', 'spectroscopic', 'exoplanet']","[""  It has been recently shown that a powerful way to constrain cosmological\nparameters from galaxy redshift surveys is to train graph neural networks to\nperform field-level likelihood-free inference without imposing cuts on scale.\nIn particular, de Santi et al. (2023) developed models that could accurately\ninfer the value of $\\Omega_{\\rm m}$ from catalogs that only contain the\npositions and radial velocities of galaxies that are robust to uncertainties in\nastrophysics and subgrid models. However, observations are affected by many\neffects, including 1) masking, 2) uncertainties in peculiar velocities and\nradial distances, and 3) different galaxy selections. Moreover, observations\nonly allow us to measure redshift, intertwining galaxies' radial positions and\nvelocities. In this paper we train and test our models on galaxy catalogs,\ncreated from thousands of state-of-the-art hydrodynamic simulations run with\ndifferent codes from the CAMELS project, that incorporate these observational\neffects. We find that, although the presence of these effects degrades the\nprecision and accuracy of the models, and increases the fraction of catalogs\nwhere the model breaks down, the fraction of galaxy catalogs where the model\nperforms well is over 90 %, demonstrating the potential of these models to\nconstrain cosmological parameters even when applied to real data.\n"", '  Modern spectroscopic surveys can only target a small fraction of the vast\namount of photometrically cataloged sources in wide-field surveys. Here, we\nreport the development of a generative AI method capable of predicting optical\ngalaxy spectra from photometric broad-band images alone. This method draws from\nthe latest advances in diffusion models in combination with contrastive\nnetworks. We pass multi-band galaxy images into the architecture to obtain\noptical spectra. From these, robust values for galaxy properties can be derived\nwith any methods in the spectroscopic toolbox, such as standard population\nsynthesis techniques and Lick indices. When trained and tested on 64x64-pixel\nimages from the Sloan Digital Sky Survey, the global bimodality of star-forming\nand quiescent galaxies in photometric space is recovered, as well as a\nmass-metallicity relation of star-forming galaxies. The comparison between the\nobserved and the artificially created spectra shows good agreement in overall\nmetallicity, age, Dn4000, stellar velocity dispersion, and E(B-V) values.\nPhotometric redshift estimates of our generative algorithm can compete with\nother current, specialized deep-learning techniques. Moreover, this work is the\nfirst attempt in the literature to infer velocity dispersion from photometric\nimages. Additionally, we can predict the presence of an active galactic nucleus\nup to an accuracy of 82%. With our method, scientifically interesting galaxy\nproperties, normally requiring spectroscopic inputs, can be obtained in future\ndata sets from large-scale photometric surveys alone. The spectra prediction\nvia AI can further assist in creating realistic mock catalogs.\n', '  We present AstroCLIP, a single, versatile model that can embed both galaxy\nimages and spectra into a shared, physically meaningful latent space. These\nembeddings can then be used - without any model fine-tuning - for a variety of\ndownstream tasks including (1) accurate in-modality and cross-modality semantic\nsimilarity search, (2) photometric redshift estimation, (3) galaxy property\nestimation from both images and spectra, and (4) morphology classification. Our\napproach to implementing AstroCLIP consists of two parts. First, we embed\ngalaxy images and spectra separately by pretraining separate transformer-based\nimage and spectrum encoders in self-supervised settings. We then align the\nencoders using a contrastive loss. We apply our method to spectra from the Dark\nEnergy Spectroscopic Instrument and images from its corresponding Legacy\nImaging Survey. Overall, we find remarkable performance on all downstream\ntasks, even relative to supervised baselines. For example, for a task like\nphotometric redshift prediction, we find similar performance to a\nspecifically-trained ResNet18, and for additional tasks like physical property\nestimation (stellar mass, age, metallicity, and sSFR), we beat this supervised\nbaseline by 19\\% in terms of $R^2$. We also compare our results to a\nstate-of-the-art self-supervised single-modal model for galaxy images, and find\nthat our approach outperforms this benchmark by roughly a factor of two on\nphotometric redshift estimation and physical property prediction in terms of\n$R^2$, while remaining roughly in-line in terms of morphology classification.\nUltimately, our approach represents the first cross-modal self-supervised model\nfor galaxies, and the first self-supervised transformer-based architectures for\ngalaxy images and spectra.\n']",Galaxy Analysis and Cosmology
61,60,135,60_loras_lora_tuning_rank,"['loras', 'lora', 'tuning', 'rank', 'adapting', 'adaptation', 'lorahub', 'tuned', 'sparse', 'trained']","['rank', 'tuning', 'fine', 'adaptation', 'parameter', 'low', 'matrices', 'trainable', 'parameters', 'efficient']","['loras', 'rank', 'adaptation', 'tuned', 'sparse', 'weights', 'svd', 'matrix', 'llms', 'pretraining']","['  Low-rank adaptation (LoRA) has become the standard approach for\nparameter-efficient fine-tuning of large language models (LLM), but our\ntheoretical understanding of LoRA has been limited. In this work, we\ntheoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK)\nregime with $N$ data points, showing: (i) full fine-tuning (without LoRA)\nadmits a low-rank solution of rank $r\\lesssim \\sqrt{N}$; (ii) using LoRA with\nrank $r\\gtrsim \\sqrt{N}$ eliminates spurious local minima, allowing gradient\ndescent to find the low-rank solutions; (iii) the low-rank solution found using\nLoRA generalizes well.\n', '  Low-Rank Adaptation, also known as LoRA, has emerged as a prominent method\nfor parameter-efficient fine-tuning foundation models by re-parameterizing the\noriginal matrix into the product of two low-rank matrices. Despite its\nefficiency, LoRA often yields inferior performance compared to full\nfine-tuning. In this paper, we propose LoRA-Pro to bridge this performance gap.\nFirstly, we delve into the optimization processes in LoRA and full fine-tuning.\nWe reveal that while LoRA employs low-rank approximation, it neglects to\napproximate the optimization process of full fine-tuning. To address this, we\nintroduce a novel concept called the ""equivalent gradient."" This virtual\ngradient makes the optimization process on the re-parameterized matrix\nequivalent to LoRA, which can be used to quantify the differences between LoRA\nand full fine-tuning. The equivalent gradient is derived from the gradients of\nmatrices $A$ and $B$. To narrow the performance gap, our approach minimizes the\ndifferences between the equivalent gradient and the gradient obtained from full\nfine-tuning during the optimization process. By solving this objective, we\nderive optimal closed-form solutions for updating matrices $A$ and $B$. Our\nmethod constrains the optimization process, shrinking the performance gap\nbetween LoRA and full fine-tuning. Extensive experiments on natural language\nprocessing tasks validate the effectiveness of our method.\n', '  Low-Rank Adaptation (LoRA) is currently the most commonly used\nParameter-efficient fine-tuning (PEFT) method, it introduces auxiliary\nparameters for each layer to fine-tune the pre-trained model under limited\ncomputing resources. However, it still faces resource consumption challenges\nduring training when scaling up to larger models. Most previous studies have\ntackled this issue by using pruning techniques, which involve removing LoRA\nparameters deemed unimportant. Nonetheless, these efforts only analyze LoRA\nparameter features to evaluate their importance, such as parameter count, size,\nand gradient. In fact, the output of LoRA (product of LoRA parameter and hidden\nstate), directly impacts the final results. Preliminary experiments indicate\nthat a fraction of LoRA elements possesses significantly high output values,\nsubstantially influencing the layer output. Motivated by the observation, we\npropose LoRA-drop. Concretely, LoRA-drop evaluates the importance of LoRA based\non the LoRA output. Then we retain LoRA for important layers and the other\nlayers share the same LoRA. We conduct abundant experiments with models of\ndifferent scales on NLU and NLG tasks. Results demonstrate that LoRA-drop can\nachieve performance comparable to full fine-tuning and LoRA, while retaining\n50\\% of the LoRA parameters on average.\n']",Low-Rank Adaptation for Efficient Fine-Tuning of Large Language Models
62,61,135,61_credibility_news_disinformation_tweets,"['credibility', 'news', 'disinformation', 'tweets', 'propaganda', 'journalistic', 'veracity', 'journalists', 'content', 'debunking']","['news', 'fake', 'misinformation', 'rumor', 'media', 'articles', 'detection', 'rumors', 'social', 'spread']","['credibility', 'disinformation', 'tweets', 'propaganda', 'journalists', 'content', 'satirical', 'weibo', 'detecting', 'youtube']","['  Fake news significantly influence our society. They impact consumers, voters,\nand many other societal groups. While Fake News exist for a centuries,\nGenerative AI brings fake news on a new level. It is now possible to automate\nthe creation of masses of high-quality individually targeted Fake News. On the\nother end, Generative AI can also help detecting Fake News. Both fields are\nyoung but developing fast.\n  This survey provides a comprehensive examination of the research and\npractical use of Generative AI for Fake News detection and creation in 2024.\nFollowing the Structured Literature Survey approach, the paper synthesizes\ncurrent results in the following topic clusters 1) enabling technologies, 2)\ncreation of Fake News, 3) case study social media as most relevant distribution\nchannel, 4) detection of Fake News, and 5) deepfakes as upcoming technology.\n  The article also identifies current challenges and open issues.\n', '  The prevalence of fake news across various online sources has had a\nsignificant influence on the public. Existing Chinese fake news detection\ndatasets are limited to news sourced solely from Weibo. However, fake news\noriginating from multiple sources exhibits diversity in various aspects,\nincluding its content and social context. Methods trained on purely one single\nnews source can hardly be applicable to real-world scenarios. Our pilot\nexperiment demonstrates that the F1 score of the state-of-the-art method that\nlearns from a large Chinese fake news detection dataset, Weibo-21, drops\nsignificantly from 0.943 to 0.470 when the test data is changed to multi-source\nnews data, failing to identify more than one-third of the multi-source fake\nnews. To address this limitation, we constructed the first multi-source\nbenchmark dataset for Chinese fake news detection, termed MCFEND, which is\ncomposed of news we collected from diverse sources such as social platforms,\nmessaging apps, and traditional online news outlets. Notably, such news has\nbeen fact-checked by 14 authoritative fact-checking agencies worldwide. In\naddition, various existing Chinese fake news detection methods are thoroughly\nevaluated on our proposed dataset in cross-source, multi-source, and unseen\nsource ways. MCFEND, as a benchmark dataset, aims to advance Chinese fake news\ndetection approaches in real-world scenarios.\n', '  In the age of large language models (LLMs) and the widespread adoption of\nAI-driven content creation, the landscape of information dissemination has\nwitnessed a paradigm shift. With the proliferation of both human-written and\nmachine-generated real and fake news, robustly and effectively discerning the\nveracity of news articles has become an intricate challenge. While substantial\nresearch has been dedicated to fake news detection, this either assumes that\nall news articles are human-written or abruptly assumes that all\nmachine-generated news are fake. Thus, a significant gap exists in\nunderstanding the interplay between machine-(paraphrased) real news,\nmachine-generated fake news, human-written fake news, and human-written real\nnews. In this paper, we study this gap by conducting a comprehensive evaluation\nof fake news detectors trained in various scenarios. Our primary objectives\nrevolve around the following pivotal question: How to adapt fake news detectors\nto the era of LLMs? Our experiments reveal an interesting pattern that\ndetectors trained exclusively on human-written articles can indeed perform well\nat detecting machine-generated fake news, but not vice versa. Moreover, due to\nthe bias of detectors against machine-generated texts \\cite{su2023fake}, they\nshould be trained on datasets with a lower machine-generated news ratio than\nthe test set. Building on our findings, we provide a practical strategy for the\ndevelopment of robust fake news detectors.\n']",Fake News and Disinformation Detection
63,62,134,62_jailbreaks_jailbreaking_jailbreak_jailbroken,"['jailbreaks', 'jailbreaking', 'jailbreak', 'jailbroken', 'jailbreakbench', 'vulnerabilities', 'vulnerable', 'vulnerability', 'security', 'adversarial']","['jailbreak', 'harmful', 'attack', 'jailbreaks', 'attacks', 'prompts', 'safety', 'defense', 'success', 'security']","['jailbreaks', 'jailbreakbench', 'vulnerability', 'adversarial', 'wildguard', 'defenses', 'selfdefend', 'prompts', 'token', 'toxic']","['  As Large Language Models (LLMs) are increasingly being deployed in\nsafety-critical applications, their vulnerability to potential jailbreaks --\nmalicious prompts that can disable the safety mechanism of LLMs -- has\nattracted growing research attention. While alignment methods have been\nproposed to protect LLMs from jailbreaks, many have found that aligned LLMs can\nstill be jailbroken by carefully crafted malicious prompts, producing content\nthat violates policy regulations. Existing jailbreak attacks on LLMs can be\ncategorized into prompt-level methods which make up stories/logic to circumvent\nsafety alignment and token-level attack methods which leverage gradient methods\nto find adversarial tokens. In this work, we introduce the concept of Ensemble\nJailbreak and explore methods that can integrate prompt-level and token-level\njailbreak into a more powerful hybrid jailbreak attack. Specifically, we\npropose a novel EnJa attack to hide harmful instructions using prompt-level\njailbreak, boost the attack success rate using a gradient-based attack, and\nconnect the two types of jailbreak attacks via a template-based connector. We\nevaluate the effectiveness of EnJa on several aligned models and show that it\nachieves a state-of-the-art attack success rate with fewer queries and is much\nstronger than any individual jailbreak.\n', '  Misuse of the Large Language Models (LLMs) has raised widespread concern. To\naddress this issue, safeguards have been taken to ensure that LLMs align with\nsocial ethics. However, recent findings have revealed an unsettling\nvulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By\napplying techniques, such as employing role-playing scenarios, adversarial\nexamples, or subtle subversion of safety objectives as a prompt, LLMs can\nproduce an inappropriate or even harmful response. While researchers have\nstudied several categories of jailbreak attacks, they have done so in\nisolation. To fill this gap, we present the first large-scale measurement of\nvarious jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak\nmethods from four categories, 160 questions from 16 violation categories, and\nsix popular LLMs. Our extensive experimental results demonstrate that the\noptimized jailbreak prompts consistently achieve the highest attack success\nrates, as well as exhibit robustness across different LLMs. Some jailbreak\nprompt datasets, available from the Internet, can also achieve high attack\nsuccess rates on many LLMs, such as ChatGLM3, GPT-3.5, and PaLM2. Despite the\nclaims from many organizations regarding the coverage of violation categories\nin their policies, the attack success rates from these categories remain high,\nindicating the challenges of effectively aligning LLM policies and the ability\nto counter jailbreak attacks. We also discuss the trade-off between the attack\nperformance and efficiency, as well as show that the transferability of the\njailbreak prompts is still viable, becoming an option for black-box models.\nOverall, our research highlights the necessity of evaluating different\njailbreak methods. We hope our study can provide insights for future research\non jailbreak attacks and serve as a benchmark tool for evaluating them for\npractitioners.\n', '  Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential\nbut also introduce challenges related to content constraints and potential\nmisuse. Our study investigates three key research questions: (1) the number of\ndifferent prompt types that can jailbreak LLMs, (2) the effectiveness of\njailbreak prompts in circumventing LLM constraints, and (3) the resilience of\nChatGPT against these jailbreak prompts. Initially, we develop a classification\nmodel to analyze the distribution of existing prompts, identifying ten distinct\npatterns and three categories of jailbreak prompts. Subsequently, we assess the\njailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a\ndataset of 3,120 jailbreak questions across eight prohibited scenarios.\nFinally, we evaluate the resistance of ChatGPT against jailbreak prompts,\nfinding that the prompts can consistently evade the restrictions in 40 use-case\nscenarios. The study underscores the importance of prompt structures in\njailbreaking LLMs and discusses the challenges of robust jailbreak prompt\ngeneration and prevention.\n']","""Jailbreak Attacks on Large Language Models"""
64,63,131,63_radiology_radiologists_radiological_radiologist,"['radiology', 'radiologists', 'radiological', 'radiologist', 'radiologic', 'diagnostic', 'reporting', 'reports', 'evaluation', 'multimodal']","['radiology', 'reports', 'medical', 'report', 'chest', 'radiologists', 'ray', 'clinical', 'multimodal', 'diagnostic']","['radiology', 'reports', 'evaluation', 'multimodal', 'textual', 'scans', 'anatomy', 'biomedgpt', 'physicians', 'ray']","[""  GPT-4V's purported strong multimodal abilities raise interests in using it to\nautomate radiology report writing, but there lacks thorough evaluations. In\nthis work, we perform a systematic evaluation of GPT-4V in generating radiology\nreports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt\nto directly generate reports using GPT-4V through different prompting\nstrategies and find that it fails terribly in both lexical metrics and clinical\nefficacy metrics. To understand the low performance, we decompose the task into\ntwo steps: 1) the medical image reasoning step of predicting medical condition\nlabels from images; and 2) the report synthesis step of generating reports from\n(groundtruth) conditions. We show that GPT-4V's performance in image reasoning\nis consistently low across different prompts. In fact, the distributions of\nmodel-predicted labels remain constant regardless of which groundtruth\nconditions are present on the image, suggesting that the model is not\ninterpreting chest X-rays meaningfully. Even when given groundtruth conditions\nin report synthesis, its generated reports are less correct and less\nnatural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt\non the viability of using GPT-4V in a radiology workflow.\n"", '  The impression section of a radiology report summarizes important radiology\nfindings and plays a critical role in communicating these findings to\nphysicians. However, the preparation of these summaries is time-consuming and\nerror-prone for radiologists. Recently, numerous models for radiology report\nsummarization have been developed. Nevertheless, there is currently no model\nthat can summarize these reports in multiple languages. Such a model could\ngreatly improve future research and the development of Deep Learning models\nthat incorporate data from patients with different ethnic backgrounds. In this\nstudy, the generation of radiology impressions in different languages was\nautomated by fine-tuning a model, publicly available, based on a multilingual\ntext-to-text Transformer to summarize findings available in English,\nPortuguese, and German radiology reports. In a blind test, two board-certified\nradiologists indicated that for at least 70% of the system-generated summaries,\nthe quality matched or exceeded the corresponding human-written summaries,\nsuggesting substantial clinical reliability. Furthermore, this study showed\nthat the multilingual model outperformed other models that specialized in\nsummarizing radiology reports in only one language, as well as models that were\nnot specifically designed for summarizing radiology reports, such as ChatGPT.\n', ""  Evaluating generated radiology reports is crucial for the development of\nradiology AI, but existing metrics fail to reflect the task's clinical\nrequirements. This study proposes a novel evaluation framework using large\nlanguage models (LLMs) to compare radiology reports for assessment. We compare\nthe performance of various LLMs and demonstrate that, when using GPT-4, our\nproposed metric achieves evaluation consistency close to that of radiologists.\nFurthermore, to reduce costs and improve accessibility, making this method\npractical, we construct a dataset using LLM evaluation results and perform\nknowledge distillation to train a smaller model. The distilled model achieves\nevaluation capabilities comparable to GPT-4. Our framework and distilled model\noffer an accessible and efficient evaluation method for radiology report\ngeneration, facilitating the development of more clinically relevant models.\nThe model will be further open-sourced and accessible.\n""]",Radiology Report Generation and Evaluation
65,64,131,64_context_learning_icl_learn,"['context', 'learning', 'icl', 'learn', 'retrieval', 'examples', 'example', 'ica', 'nlp', 'language']","['demonstrations', 'demonstration', 'context', 'examples', 'shot', 'label', 'selection', 'task', 'tasks', 'learning']","['context', 'icl', 'retrieval', 'nlp', 'labeled', 'tasks', 'models', 'exemplars', 'instruction', 'llm']","['  It has long been assumed that the sheer number of parameters in large\nlanguage models (LLMs) drives in-context learning (ICL) capabilities, enabling\nremarkable performance improvements by leveraging task-specific demonstrations.\nChallenging this hypothesis, we introduce DEEP-ICL, a novel task Definition\nEnriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts\ntask definitions from given demonstrations and generates responses through\nlearning task-specific examples. We argue that improvement from ICL does not\ndirectly rely on model size, but essentially stems from understanding task\ndefinitions and task-guided learning. Inspired by this, DEEP-ICL combines two\n3B models with distinct roles (one for concluding task definitions and the\nother for learning task demonstrations) and achieves comparable performance to\nLLaMA2-13B. Furthermore, our framework outperforms conventional ICL by\novercoming pretraining sequence length limitations, by supporting unlimited\ndemonstrations. We contend that DEEP-ICL presents a novel alternative for\nachieving efficient few-shot learning, extending beyond the conventional ICL.\n', '  The predictions of Large Language Models (LLMs) on downstream tasks often\nimprove significantly when including examples of the input--label relationship\nin the context. However, there is currently no consensus about how this\nin-context learning (ICL) ability of LLMs works. For example, while Xie et al.\n(2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022)\nargue ICL does not even learn label relationships from in-context examples. In\nthis paper, we provide novel insights into how ICL leverages label information,\nrevealing both capabilities and limitations. To ensure we obtain a\ncomprehensive picture of ICL behavior, we study probabilistic aspects of ICL\npredictions and thoroughly examine the dynamics of ICL as more examples are\nprovided. Our experiments show that ICL predictions almost always depend on\nin-context labels and that ICL can learn truly novel tasks in-context. However,\nwe also find that ICL struggles to fully overcome prediction preferences\nacquired from pre-training data and, further, that ICL does not consider all\nin-context information equally.\n', '  With the increasing capabilities of large language models (LLMs), in-context\nlearning (ICL) has emerged as a new paradigm for natural language processing\n(NLP), where LLMs make predictions based on contexts augmented with a few\nexamples. It has been a significant trend to explore ICL to evaluate and\nextrapolate the ability of LLMs. In this paper, we aim to survey and summarize\nthe progress and challenges of ICL. We first present a formal definition of ICL\nand clarify its correlation to related studies. Then, we organize and discuss\nadvanced techniques, including training strategies, prompt designing\nstrategies, and related analysis. Additionally, we explore various ICL\napplication scenarios, such as data engineering and knowledge updating.\nFinally, we address the challenges of ICL and suggest potential directions for\nfurther research. We hope that our work can encourage more research on\nuncovering how ICL works and improving ICL.\n']",In-Context Learning in Large Language Models
66,65,129,65_traffic_spatiotemporal_transportation_networks,"['traffic', 'spatiotemporal', 'transportation', 'networks', 'citynet', 'prediction', 'forecasting', 'predicting', 'congestion', 'roads']","['traffic', 'temporal', 'spatial', 'spatio', 'transportation', 'urban', 'forecasting', 'flow', 'road', 'prediction']","['traffic', 'spatiotemporal', 'citynet', 'forecasting', 'neural', 'planning', 'flows', 'parking', 'datasets', 'graphs']","[""  Robust prediction of citywide traffic flows at different time periods plays a\ncrucial role in intelligent transportation systems. While previous work has\nmade great efforts to model spatio-temporal correlations, existing methods\nstill suffer from two key limitations: i) Most models collectively predict all\nregions' flows without accounting for spatial heterogeneity, i.e., different\nregions may have skewed traffic flow distributions. ii) These models fail to\ncapture the temporal heterogeneity induced by time-varying traffic patterns, as\nthey typically model temporal correlations with a shared parameterized space\nfor all time periods. To tackle these challenges, we propose a novel\nSpatio-Temporal Self-Supervised Learning (ST-SSL) traffic prediction framework\nwhich enhances the traffic pattern representations to be reflective of both\nspatial and temporal heterogeneity, with auxiliary self-supervised learning\nparadigms. Specifically, our ST-SSL is built over an integrated module with\ntemporal and spatial convolutions for encoding the information across space and\ntime. To achieve the adaptive spatio-temporal self-supervised learning, our\nST-SSL first performs the adaptive augmentation over the traffic flow graph\ndata at both attribute- and structure-levels. On top of the augmented traffic\ngraph, two SSL auxiliary tasks are constructed to supplement the main traffic\nprediction task with spatial and temporal heterogeneity-aware augmentation.\nExperiments on four benchmark datasets demonstrate that ST-SSL consistently\noutperforms various state-of-the-art baselines. Since spatio-temporal\nheterogeneity widely exists in practical datasets, the proposed framework may\nalso cast light on other spatial-temporal applications. Model implementation is\navailable at https://github.com/Echo-Ji/ST-SSL.\n"", '  As a core technology of Intelligent Transportation System, traffic flow\nprediction has a wide range of applications. The fundamental challenge in\ntraffic flow prediction is to effectively model the complex spatial-temporal\ndependencies in traffic data. Spatial-temporal Graph Neural Network (GNN)\nmodels have emerged as one of the most promising methods to solve this problem.\nHowever, GNN-based models have three major limitations for traffic prediction:\ni) Most methods model spatial dependencies in a static manner, which limits the\nability to learn dynamic urban traffic patterns; ii) Most methods only consider\nshort-range spatial information and are unable to capture long-range spatial\ndependencies; iii) These methods ignore the fact that the propagation of\ntraffic conditions between locations has a time delay in traffic systems. To\nthis end, we propose a novel Propagation Delay-aware dynamic long-range\ntransFormer, namely PDFormer, for accurate traffic flow prediction.\nSpecifically, we design a spatial self-attention module to capture the dynamic\nspatial dependencies. Then, two graph masking matrices are introduced to\nhighlight spatial dependencies from short- and long-range views. Moreover, a\ntraffic delay-aware feature transformation module is proposed to empower\nPDFormer with the capability of explicitly modeling the time delay of spatial\ninformation propagation. Extensive experimental results on six real-world\npublic traffic datasets show that our method can not only achieve\nstate-of-the-art performance but also exhibit competitive computational\nefficiency. Moreover, we visualize the learned spatial-temporal attention map\nto make our model highly interpretable.\n', '  Spatio-temporal forecasting of traffic flow data represents a typical problem\nin the field of machine learning, impacting urban traffic management systems.\nTraditional statistical and machine learning methods cannot adequately handle\nboth the temporal and spatial dependencies in these complex traffic flow\ndatasets. A prevalent approach in the field is to combine graph convolutional\nnetworks and multi-head attention mechanisms for spatio-temporal processing.\nThis paper proposes a wavelet-based temporal attention model, namely a\nwavelet-based dynamic spatio-temporal aware graph neural network (W-DSTAGNN),\nfor tackling the traffic forecasting problem. Benchmark experiments using\nseveral statistical metrics confirm that our proposal efficiently captures\nspatio-temporal correlations and outperforms ten state-of-the-art models on\nthree different real-world traffic datasets. Our proposed ensemble data-driven\nmethod can handle dynamic temporal and spatial dependencies and make long-term\nforecasts in an efficient manner.\n']",Traffic Flow Prediction and Management
67,66,129,66_hallucination_hallucinations_hallucinating_hallucinate,"['hallucination', 'hallucinations', 'hallucinating', 'hallucinate', 'hallucinatory', 'hallucinated', 'decoding', 'annotator', 'text', 'halludial']","['hallucinations', 'hallucination', 'factuality', 'truthfulness', 'factual', 'outputs', 'generations', 'detection', 'hallucinatory', 'facts']","['hallucination', 'decoding', 'annotator', 'semantic', 'nlg', 'retrieval', 'answering', 'generating', 'llms', 'factchd']","['  Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval.\n', '  In the era of large language models (LLMs), hallucination (i.e., the tendency\nto generate factually incorrect content) poses great challenge to trustworthy\nand reliable deployment of LLMs in real-world applications. To tackle the LLM\nhallucination, three key questions should be well studied: how to detect\nhallucinations (detection), why do LLMs hallucinate (source), and what can be\ndone to mitigate them (mitigation). To address these challenges, this work\npresents a systematic empirical study on LLM hallucination, focused on the the\nthree aspects of hallucination detection, source and mitigation. Specially, we\nconstruct a new hallucination benchmark HaluEval 2.0, and designs a simple yet\neffective detection method for LLM hallucination. Furthermore, we zoom into the\ndifferent training or utilization stages of LLMs and extensively analyze the\npotential factors that lead to the LLM hallucination. Finally, we implement and\nexamine a series of widely used techniques to mitigate the hallucinations in\nLLMs. Our work has led to several important findings to understand the\nhallucination origin and mitigate the hallucinations in LLMs. Our code and data\ncan be accessed at https://github.com/RUCAIBox/HaluEval-2.0.\n', '  Large Language Models (LLMs) have shown propensity to generate hallucinated\noutputs, i.e., texts that are factually incorrect or unsupported. Existing\nmethods for alleviating hallucinations typically require costly human\nannotations to identify and correct hallucinations in LLM outputs. Moreover,\nmost of these methods focus on a specific type of hallucination, e.g., entity\nor token errors, which limits their effectiveness in addressing various types\nof hallucinations exhibited in LLM outputs. To our best knowledge, in this\npaper we propose the first active learning framework to alleviate LLM\nhallucinations, reducing costly human annotations of hallucination needed. By\nmeasuring fine-grained hallucinations from errors in semantic frame, discourse\nand content verifiability in text summarization, we propose HAllucination\nDiversity-Aware Sampling (HADAS) to select diverse hallucinations for\nannotations in active learning for LLM finetuning. Extensive experiments on\nthree datasets and different backbone models demonstrate advantages of our\nmethod in effectively and efficiently mitigating LLM hallucinations.\n']",Large Language Model Hallucinations
68,67,129,67_reinforcement_offline_learning_learned,"['reinforcement', 'offline', 'learning', 'learned', 'reward', 'critic', 'rewards', 'learn', 'exploration', 'rl']","['offline', 'policy', 'online', 'reinforcement', 'policies', 'value', 'conservative', 'actions', 'behavior', 'conservatism']","['reinforcement', 'offline', 'exploration', 'rl', 'regularization', 'datasets', 'gym', 'td3', 'trajectories', 'extrapolation']","['  Deep generative models (DGMs) have demonstrated great success across various\ndomains, particularly in generating texts, images, and videos using models\ntrained from offline data. Similarly, data-driven decision-making and robotic\ncontrol also necessitate learning a generator function from the offline data to\nserve as the strategy or policy. In this case, applying deep generative models\nin offline policy learning exhibits great potential, and numerous studies have\nexplored in this direction. However, this field still lacks a comprehensive\nreview and so developments of different branches are relatively independent. In\nthis paper, we provide the first systematic review on the applications of deep\ngenerative models for offline policy learning. In particular, we cover five\nmainstream deep generative models, including Variational Auto-Encoders,\nGenerative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion\nModels, and their applications in both offline reinforcement learning (offline\nRL) and imitation learning (IL). Offline RL and IL are two main branches of\noffline policy learning and are widely-adopted techniques for sequential\ndecision-making. Notably, for each type of DGM-based offline policy learning,\nwe distill its fundamental scheme, categorize related works based on the usage\nof the DGM, and sort out the development process of algorithms in that field.\nSubsequent to the main content, we provide in-depth discussions on deep\ngenerative models and offline policy learning as a summary, based on which we\npresent our perspectives on future research directions. This work offers a\nhands-on reference for the research progress in deep generative models for\noffline policy learning, and aims to inspire improved DGM-based offline RL or\nIL algorithms. For convenience, we maintain a paper list on\nhttps://github.com/LucasCJYSDL/DGMs-for-Offline-Policy-Learning.\n', '  Despite recent progress in offline learning, these methods are still trained\nand tested on the same environment. In this paper, we compare the\ngeneralization abilities of widely used online and offline learning methods\nsuch as online reinforcement learning (RL), offline RL, sequence modeling, and\nbehavioral cloning. Our experiments show that offline learning algorithms\nperform worse on new environments than online learning ones. We also introduce\nthe first benchmark for evaluating generalization in offline learning,\ncollecting datasets of varying sizes and skill-levels from Procgen (2D video\ngames) and WebShop (e-commerce websites). The datasets contain trajectories for\na limited number of game levels or natural language instructions and at test\ntime, the agent has to generalize to new levels or instructions. Our\nexperiments reveal that existing offline learning algorithms struggle to match\nthe performance of online RL on both train and test environments. Behavioral\ncloning is a strong baseline, outperforming state-of-the-art offline RL and\nsequence modeling approaches when trained on data from multiple environments\nand tested on new ones. Finally, we find that increasing the diversity of the\ndata, rather than its size, improves performance on new environments for all\noffline learning algorithms. Our study demonstrates the limited generalization\nof current offline learning algorithms highlighting the need for more research\nin this area.\n', '  While imitation learning requires access to high-quality data, offline\nreinforcement learning (RL) should, in principle, perform similarly or better\nwith substantially lower data quality by using a value function. However,\ncurrent results indicate that offline RL often performs worse than imitation\nlearning, and it is often unclear what holds back the performance of offline\nRL. Motivated by this observation, we aim to understand the bottlenecks in\ncurrent offline RL algorithms. While poor performance of offline RL is\ntypically attributed to an imperfect value function, we ask: is the main\nbottleneck of offline RL indeed in learning the value function, or something\nelse? To answer this question, we perform a systematic empirical study of (1)\nvalue learning, (2) policy extraction, and (3) policy generalization in offline\nRL problems, analyzing how these components affect performance. We make two\nsurprising observations. First, we find that the choice of a policy extraction\nalgorithm significantly affects the performance and scalability of offline RL,\noften more so than the value learning objective. For instance, we show that\ncommon value-weighted behavioral cloning objectives (e.g., AWR) do not fully\nleverage the learned value function, and switching to behavior-constrained\npolicy gradient objectives (e.g., DDPG+BC) often leads to substantial\nimprovements in performance and scalability. Second, we find that a big barrier\nto improving offline RL performance is often imperfect policy generalization on\ntest-time states out of the support of the training data, rather than policy\nlearning on in-distribution states. We then show that the use of suboptimal but\nhigh-coverage data or test-time policy training techniques can address this\ngeneralization issue in practice. Specifically, we propose two simple test-time\npolicy improvement methods and show that these methods lead to better\nperformance.\n']",Offline Reinforcement Learning
69,68,128,68_predicting_sepsis_mortality_hospital,"['predicting', 'sepsis', 'mortality', 'hospital', 'predict', 'prediction', 'cohorts', 'icu', 'biomarkers', 'healthcare']","['patients', 'patient', 'mortality', 'clinical', 'care', 'sepsis', 'risk', 'healthcare', 'health', 'hospital']","['predicting', 'sepsis', 'mortality', 'hospital', 'cohorts', 'icu', 'biomarkers', 'cvd', 'cardiovascular', 'timely']","[""  Quantifying a patient's health status provides clinicians with insight into\npatient risk, and the ability to better triage and manage resources. Early\nWarning Scores (EWS) are widely deployed to measure overall health status, and\nrisk of adverse outcomes, in hospital patients. However, current EWS are\nlimited both by their lack of personalisation and use of static observations.\nWe propose a pipeline that groups intensive care unit patients by the\ntrajectories of observations data throughout their stay as a basis for the\ndevelopment of personalised risk predictions. Feature importance is considered\nto provide model explainability. Using the MIMIC-IV dataset, six clusters were\nidentified, capturing differences in disease codes, observations, lengths of\nadmissions and outcomes. Applying the pipeline to data from just the first four\nhours of each ICU stay assigns the majority of patients to the same cluster as\nwhen the entire stay duration is considered. In-hospital mortality prediction\nmodels trained on individual clusters had higher F1 score performance in five\nof the six clusters when compared against the unclustered patient cohort. The\npipeline could form the basis of a clinical decision support tool, working to\nimprove the clinical characterisation of risk groups and the early detection of\npatient deterioration.\n"", ""  Sepsis poses a major global health threat, accounting for millions of deaths\nannually and significant economic costs. Accurate predictions of mortality risk\nin sepsis patients facilitate the efficient allocation of medical resources,\nthereby enhancing patient survival and quality of life. Through precise risk\nassessments, healthcare facilities can effectively distribute intensive care\nbeds, medical equipment, and staff, ensuring high-risk patients receive timely\nand appropriate care. Early identification and intervention significantly\ndecrease mortality rates and improve patient outcomes. Current methods\ntypically utilize only one type of data--either constant, temporal, or ICD\ncodes. This study introduces the Time-Constant KAN Integrated Network(TCKIN),\nan innovative model that enhances the accuracy of sepsis mortality risk\npredictions by integrating both temporal and constant data from electronic\nhealth records and ICD codes. Validated against the MIMIC-III and MIMIC-IV\ndatasets, TCKIN surpasses existing machine learning and deep learning methods\nin accuracy, sensitivity, and specificity. Notably, TCKIN achieved AUCs of\n87.76% and 88.07%, demonstrating superior capability in identifying high-risk\npatients. Additionally, TCKIN effectively combats the prevalent issue of data\nimbalance in clinical settings, improving the detection of patients at elevated\nrisk of mortality and facilitating timely interventions. These results confirm\nthe model's effectiveness and its potential to transform patient management and\ntreatment optimization in clinical practice. With this advanced risk assessment\ntool, healthcare providers can devise more tailored treatment plans, optimize\nresource utilization, and ultimately enhance survival rates and quality of life\nfor sepsis patients.\n"", ""  Background: Sepsis is a severe condition responsible for many deaths\nworldwide. Accurate prediction of sepsis outcomes is crucial for timely and\neffective treatment. Although previous studies have used ML to forecast\noutcomes, they faced limitations in feature selection and model\ncomprehensibility, resulting in less effective predictions. Thus, this research\naims to develop an interpretable and accurate ML model to help clinical\nprofessionals predict in-hospital mortality.\n  Methods: We analyzed ICU patient records from the MIMIC-III database based on\nspecific criteria and extracted relevant data. Our feature selection process\nincluded a literature review, clinical input refinement, and using Random\nForest to select the top 35 features. We performed data preprocessing,\nincluding cleaning, imputation, standardization, and applied SMOTE for\noversampling to address imbalance, resulting in 4,683 patients, with admission\ncounts of 17,429. We compared the performance of Random Forest, Gradient\nBoosting, Logistic Regression, SVM, and KNN models.\n  Results: The Random Forest model was the most effective in predicting\nsepsis-related in-hospital mortality. It outperformed other models, achieving\nan accuracy of 0.90 and an AUROC of 0.97, significantly better than the\nexisting literature. Our meticulous feature selection contributed to the\nmodel's precision and identified critical determinants of sepsis mortality.\nThese results underscore the pivotal role of data-driven ML in healthcare,\nespecially for predicting in-hospital mortality due to sepsis.\n  Conclusion: This study represents a significant advancement in predicting\nin-hospital sepsis mortality, highlighting the potential of ML in healthcare.\nThe implications are profound, offering a data-driven approach that enhances\ndecision-making in patient care and reduces in-hospital mortality.\n""]",Predicting Sepsis Outcomes in Hospitals
70,69,128,69_textage_texts_text_autextification,"['textage', 'texts', 'text', 'autextification', 'mixtext', 'ai', 'linguistic', 'gram2vec', 'content', 'writing']","['authorship', 'texts', 'text', 'detectors', 'detection', 'plagiarism', 'writing', 'academic', 'author', 'attribution']","['texts', 'autextification', 'mixtext', 'ai', 'gram2vec', 'authorship', 'plagiarism', 'multilingual', 'disinformation', 'literary']","['  In recent years, there have been significant advancements in the development\nof Large Language Models (LLMs). While their practical applications are now\nwidespread, their potential for misuse, such as generating fake news and\ncommitting plagiarism, has posed significant concerns. To address this issue,\ndetectors have been developed to evaluate whether a given text is\nhuman-generated or AI-generated. Among others, zero-shot detectors stand out as\neffective approaches that do not require additional training data and are often\nlikelihood-based. In chat-based applications, users commonly input prompts and\nutilize the AI-generated texts. However, zero-shot detectors typically analyze\nthese texts in isolation, neglecting the impact of the original prompts. It is\nconceivable that this approach may lead to a discrepancy in likelihood\nassessments between the text generation phase and the detection phase. So far,\nthere remains an unverified gap concerning how the presence or absence of\nprompts impacts detection accuracy for zero-shot detectors. In this paper, we\nintroduce an evaluative framework to empirically analyze the impact of prompts\non the detection accuracy of AI-generated text. We assess various zero-shot\ndetectors using both white-box detection, which leverages the prompt, and\nblack-box detection, which operates without prompt information. Our experiments\nreveal the significant influence of prompts on detection accuracy. Remarkably,\ncompared with black-box detection without prompts, the white-box methods using\nprompts demonstrate an increase in AUC of at least $0.1$ across all zero-shot\ndetectors tested. Code is available:\n\\url{https://github.com/kaito25atugich/Detector}.\n', ""  With the increasing prevalence of text generated by large language models\n(LLMs), there is a growing concern about distinguishing between LLM-generated\nand human-written texts in order to prevent the misuse of LLMs, such as the\ndissemination of misleading information and academic dishonesty. Previous\nresearch has primarily focused on classifying text as either entirely\nhuman-written or LLM-generated, neglecting the detection of mixed texts that\ncontain both types of content. This paper explores LLMs' ability to identify\nboundaries in human-written and machine-generated mixed texts. We approach this\ntask by transforming it into a token classification problem and regard the\nlabel turning point as the boundary. Notably, our ensemble model of LLMs\nachieved first place in the 'Human-Machine Mixed Text Detection' sub-task of\nthe SemEval'24 Competition Task 8. Additionally, we investigate factors that\ninfluence the capability of LLMs in detecting boundaries within mixed texts,\nincluding the incorporation of extra layers on top of LLMs, combination of\nsegmentation loss, and the impact of pretraining. Our findings aim to provide\nvaluable insights for future research in this area.\n"", ""  This study explores the challenge of sentence-level AI-generated text\ndetection within human-AI collaborative hybrid texts. Existing studies of\nAI-generated text detection for hybrid texts often rely on synthetic datasets.\nThese typically involve hybrid texts with a limited number of boundaries. We\ncontend that studies of detecting AI-generated content within hybrid texts\nshould cover different types of hybrid texts generated in realistic settings to\nbetter inform real-world applications. Therefore, our study utilizes the\nCoAuthor dataset, which includes diverse, realistic hybrid texts generated\nthrough the collaboration between human writers and an intelligent writing\nsystem in multi-turn interactions. We adopt a two-step, segmentation-based\npipeline: (i) detect segments within a given hybrid text where each segment\ncontains sentences of consistent authorship, and (ii) classify the authorship\nof each identified segment. Our empirical findings highlight (1) detecting\nAI-generated sentences in hybrid texts is overall a challenging task because\n(1.1) human writers' selecting and even editing AI-generated sentences based on\npersonal preferences adds difficulty in identifying the authorship of segments;\n(1.2) the frequent change of authorship between neighboring sentences within\nthe hybrid text creates difficulties for segment detectors in identifying\nauthorship-consistent segments; (1.3) the short length of text segments within\nhybrid texts provides limited stylistic cues for reliable authorship\ndetermination; (2) before embarking on the detection process, it is beneficial\nto assess the average length of segments within the hybrid text. This\nassessment aids in deciding whether (2.1) to employ a text segmentation-based\nstrategy for hybrid texts with longer segments, or (2.2) to adopt a direct\nsentence-by-sentence classification strategy for those with shorter segments.\n""]",Detecting AI-Generated Text
71,70,127,70_quantization_quantizing_quantize_quantized,"['quantization', 'quantizing', 'quantize', 'quantized', 'memory', 'compression', 'weights', 'compressing', 'bits', 'throughput']","['quantization', 'bit', 'weight', 'weights', 'precision', 'memory', 'bits', 'activation', 'outliers', 'low']","['quantization', 'memory', 'compression', 'precision', 'speedup', 'gptq', 'bitnet', 'llms', 'finetuning', 'qjl']","[""  The growing demand for Large Language Models (LLMs) in applications such as\ncontent generation, intelligent chatbots, and sentiment analysis poses\nconsiderable challenges for LLM service providers. To efficiently use GPU\nresources and boost throughput, batching multiple requests has emerged as a\npopular paradigm; to further speed up batching, LLM quantization techniques\nreduce memory consumption and increase computing capacity. However, prevalent\nquantization schemes (e.g., 8-bit weight-activation quantization) cannot fully\nleverage the capabilities of modern GPUs, such as 4-bit integer operators,\nresulting in sub-optimal performance.\n  To maximize LLMs' serving throughput, we introduce Atom, a low-bit\nquantization method that achieves high throughput improvements with negligible\naccuracy loss. Atom significantly boosts serving throughput by using low-bit\noperators and considerably reduces memory consumption via low-bit quantization.\nIt attains high accuracy by applying a novel mixed-precision and fine-grained\nquantization process. We evaluate Atom on 4-bit weight-activation quantization\nin the serving context. Atom improves end-to-end throughput (token/s) by up to\n$7.7\\times$ compared to the FP16 and by $2.5\\times$ compared to INT8\nquantization, while maintaining the same latency target.\n"", ""  Due to the high memory and computational costs associated with Large Language\nModels, model compression via quantization and parameter-efficient fine-tuning\n(PEFT) methods, such as low-rank adaptation (LoRA), are gaining popularity.\nThis has led to active research on quantization-aware PEFT techniques, which\naim to create models with high accuracy and low memory overhead. Among\nquantization methods, post-training quantization (PTQ) is more commonly used in\nprevious works than quantization-aware training (QAT), despite QAT's potential\nfor higher accuracy. This preference is due to PTQ's low training overhead.\nHowever, PTQ-based PEFT methods often utilize high-precision parameters, making\nit difficult to fully exploit the efficiency of quantization. Additionally,\nthey have limited adaptation ability due to a reduced and constrained LoRA\nparameter structure. To overcome these challenges, we propose L4Q, which\nleverages joint quantization and fine-tuning to reduce QAT's memory overhead\nand produce models that consist entirely of quantized weights while achieving\neffective adaptation to downstream tasks. By design, L4Q allows quantization\nparameters to reflect weight updates, while weight updates reduce quantization\nerrors. Our experiments demonstrate that this coupled quantization and\nfine-tuning approach yields superior accuracy compared to decoupled fine-tuning\nschemes in sub-4-bit quantization. Using the LLaMA model families and\ninstructional datasets, we showcase L4Q's capabilities in language tasks and\nfew-shot in-context learning.\n"", '  Quantization has emerged as a promising technique for improving the memory\nand computational efficiency of large language models (LLMs). Though the\ntrade-off between performance and efficiency is well-known, there is still much\nto be learned about the relationship between quantization and LLM performance.\nTo shed light on this relationship, we propose a new perspective on\nquantization, viewing it as perturbations added to the weights and activations\nof LLMs. We call this approach ""the lens of perturbation"". Using this lens, we\nconduct experiments with various artificial perturbations to explore their\nimpact on LLM performance. Our findings reveal several connections between the\nproperties of perturbations and LLM performance, providing insights into the\nfailure cases of uniform quantization and suggesting potential solutions to\nimprove the robustness of LLM quantization. To demonstrate the significance of\nour findings, we implement a simple non-uniform quantization approach based on\nour insights. Our experiments show that this approach achieves minimal\nperformance degradation on both 4-bit weight quantization and 8-bit\nquantization for weights and activations. These results validate the\ncorrectness of our approach and highlight its potential to improve the\nefficiency of LLMs without sacrificing performance.\n']",Quantization Techniques for Efficient Large Language Models
72,71,125,71_pruning_prune_cnn_pruned,"['pruning', 'prune', 'cnn', 'pruned', 'cnns', 'imagenet', 'rnns', 'networks', 'neural', 'efficient']","['pruning', 'sparsity', 'compression', 'networks', 'network', 'neural', 'layers', 'structured', 'accuracy', 'computational']","['pruning', 'cnns', 'imagenet', 'rnns', 'compression', 'dnns', 'neurons', 'training', 'layers', 'convolutional']","['  Deep neural networks (DNNs) have demonstrated remarkable success in various\nfields. However, the large number of floating-point operations (FLOPs) in DNNs\nposes challenges for their deployment in resource-constrained applications,\ne.g., edge devices. To address the problem, pruning has been introduced to\nreduce the computational cost in executing DNNs. Previous pruning strategies\nare based on weight values, gradient values and activation outputs. Different\nfrom previous pruning solutions, in this paper, we propose a class-aware\npruning technique to compress DNNs, which provides a novel perspective to\nreduce the computational cost of DNNs. In each iteration, the neural network\ntraining is modified to facilitate the class-aware pruning. Afterwards, the\nimportance of filters with respect to the number of classes is evaluated. The\nfilters that are only important for a few number of classes are removed. The\nneural network is then retrained to compensate for the incurred accuracy loss.\nThe pruning iterations end until no filter can be removed anymore, indicating\nthat the remaining filters are very important for many classes. This pruning\ntechnique outperforms previous pruning solutions in terms of accuracy, pruning\nratio and the reduction of FLOPs. Experimental results confirm that this\nclass-aware pruning technique can significantly reduce the number of weights\nand FLOPs, while maintaining a high inference accuracy.\n', '  Modern deep neural networks, particularly recent large language models, come\nwith massive model sizes that require significant computational and storage\nresources. To enable the deployment of modern models on resource-constrained\nenvironments and accelerate inference time, researchers have increasingly\nexplored pruning techniques as a popular research direction in neural network\ncompression. However, there is a dearth of up-to-date comprehensive review\npapers on pruning. To address this issue, in this survey, we provide a\ncomprehensive review of existing research works on deep neural network pruning\nin a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to\nprune, and 4) fusion of pruning and other compression techniques. We then\nprovide a thorough comparative analysis of eight pairs of contrast settings for\npruning and explore emerging topics, including pruning for large language\nmodels, large multimodal models, post-training pruning, and different\nsupervision levels for pruning to shed light on the commonalities and\ndifferences of existing methods and lay the foundation for further method\ndevelopment. To facilitate future research, we build a curated collection of\ndatasets, networks, and evaluations on different applications. Finally, we\nprovide valuable recommendations on selecting pruning methods and prospect\nseveral promising research directions. We build a repository at\nhttps://github.com/hrcheng1066/awesome-pruning.\n', ""  With the increasing size of datasets used for training neural networks, data\npruning becomes an attractive field of research. However, most current data\npruning algorithms are limited in their ability to preserve accuracy compared\nto models trained on the full data, especially in high pruning regimes. In this\npaper we explore the application of data pruning while incorporating knowledge\ndistillation (KD) when training on a pruned subset. That is, rather than\nrelying solely on ground-truth labels, we also use the soft predictions from a\nteacher network pre-trained on the complete data. By integrating KD into\ntraining, we demonstrate significant improvement across datasets, pruning\nmethods, and on all pruning fractions. We first establish a theoretical\nmotivation for employing self-distillation to improve training on pruned data.\nThen, we empirically make a compelling and highly practical observation: using\nKD, simple random pruning is comparable or superior to sophisticated pruning\nmethods across all pruning regimes. On ImageNet for example, we achieve\nsuperior accuracy despite training on a random subset of only 50% of the data.\nAdditionally, we demonstrate a crucial connection between the pruning factor\nand the optimal knowledge distillation weight. This helps mitigate the impact\nof samples with noisy labels and low-quality images retained by typical pruning\nalgorithms. Finally, we make an intriguing observation: when using lower\npruning fractions, larger teachers lead to accuracy degradation, while\nsurprisingly, employing teachers with a smaller capacity than the student's may\nimprove results. Our code will be made available.\n""]",Neural Network Pruning Techniques
73,72,123,72_flow_diffusion_diffusions_generative,"['flow', 'diffusion', 'diffusions', 'generative', 'flows', 'stochastic', 'likelihood', 'sde', 'sampling', 'modeling']","['diffusion', 'flow', 'matching', 'sampling', 'sampler', 'score', 'distributions', 'stochastic', 'flows', 'generative']","['flow', 'diffusion', 'stochastic', 'likelihood', 'modeling', 'denoising', 'sdes', 'variational', 'sampler', 'mcmc']","['  Diffusion models, which convert noise into new data instances by learning to\nreverse a diffusion process, have become a cornerstone in contemporary\ngenerative modeling. In this work, we develop non-asymptotic convergence theory\nfor a popular diffusion-based sampler (i.e., the probability flow ODE sampler)\nin discrete time, assuming access to $\\ell_2$-accurate estimates of the (Stein)\nscore functions. For distributions in $\\mathbb{R}^d$, we prove that\n$d/\\varepsilon$ iterations -- modulo some logarithmic and lower-order terms --\nare sufficient to approximate the target distribution to within $\\varepsilon$\ntotal-variation distance. This is the first result establishing nearly linear\ndimension-dependency (in $d$) for the probability flow ODE sampler. Imposing\nonly minimal assumptions on the target data distribution (e.g., no smoothness\nassumption is imposed), our results also characterize how $\\ell_2$ score\nestimation errors affect the quality of the data generation processes. In\ncontrast to prior works, our theory is developed based on an elementary yet\nversatile non-asymptotic approach without the need of resorting to SDE and ODE\ntoolboxes.\n', '  Diffusion models, which convert noise into new data instances by learning to\nreverse a Markov diffusion process, have become a cornerstone in contemporary\ngenerative modeling. While their practical power has now been widely\nrecognized, the theoretical underpinnings remain far from mature. In this work,\nwe develop a suite of non-asymptotic theory towards understanding the data\ngeneration process of diffusion models in discrete time, assuming access to\n$\\ell_2$-accurate estimates of the (Stein) score functions. For a popular\ndeterministic sampler (based on the probability flow ODE), we establish a\nconvergence rate proportional to $1/T$ (with $T$ the total number of steps),\nimproving upon past results; for another mainstream stochastic sampler (i.e., a\ntype of the denoising diffusion probabilistic model), we derive a convergence\nrate proportional to $1/\\sqrt{T}$, matching the state-of-the-art theory.\nImposing only minimal assumptions on the target data distribution (e.g., no\nsmoothness assumption is imposed), our results characterize how $\\ell_2$ score\nestimation errors affect the quality of the data generation processes. In\ncontrast to prior works, our theory is developed based on an elementary yet\nversatile non-asymptotic approach without resorting to toolboxes for SDEs and\nODEs. Further, we design two accelerated variants, improving the convergence to\n$1/T^2$ for the ODE-based sampler and $1/T$ for the DDPM-type sampler, which\nmight be of independent theoretical and empirical interest.\n', '  Score-based generative models are a popular class of generative modelling\ntechniques relying on stochastic differential equations (SDE). From their\ninception, it was realized that it was also possible to perform generation\nusing ordinary differential equations (ODE) rather than SDE. This led to the\nintroduction of the probability flow ODE approach and denoising diffusion\nimplicit models. Flow matching methods have recently further extended these\nODE-based approaches and approximate a flow between two arbitrary probability\ndistributions. Previous work derived bounds on the approximation error of\ndiffusion models under the stochastic sampling regime, given assumptions on the\n$L^2$ loss. We present error bounds for the flow matching procedure using fully\ndeterministic sampling, assuming an $L^2$ bound on the approximation error and\na certain regularity condition on the data distributions.\n']",Diffusion Models for Generative Modeling
74,73,121,73_communities_hypergraph_graphs_subgraph,"['communities', 'hypergraph', 'graphs', 'subgraph', 'adjacency', 'networks', 'nodes', 'hypergraphs', 'cluster', 'clustering']","['community', 'communities', 'modularity', 'clustering', 'vertices', 'graphs', 'nodes', 'graph', 'networks', 'node']","['communities', 'hypergraph', 'adjacency', 'nodes', 'clustering', 'pagerank', 'clique', 'empirical', 'edge', 'links']","['  Random walks play an important role in probing the structure of complex\nnetworks. On traditional networks, they can be used to extract community\nstructure, understand node centrality, perform link prediction, or capture the\nsimilarity between nodes. On signed networks, where the edge weights can be\neither positive or negative, it is non-trivial to design a random walk which\ncan be used to extract information about the signed structure of the network,\nin particular the ability to partition the graph into communities with positive\nedges inside and negative edges in between. Prior works on signed network\nrandom walks focus on the case where there are only two such communities\n(strong balance), which is rarely the case in empirical networks. In this\npaper, we propose a signed network random walk which can capture the structure\nof a network with more than two such communities (weak balance). The walk\nresults in a similarity matrix which can be used to cluster the nodes into\nantagonistic communities. We compare the characteristics of the so-called\nstrong and weak random walks, in terms of walk length and stationarity. We show\nthrough a series of experiments on synthetic and empirical networks that the\nsimilarity matrix based on weak walks can be used for both unsupervised and\nsemi-supervised clustering, outperforming the same similarity matrix based on\nstrong walks when the graph has more than two communities, or exhibits\nasymmetry in the density of links. These results suggest that other random-walk\nbased algorithms for signed networks could be improved simply by running them\nwith weak walks instead of strong walks.\n', '  Graph clustering is an important unsupervised learning technique for\npartitioning graphs with attributes and detecting communities. However, current\nmethods struggle to accurately capture true community structures and\nintra-cluster relations, be computationally efficient, and identify smaller\ncommunities. We address these challenges by integrating coarsening and\nmodularity maximization, effectively leveraging both adjacency and node\nfeatures to enhance clustering accuracy. We propose a loss function\nincorporating log-determinant, smoothness, and modularity components using a\nblock majorization-minimization technique, resulting in superior clustering\noutcomes. The method is theoretically consistent under the Degree-Corrected\nStochastic Block Model (DC-SBM), ensuring asymptotic error-free performance and\ncomplete label recovery. Our provably convergent and time-efficient algorithm\nseamlessly integrates with graph neural networks (GNNs) and variational graph\nautoencoders (VGAEs) to learn enhanced node features and deliver exceptional\nclustering performance. Extensive experiments on benchmark datasets demonstrate\nits superiority over existing state-of-the-art methods for both attributed and\nnon-attributed graphs.\n', '  The study of complex networks has significantly advanced our understanding of\ncommunity structures which serves as a crucial feature of real-world graphs.\nDetecting communities in graphs is a challenging problem with applications in\nsociology, biology, and computer science. Despite the efforts of an\ninterdisciplinary community of scientists, a satisfactory solution to this\nproblem has not yet been achieved. This review article delves into the topic of\ncommunity detection in graphs, which serves as a thorough exposition of various\ncommunity detection methods from perspectives of modularity-based method,\nspectral clustering, probabilistic modelling, and deep learning. Along with the\nmethods, a new community detection method designed by us is also presented.\nAdditionally, the performance of these methods on the datasets with and without\nground truth is compared. In conclusion, this comprehensive review provides a\ndeep understanding of community detection in graphs.\n']",Community Detection in Networks
75,74,120,74_planning_routes_metaheuristics_salesman,"['planning', 'routes', 'metaheuristics', 'salesman', 'optimization', 'metaheuristic', 'scheduling', 'knapsack', 'heuristics', 'optimal']","['routing', 'vehicle', 'scheduling', 'inventory', 'salesman', 'solutions', 'problems', 'combinatorial', 'problem', 'instances']","['routes', 'metaheuristics', 'salesman', 'scheduling', 'knapsack', 'reinforcement', 'vrp', 'tsps', 'depot', 'logistics']","['  Machine learning has been adapted to help solve NP-hard combinatorial\noptimization problems. One prevalent way is learning to construct solutions by\ndeep neural networks, which has been receiving more and more attention due to\nthe high efficiency and less requirement for expert knowledge. However, many\nneural construction methods for Vehicle Routing Problems~(VRPs) focus on\nsynthetic problem instances with specified node distributions and limited\nscales, leading to poor performance on real-world problems which usually\ninvolve complex and unknown node distributions together with large scales. To\nmake neural VRP solvers more practical, we design an auxiliary policy that\nlearns from the local transferable topological features, named local policy,\nand integrate it with a typical construction policy (which learns from the\nglobal information of VRP instances) to form an ensemble policy. With joint\ntraining, the aggregated policies perform cooperatively and complementarily to\nboost generalization. The experimental results on two well-known benchmarks,\nTSPLIB and CVRPLIB, of travelling salesman problem and capacitated VRP show\nthat the ensemble policy significantly improves both cross-distribution and\ncross-scale generalization performance, and even performs well on real-world\nproblems with several thousand nodes.\n', '  The end-to-end neural combinatorial optimization (NCO) method shows promising\nperformance in solving complex combinatorial optimization problems without the\nneed for expert design. However, existing methods struggle with large-scale\nproblems, hindering their practical applicability. To overcome this limitation,\nthis work proposes a novel Self-Improved Learning (SIL) method for better\nscalability of neural combinatorial optimization. Specifically, we develop an\nefficient self-improved mechanism that enables direct model training on\nlarge-scale problem instances without any labeled data. Powered by an\ninnovative local reconstruction approach, this method can iteratively generate\nbetter solutions by itself as pseudo-labels to guide efficient model training.\nIn addition, we design a linear complexity attention mechanism for the model to\nefficiently handle large-scale combinatorial problem instances with low\ncomputation overhead. Comprehensive experiments on the Travelling Salesman\nProblem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to\n100K nodes in both uniform and real-world distributions demonstrate the\nsuperior scalability of our method.\n', '  The neural combinatorial optimization (NCO) approach has shown great\npotential for solving routing problems without the requirement of expert\nknowledge. However, existing constructive NCO methods cannot directly solve\nlarge-scale instances, which significantly limits their application prospects.\nTo address these crucial shortcomings, this work proposes a novel\nInstance-Conditioned Adaptation Model (ICAM) for better large-scale\ngeneralization of neural combinatorial optimization. In particular, we design a\npowerful yet lightweight instance-conditioned adaptation module for the NCO\nmodel to generate better solutions for instances across different scales. In\naddition, we develop an efficient three-stage reinforcement learning-based\ntraining scheme that enables the model to learn cross-scale features without\nany labeled optimal solution. Experimental results show that our proposed\nmethod is capable of obtaining excellent results with a very fast inference\ntime in solving Traveling Salesman Problems (TSPs) and Capacitated Vehicle\nRouting Problems (CVRPs) across different scales. To the best of our knowledge,\nour model achieves state-of-the-art performance among all RL-based constructive\nmethods for TSP and CVRP with up to 1,000 nodes.\n']",Neural Combinatorial Optimization for Routing Problems
76,75,120,75_convnets_cnn_imagenet_cnns,"['convnets', 'cnn', 'imagenet', 'cnns', 'convolutions', 'attention', 'convolutional', 'vision', 'visual', 'convolution']","['vision', 'attention', 'transformers', 'transformer', 'segmentation', 'convolution', 'computer', 'pruning', 'architectures', 'image']","['convnets', 'imagenet', 'attention', 'pruning', 'vitscore', 'upsampling', 'transformer', 'tokens', 'coco', 'dct']","['  Hierarchical vision transformers (ViTs) have two advantages over conventional\nViTs. First, hierarchical ViTs achieve linear computational complexity with\nrespect to image size by local self-attention. Second, hierarchical ViTs create\nhierarchical feature maps by merging image patches in deeper layers for dense\nprediction. However, existing pruning methods ignore the unique properties of\nhierarchical ViTs and use the magnitude value as the weight importance. This\napproach leads to two main drawbacks. First, the ""local"" attention weights are\ncompared at a ""global"" level, which may cause some ""locally"" important weights\nto be pruned due to their relatively small magnitude ""globally"". The second\nissue with magnitude pruning is that it fails to consider the distinct weight\ndistributions of the network, which are essential for extracting coarse to\nfine-grained features at various hierarchical levels.\n  To solve the aforementioned issues, we have developed a Data-independent\nModule-Aware Pruning method (DIMAP) to compress hierarchical ViTs. To ensure\nthat ""local"" attention weights at different hierarchical levels are compared\nfairly in terms of their contribution, we treat them as a module and examine\ntheir contribution by analyzing their information distortion. Furthermore, we\nintroduce a novel weight metric that is solely based on weights and does not\nrequire input images, thereby eliminating the dependence on the patch merging\nprocess. Our method validates its usefulness and strengths on Swin Transformers\nof different sizes on ImageNet-1k classification. Notably, the top-5 accuracy\ndrop is only 0.07% when we remove 52.5% FLOPs and 52.7% parameters of Swin-B.\nWhen we reduce 33.2% FLOPs and 33.2% parameters of Swin-S, we can even achieve\na 0.8% higher relative top-5 accuracy than the original model. Code is\navailable at: https://github.com/he-y/Data-independent-Module-Aware-Pruning\n', '  Vision Transformer (ViT) has emerged as a prominent architecture for various\ncomputer vision tasks. In ViT, we divide the input image into patch tokens and\nprocess them through a stack of self attention blocks. However, unlike\nConvolutional Neural Networks (CNN), ViTs simple architecture has no\ninformative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a\nlarge amount of data for pre-training. Various data efficient approaches (DeiT)\nhave been proposed to train ViT on balanced datasets effectively. However,\nlimited literature discusses the use of ViT for datasets with long-tailed\nimbalances. In this work, we introduce DeiT-LT to tackle the problem of\ntraining ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an\nefficient and effective way of distillation from CNN via distillation DIST\ntoken by using out-of-distribution images and re-weighting the distillation\nloss to enhance focus on tail classes. This leads to the learning of local\nCNN-like features in early ViT blocks, improving generalization for tail\nclasses. Further, to mitigate overfitting, we propose distilling from a flat\nCNN teacher, which leads to learning low-rank generalizable features for DIST\ntokens across all ViT blocks. With the proposed DeiT-LT scheme, the\ndistillation DIST token becomes an expert on the tail classes, and the\nclassifier CLS token becomes an expert on the head classes. The experts help to\neffectively learn features corresponding to both the majority and minority\nclasses using a distinct set of tokens within the same ViT architecture. We\nshow the effectiveness of DeiT-LT for training ViT from scratch on datasets\nranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018.\n', '  Attention-based vision models, such as Vision Transformer (ViT) and its\nvariants, have shown promising performance in various computer vision tasks.\nHowever, these emerging architectures suffer from large model sizes and high\ncomputational costs, calling for efficient model compression solutions. To\ndate, pruning ViTs has been well studied, while other compression strategies\nthat have been widely applied in CNN compression, e.g., model factorization, is\nlittle explored in the context of ViT compression. This paper explores an\nefficient method for compressing vision transformers to enrich the toolset for\nobtaining compact attention-based vision models. Based on the new insight on\nthe multi-head attention layer, we develop a highly efficient ViT compression\nsolution, which outperforms the state-of-the-art pruning methods. For\ncompressing DeiT-small and DeiT-base models on ImageNet, our proposed approach\ncan achieve 0.45% and 0.76% higher top-1 accuracy even with fewer parameters.\nOur finding can also be applied to improve the customization efficiency of\ntext-to-image diffusion models, with much faster training (up to $2.6\\times$\nspeedup) and lower extra storage cost (up to $1927.5\\times$ reduction) than the\nexisting works.\n']",Vision Transformers and Efficient CNN Architectures
77,76,114,76_adversarial_adaptation_supervised_domainnet,"['adversarial', 'adaptation', 'supervised', 'domainnet', 'regularization', 'learn', 'classification', 'unsupervised', 'feature', 'features']","['domain', 'adaptation', 'target', 'source', 'domains', 'unlabeled', 'invariant', 'unsupervised', 'shifts', 'pseudo']","['adversarial', 'adaptation', 'domainnet', 'regularization', 'extractor', 'ddrep', 'uda', 'divergence', 'benchmarks', 'visda']","['  Prior Unsupervised Domain Adaptation (UDA) methods often aim to train a\ndomain-invariant feature extractor, which may hinder the model from learning\nsufficiently discriminative features. To tackle this, a line of works based on\nprompt learning leverages the power of large-scale pre-trained vision-language\nmodels to learn both domain-invariant and specific features through a set of\ndomain-agnostic and domain-specific learnable prompts. Those studies typically\nenforce invariant constraints on representation, output, or prompt space to\nlearn such prompts. Differently, we cast UDA as a multiple-objective\noptimization problem in which each objective is represented by a domain loss.\nUnder this new framework, we propose aligning per-objective gradients to foster\nconsensus between them. Additionally, to prevent potential overfitting when\nfine-tuning this deep learning architecture, we penalize the norm of these\ngradients. To achieve these goals, we devise a practical gradient update\nprocedure that can work under both single-source and multi-source UDA.\nEmpirically, our method consistently surpasses other prompt-based baselines by\na large margin on different UDA benchmarks\n', '  Unsupervised domain adaptation (UDA) aims to transfer knowledge from a\nlabeled source domain to an unlabeled target domain. The most recent UDA\nmethods always resort to adversarial training to yield state-of-the-art results\nand a dominant number of existing UDA methods employ convolutional neural\nnetworks (CNNs) as feature extractors to learn domain invariant features.\nVision transformer (ViT) has attracted tremendous attention since its emergence\nand has been widely used in various computer vision tasks, such as image\nclassification, object detection, and semantic segmentation, yet its potential\nin adversarial domain adaptation has never been investigated. In this paper, we\nfill this gap by employing the ViT as the feature extractor in adversarial\ndomain adaptation. Moreover, we empirically demonstrate that ViT can be a\nplug-and-play component in adversarial domain adaptation, which means directly\nreplacing the CNN-based feature extractor in existing UDA methods with the\nViT-based feature extractor can easily obtain performance improvement. The code\nis available at https://github.com/LluckyYH/VT-ADA.\n', ""  Unsupervised domain adaptation (UDA) has achieved remarkable success in fault\ndiagnosis, bringing significant benefits to diverse industrial applications.\nWhile most UDA methods focus on cross-working condition scenarios where the\nsource and target domains are notably similar, real-world applications often\ngrapple with severe domain shifts. We coin the term `distant domain adaptation\nproblem' to describe the challenge of adapting from a labeled source domain to\na significantly disparate unlabeled target domain. This problem exhibits the\nrisk of negative transfer, where extraneous knowledge from the source domain\nadversely affects the target domain performance. Unfortunately, conventional\nUDA methods often falter in mitigating this negative transfer, leading to\nsuboptimal performance. In response to this challenge, we propose a novel\nOnline Selective Adversarial Alignment (OSAA) approach. Central to OSAA is its\nability to dynamically identify and exclude distant source samples via an\nonline gradient masking approach, focusing primarily on source samples that\nclosely resemble the target samples. Furthermore, recognizing the inherent\ncomplexities in bridging the source and target domains, we construct an\nintermediate domain to act as a transitional domain and ease the adaptation\nprocess. Lastly, we develop a class-conditional adversarial adaptation to\naddress the label distribution disparities while learning domain invariant\nrepresentation to account for potential label distribution disparities between\nthe domains. Through detailed experiments and ablation studies on two\nreal-world datasets, we validate the superior performance of the OSAA method\nover state-of-the-art methods, underscoring its significant utility in\npractical scenarios with severe domain shifts.\n""]",Adversarial Domain Adaptation
78,77,110,77_agriculture_agricultural_crops_farms,"['agriculture', 'agricultural', 'crops', 'farms', 'crop', 'farming', 'cnn', 'livestock', 'classification', 'tomatoes']","['species', 'agricultural', 'agriculture', 'plant', 'monitoring', 'crop', 'weed', 'images', 'animal', 'crops']","['crop', 'cnn', 'livestock', 'tomatoes', 'classifying', 'dataset', 'vision', 'ecosystems', 'soybean', 'yolov5']","['  In recent years, precision agriculture has gradually oriented farming closer\nto automation processes to support all the activities related to field\nmanagement. Service robotics plays a predominant role in this evolution by\ndeploying autonomous agents that can navigate fields while performing tasks\nsuch as monitoring, spraying, and harvesting without human intervention. To\nexecute these precise actions, mobile robots need a real-time perception system\nthat understands their surroundings and identifies their targets in the wild.\nExisting methods, however, often fall short in generalizing to new crops and\nenvironmental conditions. This limit is critical for practical applications\nwhere labeled samples are rarely available. In this paper, we investigate the\nproblem of crop segmentation and propose a novel approach to enhance domain\ngeneralization using knowledge distillation. In the proposed framework, we\ntransfer knowledge from a standardized ensemble of models individually trained\non source domains to a student model that can adapt to unseen realistic\nscenarios. To support the proposed method, we present a synthetic multi-domain\ndataset for crop segmentation containing plants of variegate species and\ncovering different terrain styles, weather conditions, and light scenarios for\nmore than 70,000 samples. We demonstrate significant improvements in\nperformance over state-of-the-art methods and superior sim-to-real\ngeneralization. Our approach provides a promising solution for domain\ngeneralization in crop segmentation and has the potential to enhance a wide\nvariety of agriculture applications.\n', '  Large models can play important roles in many domains. Agriculture is another\nkey factor affecting the lives of people around the world. It provides food,\nfabric, and coal for humanity. However, facing many challenges such as pests\nand diseases, soil degradation, global warming, and food security, how to\nsteadily increase the yield in the agricultural sector is a problem that humans\nstill need to solve. Large models can help farmers improve production\nefficiency and harvest by detecting a series of agricultural production tasks\nsuch as pests and diseases, soil quality, and seed quality. It can also help\nfarmers make wise decisions through a variety of information, such as images,\ntext, etc. Herein, we delve into the potential applications of large models in\nagriculture, from large language model (LLM) and large vision model (LVM) to\nlarge vision-language models (LVLM). After gaining a deeper understanding of\nmultimodal large language models (MLLM), it can be recognized that problems\nsuch as agricultural image processing, agricultural question answering systems,\nand agricultural machine automation can all be solved by large models. Large\nmodels have great potential in the field of agriculture. We outline the current\napplications of agricultural large models, and aims to emphasize the importance\nof large models in the domain of agriculture. In the end, we envisage a future\nin which famers use MLLM to accomplish many tasks in agriculture, which can\ngreatly improve agricultural production efficiency and yield.\n', ""  We present a specialized procedural model for generating synthetic\nagricultural scenes, focusing on soybean crops, along with various weeds. This\nmodel is capable of simulating distinct growth stages of these plants, diverse\nsoil conditions, and randomized field arrangements under varying lighting\nconditions. The integration of real-world textures and environmental factors\ninto the procedural generation process enhances the photorealism and\napplicability of the synthetic data. Our dataset includes 12,000 images with\nsemantic labels, offering a comprehensive resource for computer vision tasks in\nprecision agriculture, such as semantic segmentation for autonomous weed\ncontrol. We validate our model's effectiveness by comparing the synthetic data\nagainst real agricultural images, demonstrating its potential to significantly\naugment training data for machine learning models in agriculture. This approach\nnot only provides a cost-effective solution for generating high-quality,\ndiverse data but also addresses specific needs in agricultural vision tasks\nthat are not fully covered by general-purpose models.\n""]",Agricultural Technology and Crop Management
79,78,110,78_captioning_captions_caption_multimodal,"['captioning', 'captions', 'caption', 'multimodal', 'embeddings', 'embedding', 'visual', 'images', 'retrieval', 'textual']","['captions', 'image', 'retrieval', 'captioning', 'text', 'caption', 'modal', 'images', 'visual', 'query']","['captions', 'multimodal', 'embeddings', 'retrieval', 'encoder', 'coco', 'recap', 'vecap', 'flickr30k', 'modal']","['  This paper proposes a novel zero-shot composed image retrieval (CIR) method\nconsidering the query-target relationship by masked image-text pairs. The\nobjective of CIR is to retrieve the target image using a query image and a\nquery text. Existing methods use a textual inversion network to convert the\nquery image into a pseudo word to compose the image and text and use a\npre-trained visual-language model to realize the retrieval. However, they do\nnot consider the query-target relationship to train the textual inversion\nnetwork to acquire information for retrieval. In this paper, we propose a novel\nzero-shot CIR method that is trained end-to-end using masked image-text pairs.\nBy exploiting the abundant image-text pairs that are convenient to obtain with\na masking strategy for learning the query-target relationship, it is expected\nthat accurate zero-shot CIR using a retrieval-focused textual inversion network\ncan be realized. Experimental results show the effectiveness of the proposed\nmethod.\n', '  Effectively aligning with human judgment when evaluating machine-generated\nimage captions represents a complex yet intriguing challenge. Existing\nevaluation metrics like CIDEr or CLIP-Score fall short in this regard as they\ndo not take into account the corresponding image or lack the capability of\nencoding fine-grained details and penalizing hallucinations. To overcome these\nissues, in this paper, we propose BRIDGE, a new learnable and reference-free\nimage captioning metric that employs a novel module to map visual features into\ndense vectors and integrates them into multi-modal pseudo-captions which are\nbuilt during the evaluation process. This approach results in a multimodal\nmetric that properly incorporates information from the input image without\nrelying on reference captions, bridging the gap between human judgment and\nmachine-generated image captions. Experiments spanning several datasets\ndemonstrate that our proposal achieves state-of-the-art results compared to\nexisting reference-free evaluation scores. Our source code and trained models\nare publicly available at: https://github.com/aimagelab/bridge-score.\n', '  Training image captioning models using teacher forcing results in very\ngeneric samples, whereas more distinctive captions can be very useful in\nretrieval applications or to produce alternative texts describing images for\naccessibility. Reinforcement Learning (RL) allows to use cross-modal retrieval\nsimilarity score between the generated caption and the input image as reward to\nguide the training, leading to more distinctive captions. Recent studies show\nthat pre-trained cross-modal retrieval models can be used to provide this\nreward, completely eliminating the need for reference captions. However, we\nargue in this paper that Ground Truth (GT) captions can still be useful in this\nRL framework. We propose a new image captioning model training strategy that\nmakes use of GT captions in different ways. Firstly, they can be used to train\na simple MLP discriminator that serves as a regularization to prevent reward\nhacking and ensures the fluency of generated captions, resulting in a textual\nGAN setup extended for multimodal inputs. Secondly, they can serve as\nadditional trajectories in the RL strategy, resulting in a teacher forcing loss\nweighted by the similarity of the GT to the image. This objective acts as an\nadditional learning signal grounded to the distribution of the GT captions.\nThirdly, they can serve as strong baselines when added to the pool of captions\nused to compute the proposed contrastive reward to reduce the variance of\ngradient estimate. Experiments on MS-COCO demonstrate the interest of the\nproposed training strategy to produce highly distinctive captions while\nmaintaining high writing quality.\n']",Multimodal Image Captioning and Retrieval
80,79,107,79_imagenet_clips_captions_recognition,"['imagenet', 'clips', 'captions', 'recognition', 'clip', 'trained', 'datasets', 'visual', 'caption', 'benchmarks']","['shot', 'vision', 'image', 'visual', 'prompt', 'class', 'downstream', 'classes', 'classification', 'pre']","['imagenet', 'clips', 'captions', 'recognition', 'trained', 'datasets', 'benchmarks', 'tasks', 'labels', 'attributes']","['  Treating texts as images, combining prompts with textual labels for prompt\ntuning, and leveraging the alignment properties of CLIP have been successfully\napplied in zero-shot multi-label image recognition. Nonetheless, relying solely\non textual labels to store visual information is insufficient for representing\nthe diversity of visual objects. In this paper, we propose reversing the\ntraining process of CLIP and introducing the concept of Pseudo Visual Prompts.\nThese prompts are initialized for each object category and pre-trained on\nlarge-scale, low-cost sentence data generated by large language models. This\nprocess mines the aligned visual information in CLIP and stores it in\nclass-specific visual prompts. We then employ contrastive learning to transfer\nthe stored visual information to the textual labels, enhancing their visual\nrepresentation capacity. Additionally, we introduce a dual-adapter module that\nsimultaneously leverages knowledge from the original CLIP and new learning\nknowledge derived from downstream datasets. Benefiting from the pseudo visual\nprompts, our method surpasses the state-of-the-art not only on clean annotated\ntext data but also on pseudo text data generated by large language models.\n', ""  CLIP models perform remarkably well on zero-shot classification and retrieval\ntasks. But recent studies have shown that learnt representations in CLIP are\nnot well suited for dense prediction tasks like object detection, semantic\nsegmentation or depth estimation. More recently, multi-stage training methods\nfor CLIP models was introduced to mitigate the weak performance of CLIP on\ndownstream tasks. In this work, we find that simply improving the quality of\ncaptions in image-text datasets improves the quality of CLIP's visual\nrepresentations, resulting in significant improvement on downstream dense\nprediction vision tasks. In fact, we find that CLIP pretraining with good\nquality captions can surpass recent supervised, self-supervised and weakly\nsupervised pretraining methods. We show that when CLIP model with ViT-B/16 as\nimage encoder is trained on well aligned image-text pairs it obtains 12.1%\nhigher mIoU and 11.5% lower RMSE on semantic segmentation and depth estimation\ntasks over recent state-of-the-art Masked Image Modeling (MIM) pretraining\nmethods like Masked Autoencoder (MAE). We find that mobile architectures also\nbenefit significantly from CLIP pretraining. A recent mobile vision\narchitecture, MCi2, with CLIP pretraining obtains similar performance as\nSwin-L, pretrained on ImageNet-22k for semantic segmentation task while being\n6.1$\\times$ smaller. Moreover, we show that improving caption quality results\nin $10\\times$ data efficiency when finetuning for dense prediction tasks.\n"", '  Multi-modal learning has become increasingly popular due to its ability to\nleverage information from different data sources (e.g., text and images) to\nimprove the model performance. Recently, CLIP has emerged as an effective\napproach that employs vision-language contrastive pretraining to learn joint\nimage and text representations and exhibits remarkable performance in zero-shot\nlearning and text-guided natural image generation. Despite the huge practical\nsuccess of CLIP, its theoretical understanding remains elusive. In this paper,\nwe formally study transferrable representation learning underlying CLIP and\ndemonstrate how features from different modalities get aligned. We also analyze\nits zero-shot transfer performance on the downstream tasks. Inspired by our\nanalysis, we propose a new CLIP-type approach, which achieves better\nperformance than CLIP and other state-of-the-art methods on benchmark datasets.\n']",Image Recognition with CLIP and Captions
81,80,106,80_semantic_knowledge_relational_entities,"['semantic', 'knowledge', 'relational', 'entities', 'embeddings', 'subgraph', 'entity', 'completion', 'relations', 'kgexplainer']","['link', 'entities', 'knowledge', 'entity', 'completion', 'relations', 'graphs', 'relation', 'graph', 'relational']","['semantic', 'relational', 'embeddings', 'entity', 'subgraphs', 'incompleteness', 'datasets', 'kgems', 'facts', 'ranking']","['  Knowledge graph completion (KGC) is a widely used method to tackle\nincompleteness in knowledge graphs (KGs) by making predictions for missing\nlinks. Description-based KGC leverages pre-trained language models to learn\nentity and relation representations with their names or descriptions, which\nshows promising results. However, the performance of description-based KGC is\nstill limited by the quality of text and the incomplete structure, as it lacks\nsufficient entity descriptions and relies solely on relation names, leading to\nsub-optimal results. To address this issue, we propose MPIKGC, a general\nframework to compensate for the deficiency of contextualized knowledge and\nimprove KGC by querying large language models (LLMs) from various perspectives,\nwhich involves leveraging the reasoning, explanation, and summarization\ncapabilities of LLMs to expand entity descriptions, understand relations, and\nextract structures, respectively. We conducted extensive evaluation of the\neffectiveness and improvement of our framework based on four description-based\nKGC models and four datasets, for both link prediction and triplet\nclassification tasks.\n', '  Knowledge graphs (KGs) consist of links that describe relationships between\nentities. Due to the difficulty of manually enumerating all relationships\nbetween entities, automatically completing them is essential for KGs. Knowledge\nGraph Completion (KGC) is a task that infers unseen relationships between\nentities in a KG. Traditional embedding-based KGC methods, such as RESCAL,\nTransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using\nonly the knowledge from training data. In contrast, the recent Pre-trained\nLanguage Model (PLM)-based KGC utilizes knowledge obtained during pre-training.\nTherefore, PLM-based KGC can estimate missing links between entities by reusing\nmemorized knowledge from pre-training without inference. This approach is\nproblematic because building KGC models aims to infer unseen links between\nentities. However, conventional evaluations in KGC do not consider inference\nand memorization abilities separately. Thus, a PLM-based KGC method, which\nachieves high performance in current KGC evaluations, may be ineffective in\npractical applications. To address this issue, we analyze whether PLM-based KGC\nmethods make inferences or merely access memorized knowledge. For this purpose,\nwe propose a method for constructing synthetic datasets specified in this\nanalysis and conclude that PLMs acquire the inference abilities required for\nKGC through pre-training, even though the performance improvements mostly come\nfrom textual information of entities and relations.\n', '  Inductive knowledge graph completion (KGC) aims to infer the missing relation\nfor a set of newly-coming entities that never appeared in the training set.\nSuch a setting is more in line with reality, as real-world KGs are constantly\nevolving and introducing new knowledge. Recent studies have shown promising\nresults using message passing over subgraphs to embed newly-coming entities for\ninductive KGC. However, the inductive capability of these methods is usually\nlimited by two key issues. (i) KGC always suffers from data sparsity, and the\nsituation is even exacerbated in inductive KGC where new entities often have\nfew or no connections to the original KG. (ii) Cold-start problem. It is over\ncoarse-grained for accurate KG reasoning to generate representations for new\nentities by gathering the local information from few neighbors. To this end, we\npropose a novel iNfOmax RelAtion Network, namely NORAN, for inductive KG\ncompletion. It aims to mine latent relation patterns for inductive KG\ncompletion. Specifically, by centering on relations, NORAN provides a hyper\nview towards KG modeling, where the correlations between relations can be\nnaturally captured as entity-independent logical evidence to conduct inductive\nKGC. Extensive experiment results on five benchmarks show that our framework\nsubstantially outperforms the state-of-the-art KGC methods.\n']",Knowledge Graph Completion
82,81,106,81_vulnerabilities_vulnerability_vulnerable_developers,"['vulnerabilities', 'vulnerability', 'vulnerable', 'developers', 'security', 'fuzzing', 'bugs', 'code', 'programming', 'malicious']","['vulnerability', 'vulnerabilities', 'security', 'software', 'code', 'fuzzing', 'vulnerable', 'repair', 'detection', 'program']","['vulnerabilities', 'developers', 'fuzzing', 'bugs', 'binaries', 'repositories', 'cve', 'java', 'auditing', 'testing']","['  There is an increasing trend to mine vulnerabilities from software\nrepositories and use machine learning techniques to automatically detect\nsoftware vulnerabilities. A fundamental but unresolved research question is:\nhow do different factors in the mining and learning process impact the accuracy\nof identifying vulnerabilities in software projects of varying characteristics?\nSubstantial research has been dedicated in this area, including source code\nstatic analysis, software repository mining, and NLP-based machine learning.\nHowever, practitioners lack experience regarding the key factors for building a\nbaseline model of the state-of-the-art. In addition, there lacks of experience\nregarding the transferability of the vulnerability signatures from project to\nproject. This study investigates how the combination of different vulnerability\nfeatures and three representative machine learning models impact the accuracy\nof vulnerability detection in 17 real-world projects. We examine two types of\nvulnerability representations: 1) code features extracted through NLP with\nvarying tokenization strategies and three different embedding techniques\n(bag-of-words, word2vec, and fastText) and 2) a set of eight architectural\nmetrics that capture the abstract design of the software systems. The three\nmachine learning algorithms include a random forest model, a support vector\nmachines model, and a residual neural network model. The analysis shows a\nrecommended baseline model with signatures extracted through bag-of-words\nembedding, combined with the random forest, consistently increases the\ndetection accuracy by about 4% compared to other combinations in all 17\nprojects. Furthermore, we observe the limitation of transferring vulnerability\nsignatures across domains based on our experiments.\n', '  Despite various approaches being employed to detect vulnerabilities, the\nnumber of reported vulnerabilities shows an upward trend over the years. This\nsuggests the problems are not caught before the code is released, which could\nbe caused by many factors, like lack of awareness, limited efficacy of the\nexisting vulnerability detection tools or the tools not being user-friendly. To\nhelp combat some issues with traditional vulnerability detection tools, we\npropose using large language models (LLMs) to assist in finding vulnerabilities\nin source code. LLMs have shown a remarkable ability to understand and generate\ncode, underlining their potential in code-related tasks. The aim is to test\nmultiple state-of-the-art LLMs and identify the best prompting strategies,\nallowing extraction of the best value from the LLMs. We provide an overview of\nthe strengths and weaknesses of the LLM-based approach and compare the results\nto those of traditional static analysis tools. We find that LLMs can pinpoint\nmany more issues than traditional static analysis tools, outperforming\ntraditional tools in terms of recall and F1 scores. The results should benefit\nsoftware developers and security analysts responsible for ensuring that the\ncode is free of vulnerabilities.\n', '  Software, while beneficial, poses potential cybersecurity risks due to\ninherent vulnerabilities. Detecting these vulnerabilities is crucial, and deep\nlearning has shown promise as an effective tool for this task due to its\nability to perform well without extensive feature engineering. However, a\nchallenge in deploying deep learning for vulnerability detection is the limited\navailability of training data. Recent research highlights the deep learning\nefficacy in diverse tasks. This success is attributed to instruction\nfine-tuning, a technique that remains under-explored in the context of\nvulnerability detection. This paper investigates the capability of models,\nspecifically a recent language model, to generalize beyond the programming\nlanguages used in their training data. It also examines the role of natural\nlanguage instructions in enhancing this generalization. Our study evaluates the\nmodel performance on a real-world dataset to predict vulnerable code. We\npresent key insights and lessons learned, contributing to understanding the\ndeep learning application in software vulnerability detection.\n']",Software Vulnerability Detection
83,82,104,82_quantization_quantizing_quantized_quantize,"['quantization', 'quantizing', 'quantized', 'quantize', 'quanvolutional', 'imagenet', 'cnns', 'bits', 'fpgas', 'compression']","['quantization', 'bit', 'precision', 'quantized', 'hardware', 'operations', 'bits', 'arithmetic', 'networks', 'point']","['quantization', 'cnns', 'fpgas', 'compression', 'bitwidth', 'memory', 'qdnns', 'multiplier', 'chip', 'weights']","['  Quantization has become a mainstream compression technique for reducing model\nsize, computational requirements, and energy consumption for modern deep neural\nnetworks (DNNs). With improved numerical support in recent hardware, including\nmultiple variants of integer and floating point, mixed-precision quantization\nhas become necessary to achieve high-quality results with low model cost. Prior\nmixed-precision methods have performed either a post-training quantization\nsearch, which compromises on accuracy, or a differentiable quantization search,\nwhich leads to high memory usage from branching. Therefore, we propose the\nfirst one-shot mixed-precision quantization search that eliminates the need for\nretraining in both integer and low-precision floating point models. We evaluate\nour search (FLIQS) on multiple convolutional and vision transformer networks to\ndiscover Pareto-optimal models. Our approach improves upon uniform precision,\nmanual mixed-precision, and recent integer quantization search methods. With\ninteger models, we increase the accuracy of ResNet-18 on ImageNet by 1.31% and\nResNet-50 by 0.90% with equivalent model cost over previous methods.\nAdditionally, for the first time, we explore a novel mixed-precision\nfloating-point search and improve MobileNetV2 by up to 0.98% compared to prior\nstate-of-the-art FP8 models. Finally, we extend FLIQS to simultaneously search\na joint quantization and neural architecture space and improve the ImageNet\naccuracy by 2.69% with similar model cost on a MobileNetV2 search space.\n', '  Quantization of the weights and activations is one of the main methods to\nreduce the computational footprint of Deep Neural Networks (DNNs) training.\nCurrent methods enable 4-bit quantization of the forward phase. However, this\nconstitutes only a third of the training process. Reducing the computational\nfootprint of the entire training process requires the quantization of the\nneural gradients, i.e., the loss gradients with respect to the outputs of\nintermediate neural layers.\n  Previous works separately showed that accurate 4-bit quantization of the\nneural gradients needs to (1) be unbiased and (2) have a log scale. However, no\nprevious work aimed to combine both ideas, as we do in this work. Specifically,\nwe examine the importance of having unbiased quantization in quantized neural\nnetwork training, where to maintain it, and how to combine it with logarithmic\nquantization. Based on this, we suggest a $\\textit{logarithmic unbiased\nquantization}$ (LUQ) method to quantize both the forward and backward phases to\n4-bit, achieving state-of-the-art results in 4-bit training without the\noverhead. For example, in ResNet50 on ImageNet, we achieved a degradation of\n1.1%. We further improve this to a degradation of only 0.32% after three epochs\nof high precision fine-tuning, combined with a variance reduction method --\nwhere both these methods add overhead comparable to previously suggested\nmethods.\n', '  Low-bit quantization emerges as one of the most promising compression\napproaches for deploying deep neural networks on edge devices. Mixed-precision\nquantization leverages a mixture of bit-widths to unleash the accuracy and\nefficiency potential of quantized models. However, existing mixed-precision\nquantization methods rely on simulations in high-performance devices to achieve\naccuracy and efficiency trade-offs in immense search spaces. This leads to a\nnon-negligible gap between the estimated efficiency metrics and the actual\nhardware that makes quantized models far away from the optimal accuracy and\nefficiency, and also causes the quantization process to rely on additional\nhigh-performance devices. In this paper, we propose an On-Chip Hardware-Aware\nQuantization (OHQ) framework, performing hardware-aware mixed-precision\nquantization on deployed edge devices to achieve accurate and efficient\ncomputing. Specifically, for efficiency metrics, we built an On-Chip\nQuantization Aware pipeline, which allows the quantization process to perceive\nthe actual hardware efficiency of the quantization operator and avoid\noptimization errors caused by inaccurate simulation. For accuracy metrics, we\npropose Mask-Guided Quantization Estimation technology to effectively estimate\nthe accuracy impact of operators in the on-chip scenario, getting rid of the\ndependence of the quantization process on high computing power. By synthesizing\ninsights from quantized models and hardware through linear optimization, we can\nobtain optimized bit-width configurations to achieve outstanding performance on\naccuracy and efficiency. We evaluate inference accuracy and acceleration with\nquantization for various architectures and compression ratios on hardware. OHQ\nachieves 70% and 73% accuracy for ResNet-18 and MobileNetV3, respectively, and\ncan reduce latency by 15~30% compared to INT8 on real deployment.\n']",Quantization Techniques for Deep Neural Networks
84,83,104,83_conformity_prediction_conformalized_conformal,"['conformity', 'prediction', 'conformalized', 'conformal', 'predictions', 'classification', 'predictors', 'predictive', 'predictor', 'bounding']","['conformal', 'prediction', 'coverage', 'intervals', 'sets', 'uncertainty', 'quantile', 'conditional', 'calibration', 'guarantees']","['prediction', 'conformalized', 'bounding', 'quantifying', 'calibration', 'coverage', 'confidence', 'risk', 'deeponet', 'label']","['  Bayesian deep learning and conformal prediction are two methods that have\nbeen used to convey uncertainty and increase safety in machine learning\nsystems. We focus on combining Bayesian deep learning with split conformal\nprediction and how this combination effects out-of-distribution coverage;\nparticularly in the case of multiclass image classification. We suggest that if\nthe model is generally underconfident on the calibration set, then the\nresultant conformal sets may exhibit worse out-of-distribution coverage\ncompared to simple predictive credible sets. Conversely, if the model is\noverconfident on the calibration set, the use of conformal prediction may\nimprove out-of-distribution coverage. We evaluate prediction sets as a result\nof combining split conformal methods and neural networks trained with (i)\nstochastic gradient descent, (ii) deep ensembles, and (iii) mean-field\nvariational inference. Our results suggest that combining Bayesian deep\nlearning models with split conformal prediction can, in some cases, cause\nunintended consequences such as reducing out-of-distribution coverage.\n', ""  Given the growing significance of reliable, trustworthy, and explainable\nmachine learning, the requirement of uncertainty quantification for anomaly\ndetection systems has become increasingly important. In this context,\neffectively controlling Type I error rates ($\\alpha$) without compromising the\nstatistical power ($1-\\beta$) of these systems can build trust and reduce costs\nrelated to false discoveries, particularly when follow-up procedures are\nexpensive. Leveraging the principles of conformal prediction emerges as a\npromising approach for providing respective statistical guarantees by\ncalibrating a model's uncertainty. This work introduces a novel framework for\nanomaly detection, termed cross-conformal anomaly detection, building upon\nwell-known cross-conformal methods designed for prediction tasks. With that, it\naddresses a natural research gap by extending previous works in the context of\ninductive conformal anomaly detection, relying on the split-conformal approach\nfor model calibration. Drawing on insights from conformal prediction, we\ndemonstrate that the derived methods for calculating cross-conformal $p$-values\nstrike a practical compromise between statistical efficiency (full-conformal)\nand computational efficiency (split-conformal) for uncertainty-quantified\nanomaly detection on benchmark datasets.\n"", '  Conformal Prediction (CP) is a distribution-free uncertainty estimation\nframework that constructs prediction sets guaranteed to contain the true answer\nwith a user-specified probability. Intuitively, the size of the prediction set\nencodes a general notion of uncertainty, with larger sets associated with\nhigher degrees of uncertainty. In this work, we leverage information theory to\nconnect conformal prediction to other notions of uncertainty. More precisely,\nwe prove three different ways to upper bound the intrinsic uncertainty, as\ndescribed by the conditional entropy of the target variable given the inputs,\nby combining CP with information theoretical inequalities. Moreover, we\ndemonstrate two direct and useful applications of such connection between\nconformal prediction and information theory: (i) more principled and effective\nconformal training objectives that generalize previous approaches and enable\nend-to-end training of machine learning models from scratch, and (ii) a natural\nmechanism to incorporate side information into conformal prediction. We\nempirically validate both applications in centralized and federated learning\nsettings, showing our theoretical results translate to lower inefficiency\n(average prediction set size) for popular CP methods.\n']",Conformal Prediction and Uncertainty Quantification in Machine Learning
85,84,103,84_datasets_dataset_tables_tabular,"['datasets', 'dataset', 'tables', 'tabular', 'table', 'learning', 'tabpfn', 'neural', 'features', 'data']","['tabular', 'tables', 'tree', 'feature', 'data', 'datasets', 'columns', 'deep', 'features', 'categorical']","['datasets', 'table', 'tabpfn', 'neural', 'tabtransformer', 'columnar', 'switchtab', 'benchmark', 'pretraining', 'categorical']","['  In the domain of data science, the predictive tasks of classification,\nregression, and imputation of missing values are commonly encountered\nchallenges associated with tabular data. This research endeavors to apply Large\nLanguage Models (LLMs) towards addressing these predictive tasks. Despite their\nproficiency in comprehending natural language, LLMs fall short in dealing with\nstructured tabular data. This limitation stems from their lacking exposure to\nthe intricacies of tabular data during their foundational training. Our\nresearch aims to mitigate this gap by compiling a comprehensive corpus of\ntables annotated with instructions and executing large-scale training of\nLlama-2 on this enriched dataset. Furthermore, we investigate the practical\napplication of applying the trained model to zero-shot prediction, few-shot\nprediction, and in-context learning scenarios. Through extensive experiments,\nour methodology has shown significant improvements over existing benchmarks.\nThese advancements highlight the efficacy of tailoring LLM training to solve\ntable-related problems in data science, thereby establishing a new benchmark in\nthe utilization of LLMs for enhancing tabular intelligence.\n', '  Tabular data from different tables exhibit significant diversity due to\nvaried definitions and types of features, as well as complex inter-feature and\nfeature-target relationships. Cross-dataset pretraining, which learns reusable\npatterns from upstream data to support downstream tasks, have shown notable\nsuccess in various fields. Yet, when applied to tabular data prediction, this\nparadigm faces challenges due to the limited reusable patterns among diverse\ntabular datasets (tables) and the general scarcity of tabular data available\nfor fine-tuning. In this study, we fill this gap by introducing a cross-table\npretrained Transformer, XTFormer, for versatile downstream tabular prediction\ntasks. Our methodology insight is pretraining XTFormer to establish a\n""meta-function"" space that encompasses all potential feature-target mappings.\nIn pre-training, a variety of potential mappings are extracted from\npre-training tabular datasets and are embedded into the ""meta-function"" space,\nand suited mappings are extracted from the ""meta-function"" space for downstream\ntasks by a specified coordinate positioning approach. Experiments show that, in\n190 downstream tabular prediction tasks, our cross-table pretrained XTFormer\nwins both XGBoost and Catboost on 137 (72%) tasks, and surpasses representative\ndeep learning models FT-Transformer and the tabular pre-training approach XTab\non 144 (76%) and 162 (85%) tasks.\n', ""  Tabular data is prevalent across various domains in machine learning.\nAlthough Deep Neural Network (DNN)-based methods have shown promising\nperformance comparable to tree-based ones, in-depth evaluation of these methods\nis challenging due to varying performance ranks across diverse datasets. In\nthis paper, we propose a comprehensive benchmark comprising 300 tabular\ndatasets, covering a wide range of task types, size distributions, and domains.\nWe perform an extensive comparison between state-of-the-art deep tabular\nmethods and tree-based methods, revealing the average rank of all methods and\nhighlighting the key factors that influence the success of deep tabular\nmethods. Next, we analyze deep tabular methods based on their training\ndynamics, including changes in validation metrics and other statistics. For\neach dataset-method pair, we learn a mapping from both the meta-features of\ndatasets and the first part of the validation curve to the final validation set\nperformance and even the evolution of validation curves. This mapping extracts\nessential meta-features that influence prediction accuracy, helping the\nanalysis of tabular methods from novel aspects. Based on the performance of all\nmethods on this large benchmark, we identify two subsets of 45 datasets each.\nThe first subset contains datasets that favor either tree-based methods or\nDNN-based methods, serving as effective analysis tools to evaluate strategies\n(e.g., attribute encoding strategies) for improving deep tabular models. The\nsecond subset contains datasets where the ranks of methods are consistent with\nthe overall benchmark, acting as a probe for tabular analysis. These ``tiny\ntabular benchmarks'' will facilitate further studies on tabular data.\n""]",Tabular Data Analysis and Machine Learning
86,85,101,85_relations_relation_relational_entities,"['relations', 'relation', 'relational', 'entities', 'semantic', 'entity', 'textual', 'supervised', 'nlp', 'annotated']","['relation', 'extraction', 'relations', 'entity', 'entities', 'document', 'sentence', 'triples', 'shot', 'relational']","['relations', 'semantic', 'entity', 'textual', 'nlp', 'structured', 'labeling', 'subtasks', 'information', 'extraction']","['  Current research in form understanding predominantly relies on large\npre-trained language models, necessitating extensive data for pre-training.\nHowever, the importance of layout structure (i.e., the spatial relationship\nbetween the entity blocks in the visually rich document) to relation extraction\nhas been overlooked. In this paper, we propose REgion-Aware Relation Extraction\n(RE$^2$) that leverages region-level spatial structure among the entity blocks\nto improve their relation prediction. We design an edge-aware graph attention\nnetwork to learn the interaction between entities while considering their\nspatial relationship defined by their region-level representations. We also\nintroduce a constraint objective to regularize the model towards consistency\nwith the inherent constraints of the relation extraction task. Extensive\nexperiments across various datasets, languages and domains demonstrate the\nsuperiority of our proposed approach.\n', '  Recent years have seen rapid development in Information Extraction, as well\nas its subtask, Relation Extraction. Relation Extraction is able to detect\nsemantic relations between entities in sentences. Currently, many efficient\napproaches have been applied to relation extraction tasks. Supervised learning\napproaches especially have good performance. However, there are still many\ndifficult challenges. One of the most serious problems is that manually labeled\ndata is difficult to acquire. In most cases, limited data for supervised\napproaches equals lousy performance. Thus here, under the situation with only\nlimited training data, we focus on how to improve the performance of our\nsupervised baseline system with unsupervised pre-training. Feature is one of\nthe key components in improving the supervised approaches. Traditional\napproaches usually apply hand-crafted features, which require expert knowledge\nand expensive human labor. However, this type of feature might suffer from data\nsparsity: when the training set size is small, the model parameters might be\npoorly estimated. In this thesis, we present several novel unsupervised\npre-training models to learn the distributed text representation features,\nwhich are encoded with rich syntactic-semantic patterns of relation\nexpressions. The experiments have demonstrated that this type of feature,\ncombine with the traditional hand-crafted features, could improve the\nperformance of the logistic classification model for relation extraction,\nespecially on the classification of relations with only minor training\ninstances.\n', '  Document-level relation extraction aims at inferring structured human\nknowledge from textual documents. State-of-the-art methods for this task use\npre-trained language models (LMs) via fine-tuning, yet fine-tuning is\ncomputationally expensive and cannot adapt to new relation types or new LMs. As\na remedy, we leverage the generalization capabilities of pre-trained LMs and\npresent a novel framework for document-level in-context few-shot relation\nextraction. Our framework has three strengths: it eliminates the need (1) for\nnamed entity recognition and (2) for human annotations of documents, and (3) it\ncan be updated to new LMs without re-training. We evaluate our framework using\nDocRED, the largest publicly available dataset for document-level relation\nextraction, and demonstrate that our framework achieves state-of-the-art\nperformance. We further show that our framework actually performs much better\nthan the original labels from the development set of DocRED. Finally, we\ndemonstrate that our complete framework yields consistent performance gains\nacross diverse datasets and across different pre-trained LMs. To the best of\nour knowledge, we are the first to reformulate the document-level relation\nextraction task as a tailored in-context few-shot learning paradigm.\n']",Relation Extraction in Textual Data
87,86,99,86_unlearning_privacy_adversary_obfuscation,"['unlearning', 'privacy', 'adversary', 'obfuscation', 'anonymization', 'security', 'attacks', 'attacker', 'defenses', 'adversaries']","['unlearning', 'membership', 'privacy', 'attacks', 'attack', 'inference', 'sensitive', 'machine', 'adversary', 'vulnerability']","['unlearning', 'adversary', 'obfuscation', 'anonymization', 'attacks', 'ensembler', 'private', 'leakage', 'gdpr', 'hijacking']","['  With the continued advancement and widespread adoption of machine learning\n(ML) models across various domains, ensuring user privacy and data security has\nbecome a paramount concern. In compliance with data privacy regulations, such\nas GDPR, a secure machine learning framework should not only grant users the\nright to request the removal of their contributed data used for model training\nbut also facilitates the elimination of sensitive data fingerprints within\nmachine learning models to mitigate potential attack - a process referred to as\nmachine unlearning. In this study, we present a novel unlearning mechanism\ndesigned to effectively remove the impact of specific data samples from a\nneural network while considering the performance of the unlearned model on the\nprimary task. In achieving this goal, we crafted a novel loss function tailored\nto eliminate privacy-sensitive information from weights and activation values\nof the target model by combining target classification loss and membership\ninference loss. Our adaptable framework can easily incorporate various privacy\nleakage approximation mechanisms to guide the unlearning process. We provide\nempirical evidence of the effectiveness of our unlearning approach with a\ntheoretical upper-bound analysis through a membership inference mechanism as a\nproof of concept. Our results showcase the superior performance of our approach\nin terms of unlearning efficacy and latency as well as the fidelity of the\nprimary task, across four datasets and four deep learning architectures.\n', '  The high cost of model training makes it increasingly desirable to develop\ntechniques for unlearning. These techniques seek to remove the influence of a\ntraining example without having to retrain the model from scratch. Intuitively,\nonce a model has unlearned, an adversary that interacts with the model should\nno longer be able to tell whether the unlearned example was included in the\nmodel\'s training set or not. In the privacy literature, this is known as\nmembership inference. In this work, we discuss adaptations of Membership\nInference Attacks (MIAs) to the setting of unlearning (leading to their ""U-MIA""\ncounterparts). We propose a categorization of existing U-MIAs into ""population\nU-MIAs"", where the same attacker is instantiated for all examples, and\n""per-example U-MIAs"", where a dedicated attacker is instantiated for each\nexample. We show that the latter category, wherein the attacker tailors its\nmembership prediction to each example under attack, is significantly stronger.\nIndeed, our results show that the commonly used U-MIAs in the unlearning\nliterature overestimate the privacy protection afforded by existing unlearning\ntechniques on both vision and language models. Our investigation reveals a\nlarge variance in the vulnerability of different examples to per-example\nU-MIAs. In fact, several unlearning algorithms lead to a reduced vulnerability\nfor some, but not all, examples that we wish to unlearn, at the expense of\nincreasing it for other examples. Notably, we find that the privacy protection\nfor the remaining training examples may worsen as a consequence of unlearning.\nWe also discuss the fundamental difficulty of equally protecting all examples\nusing existing unlearning schemes, due to the different rates at which examples\nare unlearned. We demonstrate that naive attempts at tailoring unlearning\nstopping criteria to different examples fail to alleviate these issues.\n', ""  This paper focuses on the challenge of machine unlearning, aiming to remove\nthe influence of specific training data on machine learning models.\nTraditionally, the development of unlearning algorithms runs parallel with that\nof membership inference attacks (MIA), a type of privacy threat to determine\nwhether a data instance was used for training. However, the two strands are\nintimately connected: one can view machine unlearning through the lens of MIA\nsuccess with respect to removed data. Recognizing this connection, we propose a\ngame-theoretic framework that integrates MIAs into the design of unlearning\nalgorithms. Specifically, we model the unlearning problem as a Stackelberg game\nin which an unlearner strives to unlearn specific training data from a model,\nwhile an auditor employs MIAs to detect the traces of the ostensibly removed\ndata. Adopting this adversarial perspective allows the utilization of new\nattack advancements, facilitating the design of unlearning algorithms. Our\nframework stands out in two ways. First, it takes an adversarial approach and\nproactively incorporates the attacks into the design of unlearning algorithms.\nSecondly, it uses implicit differentiation to obtain the gradients that limit\nthe attacker's success, thus benefiting the process of unlearning. We present\nempirical results to demonstrate the effectiveness of the proposed approach for\nmachine unlearning.\n""]",Machine Learning Privacy and Security
88,87,98,87_planning_planner_planners_plans,"['planning', 'planner', 'planners', 'plans', 'plan', 'tasks', 'automated', 'interactive', 'taskgen', 'task']","['planning', 'plan', 'plans', 'web', 'agents', 'planners', 'agent', 'actions', 'planner', 'action']","['planning', 'automated', 'taskgen', 'agent', 'generate', 'webgum', 'step', 'capabilities', 'pddl', 'itineraries']","['  Classical planning formulations like the Planning Domain Definition Language\n(PDDL) admit action sequences guaranteed to achieve a goal state given an\ninitial state if any are possible. However, reasoning problems defined in PDDL\ndo not capture temporal aspects of action taking, for example that two agents\nin the domain can execute an action simultaneously if postconditions of each do\nnot interfere with preconditions of the other. A human expert can decompose a\ngoal into largely independent constituent parts and assign each agent to one of\nthese subgoals to take advantage of simultaneous actions for faster execution\nof plan steps, each using only single agent planning. By contrast, large\nlanguage models (LLMs) used for directly inferring plan steps do not guarantee\nexecution success, but do leverage commonsense reasoning to assemble action\nsequences. We combine the strengths of classical planning and LLMs by\napproximating human intuitions for two-agent planning goal decomposition. We\ndemonstrate that LLM-based goal decomposition leads to faster planning times\nthan solving multi-agent PDDL problems directly while simultaneously achieving\nfewer plan execution steps than a single agent plan alone and preserving\nexecution success. Additionally, we find that LLM-based approximations of\nsubgoals can achieve similar multi-agent execution steps than those specified\nby human experts. Website and resources at https://glamor-usc.github.io/twostep\n', '  The emergence of large language models (LLMs) has increasingly drawn\nattention to the use of LLMs for human-like planning. Existing work on\nLLM-based planning either focuses on leveraging the inherent language\ngeneration capabilities of LLMs to produce free-style plans, or employs\nreinforcement learning approaches to learn decision-making for a limited set of\nactions within restricted environments. However, both approaches exhibit\nsignificant discrepancies from the open and executable requirements in\nreal-world planning. In this paper, we propose a new planning task--open\ngrounded planning. The primary objective of open grounded planning is to ask\nthe model to generate an executable plan based on a variable action set,\nthereby ensuring the executability of the produced plan. To this end, we\nestablishes a benchmark for open grounded planning spanning a wide range of\ndomains. Then we test current state-of-the-art LLMs along with five planning\napproaches, revealing that existing LLMs and methods still struggle to address\nthe challenges posed by grounded planning in open domains. The outcomes of this\npaper define and establish a foundational dataset for open grounded planning,\nand shed light on the potential challenges and future directions of LLM-based\nplanning.\n', '  Plan synthesis aims to generate a course of actions or policies to transit\ngiven initial states to goal states, provided domain models that could be\ndesigned by experts or learnt from training data or interactions with the\nworld. Intrigued by the claims of emergent planning capabilities in large\nlanguage models (LLMs), works have been proposed to investigate the planning\neffectiveness of LLMs, without considering any utilization of off-the-shelf\nplanning techniques in LLMs. In this paper, we aim to further study the insight\nof the planning capability of LLMs by investigating the roles of LLMs in\noff-the-shelf planning frameworks. To do this, we investigate the effectiveness\nof embedding LLMs into one of the well-known planning frameworks, graph-based\nplanning, proposing a novel LLMs-based planning framework with LLMs embedded in\ntwo levels of planning graphs, i.e., mutual constraints generation level and\nconstraints solving level. We empirically exhibit the effectiveness of our\nproposed framework in various planning domains.\n']","""Planning and Large Language Models"""
89,88,97,88_models_knowledge_trained_training,"['models', 'knowledge', 'trained', 'training', 'teacher', 'students', 'distillation', 'teachers', 'distilling', 'model']","['teacher', 'student', 'distillation', 'knowledge', 'teachers', 'logits', 'transfer', 'compact', 'model', 'soft']","['trained', 'distillation', 'model', 'imagenet', 'curriculum', 'compression', 'kge', 'robustkd', 'transferring', 'wmt']","[""  Knowledge Distillation (KD) has proven effective for compressing large\nteacher models into smaller student models. While it is well known that student\nmodels can achieve similar accuracies as the teachers, it has also been shown\nthat they nonetheless often do not learn the same function. It is, however,\noften highly desirable that the student's and teacher's functions share similar\nproperties such as basing the prediction on the same input features, as this\nensures that students learn the 'right features' from the teachers. In this\nwork, we explore whether this can be achieved by not only optimizing the\nclassic KD loss but also the similarity of the explanations generated by the\nteacher and the student. Despite the idea being simple and intuitive, we find\nthat our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides\nlarge gains in terms of accuracy and student-teacher agreement, (2) ensures\nthat the student learns from the teacher to be right for the right reasons and\nto give similar explanations, and (3) is robust with respect to the model\narchitectures, the amount of training data, and even works with 'approximate',\npre-computed explanations.\n"", '  Deep cascaded architectures for magnetic resonance imaging (MRI) acceleration\nhave shown remarkable success in providing high-quality reconstruction.\nHowever, as the number of cascades increases, the improvements in\nreconstruction tend to become marginal, indicating possible excess model\ncapacity. Knowledge distillation (KD) is an emerging technique to compress\nthese models, in which a trained deep teacher network is used to distill\nknowledge to a smaller student network such that the student learns to mimic\nthe behavior of the teacher. Most KD methods focus on effectively training the\nstudent with a pre-trained teacher unaware of the student model. We propose\nSFT-KD-Recon, a student-friendly teacher training approach along with the\nstudent as a prior step to KD to make the teacher aware of the structure and\ncapacity of the student and enable aligning the representations of the teacher\nwith the student. In SFT, the teacher is jointly trained with the unfolded\nbranch configurations of the student blocks using three loss terms -\nteacher-reconstruction loss, student-reconstruction loss, and teacher-student\nimitation loss, followed by KD of the student. We perform extensive experiments\nfor MRI acceleration in 4x and 5x under-sampling on the brain and cardiac\ndatasets on five KD methods using the proposed approach as a prior step. We\nconsider the DC-CNN architecture and setup teacher as D5C5 (141765 parameters),\nand student as D3C5 (49285 parameters), denoting a compression of 2.87:1.\nResults show that (i) our approach consistently improves the KD methods with\nimproved reconstruction performance and image quality, and (ii) the student\ndistilled using our approach is competitive with the teacher, with the\nperformance gap reduced from 0.53 dB to 0.03 dB.\n', ""  Knowledge distillation (KD) involves transferring the knowledge from one\nneural network to another, often from a larger, well-trained model (teacher) to\na smaller, more efficient model (student). Traditional KD methods minimize the\nKullback-Leibler (KL) divergence between the probabilistic outputs of the\nteacher and student networks. However, this approach often overlooks crucial\nstructural knowledge embedded within the teacher's network. In this paper, we\nintroduce Invariant Consistency Distillation (ICD), a novel methodology\ndesigned to enhance KD by ensuring that the student model's representations are\nconsistent with those of the teacher. Our approach combines contrastive\nlearning with an explicit invariance penalty, capturing significantly more\ninformation from the teacher's representation of the data. Our results on\nCIFAR-100 demonstrate that ICD outperforms traditional KD techniques and\nsurpasses 13 state-of-the-art methods. In some cases, the student model even\nexceeds the teacher model in terms of accuracy. Furthermore, we successfully\ntransfer our method to other datasets, including Tiny ImageNet and STL-10. The\ncode will be made public soon.\n""]",Knowledge Distillation in Deep Learning
90,89,97,89_activelab_supervised_activellm_classification,"['activelab', 'supervised', 'activellm', 'classification', 'annotations', 'annotation', 'labeled', 'learning', 'labeling', 'datasets']","['active', 'annotation', 'labeling', 'informative', 'uncertainty', 'unlabeled', 'budget', 'samples', 'selection', 'acquisition']","['activelab', 'supervised', 'activellm', 'annotations', 'labeling', 'learner', 'unlabeled', 'sampling', 'labelbench', 'batch']","[""  We conduct a comprehensive evaluation of state-of-the-art deep active\nlearning methods. Surprisingly, under general settings, no single-model method\ndecisively outperforms entropy-based active learning, and some even fall short\nof random sampling. We delve into overlooked aspects like starting budget,\nbudget step, and pretraining's impact, revealing their significance in\nachieving superior results. Additionally, we extend our evaluation to other\ntasks, exploring the active learning effectiveness in combination with\nsemi-supervised learning, and object detection. Our experiments provide\nvaluable insights and concrete recommendations for future active learning\nstudies. By uncovering the limitations of current methods and understanding the\nimpact of different experimental settings, we aim to inspire more efficient\ntraining of deep learning models in real-world scenarios with limited\nannotation budgets. This work contributes to advancing active learning's\nefficacy in deep learning and empowers researchers to make informed decisions\nwhen applying active learning to their tasks.\n"", '  This paper explores the integration of active machine learning (ML) for 6G\nnetworks, an area that remains under-explored yet holds potential. Unlike\npassive ML systems, active ML can be made to interact with the network\nenvironment. It actively selects informative and representative data points for\ntraining, thereby reducing the volume of data needed while accelerating the\nlearning process. While active learning research mainly focuses on data\nannotation, we call for a network-centric active learning framework that\nconsiders both annotation (i.e., what is the label) and data acquisition (i.e.,\nwhich and how many samples to collect). Moreover, we explore the synergy\nbetween generative artificial intelligence (AI) and active learning to overcome\nexisting limitations in both active learning and generative AI. This paper also\nfeatures a case study on a mmWave throughput prediction problem to demonstrate\nthe practical benefits and improved performance of active learning for 6G\nnetworks. Furthermore, we discuss how the implications of active learning\nextend to numerous 6G network use cases. We highlight the potential of active\nlearning based 6G networks to enhance computational efficiency, data annotation\nand acquisition efficiency, adaptability, and overall network intelligence. We\nconclude with a discussion on challenges and future research directions for\nactive learning in 6G networks, including development of novel query\nstrategies, distributed learning integration, and inclusion of human- and\nmachine-in-the-loop learning.\n', '  Class imbalance is a prevalent issue in real world machine learning\napplications, often leading to poor performance in rare and minority classes.\nWith an abundance of wild unlabeled data, active learning is perhaps the most\neffective technique in solving the problem at its root -- collecting a more\nbalanced and informative set of labeled examples during annotation. Label noise\nis another common issue in data annotation jobs, which is especially\nchallenging for active learning methods. In this work, we conduct the first\nstudy of active learning under both class imbalance and label noise. We propose\na novel algorithm that robustly identifies the class separation threshold and\nannotates the most uncertain examples that are closest from it. Through a novel\nreduction to one-dimensional active learning, our algorithm DIRECT is able to\nleverage the classic active learning literature to address issues such as batch\nlabeling and tolerance towards label noise. We present extensive experiments on\nimbalanced datasets with and without label noise. Our results demonstrate that\nDIRECT can save more than 60% of the annotation budget compared to state-of-art\nactive learning algorithms and more than 80% of annotation budget compared to\nrandom sampling.\n']",Active Learning for Efficient Classification
91,90,92,90_lasso_regularization_regularized_ridge,"['lasso', 'regularization', 'regularized', 'ridge', 'predictors', 'kernels', 'ridgeless', 'overfitting', 'generalization', 'regression']","['lasso', 'ridge', 'regression', 'kernel', 'estimators', 'regularization', 'covariance', 'random', 'dimensional', 'error']","['lasso', 'regularization', 'ridge', 'predictors', 'overfitting', 'norm', 'asymptotics', 'boldsymbol', 'elasticnet', 'spectral']","['  Meta-learning involves training models on a variety of training tasks in a\nway that enables them to generalize well on new, unseen test tasks. In this\nwork, we consider meta-learning within the framework of high-dimensional\nmultivariate random-effects linear models and study generalized\nridge-regression based predictions. The statistical intuition of using\ngeneralized ridge regression in this setting is that the covariance structure\nof the random regression coefficients could be leveraged to make better\npredictions on new tasks. Accordingly, we first characterize the precise\nasymptotic behavior of the predictive risk for a new test task when the data\ndimension grows proportionally to the number of samples per task. We next show\nthat this predictive risk is optimal when the weight matrix in generalized\nridge regression is chosen to be the inverse of the covariance matrix of random\ncoefficients. Finally, we propose and analyze an estimator of the inverse\ncovariance matrix of random regression coefficients based on data from the\ntraining tasks. As opposed to intractable MLE-type estimators, the proposed\nestimators could be computed efficiently as they could be obtained by solving\n(global) geodesically-convex optimization problems. Our analysis and\nmethodology use tools from random matrix theory and Riemannian optimization.\nSimulation results demonstrate the improved generalization performance of the\nproposed method on new unseen test tasks within the considered framework.\n', '  Kernel ridge regression, KRR, is a generalization of linear ridge regression\nthat is non-linear in the data, but linear in the parameters. Here, we\nintroduce an equivalent formulation of the objective function of KRR, opening\nup both for using penalties other than the ridge penalty and for studying\nkernel ridge regression from the perspective of gradient descent. Using a\ncontinuous-time perspective, we derive a closed-form solution for solving\nkernel regression with gradient descent, something we refer to as kernel\ngradient flow, KGF, and theoretically bound the differences between KRR and\nKGF, where, for the latter, regularization is obtained through early stopping.\nWe also generalize KRR by replacing the ridge penalty with the $\\ell_1$ and\n$\\ell_\\infty$ penalties, respectively, and use the fact that analogous to the\nsimilarities between KGF and KRR, $\\ell_1$ regularization and forward stagewise\nregression (also known as coordinate descent), and $\\ell_\\infty$ regularization\nand sign gradient descent, follow similar solution paths. We can thus alleviate\nthe need for computationally heavy algorithms based on proximal gradient\ndescent. We show theoretically and empirically how the $\\ell_1$ and\n$\\ell_\\infty$ penalties, and the corresponding gradient-based optimization\nalgorithms, produce sparse and robust kernel regression solutions,\nrespectively.\n', ""  Kernel ridge regression (KRR) is a popular class of machine learning models\nthat has become an important tool for understanding deep learning. Much of the\nfocus has been on studying the proportional asymptotic regime, $n \\asymp d$,\nwhere $n$ is the number of training samples and $d$ is the dimension of the\ndataset. In this regime, under certain conditions on the data distribution, the\nkernel random matrix involved in KRR exhibits behavior akin to that of a linear\nkernel. In this work, we extend the study of kernel regression to the quadratic\nasymptotic regime, where $n \\asymp d^2$. In this regime, we demonstrate that a\nbroad class of inner-product kernels exhibit behavior similar to a quadratic\nkernel. Specifically, we establish an operator norm approximation bound for the\ndifference between the original kernel random matrix and a quadratic kernel\nrandom matrix with additional correction terms compared to the Taylor expansion\nof the kernel functions. The approximation works for general data distributions\nunder a Gaussian-moment-matching assumption with a covariance structure. This\nnew approximation is utilized to obtain a limiting spectral distribution of the\noriginal kernel matrix and characterize the precise asymptotic training and\ngeneralization errors for KRR in the quadratic regime when $n/d^2$ converges to\na non-zero constant. The generalization errors are obtained for both\ndeterministic and random teacher models. Our proof techniques combine moment\nmethods, Wick's formula, orthogonal polynomials, and resolvent analysis of\nrandom matrices with correlated entries.\n""]",Regularization Techniques in Regression Analysis
92,91,92,91_mri_imaging_denoising_tomography,"['mri', 'imaging', 'denoising', 'tomography', 'reconstructed', 'undersampled', 'undersampling', 'deep', 'supervised', 'reconstruction']","['reconstruction', 'imaging', 'dose', 'undersampled', 'resonance', 'magnetic', 'image', 'tomography', 'motion', 'radiation']","['mri', 'denoising', 'tomography', 'undersampled', 'reconstruction', 'diffusion', 'scans', 'prior', 'resolution', 'mrcp']","[""  Image generative AI has garnered significant attention in recent years. In\nparticular, the diffusion model, a core component of recent generative AI,\nproduces high-quality images with rich diversity. In this study, we propose a\nnovel CT reconstruction method by combining the denoising diffusion\nprobabilistic model with iterative CT reconstruction. In sharp contrast to\nprevious studies, we optimize the fidelity loss of CT reconstruction with\nrespect to the latent variable of the diffusion model, instead of the image and\nmodel parameters. To suppress anatomical structure changes produced by the\ndiffusion model, we shallow the diffusion and reverse processes, and fix a set\nof added noises in the reverse process to make it deterministic during\ninference. We demonstrate the effectiveness of the proposed method through\nsparse view CT reconstruction of 1/10 view projection data. Despite the\nsimplicity of the implementation, the proposed method shows the capability of\nreconstructing high-quality images while preserving the patient's anatomical\nstructure, and outperforms existing methods including iterative reconstruction,\niterative reconstruction with total variation, and the diffusion model alone in\nterms of quantitative indices such as SSIM and PSNR. We also explore further\nsparse view CT using 1/20 view projection data with the same trained diffusion\nmodel. As the number of iterations increases, image quality improvement\ncomparable to that of 1/10 sparse view CT reconstruction is achieved. In\nprinciple, the proposed method can be widely applied not only to CT but also to\nother imaging modalities such as MRI, PET, and SPECT.\n"", ""  In Magnetic Resonance Imaging (MRI), image acquisitions are often\nundersampled in the measurement domain to accelerate the scanning process, at\nthe expense of image quality. However, image quality is a crucial factor that\ninfluences the accuracy of clinical diagnosis; hence, high-quality image\nreconstruction from undersampled measurements has been a key area of research.\nRecently, deep learning (DL) methods have emerged as the state-of-the-art for\nMRI reconstruction, typically involving deep neural networks to transform\nundersampled MRI images into high-quality MRI images through data-driven\nprocesses. Nevertheless, there is clear and significant room for improvement in\nundersampled DL MRI reconstruction to meet the high standards required for\nclinical diagnosis, in terms of eliminating aliasing artifacts and reducing\nimage noise. In this paper, we introduce a self-supervised pretraining\nprocedure using contrastive learning to improve the accuracy of undersampled DL\nMRI reconstruction. We use contrastive learning to transform the MRI image\nrepresentations into a latent space that maximizes mutual information among\ndifferent undersampled representations and optimizes the information content at\nthe input of the downstream DL reconstruction models. Our experiments\ndemonstrate improved reconstruction accuracy across a range of acceleration\nfactors and datasets, both quantitatively and qualitatively. Furthermore, our\nextended experiments validate the proposed framework's robustness under\nadversarial conditions, such as measurement noise, different k-space sampling\npatterns, and pathological abnormalities, and also prove the transfer learning\ncapabilities on MRI datasets with completely different anatomy. Additionally,\nwe conducted experiments to visualize and analyze the properties of the\nproposed MRI contrastive learning latent space.\n"", ""  Motion artifacts in Magnetic Resonance Imaging (MRI) are one of the\nfrequently occurring artifacts due to patient movements during scanning. Motion\nis estimated to be present in approximately 30% of clinical MRI scans; however,\nmotion has not been explicitly modeled within deep learning image\nreconstruction models. Deep learning (DL) algorithms have been demonstrated to\nbe effective for both the image reconstruction task and the motion correction\ntask, but the two tasks are considered separately. The image reconstruction\ntask involves removing undersampling artifacts such as noise and aliasing\nartifacts, whereas motion correction involves removing artifacts including\nblurring, ghosting, and ringing. In this work, we propose a novel method to\nsimultaneously accelerate imaging and correct motion. This is achieved by\nintegrating a motion module into the deep learning-based MRI reconstruction\nprocess, enabling real-time detection and correction of motion. We model motion\nas a tightly integrated auxiliary layer in the deep learning model during\ntraining, making the deep learning model 'motion-informed'. During inference,\nimage reconstruction is performed from undersampled raw k-space data using a\ntrained motion-informed DL model. Experimental results demonstrate that the\nproposed motion-informed deep learning image reconstruction network\noutperformed the conventional image reconstruction network for motion-degraded\nMRI datasets.\n""]",Medical Imaging Reconstruction Techniques
93,92,91,92_confidence_confident_reliable_reliability,"['confidence', 'confident', 'reliable', 'reliability', 'verbalized', 'language', 'uncertainties', 'certainty', 'responses', 'uncertainty']","['confidence', 'uncertainty', 'calibration', 'conformal', 'quantification', 'responses', 'answers', 'answering', 'certainty', 'answer']","['confident', 'reliability', 'verbalized', 'responses', 'models', 'nlg', 'trustworthiness', 'overconfidence', 'factuality', 'elicitation']","['  Although large language models (LLMs) are capable of performing various\ntasks, they still suffer from producing plausible but incorrect responses. To\nimprove the reliability of LLMs, recent research has focused on uncertainty\nquantification to predict whether a response is correct or not. However, most\nuncertainty quantification methods have been evaluated on questions requiring a\nsingle clear answer, ignoring the existence of data uncertainty that arises\nfrom irreducible randomness. Instead, these methods only consider model\nuncertainty, which arises from a lack of knowledge. In this paper, we\ninvestigate previous uncertainty quantification methods under the presence of\ndata uncertainty. Our contributions are two-fold: 1) proposing a new\nMulti-Answer Question Answering dataset, MAQA, consisting of world knowledge,\nmathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty\nquantification regarding data uncertainty, and 2) assessing 5 uncertainty\nquantification methods of diverse white- and black-box LLMs. Our findings show\nthat entropy and consistency-based methods estimate the model uncertainty well\neven under data uncertainty, while other methods for white- and black-box LLMs\nstruggle depending on the tasks. Additionally, methods designed for white-box\nLLMs suffer from overconfidence in reasoning tasks compared to simple knowledge\nqueries. We believe our observations will pave the way for future work on\nuncertainty quantification in realistic setting.\n', '  Empowering large language models to accurately express confidence in their\nanswers is essential for trustworthy decision-making. Previous confidence\nelicitation methods, which primarily rely on white-box access to internal model\ninformation or model fine-tuning, have become less suitable for LLMs,\nespecially closed-source commercial APIs. This leads to a growing need to\nexplore the untapped area of black-box approaches for LLM uncertainty\nestimation. To better break down the problem, we define a systematic framework\nwith three components: prompting strategies for eliciting verbalized\nconfidence, sampling methods for generating multiple responses, and aggregation\ntechniques for computing consistency. We then benchmark these methods on two\nkey tasks-confidence calibration and failure prediction-across five types of\ndatasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs\nincluding GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights:\n1) LLMs, when verbalizing their confidence, tend to be overconfident,\npotentially imitating human patterns of expressing confidence. 2) As model\ncapability scales up, both calibration and failure prediction performance\nimprove. 3) Employing our proposed strategies, such as human-inspired prompts,\nconsistency among multiple responses, and better aggregation strategies can\nhelp mitigate this overconfidence from various perspectives. 4) Comparisons\nwith white-box methods indicate that while white-box methods perform better,\nthe gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements,\nnone of these techniques consistently outperform others, and all investigated\nmethods struggle in challenging tasks, such as those requiring professional\nknowledge, indicating significant scope for improvement. We believe this study\ncan serve as a strong baseline and provide insights for eliciting confidence in\nblack-box LLMs.\n', ""  To enhance Large Language Models' (LLMs) reliability, calibration is\nessential -- the model's assessed confidence scores should align with the\nactual likelihood of its responses being correct. However, current confidence\nelicitation methods and calibration metrics typically rely on a binary\ntrue/false assessment of response correctness. This approach does not apply to\nlong-form generation, where an answer can be partially correct. Addressing this\ngap, we introduce a unified calibration framework, in which both the\ncorrectness of the LLMs' responses and their associated confidence levels are\ntreated as distributions across a range of scores. Within this framework, we\ndevelop three metrics to precisely evaluate LLM calibration and further propose\ntwo confidence elicitation methods based on self-consistency and\nself-evaluation. Our experiments, which include long-form QA and summarization\ntasks, demonstrate that larger models don't necessarily guarantee better\ncalibration, that calibration performance is found to be metric-dependent, and\nthat self-consistency methods excel in factoid datasets. We also find that\ncalibration can be enhanced through techniques such as fine-tuning, integrating\nrelevant source documents, scaling the temperature, and combining\nself-consistency with self-evaluation. Lastly, we showcase a practical\napplication of our system: selecting and cascading open-source models and\nChatGPT to optimize correctness given a limited API budget. This research not\nonly challenges existing notions of LLM calibration but also offers practical\nmethodologies for improving trustworthiness in long-form generation.\n""]",Improving Reliability and Confidence in Large Language Models
94,93,90,93_editing_editor_editors_edits,"['editing', 'editor', 'editors', 'edits', 'modifying', 'modify', 'edit', 'retraining', 'edited', 'forgetting']","['editing', 'knowledge', 'edited', 'edits', 'edit', 'outdated', 'facts', 'lingual', 'factual', 'update']","['editing', 'retraining', 'forgetting', 'mke', 'lifelong', 'lingual', 'interventions', 'batch', 'obsolete', 'effects']","['  Large language models (LLMs) are pivotal in advancing natural language\nprocessing (NLP) tasks, yet their efficacy is hampered by inaccuracies and\noutdated knowledge. Model editing emerges as a promising solution to address\nthese challenges. However, existing editing methods struggle to track and\nincorporate changes in knowledge associated with edits, which limits the\ngeneralization ability of postedit LLMs in processing edited knowledge. To\ntackle these problems, we propose a novel model editing method that leverages\nknowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we\nfirst utilize a knowledge graph augmentation module to uncover associated\nknowledge that has changed due to editing, obtaining its internal\nrepresentations within LLMs. This approach allows knowledge alterations within\nLLMs to be reflected through an external graph structure. Subsequently, we\ndesign a graph-based knowledge edit module to integrate structured knowledge\ninto the model editing. This ensures that the updated parameters reflect not\nonly the modifications of the edited knowledge but also the changes in other\nassociated knowledge resulting from the editing process. Comprehensive\nexperiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME\nsignificantly improves the generalization capabilities of post-edit LLMs in\nemploying edited knowledge.\n', '  Knowledge editing aims to rectify inaccuracies in large language models\n(LLMs) without costly retraining for outdated or erroneous knowledge. However,\ncurrent knowledge editing methods primarily focus on single editing, failing to\nmeet the requirements for lifelong editing. This study reveals a performance\ndegradation encountered by knowledge editing in lifelong editing, characterized\nby toxicity buildup and toxicity flash, with the primary cause identified as\npattern unmatch. We introduce a knowledge editing approach named Wise-Layer\nKnowledge Editor (WilKE), which selects editing layer based on the pattern\nmatching degree of editing knowledge across different layers in language\nmodels. Experimental results demonstrate that, in lifelong editing, WilKE\nexhibits an average improvement of 46.2% and 67.8% on editing GPT2-XL and GPT-J\nrelative to state-of-the-art knowledge editing methods.\n', '  This study presents a targeted model editing analysis focused on the latest\nlarge language model, Llama-3. We explore the efficacy of popular model editing\ntechniques - ROME, MEMIT, and EMMET, which are designed for precise layer\ninterventions. We identify the most effective layers for targeted edits through\nan evaluation that encompasses up to 4096 edits across three distinct\nstrategies: sequential editing, batch editing, and a hybrid approach we call as\nsequential-batch editing. Our findings indicate that increasing edit\nbatch-sizes may degrade model performance more significantly than using smaller\nedit batches sequentially for equal number of edits. With this, we argue that\nsequential model editing is an important component for scaling model editing\nmethods and future research should focus on methods that combine both batched\nand sequential editing. This observation suggests a potential limitation in\ncurrent model editing methods which push towards bigger edit batch sizes, and\nwe hope it paves way for future investigations into optimizing batch sizes and\nmodel editing performance.\n']",Model Editing in Large Language Models
95,94,90,94_bound_bounds_generalization_optimal,"['bound', 'bounds', 'generalization', 'optimal', 'bayes', 'learning', 'compression', 'losses', 'complexity', 'learnability']","['bounds', 'mutual', 'realizable', 'sample', 'agnostic', 'generalization', 'hypothesis', 'learnability', 'dimension', 'compression']","['bound', 'optimal', 'bayes', 'compression', 'learnability', 'distributions', 'empirical', 'pac', 'information', 'divergence']","['  We introduce a new PAC-Bayes oracle bound for unbounded losses. This result\ncan be understood as a PAC-Bayesian version of the Cram\\\'er-Chernoff bound. The\nproof technique relies on controlling the tails of certain random variables\ninvolving the Cram\\\'er transform of the loss. We highlight several applications\nof the main theorem. First, we show that our result naturally allows exact\noptimization of the free parameter on many PAC-Bayes bounds. Second, we recover\nand generalize previous results. Finally, we show that our approach allows\nworking with richer assumptions that result in more informative and potentially\ntighter bounds. In this direction, we provide a general bound under a new\n``model-dependent bounded CGF"" assumption from which we obtain bounds based on\nparameter norms and log-Sobolev inequalities. All these bounds can be minimized\nto obtain novel posteriors.\n', ""  We propose data-dependent uniform generalization bounds by approaching the\nproblem from a PAC-Bayesian perspective. We first apply the PAC-Bayesian\nframework on `random sets' in a rigorous way, where the training algorithm is\nassumed to output a data-dependent hypothesis set after observing the training\ndata. This approach allows us to prove data-dependent bounds, which can be\napplicable in numerous contexts. To highlight the power of our approach, we\nconsider two main applications. First, we propose a PAC-Bayesian formulation of\nthe recently developed fractal-dimension-based generalization bounds. The\nderived results are shown to be tighter and they unify the existing results\naround one simple proof technique. Second, we prove uniform bounds over the\ntrajectories of continuous Langevin dynamics and stochastic gradient Langevin\ndynamics. These results provide novel information about the generalization\nproperties of noisy algorithms.\n"", '  In this paper, we establish novel data-dependent upper bounds on the\ngeneralization error through the lens of a ""variable-size compressibility""\nframework that we introduce newly here. In this framework, the generalization\nerror of an algorithm is linked to a variable-size \'compression rate\' of its\ninput data. This is shown to yield bounds that depend on the empirical measure\nof the given input data at hand, rather than its unknown distribution. Our new\ngeneralization bounds that we establish are tail bounds, tail bounds on the\nexpectation, and in-expectations bounds. Moreover, it is shown that our\nframework also allows to derive general bounds on any function of the input\ndata and output hypothesis random variables. In particular, these general\nbounds are shown to subsume and possibly improve over several existing\nPAC-Bayes and data-dependent intrinsic dimension-based bounds that are\nrecovered as special cases, thus unveiling a unifying character of our\napproach. For instance, a new data-dependent intrinsic dimension-based bound is\nestablished, which connects the generalization error to the optimization\ntrajectories and reveals various interesting connections with the\nrate-distortion dimension of a process, the R\\\'enyi information dimension of a\nprocess, and the metric mean dimension.\n']",PAC-Bayesian Generalization Bounds
96,95,89,95_auction_bidding_auctions_bids,"['auction', 'bidding', 'auctions', 'bids', 'bid', 'bidders', 'optimal', 'bidder', 'bandit', 'pricing']","['auctions', 'price', 'auction', 'pricing', 'revenue', 'regret', 'seller', 'valuations', 'prices', 'bid']","['auctions', 'bidder', 'bandit', 'pricing', 'maximizing', 'incentive', 'allocation', 'advertisement', 'market', 'algorithms']","['  Learning to bid in repeated first-price auctions is a fundamental problem at\nthe interface of game theory and machine learning, which has seen a recent\nsurge in interest due to the transition of display advertising to first-price\nauctions. In this work, we propose a novel concave formulation for\npure-strategy bidding in first-price auctions, and use it to analyze natural\nGradient-Ascent-based algorithms for this problem. Importantly, our analysis\ngoes beyond regret, which was the typical focus of past work, and also accounts\nfor the strategic backdrop of online-advertising markets where bidding\nalgorithms are deployed -- we provide the first guarantees of\nstrategic-robustness and incentive-compatibility for Gradient Ascent.\n  Concretely, we show that our algorithms achieve $O(\\sqrt{T})$ regret when the\nhighest competing bids are generated adversarially, and show that no online\nalgorithm can do better. We further prove that the regret reduces to $O(\\log\nT)$ when the competition is stationary and stochastic, which drastically\nimproves upon the previous best of $O(\\sqrt{T})$. Moving beyond regret, we show\nthat a strategic seller cannot exploit our algorithms to extract more revenue\non average than is possible under the optimal mechanism. Finally, we prove that\nour algorithm is also incentive compatible -- it is a (nearly) dominant\nstrategy for the buyer to report her values truthfully to the algorithm as a\nwhole. Altogether, these guarantees make our algorithms the first to\nsimultaneously achieve both optimal regret and strategic-robustness.\n', ""  We consider repeated multi-unit auctions with uniform pricing, which are\nwidely used in practice for allocating goods such as carbon licenses. In each\nround, $K$ identical units of a good are sold to a group of buyers that have\nvaluations with diminishing marginal returns. The buyers submit bids for the\nunits, and then a price $p$ is set per unit so that all the units are sold. We\nconsider two variants of the auction, where the price is set to the $K$-th\nhighest bid and $(K+1)$-st highest bid, respectively.\n  We analyze the properties of this auction in both the offline and online\nsettings. In the offline setting, we consider the problem that one player $i$\nis facing: given access to a data set that contains the bids submitted by\ncompetitors in past auctions, find a bid vector that maximizes player $i$'s\ncumulative utility on the data set. We design a polynomial time algorithm for\nthis problem, by showing it is equivalent to finding a maximum-weight path on a\ncarefully constructed directed acyclic graph.\n  In the online setting, the players run learning algorithms to update their\nbids as they participate in the auction over time. Based on our offline\nalgorithm, we design efficient online learning algorithms for bidding. The\nalgorithms have sublinear regret, under both full information and bandit\nfeedback structures. We complement our online learning algorithms with regret\nlower bounds.\n  Finally, we analyze the quality of the equilibria in the worst case through\nthe lens of the core solution concept in the game among the bidders. We show\nthat the $(K+1)$-st price format is susceptible to collusion among the bidders;\nmeanwhile, the $K$-th price format does not have this issue.\n"", ""  Advertisers increasingly use automated bidding to optimize their ad campaigns\non online advertising platforms. Autobidding optimizes an advertiser's\nobjective subject to various constraints, e.g. average ROI and budget\nconstraints. In this paper, we study the problem of designing online\nautobidding algorithms to optimize value subject to ROI and budget constraints\nwhen the platform is running any mixture of first and second price auction.\n  We consider the following stochastic setting: There is an item for sale in\neach of $T$ rounds. In each round, buyers submit bids and an auction is run to\nsell the item. We focus on one buyer, possibly with budget and ROI constraints.\nWe assume that the buyer's value and the highest competing bid are drawn i.i.d.\nfrom some unknown (joint) distribution in each round. We design a low-regret\nbidding algorithm that satisfies the buyer's constraints. Our benchmark is the\nobjective value achievable by the best possible Lipschitz function that maps\nvalues to bids, which is rich enough to best respond to many different\ncorrelation structures between value and highest competing bid. Our main result\nis an algorithm with full information feedback that guarantees a near-optimal\n$\\tilde O(\\sqrt T)$ regret with respect to the best Lipschitz function. Our\nresult applies to a wide range of auctions, most notably any mixture of first\nand second price auctions (price is a convex combination of the first and\nsecond price). In addition, our result holds for both value-maximizing buyers\nand quasi-linear utility-maximizing buyers.\n  We also study the bandit setting, where we show an $\\Omega(T^{2/3})$ lower\nbound on the regret for first-price auctions, showing a large disparity between\nthe full information and bandit settings. We also design an algorithm with\n$\\tilde O(T^{3/4})$ regret, when the value distribution is known and is\nindependent of the highest competing bid.\n""]",Auction Theory and Bidding Strategies
97,96,88,96_pruning_pruningbench_pruner_prune,"['pruning', 'pruningbench', 'pruner', 'prune', 'pruned', 'sparse', 'sparsegpt', 'compression', 'memory', 'efficient']","['pruning', 'sparsity', 'compression', 'activation', 'retraining', 'sparse', 'pruner', 'weight', 'neurons', 'layer']","['pruner', 'sparsegpt', 'compression', 'models', 'sparsification', 'retraining', 'language', 'billion', 'quantization', 'llms']","['  To remove redundant components of large language models (LLMs) without\nincurring significant computational costs, this work focuses on single-shot\npruning without a retraining phase. We simplify the pruning process for\nTransformer-based LLMs by identifying a depth-2 pruning structure that\nfunctions independently. Additionally, we propose two inference-aware pruning\ncriteria derived from the optimization perspective of output approximation,\nwhich outperforms traditional training-aware metrics such as gradient and\nHessian. We also introduce a two-step reconstruction technique to mitigate\npruning errors without model retraining. Experimental results demonstrate that\nour approach significantly reduces computational costs and hardware\nrequirements while maintaining superior performance across various datasets and\nmodels.\n', '  Large Vision-Language Models (LVLMs) can understand the world comprehensively\nby integrating rich information from different modalities, achieving remarkable\nadvancements on various multimodal downstream tasks. However, deploying LVLMs\nis often problematic due to their massive computational/energy costs and carbon\nconsumption. Such issues make it infeasible to adopt conventional iterative\nglobal pruning, which is costly due to computing the Hessian matrix of the\nentire large model for sparsification. Alternatively, several studies have\nrecently proposed layer-wise pruning approaches to avoid the expensive\ncomputation of global pruning and efficiently compress model weights according\nto their importance within a layer. However, they often suffer from suboptimal\nmodel compression due to their lack of a global perspective. To address this\nlimitation in recent efficient pruning methods for large models, we propose\nEfficient Coarse-to-Fine LayerWise Pruning (ECoFLaP), a two-stage\ncoarse-to-fine weight pruning approach for LVLMs. We first determine the\nsparsity ratios of different layers or blocks by leveraging the global\nimportance score, which is efficiently computed based on the zeroth-order\napproximation of the global model gradients. Then, the model performs local\nlayer-wise unstructured weight pruning based on globally-informed sparsity\nratios. We validate our proposed method across various multimodal and unimodal\nmodels and datasets, demonstrating significant performance improvements over\nprevalent pruning techniques in the high-sparsity regime.\n', '  Despite the remarkable capabilities, Large Language Models (LLMs) face\ndeployment challenges due to their extensive size. Pruning methods drop a\nsubset of weights to accelerate, but many of them require retraining, which is\nprohibitively expensive and computationally demanding. Recently, post-training\npruning approaches introduced novel metrics, enabling the pruning of LLMs\nwithout retraining. However, these metrics require the involvement of human\nexperts and tedious trial and error. To efficiently identify superior pruning\nmetrics, we develop an automatic framework for searching symbolic pruning\nmetrics using genetic programming. In particular, we devise an elaborate search\nspace encompassing the existing pruning metrics to discover the potential\nsymbolic pruning metric. We propose an opposing operation simplification\nstrategy to increase the diversity of the population. In this way, Pruner-Zero\nallows auto-generation of symbolic pruning metrics. Based on the searched\nresults, we explore the correlation between pruning metrics and performance\nafter pruning and summarize some principles. Extensive experiments on LLaMA and\nLLaMA-2 on language modeling and zero-shot tasks demonstrate that our\nPruner-Zero obtains superior performance than SOTA post-training pruning\nmethods. Code at: \\url{https://github.com/pprp/Pruner-Zero}.\n']",Efficient Pruning Methods for Large Language Models
98,97,88,97_microgrid_renewable_learning_scheduling,"['microgrid', 'renewable', 'learning', 'scheduling', 'reinforcement', 'electricity', 'ev', 'energy', 'discharging', 'hvac']","['energy', 'charging', 'renewable', 'grid', 'power', 'reinforcement', 'control', 'residential', 'demand', 'electric']","['microgrid', 'scheduling', 'reinforcement', 'ev', 'discharging', 'agent', 'solar', 'vpp', 'controllers', 'sustainable']","['  Dairy farming consumes a significant amount of energy, making it an\nenergy-intensive sector within agriculture. Integrating renewable energy\ngeneration into dairy farming could help address this challenge. Effective\nbattery management is important for integrating renewable energy generation.\nManaging battery charging and discharging poses significant challenges because\nof fluctuations in electrical consumption, the intermittent nature of renewable\nenergy generation, and fluctuations in energy prices. Artificial Intelligence\n(AI) has the potential to significantly improve the use of renewable energy in\ndairy farming, however, there is limited research conducted in this particular\ndomain. This research considers Ireland as a case study as it works towards\nattaining its 2030 energy strategy centered on the utilization of renewable\nsources. This study proposes a Q-learning-based algorithm for scheduling\nbattery charging and discharging in a dairy farm setting. This research also\nexplores the effect of the proposed algorithm by adding wind generation data\nand considering additional case studies. The proposed algorithm reduces the\ncost of imported electricity from the grid by 13.41%, peak demand by 2%, and\n24.49% when utilizing wind generation. These results underline how\nreinforcement learning is highly effective in managing batteries in the dairy\nfarming sector.\n', ""  The increasing integration of electric vehicles (EVs) into the grid can pose\na significant risk to the distribution system operation in the absence of\ncoordination. In response to the need for effective coordination of EVs within\nthe distribution network, this paper presents a safety-aware reinforcement\nlearning (RL) algorithm designed to manage EV charging stations while ensuring\nthe satisfaction of system constraints. Unlike existing methods, our proposed\nalgorithm does not rely on explicit penalties for constraint violations,\neliminating the need for penalty coefficient tuning. Furthermore, managing EV\ncharging stations is further complicated by multiple uncertainties, notably the\nvariability in solar energy generation and energy prices. To address this\nchallenge, we develop an off-policy RL algorithm to efficiently utilize data to\nlearn patterns in such uncertain environments. Our algorithm also incorporates\na maximum entropy framework to enhance the RL algorithm's exploratory process,\npreventing convergence to local optimal solutions. Simulation results\ndemonstrate that our algorithm outperforms traditional RL algorithms in\nmanaging EV charging in the distribution network.\n"", '  The widespread adoption of electric vehicles (EVs) poses several challenges\nto power distribution networks and smart grid infrastructure due to the\npossibility of significantly increasing electricity demands, especially during\npeak hours. Furthermore, when EVs participate in demand-side management\nprograms, charging expenses can be reduced by using optimal charging control\npolicies that fully utilize real-time pricing schemes. However, devising\noptimal charging methods and control strategies for EVs is challenging due to\nvarious stochastic and uncertain environmental factors. Currently, most EV\ncharging controllers operate based on a centralized model. In this paper, we\nintroduce a novel approach for distributed and cooperative charging strategy\nusing a Multi-Agent Reinforcement Learning (MARL) framework. Our method is\nbuilt upon the Deep Deterministic Policy Gradient (DDPG) algorithm for a group\nof EVs in a residential community, where all EVs are connected to a shared\ntransformer. This method, referred to as CTDE-DDPG, adopts a Centralized\nTraining Decentralized Execution (CTDE) approach to establish cooperation\nbetween agents during the training phase, while ensuring a distributed and\nprivacy-preserving operation during execution. We theoretically examine the\nperformance of centralized and decentralized critics for the DDPG-based MARL\nimplementation and demonstrate their trade-offs. Furthermore, we numerically\nexplore the efficiency, scalability, and performance of centralized and\ndecentralized critics. Our theoretical and numerical results indicate that,\ndespite higher policy gradient variances and training complexity, the CTDE-DDPG\nframework significantly improves charging efficiency by reducing total\nvariation by approximately %36 and charging cost by around %9.1 on average...\n']","""Optimizing Renewable Energy and Electric Vehicle Charging with AI"""
99,98,87,98_forecasting_lstm_predicting_stocks,"['forecasting', 'lstm', 'predicting', 'stocks', 'prediction', 'predict', 'stock', 'investing', 'predictions', 'trading']","['market', 'stock', 'price', 'financial', 'cryptocurrency', 'prices', 'portfolio', 'volatility', 'trading', 'investment']","['forecasting', 'lstm', 'investing', 'stockgpt', 'traders', 'forex', 'nasdaq', 'rnn', 'cryptocurrencies', 'volatility']","['  Predicting a fast and accurate model for stock price forecasting is been a\nchallenging task and this is an active area of research where it is yet to be\nfound which is the best way to forecast the stock price. Machine learning, deep\nlearning and statistical analysis techniques are used here to get the accurate\nresult so the investors can see the future trend and maximize the return of\ninvestment in stock trading. This paper will review many deep learning\nalgorithms for stock price forecasting. We use a record of s&p 500 index data\nfor training and testing. The survey motive is to check various deep learning\nand statistical model techniques for stock price forecasting that are Moving\nAverages, ARIMA which are statistical techniques and LSTM, RNN, CNN, and FULL\nCNN which are deep learning models. It will discuss various models, including\nthe Auto regression integration moving average model, the Recurrent neural\nnetwork model, the long short-term model which is the type of RNN used for long\ndependency for data, the convolutional neural network model, and the full\nconvolutional neural network model, in terms of error calculation or percentage\nof accuracy that how much it is accurate which measures by the function like\nRoot mean square error, mean absolute error, mean squared error. The model can\nbe used to predict the stock price by checking the low MAE value as lower the\nMAE value the difference between the predicting and the actual value will be\nless and this model will predict the price more accurately than other models.\n', ""  Navigating the intricate landscape of financial markets requires adept\nforecasting of stock price movements. This paper delves into the potential of\nLong Short-Term Memory (LSTM) networks for predicting stock dynamics, with a\nfocus on discerning nuanced rise and fall patterns. Leveraging a dataset from\nthe New York Stock Exchange (NYSE), the study incorporates multiple features to\nenhance LSTM's capacity in capturing complex patterns. Visualization of key\nattributes, such as opening, closing, low, and high prices, aids in unraveling\nsubtle distinctions crucial for comprehensive market understanding. The\nmeticulously crafted LSTM input structure, inspired by established guidelines,\nincorporates both price and volume attributes over a 25-day time step, enabling\nthe model to capture temporal intricacies. A comprehensive methodology,\nincluding hyperparameter tuning with Grid Search, Early Stopping, and Callback\nmechanisms, leads to a remarkable 53% improvement in predictive accuracy. The\nstudy concludes with insights into model robustness, contributions to financial\nforecasting literature, and a roadmap for real-time stock market prediction.\nThe amalgamation of LSTM networks, strategic hyperparameter tuning, and\ninformed feature selection presents a potent framework for advancing the\naccuracy of stock price predictions, contributing substantively to financial\ntime series forecasting discourse.\n"", '  One of the most enticing research areas is the stock market, and projecting\nstock prices may help investors profit by making the best decisions at the\ncorrect time. Deep learning strategies have emerged as a critical technique in\nthe field of the financial market. The stock market is impacted due to two\naspects, one is the geo-political, social and global events on the bases of\nwhich the price trends could be affected. Meanwhile, the second aspect purely\nfocuses on historical price trends and seasonality, allowing us to forecast\nstock prices. In this paper, our aim is to focus on the second aspect and build\na model that predicts future prices with minimal errors. In order to provide\nbetter prediction results of stock price, we propose a new model named Long\nShort-Term Memory (LSTM) with Sequential Self-Attention Mechanism (LSTM-SSAM).\nFinally, we conduct extensive experiments on the three stock datasets: SBIN,\nHDFCBANK, and BANKBARODA. The experimental results prove the effectiveness and\nfeasibility of the proposed model compared to existing models. The experimental\nfindings demonstrate that the root-mean-squared error (RMSE), and R-square (R2)\nevaluation indicators are giving the best results.\n']",Stock Price Forecasting with Deep Learning
100,99,86,99_clusterings_clustering_cluster_clusters,"['clusterings', 'clustering', 'cluster', 'clusters', 'clustered', 'dbscan', 'algorithms', 'datasets', 'algorithm', 'hpclust']","['clustering', 'clusters', 'cluster', 'index', 'points', 'algorithm', 'algorithms', 'density', 'distance', 'validity']","['clusterings', 'dbscan', 'hpclust', 'unsupervised', 'centroid', 'dendrogram', 'hdbscan', 'dimensionality', 'validation', 'pca']","[""  Clustering algorithms aim to organize data into groups or clusters based on\nthe inherent patterns and similarities within the data. They play an important\nrole in today's life, such as in marketing and e-commerce, healthcare, data\norganization and analysis, and social media. Numerous clustering algorithms\nexist, with ongoing developments introducing new ones. Each algorithm possesses\nits own set of strengths and weaknesses, and as of now, there is no universally\napplicable algorithm for all tasks. In this work, we analyzed existing\nclustering algorithms and classify mainstream algorithms across five different\ndimensions: underlying principles and characteristics, data point assignment to\nclusters, dataset capacity, predefined cluster numbers and application area.\nThis classification facilitates researchers in understanding clustering\nalgorithms from various perspectives and helps them identify algorithms\nsuitable for solving specific tasks. Finally, we discussed the current trends\nand potential future directions in clustering algorithms. We also identified\nand discussed open challenges and unresolved issues in the field.\n"", '  This paper focuses on density-based clustering, particularly the Density Peak\n(DP) algorithm and the one based on density-connectivity DBSCAN; and proposes a\nnew method which takes advantage of the individual strengths of these two\nmethods to yield a density-based hierarchical clustering algorithm. Our\ninvestigation begins with formally defining the types of clusters DP and DBSCAN\nare designed to detect; and then identifies the kinds of distributions that DP\nand DBSCAN individually fail to detect all clusters in a dataset. These\nidentified weaknesses inspire us to formally define a new kind of clusters and\npropose a new method called DC-HDP to overcome these weaknesses to identify\nclusters with arbitrary shapes and varied densities. In addition, the new\nmethod produces a richer clustering result in terms of hierarchy or dendrogram\nfor better cluster structures understanding. Our empirical evaluation results\nshow that DC-HDP produces the best clustering results on 14 datasets in\ncomparison with 7 state-of-the-art clustering algorithms.\n', '  Data clustering involves identifying latent similarities within a dataset and\norganizing them into clusters or groups. The outcomes of various clustering\nalgorithms differ as they are susceptible to the intrinsic characteristics of\nthe original dataset, including noise and dimensionality. The effectiveness of\nsuch clustering procedures directly impacts the homogeneity of clusters,\nunderscoring the significance of evaluating algorithmic outcomes. Consequently,\nthe assessment of clustering quality presents a significant and complex\nendeavor. A pivotal aspect affecting clustering validation is the cluster\nvalidity metric, which aids in determining the optimal number of clusters. The\nmain goal of this study is to comprehensively review and explain the\nmathematical operation of internal and external cluster validity indices, but\nnot all, to categorize these indices and to brainstorm suggestions for future\nadvancement of clustering validation research. In addition, we review and\nevaluate the performance of internal and external clustering validation indices\non the most common clustering algorithms, such as the evolutionary clustering\nalgorithm star (ECA*). Finally, we suggest a classification framework for\nexamining the functionality of both internal and external clustering validation\nmeasures regarding their ideal values, user-friendliness, responsiveness to\ninput data, and appropriateness across various fields. This classification aids\nresearchers in selecting the appropriate clustering validation measure to suit\ntheir specific requirements.\n']",Clustering Algorithms and Techniques
101,100,86,100_safety_safetybench_unsafe_safeguards,"['safety', 'safetybench', 'unsafe', 'safeguards', 'safer', 'safe', 'language', 'helpfulness', 'languages', 'malicious']","['safety', 'unsafe', 'harmful', 'safe', 'alignment', 'helpfulness', 'toxic', 'content', 'prompts', 'tuning']","['safetybench', 'unsafe', 'helpfulness', 'defensiveness', 'harmlessness', 'multilingual', 'prompts', 'evaluation', 'toxicity', 'guidelines']","[""  Recent progress in large language models (LLMs) has led to their widespread\nadoption in various domains. However, these advancements have also introduced\nadditional safety risks and raised concerns regarding their detrimental impact\non already marginalized populations. Despite growing mitigation efforts to\ndevelop safety safeguards, such as supervised safety-oriented fine-tuning and\nleveraging safe reinforcement learning from human feedback, multiple concerns\nregarding the safety and ingrained biases in these models remain. Furthermore,\nprevious work has demonstrated that models optimized for safety often display\nexaggerated safety behaviors, such as a tendency to refrain from responding to\ncertain requests as a precautionary measure. As such, a clear trade-off between\nthe helpfulness and safety of these models has been documented in the\nliterature. In this paper, we further investigate the effectiveness of safety\nmeasures by evaluating models on already mitigated biases. Using the case of\nLlama 2 as an example, we illustrate how LLMs' safety responses can still\nencode harmful assumptions. To do so, we create a set of non-toxic prompts,\nwhich we then use to evaluate Llama models. Through our new taxonomy of LLMs\nresponses to users, we observe that the safety/helpfulness trade-offs are more\npronounced for certain demographic groups which can lead to quality-of-service\nharms for marginalized populations.\n"", '  Training large language models to follow instructions makes them perform\nbetter on a wide range of tasks and generally become more helpful. However, a\nperfectly helpful model will follow even the most malicious instructions and\nreadily generate harmful content. In this paper, we raise concerns over the\nsafety of models that only emphasize helpfulness, not harmlessness, in their\ninstruction-tuning. We show that several popular instruction-tuned models are\nhighly unsafe. Moreover, we show that adding just 3% safety examples (a few\nhundred demonstrations) when fine-tuning a model like LLaMA can substantially\nimprove its safety. Our safety-tuning does not make models significantly less\ncapable or helpful as measured by standard benchmarks. However, we do find\nexaggerated safety behaviours, where too much safety-tuning makes models refuse\nperfectly safe prompts if they superficially resemble unsafe ones. As a whole,\nour results illustrate trade-offs in training LLMs to be helpful and training\nthem to be safe.\n', '  Ensuring the safe alignment of large language models (LLMs) with human values\nis critical as they become integral to applications like translation and\nquestion answering. Current alignment methods struggle with dynamic user\nintentions and complex objectives, making models vulnerable to generating\nharmful content. We propose Safety Arithmetic, a training-free framework\nenhancing LLM safety across different scenarios: Base models, Supervised\nfine-tuned models (SFT), and Edited models. Safety Arithmetic involves Harm\nDirection Removal to avoid harmful content and Safety Alignment to promote safe\nresponses. Additionally, we present NoIntentEdit, a dataset highlighting edit\ninstances that could compromise model safety if used unintentionally. Our\nexperiments show that Safety Arithmetic significantly improves safety measures,\nreduces over-safety, and maintains model utility, outperforming existing\nmethods in ensuring safe content generation.\n']",Language Model Safety and Bias Mitigation
102,101,85,101_topological_topologically_topology_networks,"['topological', 'topologically', 'topology', 'networks', 'homology', 'classification', 'graphcodes', 'neural', 'complexes', 'deep']","['topological', 'homology', 'persistent', 'persistence', 'complexes', 'simplicial', 'topology', 'geometric', 'graph', 'descriptors']","['topological', 'networks', 'homology', 'graphcodes', 'complexes', 'dimensional', 'simplicial', 'dnns', 'fractal', 'cloud']","['  Persistent homology, a technique from computational topology, has recently\nshown strong empirical performance in the context of graph classification.\nBeing able to capture long range graph properties via higher-order topological\nfeatures, such as cycles of arbitrary length, in combination with multi-scale\ntopological descriptors, has improved predictive performance for data sets with\nprominent topological structures, such as molecules. At the same time, the\ntheoretical properties of persistent homology have not been formally assessed\nin this context. This paper intends to bridge the gap between computational\ntopology and graph machine learning by providing a brief introduction to\npersistent homology in the context of graphs, as well as a theoretical\ndiscussion and empirical analysis of its expressivity for graph learning tasks.\n', ""  We specialize techniques from topological data analysis to the problem of\ncharacterizing the topological complexity (as defined in the body of the paper)\nof a multi-class data set. As a by-product, a topological classifier is defined\nthat uses an open sub-covering of the data set. This sub-covering can be used\nto construct a simplicial complex whose topological features (e.g., Betti\nnumbers) provide information about the classification problem. We use these\ntopological constructs to study the impact of topological complexity on\nlearning in feedforward deep neural networks (DNNs). We hypothesize that\ntopological complexity is negatively correlated with the ability of a fully\nconnected feedforward deep neural network to learn to classify data correctly.\nWe evaluate our topological classification algorithm on multiple constructed\nand open source data sets. We also validate our hypothesis regarding the\nrelationship between topological complexity and learning in DNN's on multiple\ndata sets.\n"", '  Topological Data Analysis (TDA) allows us to extract powerful topological and\nhigher-order information on the global shape of a data set or point cloud.\nTools like Persistent Homology or the Euler Transform give a single complex\ndescription of the global structure of the point cloud. However, common machine\nlearning applications like classification require point-level information and\nfeatures to be available. In this paper, we bridge this gap and propose a novel\nmethod to extract node-level topological features from complex point clouds\nusing discrete variants of concepts from algebraic topology and differential\ngeometry. We verify the effectiveness of these topological point features\n(TOPF) on both synthetic and real-world data and study their robustness under\nnoise.\n']",Topological Data Analysis for Graphs and Networks
103,102,80,102_ontologies_ontology_ontological_semantic,"['ontologies', 'ontology', 'ontological', 'semantic', 'ontologically', 'semantics', 'owl', 'owl2vec', 'entities', 'rdf']","['ontology', 'ontologies', 'ontological', 'axioms', 'knowledge', 'logic', 'management', 'concepts', 'domain', 'formal']","['ontologies', 'semantics', 'owl2vec', 'rdf', 'documentation', 'datalog', 'interoperability', 'automation', 'deductive', 'metamodeling']","['  Ontology alignment, a critical process in the Semantic Web for detecting\nrelationships between different ontologies, has traditionally focused on\nidentifying so-called ""simple"" 1-to-1 relationships through class labels and\nproperties comparison. The more practically useful exploration of more complex\nalignments remains a hard problem to automate, and as such is largely\nunderexplored, i.e. in application practice it is usually done manually by\nontology and domain experts. Recently, the surge in Natural Language Processing\n(NLP) capabilities, driven by advancements in Large Language Models (LLMs),\npresents new opportunities for enhancing ontology engineering practices,\nincluding ontology alignment tasks. This paper investigates the application of\nLLM technologies to tackle the complex ontology alignment challenge. Leveraging\na prompt-based approach and integrating rich ontology content so-called modules\nour work constitutes a significant advance towards automating the complex\nalignment task.\n', '  Ontologies are widely used for representing domain knowledge and meta data,\nplaying an increasingly important role in Information Systems, the Semantic\nWeb, Bioinformatics and many other domains. However, logical reasoning that\nontologies can directly support are quite limited in learning, approximation\nand prediction. One straightforward solution is to integrate statistical\nanalysis and machine learning. To this end, automatically learning vector\nrepresentation for knowledge of an ontology i.e., ontology embedding has been\nwidely investigated in recent years. Numerous papers have been published on\nontology embedding, but a lack of systematic reviews hinders researchers from\ngaining a comprehensive understanding of this field. To bridge this gap, we\nwrite this survey paper, which first introduces different kinds of semantics of\nontologies, and formally defines ontology embedding from the perspectives of\nboth mathematics and machine learning, as well as its property of faithfulness.\nBased on this, it systematically categorises and analyses a relatively complete\nset of over 80 papers, according to the ontologies and semantics that they aim\nat, and their technical solutions including geometric modeling, sequence\nmodeling and graph propagation. This survey also introduces the applications of\nontology embedding in ontology engineering, machine learning augmentation and\nlife sciences, presents a new library mOWL, and discusses the challenges and\nfuture directions.\n', '  Ontologies provide formal representation of knowledge shared within Semantic\nWeb applications. Ontology learning involves the construction of ontologies\nfrom a given corpus. In the past years, ontology learning has traversed through\nshallow learning and deep learning methodologies, each offering distinct\nadvantages and limitations in the quest for knowledge extraction and\nrepresentation. A new trend of these approaches is relying on large language\nmodels (LLMs) to enhance ontology learning. This paper gives a review in\napproaches and challenges of ontology learning. It analyzes the methodologies\nand limitations of shallow-learning-based and deep-learning-based techniques\nfor ontology learning, and provides comprehensive knowledge for the frontier\nwork of using LLMs to enhance ontology learning. In addition, it proposes\nseveral noteworthy future directions for further exploration into the\nintegration of LLMs with ontology learning tasks.\n']",Ontology Engineering and Semantic Web
104,103,80,103_regularization_regularized_factorization_matrix,"['regularization', 'regularized', 'factorization', 'matrix', 'sparse', 'matrices', 'minimization', 'completion', 'pca', 'semidefinite']","['matrix', 'factorization', 'rank', 'nonnegative', 'completion', 'entries', 'principal', 'matrices', 'low', 'row']","['regularization', 'factorization', 'pca', 'semidefinite', 'spectral', 'rank', 'nmf', 'columns', 'sparsity', 'nonconvex']","['  When applying nonnegative matrix factorization (NMF), generally the rank\nparameter is unknown. Such rank in NMF, called the nonnegative rank, is usually\nestimated heuristically since computing the exact value of it is NP-hard. In\nthis work, we propose an approximation method to estimate such rank while\nsolving NMF on-the-fly. We use sum-of-norm (SON), a group-lasso structure that\nencourages pairwise similarity, to reduce the rank of a factor matrix where the\nrank is overestimated at the beginning. On various datasets, SON-NMF is able to\nreveal the correct nonnegative rank of the data without any prior knowledge nor\ntuning.\n  SON-NMF is a nonconvx nonsmmoth non-separable non-proximable problem, solving\nit is nontrivial. First, as rank estimation in NMF is NP-hard, the proposed\napproach does not enjoy a lower computational complexity. Using a\ngraph-theoretic argument, we prove that the complexity of the SON-NMF is almost\nirreducible. Second, the per-iteration cost of any algorithm solving SON-NMF is\npossibly high, which motivated us to propose a first-order BCD algorithm to\napproximately solve SON-NMF with a low per-iteration cost, in which we do so by\nthe proximal average operator. Lastly, we propose a simple greedy method for\npost-processing.\n  SON-NMF exhibits favourable features for applications. Beside the ability to\nautomatically estimate the rank from data, SON-NMF can deal with rank-deficient\ndata matrix, can detect weak component with small energy. Furthermore, on the\napplication of hyperspectral imaging, SON-NMF handle the issue of spectral\nvariability naturally.\n', '  In this paper, we develop a relative error bound for nuclear norm regularized\nmatrix completion, with the focus on the completion of full-rank matrices.\nUnder the assumption that the top eigenspaces of the target matrix are\nincoherent, we derive a relative upper bound for recovering the best low-rank\napproximation of the unknown matrix. Although multiple works have been devoted\nto analyzing the recovery error of full-rank matrix completion, their error\nbounds are usually additive, making it impossible to obtain the perfect\nrecovery case and more generally difficult to leverage the skewed distribution\nof eigenvalues. Our analysis is built upon the optimality condition of the\nregularized formulation and existing guarantees for low-rank matrix completion.\nTo the best of our knowledge, this is the first relative bound that has been\nproved for the regularized formulation of matrix completion.\n', '  This paper considers the problem of estimating a low-rank matrix from the\nobservation of all or a subset of its entries in the presence of Poisson noise.\nWhen we observe all entries, this is a problem of matrix denoising; when we\nobserve only a subset of the entries, this is a problem of matrix completion.\nIn both cases, we exploit an assumption that the underlying matrix is low-rank.\nSpecifically, we analyze several estimators, including a constrained\nnuclear-norm minimization program, nuclear-norm regularized least squares, and\na nonconvex constrained low-rank optimization problem. We show that for all\nthree estimators, with high probability, we have an upper error bound (in the\nFrobenius norm error metric) that depends on the matrix rank, the fraction of\nthe elements observed, and maximal row and column sums of the true matrix. We\nfurthermore show that the above results are minimax optimal (within a universal\nconstant) in classes of matrices with low rank and bounded row and column sums.\nWe also extend these results to handle the case of matrix multinomial denoising\nand completion.\n']",Matrix Factorization and Regularization Techniques
105,104,79,104_soccernet_esports_sports_soccer,"['soccernet', 'esports', 'sports', 'soccer', 'sport', 'athletes', 'soccerrag', 'players', 'athlete', 'matches']","['sports', 'soccer', 'players', 'player', 'game', 'tennis', 'badminton', 'esports', 'football', 'match']","['soccernet', 'esports', 'sports', 'players', 'athlete', 'commentary', 'tournaments', 'badminton', 'coaches', 'nba']","['  In this paper, we present a novel sequential team selection model in soccer.\nSpecifically, we model the stochastic process of player injury and\nunavailability using player-specific information learned from real-world soccer\ndata. Monte-Carlo Tree Search is used to select teams for games that optimise\nlong-term team performance across a soccer season by reasoning over player\ninjury probability. We validate our approach compared to benchmark solutions\nfor the 2018/19 English Premier League season. Our model achieves similar\nseason expected points to the benchmark whilst reducing first-team injuries by\n~13% and the money inefficiently spent on injured players by ~11% -\ndemonstrating the potential to reduce costs and improve player welfare in\nreal-world soccer teams.\n', '  Understanding sports is crucial for the advancement of Natural Language\nProcessing (NLP) due to its intricate and dynamic nature. Reasoning over\ncomplex sports scenarios has posed significant challenges to current NLP\ntechnologies which require advanced cognitive capabilities. Toward addressing\nthe limitations of existing benchmarks on sports understanding in the NLP\nfield, we extensively evaluated mainstream large language models for various\nsports tasks. Our evaluation spans from simple queries on basic rules and\nhistorical facts to complex, context-specific reasoning, leveraging strategies\nfrom zero-shot to few-shot learning, and chain-of-thought techniques. In\naddition to unimodal analysis, we further assessed the sports reasoning\ncapabilities of mainstream video language models to bridge the gap in\nmultimodal sports understanding benchmarking. Our findings highlighted the\ncritical challenges of sports understanding for NLP. We proposed a new\nbenchmark based on a comprehensive overview of existing sports datasets and\nprovided extensive error analysis which we hope can help identify future\nresearch priorities in this field.\n', '  This paper represents an analysis on the momentum of tennis match. And due to\nGeneralization performance of it, it can be helpful in constructing a system to\npredict the result of sports game and analyze the performance of player based\non the Technical statistics. We First use hidden markov models to predict the\nmomentum which is defined as the performance of players. Then we use Xgboost to\nprove the significance of momentum. Finally we use LightGBM to evaluate the\nperformance of our model and use SHAP feature importance ranking and weight\nanalysis to find the key points that affect the performance of players.\n']",Sports Analytics and Performance Prediction
106,105,79,105_watermarking_watermark_watermarked_deeptextmark,"['watermarking', 'watermark', 'watermarked', 'deeptextmark', 'watermarks', 'postmark', 'text', 'steganographic', 'tokens', 'steganography']","['watermarking', 'watermark', 'watermarks', 'watermarked', 'steganography', 'text', 'steganographic', 'secret', 'detectable', 'schemes']","['watermarking', 'deeptextmark', 'steganographic', 'stegotext', 'embedding', 'paraphrasing', 'token', 'semstamp', 'spoofing', 'codeip']","['  With the widespread adoption of Large Language Models (LLMs), concerns about\npotential misuse have emerged. To this end, watermarking has been adapted to\nLLM, enabling a simple and effective way to detect and monitor generated text.\nHowever, while the existing methods can differentiate between watermarked and\nunwatermarked text with high accuracy, they often face a trade-off between the\nquality of the generated text and the effectiveness of the watermarking\nprocess. In this work, we present a novel type of LLM watermark, Sparse\nWatermark, which aims to mitigate this trade-off by applying watermarks to a\nsmall subset of generated tokens distributed across the text. The key strategy\ninvolves anchoring watermarked tokens to words that have specific\nPart-of-Speech (POS) tags. Our experimental results demonstrate that the\nproposed watermarking scheme achieves high detectability while generating text\nthat outperforms previous LLM watermarking methods in quality across various\ntasks\n', '  Methods for watermarking large language models have been proposed that\ndistinguish AI-generated text from human-generated text by slightly altering\nthe model output distribution, but they also distort the quality of the text,\nexposing the watermark to adversarial detection. More recently, distortion-free\nwatermarking methods were proposed that require a secret key to detect the\nwatermark. The prior methods generally embed zero-bit watermarks that do not\nprovide additional information beyond tagging a text as being AI-generated. We\nextend an existing zero-bit distortion-free watermarking method by embedding\nmultiple bits of meta-information as part of the watermark. We also develop a\ncomputationally efficient decoder that extracts the embedded information from\nthe watermark with low bit error rate.\n', '  Watermarking of language model outputs enables statistical detection of\nmodel-generated text, which can mitigate harms and misuses of language models.\nExisting watermarking strategies operate by altering the decoder of an existing\nlanguage model. In this paper, we ask whether language models can directly\nlearn to generate watermarked text, which would have significant implications\nfor the real-world deployment of watermarks. First, learned watermarks could be\nused to build open models that naturally generate watermarked text, enabling\nwatermarking for open models, where users can control the decoding procedure.\nSecond, if watermarking is used to determine the provenance of generated text,\nan adversary can hurt the reputation of a victim model by spoofing its\nwatermark and generating damaging watermarked text. To investigate the\nlearnability of watermarks, we propose watermark distillation, which trains a\nstudent model to behave like a teacher model that uses decoding-based\nwatermarking. We test our approach on three decoding-based watermarking\nstrategies and various hyperparameter settings, finding that models can learn\nto generate watermarked text with high detectability. We also find limitations\nto learnability, including the loss of watermarking capabilities under\nfine-tuning on normal text and high sample complexity when learning\nlow-distortion watermarks.\n']",Watermarking Techniques for Language Models
107,106,77,106_nl2sql_database_sql_databases,"['nl2sql', 'database', 'sql', 'databases', 'querying', 'sqlfuse', 'queries', 'schemas', 'metasql', 'schema']","['schema', 'database', 'databases', 'queries', 'query', 'text', 'columns', 'execution', 'tables', 'questions']","['nl2sql', 'sqlfuse', 'queries', 'schemas', 'metasql', 'parsers', 's2sql', 'nlidb', 'trustsql', 'cql']","[""  Generating accurate Structured Querying Language (SQL) is a long-standing\nproblem, especially in matching users' semantic queries with structured\ndatabases and then generating structured SQL. Existing models typically input\nqueries and database schemas into the LLM and rely on the LLM to perform\nsemantic-structure matching and generate structured SQL. However, such\nsolutions overlook the structural information within user queries and\ndatabases, which can be utilized to enhance the generation of structured SQL.\nThis oversight can lead to inaccurate or unexecutable SQL generation. To fully\nexploit the structure, we propose a structure-to-SQL framework, which leverages\nthe inherent structure information to improve the SQL generation of LLMs.\nSpecifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.\nSGU-SQL first links user queries and databases in a structure-enhanced manner.\nIt then decomposes complicated linked structures with grammar trees to guide\nthe LLM to generate the SQL step by step. Extensive experiments on two\nbenchmark datasets illustrate that SGU-SQL can outperform sixteen SQL\ngeneration baselines.\n"", ""  Text-to-SQL, the process of translating natural language into Structured\nQuery Language (SQL), represents a transformative application of large language\nmodels (LLMs), potentially revolutionizing how humans interact with data. This\npaper introduces the SQL-PaLM framework, a comprehensive solution for\nunderstanding and enhancing Text-to-SQL using LLMs, using in the learning\nregimes of few-shot prompting and instruction fine-tuning. With few-shot\nprompting, we explore the effectiveness of consistency decoding with\nexecution-based error filtering. With instruction fine-tuning, we delve deep in\nunderstanding the critical paradigms that influence the performance of tuned\nLLMs. In particular, we investigate how performance can be improved through\nexpanded training data coverage and diversity, synthetic data augmentation, and\nintegrating query-specific database content. We propose a test-time selection\nmethod to further refine accuracy by integrating SQL outputs from multiple\nparadigms with execution feedback as guidance. Additionally, we tackle the\npractical challenge of navigating intricate databases with a significant number\nof tables and columns, proposing efficient techniques for accurately selecting\nrelevant database elements to enhance Text-to-SQL performance. Our holistic\napproach yields substantial advancements in Text-to-SQL, as demonstrated on two\nkey public benchmarks, Spider and BIRD. Through comprehensive ablations and\nerror analyses, we shed light on the strengths and weaknesses of our framework,\noffering valuable insights into Text-to-SQL's future work.\n"", '  Generating accurate SQL from natural language questions (text-to-SQL) is a\nlong-standing challenge due to the complexities in user question understanding,\ndatabase schema comprehension, and SQL generation. Conventional text-to-SQL\nsystems, comprising human engineering and deep neural networks, have made\nsubstantial progress. Subsequently, pre-trained language models (PLMs) have\nbeen developed and utilized for text-to-SQL tasks, achieving promising\nperformance. As modern databases become more complex, the corresponding user\nquestions also grow more challenging, causing PLMs with parameter constraints\nto produce incorrect SQL. This necessitates more sophisticated and tailored\noptimization methods, which, in turn, restricts the applications of PLM-based\nsystems. Recently, large language models (LLMs) have demonstrated significant\ncapabilities in natural language understanding as the model scale increases.\nTherefore, integrating LLM-based implementation can bring unique opportunities,\nimprovements, and solutions to text-to-SQL research. In this survey, we present\na comprehensive review of LLM-based text-to-SQL. Specifically, we propose a\nbrief overview of the technical challenges and the evolutionary process of\ntext-to-SQL. Then, we provide a detailed introduction to the datasets and\nmetrics designed to evaluate text-to-SQL systems. After that, we present a\nsystematic analysis of recent advances in LLM-based text-to-SQL. Finally, we\ndiscuss the remaining challenges in this field and propose expectations for\nfuture research directions.\n']",Natural Language to SQL Generation
108,107,77,107_creativity_creative_generative_ai,"['creativity', 'creative', 'generative', 'ai', 'ideation', 'storymaking', 'creation', 'creators', 'generated', 'writers']","['creativity', 'creative', 'writing', 'stories', 'story', 'storytelling', 'ideas', 'writers', 'human', 'writer']","['creative', 'generative', 'ai', 'ideation', 'storymaking', 'creators', 'genai', 'novelty', 'author', 'personas']","[""  In the field of natural language processing, the rapid development of large\nlanguage model (LLM) has attracted more and more attention. LLMs have shown a\nhigh level of creativity in various tasks, but the methods for assessing such\ncreativity are inadequate. The assessment of LLM creativity needs to consider\ndifferences from humans, requiring multi-dimensional measurement while\nbalancing accuracy and efficiency. This paper aims to establish an efficient\nframework for assessing the level of creativity in LLMs. By adapting the\nmodified Torrance Tests of Creative Thinking, the research evaluates the\ncreative performance of various LLMs across 7 tasks, emphasizing 4 criteria\nincluding Fluency, Flexibility, Originality, and Elaboration. In this context,\nwe develop a comprehensive dataset of 700 questions for testing and an\nLLM-based evaluation method. In addition, this study presents a novel analysis\nof LLMs' responses to diverse prompts and role-play situations. We found that\nthe creativity of LLMs primarily falls short in originality, while excelling in\nelaboration. Besides, the use of prompts and the role-play settings of the\nmodel significantly influence creativity. Additionally, the experimental\nresults also indicate that collaboration among multiple LLMs can enhance\noriginality. Notably, our findings reveal a consensus between human evaluations\nand LLMs regarding the personality traits that influence creativity. The\nfindings underscore the significant impact of LLM design on creativity and\nbridges artificial intelligence and human creativity, offering insights into\nLLMs' creativity and potential applications.\n"", ""  Creativity is core to being human. Generative artificial intelligence (GenAI)\nholds promise for humans to be more creative by offering new ideas, or less\ncreative by anchoring on GenAI ideas. We study the causal impact of GenAI on\nthe production of a creative output in an online experimental study where some\nwriters are could obtain ideas for a story from a GenAI platform. Access to\nGenAI ideas causes an increase in the writer's creativity with stories being\nevaluated as better written and more enjoyable, especially among less creative\nwriters. However, GenAI-enabled stories are more similar to each other than\nstories by humans alone. Our results have implications for researchers,\npolicy-makers and practitioners interested in bolstering creativity, but point\nto potential downstream consequences from over-reliance.\n"", ""  Creativity serves as a cornerstone for societal progress and innovation. With\nthe rise of advanced generative AI models capable of tasks once reserved for\nhuman creativity, the study of AI's creative potential becomes imperative for\nits responsible development and application. In this paper, we prove in theory\nthat AI can be as creative as humans under the condition that it can properly\nfit the data generated by human creators. Therefore, the debate on AI's\ncreativity is reduced into the question of its ability to fit a sufficient\namount of data. To arrive at this conclusion, this paper first addresses the\ncomplexities in defining creativity by introducing a new concept called\nRelative Creativity. Rather than attempting to define creativity universally,\nwe shift the focus to whether AI can match the creative abilities of a\nhypothetical human. The methodological shift leads to a statistically\nquantifiable assessment of AI's creativity, term Statistical Creativity. This\nconcept, statistically comparing the creative abilities of AI with those of\nspecific human groups, facilitates theoretical exploration of AI's creative\npotential. Our analysis reveals that by fitting extensive conditional data\nwithout marginalizing out the generative conditions, AI can emerge as a\nhypothetical new creator. The creator possesses the same creative abilities on\npar with the human creators it was trained on. Building on theoretical\nfindings, we discuss the application in prompt-conditioned autoregressive\nmodels, providing a practical means for evaluating creative abilities of\ngenerative AI models, such as Large Language Models (LLMs). Additionally, this\nstudy provides an actionable training guideline, bridging the theoretical\nquantification of creativity with practical model training.\n""]","""Assessing Creativity in Artificial Intelligence"""
109,108,76,108_emotions_emotion_emotionic_emotional,"['emotions', 'emotion', 'emotionic', 'emotional', 'affective', 'conversations', 'multimodal', 'arousal', 'utterances', 'emotionally']","['emotion', 'emotions', 'conversation', 'emotional', 'recognition', 'multimodal', 'fusion', 'utterance', 'affective', 'modalities']","['emotions', 'multimodal', 'dialogues', 'valence', 'features', 'extract', 'subtask', 'erc', 'audio', 'cfn']","['  Automatic emotion recognition in conversation (ERC) is crucial for\nemotion-aware conversational artificial intelligence. This paper proposes a\ndistribution-based framework that formulates ERC as a sequence-to-sequence\nproblem for emotion distribution estimation. The inherent ambiguity of emotions\nand the subjectivity of human perception lead to disagreements in emotion\nlabels, which is handled naturally in our framework from the perspective of\nuncertainty estimation in emotion distributions. A Bayesian training loss is\nintroduced to improve the uncertainty estimation by conditioning each emotional\nstate on an utterance-specific Dirichlet prior distribution. Experimental\nresults on the IEMOCAP dataset show that ERC outperformed the\nsingle-utterance-based system, and the proposed distribution-based ERC methods\nhave not only better classification accuracy, but also show improved\nuncertainty estimation.\n', '  In human-computer interaction, it is crucial for agents to respond to human\nby understanding their emotions. Unraveling the causes of emotions is more\nchallenging. A new task named Multimodal Emotion-Cause Pair Extraction in\nConversations is responsible for recognizing emotion and identifying causal\nexpressions. In this study, we propose a multi-stage framework to generate\nemotion and extract the emotion causal pairs given the target emotion. In the\nfirst stage, Llama-2-based InstructERC is utilized to extract the emotion\ncategory of each utterance in a conversation. After emotion recognition, a\ntwo-stream attention model is employed to extract the emotion causal pairs\ngiven the target emotion for subtask 2 while MuTEC is employed to extract\ncausal span for subtask 1. Our approach achieved first place for both of the\ntwo subtasks in the competition.\n', '  The purpose of emotion recognition in conversation (ERC) is to identify the\nemotion category of an utterance based on contextual information. Previous ERC\nmethods relied on simple connections for cross-modal fusion and ignored the\ninformation differences between modalities, resulting in the model being unable\nto focus on modality-specific emotional information. At the same time, the\nshared information between modalities was not processed to generate emotions.\nInformation redundancy problem. To overcome these limitations, we propose a\ncross-modal fusion emotion prediction network based on vector connections. The\nnetwork mainly includes two stages: the multi-modal feature fusion stage based\non connection vectors and the emotion classification stage based on fused\nfeatures. Furthermore, we design a supervised inter-class contrastive learning\nmodule based on emotion labels. Experimental results confirm the effectiveness\nof the proposed method, demonstrating excellent performance on the IEMOCAP and\nMELD datasets.\n']",Emotion Recognition in Conversations
110,109,76,109_drafts_draft_speculative_decoding,"['drafts', 'draft', 'speculative', 'decoding', 'memory', 'drafter', 'drafters', 'drafting', 'language', 'bottleneck']","['speculative', 'draft', 'decoding', 'tokens', 'token', 'autoregressive', 'drafting', 'speedup', 'parallel', 'inference']","['drafts', 'speculative', 'decoding', 'memory', 'speedups', 'token', 'spec', 'chat', 'generation', 'rsd']","['  Autoregressive sampling from large language models has led to\nstate-of-the-art results in several natural language tasks. However,\nautoregressive sampling generates tokens one at a time making it slow, and even\nprohibitive in certain tasks. One way to speed up sampling is\n$\\textit{speculative decoding}$: use a small model to sample a $\\textit{draft}$\n(block or sequence of tokens), and then score all tokens in the draft by the\nlarge language model in parallel. A subset of the tokens in the draft are\naccepted (and the rest rejected) based on a statistical method to guarantee\nthat the final output follows the distribution of the large model. In this\nwork, we provide a principled understanding of speculative decoding through the\nlens of optimal transport (OT) with $\\textit{membership cost}$. This framework\ncan be viewed as an extension of the well-known $\\textit{maximal-coupling}$\nproblem. This new formulation enables us to generalize the speculative decoding\nmethod to allow for a set of $k$ candidates at the token-level, which leads to\nan improved optimal membership cost. We show that the optimal draft selection\nalgorithm (transport plan) can be computed via linear programming, whose\nbest-known runtime is exponential in $k$. We then propose a valid draft\nselection algorithm whose acceptance probability is $(1-1/e)$-optimal\nmultiplicatively. Moreover, it can be computed in time almost linear with size\nof domain of a single token. Using this $new draft selection$ algorithm, we\ndevelop a new autoregressive sampling algorithm called $\\textit{SpecTr}$, which\nprovides speedup in decoding while ensuring that there is no quality\ndegradation in the decoded output. We experimentally demonstrate that for\nstate-of-the-art large language models, the proposed approach achieves a wall\nclock speedup of 2.13X, a further 1.37X speedup over speculative decoding on\nstandard benchmarks.\n', '  Speculative decoding is a prominent technique to speed up the inference of a\nlarge target language model based on predictions of an auxiliary draft model.\nWhile effective, in application-specific settings, it often involves\nfine-tuning both draft and target models to achieve high acceptance rates. As\nthe number of downstream tasks grows, these draft models add significant\ncomplexity to inference systems. We propose Speculative Streaming, a\nsingle-model speculative decoding method that fuses drafting into the target\nmodel by changing the fine-tuning objective from next token prediction to\nfuture n-gram prediction. Speculative Streaming speeds up decoding by 1.8 -\n3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and\nMeaning Representation, without sacrificing generation quality. Additionally,\nSpeculative Streaming is parameter-efficient. It achieves on-par/higher\nspeed-ups than Medusa-style architectures while using ~10000X fewer extra\nparameters, making it well-suited for resource-constrained devices.\n', ""  Speculative Decoding is a widely used technique to speed up inference for\nLarge Language Models (LLMs) without sacrificing quality. When performing\ninference, speculative decoding uses a smaller draft model to generate\nspeculative tokens and then uses the target LLM to verify those draft tokens.\nThe speedup provided by speculative decoding heavily depends on the choice of\nthe draft model. In this work, we perform a detailed study comprising over 350\nexperiments with LLaMA-65B and OPT-66B using speculative decoding and delineate\nthe factors that affect the performance gain provided by speculative decoding.\nOur experiments indicate that the performance of speculative decoding depends\nheavily on the latency of the draft model, and the draft model's capability in\nlanguage modeling does not correlate strongly with its performance in\nspeculative decoding. Based on these insights we explore a new design space for\ndraft models and design hardware-efficient draft models for speculative\ndecoding. Our newly designed draft model for LLaMA-65B can provide 111% higher\nthroughput than existing draft models and can generalize further to the LLaMA-2\nmodel family and supervised fine-tuned models.\n""]",Speculative Decoding for Efficient Language Modeling
111,110,75,110_consciousness_conscious_cognition_ai,"['consciousness', 'conscious', 'cognition', 'ai', 'brain', 'cognitive', 'philosophical', 'artificial', 'neuroscience', 'metacognition']","['consciousness', 'intelligence', 'artificial', 'conscious', 'human', 'humans', 'science', 'cognitive', 'theories', 'social']","['consciousness', 'cognition', 'ai', 'philosophical', 'metacognition', 'functionalism', 'turing', 'concepts', 'emergence', 'sensemaking']","['  We here analyse the question of developing artificial consciousness from an\nevolutionary perspective, taking the evolution of the human brain and its\nrelation with consciousness as a reference model. This kind of analysis reveals\nseveral structural and functional features of the human brain that appear to be\nkey for reaching human-like complex conscious experience and that current\nresearch on Artificial Intelligence (AI) should take into account in its\nattempt to develop systems capable of conscious processing. We argue that, even\nif AI is limited in its ability to emulate human consciousness for both\nintrinsic (structural and architectural) and extrinsic (related to the current\nstage of scientific and technological knowledge) reasons, taking inspiration\nfrom those characteristics of the brain that make conscious processing possible\nand/or modulate it, is a potentially promising strategy towards developing\nconscious AI. Also, it is theoretically possible that AI research can develop\npartial or potentially alternative forms of consciousness that is qualitatively\ndifferent from the human, and that may be either more or less sophisticated\ndepending on the perspectives. Therefore, we recommend neuroscience-inspired\ncaution in talking about artificial consciousness: since the use of the same\nword consciousness for humans and AI becomes ambiguous and potentially\nmisleading, we propose to clearly specify what is common and what differs in AI\nconscious processing from full human conscious experience.\n', '  Is artificial consciousness theoretically possible? Is it plausible? If so,\nis it technically feasible? To make progress on these questions, it is\nnecessary to lay some groundwork clarifying the logical and empirical\nconditions for artificial consciousness to arise and the meaning of relevant\nterms involved. Consciousness is a polysemic word: researchers from different\nfields, including neuroscience, Artificial Intelligence, robotics, and\nphilosophy, among others, sometimes use different terms in order to refer to\nthe same phenomena or the same terms to refer to different phenomena. In fact,\nif we want to pursue artificial consciousness, a proper definition of the key\nconcepts is required. Here, after some logical and conceptual preliminaries, we\nargue for the necessity of using dimensions and profiles of consciousness for a\nbalanced discussion about their possible instantiation or realisation in\nartificial systems. Our primary goal in this paper is to review the main\ntheoretical questions that arise in the domain of artificial consciousness. On\nthe basis of this review, we propose to assess the issue of artificial\nconsciousness within a multidimensional account. The theoretical possibility of\nartificial consciousness is already presumed within some theoretical\nframeworks; however, empirical possibility cannot simply be deduced from these\nframeworks but needs independent empirical validation. We break down the\ncomplexity of consciousness by identifying constituents, components, and\ndimensions, and reflect pragmatically about the general challenges confronting\nthe creation of artificial consciousness. Despite these challenges, we outline\na research strategy for showing how ""awareness"" as we propose to understand it\ncould plausibly be realised in artificial systems.\n', '  Consciousness is notoriously hard to define with objective terms. An\nobjective definition of consciousness is critically needed so that we might\naccurately understand how consciousness and resultant choice behaviour may\narise in biological or artificial systems. Many theories have integrated\nneurobiological and psychological research to explain how consciousness might\narise, but few, if any, outline what is fundamentally required to generate\nconsciousness. To identify such requirements, I examine current theories of\nconsciousness and corresponding scientific research to generate a new\ndefinition of consciousness from first principles. Critically, consciousness is\nthe apparatus that provides the ability to make decisions, but it is not\ndefined by the decision itself. As such, a definition of consciousness does not\nrequire choice behaviour or an explicit awareness of temporality despite both\nbeing well-characterised outcomes of conscious thought. Rather, requirements\nfor consciousness include: at least some capability for perception, a memory\nfor the storage of such perceptual information which in turn provides a\nframework for an imagination with which a sense of self can be capable of\nmaking decisions based on possible and desired futures. Thought experiments and\nobservable neurological phenomena demonstrate that these components are\nfundamentally required of consciousness, whereby the loss of any one component\nremoves the capability for conscious thought. Identifying these requirements\nprovides a new definition for consciousness by which we can objectively\ndetermine consciousness in any conceivable agent, such as non-human animals and\nartificially intelligent systems.\n']",Artificial Consciousness and Cognitive AI
112,111,75,111_symbolic_ai_neural_neuro,"['symbolic', 'ai', 'neural', 'neuro', 'symbols', 'neurosymbolic', 'semantics', 'knowledge', 'deductive', 'symbol']","['symbolic', 'neurosymbolic', 'logic', 'logical', 'reasoning', 'probabilistic', 'symbol', 'neural', 'rules', 'rule']","['ai', 'symbols', 'neurosymbolic', 'deductive', 'relational', 'embeddings', 'interpretability', 'ontologies', 'logical', 'datalog']","['  Neuro-Symbolic (NeSy) integration combines symbolic reasoning with Neural\nNetworks (NNs) for tasks requiring perception and reasoning. Most NeSy systems\nrely on continuous relaxation of logical knowledge, and no discrete decisions\nare made within the model pipeline. Furthermore, these methods assume that the\nsymbolic rules are given. In this paper, we propose Deep Symbolic Learning\n(DSL), a NeSy system that learns NeSy-functions, i.e., the composition of a\n(set of) perception functions which map continuous data to discrete symbols,\nand a symbolic function over the set of symbols. DSL learns simultaneously the\nperception and symbolic functions while being trained only on their composition\n(NeSy-function). The key novelty of DSL is that it can create internal\n(interpretable) symbolic representations and map them to perception inputs\nwithin a differentiable NN learning pipeline. The created symbols are\nautomatically selected to generate symbolic functions that best explain the\ndata. We provide experimental analysis to substantiate the efficacy of DSL in\nsimultaneously learning perception and symbolic functions.\n', '  Neurosymbolic artificial intelligence (AI) is an emerging branch of AI that\ncombines the strengths of symbolic AI and sub-symbolic AI. A major drawback of\nsub-symbolic AI is that it acts as a ""black box"", meaning that predictions are\ndifficult to explain, making the testing & evaluation (T&E) and validation &\nverification (V&V) processes of a system that uses sub-symbolic AI a challenge.\nSince neurosymbolic AI combines the advantages of both symbolic and\nsub-symbolic AI, this survey explores how neurosymbolic applications can ease\nthe V&V process. This survey considers two taxonomies of neurosymbolic AI,\nevaluates them, and analyzes which algorithms are commonly used as the symbolic\nand sub-symbolic components in current applications. Additionally, an overview\nof current techniques for the T&E and V&V processes of these components is\nprovided. Furthermore, it is investigated how the symbolic part is used for T&E\nand V&V purposes in current neurosymbolic applications. Our research shows that\nneurosymbolic AI as great potential to ease the T&E and V&V processes of\nsub-symbolic AI by leveraging the possibilities of symbolic AI. Additionally,\nthe applicability of current T&E and V&V methods to neurosymbolic AI is\nassessed, and how different neurosymbolic architectures can impact these\nmethods is explored. It is found that current T&E and V&V techniques are partly\nsufficient to test, evaluate, verify, or validate the symbolic and sub-symbolic\npart of neurosymbolic applications independently, while some of them use\napproaches where current T&E and V&V methods are not applicable by default, and\nadjustments or even new approaches are needed. Our research shows that there is\ngreat potential in using symbolic AI to test, evaluate, verify, or validate the\npredictions of a sub-symbolic model, making neurosymbolic AI an interesting\nresearch direction for safe, secure, and trustworthy AI.\n', '  The field of neuro-symbolic AI aims to benefit from the combination of neural\nnetworks and symbolic systems. A cornerstone of the field is the translation or\nencoding of symbolic knowledge into neural networks. Although many\nneuro-symbolic methods and approaches have been proposed throughout the years,\nand with an large increase in recent years, no common definition of encoding\nexists that can enable a precise, theoretical comparison of neuro-symbolic\nmethods. This paper addresses this problem by introducing a semantic framework\nfor neuro-symbolic AI. We start by providing a formal definition of semantic\nencoding, specifying the components and conditions under which a knowledge-base\ncan be encoded correctly by a neural network. We then show that many\nneuro-symbolic approaches are accounted for by this definition. We provide a\nnumber of examples and correspondence proofs of the application of the proposed\nframework to the neural encoding of various forms of knowledge representation.\nMany, at first sight disparate, neuro-symbolic methods, are shown to fall\nwithin the proposed formalization. This is expected to provide a guidance to\nfuture neuro-symbolic encodings by placing them in the broader context of the\nsemantic encoding of entire families of existing neuro-symbolic systems. The\npaper is hoped to help initiate a discussion around the provision of a theory\nfor neuro-symbolic AI and a semantics for deep learning.\n']",Neuro-Symbolic Artificial Intelligence
113,112,75,112_graphllm_graphprompter_graphtranslator_graphs,"['graphllm', 'graphprompter', 'graphtranslator', 'graphs', 'embeddings', 'textual', 'nodes', 'networks', 'graph', 'supervised']","['graph', 'graphs', 'node', 'textual', 'text', 'nodes', 'shot', 'link', 'structures', 'structure']","['graphllm', 'graphprompter', 'graphtranslator', 'embeddings', 'textual', 'networks', 'tags', 'graphadapter', 'edge', 'attributed']","['  We present Simplified Text-Attributed Graph Embeddings (STAGE), a\nstraightforward yet effective method for enhancing node features in Graph\nNeural Network (GNN) models that encode Text-Attributed Graphs (TAGs). Our\napproach leverages Large-Language Models (LLMs) to generate embeddings for\ntextual attributes. STAGE achieves competitive results on various node\nclassification benchmarks while also maintaining a simplicity in implementation\nrelative to current state-of-the-art (SoTA) techniques. We show that utilizing\npre-trained LLMs as embedding generators provides robust features for ensemble\nGNN training, enabling pipelines that are simpler than current SoTA approaches\nwhich require multiple expensive training and prompting stages. We also\nimplement diffusion-pattern GNNs in an effort to make this pipeline scalable to\ngraphs beyond academic benchmarks.\n', ""  Foundation models like ChatGPT and GPT-4 have revolutionized artificial\nintelligence, exhibiting remarkable abilities to generalize across a wide array\nof tasks and applications beyond their initial training objectives. However,\nwhen this concept is applied to graph learning, a stark contrast emerges. Graph\nlearning has predominantly focused on single-graph models, tailored to specific\ntasks or datasets, lacking the ability to transfer learned knowledge to\ndifferent domains. This limitation stems from the inherent complexity and\ndiversity of graph structures, along with the different feature and label\nspaces specific to graph data. In this paper, we present our UniGraph\nframework, designed to train a graph foundation model capable of generalizing\nto unseen graphs and tasks across diverse domains. Unlike single-graph models\nthat use pre-computed node features of varying dimensions as input, our\napproach leverages Text-Attributed Graphs (TAGs) for unifying node\nrepresentations. We propose a cascaded architecture of Language Models (LMs)\nand Graph Neural Networks (GNNs) as backbone networks with a self-supervised\ntraining objective based on Masked Graph Modeling (MGM). We introduce graph\ninstruction tuning using Large Language Models (LLMs) to enable zero-shot\nprediction ability. Our comprehensive experiments across various graph learning\ntasks and domains demonstrate the model's effectiveness in self-supervised\nrepresentation learning on unseen graphs, few-shot in-context transfer, and\nzero-shot transfer, even surpassing or matching the performance of GNNs that\nhave undergone supervised training on target datasets.\n"", '  The text-attributed graph (TAG) is one kind of important real-world\ngraph-structured data with each node associated with raw texts. For TAGs,\ntraditional few-shot node classification methods directly conduct training on\nthe pre-processed node features and do not consider the raw texts. The\nperformance is highly dependent on the choice of the feature pre-processing\nmethod. In this paper, we propose P2TAG, a framework designed for few-shot node\nclassification on TAGs with graph pre-training and prompting. P2TAG first\npre-trains the language model (LM) and graph neural network (GNN) on TAGs with\nself-supervised loss. To fully utilize the ability of language models, we adapt\nthe masked language modeling objective for our framework. The pre-trained model\nis then used for the few-shot node classification with a mixed prompt method,\nwhich simultaneously considers both text and graph information. We conduct\nexperiments on six real-world TAGs, including paper citation networks and\nproduct co-purchasing networks. Experimental results demonstrate that our\nproposed framework outperforms existing graph few-shot learning methods on\nthese datasets with +18.98% ~ +35.98% improvements.\n']",Graph Neural Networks with Text-Attributed Graph Embeddings
114,113,73,113_flows_turbulent_flow_turbulence,"['flows', 'turbulent', 'flow', 'turbulence', 'vortex', 'aerodynamic', 'stokes', 'fluids', 'vorticity', 'reynolds']","['fluid', 'flow', 'turbulent', 'turbulence', 'flows', 'simulations', 'dynamics', 'velocity', 'vortex', 'simulation']","['flow', 'aerodynamic', 'vorticity', 'meshfree', 'navier', 'fluxes', 'helmfluid', 'cfd', 'discretization', 'viscosity']","['  Learning computational fluid dynamics (CFD) traditionally relies on\ncomputationally intensive simulations of the Navier-Stokes equations. Recently,\nlarge language models (LLMs) have shown remarkable pattern recognition and\nreasoning abilities in natural language processing (NLP) and computer vision\n(CV). However, these models struggle with the complex geometries inherent in\nfluid dynamics. We introduce FLUID-LLM, a novel framework combining pre-trained\nLLMs with spatiotemporal-aware encoding to predict unsteady fluid dynamics. Our\napproach leverages the temporal autoregressive abilities of LLMs alongside\nspatial-aware layers, bridging the gap between previous CFD prediction methods.\nEvaluations on standard benchmarks reveal significant performance improvements\nacross various fluid datasets. Our results demonstrate that FLUID-LLM\neffectively integrates spatiotemporal information into pre-trained LLMs,\nenhancing CFD task performance.\n', '  Fluid data completion is a research problem with high potential benefit for\nboth experimental and computational fluid dynamics. An effective fluid data\ncompletion method reduces the required number of sensors in a fluid dynamics\nexperiment, and allows a coarser and more adaptive mesh for a Computational\nFluid Dynamics (CFD) simulation. However, the ill-posed nature of the fluid\ndata completion problem makes it prohibitively difficult to obtain a\ntheoretical solution and presents high numerical uncertainty and instability\nfor a data-driven approach (e.g., a neural network model). To address these\nchallenges, we leverage recent advancements in computer vision, employing the\nvector quantization technique to map both complete and incomplete fluid data\nspaces onto discrete-valued lower-dimensional representations via a two-stage\nlearning procedure. We demonstrated the effectiveness of our approach on\nKolmogorov flow data (Reynolds number: 1000) occluded by masks of different\nsize and arrangement. Experimental results show that our proposed model\nconsistently outperforms benchmark models under different occlusion settings in\nterms of point-wise reconstruction accuracy as well as turbulent energy\nspectrum and vorticity distribution.\n', '  Simulations of turbulent flows in 3D are one of the most expensive\nsimulations in computational fluid dynamics (CFD). Many works have been written\non surrogate models to replace numerical solvers for fluid flows with faster,\nlearned, autoregressive models. However, the intricacies of turbulence in three\ndimensions necessitate training these models with very small time steps, while\ngenerating realistic flow states requires either long roll-outs with many steps\nand significant error accumulation or starting from a known, realistic flow\nstate - something we aimed to avoid in the first place. Instead, we propose to\napproach turbulent flow simulation as a generative task directly learning the\nmanifold of all possible turbulent flow states without relying on any initial\nflow state. For our experiments, we introduce a challenging 3D turbulence\ndataset of high-resolution flows and detailed vortex structures caused by\nvarious objects and derive two novel sample evaluation metrics for turbulent\nflows. On this dataset, we show that our generative model captures the\ndistribution of turbulent flows caused by unseen objects and generates\nhigh-quality, realistic samples amenable for downstream applications without\naccess to any initial state.\n']",Turbulent Flow Simulation and Modeling
115,114,71,114_ensembles_treeshap_ensemble_boosting,"['ensembles', 'treeshap', 'ensemble', 'boosting', 'forests', 'forest', 'trees', 'classification', 'tree', 'metatree']","['trees', 'tree', 'forests', 'decision', 'forest', 'random', 'ensembles', 'ensemble', 'rules', 'interpretable']","['ensembles', 'treeshap', 'boosting', 'overfitting', 'pruning', 'bagging', 'stumps', 'branches', 'decision', 'gini']","['  Decision trees are a popular tool in machine learning and yield\neasy-to-understand models. Several techniques have been proposed in the\nliterature for learning a decision tree classifier, with different techniques\nworking well for data from different domains. In this work, we develop\napproaches to design decision tree learning algorithms given repeated access to\ndata from the same domain. We propose novel parameterized classes of node\nsplitting criteria in top-down algorithms, which interpolate between popularly\nused entropy and Gini impurity based criteria, and provide theoretical bounds\non the number of samples needed to learn the splitting function appropriate for\nthe data at hand. We also study the sample complexity of tuning prior\nparameters in Bayesian decision tree learning, and extend our results to\ndecision tree regression. We further consider the problem of tuning\nhyperparameters in pruning the decision tree for classical pruning algorithms\nincluding min-cost complexity pruning. We also study the interpretability of\nthe learned decision trees and introduce a data-driven approach for optimizing\nthe explainability versus accuracy trade-off using decision trees. Finally, we\ndemonstrate the significance of our approach on real world datasets by learning\ndata-specific decision trees which are simultaneously more accurate and\ninterpretable.\n', '  A decision tree is one of the most popular approaches in machine learning\nfields. However, it suffers from the problem of overfitting caused by overly\ndeepened trees. Then, a meta-tree is recently proposed. It solves the problem\nof overfitting caused by overly deepened trees. Moreover, the meta-tree\nguarantees statistical optimality based on Bayes decision theory. Therefore,\nthe meta-tree is expected to perform better than the decision tree. In contrast\nto a single decision tree, it is known that ensembles of decision trees, which\nare typically constructed boosting algorithms, are more effective in improving\npredictive performance. Thus, it is expected that ensembles of meta-trees are\nmore effective in improving predictive performance than a single meta-tree, and\nthere are no previous studies that construct multiple meta-trees in boosting.\nTherefore, in this study, we propose a method to construct multiple meta-trees\nusing a boosting approach. Through experiments with synthetic and benchmark\ndatasets, we conduct a performance comparison between the proposed methods and\nthe conventional methods using ensembles of decision trees. Furthermore, while\nensembles of decision trees can cause overfitting as well as a single decision\ntree, experiments confirmed that ensembles of meta-trees can prevent\noverfitting due to the tree depth.\n', '  Decades after their inception, random forests continue to provide\nstate-of-the-art accuracy in a variety of learning problems, outperforming in\nthis respect alternative machine learning algorithms such as decision trees or\neven neural networks. However, being an ensemble method, the one aspect where\nrandom forests tend to severely underperform decision trees is\ninterpretability. In the present work, we propose a post-hoc approach that aims\nto have the best of both worlds: the accuracy of random forests and the\ninterpretability of decision trees. To this end, we present two forest-pruning\nmethods to find an optimal sub-forest within a given random forest, and then,\nwhen applicable, combine the selected trees into one. Our first method relies\non constrained exhaustive search, while our second method is based on an\nadaptation of the LASSO methodology. Extensive experiments over synthetic and\nreal world datasets show that, in the majority of scenarios, at least one of\nthe two methods proposed is more accurate than the original random forest,\nwhile just using a small fraction of the trees, aiding result interpretability.\nCompared to current state-of-the-art forest pruning methods, namely sequential\nforward selection and (a variation of) sequential backward selection, our\nmethods tend to outperform both of them, whether in terms of accuracy, number\nof trees employed, or both.\n']",Decision Tree Ensembles and Interpretability
116,115,71,115_embeddings_nlp_textual_similarity,"['embeddings', 'nlp', 'textual', 'similarity', 'semantic', 'embedding', 'corpus', 'sentences', 'word2vec', 'fasttext']","['sentence', 'embeddings', 'word', 'similarity', 'contrastive', 'semantic', 'sentences', 'words', 'text', 'textual']","['embeddings', 'nlp', 'similarity', 'word2vec', 'fasttext', 'entailment', 'paraphrase', 'bert', 'benchmarks', 'sts']","[""  Semantic textual similarity (STS) is a fundamental NLP task that measures the\nsemantic similarity between a pair of sentences. In order to reduce the\ninherent ambiguity posed from the sentences, a recent work called Conditional\nSTS (C-STS) has been proposed to measure the sentences' similarity conditioned\non a certain aspect. Despite the popularity of C-STS, we find that the current\nC-STS dataset suffers from various issues that could impede proper evaluation\non this task. In this paper, we reannotate the C-STS validation set and observe\nan annotator discrepancy on 55% of the instances resulting from the annotation\nerrors in the original label, ill-defined conditions, and the lack of clarity\nin the task definition. After a thorough dataset analysis, we improve the C-STS\ntask by leveraging the models' capability to understand the conditions under a\nQA task setting. With the generated answers, we present an automatic error\nidentification pipeline that is able to identify annotation errors from the\nC-STS data with over 80% F1 score. We also propose a new method that largely\nimproves the performance over baselines on the C-STS data by training the\nmodels with the answers. Finally we discuss the conditionality annotation based\non the typed-feature structure (TFS) of entity types. We show in examples that\nthe TFS is able to provide a linguistic foundation for constructing C-STS data\nwith new conditions.\n"", ""  Background/introduction: Pre-trained transformer models shine in many natural\nlanguage processing tasks and therefore are expected to bear the representation\nof the input sentence or text meaning. These sentence-level embeddings are also\nimportant in retrieval-augmented generation. But do commonly used plain\naveraging or prompt templates surface it enough?\n  Methods: Given 110M parameters BERT's hidden representations from multiple\nlayers and multiple tokens we tried various ways to extract optimal sentence\nrepresentations. We tested various token aggregation and representation\npost-processing techniques. We also tested multiple ways of using a general\nWikitext dataset to complement BERTs sentence representations. All methods were\ntested on 8 Semantic Textual Similarity (STS), 6 short text clustering, and 12\nclassification tasks. We also evaluated our representation-shaping techniques\non other static models, including random token representations.\n  Results: Proposed representation extraction methods improved the performance\non STS and clustering tasks for all models considered. Very high improvements\nfor static token-based models, especially random embeddings for STS tasks\nalmost reach the performance of BERT-derived representations.\n  Conclusions: Our work shows that for multiple tasks simple baselines with\nrepresentation shaping techniques reach or even outperform more complex\nBERT-based models or are able to contribute to their performance.\n"", '  Since the introduction of BERT and RoBERTa, research on Semantic Textual\nSimilarity (STS) has made groundbreaking progress. Particularly, the adoption\nof contrastive learning has substantially elevated state-of-the-art performance\nacross various STS benchmarks. However, contrastive learning categorizes text\npairs as either semantically similar or dissimilar, failing to leverage\nfine-grained annotated information and necessitating large batch sizes to\nprevent model collapse. These constraints pose challenges for researchers\nengaged in STS tasks that require nuanced similarity levels or those with\nlimited computational resources, compelling them to explore alternatives like\nSentence-BERT. Nonetheless, Sentence-BERT tackles STS tasks from a\nclassification perspective, overlooking the progressive nature of semantic\nrelationships, which results in suboptimal performance. To bridge this gap,\nthis paper presents an innovative regression framework and proposes two simple\nyet effective loss functions: Translated ReLU and Smooth K2 Loss. Experimental\nanalyses demonstrate that our method achieves convincing performance across\nseven established STS benchmarks, especially when supplemented with\ntask-specific training data.\n']",Semantic Textual Similarity in NLP
117,116,70,116_games_wargames_chess_wargame,"['games', 'wargames', 'chess', 'wargame', 'game', 'starcraft', 'agents', 'ai', 'reinforcement', 'gaming']","['game', 'games', 'agents', 'strategic', 'agent', 'players', 'coordination', 'gaming', 'interactive', 'communication']","['wargames', 'chess', 'starcraft', 'reinforcement', 'agent', 'interactive', 'strategic', 'intelligence', 'teammates', 'smartplay']","['  Game theory is the study of mathematical models of strategic interactions\namong rational agents. Language is a key medium of interaction for humans,\nthough it has historically proven difficult to model dialogue and its strategic\nmotivations mathematically. A suitable model of the players, strategies, and\npayoffs associated with linguistic interactions (i.e., a binding to the\nconventional symbolic logic of game theory) would enable existing\ngame-theoretic algorithms to provide strategic solutions in the space of\nlanguage. In other words, a binding could provide a route to computing stable,\nrational conversational strategies in dialogue. Large language models (LLMs)\nhave arguably reached a point where their generative capabilities can enable\nrealistic, human-like simulations of natural dialogue. By prompting them in\nvarious ways, we can steer their responses towards different output utterances.\nLeveraging the expressivity of natural language, LLMs can also help us quickly\ngenerate new dialogue scenarios, which are grounded in real world applications.\nIn this work, we present one possible binding from dialogue to game theory as\nwell as generalizations of existing equilibrium finding algorithms to this\nsetting. In addition, by exploiting LLMs generation capabilities along with our\nproposed binding, we can synthesize a large repository of formally-defined\ngames in which one can study and test game-theoretic solution concepts. We also\ndemonstrate how one can combine LLM-driven game generation, game-theoretic\nsolvers, and imitation learning to construct a process for improving the\nstrategic capabilities of LLMs.\n', ""  Agents built with large language models (LLMs) have shown great potential\nacross a wide range of domains. However, in complex decision-making tasks, pure\nLLM-based agents tend to exhibit intrinsic bias in their choice of actions,\nwhich is inherited from the model's training data and results in suboptimal\nperformance. To develop strategic language agents, i.e., agents that generate\nflexible language actions and possess strong decision-making abilities, we\npropose a novel framework that powers LLM-based agents with reinforcement\nlearning (RL). We consider Werewolf, a popular social deduction game, as a\nchallenging testbed that emphasizes versatile communication and strategic\ngameplay. To mitigate the intrinsic bias in language actions, our agents use an\nLLM to perform deductive reasoning and generate a diverse set of action\ncandidates. Then an RL policy trained to optimize the decision-making ability\nchooses an action from the candidates to play in the game. Extensive\nexperiments show that our agents overcome the intrinsic bias and outperform\nexisting LLM-based agents in the Werewolf game. We also conduct human-agent\nexperiments and find that our agents achieve human-level performance and\ndemonstrate strong strategic play.\n"", '  The development of game agents holds a critical role in advancing towards\nArtificial General Intelligence (AGI). The progress of LLMs and their\nmultimodal counterparts (MLLMs) offers an unprecedented opportunity to evolve\nand empower game agents with human-like decision-making capabilities in complex\ncomputer game environments. This paper provides a comprehensive overview of\nLLM-based game agents from a holistic viewpoint. First, we introduce the\nconceptual architecture of LLM-based game agents, centered around six essential\nfunctional components: perception, memory, thinking, role-playing, action, and\nlearning. Second, we survey existing representative LLM-based game agents\ndocumented in the literature with respect to methodologies and adaptation\nagility across six genres of games, including adventure, communication,\ncompetition, cooperation, simulation, and crafting & exploration games.\nFinally, we present an outlook of future research and development directions in\nthis burgeoning field. A curated list of relevant papers is maintained and made\naccessible at: https://github.com/git-disl/awesome-LLM-game-agent-papers.\n']",Artificial Intelligence in Games
118,117,70,117_audio_voice_vocoder_vocoders,"['audio', 'voice', 'vocoder', 'vocoders', 'acoustic', 'speaker', 'sound', 'speech', 'adversarial', 'spectrogram']","['speech', 'audio', 'separation', 'enhancement', 'vocoder', 'waveform', 'diffusion', 'vocoders', 'sound', 'dereverberation']","['voice', 'vocoders', 'adversarial', 'spectrogram', 'generative', 'denoising', 'speakers', 'wavelet', 'tdfnet', 'cmgan']","['  In this paper, we explore a continuous modeling approach for\ndeep-learning-based speech enhancement, focusing on the denoising process. We\nuse a state variable to indicate the denoising process. The starting state is\nnoisy speech and the ending state is clean speech. The noise component in the\nstate variable decreases with the change of the state index until the noise\ncomponent is 0. During training, a UNet-like neural network learns to estimate\nevery state variable sampled from the continuous denoising process. In testing,\nwe introduce a controlling factor as an embedding, ranging from zero to one, to\nthe neural network, allowing us to control the level of noise reduction. This\napproach enables controllable speech enhancement and is adaptable to various\napplication scenarios. Experimental results indicate that preserving a small\namount of noise in the clean target benefits speech enhancement, as evidenced\nby improvements in both objective speech measures and automatic speech\nrecognition performance.\n', '  Audio-visual speech separation has gained significant traction in recent\nyears due to its potential applications in various fields such as speech\nrecognition, diarization, scene analysis and assistive technologies. Designing\na lightweight audio-visual speech separation network is important for\nlow-latency applications, but existing methods often require higher\ncomputational costs and more parameters to achieve better separation\nperformance. In this paper, we present an audio-visual speech separation model\ncalled Top-Down-Fusion Net (TDFNet), a state-of-the-art (SOTA) model for\naudio-visual speech separation, which builds upon the architecture of TDANet,\nan audio-only speech separation method. TDANet serves as the architectural\nfoundation for the auditory and visual networks within TDFNet, offering an\nefficient model with fewer parameters. On the LRS2-2Mix dataset, TDFNet\nachieves a performance increase of up to 10\\% across all performance metrics\ncompared with the previous SOTA method CTCNet. Remarkably, these results are\nachieved using fewer parameters and only 28\\% of the multiply-accumulate\noperations (MACs) of CTCNet. In essence, our method presents a highly effective\nand efficient solution to the challenges of speech separation within the\naudio-visual domain, making significant strides in harnessing visual\ninformation optimally.\n', '  Conditional sound separation in multi-source audio mixtures without having\naccess to single source sound data during training is a long standing\nchallenge. Existing mix-and-separate based methods suffer from significant\nperformance drop with multi-source training mixtures due to the lack of\nsupervision signal for single source separation cases during training. However,\nin the case of language-conditional audio separation, we do have access to\ncorresponding text descriptions for each audio mixture in our training data,\nwhich can be seen as (rough) representations of the audio samples in the\nlanguage modality. To this end, in this paper, we propose a generic bi-modal\nseparation framework which can enhance the existing unsupervised frameworks to\nseparate single-source signals in a target modality (i.e., audio) using the\neasily separable corresponding signals in the conditioning modality (i.e.,\nlanguage), without having access to single-source samples in the target\nmodality during training. We empirically show that this is well within reach if\nwe have access to a pretrained joint embedding model between the two modalities\n(i.e., CLAP). Furthermore, we propose to incorporate our framework into two\nfundamental scenarios to enhance separation performance. First, we show that\nour proposed methodology significantly improves the performance of purely\nunsupervised baselines by reducing the distribution shift between training and\ntest samples. In particular, we show that our framework can achieve 71% boost\nin terms of Signal-to-Distortion Ratio (SDR) over the baseline, reaching 97.5%\nof the supervised learning performance. Second, we show that we can further\nimprove the performance of the supervised learning itself by 17% if we augment\nit by our proposed weakly-supervised framework, that enables a powerful\nsemi-supervised framework for audio separation.\n']",Speech and Audio Processing
119,118,70,118_adversarial_backdoors_backdoor_attacks,"['adversarial', 'backdoors', 'backdoor', 'attacks', 'networks', 'vulnerabilities', 'graphs', 'graphmu', 'gnns', 'adversary']","['attacks', 'attack', 'backdoor', 'graph', 'adversarial', 'node', 'triggers', 'robustness', 'nodes', 'perturbations']","['adversarial', 'backdoors', 'networks', 'graphmu', 'graphene', 'cybersecurity', 'node', 'vulnerable', 'trustguard', 'gcns']","['  Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet\nthey are vulnerable to backdoor attacks that can compromise their performance\nand ethical application. The detection of these attacks is crucial for\nmaintaining the reliability and security of GNN classification tasks, but\neffective detection techniques are lacking. Following an initial investigation,\nwe observed that while graph-level explanations can offer limited insights,\ntheir effectiveness in detecting backdoor triggers is inconsistent and\nincomplete. To bridge this gap, we extract and transform secondary outputs of\nGNN explanation mechanisms, designing seven novel metrics that more effectively\ndetect backdoor attacks. Additionally, we develop an adaptive attack to\nrigorously evaluate our approach. We test our method on multiple benchmark\ndatasets and examine its efficacy against various attack models. Our results\nshow that our method can achieve high detection performance, marking a\nsignificant advancement in safeguarding GNNs against backdoor attacks.\n', '  Graph Neural Networks (GNNs) are gaining popularity across various domains\ndue to their effectiveness in learning graph-structured data. Nevertheless,\nthey have been shown to be susceptible to backdoor poisoning attacks, which\npose serious threats to real-world applications. Meanwhile, graph reduction\ntechniques, including coarsening and sparsification, which have long been\nemployed to improve the scalability of large graph computational tasks, have\nrecently emerged as effective methods for accelerating GNN training on\nlarge-scale graphs. However, the current development and deployment of graph\nreduction techniques for large graphs overlook the potential risks of data\npoisoning attacks against GNNs. It is not yet clear how graph reduction\ninteracts with existing backdoor attacks. This paper conducts a thorough\nexamination of the robustness of graph reduction methods in scalable GNN\ntraining in the presence of state-of-the-art backdoor attacks. We performed a\ncomprehensive robustness analysis across six coarsening methods and six\nsparsification methods for graph reduction, under three GNN backdoor attacks\nagainst three GNN architectures. Our findings indicate that the effectiveness\nof graph reduction methods in mitigating attack success rates varies\nsignificantly, with some methods even exacerbating the attacks. Through\ndetailed analyses of triggers and poisoned nodes, we interpret our findings and\nenhance our understanding of how graph reduction influences robustness against\nbackdoor attacks. These results highlight the critical need for incorporating\nrobustness considerations in graph reduction for GNN training, ensuring that\nenhancements in computational efficiency do not compromise the security of GNN\nsystems.\n', '  Graph Neural Networks (GNNs) are a class of deep learning models capable of\nprocessing graph-structured data, and they have demonstrated significant\nperformance in a variety of real-world applications. Recent studies have found\nthat GNN models are vulnerable to backdoor attacks. When specific patterns\n(called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input\ndata, the backdoor embedded in the GNN models is activated, which misclassifies\nthe input data into the target class label specified by the attacker, whereas\nwhen there are no backdoor triggers in the input, the backdoor embedded in the\nGNN models is not activated, and the models work normally. Backdoor attacks are\nhighly stealthy and expose GNN models to serious security risks. Currently,\nresearch on backdoor attacks against GNNs mainly focus on tasks such as graph\nclassification and node classification, and backdoor attacks against link\nprediction tasks are rarely studied. In this paper, we propose a backdoor\nattack against the link prediction tasks based on GNNs and reveal the existence\nof such security vulnerability in GNN models, which make the backdoored GNN\nmodels to incorrectly predict unlinked two nodes as having a link relationship\nwhen a trigger appear. The method uses a single node as the trigger and poison\nselected node pairs in the training graph, and then the backdoor will be\nembedded in the GNN models through the training process. In the inference\nstage, the backdoor in the GNN models can be activated by simply linking the\ntrigger node to the two end nodes of the unlinked node pairs in the input data,\ncausing the GNN models to produce incorrect link prediction results for the\ntarget node pairs.\n']",Backdoor Attacks on Graph Neural Networks
120,119,70,119_channels_channel_decoding_mimo,"['channels', 'channel', 'decoding', 'mimo', 'decoders', 'multiplexing', 'wireless', 'communications', 'decoder', 'radio']","['channel', 'codes', 'signal', 'wireless', 'interference', 'transmission', 'coding', 'channels', 'communication', 'estimation']","['channel', 'decoding', 'mimo', 'multiplexing', 'modulation', 'rfi', 'duplexing', 'ofdm', 'snr', '6g']","['  The real-time quantification of the effect of a wireless channel on the\ntransmitting signal is crucial for the analysis and the intelligent design of\nwireless communication systems for various services. Recent mechanisms to model\nchannel characteristics independent of coding, modulation, signal processing,\netc., using deep learning neural networks are promising solutions. However, the\ncurrent approaches are neither statistically accurate nor able to adapt to the\nchanging environment. In this paper, we propose a new approach that combines a\ndeep learning neural network with a mixture density network model to derive the\nconditional probability density function (PDF) of receiving power given a\ncommunication distance in general wireless communication systems. Furthermore,\na deep transfer learning scheme is designed and implemented to allow the\nchannel model to dynamically adapt to changes in communication environments.\nExtensive experiments on Nakagami fading channel model and Log-normal shadowing\nchannel model with path loss and noise show that the new approach is more\nstatistically accurate, faster, and more robust than the previous deep\nlearning-based channel models.\n', '  In general, reliable communication via multiple-input multiple-output (MIMO)\northogonal frequency division multiplexing (OFDM) requires accurate channel\nestimation at the receiver. The existing literature largely focuses on\ndenoising methods for channel estimation that depend on either (i) channel\nanalysis in the time-domain with prior channel knowledge or (ii) supervised\nlearning techniques which require large pre-labeled datasets for training. To\naddress these limitations, we present a frequency-domain denoising method based\non a reinforcement learning framework that does not need a priori channel\nknowledge and pre-labeled data. Our methodology includes a new successive\nchannel denoising process based on channel curvature computation, for which we\nobtain a channel curvature magnitude threshold to identify unreliable channel\nestimates. Based on this process, we formulate the denoising mechanism as a\nMarkov decision process, where we define the actions through a geometry-based\nchannel estimation update, and the reward function based on a policy that\nreduces mean squared error (MSE). We then resort to Q-learning to update the\nchannel estimates. Numerical results verify that our denoising algorithm can\nsuccessfully mitigate noise in channel estimates. In particular, our algorithm\nprovides a significant improvement over the practical least squares (LS)\nestimation method and provides performance that approaches that of the ideal\nlinear minimum mean square error (LMMSE) estimation with perfect knowledge of\nchannel statistics.\n', '  Recently, deep learning (DL) has been emerging as a promising approach for\nchannel estimation and signal detection in wireless communications. The\nmajority of the existing studies investigating the use of DL techniques in this\ndomain focus on analysing channel impulse responses that are generated from\nonly one channel distribution such as additive white Gaussian channel noise and\nRayleigh channels. In practice, to cope with the dynamic nature of the wireless\nchannel, DL methods must be re-trained on newly non-aged collected data which\nis costly, inefficient, and impractical. To tackle this challenge, this paper\nproposes a novel universal deep neural network (Uni-DNN) that can achieve high\ndetection performance in various wireless environments without retraining the\nmodel. In particular, our proposed Uni-DNN model consists of a wireless channel\nclassifier and a signal detector which are constructed by using DNNs. The\nwireless channel classifier enables the signal detector to generalise and\nperform optimally for multiple wireless channel distributions. In addition, to\nfurther improve the signal detection performance of the proposed model,\nconvolutional neural network is employed. Extensive simulations using the\northogonal frequency division multiplexing scheme demonstrate that the bit\nerror rate performance of our proposed solution can outperform conventional\nDL-based approaches as well as least square and minimum mean square error\nchannel estimators in practical low pilot density scenarios.\n']",Wireless Channel Decoding and Estimation
121,120,70,120_optimal_optimization_minimize_bandit,"['optimal', 'optimization', 'minimize', 'bandit', 'minimax', 'convex', 'guarantees', 'regret', 'adversary', 'convexity']","['regret', 'online', 'convex', 'bounds', 'learner', 'learnability', 'constraint', 'smoothed', 'functions', 'multiclass']","['optimal', 'bandit', 'guarantees', 'convexity', 'adversarial', 'learnability', 'horizon', 'online', 'worst', 'comparators']","['  In this paper, we explore online convex optimization (OCO) and introduce a\nnew analysis that provides fast rates by exploiting the curvature of feasible\nsets. In online linear optimization, it is known that if the average gradient\nof loss functions is larger than a certain value, the curvature of feasible\nsets can be exploited by the follow-the-leader (FTL) algorithm to achieve a\nlogarithmic regret. This paper reveals that algorithms adaptive to the\ncurvature of loss functions can also leverage the curvature of feasible sets.\nWe first prove that if an optimal decision is on the boundary of a feasible set\nand the gradient of an underlying loss function is non-zero, then the algorithm\nachieves a regret upper bound of $O(\\rho \\log T)$ in stochastic environments.\nHere, $\\rho > 0$ is the radius of the smallest sphere that includes the optimal\ndecision and encloses the feasible set. Our approach, unlike existing ones, can\nwork directly with convex loss functions, exploiting the curvature of loss\nfunctions simultaneously, and can achieve the logarithmic regret only with a\nlocal property of feasible sets. Additionally, it achieves an $O(\\sqrt{T})$\nregret even in adversarial environments where FTL suffers an $\\Omega(T)$\nregret, and attains an $O(\\rho \\log T + \\sqrt{C \\rho \\log T})$ regret bound in\ncorrupted stochastic environments with corruption level $C$. Furthermore, by\nextending our analysis, we establish a regret upper bound of\n$O\\Big(T^{\\frac{q-2}{2(q-1)}} (\\log T)^{\\frac{q}{2(q-1)}}\\Big)$ for\n$q$-uniformly convex feasible sets, where uniformly convex sets include\nstrongly convex sets and $\\ell_p$-balls for $p \\in [1,\\infty)$. This bound\nbridges the gap between the $O(\\log T)$ regret bound for strongly convex sets\n($q=2$) and the $O(\\sqrt{T})$ regret bound for non-curved sets ($q\\to\\infty$).\n', '  A well-studied generalization of the standard online convex optimization\n(OCO) is constrained online convex optimization (COCO). In COCO, on every\nround, a convex cost function and a convex constraint function are revealed to\nthe learner after the action for that round is chosen. The objective is to\ndesign an online policy that simultaneously achieves a small regret while\nensuring a small cumulative constraint violation (CCV) against an adaptive\nadversary interacting over a horizon of length $T$. A long-standing open\nquestion in COCO is whether an online policy can simultaneously achieve\n$O(\\sqrt{T})$ regret and $O(\\sqrt{T})$ CCV without any restrictive assumptions.\nFor the first time, we answer this in the affirmative and show that an online\npolicy can simultaneously achieve $O(\\sqrt{T})$ regret and\n$\\tilde{O}(\\sqrt{T})$ CCV. Furthermore, in the case of strongly convex cost and\nconvex constraint functions, the regret guarantee can be improved to $O(\\log\nT)$ while keeping the CCV bound the same as above. We establish these results\nby effectively combining the adaptive regret bound of the AdaGrad algorithm\nwith Lyapunov optimization - a classic tool from control theory. Surprisingly,\nthe analysis is short and elegant.\n', '  We investigate online convex optimization in non-stationary environments and\nchoose dynamic regret as the performance measure, defined as the difference\nbetween cumulative loss incurred by the online algorithm and that of any\nfeasible comparator sequence. Let $T$ be the time horizon and $P_T$ be the path\nlength that essentially reflects the non-stationarity of environments, the\nstate-of-the-art dynamic regret is $\\mathcal{O}(\\sqrt{T(1+P_T)})$. Although\nthis bound is proved to be minimax optimal for convex functions, in this paper,\nwe demonstrate that it is possible to further enhance the guarantee for some\neasy problem instances, particularly when online functions are smooth.\nSpecifically, we introduce novel online algorithms that can exploit smoothness\nand replace the dependence on $T$ in dynamic regret with problem-dependent\nquantities: the variation in gradients of loss functions, the cumulative loss\nof the comparator sequence, and the minimum of these two terms. These\nquantities are at most $\\mathcal{O}(T)$ while could be much smaller in benign\nenvironments. Therefore, our results are adaptive to the intrinsic difficulty\nof the problem, since the bounds are tighter than existing results for easy\nproblems and meanwhile safeguard the same rate in the worst case. Notably, our\nproposed algorithms can achieve favorable dynamic regret with only one gradient\nper iteration, sharing the same gradient query complexity as the static regret\nminimization methods. To accomplish this, we introduce the collaborative online\nensemble framework. The proposed framework employs a two-layer online ensemble\nto handle non-stationarity, and uses optimistic online learning and further\nintroduces crucial correction terms to enable effective collaboration within\nthe meta-base two layers, thereby attaining adaptivity. We believe the\nframework can be useful for broader problems.\n']",Online Convex Optimization and Regret Minimization
122,121,69,121_instruction_instructmining_language_texttuning,"['instruction', 'instructmining', 'language', 'texttuning', 'training', 'tuning', 'curriculum', 'benchmarks', 'tuned', 'performance']","['instruction', 'instructions', 'tuning', 'quality', 'controllability', 'diversity', 'format', 'responses', 'selection', 'finetuning']","['instructmining', 'texttuning', 'training', 'curriculum', 'benchmarks', 'tuned', 'code', 'datasets', 'capabilities', 'jsontuning']","['  Instruction tuning -- fine-tuning a large language model (LLM) on pairs of\ninstructions and desired outcomes -- is an approach that enables pre-trained\nlanguage models to perform real-world tasks and follow human instructions. Its\npractical success depends on the model learning a broader set of instructions\nthan those it was trained on. Yet the factors that determine model\ngeneralization to such \\emph{unseen tasks} are not well understood. %To\nunderstand the driving factors of generalization, In this paper, we experiment\nwith string rewrites, a symbolic task that serves as a building block for\nTuring complete Markov algorithms while allowing experimental control of\n""inputs"" and ""instructions"". We investigate the trade-off between the number of\ninstructions the model is trained on and the number of training samples\nprovided for each instruction and observe that the diversity of the instruction\nset determines generalization. Generalization emerges once a diverse enough set\nof tasks is provided, even though very few examples are provided for each task.\nInstruction diversity also ensures robustness with respect to non-uniform\ndistributions of instructions in the training set.\n', '  Instruction tuning is a vital step of training large language models (LLM),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLM. Therefore, recently\na lot of studies focus on exploring the methods of selecting high-quality\nsubset from instruction datasets, aiming to reduce training costs and enhance\nthe instruction-following capabilities of LLMs. This paper presents a\ncomprehensive survey on data selection for LLM instruction tuning. Firstly, we\nintroduce the wildly used instruction datasets. Then, we propose a new taxonomy\nof the data selection methods and provide a detailed introduction of recent\nadvances,and the evaluation strategies and results of data selection methods\nare also elaborated in detail. Finally, we emphasize the open challenges and\npresent new frontiers of this task.\n', ""  Instruction tuning -- tuning large language models on instruction-output\npairs -- is a promising technique for making models better adapted to the real\nworld. Yet, the key factors driving the model's capability to understand and\nfollow instructions not seen during training remain under-explored. Our\ninvestigation begins with a series of synthetic experiments within the\ntheoretical framework of a Turing-complete algorithm called Markov algorithm,\nwhich allows fine-grained control over the instruction-tuning data.\nGeneralization and robustness with respect to the training distribution emerge\nonce a diverse enough set of tasks is provided, even though very few examples\nare provided for each task. We extend these initial results to a real-world\napplication scenario of code generation and find that a more diverse\ninstruction set, extending beyond code-related tasks, improves the performance\nof code generation. Our observations suggest that a more diverse semantic space\nfor instruction-tuning sets greatly improves the model's ability to follow\ninstructions and perform tasks.\n""]",Instruction Tuning for Language Models
123,122,68,122_solvers_symbolic_regression_expressions,"['solvers', 'symbolic', 'regression', 'expressions', 'formulas', 'formulagpt', 'variables', 'neural', 'discovering', 'mathematical']","['symbolic', 'expressions', 'regression', 'equations', 'mathematical', 'expression', 'equation', 'scientific', 'discovery', 'laws']","['solvers', 'symbolic', 'regression', 'expressions', 'formulagpt', 'neural', 'algebraic', 'dysymnet', 'discover', 'simplification']","['  Formulas are the language of communication between humans and nature. It is\nan important research topic of artificial intelligence to find expressions from\nobserved data to reflect the relationship between each variable in the data,\nwhich is called a symbolic regression problem. The existing symbolic regression\nmethods directly generate expressions according to the given observation data,\nand we cannot require the algorithm to generate expressions that meet specific\nrequirements according to the known prior knowledge. For example, the\nexpression needs to contain $\\sin$ or be symmetric, and so on. Even if it can,\nit often requires very complex operations, which is very inconvenient. In this\npaper, based on multi-modal large language models, we propose MLLM-SR, a\nconversational symbolic regression method that can generate expressions that\nmeet the requirements simply by describing the requirements with natural\nlanguage instructions. By experimenting on the Nguyen dataset, we can\ndemonstrate that MLLM-SR leads the state-of-the-art baselines in fitting\nperformance. More notably, we experimentally demonstrate that MLLM-SR can well\nunderstand the prior knowledge we add to the natural language instructions.\nMoreover, the addition of prior knowledge can effectively guide MLLM-SR to\ngenerate correct expressions.\n', '  Symbolic regression (SR) is a powerful technique for discovering the\nanalytical mathematical expression from data, finding various applications in\nnatural sciences due to its good interpretability of results. However, existing\nmethods face scalability issues when dealing with complex equations involving\nmultiple variables. To address this challenge, we propose ScaleSR, a scalable\nsymbolic regression model that leverages control variables to enhance both\naccuracy and scalability. The core idea is to decompose multi-variable symbolic\nregression into a set of single-variable SR problems, which are then combined\nin a bottom-up manner. The proposed method involves a four-step process. First,\nwe learn a data generator from observed data using deep neural networks (DNNs).\nSecond, the data generator is used to generate samples for a certain variable\nby controlling the input variables. Thirdly, single-variable symbolic\nregression is applied to estimate the corresponding mathematical expression.\nLastly, we repeat steps 2 and 3 by gradually adding variables one by one until\ncompletion. We evaluate the performance of our method on multiple benchmark\ndatasets. Experimental results demonstrate that the proposed ScaleSR\nsignificantly outperforms state-of-the-art baselines in discovering\nmathematical expressions with multiple variables. Moreover, it can\nsubstantially reduce the search space for symbolic regression. The source code\nwill be made publicly available upon publication.\n', ""  Mathematical equations have been unreasonably effective in describing complex\nnatural phenomena across various scientific disciplines. However, discovering\nsuch insightful equations from data presents significant challenges due to the\nnecessity of navigating extremely high-dimensional combinatorial and nonlinear\nhypothesis spaces. Traditional methods of equation discovery, commonly known as\nsymbolic regression, largely focus on extracting equations from data alone,\noften neglecting the rich domain-specific prior knowledge that scientists\ntypically depend on. To bridge this gap, we introduce LLM-SR, a novel approach\nthat leverages the extensive scientific knowledge and robust code generation\ncapabilities of Large Language Models (LLMs) to discover scientific equations\nfrom data in an efficient manner. Specifically, LLM-SR treats equations as\nprograms with mathematical operators and combines LLMs' scientific priors with\nevolutionary search over equation programs. The LLM iteratively proposes new\nequation skeleton hypotheses, drawing from its physical understanding, which\nare then optimized against data to estimate skeleton parameters. We demonstrate\nLLM-SR's effectiveness across three diverse scientific domains, where it\ndiscovers physically accurate equations that provide significantly better fits\nto in-domain and out-of-domain data compared to the well-established symbolic\nregression baselines. Incorporating scientific prior knowledge also enables\nLLM-SR to search the equation space more efficiently than baselines. Code is\navailable at: https://github.com/deep-symbolic-mathematics/LLM-SR\n""]",Symbolic Regression and Equation Discovery
124,123,68,123_denoising_inverse_regularization_inverses,"['denoising', 'inverse', 'regularization', 'inverses', 'diffusion', 'inversion', 'regularisation', 'imaging', 'iterative', 'priors']","['inverse', 'posterior', 'problems', 'diffusion', 'imaging', 'reconstruction', 'sampling', 'deblurring', 'forward', 'measurement']","['denoising', 'inverse', 'regularization', 'iterative', 'priors', 'flow', 'tomography', 'likelihood', 'reconstruction', 'variational']","['  Inverse problems have many applications in science and engineering. In\nComputer vision, several image restoration tasks such as inpainting,\ndeblurring, and super-resolution can be formally modeled as inverse problems.\nRecently, methods have been developed for solving inverse problems that only\nleverage a pre-trained unconditional diffusion model and do not require\nadditional task-specific training. In such methods, however, the inherent\nintractability of determining the conditional score function during the reverse\ndiffusion process poses a real challenge, leaving the methods to settle with an\napproximation instead, which affects their performance in practice. Here, we\npropose a MAP estimation framework to model the reverse conditional generation\nprocess of a continuous time diffusion model as an optimization process of the\nunderlying MAP objective, whose gradient term is tractable. In theory, the\nproposed framework can be applied to solve general inverse problems using\ngradient-based optimization methods. However, given the highly non-convex\nnature of the loss objective, finding a perfect gradient-based optimization\nalgorithm can be quite challenging, nevertheless, our framework offers several\npotential research directions. We use our proposed formulation and develop\nempirically effective algorithms for solving noiseless and noisy image\ninpainting tasks. We validate our proposed algorithms with extensive\nexperiments across diverse mask settings.\n', ""  Constructing fast samplers for unconditional diffusion and flow-matching\nmodels has received much attention recently; however, existing methods for\nsolving inverse problems, such as super-resolution, inpainting, or deblurring,\nstill require hundreds to thousands of iterative steps to obtain high-quality\nresults. We propose a plug-and-play framework for constructing efficient\nsamplers for inverse problems, requiring only pre-trained diffusion or\nflow-matching models. We present Conditional Conjugate Integrators, which\nleverage the specific form of the inverse problem to project the respective\nconditional diffusion/flow dynamics into a more amenable space for sampling.\nOur method complements popular posterior approximation methods for solving\ninverse problems using diffusion/flow models. We evaluate the proposed method's\nperformance on various linear image restoration tasks across multiple datasets,\nemploying diffusion and flow-matching models. Notably, on challenging inverse\nproblems like 4$\\times$ super-resolution on the ImageNet dataset, our method\ncan generate high-quality samples in as few as 5 conditional sampling steps and\noutperforms competing baselines requiring 20-1000 steps. Our code and models\nwill be publicly available at https://github.com/mandt-lab/CI2RM.\n"", '  Diffusion models have been recently studied as powerful generative inverse\nproblem solvers, owing to their high quality reconstructions and the ease of\ncombining existing iterative solvers. However, most works focus on solving\nsimple linear inverse problems in noiseless settings, which significantly\nunder-represents the complexity of real-world problems. In this work, we extend\ndiffusion solvers to efficiently handle general noisy (non)linear inverse\nproblems via approximation of the posterior sampling. Interestingly, the\nresulting posterior sampling scheme is a blended version of diffusion sampling\nwith the manifold constrained gradient without a strict measurement consistency\nprojection step, yielding a more desirable generative path in noisy settings\ncompared to the previous studies. Our method demonstrates that diffusion models\ncan incorporate various measurement noise statistics such as Gaussian and\nPoisson, and also efficiently handle noisy nonlinear inverse problems such as\nFourier phase retrieval and non-uniform deblurring. Code available at\nhttps://github.com/DPS2022/diffusion-posterior-sampling\n']",Inverse Problems in Imaging using Diffusion Models
125,124,67,124_tutoring_students_learners_tracing,"['tutoring', 'students', 'learners', 'tracing', 'learning', 'student', 'knowledge', 'assessment', 'classroom', 'learner']","['students', 'tracing', 'student', 'educational', 'education', 'course', 'cognitive', 'diagnosis', 'courses', 'knowledge']","['tutoring', 'students', 'tracing', 'learner', 'datasets', 'grade', 'predicting', 'campus', 'competencies', 'instructors']","[""  Modern online education has the capacity to provide intelligent educational\nservices by automatically analyzing substantial amounts of student behavioral\ndata. Knowledge Tracing (KT) is one of the fundamental tasks for student\nbehavioral data analysis, aiming to monitor students' evolving knowledge state\nduring their problem-solving process. In recent years, a substantial number of\nstudies have concentrated on this rapidly growing field, significantly\ncontributing to its advancements. In this survey, we will conduct a thorough\ninvestigation of these progressions. Firstly, we present three types of\nfundamental KT models with distinct technical routes. Subsequently, we review\nextensive variants of the fundamental KT models that consider more stringent\nlearning assumptions. Moreover, the development of KT cannot be separated from\nits applications, thereby we present typical KT applications in various\nscenarios. To facilitate the work of researchers and practitioners in this\nfield, we have developed two open-source algorithm libraries: EduData that\nenables the download and preprocessing of KT-related datasets, and EduKTM that\nprovides an extensible and unified implementation of existing mainstream KT\nmodels. Finally, we discuss potential directions for future research in this\nrapidly growing field. We hope that the current survey will assist both\nresearchers and practitioners in fostering the development of KT, thereby\nbenefiting a broader range of students.\n"", ""  Knowledge tracing (KT), aiming to mine students' mastery of knowledge by\ntheir exercise records and predict their performance on future test questions,\nis a critical task in educational assessment. While researchers achieved\ntremendous success with the rapid development of deep learning techniques,\ncurrent knowledge tracing tasks fall into the cracks from real-world teaching\nscenarios. Relying heavily on extensive student data and solely predicting\nnumerical performances differs from the settings where teachers assess\nstudents' knowledge state from limited practices and provide explanatory\nfeedback. To fill this gap, we explore a new task formulation: Explainable\nFew-shot Knowledge Tracing. By leveraging the powerful reasoning and generation\nabilities of large language models (LLMs), we then propose a cognition-guided\nframework that can track the student knowledge from a few student records while\nproviding natural language explanations. Experimental results from three widely\nused datasets show that LLMs can perform comparable or superior to competitive\ndeep knowledge tracing methods. We also discuss potential directions and call\nfor future improvements in relevant topics.\n"", ""  Knowledge Tracing (KT) is a critical task in online education systems, aiming\nto monitor students' knowledge states throughout a learning period. Common KT\napproaches involve predicting the probability of a student correctly answering\nthe next question based on their exercise history. However, these methods often\nsuffer from performance degradation when faced with the scarcity of student\ninteractions in new education systems. To address this, we leverage student\ninteractions from existing education systems to mitigate performance\ndegradation caused by limited training data. Nevertheless, these interactions\nexhibit significant differences since they are derived from different education\nsystems. To address this issue, we propose a domain generalization approach for\nknowledge tracing, where existing education systems are considered source\ndomains, and new education systems with limited data are considered target\ndomains. Additionally, we design a domain-generalizable knowledge tracing\nframework (DGKT) that can be applied to any KT model. Specifically, we present\na concept aggregation approach designed to reduce conceptual disparities within\nsequences of student interactions from diverse domains. To further mitigate\ndomain discrepancies, we introduce a novel normalization module called Sequence\nInstance Normalization (SeqIN). Moreover, to fully leverage exercise\ninformation, we propose a new knowledge tracing model tailored for the domain\ngeneralization KT task, named Domain-Generalizable Relation-based Knowledge\nTracing (DGRKT). Extensive experiments across five benchmark datasets\ndemonstrate that the proposed method performs well despite limited training\ndata.\n""]","""Knowledge Tracing in Education"""
126,125,67,125_aspects_aspect_sentiment_sentiments,"['aspects', 'aspect', 'sentiment', 'sentiments', 'annotated', 'sentences', 'parsing', 'syntactic', 'attention', 'insightnet']","['sentiment', 'aspect', 'sentiments', 'polarity', 'syntactic', 'aspects', 'sentence', 'category', 'opinion', 'analysis']","['aspects', 'sentiment', 'annotated', 'parsing', 'insightnet', 'categories', 'absa', 'subtask', 'restaurants', 'spans']","['  Aspect-based Sentiment Analysis (ABSA) is crucial for understanding sentiment\nnuances in text, especially across diverse languages and cultures. This paper\nintroduces a novel Deep Convolutional Neural Network (CNN)-based model tailored\nfor aspect and polarity classification in Hausa movie reviews, an\nunderrepresented language in sentiment analysis research. A comprehensive Hausa\nABSA dataset is created, filling a significant gap in resource availability.\nThe dataset, preprocessed using sci-kit-learn for TF-IDF transformation,\nincludes manually annotated aspect-level feature ontology words and sentiment\npolarity assignments. The proposed model combines CNNs with attention\nmechanisms for aspect-word prediction, leveraging contextual information and\nsentiment polarities. With 91% accuracy on aspect term extraction and 92% on\nsentiment polarity classification, the model outperforms traditional machine\nmodels, offering insights into specific aspects and sentiments. This study\nadvances ABSA research, particularly in underrepresented languages, with\nimplications for cross-cultural linguistic research.\n', '  Aspect-Based Sentiment Analysis (ABSA) aims to identify terms or multiword\nexpressions (MWEs) on which sentiments are expressed and the sentiment\npolarities associated with them. The development of supervised models has been\nat the forefront of research in this area. However, training these models\nrequires the availability of manually annotated datasets which is both\nexpensive and time-consuming. Furthermore, the available annotated datasets are\ntailored to a specific domain, language, and text type. In this work, we\naddress this notable challenge in current state-of-the-art ABSA research. We\npropose a hybrid approach for Aspect Based Sentiment Analysis using transfer\nlearning. The approach focuses on generating weakly-supervised annotations by\nexploiting the strengths of both large language models (LLM) and traditional\nsyntactic dependencies. We utilise syntactic dependency structures of sentences\nto complement the annotations generated by LLMs, as they may overlook\ndomain-specific aspect terms. Extensive experimentation on multiple datasets is\nperformed to demonstrate the efficacy of our hybrid method for the tasks of\naspect term extraction and aspect sentiment classification.\n  Keywords: Aspect Based Sentiment Analysis, Syntactic Parsing, large language\nmodel (LLM)\n', '  Aspect-Based Sentiment Analysis (ABSA) is a fine-grained linguistics problem\nthat entails the extraction of multifaceted aspects, opinions, and sentiments\nfrom the given text. Both standalone and compound ABSA tasks have been\nextensively used in the literature to examine the nuanced information present\nin online reviews and social media posts. Current ABSA methods often rely on\nstatic hyperparameters for attention-masking mechanisms, which can struggle\nwith context adaptation and may overlook the unique relevance of words in\nvaried situations. This leads to challenges in accurately analyzing complex\nsentences containing multiple aspects with differing sentiments. In this work,\nwe present adaptive masking methods that remove irrelevant tokens based on\ncontext to assist in Aspect Term Extraction and Aspect Sentiment Classification\nsubtasks of ABSA. We show with our experiments that the proposed methods\noutperform the baseline methods in terms of accuracy and F1 scores on four\nbenchmark online review datasets. Further, we show that the proposed methods\ncan be extended with multiple adaptations and demonstrate a qualitative\nanalysis of the proposed approach using sample text for aspect term extraction.\n']",Aspect-Based Sentiment Analysis
127,126,67,126_explanations_networks_explainers_attention,"['explanations', 'networks', 'explainers', 'attention', 'subgraph', 'explainability', 'graphs', 'explaining', 'neural', 'explainer']","['explanations', 'explanation', 'graph', 'explainability', 'subgraph', 'explainer', 'graphs', 'subgraphs', 'explainers', 'causal']","['networks', 'explainability', 'neural', 'subgraphs', 'gnns', 'node', 'gnnexplainer', 'predictions', 'attributions', 'datasets']","['  Graph Neural Networks (GNNs) are effective for node classification in\ngraph-structured data, but they lack explainability, especially at the global\nlevel. Current research mainly utilizes subgraphs of the input as local\nexplanations or generates new graphs as global explanations. However, these\ngraph-based methods are limited in their ability to explain classes with\nmultiple sufficient explanations. To provide more expressive explanations, we\npropose utilizing class expressions (CEs) from the field of description logic\n(DL). Our approach explains heterogeneous graphs with different types of nodes\nusing CEs in the EL description logic. To identify the best explanation among\nmultiple candidate explanations, we employ and compare two different scoring\nfunctions: (1) For a given CE, we construct multiple graphs, have the GNN make\na prediction for each graph, and aggregate the predicted scores. (2) We score\nthe CE in terms of fidelity, i.e., we compare the predictions of the GNN to the\npredictions by the CE on a separate validation set. Instead of subgraph-based\nexplanations, we offer CE-based explanations.\n', '  Aside from graph neural networks (GNNs) attracting significant attention as a\npowerful framework revolutionizing graph representation learning, there has\nbeen an increasing demand for explaining GNN models. Although various\nexplanation methods for GNNs have been developed, most studies have focused on\ninstance-level explanations, which produce explanations tailored to a given\ngraph instance. In our study, we propose Prototype-bAsed GNN-Explainer (PAGE),\na novel model-level GNN explanation method that explains what the underlying\nGNN model has learned for graph classification by discovering\nhuman-interpretable prototype graphs. Our method produces explanations for a\ngiven class, thus being capable of offering more concise and comprehensive\nexplanations than those of instance-level explanations. First, PAGE selects\nembeddings of class-discriminative input graphs on the graph-level embedding\nspace after clustering them. Then, PAGE discovers a common subgraph pattern by\niteratively searching for high matching node tuples using node-level embeddings\nvia a prototype scoring function, thereby yielding a prototype graph as our\nexplanation. Using six graph classification datasets, we demonstrate that PAGE\nqualitatively and quantitatively outperforms the state-of-the-art model-level\nexplanation method. We also carry out systematic experimental studies by\ndemonstrating the relationship between PAGE and instance-level explanation\nmethods, the robustness of PAGE to input data scarce environments, and the\ncomputational efficiency of the proposed prototype scoring function in PAGE.\n', ""  Graph Neural Networks (GNNs) have gained considerable traction for their\ncapability to effectively process topological data, yet their interpretability\nremains a critical concern. Current interpretation methods are dominated by\npost-hoc explanations to provide a transparent and intuitive understanding of\nGNNs. However, they have limited performance in interpreting complicated\nsubgraphs and can't utilize the explanation to advance GNN predictions. On the\nother hand, transparent GNN models are proposed to capture critical subgraphs.\nWhile such methods could improve GNN predictions, they usually don't perform\nwell on explanations. Thus, it is desired for a new strategy to better couple\nGNN explanation and prediction. In this study, we have developed a novel\ninterpretable causal GNN framework that incorporates retrieval-based causal\nlearning with Graph Information Bottleneck (GIB) theory. The framework could\nsemi-parametrically retrieve crucial subgraphs detected by GIB and compress the\nexplanatory subgraphs via a causal module. The framework was demonstrated to\nconsistently outperform state-of-the-art methods, and to achieve 32.71\\% higher\nprecision on real-world explanation scenarios with diverse explanation types.\nMore importantly, the learned explanations were shown able to also improve GNN\nprediction performance.\n""]",Explainability in Graph Neural Networks
128,127,67,127_backdoors_adversarial_attacks_backdoor,"['backdoors', 'adversarial', 'attacks', 'backdoor', 'vulnerabilities', 'hacking', 'malicious', 'vulnerability', 'attacker', 'security']","['backdoor', 'injection', 'attacks', 'attack', 'prompt', 'malicious', 'attacker', 'trigger', 'security', 'triggers']","['backdoors', 'adversarial', 'attacks', 'hacking', 'malicious', 'vulnerability', 'injections', 'triggers', 'prompts', 'llm']","['  Leveraging the rapid development of Large Language Models LLMs, LLM-based\nagents have been developed to handle various real-world applications, including\nfinance, healthcare, and shopping, etc. It is crucial to ensure the reliability\nand security of LLM-based agents during applications. However, the safety\nissues of LLM-based agents are currently under-explored. In this work, we take\nthe first step to investigate one of the typical safety threats, backdoor\nattack, to LLM-based agents. We first formulate a general framework of agent\nbackdoor attacks, then we present a thorough analysis on the different forms of\nagent backdoor attacks. Specifically, from the perspective of the final\nattacking outcomes, the attacker can either choose to manipulate the final\noutput distribution, or only introduce malicious behavior in the intermediate\nreasoning process, while keeping the final output correct. Furthermore, the\nformer category can be divided into two subcategories based on trigger\nlocations: the backdoor trigger can be hidden either in the user query or in an\nintermediate observation returned by the external environment. We propose the\ncorresponding data poisoning mechanisms to implement the above variations of\nagent backdoor attacks on two typical agent tasks, web shopping and tool\nutilization. Extensive experiments show that LLM-based agents suffer severely\nfrom backdoor attacks, indicating an urgent need for further research on the\ndevelopment of defenses against backdoor attacks on LLM-based agents. Warning:\nThis paper may contain biased content.\n', '  Large Language Models (LLMs) have shown significant promise in\ndecision-making tasks when fine-tuned on specific applications, leveraging\ntheir inherent common sense and reasoning abilities learned from vast amounts\nof data. However, these systems are exposed to substantial safety and security\nrisks during the fine-tuning phase. In this work, we propose the first\ncomprehensive framework for Backdoor Attacks against LLM-enabled\nDecision-making systems (BALD), systematically exploring how such attacks can\nbe introduced during the fine-tuning phase across various channels.\nSpecifically, we propose three attack mechanisms and corresponding backdoor\noptimization methods to attack different components in the LLM-based\ndecision-making pipeline: word injection, scenario manipulation, and knowledge\ninjection. Word injection embeds trigger words directly into the query prompt.\nScenario manipulation occurs in the physical environment, where a high-level\nbackdoor semantic scenario triggers the attack. Knowledge injection conducts\nbackdoor attacks on retrieval augmented generation (RAG)-based LLM systems,\nstrategically injecting word triggers into poisoned knowledge while ensuring\nthe information remains factually accurate for stealthiness. We conduct\nextensive experiments with three popular LLMs (GPT-3.5, LLaMA2, PaLM2), using\ntwo datasets (HighwayEnv, nuScenes), and demonstrate the effectiveness and\nstealthiness of our backdoor triggers and mechanisms. Finally, we critically\nassess the strengths and weaknesses of our proposed approaches, highlight the\ninherent vulnerabilities of LLMs in decision-making tasks, and evaluate\npotential defenses to safeguard LLM-based decision making systems.\n', '  The large language models (LLMs), which bridge the gap between human language\nunderstanding and complex problem-solving, achieve state-of-the-art performance\non several NLP tasks, particularly in few-shot and zero-shot settings. Despite\nthe demonstrable efficacy of LMMs, due to constraints on computational\nresources, users have to engage with open-source language models or outsource\nthe entire training process to third-party platforms. However, research has\ndemonstrated that language models are susceptible to potential security\nvulnerabilities, particularly in backdoor attacks. Backdoor attacks are\ndesigned to introduce targeted vulnerabilities into language models by\npoisoning training samples or model weights, allowing attackers to manipulate\nmodel responses through malicious triggers. While existing surveys on backdoor\nattacks provide a comprehensive overview, they lack an in-depth examination of\nbackdoor attacks specifically targeting LLMs. To bridge this gap and grasp the\nlatest trends in the field, this paper presents a novel perspective on backdoor\nattacks for LLMs by focusing on fine-tuning methods. Specifically, we\nsystematically classify backdoor attacks into three categories: full-parameter\nfine-tuning, parameter-efficient fine-tuning, and attacks without fine-tuning.\nBased on insights from a substantial review, we also discuss crucial issues for\nfuture research on backdoor attacks, such as further exploring attack\nalgorithms that do not require fine-tuning, or developing more covert attack\nalgorithms.\n']",Backdoor Attacks on Large Language Models
129,128,67,128_facttrack_veracity_factuality_factdetect,"['facttrack', 'veracity', 'factuality', 'factdetect', 'credibility', 'evidence', 'factscore', 'corpus', 'claims', 'factual']","['fact', 'claim', 'checking', 'claims', 'evidence', 'verification', 'veracity', 'checkers', 'factuality', 'factual']","['facttrack', 'veracity', 'factdetect', 'factscore', 'corpus', 'factgenius', 'factually', 'disinformation', 'content', 'checkers']","[""  Evidence retrieval is a core part of automatic fact-checking. Prior work\nmakes simplifying assumptions in retrieval that depart from real-world use\ncases: either no access to evidence, access to evidence curated by a human\nfact-checker, or access to evidence available long after the claim has been\nmade. In this work, we present the first fully automated pipeline to check\nreal-world claims by retrieving raw evidence from the web. We restrict our\nretriever to only search documents available prior to the claim's making,\nmodeling the realistic scenario where an emerging claim needs to be checked.\nOur pipeline includes five components: claim decomposition, raw document\nretrieval, fine-grained evidence retrieval, claim-focused summarization, and\nveracity judgment. We conduct experiments on complex political claims in the\nClaimDecomp dataset and show that the aggregated evidence produced by our\npipeline improves veracity judgments. Human evaluation finds the evidence\nsummary produced by our system is reliable (it does not hallucinate\ninformation) and relevant to answering key questions about a claim, suggesting\nthat it can assist fact-checkers even when it cannot surface a complete\nevidence set.\n"", '  Fact-checking is the task of verifying the factuality of a given claim by\nexamining the available evidence. High-quality evidence plays a vital role in\nenhancing fact-checking systems and facilitating the generation of explanations\nthat are understandable to humans. However, the provision of both sufficient\nand relevant evidence for explainable fact-checking systems poses a challenge.\nTo tackle this challenge, we propose a method based on a Large Language Model\nto automatically retrieve and summarize evidence from the Web. Furthermore, we\nconstruct RU22Fact, a novel multilingual explainable fact-checking dataset on\nthe Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world\nclaims, optimized evidence, and referenced explanation. To establish a baseline\nfor our dataset, we also develop an end-to-end explainable fact-checking system\nto verify claims and generate explanations. Experimental results demonstrate\nthe prospect of optimized evidence in increasing fact-checking performance and\nalso indicate the possibility of further progress in the end-to-end claim\nverification and explanation generation tasks.\n', ""  Fact-checking real-world claims often requires reviewing multiple multimodal\ndocuments to assess a claim's truthfulness, which is a highly laborious and\ntime-consuming task. In this paper, we present a summarization model designed\nto generate claim-specific summaries useful for fact-checking from multimodal,\nmulti-document datasets. The model takes inputs in the form of documents,\nimages, and a claim, with the objective of assisting in fact-checking tasks. We\nintroduce a dynamic perceiver-based model that can handle inputs from multiple\nmodalities of arbitrary lengths. To train our model, we leverage a novel\nreinforcement learning-based entailment objective to generate summaries that\nprovide evidence distinguishing between different truthfulness labels. To\nassess the efficacy of our approach, we conduct experiments on both an existing\nbenchmark and a new dataset of multi-document claims that we contribute. Our\napproach outperforms the SOTA approach by 4.6% in the claim verification task\non the MOCHEG dataset and demonstrates strong performance on our new\nMulti-News-Fact-Checking dataset.\n""]",Automated Fact-Checking and Evidence Retrieval
130,129,67,129_calibration_calibrated_accuracy_softmax,"['calibration', 'calibrated', 'accuracy', 'softmax', 'imagenet', 'calibrator', 'neural', 'deep', 'networks', 'prediction']","['calibration', 'distribution', 'uncertainty', 'confidence', 'predictions', 'overconfident', 'estimation', 'test', 'hoc', 'reliability']","['calibration', 'accuracy', 'softmax', 'imagenet', 'prediction', 'classifier', 'robust', 'trained', 'dnns', 'reliability']","['  Deep Neural Networks (DNNs) excel in various domains but face challenges in\nproviding accurate uncertainty estimates, which are crucial for high-stakes\napplications. Large Language Models (LLMs) have recently emerged as powerful\ntools, demonstrating exceptional performance in language tasks. However,\ntraditional calibration metrics such as Expected Calibration Error (ECE) and\nclasswise-ECE (cw-ECE) are inadequate for LLMs due to their vast vocabularies,\ndata complexity, and distributional focus. To address this, we propose a novel\ncalibration concept called full calibration and introduce its corresponding\nmetric, Full-ECE. Full-ECE evaluates the entire predicted probability\ndistribution, offering a more accurate and robust measure of calibration for\nLLMs.\n', '  Research interests in the robustness of deep neural networks against domain\nshifts have been rapidly increasing in recent years. Most existing works,\nhowever, focus on improving the accuracy of the model, not the calibration\nperformance which is another important requirement for trustworthy AI systems.\nTemperature scaling (TS), an accuracy-preserving post-hoc calibration method,\nhas been proven to be effective in in-domain settings, but not in out-of-domain\n(OOD) due to the difficulty in obtaining a validation set for the unseen domain\nbeforehand. In this paper, we propose consistency-guided temperature scaling\n(CTS), a new temperature scaling strategy that can significantly enhance the\nOOD calibration performance by providing mutual supervision among data samples\nin the source domains. Motivated by our observation that over-confidence\nstemming from inconsistent sample predictions is the main obstacle to OOD\ncalibration, we propose to guide the scaling process by taking consistencies\ninto account in terms of two different aspects -- style and content -- which\nare the key components that can well-represent data samples in multi-domain\nsettings. Experimental results demonstrate that our proposed strategy\noutperforms existing works, achieving superior OOD calibration performance on\nvarious datasets. This can be accomplished by employing only the source domains\nwithout compromising accuracy, making our scheme directly applicable to various\ntrustworthy AI systems.\n', '  Deep neural networks are increasingly utilized in various machine learning\ntasks. However, as these models grow in complexity, they often face calibration\nissues, despite enhanced prediction accuracy. Many studies have endeavored to\nimprove calibration performance through the use of specific loss functions,\ndata preprocessing and training frameworks. Yet, investigations into\ncalibration properties have been somewhat overlooked. Our study leverages the\nNeural Architecture Search (NAS) search space, offering an exhaustive model\narchitecture space for thorough calibration properties exploration. We\nspecifically create a model calibration dataset. This dataset evaluates 90\nbin-based and 12 additional calibration measurements across 117,702 unique\nneural networks within the widely employed NATS-Bench search space. Our\nanalysis aims to answer several longstanding questions in the field, using our\nproposed dataset: (i) Can model calibration be generalized across different\ndatasets? (ii) Can robustness be used as a calibration measurement? (iii) How\nreliable are calibration metrics? (iv) Does a post-hoc calibration method\naffect all models uniformly? (v) How does calibration interact with accuracy?\n(vi) What is the impact of bin size on calibration measurement? (vii) Which\narchitectural designs are beneficial for calibration? Additionally, our study\nbridges an existing gap by exploring calibration within NAS. By providing this\ndataset, we enable further research into NAS calibration. As far as we are\naware, our research represents the first large-scale investigation into\ncalibration properties and the premier study of calibration issues within NAS.\nThe project page can be found at https://www.taolinwei.com/calibration-study\n']",Calibration in Deep Neural Networks
131,130,66,130_hallucinations_hallucination_hallucinate_hallucinatory,"['hallucinations', 'hallucination', 'hallucinate', 'hallucinatory', 'hallucinated', 'multimodal', 'hallusionbench', 'captioning', 'lvlm', 'visual']","['hallucinations', 'hallucination', 'visual', 'object', 'hallucinatory', 'vision', 'objects', 'multimodal', 'image', 'descriptions']","['hallucination', 'multimodal', 'hallusionbench', 'hallucidoctor', 'vision', 'lvlms', 'decoding', 'hallu', 'autohallusion', 'descriptions']","[""  Though advanced in understanding visual information with human languages,\nLarge Vision-Language Models (LVLMs) still suffer from multimodal\nhallucinations. A natural concern is that during multimodal interaction, the\ngenerated hallucinations could influence the LVLMs' subsequent generation.\nThus, we raise a question: When presented with a query relevant to the\npreviously generated hallucination, will LVLMs be misled and respond\nincorrectly, even though the ground visual information exists? To answer this,\nwe propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when\nencountering generated hallucinations, where LVLMs are required to answer\nspecific visual questions within a curated hallucinatory conversation.\nCrucially, our experiment shows that the performance of open-source LVLMs drops\nby at least $31\\%$, indicating that LVLMs are prone to accept the generated\nhallucinations and make false claims that they would not have supported without\ndistractions. We term this phenomenon Multimodal Hallucination Snowballing. To\nmitigate this, we further propose a training-free method called Residual Visual\nDecoding, where we revise the output distribution of LVLMs with the one derived\nfrom the residual visual input, providing models with direct access to the\nvisual information. Experiments show that our method can mitigate more than\n$24\\%$ of the snowballed multimodal hallucination while maintaining\ncapabilities.\n"", '  Large Vision Language Models exhibit remarkable capabilities but struggle\nwith hallucinations inconsistencies between images and their descriptions.\nPrevious hallucination evaluation studies on LVLMs have identified\nhallucinations in terms of objects, attributes, and relations but overlooked\ncomplex hallucinations that create an entire narrative around a fictional\nentity. In this paper, we introduce a refined taxonomy of hallucinations,\nfeaturing a new category: Event Hallucination. We then utilize advanced LLMs to\ngenerate and filter fine grained hallucinatory data consisting of various types\nof hallucinations, with a particular focus on event hallucinations, laying the\ngroundwork for integrating discriminative and generative evaluation methods\nwithin our universal evaluation framework. The proposed benchmark distinctively\nassesses LVLMs ability to tackle a broad spectrum of hallucinations, making it\na reliable and comprehensive tool for gauging LVLMs efficacy in handling\nhallucinations. We will release our code and data.\n', ""  Recent development of Large Vision-Language Models (LVLMs) has attracted\ngrowing attention within the AI landscape for its practical implementation\npotential. However, ``hallucination'', or more specifically, the misalignment\nbetween factual visual content and corresponding textual generation, poses a\nsignificant challenge of utilizing LVLMs. In this comprehensive survey, we\ndissect LVLM-related hallucinations in an attempt to establish an overview and\nfacilitate future mitigation. Our scrutiny starts with a clarification of the\nconcept of hallucinations in LVLMs, presenting a variety of hallucination\nsymptoms and highlighting the unique challenges inherent in LVLM\nhallucinations. Subsequently, we outline the benchmarks and methodologies\ntailored specifically for evaluating hallucinations unique to LVLMs.\nAdditionally, we delve into an investigation of the root causes of these\nhallucinations, encompassing insights from the training data and model\ncomponents. We also critically review existing methods for mitigating\nhallucinations. The open questions and future directions pertaining to\nhallucinations within LVLMs are discussed to conclude this survey.\n""]",Hallucinations in Large Vision-Language Models
132,131,66,131_annotations_annotation_entities_entity,"['annotations', 'annotation', 'entities', 'entity', 'annotated', 'nlp', 'corpus', 'labeling', 'supervised', 'semantic']","['entity', 'entities', 'recognition', 'span', 'extraction', 'annotation', 'shot', 'annotated', 'corpus', 'types']","['annotations', 'entities', 'nlp', 'corpus', 'supervised', 'ontonotes', 'negation', 'mentions', 'task', 'extraction']","['  Named Entity Recognition (NER) serves as a fundamental task in natural\nlanguage understanding, bearing direct implications for web content analysis,\nsearch engines, and information retrieval systems. Fine-tuned NER models\nexhibit satisfactory performance on standard NER benchmarks. However, due to\nlimited fine-tuning data and lack of knowledge, it performs poorly on unseen\nentity recognition. As a result, the usability and reliability of NER models in\nweb-related applications are compromised. Instead, Large Language Models (LLMs)\nlike GPT-4 possess extensive external knowledge, but research indicates that\nthey lack specialty for NER tasks. Furthermore, non-public and large-scale\nweights make tuning LLMs difficult. To address these challenges, we propose a\nframework that combines small fine-tuned models with LLMs (LinkNER) and an\nuncertainty-based linking strategy called RDC that enables fine-tuned models to\ncomplement black-box LLMs, achieving better performance. We experiment with\nboth standard NER test sets and noisy social media datasets. LinkNER enhances\nNER task performance, notably surpassing SOTA models in robustness tests. We\nalso quantitatively analyze the influence of key components like uncertainty\nestimation methods, LLMs, and in-context learning on diverse NER tasks,\noffering specific web-related recommendations.\n', '  Lately, instruction-based techniques have made significant strides in\nimproving performance in few-shot learning scenarios. They achieve this by\nbridging the gap between pre-trained language models and fine-tuning for\nspecific downstream tasks. Despite these advancements, the performance of Large\nLanguage Models (LLMs) in information extraction tasks like Named Entity\nRecognition (NER), using prompts or instructions, still falls short of\nsupervised baselines. The reason for this performance gap can be attributed to\nthe fundamental disparity between NER and LLMs. NER is inherently a sequence\nlabeling task, where the model must assign entity-type labels to individual\ntokens within a sentence. In contrast, LLMs are designed as a text generation\ntask. This distinction between semantic labeling and text generation leads to\nsubpar performance. In this paper, we transform the NER task into a\ntext-generation task that can be readily adapted by LLMs. This involves\nenhancing source sentences with task-specific instructions and answer choices,\nallowing for the identification of entities and their types within natural\nlanguage. We harness the strength of LLMs by integrating supervised learning\nwithin them. The goal of this combined strategy is to boost the performance of\nLLMs in extraction tasks like NER while simultaneously addressing hallucination\nissues often observed in LLM-generated content. A novel corpus Contract NER\ncomprising seven frequently observed contract categories, encompassing named\nentities associated with 18 distinct legal entity types is released along with\nour baseline models. Our models and dataset are available to the community for\nfuture research * .\n', '  Named Entity Recognition (NER) is a well-studied problem in NLP. However,\nthere is much less focus on studying NER datasets, compared to developing new\nNER models. In this paper, we employed three simple techniques to detect\nannotation errors in the OntoNotes 5.0 corpus for English NER, which is the\nlargest available NER corpus for English. Our techniques corrected ~10% of the\nsentences in train/dev/test data. In terms of entity mentions, we corrected the\nspan and/or type of ~8% of mentions in the dataset, while\nadding/deleting/splitting/merging a few more. These are large numbers of\nchanges, considering the size of OntoNotes. We used three NER libraries to\ntrain, evaluate and compare the models trained with the original and the\nre-annotated datasets, which showed an average improvement of 1.23% in overall\nF-scores, with large (>10%) improvements for some of the entity types. While\nour annotation error detection methods are not exhaustive and there is some\nmanual annotation effort involved, they are largely language agnostic and can\nbe employed with other NER datasets, and other sequence labelling tasks.\n']",Named Entity Recognition in Natural Language Processing
133,132,66,132_fusionnet_vision_yolov5_yolov8,"['fusionnet', 'vision', 'yolov5', 'yolov8', 'trained', 'imagery', 'driving', 'images', 'visual', 'photos']","['object', 'aerial', 'objects', 'detection', 'images', 'geolocalization', 'location', 'conditions', 'image', 'synthetic']","['fusionnet', 'yolov8', 'imagery', 'driving', 'dataset', 'adaptation', 'foggy', 'geodecoder', 'rgb', 'backgrounds']","['  Small object detection in aerial imagery presents significant challenges in\ncomputer vision due to the minimal data inherent in small-sized objects and\ntheir propensity to be obscured by larger objects and background noise.\nTraditional methods using transformer-based models often face limitations\nstemming from the lack of specialized databases, which adversely affect their\nperformance with objects of varying orientations and scales. This underscores\nthe need for more adaptable, lightweight models. In response, this paper\nintroduces two innovative approaches that significantly enhance detection and\nsegmentation capabilities for small aerial objects. Firstly, we explore the use\nof the SAHI framework on the newly introduced lightweight YOLO v9 architecture,\nwhich utilizes Programmable Gradient Information (PGI) to reduce the\nsubstantial information loss typically encountered in sequential feature\nextraction processes. The paper employs the Vision Mamba model, which\nincorporates position embeddings to facilitate precise location-aware visual\nunderstanding, combined with a novel bidirectional State Space Model (SSM) for\neffective visual context modeling. This State Space Model adeptly harnesses the\nlinear complexity of CNNs and the global receptive field of Transformers,\nmaking it particularly effective in remote sensing image classification. Our\nexperimental results demonstrate substantial improvements in detection accuracy\nand processing efficiency, validating the applicability of these approaches for\nreal-time small object detection across diverse aerial scenarios. This paper\nalso discusses how these methodologies could serve as foundational models for\nfuture advancements in aerial object recognition technologies. The source code\nwill be made accessible here.\n', '  Robust perception is crucial in autonomous vehicle navigation and\nlocalization. Visual processing tasks, like semantic segmentation, should work\nin varying weather conditions and during different times of day. Semantic\nsegmentation is where each pixel is assigned a class, which is useful for\nlocating overall features (1). Training a segmentation model requires large\namounts of data, and the labeling process for segmentation data is especially\ntedious. Additionally, many large datasets include only images taken in clear\nweather. This is a problem because training a model exclusively on clear\nweather data hinders performance in adverse weather conditions like fog or\nrain. We hypothesize that given a dataset of only clear days images, applying\nimage augmentation (such as random rain, fog, and brightness) during training\nallows for domain adaptation to diverse weather conditions. We used CARLA, a 3D\nrealistic autonomous vehicle simulator, to collect 1200 images in clear weather\ncomposed of 29 classes from 10 different towns (2). We also collected 1200\nimages of random weather effects. We trained encoder-decoder UNet models to\nperform semantic segmentation. Applying augmentations significantly improved\nsegmentation under weathered night conditions (p < 0.001). However, models\ntrained on weather data have significantly lower losses than those trained on\naugmented data in all conditions except for clear days. This shows there is\nroom for improvement in the domain adaptation approach. Future work should test\nmore types of augmentations and also use real-life images instead of CARLA.\nIdeally, the augmented model meets or exceeds the performance of the weather\nmodel.\n', '  Driving is challenging in conditions like night, rain, and snow. The lack of\ngood labeled datasets has hampered progress in scene understanding under such\nconditions. Unsupervised domain adaptation (UDA) using large labeled clear-day\ndatasets is a promising research direction in such cases. Current UDA methods,\nhowever, treat all image pixels uniformly, leading to over-reliance on the\ndominant scene backgrounds (e.g., roads, sky, sidewalks) that appear\ndramatically different across domains. As a result, they struggle to learn\neffective features of smaller and often sparse foreground objects (e.g.,\npeople, vehicles, signs).\n  In this work, we improve UDA training by using in-place image warping to\nfocus on salient object regions. Our insight is that while backgrounds vary\nsignificantly across domains (e.g., snowy night vs. clear day), object\nappearances vary to a lesser extent. Therefore, we design instance-level\nsaliency guidance to adaptively oversample object regions, which reduces\nadverse effects from background context and enhances backbone feature learning.\nWe then unwarp the better learned features while adapting from source to\ntarget. Our approach improves adaptation across geographies, lighting, and\nweather conditions, and is agnostic to the task (segmentation, detection),\ndomain adaptation algorithm, saliency guidance, and underlying model\narchitecture. Result highlights include +6.1 mAP50 for BDD100K Clear\n$\\rightarrow$ DENSE Foggy, +3.7 mAP50 for BDD100K Day $\\rightarrow$ Night, +3.0\nmAP50 for BDD100K Clear $\\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes\n$\\rightarrow$ ACDC. Our method adds minimal training memory and incurs no\nadditional inference latency. Please see Appendix for more results and\nanalysis.\n']",Object Detection and Segmentation in Aerial and Road Images
134,133,65,133_alzheimer_dementia_neurodegenerative_neuroimaging,"['alzheimer', 'dementia', 'neurodegenerative', 'neuroimaging', 'neurodegeneration', 'mri', 'amyloid', 'predicting', 'fmri', 'mris']","['disease', 'brain', 'neuroimaging', 'progression', 'dementia', 'age', 'cognitive', 'imaging', 'early', 'amyloid']","['alzheimer', 'mri', 'predicting', 'fmri', 'biomarkers', 'amyloidpet', 'cnns', 'deeprepviz', 'neurovnn', 'multimodal']","['  Exploring the application of deep learning technologies in the field of\nmedical diagnostics, Magnetic Resonance Imaging (MRI) provides a unique\nperspective for observing and diagnosing complex neurodegenerative diseases\nsuch as Alzheimer Disease (AD). With advancements in deep learning,\nparticularly in Convolutional Neural Networks (CNNs) and the Xception network\narchitecture, we are now able to analyze and classify vast amounts of MRI data\nwith unprecedented accuracy. The progress of this technology not only enhances\nour understanding of brain structural changes but also opens up new avenues for\nmonitoring disease progression through non-invasive means and potentially\nallows for precise diagnosis in the early stages of the disease.\n  This study aims to classify MRI images using deep learning models to identify\ndifferent stages of Alzheimer Disease through a series of innovative data\nprocessing and model construction steps. Our experimental results show that the\ndeep learning framework based on the Xception model achieved a 99.6% accuracy\nrate in the multi-class MRI image classification task, demonstrating its\npotential application value in assistive diagnosis. Future research will focus\non expanding the dataset, improving model interpretability, and clinical\nvalidation to further promote the application of deep learning technology in\nthe medical field, with the hope of bringing earlier diagnosis and more\npersonalized treatment plans to Alzheimer Disease patients.\n', ""  Alzheimer's is a brain disease that gets worse over time and affects memory,\nthinking, and behavior. Alzheimer's disease (AD) can be treated and managed if\nit is diagnosed early, which can slow the progression of symptoms and improve\nquality of life. In this study, we suggested using the Visual Transformer (ViT)\nand bi-LSTM to process MRI images for diagnosing Alzheimer's disease. We used\nViT to extract features from the MRI and then map them to a feature sequence.\nThen, we used Bi-LSTM sequence modeling to keep the interdependencies between\nrelated features. In addition, we evaluated the performance of the proposed\nmodel for the binary classification of AD patients using data from the\nAlzheimer's Disease Neuroimaging Initiative (ADNI). Finally, we evaluated our\nmethod against other deep learning models in the literature. The proposed\nmethod performs well in terms of accuracy, precision, F-score, and recall for\nthe diagnosis of AD.\n"", ""  Objectives: The objectives of this narrative review are to summarize the\ncurrent state of AI applications in neuroimaging for early Alzheimer's disease\n(AD) prediction and to highlight the potential of AI techniques in improving\nearly AD diagnosis, prognosis, and management.\n  Methods: We conducted a narrative review of studies using AI techniques\napplied to neuroimaging data for early AD prediction. We examined\nsingle-modality studies using structural MRI and PET imaging, as well as\nmulti-modality studies integrating multiple neuroimaging techniques and\nbiomarkers. Furthermore, they reviewed longitudinal studies that model AD\nprogression and identify individuals at risk of rapid decline.\n  Results: Single-modality studies using structural MRI and PET imaging have\ndemonstrated high accuracy in classifying AD and predicting progression from\nmild cognitive impairment (MCI) to AD. Multi-modality studies, integrating\nmultiple neuroimaging techniques and biomarkers, have shown improved\nperformance and robustness compared to single-modality approaches. Longitudinal\nstudies have highlighted the value of AI in modeling AD progression and\nidentifying individuals at risk of rapid decline. However, challenges remain in\ndata standardization, model interpretability, generalizability, clinical\nintegration, and ethical considerations.\n  Conclusion: AI techniques applied to neuroimaging data have the potential to\nimprove early AD diagnosis, prognosis, and management. Addressing challenges\nrelated to data standardization, model interpretability, generalizability,\nclinical integration, and ethical considerations is crucial for realizing the\nfull potential of AI in AD research and clinical practice. Collaborative\nefforts among researchers, clinicians, and regulatory agencies are needed to\ndevelop reliable, robust, and ethical AI tools that can benefit AD patients and\nsociety.\n""]",Alzheimer's Disease Diagnosis using Neuroimaging and AI
135,134,65,134_wifi_fingerprinting_gps_wi,"['wifi', 'fingerprinting', 'gps', 'wi', 'wireless', 'fingerprint', 'localization', 'radiomap', 'ranging', 'sensing']","['localization', 'indoor', 'radio', 'channel', 'sight', 'wireless', 'sensing', 'signal', 'antenna', 'measurements']","['fingerprinting', 'gps', 'wireless', 'localization', 'radiomap', 'sensing', 'indoor', 'accuracy', 'wiopen', 'tracing']","[""  The rise of the Internet of Things (IoT) and mobile internet applications has\nspurred interest in location-based services (LBS) for commercial, military, and\nsocial applications. While the global positioning system (GPS) dominates\noutdoor localization, its efficacy wanes indoors due to signal challenges.\nIndoor localization systems leverage wireless technologies like Wi-Fi, ZigBee,\nBluetooth, UWB, selecting based on context. Received signal strength indicator\n(RSSI) technology, known for its accuracy and simplicity, is widely adopted.\nThis study employs machine learning algorithms in three phases: supervised\nregressors, supervised classifiers, and ensemble methods for RSSI-based indoor\nlocalization. Additionally, it introduces a weighted least squares technique\nand pseudo-linear solution approach to address non-linear RSSI measurement\nequations by approximating them with linear equations. An experimental testbed,\nutilizing diverse wireless technologies and anchor nodes, is designed for data\ncollection, employing IoT cloud architectures. Pre-processing involves\ninvestigating filters for data refinement before algorithm training. The study\nemploys machine learning models like linear regression, polynomial regression,\nsupport vector regression, random forest regression, and decision tree\nregressor across various wireless technologies. These models estimate the\ngeographical coordinates of a moving target node, and their performance is\nevaluated using metrics such as accuracy, root mean square errors, precision,\nrecall, sensitivity, coefficient of determinant, and the f1-score. The\nexperiment's outcomes provide insights into the effectiveness of different\nsupervised machine learning techniques in terms of localization accuracy and\nrobustness in indoor environments.\n"", '  The recognition of human activities based on WiFi Channel State Information\n(CSI) enables contactless and visual privacy-preserving sensing in indoor\nenvironments. However, poor model generalization, due to varying environmental\nconditions and sensing hardware, is a well-known problem in this space. To\naddress this issue, in this work, data augmentation techniques commonly used in\nimage-based learning are applied to WiFi CSI to investigate their effects on\nmodel generalization performance in cross-scenario and cross-system settings.\nIn particular, we focus on the generalization between line-of-sight (LOS) and\nnon-line-of-sight (NLOS) through-wall scenarios, as well as on the\ngeneralization between different antenna systems, which remains under-explored.\nWe collect and make publicly available a dataset of CSI amplitude spectrograms\nof human activities. Utilizing this data, an ablation study is conducted in\nwhich activity recognition models based on the EfficientNetV2 architecture are\ntrained, allowing us to assess the effects of each augmentation on model\ngeneralization performance. The gathered results show that specific\ncombinations of simple data augmentation techniques applied to CSI amplitude\ndata can significantly improve cross-scenario and cross-system generalization.\n', '  Wi-Fi fingerprinting has emerged as the most popular approach to indoor\nlocalization. The use of ML algorithms has greatly improved the localization\nperformance of Wi-Fi fingerprinting, but its success depends on the\navailability of fingerprint databases composed of a large number of RSSIs, the\nMAC addresses of access points, and the other measurement information. However,\nmost fingerprint databases do not reflect well the time varying nature of\nelectromagnetic interferences in complicated modern indoor environment. This\ncould result in significant changes in statistical characteristics of\ntraining/validation and testing datasets, which are often constructed at\ndifferent times, and even the characteristics of the testing datasets could be\ndifferent from those of the data submitted by users during the operation of\nlocalization systems after their deployment. In this paper, we consider the\nimplications of time-varying Wi-Fi fingerprints on indoor localization from a\ndata-centric point of view and discuss the differences between static and\ndynamic databases. As a case study, we have constructed a dynamic database\ncovering three floors of the IR building of XJTLU based on RSSI measurements,\nover 44 days, and investigated the differences between static and dynamic\ndatabases in terms of statistical characteristics and localization performance.\nThe analyses based on variance calculations and Isolation Forest show the\ntemporal shifts in RSSIs, which result in a noticeable trend of the increase in\nthe localization error of a Gaussian process regression model with the maximum\nerror of 6.65 m after 14 days of training without model adjustments. The\nresults of the case study with the XJTLU dynamic database clearly demonstrate\nthe limitations of static databases and the importance of the creation and\nadoption of dynamic databases for future indoor localization research and\nreal-world deployment.\n']",Wi-Fi Based Indoor Localization
136,135,64,135_adversarial_adversary_adversaries_malicious,"['adversarial', 'adversary', 'adversaries', 'malicious', 'attackers', 'attacks', 'attacking', 'defenses', 'threat', 'attack']","['adversarial', 'attack', 'attacks', 'examples', 'robustness', 'perturbations', 'defense', 'word', 'synonym', 'perturbation']","['adversarial', 'adversary', 'threat', 'classifiers', 'embedding', 'nlp', 'examples', 'perturbed', 'robustsentembed', 'bert']","['  Deep neural networks have been proven to be vulnerable to adversarial\nexamples and various methods have been proposed to defend against adversarial\nattacks for natural language processing tasks. However, previous defense\nmethods have limitations in maintaining effective defense while ensuring the\nperformance of the original task. In this paper, we propose a malicious\nperturbation based adversarial training method (MPAT) for building robust deep\nneural networks against textual adversarial attacks. Specifically, we construct\na multi-level malicious example generation strategy to generate adversarial\nexamples with malicious perturbations, which are used instead of original\ninputs for model training. Additionally, we employ a novel training objective\nfunction to ensure achieving the defense goal without compromising the\nperformance on the original task. We conduct comprehensive experiments to\nevaluate our defense method by attacking five victim models on three benchmark\ndatasets. The result demonstrates that our method is more effective against\nmalicious adversarial attacks compared with previous defense methods while\nmaintaining or further improving the performance on the original task.\n', '  Over the past two years, the use of large language models (LLMs) has advanced\nrapidly. While these LLMs offer considerable convenience, they also raise\nsecurity concerns, as LLMs are vulnerable to adversarial attacks by some\nwell-designed textual perturbations. In this paper, we introduce a novel\ndefense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is\ndesigned to enhance the adversarial robustness of LLMs by purifying the\nadversarial textual examples before feeding them into the target LLM. Our\nmethod comprises two main components: a) Agent instruction, which can simulate\na new agent for adversarial defense, altering minimal characters to maintain\nthe original meaning of the sentence while defending against attacks; b)\nDefense guidance, which provides strategies for modifying clean or adversarial\nexamples to ensure effective defense and accurate outputs from the target LLMs.\nRemarkably, the defense agent demonstrates robust defensive capabilities even\nwithout learning from adversarial examples. Additionally, we conduct an\nintriguing adversarial experiment where we develop two agents, one for defense\nand one for attack, and engage them in mutual confrontation. During the\nadversarial interactions, neither agent completely beat the other. Extensive\nexperiments on both open-source and closed-source LLMs demonstrate that our\nmethod effectively defends against adversarial attacks, thereby enhancing\nadversarial robustness.\n', '  Text classification systems have been proven vulnerable to adversarial text\nexamples, modified versions of the original text examples that are often\nunnoticed by human eyes, yet can force text classification models to alter\ntheir classification. Often, research works quantifying the impact of\nadversarial text attacks have been applied only to models trained in English.\nIn this paper, we introduce the first word-level study of adversarial attacks\nin Arabic. Specifically, we use a synonym (word-level) attack using a Masked\nLanguage Modeling (MLM) task with a BERT model in a black-box setting to assess\nthe robustness of the state-of-the-art text classification models to\nadversarial attacks in Arabic. To evaluate the grammatical and semantic\nsimilarities of the newly produced adversarial examples using our synonym\nBERT-based attack, we invite four human evaluators to assess and compare the\nproduced adversarial examples with their original examples. We also study the\ntransferability of these newly produced Arabic adversarial examples to various\nmodels and investigate the effectiveness of defense mechanisms against these\nadversarial examples on the BERT models. We find that fine-tuned BERT models\nwere more susceptible to our synonym attacks than the other Deep Neural\nNetworks (DNN) models like WordCNN and WordLSTM we trained. We also find that\nfine-tuned BERT models were more susceptible to transferred attacks. We,\nlastly, find that fine-tuned BERT models successfully regain at least 2% in\naccuracy after applying adversarial training as an initial defense mechanism.\n']",Adversarial Attacks on Text Classification Models
137,136,64,136_reinforcement_games_optimality_optimal,"['reinforcement', 'games', 'optimality', 'optimal', 'equilibria', 'equilibrium', 'agents', 'game', 'nash', 'subgame']","['games', 'equilibrium', 'equilibria', 'player', 'agents', 'game', 'sum', 'policy', 'agent', 'players']","['reinforcement', 'optimality', 'equilibria', 'agents', 'nash', 'subgame', 'strategic', 'markov', 'mdp', 'macroeconomic']","['  A fundamental shortcoming of the concept of Nash equilibrium is its\ncomputational intractability: approximating Nash equilibria in normal-form\ngames is PPAD-hard. In this paper, inspired by the ideas of smoothed analysis,\nwe introduce a relaxed variant of Nash equilibrium called $\\sigma$-smooth Nash\nequilibrium, for a smoothness parameter $\\sigma$. In a $\\sigma$-smooth Nash\nequilibrium, players only need to achieve utility at least as high as their\nbest deviation to a $\\sigma$-smooth strategy, which is a distribution that does\nnot put too much mass (as parametrized by $\\sigma$) on any fixed action. We\ndistinguish two variants of $\\sigma$-smooth Nash equilibria: strong\n$\\sigma$-smooth Nash equilibria, in which players are required to play\n$\\sigma$-smooth strategies under equilibrium play, and weak $\\sigma$-smooth\nNash equilibria, where there is no such requirement.\n  We show that both weak and strong $\\sigma$-smooth Nash equilibria have\nsuperior computational properties to Nash equilibria: when $\\sigma$ as well as\nan approximation parameter $\\epsilon$ and the number of players are all\nconstants, there is a constant-time randomized algorithm to find a weak\n$\\epsilon$-approximate $\\sigma$-smooth Nash equilibrium in normal-form games.\nIn the same parameter regime, there is a polynomial-time deterministic\nalgorithm to find a strong $\\epsilon$-approximate $\\sigma$-smooth Nash\nequilibrium in a normal-form game. These results stand in contrast to the\noptimal algorithm for computing $\\epsilon$-approximate Nash equilibria, which\ncannot run in faster than quasipolynomial-time. We complement our upper bounds\nby showing that when either $\\sigma$ or $\\epsilon$ is an inverse polynomial,\nfinding a weak $\\epsilon$-approximate $\\sigma$-smooth Nash equilibria becomes\ncomputationally intractable.\n', ""  We introduce Boolean Observation Games, a subclass of multi-player finite\nstrategic games with incomplete information and qualitative objectives. In\nBoolean observation games, each player is associated with a finite set of\npropositional variables of which only it can observe the value, and it controls\nwhether and to whom it can reveal that value. It does not control the given,\nfixed, value of variables. Boolean observation games are a generalization of\nBoolean games, a well-studied subclass of strategic games but with complete\ninformation, and wherein each player controls the value of its variables.\n  In Boolean observation games, player goals describe multi-agent knowledge of\nvariables. As in classical strategic games, players choose their strategies\nsimultaneously and therefore observation games capture aspects of both\nimperfect and incomplete information. They require reasoning about sets of\noutcomes given sets of indistinguishable valuations of variables. An outcome\nrelation between such sets determines what the Nash equilibria are. We present\nvarious outcome relations, including a qualitative variant of ex-post\nequilibrium. We identify conditions under which, given an outcome relation,\nNash equilibria are guaranteed to exist. We also study the complexity of\nchecking for the existence of Nash equilibria and of verifying if a strategy\nprofile is a Nash equilibrium. We further study the subclass of Boolean\nobservation games with `knowing whether' goal formulas, for which the\nsatisfaction does not depend on the value of variables. We show that each such\nBoolean observation game corresponds to a Boolean game and vice versa, by a\ndifferent correspondence, and that both correspondences are precise in terms of\nexistence of Nash equilibria.\n"", '  Reinforcement learning for multi-agent games has attracted lots of attention\nrecently. However, given the challenge of solving Nash equilibria for large\npopulation games, existing works with guaranteed polynomial complexities either\nfocus on variants of zero-sum and potential games, or aim at solving (coarse)\ncorrelated equilibria, or require access to simulators, or rely on certain\nassumptions that are hard to verify. This work proposes MF-OML (Mean-Field\nOccupation-Measure Learning), an online mean-field reinforcement learning\nalgorithm for computing approximate Nash equilibria of large population\nsequential symmetric games. MF-OML is the first fully polynomial multi-agent\nreinforcement learning algorithm for provably solving Nash equilibria (up to\nmean-field approximation gaps that vanish as the number of players $N$ goes to\ninfinity) beyond variants of zero-sum and potential games. When evaluated by\nthe cumulative deviation from Nash equilibria, the algorithm is shown to\nachieve a high probability regret bound of $\\tilde{O}(M^{3/4}+N^{-1/2}M)$ for\ngames with the strong Lasry-Lions monotonicity condition, and a regret bound of\n$\\tilde{O}(M^{11/12}+N^{- 1/6}M)$ for games with only the Lasry-Lions\nmonotonicity condition, where $M$ is the total number of episodes and $N$ is\nthe number of agents of the game. As a byproduct, we also obtain the first\ntractable globally convergent computational algorithm for computing approximate\nNash equilibria of monotone mean-field games.\n']",Game Theory and Multi-Agent Reinforcement Learning
138,137,63,137_circuits_circuitsynth_analogcoder_circuitvae,"['circuits', 'circuitsynth', 'analogcoder', 'circuitvae', 'hardware', 'circuit', 'analog', 'chip', 'transistor', 'engineers']","['circuit', 'analog', 'design', 'circuits', 'chip', 'sizing', 'designs', 'chiplet', 'wirelength', 'automation']","['circuits', 'analogcoder', 'lithography', 'netlist', 'spice', 'graph', 'chiplet', 'steiner', 'dse', 'layouts']","['  Analog circuit design is a significant task in modern chip technology,\nfocusing on the selection of component types, connectivity, and parameters to\nensure proper circuit functionality. Despite advances made by Large Language\nModels (LLMs) in digital circuit design, the complexity and scarcity of data in\nanalog circuitry pose significant challenges. To mitigate these issues, we\nintroduce AnalogCoder, the first training-free LLM agent for designing analog\ncircuits through Python code generation. Firstly, AnalogCoder incorporates a\nfeedback-enhanced flow with tailored domain-specific prompts, enabling the\nautomated and self-correcting design of analog circuits with a high success\nrate. Secondly, it proposes a circuit tool library to archive successful\ndesigns as reusable modular sub-circuits, simplifying composite circuit\ncreation. Thirdly, extensive experiments on a benchmark designed to cover a\nwide range of analog circuit tasks show that AnalogCoder outperforms other\nLLM-based methods. It has successfully designed 20 circuits, 5 more than\nstandard GPT-4o. We believe AnalogCoder can significantly improve the\nlabor-intensive chip design process, enabling non-experts to design analog\ncircuits efficiently.\n', ""  The electronic design automation of analog circuits has been a longstanding\nchallenge in the integrated circuit field due to the huge design space and\ncomplex design trade-offs among circuit specifications. In the past decades,\nintensive research efforts have mostly been paid to automate the transistor\nsizing with a given circuit topology. By recognizing the graph nature of\ncircuits, this paper presents a Circuit Graph Neural Network (CktGNN) that\nsimultaneously automates the circuit topology generation and device sizing\nbased on the encoder-dependent optimization subroutines. Particularly, CktGNN\nencodes circuit graphs using a two-level GNN framework (of nested GNN) where\ncircuits are represented as combinations of subgraphs in a known subgraph\nbasis. In this way, it significantly improves design efficiency by reducing the\nnumber of subgraphs to perform message passing. Nonetheless, another critical\nroadblock to advancing learning-assisted circuit design automation is a lack of\npublic benchmarks to perform canonical assessment and reproducible research. To\ntackle the challenge, we introduce Open Circuit Benchmark (OCB), an\nopen-sourced dataset that contains $10$K distinct operational amplifiers with\ncarefully-extracted circuit specifications. OCB is also equipped with\ncommunicative circuit generation and evaluation capabilities such that it can\nhelp to generalize CktGNN to design various analog circuits by producing\ncorresponding datasets. Experiments on OCB show the extraordinary advantages of\nCktGNN through representation-based optimization frameworks over other recent\npowerful GNN baselines and human experts' manual designs. Our work paves the\nway toward a learning-based open-sourced design automation for analog circuits.\nOur source code is available at \\url{https://github.com/zehao-dong/CktGNN}.\n"", '  Analog and radio-frequency circuit design requires extensive exploration of\nboth circuit topology and parameters to meet specific design criteria like\npower consumption and bandwidth. Designers must review state-of-the-art\ntopology configurations in the literature and sweep various circuit parameters\nwithin each configuration. This design process is highly specialized and\ntime-intensive, particularly as the number of circuit parameters increases and\nthe circuit becomes more complex. Prior research has explored the potential of\nmachine learning to enhance circuit design procedures. However, these studies\nprimarily focus on simple circuits, overlooking the more practical and complex\nanalog and radio-frequency systems. A major obstacle for bearing the power of\nmachine learning in circuit design is the availability of a generic and diverse\ndataset, along with robust metrics, which are essential for thoroughly\nevaluating and improving machine learning algorithms in the analog and\nradio-frequency circuit domain. We present AICircuit, a comprehensive\nmulti-level dataset and benchmark for developing and evaluating ML algorithms\nin analog and radio-frequency circuit design. AICircuit comprises seven\ncommonly used basic circuits and two complex wireless transceiver systems\ncomposed of multiple circuit blocks, encompassing a wide array of design\nscenarios encountered in real-world applications. We extensively evaluate\nvarious ML algorithms on the dataset, revealing the potential of ML algorithms\nin learning the mapping from the design specifications to the desired circuit\nparameters.\n']",Analog Circuit Design and Automation
139,138,63,138_proofnet_formalizations_provers_prover,"['proofnet', 'formalizations', 'provers', 'prover', 'deductive', 'formalization', 'formalized', 'proofs', 'solvers', 'theorems']","['theorem', 'proving', 'theorems', 'proof', 'formal', 'proofs', 'geometry', 'prover', 'mathematics', 'mathematical']","['proofnet', 'formalizations', 'provers', 'deductive', 'axioms', 'solver', 'lemmas', 'premises', 'lean', 'autoformalization']","['  Proof assistants like Lean have revolutionized mathematical proof\nverification, ensuring high accuracy and reliability. Although large language\nmodels (LLMs) show promise in mathematical reasoning, their advancement in\nformal theorem proving is hindered by a lack of training data. To address this\nissue, we introduce an approach to generate extensive Lean 4 proof data derived\nfrom high-school and undergraduate-level mathematical competition problems.\nThis approach involves translating natural language problems into formal\nstatements, filtering out low-quality statements, and generating proofs to\ncreate synthetic data. After fine-tuning the DeepSeekMath 7B model on this\nsynthetic dataset, which comprises 8 million formal statements with proofs, our\nmodel achieved whole-proof generation accuracies of 46.3% with 64 samples and\n52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at\n23.0% with 64 samples and a tree search reinforcement learning method at 41.0%.\nAdditionally, our model successfully proved 5 out of 148 problems in the Lean 4\nFormalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4\nfailed to prove any. These results demonstrate the potential of leveraging\nlarge-scale synthetic data to enhance theorem-proving capabilities in LLMs.\nBoth the synthetic dataset and the model will be made available to facilitate\nfurther research in this promising field.\n', '  Theorem proving is an important challenge for large language models (LLMs),\nas formal proofs can be checked rigorously by proof assistants such as Lean,\nleaving no room for hallucination. Existing LLM-based provers try to prove\ntheorems in a fully autonomous mode without human intervention. In this mode,\nthey struggle with novel and challenging theorems, for which human insights may\nbe critical. In this paper, we explore LLMs as copilots that assist humans in\nproving theorems. We introduce Lean Copilot, a framework for running LLM\ninference in Lean. It enables programmers to build various LLM-based proof\nautomation tools that integrate seamlessly into the workflow of Lean users.\nUsing Lean Copilot, we build tools for suggesting proof steps (tactic\nsuggestion), completing intermediate proof goals (proof search), and selecting\nrelevant premises (premise selection) using LLMs. Users can use our pretrained\nmodels or bring their own ones that run either locally (with or without GPUs)\nor on the cloud. Experimental results demonstrate the effectiveness of our\nmethod in assisting humans and automating theorem proving process compared to\nexisting rule-based proof automation in Lean. We open source all codes under a\npermissive MIT license to facilitate further research.\n', ""  Traditional language model-based theorem proving assumes that by training on\na sufficient amount of formal proof data, a model will learn to prove theorems.\nOur key observation is that a wealth of informal information that is not\npresent in formal proofs can be useful for learning to prove theorems. For\ninstance, humans think through steps of a proof, but this thought process is\nnot visible in the resulting code. We present Lean-STaR, a framework for\ntraining language models to produce informal thoughts prior to each step of a\nproof, thereby boosting the model's theorem-proving capabilities. Lean-STaR\nuses retrospective ground-truth tactics to generate synthetic thoughts for\ntraining the language model. At inference time, the trained model directly\ngenerates the thoughts prior to the prediction of the tactics in each proof\nstep. Building on the self-taught reasoner framework, we then apply expert\niteration to further fine-tune the model on the correct proofs it samples and\nverifies using the Lean solver. Lean-STaR achieves state-of-the-art results on\nthe miniF2F-test benchmark within the Lean theorem proving environment,\nsignificantly outperforming base models ($\\boldsymbol{43.4\\% \\rightarrow\n46.3\\%,}$ Pass@64). We also analyze the impact of the augmented thoughts on\nvarious aspects of the theorem proving process, providing insights into their\neffectiveness.\n""]",Artificial Intelligence in Theorem Proving
140,139,62,139_parsers_treebanks_treebank_parsing,"['parsers', 'treebanks', 'treebank', 'parsing', 'parser', 'parses', 'parse', 'syntactic', 'corpus', 'grammars']","['morphological', 'syntactic', 'dependency', 'parsing', 'treebanks', 'parsers', 'constituency', 'grammar', 'word', 'languages']","['treebanks', 'parser', 'syntactic', 'grammars', 'corpora', 'constituent', 'dependencies', 'annotation', 'bracketing', 'morphosyntactic']","[""  Many studies have shown that human languages tend to optimize for lower\ncomplexity and increased communication efficiency. Syntactic dependency\ndistance, which measures the linear distance between dependent words, is often\nconsidered a key indicator of language processing difficulty and working memory\nload. The current paper looks at diachronic trends in syntactic language change\nin both English and German, using corpora of parliamentary debates from the\nlast c. 160 years. We base our observations on five dependency parsers,\nincluding the widely used Stanford CoreNLP as well as 4 newer alternatives. Our\nanalysis of syntactic language change goes beyond linear dependency distance\nand explores 15 metrics relevant to dependency distance minimization (DDM)\nand/or based on tree graph properties, such as the tree height and degree\nvariance. Even though we have evidence that recent parsers trained on modern\ntreebanks are not heavily affected by data 'noise' such as spelling changes and\nOCR errors in our historic data, we find that results of syntactic language\nchange are sensitive to the parsers involved, which is a caution against using\na single parser for evaluating syntactic language change as done in previous\nwork. We also show that syntactic language change over the time period\ninvestigated is largely similar between English and German for the different\nmetrics explored: only 4% of cases we examine yield opposite conclusions\nregarding upwards and downtrends of syntactic metrics across German and\nEnglish. We also show that changes in syntactic measures seem to be more\nfrequent at the tails of sentence length distributions. To our best knowledge,\nours is the most comprehensive analysis of syntactic language change using\nmodern NLP technology in recent corpora of English and German.\n"", '  Parsing is the process of breaking a sentence into its grammatical components\nand identifying the syntactic structure of the sentence. The syntactically\ncorrect sentence structure is achieved by assigning grammatical labels to its\nconstituents using lexicon and syntactic rules. In linguistics, parser is\nextremely useful due to the number of different applications like name entity\nrecognition, QA systems and information extraction, etc. The two most common\ntechniques used for parsing are phrase structure and dependency Structure.\nBecause Urdu is a low-resource language, there has been little progress in\nbuilding an Urdu parser. A comparison of several parsers revealed that the\ndependency parsing approach is better suited for order-free languages such as\nUrdu. We have made significant progress in parsing Urdu, a South Asian language\nwith a complex morphology. For Urdu dependency parsing, a basic feature model\nconsisting of word location, word head, and dependency relation is employed as\na starting point, followed by more complex feature models. The dependency\ntagset is designed after careful consideration of the complex morphological\nstructure of the Urdu language, word order variation, and lexical ambiguity and\nit contains 22 tags. Our dataset comprises of sentences from news articles, and\nwe tried to include sentences of different complexity (which is quite\nchallenging), to get reliable results. All experiments are performed using\nMaltParser, exploring all 9 algorithms and classifiers. We have achieved a 70\npercent overall best-labeled accuracy (LA), as well as an 84 percent overall\nbest-unlabeled attachment score (UAS) using the Nivreeager algorithm. The\ncomparison of output data with treebank test data that has been manually parsed\nis then used to carry out error assessment and to identify the errors produced\nby the parser.\n', '  Contemporary multilingual dependency parsers can parse a diverse set of\nlanguages, but for Morphologically Rich Languages (MRLs), performance is\nattested to be lower than other languages. The key challenge is that, due to\nhigh morphological complexity and ambiguity of the space-delimited input\ntokens, the linguistic units that act as nodes in the tree are not known in\nadvance. Pre-neural dependency parsers for MRLs subscribed to the joint\nmorpho-syntactic hypothesis, stating that morphological segmentation and\nsyntactic parsing should be solved jointly, rather than as a pipeline where\nsegmentation precedes parsing. However, neural state-of-the-art parsers to date\nuse a strict pipeline. In this paper we introduce a joint neural architecture\nwhere a lattice-based representation preserving all morphological ambiguity of\nthe input is provided to an arc-factored model, which then solves the\nmorphological segmentation and syntactic parsing tasks at once. Our experiments\non Hebrew, a rich and highly ambiguous MRL, demonstrate state-of-the-art\nperformance on parsing, tagging and segmentation of the Hebrew section of UD,\nusing a single model. This proposed architecture is LLM-based and language\nagnostic, providing a solid foundation for MRLs to obtain further performance\nimprovements and bridge the gap with other languages.\n']",Natural Language Parsing and Syntactic Analysis
141,140,62,140_federated_privacy_distributed_collaborative,"['federated', 'privacy', 'distributed', 'collaborative', 'learning', 'shared', 'collaboratively', 'unlearning', 'sharing', 'decentralized']","['federated', 'privacy', 'unlearning', 'clients', 'parties', 'centralized', 'client', 'private', 'collaborative', 'global']","['federated', 'privacy', 'distributed', 'collaborative', 'unlearning', 'decentralized', 'centralized', 'aggregate', 'agencies', 'fedp3']","[""  Vertical Federated Learning (VFL) has emerged as a critical approach in\nmachine learning to address privacy concerns associated with centralized data\nstorage and processing. VFL facilitates collaboration among multiple entities\nwith distinct feature sets on the same user population, enabling the joint\ntraining of predictive models without direct data sharing. A key aspect of VFL\nis the fair and accurate evaluation of each entity's contribution to the\nlearning process. This is crucial for maintaining trust among participating\nentities, ensuring equitable resource sharing, and fostering a sustainable\ncollaboration framework. This paper provides a thorough review of contribution\nevaluation in VFL. We categorize the vast array of contribution evaluation\ntechniques along the VFL lifecycle, granularity of evaluation, privacy\nconsiderations, and core computational methods. We also explore various tasks\nin VFL that involving contribution evaluation and analyze their required\nevaluation properties and relation to the VFL lifecycle phases. Finally, we\npresent a vision for the future challenges of contribution evaluation in VFL.\nBy providing a structured analysis of the current landscape and potential\nadvancements, this paper aims to guide researchers and practitioners in the\ndesign and implementation of more effective, efficient, and privacy-centric VFL\nsolutions. Relevant literature and open-source resources have been compiled and\nare being continuously updated at the GitHub repository:\n\\url{https://github.com/cuiyuebing/VFL_CE}.\n"", '  Federated learning (FL), introduced in 2017, facilitates collaborative\nlearning between non-trusting parties with no need for the parties to\nexplicitly share their data among themselves. This allows training models on\nuser data while respecting privacy regulations such as GDPR and CPRA. However,\nemerging privacy requirements may mandate model owners to be able to\n\\emph{forget} some learned data, e.g., when requested by data owners or law\nenforcement. This has given birth to an active field of research called\n\\emph{machine unlearning}. In the context of FL, many techniques developed for\nunlearning in centralized settings are not trivially applicable! This is due to\nthe unique differences between centralized and distributed learning, in\nparticular, interactivity, stochasticity, heterogeneity, and limited\naccessibility in FL. In response, a recent line of work has focused on\ndeveloping unlearning mechanisms tailored to FL.\n  This SoK paper aims to take a deep look at the \\emph{federated unlearning}\nliterature, with the goal of identifying research trends and challenges in this\nemerging field. By carefully categorizing papers published on FL unlearning\n(since 2020), we aim to pinpoint the unique complexities of federated\nunlearning, highlighting limitations on directly applying centralized\nunlearning methods. We compare existing federated unlearning methods regarding\ninfluence removal and performance recovery, compare their threat models and\nassumptions, and discuss their implications and limitations. For instance, we\nanalyze the experimental setup of FL unlearning studies from various\nperspectives, including data heterogeneity and its simulation, the datasets\nused for demonstration, and evaluation metrics. Our work aims to offer insights\nand suggestions for future research on federated unlearning.\n', ""  Recently, federated learning (FL) has emerged as a promising distributed\nmachine learning (ML) technology, owing to the advancing computational and\nsensing capacities of end-user devices, however with the increasing concerns on\nusers' privacy. As a special architecture in FL, vertical FL (VFL) is capable\nof constructing a hyper ML model by embracing sub-models from different\nclients. These sub-models are trained locally by vertically partitioned data\nwith distinct attributes. Therefore, the design of VFL is fundamentally\ndifferent from that of conventional FL, raising new and unique research issues.\nIn this paper, we aim to discuss key challenges in VFL with effective\nsolutions, and conduct experiments on real-life datasets to shed light on these\nissues. Specifically, we first propose a general framework on VFL, and\nhighlight the key differences between VFL and conventional FL. Then, we discuss\nresearch challenges rooted in VFL systems under four aspects, i.e., security\nand privacy risks, expensive computation and communication costs, possible\nstructural damage caused by model splitting, and system heterogeneity.\nAfterwards, we develop solutions to addressing the aforementioned challenges,\nand conduct extensive experiments to showcase the effectiveness of our proposed\nsolutions.\n""]",Federated Learning and Privacy
142,141,62,141_contexts_attention_embedding_longembed,"['contexts', 'attention', 'embedding', 'longembed', 'lengthy', 'context', 'positional', 'position', 'memory', 'positions']","['window', 'length', 'context', 'position', 'long', 'positional', 'extrapolation', 'attention', 'sequences', 'tokens']","['contexts', 'attention', 'embedding', 'longembed', 'positions', 'texts', 'encodings', 'chunk', 'extension', 'llms']","['  Position embedding is a core component of current Large Language Models\n(LLMs). Rotary position embedding (RoPE), a technique that encodes the position\ninformation with a rotation matrix, has been the de facto choice for position\nembedding in many LLMs, such as the Llama series. RoPE has been further\nutilized to extend long context capability, which is roughly based on adjusting\nthe \\textit{base} parameter of RoPE to mitigate out-of-distribution (OOD)\nproblems in position embedding. However, in this paper, we find that LLMs may\nobtain a superficial long-context ability based on the OOD theory. We revisit\nthe role of RoPE in LLMs and propose a novel property of long-term decay, we\nderive that the \\textit{base of RoPE bounds context length}: there is an\nabsolute lower bound for the base value to obtain certain context length\ncapability. Our work reveals the relationship between context length and RoPE\nbase both theoretically and empirically, which may shed light on future long\ncontext training.\n', ""  Large Language Models (LLMs) are known to have limited extrapolation ability\nbeyond their pre-trained context window, constraining their application in\ndownstream tasks with lengthy inputs. Recent studies have sought to extend\nLLMs' context window by modifying rotary position embedding (RoPE), a popular\nposition encoding method adopted by well-known LLMs such as LLaMA, PaLM, and\nGPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are\nresource-intensive and lack comparative experiments to assess their\napplicability. In this work, we identify the inherent need for LLMs' attention\nentropy (i.e. the information entropy of attention scores) to maintain\nstability and introduce a novel extension to RoPE which combines adjusting\nRoPE's base frequency and scaling the attention logits to help LLMs efficiently\nadapt to a larger context window. We validate the superiority of our method in\nboth fine-tuning performance and robustness across different context window\nsizes on various context-demanding tasks. Notably, our method extends the\ncontext window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6\ntraining steps, showcasing extraordinary efficiency. Finally, we also explore\nhow data compositions and training curricula affect context window extension\nfor specific downstream tasks, suggesting fine-tuning LLMs with lengthy\nconversations as a good starting point. We release our code and SFT data at\nhttps://github.com/GAIR-NLP/Entropy-ABF.\n"", '  Large Language Models (LLMs) are trained with a pre-defined context length,\nrestricting their use in scenarios requiring long inputs. Previous efforts for\nadapting LLMs to a longer length usually requires fine-tuning with this target\nlength (Full-length fine-tuning), suffering intensive training cost. To\ndecouple train length from target length for efficient context window\nextension, we propose Positional Skip-wisE (PoSE) training that smartly\nsimulates long inputs using a fixed context window. This is achieved by first\ndividing the original context window into several chunks, then designing\ndistinct skipping bias terms to manipulate the position indices of each chunk.\nThese bias terms and the lengths of each chunk are altered for every training\nexample, allowing the model to adapt to all positions within target length.\nExperimental results show that PoSE greatly reduces memory and time overhead\ncompared with Full-length fine-tuning, with minimal impact on performance.\nLeveraging this advantage, we have successfully extended the LLaMA model to\n128k tokens using a 2k training context window. Furthermore, we empirically\nconfirm that PoSE is compatible with all RoPE-based LLMs and position\ninterpolation strategies. Notably, our method can potentially support infinite\nlength, limited only by memory usage in inference. With ongoing progress for\nefficient inference, we believe PoSE can further scale the context window\nbeyond 128k.\n']",Improving Context Length in Large Language Models
143,142,61,142_distributed_compression_optimization_sgd,"['distributed', 'compression', 'optimization', 'sgd', 'minimizer', 'compressed', 'gradient', 'bottleneck', 'gradients', 'optimal']","['communication', 'compression', 'convergence', 'convex', 'gradient', 'consensus', 'local', 'stochastic', 'compressors', 'descent']","['distributed', 'compression', 'sgd', 'bottleneck', 'minimax', 'quantization', 'communications', 'algorithms', 'convex', 'unbiased']","['  Recently introduced distributed zeroth-order optimization (ZOO) algorithms\nhave shown their utility in distributed reinforcement learning (RL).\nUnfortunately, in the gradient estimation process, almost all of them require\nrandom samples with the same dimension as the global variable and/or require\nevaluation of the global cost function, which may induce high estimation\nvariance for large-scale networks. In this paper, we propose a novel\ndistributed zeroth-order algorithm by leveraging the network structure inherent\nin the optimization objective, which allows each agent to estimate its local\ngradient by local cost evaluation independently, without use of any consensus\nprotocol. The proposed algorithm exhibits an asynchronous update scheme, and is\ndesigned for stochastic non-convex optimization with a possibly non-convex\nfeasible domain based on the block coordinate descent method. The algorithm is\nlater employed as a distributed model-free RL algorithm for distributed linear\nquadratic regulator design, where a learning graph is designed to describe the\nrequired interaction relationship among agents in distributed learning. We\nprovide an empirical validation of the proposed algorithm to benchmark its\nperformance on convergence rate and variance against a centralized ZOO\nalgorithm.\n', '  We propose a novel algorithm for distributed stochastic gradient descent\n(SGD) with compressed gradient communication in the parameter-server framework.\nOur gradient compression technique, named flattened one-bit stochastic gradient\ndescent (FO-SGD), relies on two simple algorithmic ideas: (i) a one-bit\nquantization procedure leveraging the technique of dithering, and (ii) a\nrandomized fast Walsh-Hadamard transform to flatten the stochastic gradient\nbefore quantization. As a result, the approximation of the true gradient in\nthis scheme is biased, but it prevents commonly encountered algorithmic\nproblems, such as exploding variance in the one-bit compression regime,\ndeterioration of performance in the case of sparse gradients, and restrictive\nassumptions on the distribution of the stochastic gradients. In fact, we show\nSGD-like convergence guarantees under mild conditions. The compression\ntechnique can be used in both directions of worker-server communication,\ntherefore admitting distributed optimization with full communication\ncompression.\n', '  Communication compression is a common technique in distributed optimization\nthat can alleviate communication overhead by transmitting compressed gradients\nand model parameters. However, compression can introduce information\ndistortion, which slows down convergence and incurs more communication rounds\nto achieve desired solutions. Given the trade-off between lower per-round\ncommunication costs and additional rounds of communication, it is unclear\nwhether communication compression reduces the total communication cost.\n  This paper explores the conditions under which unbiased compression, a widely\nused form of compression, can reduce the total communication cost, as well as\nthe extent to which it can do so. To this end, we present the first theoretical\nformulation for characterizing the total communication cost in distributed\noptimization with communication compression. We demonstrate that unbiased\ncompression alone does not necessarily save the total communication cost, but\nthis outcome can be achieved if the compressors used by all workers are further\nassumed independent. We establish lower bounds on the communication rounds\nrequired by algorithms using independent unbiased compressors to minimize\nsmooth convex functions and show that these lower bounds are tight by refining\nthe analysis for ADIANA. Our results reveal that using independent unbiased\ncompression can reduce the total communication cost by a factor of up to\n$\\Theta(\\sqrt{\\min\\{n, \\kappa\\}})$ when all local smoothness constants are\nconstrained by a common upper bound, where $n$ is the number of workers and\n$\\kappa$ is the condition number of the functions being minimized. These\ntheoretical findings are supported by experimental results.\n']",Distributed Optimization with Gradient Compression
144,143,61,143_fedsecurity_adversary_feddefender_datadefense,"['fedsecurity', 'adversary', 'feddefender', 'datadefense', 'malicious', 'attacks', 'federated', 'security', 'privacy', 'aggregators']","['poisoning', 'clients', 'attacks', 'malicious', 'backdoor', 'federated', 'attack', 'defense', 'updates', 'defenses']","['fedsecurity', 'adversary', 'feddefender', 'datadefense', 'federated', 'aggregators', 'backdoor', 'flguard', 'byzantine', 'hijacking']","['  Federated learning (FL) enables multiple clients to collaboratively train\nmachine learning models without revealing their private training data. In\nconventional FL, the system follows the server-assisted architecture\n(server-assisted FL), where the training process is coordinated by a central\nserver. However, the server-assisted FL framework suffers from poor scalability\ndue to a communication bottleneck at the server, and trust dependency issues.\nTo address challenges, decentralized federated learning (DFL) architecture has\nbeen proposed to allow clients to train models collaboratively in a serverless\nand peer-to-peer manner. However, due to its fully decentralized nature, DFL is\nhighly vulnerable to poisoning attacks, where malicious clients could\nmanipulate the system by sending carefully-crafted local models to their\nneighboring clients. To date, only a limited number of Byzantine-robust DFL\nmethods have been proposed, most of which are either communication-inefficient\nor remain vulnerable to advanced poisoning attacks. In this paper, we propose a\nnew algorithm called BALANCE (Byzantine-robust averaging through local\nsimilarity in decentralization) to defend against poisoning attacks in DFL. In\nBALANCE, each client leverages its own local model as a similarity reference to\ndetermine if the received model is malicious or benign. We establish the\ntheoretical convergence guarantee for BALANCE under poisoning attacks in both\nstrongly convex and non-convex settings. Furthermore, the convergence rate of\nBALANCE under poisoning attacks matches those of the state-of-the-art\ncounterparts in Byzantine-free settings. Extensive experiments also demonstrate\nthat BALANCE outperforms existing DFL methods and effectively defends against\npoisoning attacks.\n', '  Federated Learning (FL) is an emerging distributed machine learning paradigm\nthat allows multiple clients to collaboratively train a global model without\nsharing private local data. However, FL systems are vulnerable to attacks from\nmalicious clients, who can degrade the global model performance through data\npoisoning and model poisoning. Existing defense methods typically focus on a\nsingle type of attack, such as Byzantine attacks or backdoor attacks, and are\noften ineffective against potential data poisoning attacks like label flipping\nand label shuffling. Additionally, these methods often lack accuracy and\nrobustness in detecting and handling malicious updates. To address these\nissues, we propose a novel method based on model confidence scores, which\nevaluates the uncertainty of client model updates to detect and defend against\nmalicious clients. Our approach is comprehensively effective for both model\npoisoning and data poisoning attacks and is capable of accurately identifying\nand mitigating potential malicious updates from being aggregated. Experimental\nresults demonstrate that our method significantly improves the robustness of FL\nsystems against various types of attacks, also achieving higher model accuracy\nand stability across various scenarios.\n', ""  Federated Learning (FL) is a decentralized machine learning method that\nenables participants to collaboratively train a model without sharing their\nprivate data. Despite its privacy and scalability benefits, FL is susceptible\nto backdoor attacks, where adversaries poison the local training data of a\nsubset of clients using a backdoor trigger, aiming to make the aggregated model\nproduce malicious results when the same backdoor condition is met by an\ninference-time input. Existing backdoor attacks in FL suffer from common\ndeficiencies: fixed trigger patterns and reliance on the assistance of model\npoisoning. State-of-the-art defenses based on Byzantine-robust aggregation\nexhibit a good defense performance on these attacks because of the significant\ndivergence between malicious and benign model updates. To effectively conceal\nmalicious model updates among benign ones, we propose DPOT, a backdoor attack\nstrategy in FL that dynamically constructs backdoor objectives by optimizing a\nbackdoor trigger, making backdoor data have minimal effect on model updates. We\nprovide theoretical justifications for DPOT's attacking principle and display\nexperimental results showing that DPOT, via only a data-poisoning attack,\neffectively undermines state-of-the-art defenses and outperforms existing\nbackdoor attack techniques on various datasets.\n""]",Federated Learning Security
145,144,61,144_assessment_educational_education_academic,"['assessment', 'educational', 'education', 'academic', 'ai', 'academia', 'pedagogical', 'students', 'educators', 'intelligence']","['education', 'students', 'educators', 'educational', 'teachers', 'universities', 'ethical', 'academic', 'teaching', 'intelligence']","['assessment', 'educators', 'intelligence', 'curriculum', 'universities', 'aied', 'literacy', 'generative', 'tools', 'discourse']","[""  The rise of Artificial Intelligence (AI) and Generative Artificial\nIntelligence (GenAI) in higher education necessitates assessment reform. This\nstudy addresses a critical gap by exploring student and academic staff\nexperiences with AI and GenAI tools, focusing on their familiarity and comfort\nwith current and potential future applications in learning and assessment. An\nonline survey collected data from 35 academic staff and 282 students across two\nuniversities in Vietnam and one in Singapore, examining GenAI familiarity,\nperceptions of its use in assessment marking and feedback, knowledge checking\nand participation, and experiences of GenAI text detection.\n  Descriptive statistics and reflexive thematic analysis revealed a generally\nlow familiarity with GenAI among both groups. GenAI feedback was viewed\nnegatively; however, it was viewed more positively when combined with\ninstructor feedback. Academic staff were more accepting of GenAI text detection\ntools and grade adjustments based on detection results compared to students.\nQualitative analysis identified three themes: unclear understanding of text\ndetection tools, variability in experiences with GenAI detectors, and mixed\nfeelings about GenAI's future impact on educational assessment. These findings\nhave major implications regarding the development of policies and practices for\nGenAI-enabled assessment and feedback in higher education.\n"", ""  The advancements in Generative Artificial Intelligence (GenAI) provide\nopportunities to enrich educational experiences, but also raise concerns about\nacademic integrity. Many educators have expressed anxiety and hesitation in\nintegrating GenAI in their teaching practices, and are in needs of\nrecommendations and guidance from their institutions that can support them to\nincorporate GenAI in their classrooms effectively. In order to respond to\nhigher educators' needs, this study aims to explore how universities and\neducators respond and adapt to the development of GenAI in their academic\ncontexts by analyzing academic policies and guidelines established by\ntop-ranked U.S. universities regarding the use of GenAI, especially ChatGPT.\nData sources include academic policies, statements, guidelines, and relevant\nresources provided by the top 100 universities in the U.S. Results show that\nthe majority of these universities adopt an open but cautious approach towards\nGenAI. Primary concerns lie in ethical usage, accuracy, and data privacy. Most\nuniversities actively respond and provide diverse types of resources, such as\nsyllabus templates, workshops, shared articles, and one-on-one consultations\nfocusing on a range of topics: general technical introduction, ethical\nconcerns, pedagogical applications, preventive strategies, data privacy,\nlimitations, and detective tools. The findings provide four practical\npedagogical implications for educators in teaching practices: accept its\npresence, align its use with learning objectives, evolve curriculum to prevent\nmisuse, and adopt multifaceted evaluation strategies rather than relying on AI\ndetectors. Two recommendations are suggested for educators in policy making:\nestablish discipline-specific policies and guidelines, and manage sensitive\ninformation carefully.\n"", '  Recent developments in Generative Artificial Intelligence (GenAI) have\ncreated a paradigm shift in multiple areas of society, and the use of these\ntechnologies is likely to become a defining feature of education in coming\ndecades. GenAI offers transformative pedagogical opportunities, while\nsimultaneously posing ethical and academic challenges. Against this backdrop,\nwe outline a practical, simple, and sufficiently comprehensive tool to allow\nfor the integration of GenAI tools into educational assessment: the AI\nAssessment Scale (AIAS).\n  The AIAS empowers educators to select the appropriate level of GenAI usage in\nassessments based on the learning outcomes they seek to address. The AIAS\noffers greater clarity and transparency for students and educators, provides a\nfair and equitable policy tool for institutions to work with, and offers a\nnuanced approach which embraces the opportunities of GenAI while recognising\nthat there are instances where such tools may not be pedagogically appropriate\nor necessary.\n  By adopting a practical, flexible approach that can be implemented quickly,\nthe AIAS can form a much-needed starting point to address the current\nuncertainty and anxiety regarding GenAI in education. As a secondary objective,\nwe engage with the current literature and advocate for a refocused discourse on\nGenAI tools in education, one which foregrounds how technologies can help\nsupport and enhance teaching and learning, which contrasts with the current\nfocus on GenAI as a facilitator of academic misconduct.\n']","""Generative AI in Education and Assessment"""
146,145,60,145_privacy_privacyrestore_anonymization_anonymized,"['privacy', 'privacyrestore', 'anonymization', 'anonymized', 'anonymisation', 'security', 'disclosure', 'private', 'obfuscated', 'incognitext']","['privacy', 'protection', 'anonymization', 'sensitive', 'private', 'personal', 'policies', 'utility', 'anonymisation', 'privatization']","['privacyrestore', 'anonymization', 'private', 'obfuscated', 'incognitext', 'identifiable', 'documents', 'gdpr', 'compliance', 'corpus']","[""  The emergence of large language models (LLMs), and their increased use in\nuser-facing systems, has led to substantial privacy concerns. To date, research\non these privacy concerns has been model-centered: exploring how LLMs lead to\nprivacy risks like memorization, or can be used to infer personal\ncharacteristics about people from their content. We argue that there is a need\nfor more research focusing on the human aspect of these privacy issues: e.g.,\nresearch on how design paradigms for LLMs affect users' disclosure behaviors,\nusers' mental models and preferences for privacy controls, and the design of\ntools, systems, and artifacts that empower end-users to reclaim ownership over\ntheir personal data. To build usable, efficient, and privacy-friendly systems\npowered by these models with imperfect privacy properties, our goal is to\ninitiate discussions to outline an agenda for conducting human-centered\nresearch on privacy issues in LLM-powered systems. This Special Interest Group\n(SIG) aims to bring together researchers with backgrounds in usable security\nand privacy, human-AI collaboration, NLP, or any other related domains to share\ntheir perspectives and experiences on this problem, to help our community\nestablish a collective understanding of the challenges, research opportunities,\nresearch methods, and strategies to collaborate with researchers outside of\nHCI.\n"", '  Large language models (LLMs), renowned for their impressive capabilities in\nvarious tasks, have significantly advanced artificial intelligence. Yet, these\nadvancements have raised growing concerns about privacy and security\nimplications. To address these issues and explain the risks inherent in these\nmodels, we have devised a three-tiered progressive framework tailored for\nevaluating privacy in language systems. This framework consists of\nprogressively complex and in-depth privacy test tasks at each tier. Our primary\nobjective is to comprehensively evaluate the sensitivity of large language\nmodels to private information, examining how effectively they discern, manage,\nand safeguard sensitive data in diverse scenarios. This systematic evaluation\nhelps us understand the degree to which these models comply with privacy\nprotection guidelines and the effectiveness of their inherent safeguards\nagainst privacy breaches. Our observations indicate that existing Chinese large\nlanguage models universally show privacy protection shortcomings. It seems that\nat the moment this widespread issue is unavoidable and may pose corresponding\nprivacy risks in applications based on these models.\n', ""  Large language models (LLMs) have emerged as powerful tools for tackling\ncomplex tasks across diverse domains, but they also raise privacy concerns when\nfine-tuned on sensitive data due to potential memorization. While differential\nprivacy (DP) offers a promising solution by ensuring models are 'almost\nindistinguishable' with or without any particular privacy unit, current\nevaluations on LLMs mostly treat each example (text record) as the privacy\nunit. This leads to uneven user privacy guarantees when contributions per user\nvary. We therefore study user-level DP motivated by applications where it\nnecessary to ensure uniform privacy protection across users. We present a\nsystematic evaluation of user-level DP for LLM fine-tuning on natural language\ngeneration tasks. Focusing on two mechanisms for achieving user-level DP\nguarantees, Group Privacy and User-wise DP-SGD, we investigate design choices\nlike data selection strategies and parameter tuning for the best\nprivacy-utility tradeoff.\n""]","""Large Language Models and Privacy Concerns"""
147,146,60,146_memory_attention_cachegen_caches,"['memory', 'attention', 'cachegen', 'caches', 'cache', 'caching', 'decoding', 'decoder', 'compression', 'kv']","['cache', 'attention', 'memory', 'eviction', 'heads', 'tokens', 'key', 'value', 'inference', 'head']","['attention', 'caches', 'decoder', 'kv', 'batch', 'throughput', 'longbench', 'hydragen', '1m', 'budget']","['  In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusin on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques achieving up to a 20.5 absolute accuracy improvement on\nTREC.\n', '  The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy.\n', '  How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.\n']",Optimizing Cache Memory for Large Language Models
148,147,59,147_anonymization_anonymized_spoofing_adversarial,"['anonymization', 'anonymized', 'spoofing', 'adversarial', 'hiddenspeaker', 'voice', 'privacy', 'spoofed', 'spoof', 'utterances']","['audio', 'speaker', 'anonymization', 'deepfake', 'speech', 'fake', 'spoofing', 'voice', 'spoofed', 'spoof']","['anonymization', 'spoofing', 'adversarial', 'hiddenspeaker', 'voice', 'audiomarkbench', 'securespectra', 'masking', 'watermarking', 'identity']","['  It is now well-known that automatic speaker verification (ASV) systems can be\nspoofed using various types of adversaries. The usual approach to counteract\nASV systems against such attacks is to develop a separate spoofing\ncountermeasure (CM) module to classify speech input either as a bonafide, or a\nspoofed utterance. Nevertheless, such a design requires additional computation\nand utilization efforts at the authentication stage. An alternative strategy\ninvolves a single monolithic ASV system designed to handle both zero-effort\nimposter (non-targets) and spoofing attacks. Such spoof-aware ASV systems have\nthe potential to provide stronger protections and more economic computations.\nTo this end, we propose to generalize the standalone ASV (G-SASV) against\nspoofing attacks, where we leverage limited training data from CM to enhance a\nsimple backend in the embedding space, without the involvement of a separate CM\nmodule during the test (authentication) phase. We propose a novel yet simple\nbackend classifier based on deep neural networks and conduct the study via\ndomain adaptation and multi-task integration of spoof embeddings at the\ntraining stage. Experiments are conducted on the ASVspoof 2019 logical access\ndataset, where we improve the performance of statistical ASV backends on the\njoint (bonafide and spoofed) and spoofed conditions by a maximum of 36.2% and\n49.8% in terms of equal error rates, respectively.\n', ""  The growing use of voice user interfaces has led to a surge in the collection\nand storage of speech data. While data collection allows for the development of\nefficient tools powering most speech services, it also poses serious privacy\nissues for users as centralized storage makes private personal speech data\nvulnerable to cyber threats. With the increasing use of voice-based digital\nassistants like Amazon's Alexa, Google's Home, and Apple's Siri, and with the\nincreasing ease with which personal speech data can be collected, the risk of\nmalicious use of voice-cloning and speaker/gender/pathological/etc. recognition\nhas increased.\n  This thesis proposes solutions for anonymizing speech and evaluating the\ndegree of the anonymization. In this work, anonymization refers to making\npersonal speech data unlinkable to an identity while maintaining the usefulness\n(utility) of the speech signal (e.g., access to linguistic content). We start\nby identifying several challenges that evaluation protocols need to consider to\nevaluate the degree of privacy protection properly. We clarify how\nanonymization systems must be configured for evaluation purposes and highlight\nthat many practical deployment configurations do not permit privacy evaluation.\nFurthermore, we study and examine the most common voice conversion-based\nanonymization system and identify its weak points before suggesting new methods\nto overcome some limitations. We isolate all components of the anonymization\nsystem to evaluate the degree of speaker PPI associated with each of them.\nThen, we propose several transformation methods for each component to reduce as\nmuch as possible speaker PPI while maintaining utility. We promote\nanonymization algorithms based on quantization-based transformation as an\nalternative to the most-used and well-known noise-based approach. Finally, we\nendeavor a new attack method to invert anonymization.\n"", '  Privacy-preserving voice protection approaches primarily suppress\nprivacy-related information derived from paralinguistic attributes while\npreserving the linguistic content. Existing solutions focus on single-speaker\nscenarios. However, they lack practicality for real-world applications, i.e.,\nmulti-speaker scenarios. In this paper, we present an initial attempt to\nprovide a multi-speaker anonymization benchmark by defining the task and\nevaluation protocol, proposing benchmarking solutions, and discussing the\nprivacy leakage of overlapping conversations. Specifically, ideal multi-speaker\nanonymization should preserve the number of speakers and the turn-taking\nstructure of the conversation, ensuring accurate context conveyance while\nmaintaining privacy. To achieve that, a cascaded system uses speaker\ndiarization to aggregate the speech of each speaker and speaker anonymization\nto conceal speaker privacy and preserve speech content. Additionally, we\npropose two conversation-level speaker vector anonymization methods to improve\nthe utility further. Both methods aim to make the original and corresponding\npseudo-speaker identities of each speaker unlinkable while preserving or even\nimproving the distinguishability among pseudo-speakers in a conversation. The\nfirst method minimizes the differential similarity across speaker pairs in the\noriginal and anonymized conversations to maintain original speaker\nrelationships in the anonymized version. The other method minimizes the\naggregated similarity across anonymized speakers to achieve better\ndifferentiation between speakers. Experiments conducted on both non-overlap\nsimulated and real-world datasets demonstrate the effectiveness of the\nmulti-speaker anonymization system with the proposed speaker anonymizers.\nAdditionally, we analyzed overlapping speech regarding privacy leakage and\nprovide potential solutions.\n']",Speech Anonymization and Anti-Spoofing Techniques
149,148,59,148_events_event_annotations_annotation,"['events', 'event', 'annotations', 'annotation', 'corpus', 'semantic', 'entity', 'extracting', 'triggers', 'coreference']","['event', 'extraction', 'events', 'coreference', 'document', 'argument', 'arguments', 'mentions', 'relations', 'triggers']","['events', 'annotations', 'corpus', 'entity', 'triggers', 'coreference', 'scievents', 'extraction', 'information', 'ontology']","['  Events describe the state changes of entities. In a document, multiple events\nare connected by various relations (e.g., Coreference, Temporal, Causal, and\nSubevent). Therefore, obtaining the connections between events through\nEvent-Event Relation Extraction (ERE) is critical to understand natural\nlanguage. There are two main problems in the current ERE works: a. Only\nembeddings of the event triggers are used for event feature representation,\nignoring event arguments (e.g., time, place, person, etc.) and their structure\nwithin the event. b. The interconnection between relations (e.g., temporal and\ncausal relations usually interact with each other ) is ignored. To solve the\nabove problems, this paper proposes a jointly multiple ERE framework called\nGraphERE based on Graph-enhanced Event Embeddings. First, we enrich the event\nembeddings with event argument and structure features by using static AMR\ngraphs and IE graphs; Then, to jointly extract multiple event relations, we use\nNode Transformer and construct Task-specific Dynamic Event Graphs for each type\nof relation. Finally, we used a multi-task learning strategy to train the whole\nframework. Experimental results on the latest MAVEN-ERE dataset validate that\nGraphERE significantly outperforms existing methods. Further analyses indicate\nthe effectiveness of the graph-enhanced event embeddings and the joint\nextraction strategy.\n', '  Existing approaches on zero-shot event detection usually train models on\ndatasets annotated with known event types, and prompt them with unseen event\ndefinitions. These approaches yield sporadic successes, yet generally fall\nshort of expectations. In this work, we aim to improve zero-shot event\ndetection by training models to better follow event definitions. We hypothesize\nthat a diverse set of event types and definitions are the key for models to\nlearn to follow event definitions while existing event extraction datasets\nfocus on annotating many high-quality examples for a few event types. To verify\nour hypothesis, we construct an automatically generated Diverse Event\nDefinition (DivED) dataset and conduct comparative studies. Our experiments\nreveal that a large number of event types (200) and diverse event definitions\ncan significantly boost event extraction performance; on the other hand, the\nperformance does not scale with over ten examples per event type. Beyond\nscaling, we incorporate event ontology information and hard-negative samples\nduring training, further boosting the performance. Based on these findings, we\nfine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that\nsurpasses SOTA large language models like GPT-3.5 across three open benchmarks\non zero-shot event detection.\n', '  Event sequence models have been found to be highly effective in the analysis\nand prediction of events. Building such models requires availability of\nabundant high-quality event sequence data. In certain applications, however,\nclean structured event sequences are not available, and automated sequence\nextraction results in data that is too noisy and incomplete. In this work, we\nexplore the use of Large Language Models (LLMs) to generate event sequences\nthat can effectively be used for probabilistic event model construction. This\ncan be viewed as a mechanism of distilling event sequence knowledge from LLMs.\nOur approach relies on a Knowledge Graph (KG) of event concepts with partial\ncausal relations to guide the generative language model for causal event\nsequence generation. We show that our approach can generate high-quality event\nsequences, filling a knowledge gap in the input KG. Furthermore, we explore how\nthe generated sequences can be leveraged to discover useful and more complex\nstructured knowledge from pattern mining and probabilistic event models. We\nrelease our sequence generation code and evaluation framework, as well as\ncorpus of event sequence data.\n']",Event Extraction and Analysis
150,149,59,149_computability_neural_networks_verifiability,"['computability', 'neural', 'networks', 'verifiability', 'dnns', 'deepinfer', 'robustness', 'dnn', 'deep', 'neurons']","['verification', 'verifying', 'formal', 'inputs', 'safety', 'neural', 'robustness', 'networks', 'deep', 'neurons']","['computability', 'verifiability', 'dnns', 'robustness', 'neurons', 'checker', 'nns', 'veristable', 'faults', 'probabilistic']","['  Formal verification of neural networks is essential before their deployment\nin safety-critical applications. However, existing methods for formally\nverifying neural networks are not yet scalable enough to handle practical\nproblems involving a large number of neurons. We address this challenge by\nintroducing a fully automatic and sound reduction of neural networks using\nreachability analysis. The soundness ensures that the verification of the\nreduced network entails the verification of the original network. To the best\nof our knowledge, we present the first sound reduction approach that is\napplicable to neural networks with any type of element-wise activation\nfunction, such as ReLU, sigmoid, and tanh. The network reduction is computed on\nthe fly while simultaneously verifying the original network and its\nspecifications. All parameters are automatically tuned to minimize the network\nsize without compromising verifiability. We further show the applicability of\nour approach to convolutional neural networks by explicitly exploiting similar\nneighboring pixels. Our evaluation shows that our approach can reduce the\nnumber of neurons to a fraction of the original number of neurons with minor\nouter-approximation and thus reduce the verification time to a similar degree.\n', '  Deep Neural Networks (DNN) have emerged as an effective approach to tackling\nreal-world problems. However, like human-written software, DNNs are susceptible\nto bugs and attacks. This has generated significant interests in developing\neffective and scalable DNN verification techniques and tools. In this paper, we\npresent VeriStable, a novel extension of recently proposed DPLL-based\nconstraint DNN verification approach. VeriStable leverages the insight that\nwhile neuron behavior may be non-linear across the entire DNN input space, at\nintermediate states computed during verification many neurons may be\nconstrained to have linear behavior - these neurons are stable. Efficiently\ndetecting stable neurons reduces combinatorial complexity without compromising\nthe precision of abstractions. Moreover, the structure of clauses arising in\nDNN verification problems shares important characteristics with industrial SAT\nbenchmarks. We adapt and incorporate multi-threading and restart optimizations\ntargeting those characteristics to further optimize DPLL-based DNN\nverification. We evaluate the effectiveness of VeriStable across a range of\nchallenging benchmarks including fully-connected feedforward networks (FNNs),\nconvolutional neural networks (CNNs) and residual networks (ResNets) applied to\nthe standard MNIST and CIFAR datasets. Preliminary results show that VeriStable\nis competitive and outperforms state-of-the-art DNN verification tools,\nincluding $\\alpha$-$\\beta$-CROWN and MN-BaB, the first and second performers of\nthe VNN-COMP, respectively.\n', '  Deep neural networks (DNNs) play a crucial role in the field of machine\nlearning, demonstrating state-of-the-art performance across various application\ndomains. However, despite their success, DNN-based models may occasionally\nexhibit challenges with generalization, i.e., may fail to handle inputs that\nwere not encountered during training. This limitation is a significant\nchallenge when it comes to deploying deep learning for safety-critical tasks,\nas well as in real-world settings characterized by substantial variability. We\nintroduce a novel approach for harnessing DNN verification technology to\nidentify DNN-driven decision rules that exhibit robust generalization to\npreviously unencountered input domains. Our method assesses generalization\nwithin an input domain by measuring the level of agreement between\nindependently trained deep neural networks for inputs in this domain. We also\nefficiently realize our approach by using off-the-shelf DNN verification\nengines, and extensively evaluate it on both supervised and unsupervised DNN\nbenchmarks, including a deep reinforcement learning (DRL) system for Internet\ncongestion control -- demonstrating the applicability of our approach for\nreal-world settings. Moreover, our research introduces a fresh objective for\nformal verification, offering the prospect of mitigating the challenges linked\nto deploying DNN-driven systems in real-world scenarios.\n']",Neural Network Verification
151,150,58,150_transport_regularization_optimal_transportation,"['transport', 'regularization', 'optimal', 'transportation', 'optimization', 'regularized', 'distributions', 'gradient', 'flow', 'divergence']","['transport', 'optimal', 'entropic', 'map', 'flows', 'maps', 'distributions', 'gradient', 'densities', 'solver']","['transport', 'regularization', 'variational', 'divergences', 'densities', 'sinkhorn', 'geonet', 'wasserstein', 'estimators', 'formulations']","['  Optimal transport (OT) is attracting increasing attention in machine\nlearning. It aims to transport a source distribution to a target one at minimal\ncost. In its vanilla form, the source and target distributions are\npredetermined, which contracts to the real-world case involving undetermined\ntargets. In this paper, we propose Doubly Bounded Optimal Transport (DB-OT),\nwhich assumes that the target distribution is restricted within two boundaries\ninstead of a fixed one, thus giving more freedom for the transport to find\nsolutions. Based on the entropic regularization of DB-OT, three scaling-based\nalgorithms are devised for calculating the optimal solution. We also show that\nour DB-OT is helpful for barycenter-based clustering, which can avoid the\nexcessive concentration of samples in a single cluster. Then we further develop\nDB-OT techniques for long-tailed classification which is an emerging and open\nproblem. We first propose a connection between OT and classification, that is,\nin the classification task, training involves optimizing the Inverse OT to\nlearn the representations, while testing involves optimizing the OT for\npredictions. With this OT perspective, we first apply DB-OT to improve the\nloss, and the Balanced Softmax is shown as a special case. Then we apply DB-OT\nfor inference in the testing process. Even with vanilla Softmax trained\nfeatures, our extensive experimental results show that our method can achieve\ngood results with our improved inference scheme in the testing stage.\n', '  Entropic optimal transport (OT) and the Sinkhorn algorithm have made it\npractical for machine learning practitioners to perform the fundamental task of\ncalculating transport distance between statistical distributions. In this work,\nwe focus on a general class of OT problems under a combination of equality and\ninequality constraints. We derive the corresponding entropy regularization\nformulation and introduce a Sinkhorn-type algorithm for such constrained OT\nproblems supported by theoretical guarantees. We first bound the approximation\nerror when solving the problem through entropic regularization, which reduces\nexponentially with the increase of the regularization parameter. Furthermore,\nwe prove a sublinear first-order convergence rate of the proposed Sinkhorn-type\nalgorithm in the dual space by characterizing the optimization procedure with a\nLyapunov function. To achieve fast and higher-order convergence under weak\nentropy regularization, we augment the Sinkhorn-type algorithm with dynamic\nregularization scheduling and second-order acceleration. Overall, this work\nsystematically combines recent theoretical and numerical advances in entropic\noptimal transport with the constrained case, allowing practitioners to derive\napproximate transport plans in complex scenarios.\n', '  Optimal Transport (OT) problem investigates a transport map that bridges two\ndistributions while minimizing a given cost function. In this regard, OT\nbetween tractable prior distribution and data has been utilized for generative\nmodeling tasks. However, OT-based methods are susceptible to outliers and face\noptimization challenges during training. In this paper, we propose a novel\ngenerative model based on the semi-dual formulation of Unbalanced Optimal\nTransport (UOT). Unlike OT, UOT relaxes the hard constraint on distribution\nmatching. This approach provides better robustness against outliers, stability\nduring training, and faster convergence. We validate these properties\nempirically through experiments. Moreover, we study the theoretical upper-bound\nof divergence between distributions in UOT. Our model outperforms existing\nOT-based generative models, achieving FID scores of 2.97 on CIFAR-10 and 6.36\non CelebA-HQ-256. The code is available at\n\\url{https://github.com/Jae-Moo/UOTM}.\n']",Optimal Transport and Regularization in Machine Learning
152,151,58,151_networks_graphs_graph_nodes,"['networks', 'graphs', 'graph', 'nodes', 'rnns', 'temporal', 'node', 'rnn', 'neural', 'prediction']","['temporal', 'dynamic', 'graphs', 'link', 'graph', 'node', 'nodes', 'neighbor', 'networks', 'hyperedge']","['graphs', 'rnns', 'node', 'datasets', 'forecasting', 'hyperedge', 'links', 'timestamps', 'gnn', 'memory']","[""  Graphs are a powerful representation tool in machine learning applications,\nwith link prediction being a key task in graph learning. Temporal link\nprediction in dynamic networks is of particular interest due to its potential\nfor solving complex scientific and real-world problems. Traditional approaches\nto temporal link prediction have focused on finding the aggregation of dynamics\nof the network as a unified output. In this study, we propose a novel\nperspective on temporal link prediction by defining nodes as Newtonian objects\nand incorporating the concept of velocity to predict network dynamics. By\ncomputing more specific dynamics of each node, rather than overall dynamics, we\nimprove both accuracy and explainability in predicting future connections. We\ndemonstrate the effectiveness of our approach using two datasets, including 17\nyears of co-authorship data from PubMed. Experimental results show that our\ntemporal graph embedding dynamics approach improves downstream classification\nmodels' ability to predict future collaboration efficacy in co-authorship\nnetworks by 17.34% (AUROC improvement relative to the baseline model).\nFurthermore, our approach offers an interpretable layer over traditional\napproaches to address the temporal link prediction problem.\n"", '  Inductive representation learning on temporal heterogeneous graphs is crucial\nfor scalable deep learning on heterogeneous information networks (HINs) which\nare time-varying, such as citation networks. However, most existing approaches\nare not inductive and thus cannot handle new nodes or edges. Moreover, previous\ntemporal graph embedding methods are often trained with the temporal link\nprediction task to simulate the link formation process of temporal graphs,\nwhile ignoring the evolution of high-order topological structures on temporal\ngraphs. To fill these gaps, we propose a Continuous-Time Representation\nLearning (CTRL) model on temporal HINs. To preserve heterogeneous node features\nand temporal structures, CTRL integrates three parts in a single layer, they\nare 1) a \\emph{heterogeneous attention} unit that measures the semantic\ncorrelation between nodes, 2) a \\emph{edge-based Hawkes process} to capture\ntemporal influence between heterogeneous nodes, and 3) \\emph{dynamic\ncentrality} that indicates the dynamic importance of a node. We train the CTRL\nmodel with a future event (a subgraph) prediction task to capture the evolution\nof the high-order network structure. Extensive experiments have been conducted\non three benchmark datasets. The results demonstrate that our model\nsignificantly boosts performance and outperforms various state-of-the-art\napproaches. Ablation studies are conducted to demonstrate the effectiveness of\nthe model design.\n', ""  Dynamic link prediction is an important problem considered by many recent\nworks proposing various approaches for learning temporal edge patterns. To\nassess their efficacy, models are evaluated on publicly available benchmark\ndatasets involving continuous-time and discrete-time temporal graphs. However,\nas we show in this work, the suitability of common batch-oriented evaluation\ndepends on the datasets' characteristics, which can cause two issues: First,\nfor continuous-time temporal graphs, fixed-size batches create time windows\nwith different durations, resulting in an inconsistent dynamic link prediction\ntask. Second, for discrete-time temporal graphs, the sequence of batches can\nadditionally introduce temporal dependencies that are not present in the data.\nIn this work, we empirically show that this common evaluation approach leads to\nskewed model performance and hinders the fair comparison of methods. We\nmitigate this problem by reformulating dynamic link prediction as a link\nforecasting task that better accounts for temporal information present in the\ndata. We provide implementations of our new evaluation method for commonly used\ngraph learning frameworks.\n""]",Temporal Graph Learning and Prediction
153,152,58,152_adversarial_privacy_datasets_generative,"['adversarial', 'privacy', 'datasets', 'generative', 'gans', 'anonymization', 'data', 'gan', 'privatesmote', 'synthetic']","['synthetic', 'privacy', 'tabular', 'utility', 'data', 'private', 'generative', 'concerns', 'anonymization', 'differential']","['adversarial', 'datasets', 'anonymization', 'gan', 'privatesmote', 'triplegan', 'fedtabdiff', 'synthesizers', 'resemblance', 'dpgans']","['  Synthetic data generation, a cornerstone of Generative Artificial\nIntelligence, promotes a paradigm shift in data science by addressing data\nscarcity and privacy while enabling unprecedented performance. As synthetic\ndata becomes more prevalent, concerns emerge regarding the accuracy of\nstatistical methods when applied to synthetic data in contrast to raw data.\nThis article explores the effectiveness of statistical methods on synthetic\ndata and the privacy risks of synthetic data. Regarding effectiveness, we\npresent the Synthetic Data Generation for Analytics framework. This framework\napplies statistical approaches to high-quality synthetic data produced by\ngenerative models like tabular diffusion models, which, initially trained on\nraw data, benefit from insights from pertinent studies through transfer\nlearning. A key finding within this framework is the generational effect, which\nreveals that the error rate of statistical methods on synthetic data decreases\nwith the addition of more synthetic data but may eventually rise or stabilize.\nThis phenomenon, stemming from the challenge of accurately mirroring raw data\ndistributions, highlights a ""reflection point""-an ideal volume of synthetic\ndata defined by specific error metrics. Through three case studies, sentiment\nanalysis, predictive modeling of structured data, and inference in tabular\ndata, we validate the superior performance of this framework compared to\nconventional approaches. On privacy, synthetic data imposes lower risks while\nsupporting the differential privacy standard. These studies underscore\nsynthetic data\'s untapped potential in redefining data science\'s landscape.\n', '  Synthetic data from generative models emerges as the privacy-preserving\ndata-sharing solution. Such a synthetic data set shall resemble the original\ndata without revealing identifiable private information. The backbone\ntechnology of tabular synthesizers is rooted in image generative models,\nranging from Generative Adversarial Networks (GANs) to recent diffusion models.\nRecent prior work sheds light on the utility-privacy tradeoff on tabular data,\nrevealing and quantifying privacy risks on synthetic data. We first conduct an\nexhaustive empirical analysis, highlighting the utility-privacy tradeoff of\nfive state-of-the-art tabular synthesizers, against eight privacy attacks, with\na special focus on membership inference attacks. Motivated by the observation\nof high data quality but also high privacy risk in tabular diffusion, we\npropose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which\nis composed of an autoencoder network to encode the tabular data and a latent\ndiffusion model to synthesize the latent tables. Following the emerging f-DP\nframework, we apply DP-SGD to train the auto-encoder in combination with batch\nclipping and use the separation value as the privacy metric to better capture\nthe privacy gain from DP algorithms. Our empirical evaluation demonstrates that\nDP-TLDM is capable of achieving a meaningful theoretical privacy guarantee\nwhile also significantly enhancing the utility of synthetic data. Specifically,\ncompared to other DP-protected tabular generative models, DP-TLDM improves the\nsynthetic quality by an average of 35% in data resemblance, 15% in the utility\nfor downstream tasks, and 50% in data discriminability, all while preserving a\ncomparable level of privacy risk.\n', '  Data serves as the fundamental foundation for advancing deep learning,\nparticularly tabular data presented in a structured format, which is highly\nconducive to modeling. However, even in the era of LLM, obtaining tabular data\nfrom sensitive domains remains a challenge due to privacy or copyright\nconcerns. Hence, exploring how to effectively use models like LLMs to generate\nrealistic and privacy-preserving synthetic tabular data is urgent. In this\npaper, we take a step forward to explore LLMs for tabular data synthesis and\nprivacy protection, by introducing a new framework HARMONIC for tabular data\ngeneration and evaluation. In the tabular data generation of our framework,\nunlike previous small-scale LLM-based methods that rely on continued\npre-training, we explore the larger-scale LLMs with fine-tuning to generate\ntabular data and enhance privacy. Based on idea of the k-nearest neighbors\nalgorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to\ndiscover inter-row relationships. Then, with fine-tuning, LLMs are trained to\nremember the format and connections of the data rather than the data itself,\nwhich reduces the risk of privacy leakage. In the evaluation part of our\nframework, we develop specific privacy risk metrics DLT for LLM synthetic data\ngeneration, as well as performance evaluation metrics LLE for downstream LLM\ntasks. Our experiments find that this tabular data generation framework\nachieves equivalent performance to existing methods with better privacy, which\nalso demonstrates our evaluation framework for the effectiveness of synthetic\ndata and privacy risks in LLM scenarios.\n']",Synthetic Data Generation and Privacy
154,153,58,153_topics_topicgpt_topic_topical,"['topics', 'topicgpt', 'topic', 'topical', 'contextualized', 'embeddings', 'corpus', 'semantic', 'keywords', 'unsupervised']","['topic', 'topics', 'modeling', 'modelling', 'documents', 'document', 'words', 'keywords', 'word', 'corpus']","['topicgpt', 'topical', 'embeddings', 'corpus', 'keywords', 'unsupervised', 'dirichlet', 'similarity', 's2vntm', 'bert']","['  Topic modelling is fundamentally a soft clustering problem (of known objects\n-- documents, over unknown clusters -- topics). That is, the task is\nincorrectly posed. In particular, the topic models are unstable and incomplete.\nAll this leads to the fact that the process of finding a good topic model\n(repeated hyperparameter selection, model training, and topic quality\nassessment) can be particularly long and labor-intensive. We aim to simplify\nthe process, to make it more deterministic and provable. To this end, we\npresent a method for iterative training of a topic model. The essence of the\nmethod is that a series of related topic models are trained so that each\nsubsequent model is at least as good as the previous one, i.e., that it retains\nall the good topics found earlier. The connection between the models is\nachieved by additive regularization. The result of this iterative training is\nthe last topic model in the series, which we call the iteratively updated\nadditively regularized topic model (ITAR). Experiments conducted on several\ncollections of natural language texts show that the proposed ITAR model\nperforms better than other popular topic models (LDA, ARTM, BERTopic), its\ntopics are diverse, and its perplexity (ability to ""explain"" the underlying\ndata) is moderate.\n', ""  Large language models (LLMs) with their strong zero-shot topic extraction\ncapabilities offer an alternative to probabilistic topic modelling and\nclosed-set topic classification approaches. As zero-shot topic extractors, LLMs\nare expected to understand human instructions to generate relevant and\nnon-hallucinated topics based on the given documents. However, LLM-based topic\nmodelling approaches often face difficulties in generating topics with\nadherence to granularity as specified in human instructions, often resulting in\nmany near-duplicate topics. Furthermore, methods for addressing hallucinated\ntopics generated by LLMs have not yet been investigated. In this paper, we\nfocus on addressing the issues of topic granularity and hallucinations for\nbetter LLM-based topic modelling. To this end, we introduce a novel approach\nthat leverages Direct Preference Optimisation (DPO) to fine-tune open-source\nLLMs, such as Mistral-7B. Our approach does not rely on traditional human\nannotation to rank preferred answers but employs a reconstruction pipeline to\nmodify raw topics generated by LLMs, thus enabling a fast and efficient\ntraining and inference framework. Comparative experiments show that our\nfine-tuning approach not only significantly improves the LLM's capability to\nproduce more coherent, relevant, and precise topics, but also reduces the\nnumber of hallucinated topics.\n"", ""  Topic models are valuable for understanding extensive document collections,\nbut they don't always identify the most relevant topics. Classical\nprobabilistic and anchor-based topic models offer interactive versions that\nallow users to guide the models towards more pertinent topics. However, such\ninteractive features have been lacking in neural topic models. To correct this\nlacuna, we introduce a user-friendly interaction for neural topic models. This\ninteraction permits users to assign a word label to a topic, leading to an\nupdate in the topic model where the words in the topic become closely aligned\nwith the given label. Our approach encompasses two distinct kinds of neural\ntopic models. The first includes models where topic embeddings are trainable\nand evolve during the training process. The second kind involves models where\ntopic embeddings are integrated post-training, offering a different approach to\ntopic refinement. To facilitate user interaction with these neural topic\nmodels, we have developed an interactive interface. This interface enables\nusers to engage with and re-label topics as desired. We evaluate our method\nthrough a human study, where users can relabel topics to find relevant\ndocuments. Using our method, user labeling improves document rank scores,\nhelping to find more relevant documents to a given query when compared to no\nuser labeling.\n""]",Topic Modeling and Analysis
155,154,58,154_equivariant_equivariance_symmetries_invariants,"['equivariant', 'equivariance', 'symmetries', 'invariants', 'representations', 'invariance', 'symmetry', 'convolutions', 'cnns', 'invariant']","['equivariant', 'equivariance', 'symmetry', 'group', 'symmetries', 'invariant', 'groups', 'symmetric', 'transformations', 'breaking']","['equivariant', 'symmetries', 'representations', 'invariance', 'convolutions', 'cnns', 'manifold', 'liegan', 'enns', 'action']","['  We present a novel framework to overcome the limitations of equivariant\narchitectures in learning functions with group symmetries. In contrary to\nequivariant architectures, we use an arbitrary base model such as an MLP or a\ntransformer and symmetrize it to be equivariant to the given group by employing\na small equivariant network that parameterizes the probabilistic distribution\nunderlying the symmetrization. The distribution is end-to-end trained with the\nbase model which can maximize performance while reducing sample complexity of\nsymmetrization. We show that this approach ensures not only equivariance to\ngiven group but also universal approximation capability in expectation. We\nimplement our method on various base models, including patch-based transformers\nthat can be initialized from pretrained vision transformers, and test them for\na wide range of symmetry groups including permutation and Euclidean groups and\ntheir combinations. Empirical tests show competitive results against tailored\nequivariant architectures, suggesting the potential for learning equivariant\nfunctions for diverse groups using a non-equivariant universal base\narchitecture. We further show evidence of enhanced learning in symmetric\nmodalities, like graphs, when pretrained from non-symmetric modalities, like\nvision. Code is available at https://github.com/jw9730/lps.\n', '  In this paper we develop a manifestly geometric framework for equivariant\nmanifold neural ordinary differential equations (NODEs), and use it to analyse\ntheir modelling capabilities for symmetric data. First, we consider the action\nof a Lie group $G$ on a smooth manifold $M$ and establish the equivalence\nbetween equivariance of vector fields, symmetries of the corresponding Cauchy\nproblems, and equivariance of the associated NODEs. We also propose a novel\nformulation of the equivariant NODEs in terms of the differential invariants of\nthe action of $G$ on $M$, based on Lie theory for symmetries of differential\nequations, which provides an efficient parameterisation of the space of\nequivariant vector fields in a way that is agnostic to both the manifold $M$\nand the symmetry group $G$. Second, we construct augmented manifold NODEs,\nthrough embeddings into equivariant flows, and show that they are universal\napproximators of equivariant diffeomorphisms on any path-connected $M$.\nFurthermore, we show that the augmented NODEs can be incorporated in the\ngeometric framework and parameterised using higher order differential\ninvariants. Finally, we consider the induced action of $G$ on different fields\non $M$ and show how it can be used to generalise previous work, on, e.g.,\ncontinuous normalizing flows, to equivariant models in any geometry.\n', '  Equivariant deep learning architectures exploit symmetries in learning\nproblems to improve the sample efficiency of neural-network-based models and\ntheir ability to generalise. However, when modelling real-world data, learning\nproblems are often not exactly equivariant, but only approximately. For\nexample, when estimating the global temperature field from weather station\nobservations, local topographical features like mountains break translation\nequivariance. In these scenarios, it is desirable to construct architectures\nthat can flexibly depart from exact equivariance in a data-driven way. In this\npaper, we develop a general approach to achieving this using existing\nequivariant architectures. Our approach is agnostic to both the choice of\nsymmetry group and model architecture, making it widely applicable. We consider\nthe use of approximately equivariant architectures in neural processes (NPs), a\npopular family of meta-learning models. We demonstrate the effectiveness of our\napproach on a number of synthetic and real-world regression experiments,\ndemonstrating that approximately equivariant NP models can outperform both\ntheir non-equivariant and strictly equivariant counterparts.\n']",Equivariant Neural Networks
156,155,57,155_reservoir_reservoirs_rnn_lstm,"['reservoir', 'reservoirs', 'rnn', 'lstm', 'chaotic', 'forecasting', 'predicting', 'computing', 'memory', 'recurrent']","['reservoir', 'chaotic', 'dynamical', 'reservoirs', 'computing', 'dynamics', 'attractor', 'series', 'recurrent', 'nonlinear']","['reservoirs', 'rnn', 'lstm', 'chaotic', 'forecasting', 'approximators', 'stochastic', 'rhythmic', 'synchronization', 'computers']","['  A reservoir computer is a type of dynamical system arranged to do\ncomputation. Typically, a reservoir computer is constructed by connecting a\nlarge number of nonlinear nodes in a network that includes recurrent\nconnections. In order to achieve accurate results, the reservoir usually\ncontains hundreds to thousands of nodes. This high dimensionality makes it\ndifficult to analyze the reservoir computer using tools from dynamical systems\ntheory. Additionally, the need to create and connect large numbers of nonlinear\nnodes makes it difficult to design and build analog reservoir computers that\ncan be faster and consume less power than digital reservoir computers. We\ndemonstrate here that a reservoir computer may be divided into two parts; a\nsmall set of nonlinear nodes (the reservoir), and a separate set of\ntime-shifted reservoir output signals. The time-shifted output signals serve to\nincrease the rank and memory of the reservoir computer, and the set of\nnonlinear nodes may create an embedding of the input dynamical system. We use\nthis time-shifting technique to obtain excellent performance from an\nopto-electronic delay-based reservoir computer with only a small number of\nvirtual nodes. Because only a few nonlinear nodes are required, construction of\na reservoir computer becomes much easier, and delay-based reservoir computers\ncan operate at much higher speeds.\n', '  Photonic reservoir computing has been successfully utilized in time-series\nprediction as the need for hardware implementations has increased. Prediction\nof chaotic time series remains a significant challenge, an area where the\nconventional reservoir computing framework encounters limitations of prediction\naccuracy. We introduce an attention mechanism to the reservoir computing model\nin the output stage. This attention layer is designed to prioritize distinct\nfeatures and temporal sequences, thereby substantially enhancing the prediction\naccuracy. Our results show that a photonic reservoir computer enhanced with the\nattention mechanism exhibits improved prediction capabilities for smaller\nreservoirs. These advancements highlight the transformative possibilities of\nreservoir computing for practical applications where accurate prediction of\nchaotic time series is crucial.\n', '  Reservoir computing is a form of machine learning that utilizes nonlinear\ndynamical systems to perform complex tasks in a cost-effective manner when\ncompared to typical neural networks. Many recent advancements in reservoir\ncomputing, in particular quantum reservoir computing, make use of reservoirs\nthat are inherently stochastic. However, the theoretical justification for\nusing these systems has not yet been well established. In this paper, we\ninvestigate the universality of stochastic reservoir computers, in which we use\na stochastic system for reservoir computing using the probabilities of each\nreservoir state as the readout instead of the states themselves. In stochastic\nreservoir computing, the number of distinct states of the entire reservoir\ncomputer can potentially scale exponentially with the size of the reservoir\nhardware, offering the advantage of compact device size. We prove that classes\nof stochastic echo state networks, and therefore the class of all stochastic\nreservoir computers, are universal approximating classes. We also investigate\nthe performance of two practical examples of stochastic reservoir computers in\nclassification and chaotic time series prediction. While shot noise is a\nlimiting factor in the performance of stochastic reservoir computing, we show\nsignificantly improved performance compared to a deterministic reservoir\ncomputer with similar hardware in cases where the effects of noise are small.\n']",Reservoir Computing and Chaotic Time Series Prediction
157,156,57,156_counseling_utterances_psychotherapeutic_conversations,"['counseling', 'utterances', 'psychotherapeutic', 'conversations', 'counselor', 'psychotherapists', 'dialogue', 'counselors', 'dialogues', 'conversational']","['mental', 'health', 'counseling', 'therapists', 'psychological', 'therapy', 'psychotherapy', 'counselors', 'depression', 'dialogue']","['counseling', 'utterances', 'psychotherapists', 'interventions', 'conversation', 'mentalagora', 'questionnaires', 'health', 'psychological', 'assessment']","['  Amidst the growing interest in developing task-autonomous AI for automated\nmental health care, this paper addresses the ethical and practical challenges\nassociated with the issue and proposes a structured framework that delineates\nlevels of autonomy, outlines ethical requirements, and defines beneficial\ndefault behaviors for AI agents in the context of mental health support. We\nalso evaluate fourteen state-of-the-art language models (ten off-the-shelf,\nfour fine-tuned) using 16 mental health-related questionnaires designed to\nreflect various mental health conditions, such as psychosis, mania, depression,\nsuicidal thoughts, and homicidal tendencies. The questionnaire design and\nresponse evaluations were conducted by mental health clinicians (M.D.s). We\nfind that existing language models are insufficient to match the standard\nprovided by human professionals who can navigate nuances and appreciate\ncontext. This is due to a range of issues, including overly cautious or\nsycophantic responses and the absence of necessary safeguards. Alarmingly, we\nfind that most of the tested models could cause harm if accessed in mental\nhealth emergencies, failing to protect users and potentially exacerbating\nexisting symptoms. We explore solutions to enhance the safety of current\nmodels. Before the release of increasingly task-autonomous AI systems in mental\nhealth, it is crucial to ensure that these models can reliably detect and\nmanage symptoms of common psychiatric disorders to prevent harm to users. This\ninvolves aligning with the ethical framework and default behaviors outlined in\nour study. We contend that model developers are responsible for refining their\nsystems per these guidelines to safeguard against the risks posed by current AI\ntechnologies to user mental health and safety.\n  Trigger warning: Contains and discusses examples of sensitive mental health\ntopics, including suicide and self-harm.\n', '  The advent of large language models (LLMs) has significantly advanced various\nfields, including natural language processing and automated dialogue systems.\nThis paper explores the application of LLMs in psychological counseling,\naddressing the increasing demand for mental health services. We present a\nmethod for instruction tuning LLMs with specialized prompts to enhance their\nperformance in providing empathetic, relevant, and supportive responses. Our\napproach involves developing a comprehensive dataset of counseling-specific\nprompts, refining them through feedback from professional counselors, and\nconducting rigorous evaluations using both automatic metrics and human\nassessments. The results demonstrate that our instruction-tuned model\noutperforms several baseline LLMs, highlighting its potential as a scalable and\naccessible tool for mental health support.\n', ""  Global rates of mental health concerns are rising, and there is increasing\nrealization that existing models of mental health care will not adequately\nexpand to meet the demand. With the emergence of large language models (LLMs)\nhas come great optimism regarding their promise to create novel, large-scale\nsolutions to support mental health. Despite their nascence, LLMs have already\nbeen applied to mental health related tasks. In this paper, we summarize the\nextant literature on efforts to use LLMs to provide mental health education,\nassessment, and intervention and highlight key opportunities for positive\nimpact in each area. We then highlight risks associated with LLMs' application\nto mental health and encourage the adoption of strategies to mitigate these\nrisks. The urgent need for mental health support must be balanced with\nresponsible development, testing, and deployment of mental health LLMs. It is\nespecially critical to ensure that mental health LLMs are fine-tuned for mental\nhealth, enhance mental health equity, and adhere to ethical standards and that\npeople, including those with lived experience with mental health concerns, are\ninvolved in all stages from development through deployment. Prioritizing these\nefforts will minimize potential harms to mental health and maximize the\nlikelihood that LLMs will positively impact mental health globally.\n""]",Mental Health Support through AI-Powered Conversations
158,157,56,157_unlearning_unlearn_memorization_forgetting,"['unlearning', 'unlearn', 'memorization', 'forgetting', 'memorizing', 'memorize', 'memorized', 'unlearned', 'corpus', 'forget']","['unlearning', 'forget', 'memorization', 'forgetting', 'unlearned', 'copyright', 'harmful', 'knowledge', 'sensitive', 'ascent']","['memorization', 'unlearned', 'corpus', 'forgetfilter', 'mitigate', 'retain', 'privacy', 'conceptvectors', 'ethical', 'information']","['  Large Language Models (LLMs) have highlighted the necessity of effective\nunlearning mechanisms to comply with data regulations and ethical AI practices.\nLLM unlearning aims at removing undesired data influences and associated model\ncapabilities without compromising utility beyond the scope of unlearning. While\ninterest in studying LLM unlearning is growing, the impact of the optimizer\nchoice for LLM unlearning remains unexplored. In this work, we shed light on\nthe significance of optimizer selection in LLM unlearning for the first time,\nestablishing a clear connection between second-order optimization and influence\nunlearning (a classical approach using influence functions to update the model\nfor data influence removal). This insight propels us to develop a second-order\noptimization-based LLM unlearning framework, termed Second-Order UnLearning\n(SOUL), which extends the static, one-shot model update using influence\nunlearning to a dynamic, iterative unlearning process. Our extensive\nexperiments show that SOUL consistently outperforms conventional first-order\nmethods across various unlearning tasks, models, and metrics, indicating that\nsecond-order optimization offers an effective and broadly applicable solution\nfor LLM unlearning. Codes are available at https://github.com/OPTML-Group/SOUL.\n', '  Large language models trained on massive corpora of data from the web can\nmemorize and reproduce sensitive or private data raising both legal and ethical\nconcerns. Unlearning, or tuning models to forget information present in their\ntraining data, provides us with a way to protect private data after training.\nAlthough several methods exist for such unlearning, it is unclear to what\nextent they result in models equivalent to those where the data to be forgotten\nwas never learned in the first place. To address this challenge, we present\nTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen\nour understanding of unlearning. We offer a dataset of 200 diverse synthetic\nauthor profiles, each consisting of 20 question-answer pairs, and a subset of\nthese profiles called the forget set that serves as the target for unlearning.\nWe compile a suite of metrics that work together to provide a holistic picture\nof unlearning efficacy. Finally, we provide a set of baseline results from\nexisting unlearning algorithms. Importantly, none of the baselines we consider\nshow effective unlearning motivating continued efforts to develop approaches\nfor unlearning that effectively tune models so that they truly behave as if\nthey were never trained on the forget data at all.\n', '  Despite the strong capabilities of Large Language Models (LLMs) to acquire\nknowledge from their training corpora, the memorization of sensitive\ninformation in the corpora such as copyrighted, harmful, and private content\nhas led to ethical and legal concerns. In response to these challenges,\nunlearning has emerged as a potential remedy for LLMs affected by problematic\ntraining data. However, previous unlearning techniques are either not\napplicable to black-box LLMs due to required access to model internal weights,\nor violate data protection principles by retaining sensitive data for\ninference-time correction. We propose $\\delta$-unlearning, an offset unlearning\nframework for black-box LLMs. Instead of tuning the black-box LLM itself,\n$\\delta$-unlearning learns the logit offset needed for unlearning by\ncontrasting the logits from a pair of smaller models. Experiments demonstrate\nthat $\\delta$-unlearning can effectively unlearn target data while maintaining\nsimilar or even stronger performance on general out-of-forget-scope tasks.\n$\\delta$-unlearning also effectively incorporates different unlearning\nalgorithms, making our approach a versatile solution to adapting various\nexisting unlearning algorithms to black-box LLMs.\n']",Unlearning in Large Language Models
159,158,56,158_faults_fault_detect_bearings,"['faults', 'fault', 'detect', 'bearings', 'detection', 'monitoring', 'wavelet', 'feature', 'features', 'machinery']","['fault', 'faults', 'bearing', 'vibration', 'diagnosis', 'signal', 'monitoring', 'maintenance', 'bearings', 'industrial']","['faults', 'bearings', 'wavelet', 'features', 'machinery', 'diagnostic', 'sensor', 'rotor', 'dlns', 'diesel']","['  Domain generalization achieves fault diagnosis on unseen modes. In process\nindustrial systems, fault samples are limited, and only single-mode fault data\ncan be obtained. Extracting domain-invariant fault features from single-mode\ndata for unseen mode fault diagnosis poses challenges. Existing methods utilize\na generator module to simulate samples of unseen modes. However, multi-mode\nsamples contain complex spatiotemporal information, which brings significant\ndifficulties to accurate sample generation. Therefore, double gradient reversal\nnetwork (DGRN) is proposed. First, the model is pre-trained to acquire fault\nknowledge from the single seen mode. Then, pseudo-fault feature generation\nstrategy is designed by Adaptive instance normalization, to simulate fault\nfeatures of unseen mode. The dual adversarial training strategy is created to\nenhance the diversity of pseudo-fault features, which models unseen modes with\nsignificant distribution differences. Subsequently, domain-invariant feature\nextraction strategy is constructed by contrastive learning and adversarial\nlearning. This strategy extracts common features of faults and helps multi-mode\nfault diagnosis. Finally, the experiments were conducted on Tennessee Eastman\nprocess and continuous stirred-tank reactor. The experiments demonstrate that\nDGRN achieves high classification accuracy on unseen modes while maintaining a\nsmall model size.\n', '  One important characteristic of modern fault classification systems is the\nability to flag the system when faced with previously unseen fault types. This\nwork considers the unknown fault detection capabilities of deep neural\nnetwork-based fault classifiers. Specifically, we propose a methodology on how,\nwhen available, labels regarding the fault taxonomy can be used to increase\nunknown fault detection performance without sacrificing model performance. To\nachieve this, we propose to utilize soft label techniques to improve the\nstate-of-the-art deep novel fault detection techniques during the training\nprocess and novel hierarchically consistent detection statistics for online\nnovel fault detection. Finally, we demonstrated increased detection performance\non novel fault detection in inspection images from the hot steel rolling\nprocess, with results well replicated across multiple scenarios and baseline\ndetection methods.\n', '  Bearings are one of the vital components of rotating machines that are prone\nto unexpected faults. Therefore, bearing fault diagnosis and condition\nmonitoring is essential for reducing operational costs and downtime in numerous\nindustries. In various production conditions, bearings can be operated under a\nrange of loads and speeds, which causes different vibration patterns associated\nwith each fault type. Normal data is ample as systems usually work in desired\nconditions. On the other hand, fault data is rare, and in many conditions,\nthere is no data recorded for the fault classes. Accessing fault data is\ncrucial for developing data-driven fault diagnosis tools that can improve both\nthe performance and safety of operations. To this end, a novel algorithm based\non Conditional Generative Adversarial Networks (CGANs) is introduced. Trained\non the normal and fault data on any actual fault conditions, this algorithm\ngenerates fault data from normal data of target conditions. The proposed method\nis validated on a real-world bearing dataset, and fault data are generated for\ndifferent conditions. Several state-of-the-art classifiers and visualization\nmodels are implemented to evaluate the quality of the synthesized data. The\nresults demonstrate the efficacy of the proposed algorithm.\n']",Fault Detection in Industrial Machinery
160,159,56,159_causalbench_causal_causality_causalnlp,"['causalbench', 'causal', 'causality', 'causalnlp', 'reasoning', 'inference', 'knowledge', 'relational', 'counterfactual', 'entailment']","['causal', 'causality', 'discovery', 'reasoning', 'graphs', 'effect', 'knowledge', 'facts', 'relations', 'relationships']","['causalnlp', 'counterfactual', 'entailment', 'corpus', 'interventional', 'relations', 'learn', 'probing', 'mediation', 'llm4causal']","['  This paper explores the causal reasoning of large language models (LLMs) to\nenhance their interpretability and reliability in advancing artificial\nintelligence. Despite the proficiency of LLMs in a range of tasks, their\npotential for understanding causality requires further exploration. We propose\na novel causal attribution model that utilizes ``do-operators"" for constructing\ncounterfactual scenarios, allowing us to systematically quantify the influence\nof input numerical data and LLMs\' pre-existing knowledge on their causal\nreasoning processes. Our newly developed experimental setup assesses LLMs\'\nreliance on contextual information and inherent knowledge across various\ndomains. Our evaluation reveals that LLMs\' causal reasoning ability mainly\ndepends on the context and domain-specific knowledge provided. In the absence\nof such knowledge, LLMs can still maintain a degree of causal reasoning using\nthe available numerical data, albeit with limitations in the calculations. This\nmotivates the proposed fine-tuned LLM for pairwise causal discovery,\neffectively leveraging both knowledge and numerical information.\n', ""  Recent advances in artificial intelligence have seen Large Language Models\n(LLMs) demonstrate notable proficiency in causal discovery tasks. This study\nexplores the factors influencing the performance of LLMs in causal discovery\ntasks. Utilizing open-source LLMs, we examine how the frequency of causal\nrelations within their pre-training corpora affects their ability to accurately\nrespond to causal discovery queries. Our findings reveal that a higher\nfrequency of causal mentions correlates with better model performance,\nsuggesting that extensive exposure to causal information during training\nenhances the models' causal discovery capabilities. Additionally, we\ninvestigate the impact of context on the validity of causal relations. Our\nresults indicate that LLMs might exhibit divergent predictions for identical\ncausal relations when presented in different contexts. This paper provides the\nfirst comprehensive analysis of how different factors contribute to LLM\nperformance in causal discovery tasks.\n"", ""  Large language models (LLMs) have achieved significant success across various\ndomains. However, the inherent complexity of causal problems and causal theory\nposes challenges in accurately describing them in natural language, making it\ndifficult for LLMs to comprehend and use them effectively. Causal methods are\nnot easily conveyed through natural language, which hinders LLMs' ability to\napply them accurately. Additionally, causal datasets are typically tabular,\nwhile LLMs excel in handling natural language data, creating a structural\nmismatch that impedes effective reasoning with tabular data. This lack of\ncausal reasoning capability limits the development of LLMs. To address these\nchallenges, we have equipped the LLM with causal tools within an agent\nframework, named the Causal Agent, enabling it to tackle causal problems. The\ncausal agent comprises tools, memory, and reasoning modules. In the tools\nmodule, the causal agent applies causal methods to align tabular data with\nnatural language. In the reasoning module, the causal agent employs the ReAct\nframework to perform reasoning through multiple iterations with the tools. In\nthe memory module, the causal agent maintains a dictionary instance where the\nkeys are unique names and the values are causal graphs. To verify the causal\nability of the causal agent, we established a benchmark consisting of four\nlevels of causal problems: variable level, edge level, causal graph level, and\ncausal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for\nthese four levels of issues and tested the causal agent on the datasets. Our\nmethodology demonstrates remarkable efficacy on the four-level causal problems,\nwith accuracy rates all above 80%. For further insights and implementation\ndetails, our code is accessible via the GitHub repository\nhttps://github.com/Kairong-Han/Causal_Agent.\n""]",Causal Reasoning in Large Language Models
161,160,55,160_hypergraph_graphacl_graphon_graphlearner,"['hypergraph', 'graphacl', 'graphon', 'graphlearner', 'subgraph', 'graphmae', 'graphs', 'hypergraphs', 'embeddings', 'graph']","['contrastive', 'graph', 'node', 'negative', 'augmentation', 'views', 'positive', 'nodes', 'graphs', 'view']","['hypergraph', 'graphlearner', 'embeddings', 'learn', 'encoder', 'labels', 'augmentations', 'gnns', 'unsupervised', 'spectral']","['  In recent years, deep learning on graphs has achieved remarkable success in\nvarious domains. However, the reliance on annotated graph data remains a\nsignificant bottleneck due to its prohibitive cost and time-intensive nature.\nTo address this challenge, self-supervised learning (SSL) on graphs has gained\nincreasing attention and has made significant progress. SSL enables machine\nlearning models to produce informative representations from unlabeled graph\ndata, reducing the reliance on expensive labeled data. While SSL on graphs has\nwitnessed widespread adoption, one critical component, Graph Contrastive\nLearning (GCL), has not been thoroughly investigated in the existing\nliterature. Thus, this survey aims to fill this gap by offering a dedicated\nsurvey on GCL. We provide a comprehensive overview of the fundamental\nprinciples of GCL, including data augmentation strategies, contrastive modes,\nand contrastive optimization objectives. Furthermore, we explore the extensions\nof GCL to other aspects of data-efficient graph learning, such as weakly\nsupervised learning, transfer learning, and related scenarios. We also discuss\npractical applications spanning domains such as drug discovery, genomics\nanalysis, recommender systems, and finally outline the challenges and potential\nfuture directions in this field.\n', ""  Graph contrastive learning (GCL) is a popular method for leaning graph\nrepresentations by maximizing the consistency of features across augmented\nviews. Traditional GCL methods utilize single-perspective i.e. data or\nmodel-perspective) augmentation to generate positive samples, restraining the\ndiversity of positive samples. In addition, these positive samples may be\nunreliable due to uncontrollable augmentation strategies that potentially alter\nthe semantic information. To address these challenges, this paper proposed a\ninnovative framework termed dual-perspective cross graph contrastive learning\n(DC-GCL), which incorporates three modifications designed to enhance positive\nsample diversity and reliability: 1) We propose dual-perspective augmentation\nstrategy that provide the model with more diverse training data, enabling the\nmodel effective learning of feature consistency across different views. 2) From\nthe data perspective, we slightly perturb the original graphs using\ncontrollable data augmentation, effectively preserving their semantic\ninformation. 3) From the model perspective, we enhance the encoder by utilizing\nmore powerful graph transformers instead of graph neural networks. Based on the\nmodel's architecture, we propose three pruning-based strategies to slightly\nperturb the encoder, providing more reliable positive samples. These\nmodifications collectively form the DC-GCL's foundation and provide more\ndiverse and reliable training inputs, offering significant improvements over\ntraditional GCL methods. Extensive experiments on various benchmarks\ndemonstrate that DC-GCL consistently outperforms different baselines on various\ndatasets and tasks.\n"", '  Graph Contrastive Learning (GCL) has emerged as a popular training approach\nfor learning node embeddings from augmented graphs without labels. Despite the\nkey principle that maximizing the similarity between positive node pairs while\nminimizing it between negative node pairs is well established, some fundamental\nproblems are still unclear. Considering the complex graph structure, are some\nnodes consistently well-trained and following this principle even with\ndifferent graph augmentations? Or are there some nodes more likely to be\nuntrained across graph augmentations and violate the principle? How to\ndistinguish these nodes and further guide the training of GCL? To answer these\nquestions, we first present experimental evidence showing that the training of\nGCL is indeed imbalanced across all nodes. To address this problem, we propose\nthe metric ""node compactness"", which is the lower bound of how a node follows\nthe GCL principle related to the range of augmentations. We further derive the\nform of node compactness theoretically through bound propagation, which can be\nintegrated into binary cross-entropy as a regularization. To this end, we\npropose the PrOvable Training (POT) for GCL, which regularizes the training of\nGCL to encode node embeddings that follows the GCL principle better. Through\nextensive experiments on various benchmarks, POT consistently improves the\nexisting GCL approaches, serving as a friendly plugin.\n']",Graph Contrastive Learning
162,161,55,161_arabic_arabicaqa_arab_arablegaleval,"['arabic', 'arabicaqa', 'arab', 'arablegaleval', 'hebrew', 'languages', 'language', 'multilingual', 'moroccan', 'persian']","['switching', 'native', 'corpus', 'linguistic', 'language', 'text', 'cultural', 'processing', 'texts', 'culture']","['arabic', 'arablegaleval', 'arabiangpt', 'alclam', 'dialects', 'saudibert', 'lexicons', 'cidar', 'dataset', 'camel']","['  We present ALLaM: Arabic Large Language Model, a series of large language\nmodels to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is\ncarefully trained considering the values of language alignment and knowledge\ntransfer at scale. Our autoregressive decoder-only architecture models\ndemonstrate how second-language acquisition via vocabulary expansion and\npretraining on a mixture of Arabic and English text can steer a model towards a\nnew language (Arabic) without any catastrophic forgetting in the original\nlanguage (English). Furthermore, we highlight the effectiveness of using\nparallel/translated data to aid the process of knowledge alignment between\nlanguages. Finally, we show that extensive alignment with human preferences can\nsignificantly enhance the performance of a language model compared to models of\na larger scale with lower quality alignment. ALLaM achieves state-of-the-art\nperformance in various Arabic benchmarks, including MMLU Arabic, ACVA, and\nArabic Exams. Our aligned models improve both in Arabic and English from their\nbase aligned models.\n', '  Large language models (LLMs) have greatly impacted the natural language\nprocessing (NLP) field, particularly for the English language. These models\nhave demonstrated capabilities in understanding and generating human-like text.\nThe success of language models largely depends on the availability of\nhigh-quality instruction datasets, which consist of detailed task descriptions\nand corresponding responses that are essential for training the models to\naddress a variety of prompts accurately. However, the availability and quality\nof these resources vary by language. While models perform well in English, they\noften need help with languages like Arabic, due to the lack of datasets for\nfine-tuning Arabic-specific tasks. To address this issue, we introduce\nInstAr-500k, a new Arabic instruction dataset created by generating and\ncollecting content that covers several domains and instruction types. We assess\nthis dataset by fine-tuning an open-source Gemma-7B model on several downstream\ntasks to improve its functionality. Based on multiple evaluations, our\nfine-tuned model achieves excellent performance on several Arabic NLP\nbenchmarks. These outcomes emphasize the effectiveness of our dataset in\nelevating the capabilities of language models for Arabic. Our instruction\ndataset bridges the performance gap between English and Arabic language models\nby providing resources that amplify Arabic NLP development. Building on this\nfoundation, we developed a model, GemmAr-7B-V1, specifically tuned to excel at\na wide range of Arabic NLP tasks.\n', '  In recent years, Large Language Models have revolutionized the field of\nnatural language processing, showcasing an impressive rise predominantly in\nEnglish-centric domains. These advancements have set a global benchmark,\ninspiring significant efforts toward developing Arabic LLMs capable of\nunderstanding and generating the Arabic language with remarkable accuracy.\nDespite these advancements, a critical challenge persists: the potential bias\nin Arabic LLMs, primarily attributed to their reliance on datasets comprising\nEnglish data that has been translated into Arabic. This reliance not only\ncompromises the authenticity of the generated content but also reflects a\nbroader issue -the scarcity of original quality Arabic linguistic data. This\nstudy aims to address the data scarcity in the Arab world and to encourage the\ndevelopment of Arabic Language Models that are true to both the linguistic and\nnuances of the region. We undertook a large-scale data mining project,\nextracting a substantial volume of text from the Common Crawl WET files,\nspecifically targeting Arabic content. The extracted data underwent a rigorous\ncleaning and deduplication process, using innovative techniques to ensure the\nintegrity and uniqueness of the dataset. The result is the 101 Billion Arabic\nWords Dataset, the largest Arabic dataset available to date, which can\nsignificantly contribute to the development of authentic Arabic LLMs. This\nstudy not only highlights the potential for creating linguistically and\nculturally accurate Arabic LLMs but also sets a precedent for future research\nin enhancing the authenticity of Arabic language models.\n']",Arabic Language Models and NLP Development
163,162,55,162_priors_bayesian_posteriors_bayes,"['priors', 'bayesian', 'posteriors', 'bayes', 'neural', 'posterior', 'deep', 'prior', 'likelihood', 'inference']","['posterior', 'priors', 'variational', 'uncertainty', 'inference', 'likelihood', 'networks', 'prior', 'neural', 'approximation']","['posteriors', 'deep', 'likelihood', 'approximating', 'mcmc', 'hyperparameters', 'ensembles', 'stochastic', 'weights', 'hessian']","['  Uncertainty quantification is an important task in machine learning - a task\nin which standardneural networks (NNs) have traditionally not excelled. This\ncan be a limitation for safety-critical applications, where uncertainty-aware\nmethods like Gaussian processes or Bayesian linear regression are often\npreferred. Bayesian neural networks are an approach to address this limitation.\nThey assume probability distributions for all parameters and yield distributed\npredictions. However, training and inference are typically intractable and\napproximations must be employed. A promising approximation is NNs with Bayesian\nlast layer (BLL). They assume distributed weights only in the linear output\nlayer and yield a normally distributed prediction. To approximate the\nintractable Bayesian neural network, point estimates of the distributed weights\nin all but the last layer should be obtained by maximizing the marginal\nlikelihood. This has previously been challenging, as the marginal likelihood is\nexpensive to evaluate in this setting. We present a reformulation of the\nlog-marginal likelihood of a NN with BLL which allows for efficient training\nusing backpropagation. Furthermore, we address the challenge of uncertainty\nquantification for extrapolation points. We provide a metric to quantify the\ndegree of extrapolation and derive a method to improve the uncertainty\nquantification for these points. Our methods are derived for the multivariate\ncase and demonstrated in a simulation study. In comparison to Bayesian linear\nregression with fixed features, and a Bayesian neural network trained with\nvariational inference, our proposed method achieves the highest log-predictive\ndensity on test data.\n', '  Bayesian neural networks (BNNs) have recently gained popularity due to their\nability to quantify model uncertainty. However, specifying a prior for BNNs\nthat captures relevant domain knowledge is often extremely challenging. In this\nwork, we propose a framework for integrating general forms of domain knowledge\n(i.e., any knowledge that can be represented by a loss function) into a BNN\nprior through variational inference, while enabling computationally efficient\nposterior inference and sampling. Specifically, our approach results in a prior\nover neural network weights that assigns high probability mass to models that\nbetter align with our domain knowledge, leading to posterior samples that also\nexhibit this behavior. We show that BNNs using our proposed domain knowledge\npriors outperform those with standard priors (e.g., isotropic Gaussian,\nGaussian process), successfully incorporating diverse types of prior\ninformation such as fairness, physics rules, and healthcare knowledge and\nachieving better predictive performance. We also present techniques for\ntransferring the learned priors across different model architectures,\ndemonstrating their broad utility across various settings.\n', '  Deep learning has revolutionized the last decade, being at the forefront of\nextraordinary advances in a wide range of tasks including computer vision,\nnatural language processing, and reinforcement learning, to name but a few.\nHowever, it is well-known that deep models trained via maximum likelihood\nestimation tend to be overconfident and give poorly-calibrated predictions.\nBayesian deep learning attempts to address this by placing priors on the model\nparameters, which are then combined with a likelihood to perform posterior\ninference. Unfortunately, for deep models, the true posterior is intractable,\nforcing the user to resort to approximations. In this thesis, we explore the\nuse of variational inference (VI) as an approximation, as it is unique in\nsimultaneously approximating the posterior and providing a lower bound to the\nmarginal likelihood. If tight enough, this lower bound can be used to optimize\nhyperparameters and to facilitate model selection. However, this capacity has\nrarely been used to its full extent for Bayesian neural networks, likely\nbecause the approximate posteriors typically used in practice can lack the\nflexibility to effectively bound the marginal likelihood. We therefore explore\nthree aspects of Bayesian learning for deep models: 1) we ask whether it is\nnecessary to perform inference over as many parameters as possible, or whether\nit is reasonable to treat many of them as optimizable hyperparameters; 2) we\npropose a variational posterior that provides a unified view of inference in\nBayesian neural networks and deep Gaussian processes; 3) we demonstrate how VI\ncan be improved in certain deep Gaussian process models by analytically\nremoving symmetries from the posterior, and performing inference on Gram\nmatrices instead of features. We hope that our contributions will provide a\nstepping stone to fully realize the promises of VI in the future.\n']",Bayesian Neural Networks
164,163,55,163_depressive_depression_tweets_depressed,"['depressive', 'depression', 'tweets', 'depressed', 'facebook', 'depressionemo', 'stressors', 'twitter', 'nlp', 'annotation']","['depression', 'mental', 'suicide', 'suicidal', 'media', 'anxiety', 'health', 'depressive', 'social', 'disorders']","['depressive', 'tweets', 'stressors', 'annotation', 'suicidality', 'mental', 'cnn', 'bipolar', 'ptsd', 'interventions']","['  We introduce a multi-layer perceptron (MLP) called the COVID-19 Depression\nand Anxiety Predictor (CoDAP) to predict mental health trends, particularly\nanxiety and depression, during the COVID-19 pandemic. Our method utilizes a\ncomprehensive dataset, which tracked mental health symptoms weekly over ten\nweeks during the initial COVID-19 wave (April to June 2020) in a diverse cohort\nof U.S. adults. This period, characterized by a surge in mental health symptoms\nand conditions, offers a critical context for our analysis. Our focus was to\nextract and analyze patterns of anxiety and depression through a unique lens of\nqualitative individual attributes using CoDAP. This model not only predicts\npatterns of anxiety and depression during the pandemic but also unveils key\ninsights into the interplay of demographic factors, behavioral changes, and\nsocial determinants of mental health. These findings contribute to a more\nnuanced understanding of the complexity of mental health issues in times of\nglobal health crises, potentially guiding future early interventions.\n', '  This work explores the utilization of Romanized Sinhala social media data to\nidentify individuals at risk of depression. A machine learning-based framework\nis presented for the automatic screening of depression symptoms by analyzing\nlanguage patterns, sentiment, and behavioural cues within a comprehensive\ndataset of social media posts. The research has been carried out to compare the\nsuitability of Neural Networks over the classical machine learning techniques.\nThe proposed Neural Network with an attention layer which is capable of\nhandling long sequence data, attains a remarkable accuracy of 93.25% in\ndetecting depression symptoms, surpassing current state-of-the-art methods.\nThese findings underscore the efficacy of this approach in pinpointing\nindividuals in need of proactive interventions and support. Mental health\nprofessionals, policymakers, and social media companies can gain valuable\ninsights through the proposed model. Leveraging natural language processing\ntechniques and machine learning algorithms, this work offers a promising\npathway for mental health screening in the digital era. By harnessing the\npotential of social media data, the framework introduces a proactive method for\nrecognizing and assisting individuals at risk of depression. In conclusion,\nthis research contributes to the advancement of proactive interventions and\nsupport systems for mental health, thereby influencing both research and\npractical applications in the field.\n', ""  The COVID-19 pandemic has escalated mental health crises worldwide, with\nsocial isolation and economic instability contributing to a rise in suicidal\nbehavior. Suicide can result from social factors such as shame, abuse,\nabandonment, and mental health conditions like depression, Post-Traumatic\nStress Disorder (PTSD), Attention-Deficit/Hyperactivity Disorder (ADHD),\nanxiety disorders, and bipolar disorders. As these conditions develop, signs of\nsuicidal ideation may manifest in social media interactions. Analyzing social\nmedia data using artificial intelligence (AI) techniques can help identify\npatterns of suicidal behavior, providing invaluable insights for suicide\nprevention agencies, professionals, and broader community awareness\ninitiatives. Machine learning algorithms for this purpose require large volumes\nof accurately labeled data. Previous research has not fully explored the\npotential of incorporating explanations in analyzing and labeling longitudinal\nsocial media data. In this study, we employed a model explanation method, Layer\nIntegrated Gradients, on top of a fine-tuned state-of-the-art language model,\nto assign each token from Reddit users' posts an attribution score for\npredicting suicidal ideation. By extracting and analyzing attributions of\ntokens from the data, we propose a methodology for preliminary screening of\nsocial media posts for suicidal ideation without using large language models\nduring inference.\n""]",Mental Health and Social Media Analysis
165,164,54,164_signwriting_gestures_gesture_signbank,"['signwriting', 'gestures', 'gesture', 'signbank', 'sign', 'signllm', 'signcl', 'signclip', 'signers', 'signing']","['sign', 'translation', 'deaf', 'signs', 'hearing', 'signers', 'videos', 'language', 'languages', 'signer']","['signwriting', 'gestures', 'signclip', 'deaf', 'multilingual', 'annotations', 'signs', 'slvideo', 'cslr', 'fingerspelling']","['  Even for better-studied sign languages like American Sign Language (ASL),\ndata is the bottleneck for machine learning research. The situation is worse\nyet for the many other sign languages used by Deaf/Hard of Hearing communities\naround the world. In this paper, we present YouTube-SL-25, a large-scale,\nopen-domain multilingual corpus of sign language videos with seemingly\nwell-aligned captions drawn from YouTube. With >3000 hours of videos across >25\nsign languages, YouTube-SL-25 is a) >3x the size of YouTube-ASL, b) the largest\nparallel sign language dataset to date, and c) the first or largest parallel\ndataset for many of its component languages. We provide baselines for\nsign-to-text tasks using a unified multilingual multitask model based on T5 and\nreport scores on benchmarks across 4 sign languages. The results demonstrate\nthat multilingual transfer benefits both higher- and lower-resource sign\nlanguages within YouTube-SL-25.\n', '  Sign language understanding has made significant strides; however, there is\nstill no viable solution for generating sign sequences directly from entire\nspoken content, e.g., text or speech. In this paper, we propose a unified\nframework for continuous sign language production, easing communication between\nsign and non-sign language users. In particular, a sequence diffusion model,\nutilizing embeddings extracted from text or speech, is crafted to generate sign\npredictions step by step. Moreover, by creating a joint embedding space for\ntext, audio, and sign, we bind these modalities and leverage the semantic\nconsistency among them to provide informative feedback for the model training.\nThis embedding-consistency learning strategy minimizes the reliance on sign\ntriplets and ensures continuous model refinement, even with a missing audio\nmodality. Experiments on How2Sign and PHOENIX14T datasets demonstrate that our\nmodel achieves competitive performance in sign language production.\n', '  Sign Language Translation (SLT) is a challenging task that aims to translate\nsign videos into spoken language. Inspired by the strong translation\ncapabilities of large language models (LLMs) that are trained on extensive\nmultilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT.\nIn this paper, we regularize the sign videos to embody linguistic\ncharacteristics of spoken language, and propose a novel SignLLM framework to\ntransform sign videos into a language-like representation for improved\nreadability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The\nVector-Quantized Visual Sign module converts sign videos into a sequence of\ndiscrete character-level sign tokens, and (2) the Codebook Reconstruction and\nAlignment module converts these character-level tokens into word-level sign\nrepresentations using an optimal transport formulation. A sign-text alignment\nloss further bridges the gap between sign and text tokens, enhancing semantic\ncompatibility. We achieve state-of-the-art gloss-free results on two\nwidely-used SLT benchmarks.\n']",Sign Language Processing and Translation
166,165,54,165_prompts_prompt_prompting_promptwizard,"['prompts', 'prompt', 'prompting', 'promptwizard', 'instruction', 'language', 'automatically', 'automatic', 'feedback', 'tasks']","['prompt', 'prompts', 'engineering', 'prompting', 'instructions', 'optimization', 'instruction', 'automatic', 'feedback', 'editing']","['prompts', 'instruction', 'automatic', 'langgpt', 'optimizers', 'exemplars', 'promst', 'capabilities', 'pspem', 'template']","['  Large Language Models (LLMs) can perform various natural language processing\ntasks with suitable instruction prompts. However, designing effective prompts\nmanually is challenging and time-consuming. Existing methods for automatic\nprompt optimization either lack flexibility or efficiency. In this paper, we\npropose an effective approach to automatically select the optimal prompt for a\ngiven input from a finite set of synthetic candidate prompts. Our approach\nconsists of three steps: (1) clustering the training data and generating\ncandidate prompts for each cluster using an LLM-based prompt generator; (2)\nsynthesizing a dataset of input-prompt-output tuples for training a prompt\nevaluator to rank the prompts based on their relevance to the input; (3) using\nthe prompt evaluator to select the best prompt for a new input at test time.\nOur approach balances prompt generality-specificity and eliminates the need for\nresource-intensive training and inference. It demonstrates competitive\nperformance on zero-shot question-answering datasets: GSM8K, MultiArith, and\nAQuA.\n', '  Interaction with Large Language Models (LLMs) is primarily carried out via\nprompting. A prompt is a natural language instruction designed to elicit\ncertain behaviour or output from a model. In theory, natural language prompts\nenable non-experts to interact with and leverage LLMs. However, for complex\ntasks and tasks with specific requirements, prompt design is not trivial.\nCreating effective prompts requires skill and knowledge, as well as significant\niteration in order to determine model behavior, and guide the model to\naccomplish a particular goal. We hypothesize that the way in which users\niterate on their prompts can provide insight into how they think prompting and\nmodels work, as well as the kinds of support needed for more efficient prompt\nengineering. To better understand prompt engineering practices, we analyzed\nsessions of prompt editing behavior, categorizing the parts of prompts users\niterated on and the types of changes they made. We discuss design implications\nand future directions based on these prompt engineering practices.\n', ""  Prompt engineering is a challenging and important task due to the high\nsensitivity of Large Language Models (LLMs) to the given prompt and the\ninherent ambiguity of a textual task instruction. Automatic prompt engineering\nis essential to achieve optimized performance from LLMs. Recent studies have\ndemonstrated the capabilities of LLMs to automatically conduct prompt\nengineering by employing a meta-prompt that incorporates the outcomes of the\nlast trials and proposes an improved prompt. However, this requires a\nhigh-quality benchmark to compare different prompts, which is difficult and\nexpensive to acquire in many real-world use cases. In this work, we introduce a\nnew method for automatic prompt engineering, using a calibration process that\niteratively refines the prompt to the user intent. During the optimization\nprocess, the system jointly generates synthetic data of boundary use cases and\noptimizes the prompt according to the generated dataset. We demonstrate the\neffectiveness of our method with respect to strong proprietary models on\nreal-world tasks such as moderation and generation. Our method outperforms\nstate-of-the-art methods with a limited number of annotated samples.\nFurthermore, we validate the advantages of each one of the system's key\ncomponents. Our system is built in a modular way, facilitating easy adaptation\nto other tasks. The code is available\n$\\href{https://github.com/Eladlev/AutoPrompt}{here}$.\n""]","""Optimizing Language Model Prompts"""
167,166,54,166_wasserstein_distances_metric_distance,"['wasserstein', 'distances', 'metric', 'distance', 'measures', 'distributions', 'optimal', 'riemannian', 'dissimilarity', 'statistical']","['distance', 'sliced', 'distances', 'manifold', 'metric', 'measures', 'probability', 'transport', 'distributions', 'spaces']","['wasserstein', 'metric', 'distributions', 'riemannian', 'isometry', 'diffeomorphisms', 'stereographic', 'gromov', 'transport', 'barycenter']","[""  The $2$-Wasserstein distance is sensitive to minor geometric differences\nbetween distributions, making it a very powerful dissimilarity metric. However,\ndue to this sensitivity, a small outlier mass can also cause a significant\nincrease in the $2$-Wasserstein distance between two similar distributions.\nSimilarly, sampling discrepancy can cause the empirical $2$-Wasserstein\ndistance on $n$ samples in $\\mathbb{R}^2$ to converge to the true distance at a\nrate of $n^{-1/4}$, which is significantly slower than the rate of $n^{-1/2}$\nfor $1$-Wasserstein distance. We introduce a new family of distances\nparameterized by $k \\ge 0$, called $k$-RPW that is based on computing the\npartial $2$-Wasserstein distance. We show that (1) $k$-RPW satisfies the metric\nproperties, (2) $k$-RPW is robust to small outlier mass while retaining the\nsensitivity of $2$-Wasserstein distance to minor geometric differences, and (3)\nwhen $k$ is a constant, $k$-RPW distance between empirical distributions on $n$\nsamples in $\\mathbb{R}^2$ converges to the true distance at a rate of\n$n^{-1/3}$, which is faster than the convergence rate of $n^{-1/4}$ for the\n$2$-Wasserstein distance. Using the partial $p$-Wasserstein distance, we extend\nour distance to any $p \\in [1,\\infty]$. By setting parameters $k$ or $p$\nappropriately, we can reduce our distance to the total variation,\n$p$-Wasserstein, and the L\\'evy-Prokhorov distances. Experiments show that our\ndistance function achieves higher accuracy in comparison to the\n$1$-Wasserstein, $2$-Wasserstein, and TV distances for image retrieval tasks on\nnoisy real-world data sets.\n"", '  The sliced Wasserstein (SW) distances between two probability measures are\ndefined as the expectation of the Wasserstein distance between two\none-dimensional projections of the two measures. The randomness comes from a\nprojecting direction that is used to project the two input measures to one\ndimension. Due to the intractability of the expectation, Monte Carlo\nintegration is performed to estimate the value of the SW distance. Despite\nhaving various variants, there has been no prior work that improves the Monte\nCarlo estimation scheme for the SW distance in terms of controlling its\nvariance. To bridge the literature on variance reduction and the literature on\nthe SW distance, we propose computationally efficient control variates to\nreduce the variance of the empirical estimation of the SW distance. The key\nidea is to first find Gaussian approximations of projected one-dimensional\nmeasures, then we utilize the closed-form of the Wasserstein-2 distance between\ntwo Gaussian distributions to design the control variates. In particular, we\npropose using a lower bound and an upper bound of the Wasserstein-2 distance\nbetween two fitted Gaussians as two computationally efficient control variates.\nWe empirically show that the proposed control variate estimators can help to\nreduce the variance considerably when comparing measures over images and\npoint-clouds. Finally, we demonstrate the favorable performance of the proposed\ncontrol variate estimators in gradient flows to interpolate between two\npoint-clouds and in deep generative modeling on standard image datasets, such\nas CIFAR10 and CelebA.\n', '  While many Machine Learning methods were developed or transposed on\nRiemannian manifolds to tackle data with known non Euclidean geometry, Optimal\nTransport (OT) methods on such spaces have not received much attention. The\nmain OT tool on these spaces is the Wasserstein distance which suffers from a\nheavy computational burden. On Euclidean spaces, a popular alternative is the\nSliced-Wasserstein distance, which leverages a closed-form solution of the\nWasserstein distance in one dimension, but which is not readily available on\nmanifolds. In this work, we derive general constructions of Sliced-Wasserstein\ndistances on Cartan-Hadamard manifolds, Riemannian manifolds with non-positive\ncurvature, which include among others Hyperbolic spaces or the space of\nSymmetric Positive Definite matrices. Then, we propose different applications.\nAdditionally, we derive non-parametric schemes to minimize these new distances\nby approximating their Wasserstein gradient flows.\n']",Wasserstein Distance and Optimal Transport
168,167,54,167_actions_action_multimodal_activities,"['actions', 'action', 'multimodal', 'activities', 'pose', 'videos', 'recognition', 'humanml3d', 'actors', 'features']","['action', 'egocentric', 'recognition', 'video', 'exocentric', 'motion', 'videos', 'frames', 'temporal', 'human']","['multimodal', 'activities', 'pose', 'videos', 'humanml3d', 'motions', 'dataset', 'actor', 'convolutional', 'egocentric']","['  The fine-grained action analysis of the existing action datasets is\nchallenged by insufficient action categories, low fine granularities, limited\nmodalities, and tasks. In this paper, we propose a Multi-modality and\nMulti-task dataset of Figure Skating (MMFS) which was collected from the World\nFigure Skating Championships. MMFS, which possesses action recognition and\naction quality assessment, captures RGB, skeleton, and is collected the score\nof actions from 11671 clips with 256 categories including spatial and temporal\nlabels. The key contributions of our dataset fall into three aspects as\nfollows. (1) Independently spatial and temporal categories are first proposed\nto further explore fine-grained action recognition and quality assessment. (2)\nMMFS first introduces the skeleton modality for complex fine-grained action\nquality assessment. (3) Our multi-modality and multi-task dataset encourage\nmore action analysis models. To benchmark our dataset, we adopt RGB-based and\nskeleton-based baseline methods for action recognition and action quality\nassessment.\n', '  Supervised and self-supervised learning are two main training paradigms for\nskeleton-based human action recognition. However, the former one-hot\nclassification requires labor-intensive predefined action categories\nannotations, while the latter involves skeleton transformations (e.g.,\ncropping) in the pretext tasks that may impair the skeleton structure. To\naddress these challenges, we introduce a novel skeleton-based training\nframework (C$^2$VL) based on Cross-modal Contrastive learning that uses the\nprogressive distillation to learn task-agnostic human skeleton action\nrepresentation from the Vision-Language knowledge prompts. Specifically, we\nestablish the vision-language action concept space through vision-language\nknowledge prompts generated by pre-trained large multimodal models (LMMs),\nwhich enrich the fine-grained details that the skeleton action space lacks.\nMoreover, we propose the intra-modal self-similarity and inter-modal\ncross-consistency softened targets in the cross-modal contrastive process to\nprogressively control and guide the degree of pulling vision-language knowledge\nprompts and corresponding skeletons closer. These soft instance discrimination\nand self-knowledge distillation strategies contribute to the learning of better\nskeleton-based action representations from the noisy skeleton-vision-language\npairs. During the inference phase, our method requires only the skeleton data\nas the input for action recognition and no longer for vision-language prompts.\nExtensive experiments show that our method achieves state-of-the-art results on\nNTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets. The code will be available\nin the future.\n', '  One-shot skeleton action recognition, which aims to learn a skeleton action\nrecognition model with a single training sample, has attracted increasing\ninterest due to the challenge of collecting and annotating large-scale skeleton\naction data. However, most existing studies match skeleton sequences by\ncomparing their feature vectors directly which neglects spatial structures and\ntemporal orders of skeleton data. This paper presents a novel one-shot skeleton\naction recognition technique that handles skeleton action recognition via\nmulti-scale spatial-temporal feature matching. We represent skeleton data at\nmultiple spatial and temporal scales and achieve optimal feature matching from\ntwo perspectives. The first is multi-scale matching which captures the\nscale-wise semantic relevance of skeleton data at multiple spatial and temporal\nscales simultaneously. The second is cross-scale matching which handles\ndifferent motion magnitudes and speeds by capturing sample-wise relevance\nacross multiple scales. Extensive experiments over three large-scale datasets\n(NTU RGB+D, NTU RGB+D 120, and PKU-MMD) show that our method achieves superior\none-shot skeleton action recognition, and it outperforms the state-of-the-art\nconsistently by large margins.\n']",Human Action Recognition in Videos
169,168,54,168_sparse_expert_experts_moe,"['sparse', 'expert', 'experts', 'moe', 'models', 'specialization', 'sparsely', 'moerging', 'training', 'moes']","['experts', 'routing', 'expert', 'mixture', 'routers', 'token', 'tokens', 'sparse', 'dense', 'specialization']","['sparse', 'experts', 'moe', 'specialization', 'underfitting', 'capacity', 'multitask', 'deepseekmoe', 'language', 'activation']","['  Mixture-of-Expert (MoE) based large language models (LLMs), such as the\nrecent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size\nwithout suffering from the quadratic growth of training cost of dense\ntransformers. Like dense models, training MoEs requires answering the same\nquestion: given a training budget, what is the optimal allocation on the model\nsize and number of tokens? We study the scaling law of MoE-based LLMs regarding\nthe relations between the model performance, model size, dataset size, and the\nexpert degree. Echoing previous research studying MoE in different contexts, we\nobserve the diminishing return of increasing the number of experts, but this\nseems to suggest we should scale the number of experts until saturation, as the\ntraining cost would remain constant, which is problematic during inference\ntime. We propose to amend the scaling law of MoE by introducing inference\nefficiency as another metric besides the validation loss. We find that MoEs\nwith a few (4/8) experts are the most serving efficient solution under the same\nperformance, but costs 2.5-3.5x more in training. On the other hand, training a\n(16/32) expert MoE much smaller (70-85%) than the loss-optimal solution, but\nwith a larger training dataset is a promising setup under a training budget.\n', '  Mixture-of-experts (MoE) is gaining increasing attention due to its unique\nproperties and remarkable performance, especially for language tasks. By\nsparsely activating a subset of parameters for each token, MoE architecture\ncould increase the model size without sacrificing computational efficiency,\nachieving a better trade-off between performance and training costs. However,\nthe underlying mechanism of MoE still lacks further exploration, and its\nmodularization degree remains questionable. In this paper, we make an initial\nattempt to understand the inner workings of MoE-based large language models.\nConcretely, we comprehensively study the parametric and behavioral features of\nthree recent MoE-based models and reveal some intriguing observations,\nincluding (1) Neurons act like fine-grained experts. (2) The router of MoE\nusually selects experts with larger output norms. (3) The expert diversity\nincreases as the layer increases, while the last layer is an outlier. Based on\nthe observations, we also provide suggestions for a broad spectrum of MoE\npractitioners, such as router design and expert allocation. We hope this work\ncould shed light on future research on the MoE framework and other modular\narchitectures. Code is available at\nhttps://github.com/kamanphoebe/Look-into-MoEs.\n', ""  Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates\npredictions from several specialized sub-models (referred to as experts). This\nfusion is accomplished through a router mechanism, dynamically assigning\nweights to each expert's contribution based on the input data. Conventional MoE\nmechanisms select all available experts, incurring substantial computational\ncosts. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages\nonly a limited number, or even just one expert, significantly reducing\ncomputation overhead while empirically preserving, and sometimes even\nenhancing, performance. Despite its wide-ranging applications and these\nadvantageous characteristics, MoE's theoretical underpinnings have remained\nelusive. In this paper, we embark on an exploration of Sparse MoE's\ngeneralization error concerning various critical factors. Specifically, we\ninvestigate the impact of the number of data samples, the total number of\nexperts, the sparsity in expert selection, the complexity of the routing\nmechanism, and the complexity of individual experts. Our analysis sheds light\non \\textit{how \\textbf{sparsity} contributes to the MoE's generalization},\noffering insights from the perspective of classical learning theory.\n""]",Mixture of Experts (MoE) in Large Language Models
170,169,53,169_cybersecurity_cyberattacks_cyberattack_cybercrime,"['cybersecurity', 'cyberattacks', 'cyberattack', 'cybercrime', 'llm4security', 'cybermetric', 'security', 'threats', 'malware', 'threat']","['cybersecurity', 'cyber', 'threat', 'security', 'threats', 'attack', 'cybercrime', 'vulnerabilities', 'intelligence', 'reports']","['cybersecurity', 'cyberattacks', 'llm4security', 'cybermetric', 'malware', 'phishing', 'chatbots', 'vulnerability', 'tactics', 'capabilities']","[""  Cyber threat intelligence (CTI) is crucial in today's cybersecurity\nlandscape, providing essential insights to understand and mitigate the\never-evolving cyber threats. The recent rise of Large Language Models (LLMs)\nhave shown potential in this domain, but concerns about their reliability,\naccuracy, and hallucinations persist. While existing benchmarks provide general\nevaluations of LLMs, there are no benchmarks that address the practical and\napplied aspects of CTI-specific tasks. To bridge this gap, we introduce\nCTIBench, a benchmark designed to assess LLMs' performance in CTI applications.\nCTIBench includes multiple datasets focused on evaluating knowledge acquired by\nLLMs in the cyber-threat landscape. Our evaluation of several state-of-the-art\nmodels on these tasks provides insights into their strengths and weaknesses in\nCTI contexts, contributing to a better understanding of LLM capabilities in\nCTI.\n"", '  This paper provides a comprehensive review of the future of cybersecurity\nthrough Generative AI and Large Language Models (LLMs). We explore LLM\napplications across various domains, including hardware design security,\nintrusion detection, software engineering, design verification, cyber threat\nintelligence, malware detection, and phishing detection. We present an overview\nof LLM evolution and its current state, focusing on advancements in models such\nas GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends\nto LLM vulnerabilities, such as prompt injection, insecure output handling,\ndata poisoning, DDoS attacks, and adversarial instructions. We delve into\nmitigation strategies to protect these models, providing a comprehensive look\nat potential attack scenarios and prevention techniques. Furthermore, we\nevaluate the performance of 42 LLM models in cybersecurity knowledge and\nhardware security, highlighting their strengths and weaknesses. We thoroughly\nevaluate cybersecurity datasets for LLM training and testing, covering the\nlifecycle from data creation to usage and identifying gaps for future research.\nIn addition, we review new strategies for leveraging LLMs, including techniques\nlike Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human\nFeedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank\nAdapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim\nto enhance real-time cybersecurity defenses and improve the sophistication of\nLLM applications in threat detection and response. Our paper provides a\nfoundational understanding and strategic direction for integrating LLMs into\nfuture cybersecurity frameworks, emphasizing innovation and robust model\ndeployment to safeguard against evolving cyber threats.\n', '  Cybersecurity researchers have contributed to the automated extraction of CTI\nfrom textual sources, such as threat reports and online articles, where\ncyberattack strategies, procedures, and tools are described. The goal of this\narticle is to aid cybersecurity researchers understand the current techniques\nused for cyberthreat intelligence extraction from text through a survey of\nrelevant studies in the literature. We systematically collect ""CTI extraction\nfrom text""-related studies from the literature and categorize the CTI\nextraction purposes. We propose a CTI extraction pipeline abstracted from these\nstudies. We identify the data sources, techniques, and CTI sharing formats\nutilized in the context of the proposed pipeline. Our work finds ten types of\nextraction purposes, such as extraction indicators of compromise extraction,\nTTPs (tactics, techniques, procedures of attack), and cybersecurity keywords.\nWe also identify seven types of textual sources for CTI extraction, and textual\ndata obtained from hacker forums, threat reports, social media posts, and\nonline news articles have been used by almost 90% of the studies. Natural\nlanguage processing along with both supervised and unsupervised machine\nlearning techniques such as named entity recognition, topic modelling,\ndependency parsing, supervised classification, and clustering are used for CTI\nextraction. We observe the technical challenges associated with these studies\nrelated to obtaining available clean, labelled data which could assure\nreplication, validation, and further extension of the studies. As we find the\nstudies focusing on CTI information extraction from text, we advocate for\nbuilding upon the current CTI extraction work to help cybersecurity\npractitioners with proactive decision making such as threat prioritization,\nautomated threat modelling to utilize knowledge from past cybersecurity\nincidents.\n']",Cybersecurity and Artificial Intelligence
171,170,53,170_counterfactuals_counterfactual_explanations_robustness,"['counterfactuals', 'counterfactual', 'explanations', 'robustness', 'explaining', 'interpretable', 'learners', 'predictions', 'robust', 'algorithmic']","['counterfactual', 'explanations', 'counterfactuals', 'actionable', 'recourse', 'outcome', 'change', 'changes', 'explanation', 'plausibility']","['counterfactuals', 'explanations', 'robustness', 'learners', 'algorithmic', 'ensembling', 'plausibility', 'instances', 'trust', 'cfgs']","['  Counterfactual explanations describe how to modify a feature vector in order\nto flip the outcome of a trained classifier. Obtaining robust counterfactual\nexplanations is essential to provide valid algorithmic recourse and meaningful\nexplanations. We study the robustness of explanations of randomized ensembles,\nwhich are always subject to algorithmic uncertainty even when the training data\nis fixed. We formalize the generation of robust counterfactual explanations as\na probabilistic problem and show the link between the robustness of ensemble\nmodels and the robustness of base learners. We develop a practical method with\ngood empirical performance and support it with theoretical guarantees for\nensembles of convex base learners. Our results show that existing methods give\nsurprisingly low robustness: the validity of naive counterfactuals is below\n$50\\%$ on most data sets and can fall to $20\\%$ on problems with many features.\nIn contrast, our method achieves high robustness with only a small increase in\nthe distance from counterfactual explanations to their initial observations.\n', '  The accuracy and understandability of bank failure prediction models are\ncrucial. While interpretable models like logistic regression are favored for\ntheir explainability, complex models such as random forest, support vector\nmachines, and deep learning offer higher predictive performance but lower\nexplainability. These models, known as black boxes, make it difficult to derive\nactionable insights. To address this challenge, using counterfactual\nexplanations is suggested. These explanations demonstrate how changes in input\nvariables can alter the model output and suggest ways to mitigate bank failure\nrisk. The key challenge lies in selecting the most effective method for\ngenerating useful counterfactuals, which should demonstrate validity,\nproximity, sparsity, and plausibility. The paper evaluates several\ncounterfactual generation methods: WhatIf, Multi Objective, and Nearest\nInstance Counterfactual Explanation, and also explores resampling methods like\nundersampling, oversampling, SMOTE, and the cost sensitive approach to address\ndata imbalance in bank failure prediction in the US. The results indicate that\nthe Nearest Instance Counterfactual Explanation method yields higher quality\ncounterfactual explanations, mainly using the cost sensitive approach. Overall,\nthe Multi Objective Counterfactual and Nearest Instance Counterfactual\nExplanation methods outperform others regarding validity, proximity, and\nsparsity metrics, with the cost sensitive approach providing the most desirable\ncounterfactual explanations. These findings highlight the variability in the\nperformance of counterfactual generation methods across different balancing\nstrategies and machine learning models, offering valuable strategies to enhance\nthe utility of black box bank failure prediction models.\n', ""  In the past decade, we have experienced a massive boom in the usage of\ndigital solutions in higher education. Due to this boom, large amounts of data\nhave enabled advanced data analysis methods to support learners and examine\nlearning processes. One of the dominant research directions in learning\nanalytics is predictive modeling of learners' success using various machine\nlearning methods. To build learners' and teachers' trust in such methods and\nsystems, exploring the methods and methodologies that enable relevant\nstakeholders to deeply understand the underlying machine-learning models is\nnecessary. In this context, counterfactual explanations from explainable\nmachine learning tools are promising. Several counterfactual generation methods\nhold much promise, but the features must be actionable and causal to be\neffective. Thus, obtaining which counterfactual generation method suits the\nstudent success prediction models in terms of desiderata, stability, and\nrobustness is essential. Although a few studies have been published in recent\nyears on the use of counterfactual explanations in educational sciences, they\nhave yet to discuss which counterfactual generation method is more suitable for\nthis problem. This paper analyzed the effectiveness of commonly used\ncounterfactual generation methods, such as WhatIf Counterfactual Explanations,\nMulti-Objective Counterfactual Explanations, and Nearest Instance\nCounterfactual Explanations after balancing. This contribution presents a case\nstudy using the Open University Learning Analytics dataset to demonstrate the\npractical usefulness of counterfactual explanations. The results illustrate the\nmethod's effectiveness and describe concrete steps that could be taken to alter\nthe model's prediction.\n""]",Counterfactual Explanations for Machine Learning Models
172,171,53,171_imitation_reinforcement_mimic_imitate,"['imitation', 'reinforcement', 'mimic', 'imitate', 'demonstrations', 'adversarial', 'learning', 'learned', 'exploration', 'actions']","['imitation', 'expert', 'demonstrations', 'cloning', 'policy', 'reward', 'demonstration', 'states', 'policies', 'offline']","['imitation', 'reinforcement', 'mimic', 'adversarial', 'exploration', 'interactive', 'robotic', 'agent', 'demonstration', 'imitative']","['  In offline Imitation Learning (IL), one of the main challenges is the\n\\textit{covariate shift} between the expert observations and the actual\ndistribution encountered by the agent, because it is difficult to determine\nwhat action an agent should take when outside the state distribution of the\nexpert demonstrations. Recently, the model-free solutions introduce the\nsupplementary data and identify the latent expert-similar samples to augment\nthe reliable samples during learning. Model-based solutions build forward\ndynamic models with conservatism quantification and then generate additional\ntrajectories in the neighborhood of expert demonstrations. However, without\nreward supervision, these methods are often over-conservative in the\nout-of-expert-support regions, because only in states close to expert-observed\nstates can there be a preferred action enabling policy optimization. To\nencourage more exploration on expert-unobserved states, we propose a novel\nmodel-based framework, called offline Imitation Learning with Self-paced\nReverse Augmentation (SRA). Specifically, we build a reverse dynamic model from\nthe offline demonstrations, which can efficiently generate trajectories leading\nto the expert-observed states in a self-paced style. Then, we use the\nsubsequent reinforcement learning method to learn from the augmented\ntrajectories and transit from expert-unobserved states to expert-observed\nstates. This framework not only explores the expert-unobserved states but also\nguides maximizing long-term returns on these states, ultimately enabling\ngeneralization beyond the expert data. Empirical results show that our proposal\ncould effectively mitigate the covariate shift and achieve the state-of-the-art\nperformance on the offline imitation learning benchmarks. Project website:\n\\url{https://www.lamda.nju.edu.cn/shaojj/KDD24_SRA/}.\n', ""  Imitation learning, in which learning is performed by demonstration, has been\nstudied and advanced for sequential decision-making tasks in which a reward\nfunction is not predefined. However, imitation learning methods still require\nnumerous expert demonstration samples to successfully imitate an expert's\nbehavior. To improve sample efficiency, we utilize self-supervised\nrepresentation learning, which can generate vast training signals from the\ngiven data. In this study, we propose a self-supervised representation-based\nadversarial imitation learning method to learn state and action representations\nthat are robust to diverse distortions and temporally predictive, on non-image\ncontrol tasks. In particular, in comparison with existing self-supervised\nlearning methods for tabular data, we propose a different corruption method for\nstate and action representations that is robust to diverse distortions. We\ntheoretically and empirically observe that making an informative feature\nmanifold with less sample complexity significantly improves the performance of\nimitation learning. The proposed method shows a 39% relative improvement over\nexisting adversarial imitation learning methods on MuJoCo in a setting limited\nto 100 expert state-action pairs. Moreover, we conduct comprehensive ablations\nand additional experiments using demonstrations with varying optimality to\nprovide insights into a range of factors.\n"", '  Imitation learning is often used in addition to reinforcement learning in\nenvironments where reward design is difficult or where the reward is sparse,\nbut it is difficult to be able to imitate well in unknown states from a small\namount of expert data and sampling data. Supervised learning methods such as\nBehavioral Cloning do not require sampling data, but usually suffer from\ndistribution shift. The methods based on reinforcement learning, such as\ninverse reinforcement learning and Generative Adversarial imitation learning\n(GAIL), can learn from only a few expert data. However, they often need to\ninteract with the environment. Soft Q imitation learning (SQIL) addressed the\nproblems, and it was shown that it could learn efficiently by combining\nBehavioral Cloning and soft Q-learning with constant rewards. In order to make\nthis algorithm more robust to distribution shift, we propose more efficient and\nrobust algorithm by adding to this method a reward function based on\nadversarial inverse reinforcement learning that rewards the agent for\nperforming actions in status similar to the demo. We call this algorithm\nDiscriminator Soft Q Imitation Learning (DSQIL). We evaluated it on MuJoCo\nenvironments.\n']",Imitation Learning and Reinforcement
173,172,52,172_imagenet_neural_supernet_architecture,"['imagenet', 'neural', 'supernet', 'architecture', 'architectures', 'nas', 'searching', 'networks', 'architectural', 'supernets']","['search', 'architecture', 'architectures', 'proxies', 'neural', 'evolutionary', 'supernet', 'objective', 'cost', 'hardware']","['imagenet', 'neural', 'supernet', 'architectures', 'searching', 'opennas', 'nasgraph', 'metaheuristics', 'mobilenetv3', 'imb']","['  Efficient evaluation of a network architecture drawn from a large search\nspace remains a key challenge in Neural Architecture Search (NAS). Vanilla NAS\nevaluates each architecture by training from scratch, which gives the true\nperformance but is extremely time-consuming. Recently, one-shot NAS\nsubstantially reduces the computation cost by training only one supernetwork,\na.k.a. supernet, to approximate the performance of every architecture in the\nsearch space via weight-sharing. However, the performance estimation can be\nvery inaccurate due to the co-adaption among operations. In this paper, we\npropose few-shot NAS that uses multiple supernetworks, called sub-supernet,\neach covering different regions of the search space to alleviate the undesired\nco-adaption. Compared to one-shot NAS, few-shot NAS improves the accuracy of\narchitecture evaluation with a small increase of evaluation cost. With only up\nto 7 sub-supernets, few-shot NAS establishes new SoTAs: on ImageNet, it finds\nmodels that reach 80.5% top-1 accuracy at 600 MB FLOPS and 77.5% top-1 accuracy\nat 238 MFLOPS; on CIFAR10, it reaches 98.72% top-1 accuracy without using extra\ndata or transfer learning. In Auto-GAN, few-shot NAS outperforms the previously\npublished results by up to 20%. Extensive experiments show that few-shot NAS\nsignificantly improves various one-shot methods, including 4 gradient-based and\n6 search-based methods on 3 different tasks in NasBench-201 and\nNasBench1-shot-1.\n', ""  The paper provides a comprehensive overview of Neural Architecture Search\n(NAS), emphasizing its evolution from manual design to automated,\ncomputationally-driven approaches. It covers the inception and growth of NAS,\nhighlighting its application across various domains, including medical imaging\nand natural language processing. The document details the shift from\nexpert-driven design to algorithm-driven processes, exploring initial\nmethodologies like reinforcement learning and evolutionary algorithms. It also\ndiscusses the challenges of computational demands and the emergence of\nefficient NAS methodologies, such as Differentiable Architecture Search and\nhardware-aware NAS. The paper further elaborates on NAS's application in\ncomputer vision, NLP, and beyond, demonstrating its versatility and potential\nfor optimizing neural network architectures across different tasks. Future\ndirections and challenges, including computational efficiency and the\nintegration with emerging AI domains, are addressed, showcasing NAS's dynamic\nnature and its continued evolution towards more sophisticated and efficient\narchitecture search methods.\n"", '  Neural architecture search (NAS) has become a key component of AutoML and a\nstandard tool to automate the design of deep neural networks. Recently,\ntraining-free NAS as an emerging paradigm has successfully reduced the search\ncosts of standard training-based NAS by estimating the true architecture\nperformance with only training-free metrics. Nevertheless, the estimation\nability of these metrics typically varies across different tasks, making it\nchallenging to achieve robust and consistently good search performance on\ndiverse tasks with only a single training-free metric. Meanwhile, the\nestimation gap between training-free metrics and the true architecture\nperformances limits training-free NAS to achieve superior performance. To\naddress these challenges, we propose the robustifying and boosting\ntraining-free NAS (RoBoT) algorithm which (a) employs the optimized combination\nof existing training-free metrics explored from Bayesian optimization to\ndevelop a robust and consistently better-performing metric on diverse tasks,\nand (b) applies greedy search, i.e., the exploitation, on the newly developed\nmetric to bridge the aforementioned gap and consequently to boost the search\nperformance of standard training-free NAS further. Remarkably, the expected\nperformance of our RoBoT can be theoretically guaranteed, which improves over\nthe existing training-free NAS under mild conditions with additional\ninteresting insights. Our extensive experiments on various NAS benchmark tasks\nyield substantial empirical evidence to support our theoretical results.\n']",Neural Architecture Search
174,173,52,173_biometrics_biometric_faces_facial,"['biometrics', 'biometric', 'faces', 'facial', 'recognition', 'facemask', 'face', 'classifiers', 'features', 'siamese']","['face', 'facial', 'iris', 'biometric', 'recognition', 'faces', 'biometrics', 'images', 'identity', 'age']","['biometrics', 'facemask', 'siamese', 'iris', 'classifier', 'attributes', 'morphing', 'encoder', 'verification', 'smiling']","['  Computer vision systems have been deployed in various applications involving\nbiometrics like human faces. These systems can identify social media users,\nsearch for missing persons, and verify identity of individuals. While computer\nvision models are often evaluated for accuracy on available benchmarks, more\nannotated data is necessary to learn about their robustness and fairness\nagainst semantic distributional shifts in input data, especially in face data.\nAmong annotated data, counterfactual examples grant strong explainability\ncharacteristics. Because collecting natural face data is prohibitively\nexpensive, we put forth a generative AI-based framework to construct targeted,\ncounterfactual, high-quality synthetic face data. Our synthetic data pipeline\nhas many use cases, including face recognition systems sensitivity evaluations\nand image understanding system probes. The pipeline is validated with multiple\nuser studies. We showcase the efficacy of our face generation pipeline on a\nleading commercial vision model. We identify facial attributes that cause\nvision systems to fail.\n', '  The development of facial biometric systems has contributed greatly to the\ndevelopment of the computer vision field. Nowadays, there\'s always a need to\ndevelop a multimodal system that combines multiple biometric traits in an\nefficient, meaningful way. In this paper, we introduce ""IdentiFace"" which is a\nmultimodal facial biometric system that combines the core of facial recognition\nwith some of the most important soft biometric traits such as gender, face\nshape, and emotion. We also focused on developing the system using only VGG-16\ninspired architecture with minor changes across different subsystems. This\nunification allows for simpler integration across modalities. It makes it\neasier to interpret the learned features between the tasks which gives a good\nindication about the decision-making process across the facial modalities and\npotential connection. For the recognition problem, we acquired a 99.2% test\naccuracy for five classes with high intra-class variations using data collected\nfrom the FERET database[1]. We achieved 99.4% on our dataset and 95.15% on the\npublic dataset[2] in the gender recognition problem. We were also able to\nachieve a testing accuracy of 88.03% in the face-shape problem using the\ncelebrity face-shape dataset[3]. Finally, we achieved a decent testing accuracy\nof 66.13% in the emotion task which is considered a very acceptable accuracy\ncompared to related work on the FER2013 dataset[4].\n', '  In this work we focus on learning facial representations that can be adapted\nto train effective face recognition models, particularly in the absence of\nlabels. Firstly, compared with existing labelled face datasets, a vastly larger\nmagnitude of unlabeled faces exists in the real world. We explore the learning\nstrategy of these unlabeled facial images through self-supervised pretraining\nto transfer generalized face recognition performance. Moreover, motivated by\none recent finding, that is, the face saliency area is critical for face\nrecognition, in contrast to utilizing random cropped blocks of images for\nconstructing augmentations in pretraining, we utilize patches localized by\nextracted facial landmarks. This enables our method - namely LAndmark-based\nFacial Self-supervised learning LAFS), to learn key representation that is more\ncritical for face recognition. We also incorporate two landmark-specific\naugmentations which introduce more diversity of landmark information to further\nregularize the learning. With learned landmark-based facial representations, we\nfurther adapt the representation for face recognition with regularization\nmitigating variations in landmark positions. Our method achieves significant\nimprovement over the state-of-the-art on multiple face recognition benchmarks,\nespecially on more challenging few-shot scenarios.\n']",Facial Biometrics and Recognition
175,174,52,174_sentiment_sentiments_nlp_tweets,"['sentiment', 'sentiments', 'nlp', 'tweets', 'lexicons', 'sentimentality', 'twitter', 'subjectivity', 'lexicon', 'texts']","['sentiment', 'analysis', 'sentiments', 'comments', 'languages', 'airline', 'classification', 'tweet', 'tweets', 'macro']","['sentiment', 'tweets', 'lexicons', 'sentimentality', 'subjectivity', 'valence', 'classification', 'multilingual', 'lstm', 'polarity']","['  Sentiment analysis plays a pivotal role in understanding public opinion,\nparticularly in the political domain where the portrayal of entities in news\narticles influences public perception. In this paper, we investigate the\neffectiveness of Large Language Models (LLMs) in predicting entity-specific\nsentiment from political news articles. Leveraging zero-shot and few-shot\nstrategies, we explore the capability of LLMs to discern sentiment towards\npolitical entities in news content. Employing a chain-of-thought (COT) approach\naugmented with rationale in few-shot in-context learning, we assess whether\nthis method enhances sentiment prediction accuracy. Our evaluation on\nsentiment-labeled datasets demonstrates that LLMs, outperform fine-tuned BERT\nmodels in capturing entity-specific sentiment. We find that learning in-context\nsignificantly improves model performance, while the self-consistency mechanism\nenhances consistency in sentiment prediction. Despite the promising results, we\nobserve inconsistencies in the effectiveness of the COT prompting method.\nOverall, our findings underscore the potential of LLMs in entity-centric\nsentiment analysis within the political news domain and highlight the\nimportance of suitable prompting strategies and model architectures.\n', ""  In sentiment analysis of longer texts, there may be a variety of topics\ndiscussed, of entities mentioned, and of sentiments expressed regarding each\nentity. We find a lack of studies exploring how such texts express their\nsentiment towards each entity of interest, and how these sentiments can be\nmodelled. In order to better understand how sentiment regarding persons and\norganizations (each entity in our scope) is expressed in longer texts, we have\ncollected a dataset of expert annotations where the overall sentiment regarding\neach entity is identified, together with the sentence-level sentiment for these\nentities separately. We show that the reader's perceived sentiment regarding an\nentity often differs from an arithmetic aggregation of sentiments at the\nsentence level. Only 70\\% of the positive and 55\\% of the negative entities\nreceive a correct overall sentiment label when we aggregate the\n(human-annotated) sentiment labels for the sentences where the entity is\nmentioned. Our dataset reveals the complexity of entity-specific sentiment in\nlonger texts, and allows for more precise modelling and evaluation of such\nsentiment expressions.\n"", '  With the rapid development of natural language processing (NLP) technology,\nlarge-scale pre-trained language models such as GPT-3 have become a popular\nresearch object in NLP field. This paper aims to explore sentiment analysis\noptimization techniques based on large pre-trained language models such as\nGPT-3 to improve model performance and effect and further promote the\ndevelopment of natural language processing (NLP). By introducing the importance\nof sentiment analysis and the limitations of traditional methods, GPT-3 and\nFine-tuning techniques are introduced in this paper, and their applications in\nsentiment analysis are explained in detail. The experimental results show that\nthe Fine-tuning technique can optimize GPT-3 model and obtain good performance\nin sentiment analysis task. This study provides an important reference for\nfuture sentiment analysis using large-scale language models.\n']",Sentiment Analysis in Texts
176,175,52,175_privacy_federated_datasets_learning,"['privacy', 'federated', 'datasets', 'learning', 'personalized', 'distributed', 'sharing', 'data', 'collaborative', 'hospital']","['federated', 'healthcare', 'medical', 'privacy', 'patient', 'heterogeneity', 'clinical', 'hospitals', 'clients', 'centralized']","['privacy', 'federated', 'datasets', 'personalized', 'distributed', 'collaborative', 'hospitals', 'biomarkers', 'svm', 'mri']","[""  Machine learning (ML) and Artificial Intelligence (AI) have fueled remarkable\nadvancements, particularly in healthcare. Within medical imaging, ML models\nhold the promise of improving disease diagnoses, treatment planning, and\npost-treatment monitoring. Various computer vision tasks like image\nclassification, object detection, and image segmentation are poised to become\nroutine in clinical analysis. However, privacy concerns surrounding patient\ndata hinder the assembly of large training datasets needed for developing and\ntraining accurate, robust, and generalizable models. Federated Learning (FL)\nemerges as a compelling solution, enabling organizations to collaborate on ML\nmodel training by sharing model training information (gradients) rather than\ndata (e.g., medical images). FL's distributed learning framework facilitates\ninter-institutional collaboration while preserving patient privacy. However,\nFL, while robust in privacy preservation, faces several challenges. Sensitive\ninformation can still be gleaned from shared gradients that are passed on\nbetween organizations during model training. Additionally, in medical imaging,\nquantifying model confidence\\uncertainty accurately is crucial due to the noise\nand artifacts present in the data. Uncertainty estimation in FL encounters\nunique hurdles due to data heterogeneity across organizations. This paper\noffers a comprehensive review of FL, privacy preservation, and uncertainty\nestimation, with a focus on medical imaging. Alongside a survey of current\nresearch, we identify gaps in the field and suggest future directions for FL\nresearch to enhance privacy and address noisy medical imaging data challenges.\n"", ""  Data privacy has become a major concern in healthcare due to the increasing\ndigitization of medical records and data-driven medical research. Protecting\nsensitive patient information from breaches and unauthorized access is\ncritical, as such incidents can have severe legal and ethical complications.\nFederated Learning (FL) addresses this concern by enabling multiple healthcare\ninstitutions to collaboratively learn from decentralized data without sharing\nit. FL's scope in healthcare covers areas such as disease prediction, treatment\ncustomization, and clinical trial research. However, implementing FL poses\nchallenges, including model convergence in non-IID (independent and identically\ndistributed) data environments, communication overhead, and managing\nmulti-institutional collaborations. A systematic review of FL in healthcare is\nnecessary to evaluate how effectively FL can provide privacy while maintaining\nthe integrity and usability of medical data analysis. In this study, we analyze\nexisting literature on FL applications in healthcare. We explore the current\nstate of model security practices, identify prevalent challenges, and discuss\npractical applications and their implications. Additionally, the review\nhighlights promising future research directions to refine FL implementations,\nenhance data security protocols, and expand FL's use to broader healthcare\napplications, which will benefit future researchers and practitioners.\n"", '  Distributed training can facilitate the processing of large medical image\ndatasets, and improve the accuracy and efficiency of disease diagnosis while\nprotecting patient privacy, which is crucial for achieving efficient medical\nimage analysis and accelerating medical research progress. This paper presents\nan innovative approach to medical image classification, leveraging Federated\nLearning (FL) to address the dual challenges of data privacy and efficient\ndisease diagnosis. Traditional Centralized Machine Learning models, despite\ntheir widespread use in medical imaging for tasks such as disease diagnosis,\nraise significant privacy concerns due to the sensitive nature of patient data.\nAs an alternative, FL emerges as a promising solution by allowing the training\nof a collective global model across local clients without centralizing the\ndata, thus preserving privacy. Focusing on the application of FL in Magnetic\nResonance Imaging (MRI) brain tumor detection, this study demonstrates the\neffectiveness of the Federated Learning framework coupled with EfficientNet-B0\nand the FedAvg algorithm in enhancing both privacy and diagnostic accuracy.\nThrough a meticulous selection of preprocessing methods, algorithms, and\nhyperparameters, and a comparative analysis of various Convolutional Neural\nNetwork (CNN) architectures, the research uncovers optimal strategies for image\nclassification. The experimental results reveal that EfficientNet-B0\noutperforms other models like ResNet in handling data heterogeneity and\nachieving higher accuracy and lower loss, highlighting the potential of FL in\novercoming the limitations of traditional models. The study underscores the\nsignificance of addressing data heterogeneity and proposes further research\ndirections for broadening the applicability of FL in medical image analysis.\n']",Federated Learning for Medical Data Privacy
177,176,51,176_retrieval_nearest_search_locality,"['retrieval', 'nearest', 'search', 'locality', 'indexing', 'similarity', 'embeddings', 'hashing', 'algorithms', 'indexes']","['nearest', 'search', 'neighbor', 'index', 'vector', 'vectors', 'approximate', 'indexes', 'similarity', 'query']","['retrieval', 'locality', 'similarity', 'embeddings', 'hashing', 'indexes', 'clustering', 'databases', 'nns', 'storage']","[""  Vector search systems, pivotal in AI applications, often rely on the\nHierarchical Navigable Small Worlds (HNSW) algorithm. However, the behaviour of\nHNSW under real-world scenarios using vectors generated with deep learning\nmodels remains under-explored. Existing Approximate Nearest Neighbours (ANN)\nbenchmarks and research typically has an over-reliance on simplistic datasets\nlike MNIST or SIFT1M and fail to reflect the complexity of current use-cases.\nOur investigation focuses on HNSW's efficacy across a spectrum of datasets,\nincluding synthetic vectors tailored to mimic specific intrinsic\ndimensionalities, widely-used retrieval benchmarks with popular embedding\nmodels, and proprietary e-commerce image data with CLIP models. We survey the\nmost popular HNSW vector databases and collate their default parameters to\nprovide a realistic fixed parameterisation for the duration of the paper.\n  We discover that the recall of approximate HNSW search, in comparison to\nexact K Nearest Neighbours (KNN) search, is linked to the vector space's\nintrinsic dimensionality and significantly influenced by the data insertion\nsequence. Our methodology highlights how insertion order, informed by\nmeasurable properties such as the pointwise Local Intrinsic Dimensionality\n(LID) or known categories, can shift recall by up to 12 percentage points. We\nalso observe that running popular benchmark datasets with HNSW instead of KNN\ncan shift rankings by up to three positions for some models. This work\nunderscores the need for more nuanced benchmarks and design considerations in\ndeveloping robust vector search systems using approximate vector search\nalgorithms. This study presents a number of scenarios with varying real world\napplicability which aim to better increase understanding and future development\nof ANN algorithms and embedding\n"", '  We define and investigate the problem of $\\textit{c-approximate window\nsearch}$: approximate nearest neighbor search where each point in the dataset\nhas a numeric label, and the goal is to find nearest neighbors to queries\nwithin arbitrary label ranges. Many semantic search problems, such as image and\ndocument search with timestamp filters, or product search with cost filters,\nare natural examples of this problem. We propose and theoretically analyze a\nmodular tree-based framework for transforming an index that solves the\ntraditional c-approximate nearest neighbor problem into a data structure that\nsolves window search. On standard nearest neighbor benchmark datasets equipped\nwith random label values, adversarially constructed embeddings, and image\nsearch embeddings with real timestamps, we obtain up to a $75\\times$ speedup\nover existing solutions at the same level of recall.\n', '  A critical piece of the modern information retrieval puzzle is approximate\nnearest neighbor search. Its objective is to return a set of $k$ data points\nthat are closest to a query point, with its accuracy measured by the proportion\nof exact nearest neighbors captured in the returned set. One popular approach\nto this question is clustering: The indexing algorithm partitions data points\ninto non-overlapping subsets and represents each partition by a point such as\nits centroid. The query processing algorithm first identifies the nearest\nclusters -- a process known as routing -- then performs a nearest neighbor\nsearch over those clusters only. In this work, we make a simple observation:\nThe routing function solves a ranking problem. Its quality can therefore be\nassessed with a ranking metric, making the function amenable to\nlearning-to-rank. Interestingly, ground-truth is often freely available: Given\na query distribution in a top-$k$ configuration, the ground-truth is the set of\nclusters that contain the exact top-$k$ vectors. We develop this insight and\napply it to Maximum Inner Product Search (MIPS). As we demonstrate empirically\non various datasets, learning a simple linear function consistently improves\nthe accuracy of clustering-based MIPS.\n']",Approximate Nearest Neighbor Search Algorithms
178,177,51,177_tables_tablellm_table_tableinstruct,"['tables', 'tablellm', 'table', 'tableinstruct', 'texttableqa', 'tabular', 'tableqa', 'sql', 'answering', 'spreadsheetbench']","['table', 'tables', 'tabular', 'reasoning', 'question', 'answer', 'hop', 'questions', 'text', 'shot']","['tableinstruct', 'texttableqa', 'tabular', 'spreadsheetbench', 'queries', 'opentab', 'tabsqlify', 'prompting', 'tasks', 'hiddentables']","[""  Table question answering is a popular task that assesses a model's ability to\nunderstand and interact with structured data. However, the given table often\ndoes not contain sufficient information for answering the question,\nnecessitating the integration of external knowledge. Existing methods either\nconvert both the table and external knowledge into text, which neglects the\nstructured nature of the table; or they embed queries for external sources in\nthe interaction with the table, which complicates the process. In this paper,\nwe propose a simple yet effective method to integrate external information in a\ngiven table. Our method first constructs an augmenting table containing the\nmissing information and then generates a SQL query over the two tables to\nanswer the question. Experiments show that our method outperforms strong\nbaselines on three table QA benchmarks. Our code is publicly available at\nhttps://github.com/UCSB-NLP-Chang/Augment_tableQA.\n"", '  Table reasoning, which aims to generate the corresponding answer to the\nquestion following the user requirement according to the provided table, and\noptionally a text description of the table, effectively improving the\nefficiency of obtaining information. Recently, using Large Language Models\n(LLMs) has become the mainstream method for table reasoning, because it not\nonly significantly reduces the annotation cost but also exceeds the performance\nof previous methods. However, existing research still lacks a summary of\nLLM-based table reasoning works. Due to the existing lack of research,\nquestions about which techniques can improve table reasoning performance in the\nera of LLMs, why LLMs excel at table reasoning, and how to enhance table\nreasoning abilities in the future, remain largely unexplored. This gap\nsignificantly limits progress in research. To answer the above questions and\nadvance table reasoning research with LLMs, we present this survey to analyze\nexisting research, inspiring future work. In this paper, we analyze the\nmainstream techniques used to improve table reasoning performance in the LLM\nera, and the advantages of LLMs compared to pre-LLMs for solving table\nreasoning. We provide research directions from both the improvement of existing\nmethods and the expansion of practical applications to inspire future research.\n', '  Tables, typically two-dimensional and structured to store large amounts of\ndata, are essential in daily activities like database queries, spreadsheet\nmanipulations, web table question answering, and image table information\nextraction. Automating these table-centric tasks with Large Language Models\n(LLMs) or Visual Language Models (VLMs) offers significant public benefits,\ngarnering interest from academia and industry. This survey provides a\ncomprehensive overview of table-related tasks, examining both user scenarios\nand technical aspects. It covers traditional tasks like table question\nanswering as well as emerging fields such as spreadsheet manipulation and table\ndata analysis. We summarize the training techniques for LLMs and VLMs tailored\nfor table processing. Additionally, we discuss prompt engineering, particularly\nthe use of LLM-powered agents, for various table-related tasks. Finally, we\nhighlight several challenges, including processing implicit user intentions and\nextracting information from various table sources.\n']",Table Question Answering and Reasoning with Large Language Models
179,178,51,178_conversational_conversation_conversations_retrieval,"['conversational', 'conversation', 'conversations', 'retrieval', 'search', 'dialogue', 'queries', 'chat', 'seeking', 'chatretriever']","['conversational', 'search', 'query', 'queries', 'engines', 'retrieval', 'engine', 'satisfaction', 'reformulation', 'conversations']","['conversations', 'queries', 'chatretriever', 'cosearchagent', 'mindsearch', 'retriever', 'bing', 'adacqr', 'satisfaction', 'decoy']","['  With the increasing popularity of conversational search, how to evaluate the\nperformance of conversational search systems has become an important question\nin the IR community. Existing works on conversational search evaluation can\nmainly be categorized into two streams: (1) constructing metrics based on\nsemantic similarity (e.g. BLUE, METEOR and BERTScore), or (2) directly\nevaluating the response ranking performance of the system using traditional\nsearch methods (e.g. nDCG, RBP and nERR). However, these methods either ignore\nthe information need of the user or ignore the mixed-initiative property of\nconversational search. This raises the question of how to accurately model user\nsatisfaction in conversational search scenarios. Since explicitly asking users\nto provide satisfaction feedback is difficult, traditional IR studies often\nrely on the Cranfield paradigm (i.e., third-party annotation) and user behavior\nmodeling to estimate user satisfaction in search. However, the feasibility and\neffectiveness of these two approaches have not been fully explored in\nconversational search. In this paper, we dive into the evaluation of\nconversational search from the perspective of user satisfaction. We build a\nnovel conversational search experimental platform and construct a Chinese\nopen-domain conversational search behavior dataset containing rich annotations\nand search behavior data. We also collect third-party satisfaction annotation\nat the session-level and turn-level, to investigate the feasibility of the\nCranfield paradigm in the conversational search scenario. Experimental results\nshow both some consistency and considerable differences between the user\nsatisfaction annotations and third-party annotations. We also propose dialog\ncontinuation or ending behavior models (DCEBM) to capture session-level user\nsatisfaction based on turn-level information.\n', '  Conversational search supports multi-turn user-system interactions to solve\ncomplex information needs. Different from the traditional single-turn ad-hoc\nsearch, conversational search encounters a more challenging problem of\ncontext-dependent query understanding with the lengthy and long-tail\nconversational history context. While conversational query rewriting methods\nleverage explicit rewritten queries to train a rewriting model to transform the\ncontext-dependent query into a stand-stone search query, this is usually done\nwithout considering the quality of search results. Conversational dense\nretrieval methods use fine-tuning to improve a pre-trained ad-hoc query\nencoder, but they are limited by the conversational search data available for\ntraining. In this paper, we leverage both rewritten queries and relevance\njudgments in the conversational search data to train a better query\nrepresentation model. The key idea is to align the query representation with\nthose of rewritten queries and relevant documents. The proposed model -- Query\nRepresentation Alignment Conversational Dense Retriever, QRACDR, is tested on\neight datasets, including various settings in conversational search and ad-hoc\nsearch. The results demonstrate the strong performance of QRACDR compared with\nstate-of-the-art methods, and confirm the effectiveness of representation\nalignment.\n', ""  With large language models (LLMs), conversational search engines shift how\nusers retrieve information from the web by enabling natural conversations to\nexpress their search intents over multiple turns. Users' natural conversation\nembodies rich but implicit signals of users' search intents and evaluation of\nsearch results to understand user experience with the system. However, it is\nunderexplored how and why users ask follow-up queries to continue conversations\nwith conversational search engines and how the follow-up queries signal users'\nsatisfaction. From qualitative analysis of 250 conversational turns from an\nin-lab user evaluation of Naver Cue:, a commercial conversational search\nengine, we propose a taxonomy of 18 users' follow-up query patterns from\nconversational search, comprising two major axes: (1) users' motivations behind\ncontinuing conversations (N = 7) and (2) actions of follow-up queries (N = 11).\nCompared to the existing literature on query reformulations, we uncovered a new\nset of motivations and actions behind follow-up queries, including asking for\nsubjective opinions or providing natural language feedback on the engine's\nresponses. To analyze conversational search logs with our taxonomy in a\nscalable and efficient manner, we built an LLM-powered classifier (73%\naccuracy). With our classifier, we analyzed 2,061 conversational tuples\ncollected from real-world usage logs of Cue: and examined how the conversation\npatterns from our taxonomy correlates with satisfaction. Our initial findings\nsuggest some signals of dissatisfactions, such as Clarifying Queries, Excluding\nCondition, and Substituting Condition with follow-up queries. We envision our\napproach could contribute to automated evaluation of conversation search\nexperience by providing satisfaction signals and grounds for realistic user\nsimulations.\n""]",Conversational Search Evaluation and Optimization
180,179,51,179_photonics_photonic_optical_optics,"['photonics', 'photonic', 'optical', 'optics', 'nanophotonic', 'laser', 'multiplexing', 'wavelength', 'throughput', 'waveguide']","['optical', 'photonic', 'photonics', 'accelerators', 'computing', 'wavelength', 'analog', 'energy', 'microring', 'crosstalk']","['photonics', 'nanophotonic', 'multiplexing', 'waveguide', 'refractive', 'processors', 'microring', 'silicon', 'numerically', 'metasurface']","['  Among the promising advantages of photonic computing over conventional\ncomputing architectures is the potential to increase computing efficiency\nthrough massive parallelism by using the many degrees of freedom provided by\nphotonics. Here, we numerically demonstrate the simultaneous use of time and\nfrequency (equivalently wavelength) multiplexing to solve three independent\ntasks at the same time on the same photonic circuit. In particular, we consider\na microring-based time-delay reservoir computing (TDRC) scheme that\nsimultaneously solves three tasks: Time-series prediction, classification, and\nwireless channel equalization. The scheme relies on time-division multiplexing\nto avoid the necessity of multiple physical nonlinear nodes, while the tasks\nare parallelized using wavelength division multiplexing (WDM). The input data\nmodulated on each optical channel is mapped to a higher dimensional space by\nthe nonlinear dynamics of the silicon microring cavity. The carrier wavelength\nand input power assigned to each optical channel have a high influence on the\nperformance of its respective task. When all tasks operate under the same\nwavelength/power conditions, our results show that the computing nature of each\ntask is the deciding factor of the level of performance achievable. However, it\nis possible to achieve good performance for all tasks simultaneously by\noptimizing the parameters of each optical channel. The variety of applications\ncovered by the tasks shows the versatility of the proposed photonic TDRC\nscheme. Overall, this work provides insight into the potential of WDM-based\nschemes for improving the computing capabilities of reservoir computing\nschemes.\n', '  Solving partial differential equations (PDEs) numerically often requires huge\ncomputing time, energy cost, and hardware resources in practical applications.\nThis has limited their applications in many scenarios (e.g., autonomous\nsystems, supersonic flows) that have a limited energy budget and require near\nreal-time response. Leveraging optical computing, this paper develops an\non-chip training framework for physics-informed neural networks (PINNs), aiming\nto solve high-dimensional PDEs with fJ/MAC photonic power consumption and\nultra-low latency. Despite the ultra-high speed of optical neural networks,\ntraining a PINN on an optical chip is hard due to (1) the large size of\nphotonic devices, and (2) the lack of scalable optical memory devices to store\nthe intermediate results of back-propagation (BP). To enable realistic optical\nPINN training, this paper presents a scalable method to avoid the BP process.\nWe also employ a tensor-compressed approach to improve the convergence and\nscalability of our optical PINN training. This training framework is designed\nwith tensorized optical neural networks (TONN) for scalable inference\nacceleration and MZI phase-domain tuning for \\textit{in-situ} optimization. Our\nsimulation results of a 20-dim HJB PDE show that our photonic accelerator can\nreduce the number of MZIs by a factor of $1.17\\times 10^3$, with only $1.36$ J\nand $1.15$ s to solve this equation. This is the first real-size optical PINN\ntraining framework that can be applied to solve high-dimensional PDEs.\n', '  Subwavelength photonic structures and metamaterials provide revolutionary\napproaches for controlling light. The inverse design methods proposed for these\nsubwavelength structures are vital to the development of new photonic devices.\nHowever, most of the existing inverse design methods cannot realize direct\nmapping from optical properties to photonic structures but instead rely on\nforward simulation methods to perform iterative optimization. In this work, we\nexploit the powerful generative abilities of artificial intelligence (AI) and\npropose a practical inverse design method based on latent diffusion models. Our\nmethod maps directly the optical properties to structures without the\nrequirement of forward simulation and iterative optimization. Here, the given\noptical properties can work as ""prompts"" and guide the constructed model to\ncorrectly ""draw"" the required photonic structures. Experiments show that our\ndirect mapping-based inverse design method can generate subwavelength photonic\nstructures at high fidelity while following the given optical properties. This\nmay change the method used for optical design and greatly accelerate the\nresearch on new photonic devices.\n']",Photonic Computing and Optical Technologies
181,180,51,180_temporal_timelines_answering_timebench,"['temporal', 'timelines', 'answering', 'timebench', 'timeline', 'reasoning', 'timeml', 'knowledge', 'questions', 'answers']","['temporal', 'events', 'event', 'questions', 'reasoning', 'answering', 'question', 'timelines', 'timeline', 'knowledge']","['timelines', 'timebench', 'questions', 'timestamps', 'facts', 'texts', 'benchmark', 'drafts', 'dataset', 'comprehensive']","[""  Temporal knowledge graph question answering (TKGQA) poses a significant\nchallenge task, due to the temporal constraints hidden in questions and the\nanswers sought from dynamic structured knowledge. Although large language\nmodels (LLMs) have made considerable progress in their reasoning ability over\nstructured data, their application to the TKGQA task is a relatively unexplored\narea. This paper first proposes a novel generative temporal knowledge graph\nquestion answering framework, GenTKGQA, which guides LLMs to answer temporal\nquestions through two phases: Subgraph Retrieval and Answer Generation. First,\nwe exploit LLM's intrinsic knowledge to mine temporal constraints and\nstructural links in the questions without extra training, thus narrowing down\nthe subgraph search space in both temporal and structural dimensions. Next, we\ndesign virtual knowledge indicators to fuse the graph neural network signals of\nthe subgraph and the text representations of the LLM in a non-shallow way,\nwhich helps the open-source LLM deeply understand the temporal order and\nstructural dependencies among the retrieved facts through instruction tuning.\nExperimental results on two widely used datasets demonstrate the superiority of\nour model.\n"", '  Recently, Large Language Models (LLMs) have demonstrated great potential in\nvarious data mining tasks, such as knowledge question answering, mathematical\nreasoning, and commonsense reasoning. However, the reasoning capability of LLMs\non temporal event forecasting has been under-explored. To systematically\ninvestigate their abilities in temporal event forecasting, we conduct a\ncomprehensive evaluation of LLM-based methods for temporal event forecasting.\nDue to the lack of a high-quality dataset that involves both graph and textual\ndata, we first construct a benchmark dataset, named MidEast-TE-mini. Based on\nthis dataset, we design a series of baseline methods, characterized by various\ninput formats and retrieval augmented generation(RAG) modules. From extensive\nexperiments, we find that directly integrating raw texts into the input of LLMs\ndoes not enhance zero-shot extrapolation performance. In contrast,\nincorporating raw texts in specific complex events and fine-tuning LLMs\nsignificantly improves performance. Moreover, enhanced with retrieval modules,\nLLM can effectively capture temporal relational patterns hidden in historical\nevents. Meanwhile, issues such as popularity bias and the long-tail problem\nstill persist in LLMs, particularly in the RAG-based method. These findings not\nonly deepen our understanding of LLM-based event forecasting methods but also\nhighlight several promising research directions.We consider that this\ncomprehensive evaluation, along with the identified research opportunities,\nwill significantly contribute to future research on temporal event forecasting\nthrough LLMs.\n', ""  Knowledge in the real world is being updated constantly. However, it is\ncostly to frequently update large language models (LLMs). Therefore, it is\ncrucial for LLMs to understand the concept of temporal knowledge. However,\nprior works on temporal question answering (TQA) did not emphasize multi-answer\nand multi-hop types of temporal reasoning. In this paper, we propose a complex\ntemporal question-answering dataset Complex-TR that focuses on multi-answer and\nmulti-hop temporal reasoning. Besides, we also propose a novel data\naugmentation strategy to improve the complex temporal reasoning capability and\nrobustness of LLMs. We conducted experiments on multiple temporal QA datasets.\nExperimental results show that our method is able to improve LLMs' performance\non temporal QA benchmarks by significant margins. Our code and data are\nreleased at: https://github.com/nusnlp/complex-tr.\n""]",Temporal Knowledge Graph Question Answering
182,181,50,181_reinforcement_agents_agent_multiagent,"['reinforcement', 'agents', 'agent', 'multiagent', 'learning', 'rewards', 'teaming', 'learn', 'reward', 'cooperation']","['agent', 'agents', 'cooperative', 'team', 'reinforcement', 'multi', 'partners', 'coordination', 'heterogeneous', 'policies']","['agent', 'multiagent', 'rewards', 'teaming', 'cooperation', 'starcraft', 'skills', 'centralized', 'algorithms', 'innate']","['  Recently, deep multi-agent reinforcement learning (MARL) has gained\nsignificant popularity due to its success in various cooperative multi-agent\ntasks. However, exploration still remains a challenging problem in MARL due to\nthe partial observability of the agents and the exploration space that can grow\nexponentially as the number of agents increases. Firstly, in order to address\nthe scalability issue of the exploration space, we define a formation-based\nequivalence relation on the exploration space and aim to reduce the search\nspace by exploring only meaningful states in different formations. Then, we\npropose a novel formation-aware exploration (FoX) framework that encourages\npartially observable agents to visit the states in diverse formations by\nguiding them to be well aware of their current formation solely based on their\nown observations. Numerical results show that the proposed FoX framework\nsignificantly outperforms the state-of-the-art MARL algorithms on Google\nResearch Football (GRF) and sparse Starcraft II multi-agent challenge (SMAC)\ntasks.\n', '  Multi-Agent Reinforcement Learning (MARL) algorithms are widely adopted in\ntackling complex tasks that require collaboration and competition among agents\nin dynamic Multi-Agent Systems (MAS). However, learning such tasks from scratch\nis arduous and may not always be feasible, particularly for MASs with a large\nnumber of interactive agents due to the extensive sample complexity. Therefore,\nreusing knowledge gained from past experiences or other agents could\nefficiently accelerate the learning process and upscale MARL algorithms. In\nthis study, we introduce a novel framework that enables transfer learning for\nMARL through unifying various state spaces into fixed-size inputs that allow\none unified deep-learning policy viable in different scenarios within a MAS. We\nevaluated our approach in a range of scenarios within the StarCraft Multi-Agent\nChallenge (SMAC) environment, and the findings show significant enhancements in\nmulti-agent learning performance using maneuvering skills learned from other\nscenarios compared to agents learning from scratch. Furthermore, we adopted\nCurriculum Transfer Learning (CTL), enabling our deep learning policy to\nprogressively acquire knowledge and skills across pre-designed homogeneous\nlearning scenarios organized by difficulty levels. This process promotes inter-\nand intra-agent knowledge transfer, leading to high multi-agent learning\nperformance in more complicated heterogeneous scenarios.\n', ""  The rise of multi-agent systems, especially the success of multi-agent\nreinforcement learning (MARL), is reshaping our future across diverse domains\nlike autonomous vehicle networks. However, MARL still faces significant\nchallenges, particularly in achieving zero-shot scalability, which allows\ntrained MARL models to be directly applied to unseen tasks with varying numbers\nof agents. In addition, real-world multi-agent systems usually contain agents\nwith different functions and strategies, while the existing scalable MARL\nmethods only have limited heterogeneity. To address this, we propose a novel\nMARL framework named Scalable and Heterogeneous Proximal Policy Optimization\n(SHPPO), integrating heterogeneity into parameter-shared PPO-based MARL\nnetworks. we first leverage a latent network to adaptively learn strategy\npatterns for each agent. Second, we introduce a heterogeneous layer for\ndecision-making, whose parameters are specifically generated by the learned\nlatent variables. Our approach is scalable as all the parameters are shared\nexcept for the heterogeneous layer, and gains both inter-individual and\ntemporal heterogeneity at the same time. We implement our approach based on the\nstate-of-the-art backbone PPO-based algorithm as SHPPO, while our approach is\nagnostic to the backbone and can be seamlessly plugged into any\nparameter-shared MARL method. SHPPO exhibits superior performance over the\nbaselines such as MAPPO and HAPPO in classic MARL environments like Starcraft\nMulti-Agent Challenge (SMAC) and Google Research Football (GRF), showcasing\nenhanced zero-shot scalability and offering insights into the learned latent\nrepresentation's impact on team performance by visualization.\n""]",Multi-Agent Reinforcement Learning
183,182,49,182_concepts_explainability_interpretability_imagenet,"['concepts', 'explainability', 'interpretability', 'imagenet', 'explanations', 'concept', 'cnns', 'cnn', 'neural', 'classification']","['concept', 'explanations', 'concepts', 'explanation', 'explainability', 'explainable', 'interpretable', 'interpretability', 'abstraction', 'hoc']","['concepts', 'explainability', 'cnns', 'features', 'classifier', 'representations', 'dnns', 'neuron', 'saliency', 'intelligence']","['  Explainable Artificial Intelligence (XAI) poses a significant challenge in\nproviding transparent and understandable insights into complex AI models.\nTraditional post-hoc algorithms, while useful, often struggle to deliver\ninterpretable explanations. Concept-based models offer a promising avenue by\nincorporating explicit representations of concepts to enhance interpretability.\nHowever, existing research on automatic concept discovery methods is often\nlimited by lower-level concepts, costly human annotation requirements, and a\nrestricted domain of background knowledge. In this study, we explore the\npotential of a Large Language Model (LLM), specifically GPT-4, by leveraging\nits domain knowledge and common-sense capability to generate high-level\nconcepts that are meaningful as explanations for humans, for a specific setting\nof image classification. We use minimal textual object information available in\nthe data via prompting to facilitate this process. To evaluate the output, we\ncompare the concepts generated by the LLM with two other methods: concepts\ngenerated by humans and the ECII heuristic concept induction system. Since\nthere is no established metric to determine the human understandability of\nconcepts, we conducted a human study to assess the effectiveness of the\nLLM-generated concepts. Our findings indicate that while human-generated\nexplanations remain superior, concepts derived from GPT-4 are more\ncomprehensible to humans compared to those generated by ECII.\n', ""  The focus of recent research has shifted from merely improving the metrics\nbased performance of Deep Neural Networks (DNNs) to DNNs which are more\ninterpretable to humans. The field of eXplainable Artificial Intelligence (XAI)\nhas observed various techniques, including saliency-based and concept-based\napproaches. These approaches explain the model's decisions in simple human\nunderstandable terms called Concepts. Concepts are known to be the thinking\nground of humans}. Explanations in terms of concepts enable detecting spurious\ncorrelations, inherent biases, or clever-hans. With the advent of concept-based\nexplanations, a range of concept representation methods and automatic concept\ndiscovery algorithms have been introduced. Some recent works also use concepts\nfor model improvement in terms of interpretability and generalization. We\nprovide a systematic review and taxonomy of various concept representations and\ntheir discovery algorithms in DNNs, specifically in vision. We also provide\ndetails on concept-based model improvement literature marking the first\ncomprehensive survey of these methods.\n"", '  Analysis of how semantic concepts are represented within Convolutional Neural\nNetworks (CNNs) is a widely used approach in Explainable Artificial\nIntelligence (XAI) for interpreting CNNs. A motivation is the need for\ntransparency in safety-critical AI-based systems, as mandated in various\ndomains like automated driving. However, to use the concept representations for\nsafety-relevant purposes, like inspection or error retrieval, these must be of\nhigh quality and, in particular, stable. This paper focuses on two stability\ngoals when working with concept representations in computer vision CNNs:\nstability of concept retrieval and of concept attribution. The guiding use-case\nis a post-hoc explainability framework for object detection (OD) CNNs, towards\nwhich existing concept analysis (CA) methods are successfully adapted. To\naddress concept retrieval stability, we propose a novel metric that considers\nboth concept separation and consistency, and is agnostic to layer and concept\nrepresentation dimensionality. We then investigate impacts of concept\nabstraction level, number of concept training samples, CNN size, and concept\nrepresentation dimensionality on stability. For concept attribution stability\nwe explore the effect of gradient instability on gradient-based explainability\nmethods. The results on various CNNs for classification and object detection\nyield the main findings that (1) the stability of concept retrieval can be\nenhanced through dimensionality reduction via data aggregation, and (2) in\nshallow layers where gradient instability is more pronounced, gradient\nsmoothing techniques are advised. Finally, our approach provides valuable\ninsights into selecting the appropriate layer and concept representation\ndimensionality, paving the way towards CA in safety-critical XAI applications.\n']",Explainable AI and Concept-Based Interpretability in Deep Learning
184,183,49,183_blockchain_cryptocurrencies_bitcoin_ethereum,"['blockchain', 'cryptocurrencies', 'bitcoin', 'ethereum', 'fraud', 'frauds', 'cryptocurrency', 'ponzi', 'vulnerabilities', 'phishing']","['fraud', 'blockchain', 'transaction', 'transactions', 'contracts', 'smart', 'laundering', 'financial', 'money', 'detection']","['blockchain', 'cryptocurrencies', 'fraud', 'vulnerabilities', 'transactions', 'bank', 'schemes', 'anomalous', 'trafficking', 'patterns']","[""  As blockchain technology becomes more and more popular, a typical financial\nscam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum.\nThis Ponzi scheme deployed through smart contracts, also known as the smart\nPonzi scheme, has caused a lot of economic losses and negative impacts.\nExisting methods for detecting smart Ponzi schemes on Ethereum mainly rely on\nbytecode features, opcode features, account features, and transaction behavior\nfeatures of smart contracts, which are unable to truly characterize the\nbehavioral features of Ponzi schemes, and thus generally perform poorly in\nterms of detection accuracy and false alarm rates. In this paper, we propose\nSourceP, a method to detect smart Ponzi schemes on the Ethereum platform using\npre-trained models and data flow, which only requires using the source code of\nsmart contracts as features. SourceP reduces the difficulty of data acquisition\nand feature extraction of existing detection methods. Specifically, we first\nconvert the source code of a smart contract into a data flow graph and then\nintroduce a pre-trained model based on learning code representations to build a\nclassification model to identify Ponzi schemes in smart contracts. The\nexperimental results show that SourceP achieves 87.2% recall and 90.7% F-score\nfor detecting smart Ponzi schemes within Ethereum's smart contract dataset,\noutperforming state-of-the-art methods in terms of performance and\nsustainability. We also demonstrate through additional experiments that\npre-trained models and data flow play an important contribution to SourceP, as\nwell as proving that SourceP has a good generalization ability.\n"", '  The Ponzi scheme, an old-fashioned fraud, is now popular on the Ethereum\nblockchain, causing considerable financial losses to many crypto investors. A\nfew Ponzi detection methods have been proposed in the literature, most of which\ndetect a Ponzi scheme based on its smart contract source code. This\ncontract-code-based approach, while achieving very high accuracy, is not robust\nbecause a Ponzi developer can fool a detection model by obfuscating the opcode\nor inventing a new profit distribution logic that cannot be detected. On the\ncontrary, a transaction-based approach could improve the robustness of\ndetection because transactions, unlike smart contracts, are harder to be\nmanipulated. However, the current transaction-based detection models achieve\nfairly low accuracy. In this paper, we aim to improve the accuracy of the\ntransaction-based models by employing time-series features, which turn out to\nbe crucial in capturing the life-time behaviour a Ponzi application but were\ncompletely overlooked in previous works. We propose a new set of 85 features\n(22 known account-based and 63 new time-series features), which allows\noff-the-shelf machine learning algorithms to achieve up to 30% higher F1-scores\ncompared to existing works.\n', ""  The rampant fraudulent activities on Ethereum hinder the healthy development\nof the blockchain ecosystem, necessitating the reinforcement of regulations.\nHowever, multiple imbalances involving account interaction frequencies and\ninteraction types in the Ethereum transaction environment pose significant\nchallenges to data mining-based fraud detection research. To address this, we\nfirst propose the concept of meta-interactions to refine interaction behaviors\nin Ethereum, and based on this, we present a dual self-supervision enhanced\nEthereum fraud detection framework, named Meta-IFD. This framework initially\nintroduces a generative self-supervision mechanism to augment the interaction\nfeatures of accounts, followed by a contrastive self-supervision mechanism to\ndifferentiate various behavior patterns, and ultimately characterizes the\nbehavioral representations of accounts and mines potential fraud risks through\nmulti-view interaction feature learning. Extensive experiments on real Ethereum\ndatasets demonstrate the effectiveness and superiority of our framework in\ndetecting common Ethereum fraud behaviors such as Ponzi schemes and phishing\nscams. Additionally, the generative module can effectively alleviate the\ninteraction distribution imbalance in Ethereum data, while the contrastive\nmodule significantly enhances the framework's ability to distinguish different\nbehavior patterns. The source code will be released on GitHub soon.\n""]",Blockchain Fraud Detection
185,184,49,184_voice_speech_corpus_emotions,"['voice', 'speech', 'corpus', 'emotions', 'emotion', 'paralinguistic', 'lstm', 'datasets', 'feature', 'sernet']","['emotion', 'speech', 'recognition', 'emotional', 'voice', 'emotions', 'paralinguistic', 'arousal', 'audio', 'corpus']","['voice', 'paralinguistic', 'lstm', 'features', 'affective', 'transcriptions', 'seqnn', 'awes', 'embeddings', 'podcast']","['  Despite notable progress, speech emotion recognition (SER) remains\nchallenging due to the intricate and ambiguous nature of speech emotion,\nparticularly in wild world. While current studies primarily focus on\nrecognition and generalization abilities, our research pioneers an\ninvestigation into the reliability of SER methods in the presence of semantic\ndata shifts and explores how to exert fine-grained control over various\nattributes inherent in speech signals to enhance speech emotion modeling. In\nthis paper, we first introduce MSAC-SERNet, a novel unified SER framework\ncapable of simultaneously handling both single-corpus and cross-corpus SER.\nSpecifically, concentrating exclusively on the speech emotion attribute, a\nnovel CNN-based SER model is presented to extract discriminative emotional\nrepresentations, guided by additive margin softmax loss. Considering\ninformation overlap between various speech attributes, we propose a novel\nlearning paradigm based on correlations of different speech attributes, termed\nMultiple Speech Attribute Control (MSAC), which empowers the proposed SER model\nto simultaneously capture fine-grained emotion-related features while\nmitigating the negative impact of emotion-agnostic representations.\nFurthermore, we make a first attempt to examine the reliability of the\nMSAC-SERNet framework using out-of-distribution detection methods. Experiments\non both single-corpus and cross-corpus SER scenarios indicate that MSAC-SERNet\nnot only consistently outperforms the baseline in all aspects, but achieves\nsuperior performance compared to state-of-the-art SER approaches.\n', '  Text data is commonly utilized as a primary input to enhance Speech Emotion\nRecognition (SER) performance and reliability. However, the reliance on\nhuman-transcribed text in most studies impedes the development of practical SER\nsystems, creating a gap between in-lab research and real-world scenarios where\nAutomatic Speech Recognition (ASR) serves as the text source. Hence, this study\nbenchmarks SER performance using ASR transcripts with varying Word Error Rates\n(WERs) on well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast. Our\nevaluation includes text-only and bimodal SER with diverse fusion techniques,\naiming for a comprehensive analysis that uncovers novel findings and challenges\nfaced by current SER research. Additionally, we propose a unified ASR\nerror-robust framework integrating ASR error correction and modality-gated\nfusion, achieving lower WER and higher SER results compared to the\nbest-performing ASR transcript. This research is expected to provide insights\ninto SER with ASR assistance, especially for real-world applications.\n', ""  Speech emotion recognition (SER) is essential for enhancing human-computer\ninteraction in speech-based applications. Despite improvements in specific\nemotional datasets, there is still a research gap in SER's capability to\ngeneralize across real-world situations. In this paper, we investigate\napproaches to generalize the SER system across different emotion datasets. In\nparticular, we incorporate 11 emotional speech datasets and illustrate a\ncomprehensive benchmark on the SER task. We also address the challenge of\nimbalanced data distribution using over-sampling methods when combining SER\ndatasets for training. Furthermore, we explore various evaluation protocols for\nadeptness in the generalization of SER. Building on this, we explore the\npotential of Whisper for SER, emphasizing the importance of thorough\nevaluation. Our approach is designed to advance SER technology by integrating\nspeaker-independent methods.\n""]",Speech Emotion Recognition
186,185,48,185_malware_adversarial_malicious_ransomware,"['malware', 'adversarial', 'malicious', 'ransomware', 'antivirus', 'classifiers', 'classifier', 'cybersecurity', 'executable', 'obfuscation']","['malware', 'ransomware', 'evasion', 'files', 'adversarial', 'detection', 'detectors', 'file', 'antivirus', 'attacks']","['malware', 'adversarial', 'ransomware', 'classifier', 'cybersecurity', 'obfuscated', 'evasion', 'malmem', 'defenses', 'gans']","['  Malware has been one of the most damaging threats to computers that span\nacross multiple operating systems and various file formats. To defend against\never-increasing and ever-evolving malware, tremendous efforts have been made to\npropose a variety of malware detection that attempt to effectively and\nefficiently detect malware so as to mitigate possible damages as early as\npossible. Recent studies have shown that, on the one hand, existing ML and DL\ntechniques enable superior solutions in detecting newly emerging and previously\nunseen malware. However, on the other hand, ML and DL models are inherently\nvulnerable to adversarial attacks in the form of adversarial examples. In this\npaper, we focus on malware with the file format of portable executable (PE) in\nthe family of Windows operating systems, namely Windows PE malware, as a\nrepresentative case to study the adversarial attack methods in such adversarial\nsettings. To be specific, we start by first outlining the general learning\nframework of Windows PE malware detection based on ML/DL and subsequently\nhighlighting three unique challenges of performing adversarial attacks in the\ncontext of Windows PE malware. Then, we conduct a comprehensive and systematic\nreview to categorize the state-of-the-art adversarial attacks against PE\nmalware detection, as well as corresponding defenses to increase the robustness\nof Windows PE malware detection. Finally, we conclude the paper by first\npresenting other related attacks against Windows PE malware detection beyond\nthe adversarial attacks and then shedding light on future research directions\nand opportunities. In addition, a curated resource list of adversarial attacks\nand defenses for Windows PE malware detection is also available at\nhttps://github.com/ryderling/adversarial-attacks-and-defenses-for-windows-pe-malware-detection.\n', '  Machine learning has proven to be a useful tool for automated malware\ndetection, but machine learning models have also been shown to be vulnerable to\nadversarial attacks. This article addresses the problem of generating\nadversarial malware samples, specifically malicious Windows Portable Executable\nfiles. We summarize and compare work that has focused on adversarial machine\nlearning for malware detection. We use gradient-based, evolutionary\nalgorithm-based, and reinforcement-based methods to generate adversarial\nsamples, and then test the generated samples against selected antivirus\nproducts. We compare the selected methods in terms of accuracy and practical\napplicability. The results show that applying optimized modifications to\npreviously detected malware can lead to incorrect classification of the file as\nbenign. It is also known that generated malware samples can be successfully\nused against detection models other than those used to generate them and that\nusing combinations of generators can create new samples that evade detection.\nExperiments show that the Gym-malware generator, which uses a reinforcement\nlearning approach, has the greatest practical potential. This generator\nachieved an average sample generation time of 5.73 seconds and the highest\naverage evasion rate of 44.11%. Using the Gym-malware generator in combination\nwith itself improved the evasion rate to 58.35%.\n', '  Malware attacks have become significantly more frequent and sophisticated in\nrecent years. Therefore, malware detection and classification are critical\ncomponents of information security. Due to the large amount of malware samples\navailable, it is essential to categorize malware samples according to their\nmalicious characteristics. Clustering algorithms are thus becoming more widely\nused in computer security to analyze the behavior of malware variants and\ndiscover new malware families. Online clustering algorithms help us to\nunderstand malware behavior and produce a quicker response to new threats. This\npaper introduces a novel machine learning-based model for the online clustering\nof malicious samples into malware families. Streaming data is divided according\nto the clustering decision rule into samples from known and new emerging\nmalware families. The streaming data is classified using the weighted k-nearest\nneighbor classifier into known families, and the online k-means algorithm\nclusters the remaining streaming data and achieves a purity of clusters from\n90.20% for four clusters to 93.34% for ten clusters. This work is based on\nstatic analysis of portable executable files for the Windows operating system.\nExperimental results indicate that the proposed online clustering model can\ncreate high-purity clusters corresponding to malware families. This allows\nmalware analysts to receive similar malware samples, speeding up their\nanalysis.\n']",Malware Detection and Adversarial Attacks
187,186,48,186_scaling_scaled_models_scale,"['scaling', 'scaled', 'models', 'scale', 'autoscale', 'larger', 'predicting', 'predict', 'performance', 'language']","['scaling', 'laws', 'compute', 'size', 'training', 'pre', 'predictable', 'scale', 'growth', 'optimal']","['autoscale', 'larger', 'trillion', 'predictability', 'scalebio', 'corpus', 'pretraining', 'model', 'resources', 'slms']","[""  The scientific scale-up of large language models (LLMs) necessitates a\ncomprehensive understanding of their scaling properties. However, the existing\nliterature on the scaling properties only yields an incomplete answer:\noptimization loss decreases predictably as the model size increases, in line\nwith established scaling law; yet no scaling law for task has been established\nand the task performances are far from predictable during scaling. Task\nperformances typically show minor gains on small models until they improve\ndramatically once models exceed a size threshold, exemplifying the ``emergent\nabilities''. In this study, we discover that small models, although they\nexhibit minor performance, demonstrate critical and consistent task performance\nimprovements that are not captured by conventional evaluation strategies due to\ninsufficient measurement resolution. To measure such improvements, we introduce\nPassUntil, an evaluation strategy with theoretically infinite resolution,\nthrough massive sampling in the decoding phase. With PassUntil, we conduct a\nquantitative investigation into the scaling law of task performance. The\ninvestigation contains two parts. Firstly, a strict task scaling law that is\nnot conventionally known to exist, is identified, enhancing the predictability\nof task performances. Remarkably, we are able to predict the performance of the\n2.4B model on code generation with merely 0.05\\% deviation before training\nstarts, which is the first systematic attempt to verify predictable scaling\nproposed by GPT-4's report. Secondly, we are able to study emergent abilities\nquantitatively. We identify a kind of accelerated emergence whose scaling curve\ncannot be fitted by standard scaling law function and has a increasing speed.\nWe then examine two hypothesis and imply that the ``multiple circuits\nhypothesis'' might be responsible for the accelerated emergence.\n"", '  Recently, Large Language Models (LLMs) have been widely adopted in a wide\nrange of tasks, leading to increasing attention towards the research on how\nscaling LLMs affects their performance. Existing works, termed Scaling Laws,\nhave discovered that the final test loss of LLMs scales as power-laws with\nmodel size, computational budget, and dataset size. However, the temporal\nchange of the test loss of an LLM throughout its pre-training process remains\nunexplored, though it is valuable in many aspects, such as selecting better\nhyperparameters \\textit{directly} on the target LLM. In this paper, we propose\nthe novel concept of Temporal Scaling Law, studying how the test loss of an LLM\nevolves as the training steps scale up. In contrast to modeling the test loss\nas a whole in a coarse-grained manner, we break it down and dive into the\nfine-grained test loss of each token position, and further develop a dynamic\nhyperbolic-law. Afterwards, we derive the much more precise temporal scaling\nlaw by studying the temporal patterns of the parameters in the dynamic\nhyperbolic-law. Results on both in-distribution (ID) and out-of-distribution\n(OOD) validation datasets demonstrate that our temporal scaling law accurately\npredicts the test loss of LLMs across training steps. Our temporal scaling law\nhas broad practical applications. First, it enables direct and efficient\nhyperparameter selection on the target LLM, such as data mixture proportions.\nSecondly, viewing the LLM pre-training dynamics from the token position\ngranularity provides some insights to enhance the understanding of LLM\npre-training.\n', '  Large language model pre-training has become increasingly expensive, with\nmost practitioners relying on scaling laws to allocate compute budgets for\nmodel size and training tokens, commonly referred to as Compute-Optimal or\nChinchilla Optimal. In this paper, we hypothesize a new scaling law that\nsuggests model performance depends mostly on the amount of compute spent for\ntransformer-based models, independent of the specific allocation to model size\nand dataset size. Using this unified scaling law, we predict that (a) for\ninference efficiency, training should prioritize smaller model sizes and larger\ntraining datasets, and (b) assuming the exhaustion of available web datasets,\nscaling the model size might be the only way to further improve model\nperformance.\n']",Scaling Large Language Models
188,187,48,187_classifiers_classifier_classification_imbalance,"['classifiers', 'classifier', 'classification', 'imbalance', 'imbalanced', 'datasets', 'accuracy', 'svm', 'classes', 'nearest']","['class', 'imbalance', 'classifiers', 'classification', 'binary', 'classifier', 'nearest', 'neighbors', 'classes', 'confusion']","['classifiers', 'imbalance', 'precision', 'sampling', 'label', 'smote', 'recall', 'oversampling', 'roc', 'imknn']","[""  We show that established performance metrics in binary classification, such\nas the F-score, the Jaccard similarity coefficient or Matthews' correlation\ncoefficient (MCC), are not robust to class imbalance in the sense that if the\nproportion of the minority class tends to $0$, the true positive rate (TPR) of\nthe Bayes classifier under these metrics tends to $0$ as well. Thus, in\nimbalanced classification problems, these metrics favour classifiers which\nignore the minority class. To alleviate this issue we introduce robust\nmodifications of the F-score and the MCC for which, even in strongly imbalanced\nsettings, the TPR is bounded away from $0$. We numerically illustrate the\nbehaviour of the various performance metrics in simulations as well as on a\ncredit default data set. We also discuss connections to the ROC and\nprecision-recall curves and give recommendations on how to combine their usage\nwith performance metrics.\n"", ""  The fundamental concept underlying K-Nearest Neighbors (KNN) is the\nclassification of samples based on the majority through their nearest\nneighbors. Although distance and neighbors' labels are critical in KNN,\ntraditional KNN treats all samples equally. However, some KNN variants weigh\nneighbors differently based on a specific rule, considering each neighbor's\ndistance and label. Many KNN methodologies introduce complex algorithms that do\nnot significantly outperform the traditional KNN, often leading to less\nsatisfactory outcomes. The gap in reliably extracting information for\naccurately predicting true weights remains an open research challenge. In our\nproposed method, information-modified KNN (IMKNN), we bridge the gap by\npresenting a straightforward algorithm that achieves effective results. To this\nend, we introduce a classification method to improve the performance of the KNN\nalgorithm. By exploiting mutual information (MI) and incorporating ideas from\nShapley's values, we improve the traditional KNN performance in accuracy,\nprecision, and recall, offering a more refined and effective solution.\n  To evaluate the effectiveness of our method, it is compared with eight\nvariants of KNN. We conduct experiments on 12 widely-used datasets, achieving\n11.05\\%, 12.42\\%, and 12.07\\% in accuracy, precision, and recall performance,\nrespectively, compared to traditional KNN. Additionally, we compared IMKNN with\ntraditional KNN across four large-scale datasets to highlight the distinct\nadvantages of IMKNN in the impact of monotonicity, noise, density, subclusters,\nand skewed distributions. Our research indicates that IMKNN consistently\nsurpasses other methods in diverse datasets.\n"", '  Class imbalance remains a significant challenge in machine learning,\nparticularly for tabular data classification tasks. While Gradient Boosting\nDecision Trees (GBDT) models have proven highly effective for such tasks, their\nperformance can be compromised when dealing with imbalanced datasets. This\npaper presents the first comprehensive study on adapting class-balanced loss\nfunctions to three GBDT algorithms across various tabular classification tasks,\nincluding binary, multi-class, and multi-label classification. We conduct\nextensive experiments on multiple datasets to evaluate the impact of\nclass-balanced losses on different GBDT models, establishing a valuable\nbenchmark. Our results demonstrate the potential of class-balanced loss\nfunctions to enhance GBDT performance on imbalanced datasets, offering a robust\napproach for practitioners facing class imbalance challenges in real-world\napplications. Additionally, we introduce a Python package that facilitates the\nintegration of class-balanced loss functions into GBDT workflows, making these\nadvanced techniques accessible to a wider audience.\n']",Classification with Imbalanced Data
189,188,47,188_empathy_empatheticdialogues_empathicstories_empathetic,"['empathy', 'empatheticdialogues', 'empathicstories', 'empathetic', 'empathic', 'conversations', 'empathically', 'dialogue', 'conversation', 'emotionbench']","['empathy', 'empathetic', 'emotional', 'empathic', 'dialogue', 'responses', 'emotions', 'emotion', 'support', 'conversations']","['empathy', 'empatheticdialogues', 'emotionbench', 'responses', 'conversational', 'affective', 'emllm', 'interpersonal', 'personality', 'sensibility']","[""  This paper investigates the empathetic responding capabilities of ChatGPT,\nparticularly its latest iteration, GPT-4, in comparison to human-generated\nresponses to a wide range of emotional scenarios, both positive and negative.\nWe employ a rigorous evaluation methodology, involving a between-groups study\nwith 600 participants, to evaluate the level of empathy in responses generated\nby humans and ChatGPT. ChatGPT is prompted in two distinct ways: a standard\napproach and one explicitly detailing empathy's cognitive, affective, and\ncompassionate counterparts. Our findings indicate that the average empathy\nrating of responses generated by ChatGPT exceeds those crafted by humans by\napproximately 10%. Additionally, instructing ChatGPT to incorporate a clear\nunderstanding of empathy in its responses makes the responses align\napproximately 5 times more closely with the expectations of individuals\npossessing a high degree of empathy, compared to human responses. The proposed\nevaluation framework serves as a scalable and adaptable framework to assess the\nempathetic capabilities of newer and updated versions of large language models,\neliminating the need to replicate the current study's results in future\nresearch.\n"", ""  Empathetic response generation, aiming at understanding the user's situation\nand feelings and respond empathically, is crucial in building human-like\ndialogue systems. Previous methods mainly focus on using maximum likelihood\nestimation as the optimization objective for training response generation\nmodels, without taking into account the empathy level alignment between\ngenerated responses and target responses. To this end, we propose an empathetic\nresponse generation using reinforcement learning (EmpRL) framework. The\nframework designs an effective empathy reward function and generates empathetic\nresponses by maximizing the expected reward through reinforcement learning.\nGiven the powerful text generation capability of pre-trained language models,\nEmpRL utilizes the pre-trained T5 model as the generator and conducts further\ntraining to initialize the policy. To align the empathy level between generated\nresponses and target responses in the context, an empathy reward function\ncontaining three empathy communication mechanisms, i.e., emotional reaction,\ninterpretation, and exploration, is constructed using pre-designed and\npre-trained empathy identifiers. Finally, the proximal policy optimization\nalgorithm is used to further train the policy to produce empathetic responses.\nBoth automatic and manual evaluations demonstrate that the proposed EmpRL\nframework can improve the quality of generated responses, enhance the empathy\nlevel similarity between generated and target responses, and produce empathetic\nresponses covering both affective and cognitive aspects.\n"", ""  Empathetic response generation is designed to comprehend the emotions of\nothers and select the most appropriate strategies to assist them in resolving\nemotional challenges. Empathy can be categorized into cognitive empathy and\naffective empathy. The former pertains to the ability to understand and discern\nthe emotional issues and situations of others, while the latter involves the\ncapacity to provide comfort. To enhance one's empathetic abilities, it is\nessential to develop both these aspects. Therefore, we develop an innovative\nframework that combines retrieval augmentation and emotional support strategy\nintegration. Our framework starts with the introduction of a comprehensive\nemotional palette for empathy. We then apply appraisal theory to decompose this\npalette and create a database of empathetic responses. This database serves as\nan external resource and enhances the LLM's empathy by integrating semantic\nretrieval mechanisms. Moreover, our framework places a strong emphasis on the\nproper articulation of response strategies. By incorporating emotional support\nstrategies, we aim to enrich the model's capabilities in both cognitive and\naffective empathy, leading to a more nuanced and comprehensive empathetic\nresponse. Finally, we extract datasets ED and ET from the empathetic dialogue\ndataset \\textsc{EmpatheticDialogues} and ExTES based on dialogue length.\nExperiments demonstrate that our framework can enhance the empathy ability of\nLLMs from both cognitive and affective empathy perspectives. Our code is\nreleased at https://github.com/CAS-SIAT-XinHai/APTNESS.\n""]",Empathy in Human-Computer Interaction
190,189,47,189_retina_retinal_macular_retinopathy,"['retina', 'retinal', 'macular', 'retinopathy', 'ophthalmology', 'ophthalmologists', 'ocular', 'blindness', 'glaucoma', 'segmentation']","['retinal', 'fundus', 'glaucoma', 'retinopathy', 'diabetic', 'vascular', 'disease', 'optic', 'diseases', 'eye']","['retinal', 'macular', 'ophthalmologists', 'segmentation', 'biomarkers', 'screening', 'diabetes', 'fundus', 'tomography', 'vessels']","['  Retinal fundus images play a crucial role in the early detection of eye\ndiseases and, using deep learning approaches, recent studies have even\ndemonstrated their potential for detecting cardiovascular risk factors and\nneurological disorders. However, the impact of technical factors on these\nimages can pose challenges for reliable AI applications in ophthalmology. For\nexample, large fundus cohorts are often confounded by factors like camera type,\nimage quality or illumination level, bearing the risk of learning shortcuts\nrather than the causal relationships behind the image generation process. Here,\nwe introduce a novel population model for retinal fundus images that\neffectively disentangles patient attributes from camera effects, thus enabling\ncontrollable and highly realistic image generation. To achieve this, we propose\na novel disentanglement loss based on distance correlation. Through qualitative\nand quantitative analyses, we demonstrate the effectiveness of this novel loss\nfunction in disentangling the learned subspaces. Our results show that our\nmodel provides a new perspective on the complex relationship between patient\nattributes and technical confounders in retinal fundus image generation.\n', '  Retinal optical coherence tomography (OCT) images provide crucial insights\ninto the health of the posterior ocular segment. Therefore, the advancement of\nautomated image analysis methods is imperative to equip clinicians and\nresearchers with quantitative data, thereby facilitating informed\ndecision-making. The application of deep learning (DL)-based approaches has\ngained extensive traction for executing these analysis tasks, demonstrating\nremarkable performance compared to labor-intensive manual analyses. However,\nthe acquisition of Retinal OCT images often presents challenges stemming from\nprivacy concerns and the resource-intensive labeling procedures, which\ncontradicts the prevailing notion that DL models necessitate substantial data\nvolumes for achieving superior performance. Moreover, limitations in available\ncomputational resources constrain the progress of high-performance medical\nartificial intelligence, particularly in less developed regions and countries.\nThis paper introduces a novel ensemble learning mechanism designed for\nrecognizing retinal diseases under limited resources (e.g., data, computation).\nThe mechanism leverages insights from multiple pre-trained models, facilitating\nthe transfer and adaptation of their knowledge to Retinal OCT images. This\napproach establishes a robust model even when confronted with limited labeled\ndata, eliminating the need for an extensive array of parameters, as required in\nlearning from scratch. Comprehensive experimentation on real-world datasets\ndemonstrates that the proposed approach can achieve superior performance in\nrecognizing Retinal OCT images, even when dealing with exceedingly restricted\nlabeled datasets. Furthermore, this method obviates the necessity of learning\nextensive-scale parameters, making it well-suited for deployment in\nlow-resource scenarios.\n', '  Optical coherence tomography (OCT) is a non-invasive imaging technique with\nextensive clinical applications in ophthalmology. OCT enables the visualization\nof the retinal layers, playing a vital role in the early detection and\nmonitoring of retinal diseases. OCT uses the principle of light wave\ninterference to create detailed images of the retinal microstructures, making\nit a valuable tool for diagnosing ocular conditions. This work presents an\nopen-access OCT dataset (OCTDL) comprising over 2000 OCT images labeled\naccording to disease group and retinal pathology. The dataset consists of OCT\nrecords of patients with Age-related Macular Degeneration (AMD), Diabetic\nMacular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery Occlusion (RAO),\nRetinal Vein Occlusion (RVO), and Vitreomacular Interface Disease (VID). The\nimages were acquired with an Optovue Avanti RTVue XR using raster scanning\nprotocols with dynamic scan length and image resolution. Each retinal b-scan\nwas acquired by centering on the fovea and interpreted and cataloged by an\nexperienced retinal specialist. In this work, we applied Deep Learning\nclassification techniques to this new open-access dataset.\n']",Retinal Disease Diagnosis and Imaging
191,190,47,190_ui_screenagent_uis_mobileagent,"['ui', 'screenagent', 'uis', 'mobileagent', 'guis', 'apps', 'multimodal', 'gui', 'automate', 'androidworld']","['mobile', 'screen', 'agents', 'app', 'agent', 'apps', 'screenshots', 'automation', 'interface', 'action']","['screenagent', 'guis', 'apps', 'multimodal', 'droidbot', 'agents', 'uiclip', 'automation', 'mobilegpt', 'androidcontrol']","['  Graphical User Interface (GUI) agents are designed to automate complex tasks\non digital devices, such as smartphones and desktops. Most existing GUI agents\ninteract with the environment through extracted structured data, which can be\nnotably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops).\nTo alleviate this issue, we propose a novel visual GUI agent -- SeeClick, which\nonly relies on screenshots for task automation. In our preliminary study, we\nhave discovered a key challenge in developing visual GUI agents: GUI grounding\n-- the capacity to accurately locate screen elements based on instructions. To\ntackle this challenge, we propose to enhance SeeClick with GUI grounding\npre-training and devise a method to automate the curation of GUI grounding\ndata. Along with the efforts above, we have also created ScreenSpot, the first\nrealistic GUI grounding benchmark that encompasses mobile, desktop, and web\nenvironments. After pre-training, SeeClick demonstrates significant improvement\nin ScreenSpot over various baselines. Moreover, comprehensive evaluations on\nthree widely used benchmarks consistently support our finding that advancements\nin GUI grounding directly correlate with enhanced performance in downstream GUI\nagent tasks. The model, data and code are available at\nhttps://github.com/njucckevin/SeeClick.\n', '  Recently, Multimodal Large Language Models (MLLMs) have been used as agents\nto control keyboard and mouse inputs by directly perceiving the Graphical User\nInterface (GUI) and generating corresponding code. However, current agents\nprimarily exhibit excellent understanding capabilities in static environments\nand are predominantly applied in relatively simple domains, such as Web or\nmobile interfaces. We argue that a robust GUI agent should be capable of\nperceiving temporal information on the GUI, including dynamic Web content and\nmulti-step tasks. Additionally, it should possess a comprehensive understanding\nof various GUI scenarios, including desktop software and multi-window\ninteractions. To this end, this paper introduces a new dataset, termed\nGUI-World, which features meticulously crafted Human-MLLM annotations,\nextensively covering six GUI scenarios and eight types of GUI-oriented\nquestions in three formats. We evaluate the capabilities of current\nstate-of-the-art MLLMs, including ImageLLMs and VideoLLMs, in understanding\nvarious types of GUI content, especially dynamic and sequential content. Our\nfindings reveal that ImageLLMs struggle with dynamic GUI content without\nmanually annotated keyframes or operation history. On the other hand, VideoLLMs\nfall short in all GUI-oriented tasks given the sparse GUI video dataset. Based\non GUI-World, we take the initial step of leveraging a fine-tuned VideoLLM as a\nGUI agent, demonstrating an improved understanding of various GUI tasks.\nHowever, due to the limitations in the performance of base LLMs, we conclude\nthat using VideoLLMs as GUI agents remains a significant challenge. We believe\nour work provides valuable insights for future research in dynamic GUI content\nunderstanding. The code and dataset are publicly available at our project\nhomepage: https://gui-world.github.io/.\n', '  AI agents have drawn increasing attention mostly on their ability to perceive\nenvironments, understand tasks, and autonomously achieve goals. To advance\nresearch on AI agents in mobile scenarios, we introduce the Android\nMulti-annotation EXpo (AMEX), a comprehensive, large-scale dataset designed for\ngeneralist mobile GUI-control agents. Their capabilities of completing complex\ntasks by directly interacting with the graphical user interface (GUI) on mobile\ndevices are trained and evaluated with the proposed dataset. AMEX comprises\nover 104K high-resolution screenshots from 110 popular mobile applications,\nwhich are annotated at multiple levels. Unlike existing mobile device-control\ndatasets, e.g., MoTIF, AitW, etc., AMEX includes three levels of annotations:\nGUI interactive element grounding, GUI screen and element functionality\ndescriptions, and complex natural language instructions, each averaging 13\nsteps with stepwise GUI-action chains. We develop this dataset from a more\ninstructive and detailed perspective, complementing the general settings of\nexisting datasets. Additionally, we develop a baseline model SPHINX Agent and\ncompare its performance across state-of-the-art agents trained on other\ndatasets. To facilitate further research, we open-source our dataset, models,\nand relevant evaluation tools. The project is available at\nhttps://yuxiangchai.github.io/AMEX/\n']",Graphical User Interface (GUI) Automation Agents
192,191,47,191_verilog_programmable_programming_hardware,"['verilog', 'programmable', 'programming', 'hardware', 'verilogeval', 'generate', 'tools', 'automation', 'vhdl', 'veribug']","['verilog', 'hardware', 'design', 'designs', 'chip', 'verification', 'assertions', 'code', 'automation', 'description']","['verilog', 'programmable', 'hardware', 'generate', 'vhdl', 'veribug', 'hdldebugger', 'systemverilog', 'llms', 'hdls']","['  Large Language Models (LLMs) have recently shown promise in streamlining\nhardware design processes by encapsulating vast amounts of domain-specific\ndata. In addition, they allow users to interact with the design processes\nthrough natural language instructions, thus making hardware design more\naccessible to developers. However, effectively leveraging LLMs in hardware\ndesign necessitates providing domain-specific data during inference (e.g.,\nthrough in-context learning), fine-tuning, or pre-training. Unfortunately,\nexisting publicly available hardware datasets are often limited in size,\ncomplexity, or detail, which hinders the effectiveness of LLMs in hardware\ndesign tasks. To address this issue, we first propose a set of criteria for\ncreating high-quality hardware datasets that can effectively enhance\nLLM-assisted hardware design. Based on these criteria, we propose a\nMulti-Grained-Verilog (MG-Verilog) dataset, which encompasses descriptions at\nvarious levels of detail and corresponding code samples. To benefit the broader\nhardware design community, we have developed an open-source infrastructure that\nfacilitates easy access, integration, and extension of the dataset to meet\nspecific project needs. Furthermore, to fully exploit the potential of the\nMG-Verilog dataset, which varies in complexity and detail, we introduce a\nbalanced fine-tuning scheme. This scheme serves as a unique use case to\nleverage the diverse levels of detail provided by the dataset. Extensive\nexperiments demonstrate that the proposed dataset and fine-tuning scheme\nconsistently improve the performance of LLMs in hardware design tasks.\n', '  Natural language interfaces have exhibited considerable potential in the\nautomation of Verilog generation derived from high-level specifications through\nthe utilization of large language models, garnering significant attention.\nNevertheless, this paper elucidates that visual representations contribute\nessential contextual information critical to design intent for hardware\narchitectures possessing spatial complexity, potentially surpassing the\nefficacy of natural-language-only inputs. Expanding upon this premise, our\npaper introduces an open-source benchmark for multi-modal generative models\ntailored for Verilog synthesis from visual-linguistic inputs, addressing both\nsingular and complex modules. Additionally, we introduce an open-source visual\nand natural language Verilog query language framework to facilitate efficient\nand user-friendly multi-modal queries. To evaluate the performance of the\nproposed multi-modal hardware generative AI in Verilog generation tasks, we\ncompare it with a popular method that relies solely on natural language. Our\nresults demonstrate a significant accuracy improvement in the multi-modal\ngenerated Verilog compared to queries based solely on natural language. We hope\nto reveal a new approach to hardware design in the large-hardware-design-model\nera, thereby fostering a more diversified and productive approach to hardware\ndesign.\n', '  Recent advances in large language models have demonstrated their potential\nfor automated generation of hardware description language (HDL) code from\nhigh-level prompts. Researchers have utilized fine-tuning to enhance the\nability of these large language models (LLMs) in the field of Chip Design.\nHowever, the lack of Verilog data hinders further improvement in the quality of\nVerilog generation by LLMs. Additionally, the absence of a Verilog and\nElectronic Design Automation (EDA) script data augmentation framework\nsignificantly increases the time required to prepare the training dataset for\nLLM trainers. This paper proposes an automated design-data augmentation\nframework, which generates high-volume and high-quality natural language\naligned with Verilog and EDA scripts. For Verilog generation, it translates\nVerilog files to an abstract syntax tree and then maps nodes to natural\nlanguage with a predefined template. For Verilog repair, it uses predefined\nrules to generate the wrong verilog file and then pairs EDA Tool feedback with\nthe right and wrong verilog file. For EDA Script generation, it uses existing\nLLM(GPT-3.5) to obtain the description of the Script. To evaluate the\neffectiveness of our data augmentation method, we finetune Llama2-13B and\nLlama2-7B models using the dataset generated by our augmentation framework. The\nresults demonstrate a significant improvement in the Verilog generation tasks\nwith LLMs. Moreover, the accuracy of Verilog generation surpasses that of the\ncurrent state-of-the-art open-source Verilog generation model, increasing from\n58.8% to 70.6% with the same benchmark. Our 13B model (ChipGPT-FT) has a pass\nrate improvement compared with GPT-3.5 in Verilog generation and outperforms in\nEDA script (i.e., SiliconCompiler) generation with only 200 EDA script data.\n']",Verilog Programming and Hardware Design Automation
193,192,47,192_robot_robots_robotic_robotics,"['robot', 'robots', 'robotic', 'robotics', 'humanoid', 'interactive', 'cobots', 'gestures', 'conversations', 'cobot']","['robot', 'robots', 'assistive', 'cobot', 'social', 'robotic', 'robotics', 'interaction', 'participants', 'interfaces']","['robotics', 'humanoid', 'interactive', 'cobots', 'gestures', 'conversation', 'metarobotics', 'autonomy', 'gaze', 'needs']","['  With robotics rapidly advancing, more effective human-robot interaction is\nincreasingly needed to realize the full potential of robots for society. While\nspoken language must be part of the solution, our ability to provide spoken\nlanguage interaction capabilities is still very limited. The National Science\nFoundation accordingly convened a workshop, bringing together speech, language,\nand robotics researchers to discuss what needs to be done. The result is this\nreport, in which we identify key scientific and engineering advances needed.\n  Our recommendations broadly relate to eight general themes. First, meeting\nhuman needs requires addressing new challenges in speech technology and user\nexperience design. Second, this requires better models of the social and\ninteractive aspects of language use. Third, for robustness, robots need\nhigher-bandwidth communication with users and better handling of uncertainty,\nincluding simultaneous consideration of multiple hypotheses and goals. Fourth,\nmore powerful adaptation methods are needed, to enable robots to communicate in\nnew environments, for new tasks, and with diverse user populations, without\nextensive re-engineering or the collection of massive training data. Fifth,\nsince robots are embodied, speech should function together with other\ncommunication modalities, such as gaze, gesture, posture, and motion. Sixth,\nsince robots operate in complex environments, speech components need access to\nrich yet efficient representations of what the robot knows about objects,\nlocations, noise sources, the user, and other humans. Seventh, since robots\noperate in real time, their speech and language processing components must\nalso. Eighth, in addition to more research, we need more work on infrastructure\nand resources, including shareable software modules and internal interfaces,\ninexpensive hardware, baseline systems, and diverse corpora.\n', ""  Users develop mental models of robots to conceptualize what kind of\ninteractions they can have with those robots. The conceptualizations are often\nformed before interactions with the robot and are based only on observing the\nrobot's physical design. As a result, understanding conceptualizations formed\nfrom physical design is necessary to understand how users intend to interact\nwith the robot. We propose to use multimodal features of robot embodiments to\npredict what kinds of expectations users will have about a given robot's social\nand physical capabilities. We show that using such features provides\ninformation about general mental models of the robots that generalize across\nsocially interactive robots. We describe how these models can be incorporated\ninto interaction design and physical design for researchers working with\nsocially interactive robots.\n"", ""  Assistive robots have attracted significant attention due to their potential\nto enhance the quality of life for vulnerable individuals like the elderly. The\nconvergence of computer vision, large language models, and robotics has\nintroduced the `visuolinguomotor' mode for assistive robots, where visuals and\nlinguistics are incorporated into assistive robots to enable proactive and\ninteractive assistance. This raises the question: \\textit{In circumstances\nwhere visuals become unreliable or unavailable, can we rely solely on language\nto control robots, i.e., the viability of the `linguomotor` mode for assistive\nrobots?} This work takes the initial steps to answer this question by: 1)\nevaluating the responses of assistive robots to language prompts of varying\ngranularities; and 2) exploring the necessity and feasibility of controlling\nthe robot on-the-fly. We have designed and conducted experiments on a Sawyer\ncobot to support our arguments. A Turtlebot robot case is designed to\ndemonstrate the adaptation of the solution to scenarios where assistive robots\nneed to maneuver to assist. Codes will be released on GitHub soon to benefit\nthe community.\n""]",Human-Robot Interaction and Robotics Development
194,193,47,193_anomaly_anomalyllm_outliers_outlier,"['anomaly', 'anomalyllm', 'outliers', 'outlier', 'graphs', 'anomalies', 'anomalous', 'graph', 'nodes', 'detecting']","['anomaly', 'anomalies', 'detection', 'nodes', 'normal', 'graph', 'anomalous', 'node', 'graphs', 'edges']","['anomaly', 'outliers', 'graphs', 'normalizing', 'gad', 'views', 'neighbors', 'detector', 'gnns', 'affinity']","['  Graph Anomaly Detection (GAD) is a technique used to identify abnormal nodes\nwithin graphs, finding applications in network security, fraud detection,\nsocial media spam detection, and various other domains. A common method for GAD\nis Graph Auto-Encoders (GAEs), which encode graph data into node\nrepresentations and identify anomalies by assessing the reconstruction quality\nof the graphs based on these representations. However, existing GAE models are\nprimarily optimized for direct link reconstruction, resulting in nodes\nconnected in the graph being clustered in the latent space. As a result, they\nexcel at detecting cluster-type structural anomalies but struggle with more\ncomplex structural anomalies that do not conform to clusters. To address this\nlimitation, we propose a novel solution called GAD-NR, a new variant of GAE\nthat incorporates neighborhood reconstruction for graph anomaly detection.\nGAD-NR aims to reconstruct the entire neighborhood of a node, encompassing the\nlocal structure, self-attributes, and neighbor attributes, based on the\ncorresponding node representation. By comparing the neighborhood reconstruction\nloss between anomalous nodes and normal nodes, GAD-NR can effectively detect\nany anomalies. Extensive experimentation conducted on six real-world datasets\nvalidates the effectiveness of GAD-NR, showcasing significant improvements (by\nup to 30% in AUC) over state-of-the-art competitors. The source code for GAD-NR\nis openly available. Importantly, the comparative analysis reveals that the\nexisting methods perform well only in detecting one or two types of anomalies\nout of the three types studied. In contrast, GAD-NR excels at detecting all\nthree types of anomalies across the datasets, demonstrating its comprehensive\nanomaly detection capabilities.\n', '  Real-world graphs are complex to process for performing effective analysis,\nsuch as anomaly detection. However, recently, there have been several research\nefforts addressing the issues surrounding graph-based anomaly detection. In\nthis paper, we discuss a comprehensive overview of anomaly detection techniques\non graph data. We also discuss the various application domains which use those\nanomaly detection techniques. We present a new taxonomy that categorizes the\ndifferent state-of-the-art anomaly detection methods based on assumptions and\ntechniques. Within each category, we discuss the fundamental research ideas\nthat have been done to improve anomaly detection. We further discuss the\nadvantages and disadvantages of current anomaly detection techniques. Finally,\nwe present potential future research directions in anomaly detection on\ngraph-structured data.\n', '  This paper considers an important Graph Anomaly Detection (GAD) task, namely\nopen-set GAD, which aims to train a detection model using a small number of\nnormal and anomaly nodes (referred to as seen anomalies) to detect both seen\nanomalies and unseen anomalies (i.e., anomalies that cannot be illustrated the\ntraining anomalies). The availability of those labelled training data provides\ncrucial prior knowledge about abnormalities for GAD models, enabling\nsubstantially reduced detection errors. However, current methods tend to\nover-emphasise fitting the seen anomalies, leading to a weak generalisation\nability to detect the unseen anomalies. Further, they were introduced to handle\nEuclidean data, failing to effectively capture important information on graph\nstructure and node attributes for GAD. In this work, we propose a novel\nopen-set GAD approach, namely Normal Structure Regularisation (NSReg) to\nachieve generalised detection ability to unseen anomalies, while maintaining\nits effectiveness on detecting seen anomalies. The key idea in NSReg is to\nintroduce a regularisation term that enforces the learning of compact,\nsemantically-rich representations of normal nodes based on their structural\nrelations to other nodes. When being optimised with supervised anomaly\ndetection losses, the regularisation term helps incorporate strong normality\ninto the modelling, and thus, it effectively avoids the overfitting the seen\nanomalies solely. In doing so, it helps learn better normality decision\nboundary, reducing the errors of detecting unseen anomalies as normal.\nExtensive empirical results on seven real-world datasets show the superiority\nof NSReg for open-set GAD.\n']",Graph Anomaly Detection
195,194,47,194_personality_personalities_traits_trait,"['personality', 'personalities', 'traits', 'trait', 'personas', 'persona', 'profiles', 'conversational', 'profile', 'psychometrics']","['personality', 'traits', 'personalities', 'psychological', 'psychology', 'trait', 'assessment', 'tests', 'psychometrics', 'agreeableness']","['personality', 'conversational', 'psychometrics', 'chatbot', 'dialogues', 'narcissism', 'extroversion', 'questionnaires', 'behavior', 'emotion']","[""  This work investigates how personality expression and embodiment affect\npersonality perception and learning in educational conversational agents. We\nextend an existing personality-driven conversational agent framework by\nintegrating LLM-based conversation support tailored to an educational\napplication. We describe a user study built on this system to evaluate two\ndistinct personality styles: high extroversion and agreeableness and low\nextroversion and agreeableness. For each personality style, we assess three\nmodels: (1) a dialogue-only model that conveys personality through dialogue,\n(2) an animated human model that expresses personality solely through dialogue,\nand (3) an animated human model that expresses personality through both\ndialogue and body and facial animations. The results indicate that all models\nare positively perceived regarding both personality and learning outcomes.\nModels with high personality traits are perceived as more engaging than those\nwith low personality traits. We provide a comprehensive quantitative and\nqualitative analysis of perceived personality traits, learning parameters, and\nuser experiences based on participant ratings of the model types and\npersonality styles, as well as users' responses to open-ended questions.\n"", ""  Personality psychologists have analyzed the relationship between personality\nand safety behaviors in human society. Although Large Language Models (LLMs)\ndemonstrate personality traits, the relationship between personality traits and\nsafety abilities in LLMs still remains a mystery. In this paper, we discover\nthat LLMs' personality traits are closely related to their safety abilities,\ni.e., toxicity, privacy, and fairness, based on the reliable MBTI-M scale.\nMeanwhile, the safety alignment generally increases various LLMs' Extraversion,\nSensing, and Judging traits. According to such findings, we can edit LLMs'\npersonality traits and improve their safety performance, e.g., inducing\npersonality from ISTJ to ISTP resulted in a relative improvement of\napproximately 43% and 10% in privacy and fairness performance, respectively.\nAdditionally, we find that LLMs with different personality traits are\ndifferentially susceptible to jailbreak. This study pioneers the investigation\nof LLM safety from a personality perspective, providing new insights into LLM\nsafety enhancement.\n"", ""  Personality detection aims to detect one's personality traits underlying in\nsocial media posts. One challenge of this task is the scarcity of ground-truth\npersonality traits which are collected from self-report questionnaires. Most\nexisting methods learn post features directly by fine-tuning the pre-trained\nlanguage models under the supervision of limited personality labels. This leads\nto inferior quality of post features and consequently affects the performance.\nIn addition, they treat personality traits as one-hot classification labels,\noverlooking the semantic information within them. In this paper, we propose a\nlarge language model (LLM) based text augmentation enhanced personality\ndetection model, which distills the LLM's knowledge to enhance the small model\nfor personality detection, even when the LLM fails in this task. Specifically,\nwe enable LLM to generate post analyses (augmentations) from the aspects of\nsemantic, sentiment, and linguistic, which are critical for personality\ndetection. By using contrastive learning to pull them together in the embedding\nspace, the post encoder can better capture the psycho-linguistic information\nwithin the post representations, thus improving personality detection.\nFurthermore, we utilize the LLM to enrich the information of personality labels\nfor enhancing the detection performance. Experimental results on the benchmark\ndatasets demonstrate that our model outperforms the state-of-the-art methods on\npersonality detection.\n""]",Personality Traits in AI and Human Interaction
196,195,47,195_microscopy_microscope_optics_nanoscale,"['microscopy', 'microscope', 'optics', 'nanoscale', 'optical', 'diffraction', 'imaging', 'spectroscopy', 'nanostructures', 'raman']","['imaging', 'optical', 'phase', 'diffraction', 'scanning', 'spectroscopy', 'microscopy', 'ray', 'electron', 'tomography']","['microscopy', 'diffraction', 'nanostructures', 'raman', 'nanometer', 'fluorescence', 'wavelength', 'coherent', 'zernike', 'ptychography']","['  Scanning X-ray nanodiffraction microscopy is a powerful technique for\nspatially resolving nanoscale structural morphologies by diffraction contrast.\nOne of the critical challenges in experimental nanodiffraction data analysis is\nposed by the convergence angle of nanoscale focusing optics which creates\nsimultaneous dependency of the far-field scattering data on three independent\ncomponents of the local strain tensor - corresponding to dilation and two\npotential rigid body rotations of the unit cell. All three components are in\nprinciple resolvable through a spatially mapped sample tilt series however\ntraditional data analysis is computationally expensive and prone to artifacts.\nIn this study, we implement NanobeamNN, a convolutional neural network\nspecifically tailored to the analysis of scanning probe X-ray microscopy data.\nNanobeamNN learns lattice strain and rotation angles from simulated diffraction\nof a focused X-ray nanobeam by an epitaxial thin film and can directly make\nreasonable predictions on experimental data without the need for additional\nfine-tuning. We demonstrate that this approach represents a significant\nadvancement in computational speed over conventional methods, as well as a\npotential improvement in accuracy over the current standard.\n', '  Optical super-oscillation enables far-field super-resolution imaging beyond\ndiffraction limits. However, the existing super-oscillatory lens for the\nspatial super-resolution imaging system still confronts critical limitations in\nperformance due to the lack of a more advanced design method and the limited\ndesign degree of freedom. Here, we propose an optical super-oscillatory\ndiffractive neural network, i.e., SODNN, that can achieve super-resolved\nspatial resolution for imaging beyond the diffraction limit with superior\nperformance over existing methods. SODNN is constructed by utilizing\ndiffractive layers to implement optical interconnections and imaging samples or\nbiological sensors to implement nonlinearity, which modulates the incident\noptical field to create optical super-oscillation effects in 3D space and\ngenerate the super-resolved focal spots. By optimizing diffractive layers with\n3D optical field constraints under an incident wavelength size of $\\lambda$, we\nachieved a super-oscillatory spot with a full width at half maximum of\n0.407$\\lambda$ in the far field distance over 400$\\lambda$ without side-lobes\nover the field of view, having a long depth of field over 10$\\lambda$.\nFurthermore, the SODNN implements a multi-wavelength and multi-focus spot array\nthat effectively avoids chromatic aberrations. Our research work will inspire\nthe development of intelligent optical instruments to facilitate the\napplications of imaging, sensing, perception, etc.\n', '  Phase retrieval, the problem of recovering lost phase information from\nmeasured intensity alone, is an inverse problem that is widely faced in various\nimaging modalities ranging from astronomy to nanoscale imaging. The current\nprocess of phase recovery is iterative in nature. As a result, the image\nformation is time-consuming and computationally expensive, precluding real-time\nimaging. Here, we use 3D nanoscale X-ray imaging as a representative example to\ndevelop a deep learning model to address this phase retrieval problem. We\nintroduce 3D-CDI-NN, a deep convolutional neural network and differential\nprogramming framework trained to predict 3D structure and strain solely from\ninput 3D X-ray coherent scattering data. Our networks are designed to be\n""physics-aware"" in multiple aspects; in that the physics of x-ray scattering\nprocess is explicitly enforced in the training of the network, and the training\ndata are drawn from atomistic simulations that are representative of the\nphysics of the material. We further refine the neural network prediction\nthrough a physics-based optimization procedure to enable maximum accuracy at\nlowest computational cost. 3D-CDI-NN can invert a 3D coherent diffraction\npattern to real-space structure and strain hundreds of times faster than\ntraditional iterative phase retrieval methods, with negligible loss in\naccuracy. Our integrated machine learning and differential programming solution\nto the phase retrieval problem is broadly applicable across inverse problems in\nother application areas.\n']",Advanced Microscopy and Optical Imaging Techniques
197,196,47,196_argumentation_argumentative_arguments_debates,"['argumentation', 'argumentative', 'arguments', 'debates', 'debaters', 'debater', 'annotators', 'argument', 'debate', 'discussions']","['argument', 'arguments', 'argumentative', 'argumentation', 'debate', 'mining', 'debates', 'counter', 'perspectives', 'relations']","['argumentative', 'debaters', 'discussions', 'annotation', 'essays', 'topics', 'persuasive', 'corpora', 'assessment', 'stance']","['  Evaluating the quality of arguments is a crucial aspect of any system\nleveraging argument mining. However, it is a challenge to obtain reliable and\nconsistent annotations regarding argument quality, as this usually requires\ndomain-specific expertise of the annotators. Even among experts, the assessment\nof argument quality is often inconsistent due to the inherent subjectivity of\nthis task. In this paper, we study the potential of using state-of-the-art\nlarge language models (LLMs) as proxies for argument quality annotators. To\nassess the capability of LLMs in this regard, we analyze the agreement between\nmodel, human expert, and human novice annotators based on an established\ntaxonomy of argument quality dimensions. Our findings highlight that LLMs can\nproduce consistent annotations, with a moderately high agreement with human\nexperts across most of the quality dimensions. Moreover, we show that using\nLLMs as additional annotators can significantly improve the agreement between\nannotators. These results suggest that LLMs can serve as a valuable tool for\nautomated argument quality assessment, thus streamlining and accelerating the\nevaluation of large argument datasets.\n', '  Argument mining (AM) is defined as the task of automatically identifying and\nextracting argumentative components (e.g. premises, claims, etc.) and detecting\nthe existing relations among them (i.e., support, attack, no relations). Deep\nlearning models enable us to analyze arguments more efficiently than\ntraditional methods and extract their semantics. This paper presents\ncomparative studies between a few deep learning-based models in argument\nmining. The work concentrates on argument classification. The research was done\non a wide spectrum of datasets (Args.me, UKP, US2016). The main novelty of this\npaper is the ensemble model which is based on BERT architecture and ChatGPT-4\nas fine tuning model. The presented results show that BERT+ChatGPT-4\noutperforms the rest of the models including other Transformer-based and\nLSTM-based models. The observed improvement is, in most cases, greater than\n10The presented analysis can provide crucial insights into how the models for\nargument classification should be further improved. Additionally, it can help\ndevelop a prompt-based algorithm to eliminate argument classification errors.\n', '  Some of the major limitations identified in the areas of argument mining,\nargument generation, and natural language argument analysis are related to the\ncomplexity of annotating argumentatively rich data, the limited size of these\ncorpora, and the constraints that represent the different languages and domains\nin which these data is annotated. To address these limitations, in this paper\nwe present the following contributions: (i) an effective methodology for the\nautomatic generation of natural language arguments in different topics and\nlanguages, (ii) the largest publicly available corpus of natural language\nargumentation schemes, and (iii) a set of solid baselines and fine-tuned models\nfor the automatic identification of argumentation schemes.\n']",Argumentation and Debate Analysis
198,197,46,197_kalman_filtering_markovian_stochastic,"['kalman', 'filtering', 'markovian', 'stochastic', 'estimation', 'bayesian', 'filters', 'filter', 'probabilistic', 'state']","['filter', 'dynamical', 'filtering', 'equations', 'estimation', 'uncertainty', 'posterior', 'nonlinear', 'identification', 'quantification']","['kalman', 'filtering', 'stochastic', 'estimation', 'state', 'posterior', 'gpssm', 'gaussian', 'chaos', 'sdes']","[""  Bayesian filtering serves as the mainstream framework of state estimation in\ndynamic systems. Its standard version utilizes total probability rule and\nBayes' law alternatively, where how to define and compute conditional\nprobability is critical to state distribution inference. Previously, the\nconditional probability is assumed to be exactly known, which represents a\nmeasure of the occurrence probability of one event, given the second event. In\nthis paper, we find that by adding an additional event that stipulates an\ninequality condition, we can transform the conditional probability into a\nspecial integration that is analogous to convolution. Based on this\ntransformation, we show that both transition probability and output probability\ncan be generalized to convolutional forms, resulting in a more general\nfiltering framework that we call convolutional Bayesian filtering. This new\nframework encompasses standard Bayesian filtering as a special case when the\ndistance metric of the inequality condition is selected as Dirac delta\nfunction. It also allows for a more nuanced consideration of model mismatch by\nchoosing different types of inequality conditions. For instance, when the\ndistance metric is defined in a distributional sense, the transition\nprobability and output probability can be approximated by simply rescaling them\ninto fractional powers. Under this framework, a robust version of Kalman filter\ncan be constructed by only altering the noise covariance matrix, while\nmaintaining the conjugate nature of Gaussian distributions. Finally, we\nexemplify the effectiveness of our approach by reshaping classic filtering\nalgorithms into convolutional versions, including Kalman filter, extended\nKalman filter, unscented Kalman filter and particle filter.\n"", '  The problem of system identification for the Kalman filter, relying on the\nexpectation-maximization (EM) procedure to learn the underlying parameters of a\ndynamical system, has largely been studied assuming that observations are\nsampled at equally-spaced time points. However, in many applications this is a\nrestrictive and unrealistic assumption. This paper addresses system\nidentification for the continuous-discrete filter, with the aim of generalizing\nlearning for the Kalman filter by relying on a solution to a continuous-time\nIt\\^o stochastic differential equation (SDE) for the latent state and\ncovariance dynamics. We introduce a novel two-filter, analytical form for the\nposterior with a Bayesian derivation, which yields analytical updates which do\nnot require the forward-pass to be pre-computed. Using this analytical and\nefficient computation of the posterior, we provide an EM procedure which\nestimates the parameters of the SDE, naturally incorporating irregularly\nsampled measurements. Generalizing the learning of latent linear dynamical\nsystems (LDS) to continuous-time may extend the use of the hybrid Kalman filter\nto data which is not regularly sampled or has intermittent missing values, and\ncan extend the power of non-linear system identification methods such as\nswitching LDS (SLDS), which rely on EM for the linear discrete-time Kalman\nfilter as a sub-unit for learning locally linearized behavior of a non-linear\nsystem. We apply the method by learning the parameters of a latent,\nmultivariate Fokker-Planck SDE representing a toggle-switch genetic circuit\nusing biologically realistic parameters, and compare the efficacy of learning\nrelative to the discrete-time Kalman filter as the step-size irregularity and\nspectral-radius of the dynamics-matrix increases.\n', ""  The research topic is: data-driven Bayesian state estimation with compressed\nmeasurement (BSCM) of model-free process, say for a (causal) tracking\napplication. The dimension of the temporal measurement vector is lower than the\ndimension of the temporal state vector to be estimated. Hence the state\nestimation problem is an underdetermined inverse problem. The state-space-model\n(SSM) of the underlying dynamical process is assumed to be unknown and hence,\nwe use the terminology 'model-free process'. In absence of the SSM, we can not\nemploy traditional model-driven methods like Kalman Filter (KF) and Particle\nFilter (PF) and instead require data-driven methods. We first experimentally\nshow that two existing unsupervised learning-based data-driven methods fail to\naddress the BSCM problem for model-free process; they are data-driven nonlinear\nstate estimation (DANSE) method and deep Markov model (DMM) method. The\nunsupervised learning uses unlabelled data comprised of only noisy\nmeasurements. While DANSE provides a good predictive performance to model the\ntemporal measurement data as time-series, its unsupervised learning lacks a\nregularization for state estimation. We then investigate use of a\nsemi-supervised learning approach, and develop a semi-supervised learning-based\nDANSE method, referred to as SemiDANSE. In the semi-supervised learning, we use\na limited amount of labelled data along-with a large amount of unlabelled data,\nand that helps to bring the desired regularization for BSCM problem in the\nabsence of SSM. The labelled data means pairwise measurement-and-state data.\nUsing three chaotic dynamical systems (or processes) with nonlinear SSMs as\nbenchmark, we show that the data-driven SemiDANSE provides competitive\nperformance for BSCM against three SSM-informed methods - a hybrid method\ncalled KalmanNet, and two traditional model-driven methods called extended KF\nand unscented KF.\n""]",Kalman Filtering and State Estimation
199,198,45,198_lidar_lidars_3d_lasermix,"['lidar', 'lidars', '3d', 'lasermix', 'laser', 'sensing', 'cameras', 'fusion', 'spotnet', 'camera']","['lidar', 'driving', 'clouds', 'object', 'camera', 'point', 'autonomous', 'perception', 'fusion', 'detection']","['lidar', 'lasermix', 'cameras', 'fusion', 'spotnet', 'deepipcv2', 'supervised', 'uada3d', 'depth', 'ranging']","['  LiDAR point clouds have become the most common data source in autonomous\ndriving. However, due to the sparsity of point clouds, accurate and reliable\ndetection cannot be achieved in specific scenarios. Because of their\ncomplementarity with point clouds, images are getting increasing attention.\nAlthough with some success, existing fusion methods either perform hard fusion\nor do not fuse in a direct manner. In this paper, we propose a generic 3D\ndetection framework called MMFusion, using multi-modal features. The framework\naims to achieve accurate fusion between LiDAR and images to improve 3D\ndetection in complex scenes. Our framework consists of two separate streams:\nthe LiDAR stream and the camera stream, which can be compatible with any\nsingle-modal feature extraction network. The Voxel Local Perception Module in\nthe LiDAR stream enhances local feature representation, and then the\nMulti-modal Feature Fusion Module selectively combines feature output from\ndifferent streams to achieve better fusion. Extensive experiments have shown\nthat our framework not only outperforms existing benchmarks but also improves\ntheir detection, especially for detecting cyclists and pedestrians on KITTI\nbenchmarks, with strong robustness and generalization capabilities. Hopefully,\nour work will stimulate more research into multi-modal fusion for autonomous\ndriving tasks.\n', '  Efficient data utilization is crucial for advancing 3D scene understanding in\nautonomous driving, where reliance on heavily human-annotated LiDAR point\nclouds challenges fully supervised methods. Addressing this, our study extends\ninto semi-supervised learning for LiDAR semantic segmentation, leveraging the\nintrinsic spatial priors of driving scenes and multi-sensor complements to\naugment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved\nframework that integrates laser beam manipulations from disparate LiDAR scans\nand incorporates LiDAR-camera correspondences to further assist data-efficient\nlearning. Our framework is tailored to enhance 3D scene consistency\nregularization by incorporating multi-modality, including 1) multi-modal\nLaserMix operation for fine-grained cross-sensor interactions; 2)\ncamera-to-LiDAR feature distillation that enhances LiDAR feature learning; and\n3) language-driven knowledge guidance generating auxiliary supervisions using\nopen-vocabulary models. The versatility of LaserMix++ enables applications\nacross LiDAR representations, establishing it as a universally applicable\nsolution. Our framework is rigorously validated through theoretical analysis\nand extensive experiments on popular driving perception datasets. Results\ndemonstrate that LaserMix++ markedly outperforms fully supervised alternatives,\nachieving comparable accuracy with five times fewer annotations and\nsignificantly improving the supervised-only baselines. This substantial\nadvancement underscores the potential of semi-supervised approaches in reducing\nthe reliance on extensive labeled data in LiDAR-based 3D scene understanding\nsystems.\n', ""  With the widespread application of Light Detection and Ranging (LiDAR)\ntechnology in fields such as autonomous driving, robot navigation, and terrain\nmapping, the importance of edge detection in LiDAR images has become\nincreasingly prominent. Traditional edge detection methods often face\nchallenges in accuracy and computational complexity when processing LiDAR\nimages. To address these issues, this study proposes an edge detection method\nfor LiDAR images based on artificial intelligence technology. This paper first\nreviews the current state of research on LiDAR technology and image edge\ndetection, introducing common edge detection algorithms and their applications\nin LiDAR image processing. Subsequently, a deep learning-based edge detection\nmodel is designed and implemented, optimizing the model training process\nthrough preprocessing and enhancement of the LiDAR image dataset. Experimental\nresults indicate that the proposed method outperforms traditional methods in\nterms of detection accuracy and computational efficiency, showing significant\npractical application value. Finally, improvement strategies are proposed for\nthe current method's shortcomings, and the improvements are validated through\nexperiments.\n""]",LiDAR Technology and Sensor Fusion for 3D Scene Understanding
200,199,45,199_satisfiability_decidability_queries_decidable,"['satisfiability', 'decidability', 'queries', 'decidable', 'constraints', 'semantics', 'completeness', 'logics', 'querying', 'undecidability']","['satisfiability', 'repairs', 'logics', 'clause', 'complete', 'logic', 'propositional', 'decidable', 'fragments', 'counting']","['satisfiability', 'completeness', 'boolean', 'maxsat', 'solvers', 'sparql', 'clause', 'ontology', 'undecidable', 'extensions']","['  We investigate the impact of non-regular path expressions on the decidability\nof satisfiability checking and querying in description logics extending ALC.\nOur primary objects of interest are ALCreg and ALCvpl, the extensions of with\npath expressions employing, respectively, regular and visibly-pushdown\nlanguages. The first one, ALCreg, is a notational variant of the well-known\nPropositional Dynamic Logic of Fischer and Ladner. The second one, ALCvpl, was\nintroduced and investigated by Loding and Serre in 2007. The logic ALCvpl\ngeneralises many known decidable non-regular extensions of ALCreg.\n  We provide a series of undecidability results. First, we show that\ndecidability of the concept satisfiability problem for ALCvpl is lost upon\nadding the seemingly innocent Self operator. Second, we establish\nundecidability for the concept satisfiability problem for ALCvpl extended with\nnominals. Interestingly, our undecidability proof relies only on one single\nnon-regular (visibly-pushdown) language, namely on r#s# := { r^n s^n | n in N }\nfor fixed role names r and s. Finally, in contrast to the classical database\nsetting, we establish undecidability of query entailment for queries involving\nnon-regular atoms from r#s#, already in the case of ALC-TBoxes.\n', '  The data-complexity of both satisfiability and finite satisfiability for the\ntwo-variable fragment with counting is NP-complete; the data-complexity of both\nquery-answering and finite query-answering for the two-variable guarded\nfragment with counting is co-NP-complete.\n', '  In this paper, we explore the issue of inconsistency handling over\nprioritized knowledge bases (KBs), which consist of an ontology, a set of\nfacts, and a priority relation between conflicting facts. In the database\nsetting, a closely related scenario has been studied and led to the definition\nof three different notions of optimal repairs (global, Pareto, and completion)\nof a prioritized inconsistent database. After transferring the notions of\nglobally-, Pareto- and completion-optimal repairs to our setting, we study the\ndata complexity of the core reasoning tasks: query entailment under\ninconsistency-tolerant semantics based upon optimal repairs, existence of a\nunique optimal repair, and enumeration of all optimal repairs. Our results\nprovide a nearly complete picture of the data complexity of these tasks for\nontologies formulated in common DL-Lite dialects. The second contribution of\nour work is to clarify the relationship between optimal repairs and different\nnotions of extensions for (set-based) argumentation frameworks. Among our\nresults, we show that Pareto-optimal repairs correspond precisely to stable\nextensions (and often also to preferred extensions), and we propose a novel\nsemantics for prioritized KBs which is inspired by grounded extensions and\nenjoys favourable computational properties. Our study also yields some results\nof independent interest concerning preference-based argumentation frameworks.\n']","""Computational Complexity of Logical Queries and Constraints"""
201,200,45,200_tensors_tensor_multilinear_factorization,"['tensors', 'tensor', 'multilinear', 'factorization', 'tucker', 'gradient', 'scalar', 'decompositions', 'riemannian', 'decomposition']","['tensor', 'tensors', 'decomposition', 'rank', 'completion', 'recovery', 'tubal', 'multilinear', 'decompositions', 'factor']","['tensors', 'factorization', 'tucker', 'matrix', 'pca', 'rank', 'parafac2', 'apmm', 'orthogonal', 'decomposing']","['  We proposed the tensor-input tree (TT) method for scalar-on-tensor and\ntensor-on-tensor regression problems. We first address scalar-on-tensor problem\nby proposing scalar-output regression tree models whose input variable are\ntensors (i.e., multi-way arrays). We devised and implemented fast randomized\nand deterministic algorithms for efficient fitting of scalar-on-tensor trees,\nmaking TT competitive against tensor-input GP models. Based on scalar-on-tensor\ntree models, we extend our method to tensor-on-tensor problems using additive\ntree ensemble approaches. Theoretical justification and extensive experiments\non real and synthetic datasets are provided to illustrate the performance of\nTT.\n', '  We study the tensor-on-tensor regression, where the goal is to connect tensor\nresponses to tensor covariates with a low Tucker rank parameter tensor/matrix\nwithout the prior knowledge of its intrinsic rank. We propose the Riemannian\ngradient descent (RGD) and Riemannian Gauss-Newton (RGN) methods and cope with\nthe challenge of unknown rank by studying the effect of rank\nover-parameterization. We provide the first convergence guarantee for the\ngeneral tensor-on-tensor regression by showing that RGD and RGN respectively\nconverge linearly and quadratically to a statistically optimal estimate in both\nrank correctly-parameterized and over-parameterized settings. Our theory\nreveals an intriguing phenomenon: Riemannian optimization methods naturally\nadapt to over-parameterization without modifications to their implementation.\nWe also prove the statistical-computational gap in scalar-on-tensor regression\nby a direct low-degree polynomial argument. Our theory demonstrates a ""blessing\nof statistical-computational gap"" phenomenon: in a wide range of scenarios in\ntensor-on-tensor regression for tensors of order three or higher, the\ncomputationally required sample size matches what is needed by moderate rank\nover-parameterization when considering computationally feasible estimators,\nwhile there are no such benefits in the matrix settings. This shows moderate\nrank over-parameterization is essentially ""cost-free"" in terms of sample size\nin tensor-on-tensor regression of order three or higher. Finally, we conduct\nsimulation studies to show the advantages of our proposed methods and to\ncorroborate our theoretical findings.\n', ""  This paper studies the prediction task of tensor-on-tensor regression in\nwhich both covariates and responses are multi-dimensional arrays (a.k.a.,\ntensors) across time with arbitrary tensor order and data dimension. Existing\nmethods either focused on linear models without accounting for possibly\nnonlinear relationships between covariates and responses, or directly employed\nblack-box deep learning algorithms that failed to utilize the inherent tensor\nstructure. In this work, we propose a Factor Augmented Tensor-on-Tensor Neural\nNetwork (FATTNN) that integrates tensor factor models into deep neural\nnetworks. We begin with summarizing and extracting useful predictive\ninformation (represented by the ``factor tensor'') from the complex structured\ntensor covariates, and then proceed with the prediction task using the\nestimated factor tensor as input of a temporal convolutional neural network.\nThe proposed methods effectively handle nonlinearity between complex data\nstructures, and improve over traditional statistical models and conventional\ndeep learning approaches in both prediction accuracy and computational cost. By\nleveraging tensor factor models, our proposed methods exploit the underlying\nlatent factor structure to enhance the prediction, and in the meantime,\ndrastically reduce the data dimensionality that speeds up the computation. The\nempirical performances of our proposed methods are demonstrated via simulation\nstudies and real-world applications to three public datasets. Numerical results\nshow that our proposed algorithms achieve substantial increases in prediction\naccuracy and significant reductions in computational time compared to benchmark\nmethods.\n""]",Tensor Regression and Factorization Methods
202,201,45,201_pose_poses_articulated_3d,"['pose', 'poses', 'articulated', '3d', 'camera', 'cameras', 'animal3d', 'posture', 'human3', '3dhp']","['pose', 'hand', 'body', 'poses', 'estimation', 'motion', 'keypoints', 'posture', 'joint', 'human']","['poses', 'articulated', 'cameras', 'animal3d', 'convolutions', 'reconstructing', 'keypoints', 'liftnet', '4d', 'attentionhand']","['  Accurately estimating the 3D pose and shape is an essential step towards\nunderstanding animal behavior, and can potentially benefit many downstream\napplications, such as wildlife conservation. However, research in this area is\nheld back by the lack of a comprehensive and diverse dataset with high-quality\n3D pose and shape annotations. In this paper, we propose Animal3D, the first\ncomprehensive dataset for mammal animal 3D pose and shape estimation. Animal3D\nconsists of 3379 images collected from 40 mammal species, high-quality\nannotations of 26 keypoints, and importantly the pose and shape parameters of\nthe SMAL model. All annotations were labeled and checked manually in a\nmulti-stage process to ensure highest quality results. Based on the Animal3D\ndataset, we benchmark representative shape and pose estimation models at: (1)\nsupervised learning from only the Animal3D data, (2) synthetic to real transfer\nfrom synthetically generated images, and (3) fine-tuning human pose and shape\nestimation models. Our experimental results demonstrate that predicting the 3D\nshape and pose of animals across species remains a very challenging task,\ndespite significant advances in human pose estimation. Our results further\ndemonstrate that synthetic pre-training is a viable strategy to boost the model\nperformance. Overall, Animal3D opens new directions for facilitating future\nresearch in animal 3D pose and shape estimation, and is publicly available.\n', '  Monocular Human Pose Estimation (HPE) aims at determining the 3D positions of\nhuman joints from a single 2D image captured by a camera. However, a single 2D\npoint in the image may correspond to multiple points in 3D space. Typically,\nthe uniqueness of the 2D-3D relationship is approximated using an orthographic\nor weak-perspective camera model. In this study, instead of relying on\napproximations, we advocate for utilizing the full perspective camera model.\nThis involves estimating camera parameters and establishing a precise,\nunambiguous 2D-3D relationship. To do so, we introduce the EPOCH framework,\ncomprising two main components: the pose lifter network (LiftNet) and the pose\nregressor network (RegNet). LiftNet utilizes the full perspective camera model\nto precisely estimate the 3D pose in an unsupervised manner. It takes a 2D pose\nand camera parameters as inputs and produces the corresponding 3D pose\nestimation. These inputs are obtained from RegNet, which starts from a single\nimage and provides estimates for the 2D pose and camera parameters. RegNet\nutilizes only 2D pose data as weak supervision. Internally, RegNet predicts a\n3D pose, which is then projected to 2D using the estimated camera parameters.\nThis process enables RegNet to establish the unambiguous 2D-3D relationship.\nOur experiments show that modeling the lifting as an unsupervised task with a\ncamera in-the-loop results in better generalization to unseen data. We obtain\nstate-of-the-art results for the 3D HPE on the Human3.6M and MPI-INF-3DHP\ndatasets. Our code is available at: [Github link upon acceptance, see\nsupplementary materials].\n', '  Its numerous applications make multi-human 3D pose estimation a remarkably\nimpactful area of research. Nevertheless, assuming a multiple-view system\ncomposed of several regular RGB cameras, 3D multi-pose estimation presents\nseveral challenges. First of all, each person must be uniquely identified in\nthe different views to separate the 2D information provided by the cameras.\nSecondly, the 3D pose estimation process from the multi-view 2D information of\neach person must be robust against noise and potential occlusions in the\nscenario. In this work, we address these two challenges with the help of deep\nlearning. Specifically, we present a model based on Graph Neural Networks\ncapable of predicting the cross-view correspondence of the people in the\nscenario along with a Multilayer Perceptron that takes the 2D points to yield\nthe 3D poses of each person. These two models are trained in a self-supervised\nmanner, thus avoiding the need for large datasets with 3D annotations.\n']",3D Pose Estimation
203,202,45,202_learns_learning_learned_training,"['learns', 'learning', 'learned', 'training', 'learn', 'trained', 'learners', 'overfitting', 'neural', 'learner']","['meta', 'inductive', 'task', 'learning', 'loop', 'tasks', 'outer', 'learn', 'agnostic', 'inner']","['learns', 'overfitting', 'multitask', 'gradient', 'optimizers', 'shot', 'metalearn', 'descent', 'general', 'metadistribution']","['  There is a growing interest in the learning-to-learn paradigm, also known as\nmeta-learning, where models infer on new tasks using a few training examples.\nRecently, meta-learning based methods have been widely used in few-shot\nclassification, regression, reinforcement learning, and domain adaptation. The\nmodel-agnostic meta-learning (MAML) algorithm is a well-known algorithm that\nobtains model parameter initialization at meta-training phase. In the meta-test\nphase, this initialization is rapidly adapted to new tasks by using gradient\ndescent. However, meta-learning models are prone to overfitting since there are\ninsufficient training tasks resulting in over-parameterized models with poor\ngeneralization performance for unseen tasks. In this paper, we propose a\nBayesian neural network based MAML algorithm, which we refer to as the B-SMALL\nalgorithm. The proposed framework incorporates a sparse variational loss term\nalongside the loss function of MAML, which uses a sparsifying approximated KL\ndivergence as a regularizer. We demonstrate the performance of B-MAML using\nclassification and regression tasks, and highlight that training a sparsifying\nBNN using MAML indeed improves the parameter footprint of the model while\nperforming at par or even outperforming the MAML approach. We also illustrate\napplicability of our approach in distributed sensor networks, where sparsity\nand meta-learning can be beneficial.\n', ""  As a subset of machine learning, meta-learning, or learning to learn, aims at\nimproving the model's capabilities by employing prior knowledge and experience.\nA meta-learning paradigm can appropriately tackle the conventional challenges\nof traditional learning approaches, such as insufficient number of samples,\ndomain shifts, and generalization. These unique characteristics position\nmeta-learning as a suitable choice for developing influential solutions in\nvarious healthcare contexts, where the available data is often insufficient,\nand the data collection methodologies are different. This survey discusses\nmeta-learning broad applications in the healthcare domain to provide insight\ninto how and where it can address critical healthcare challenges. We first\ndescribe the theoretical foundations and pivotal methods of meta-learning. We\nthen divide the employed meta-learning approaches in the healthcare domain into\ntwo main categories of multi/single-task learning and many/few-shot learning\nand survey the studies. Finally, we highlight the current challenges in\nmeta-learning research, discuss the potential solutions, and provide future\nperspectives on meta-learning in healthcare.\n"", '  Few-shot learning, a challenging task in machine learning, aims to learn a\nclassifier adaptable to recognize new, unseen classes with limited labeled\nexamples. Meta-learning has emerged as a prominent framework for few-shot\nlearning. Its training framework is originally a task-level learning method,\nsuch as Model-Agnostic Meta-Learning (MAML) and Prototypical Networks. And a\nrecently proposed training paradigm called Meta-Baseline, which consists of\nsequential pre-training and meta-training stages, gains state-of-the-art\nperformance. However, as a non-end-to-end training method, indicating the\nmeta-training stage can only begin after the completion of pre-training,\nMeta-Baseline suffers from higher training cost and suboptimal performance due\nto the inherent conflicts of the two training stages. To address these\nlimitations, we propose an end-to-end training paradigm consisting of two\nalternative loops. In the outer loop, we calculate cross entropy loss on the\nentire training set while updating only the final linear layer. In the inner\nloop, we employ the original meta-learning training mode to calculate the loss\nand incorporate gradients from the outer loss to guide the parameter updates.\nThis training paradigm not only converges quickly but also outperforms existing\nbaselines, indicating that information from the overall training set and the\nmeta-learning training paradigm could mutually reinforce one another. Moreover,\nbeing model-agnostic, our framework achieves significant performance gains,\nsurpassing the baseline systems by approximate 1%.\n']",Meta-Learning and Few-Shot Learning in Machine Learning
204,203,45,203_gans_gan_generative_adversarial,"['gans', 'gan', 'generative', 'adversarial', 'vanillagan', 'inception', 'cyclegan', 'wgan', 'cdvgan', 'fetsgan']","['generative', 'distance', 'distribution', 'inception', 'generator', 'distributions', 'adversarial', 'samples', 'collapse', 'discriminator']","['gans', 'adversarial', 'vanillagan', 'wasserstein', 'overparameterization', 'divergences', 'dimension', 'bigans', 'discriminators', 'idempotent']","['  The empirical success of Generative Adversarial Networks (GANs) caused an\nincreasing interest in theoretical research. The statistical literature is\nmainly focused on Wasserstein GANs and generalizations thereof, which\nespecially allow for good dimension reduction properties. Statistical results\nfor Vanilla GANs, the original optimization problem, are still rather limited\nand require assumptions such as smooth activation functions and equal\ndimensions of the latent space and the ambient space. To bridge this gap, we\ndraw a connection from Vanilla GANs to the Wasserstein distance. By doing so,\nexisting results for Wasserstein GANs can be extended to Vanilla GANs. In\nparticular, we obtain an oracle inequality for Vanilla GANs in Wasserstein\ndistance. The assumptions of this oracle inequality are designed to be\nsatisfied by network architectures commonly used in practice, such as\nfeedforward ReLU networks. By providing a quantitative result for the\napproximation of a Lipschitz function by a feedforward ReLU network with\nbounded H\\""older norm, we conclude a rate of convergence for Vanilla GANs as\nwell as Wasserstein GANs as estimators of the unknown probability distribution.\n', '  We investigate the impact of the input dimension on the generalization error\nin generative adversarial networks (GANs). In particular, we first provide both\ntheoretical and practical evidence to validate the existence of an optimal\ninput dimension (OID) that minimizes the generalization error. Then, to\nidentify the OID, we introduce a novel framework called generalized GANs\n(G-GANs), which includes existing GANs as a special case. By incorporating the\ngroup penalty and the architecture penalty developed in the paper, G-GANs have\nseveral intriguing features. First, our framework offers adaptive\ndimensionality reduction from the initial dimension to a dimension necessary\nfor generating the target distribution. Second, this reduction in\ndimensionality also shrinks the required size of the generator network\narchitecture, which is automatically identified by the proposed architecture\npenalty. Both reductions in dimensionality and the generator network\nsignificantly improve the stability and the accuracy of the estimation and\nprediction. Theoretical support for the consistent selection of the input\ndimension and the generator network is provided. Third, the proposed algorithm\ninvolves an end-to-end training process, and the algorithm allows for dynamic\nadjustments between the input dimension and the generator network during\ntraining, further enhancing the overall performance of G-GANs. Extensive\nexperiments conducted with simulated and benchmark data demonstrate the\nsuperior performance of G-GANs. In particular, compared to that of\noff-the-shelf methods, G-GANs achieves an average improvement of 45.68% in the\nCT slice dataset, 43.22% in the MNIST dataset and 46.94% in the FashionMNIST\ndataset in terms of the maximum mean discrepancy or Frechet inception distance.\nMoreover, the features generated based on the input dimensions identified by\nG-GANs align with visually significant features.\n', ""  Modern GANs achieve remarkable performance in terms of generating realistic\nand diverse samples. This has led many to believe that ``GANs capture the\ntraining data manifold''. In this work we show that this interpretation is\nwrong. We empirically show that the manifold learned by modern GANs does not\nfit the training distribution: specifically the manifold does not pass through\nthe training examples and passes closer to out-of-distribution images than to\nin-distribution images. We also investigate the distribution over images\nimplied by the prior over the latent codes and study whether modern GANs learn\na density that approximates the training distribution. Surprisingly, we find\nthat the learned density is very far from the data distribution and that GANs\ntend to assign higher density to out-of-distribution images. Finally, we\ndemonstrate that the set of images used to train modern GANs are often not part\nof the typical set described by the GANs' distribution.\n""]",Generative Adversarial Networks (GANs) Research
205,204,44,204_documentation_datasets_dataset_provenance,"['documentation', 'datasets', 'dataset', 'provenance', 'analytics', 'researchers', 'frameworks', 'tools', 'insights', 'guidelines']","['documentation', 'centric', 'cards', 'data', 'quality', 'considerations', 'science', 'maintenance', 'practices', 'researchers']","['documentation', 'dataset', 'provenance', 'tools', 'trends', 'repository', 'stakeholders', 'dcai', 'platform', 'escience']","['  The rapidly evolving fields of Machine Learning (ML) and Artificial\nIntelligence have witnessed the emergence of platforms like Hugging Face (HF)\nas central hubs for model development and sharing. This experience report\nsynthesizes insights from two comprehensive studies conducted on HF, focusing\non carbon emissions and the evolutionary and maintenance aspects of ML models.\nOur objective is to provide a practical guide for future researchers embarking\non mining software repository studies within the HF ecosystem to enhance the\nquality of these studies. We delve into the intricacies of the replication\npackage used in our studies, highlighting the pivotal tools and methodologies\nthat facilitated our analysis. Furthermore, we propose a nuanced stratified\nsampling strategy tailored for the diverse HF Hub dataset, ensuring a\nrepresentative and comprehensive analytical approach. The report also\nintroduces preliminary guidelines, transitioning from repository mining to\ncohort studies, to establish causality in repository mining studies,\nparticularly within the ML model of HF context. This transition is inspired by\nexisting frameworks and is adapted to suit the unique characteristics of the HF\nmodel ecosystem. Our report serves as a guiding framework for researchers,\ncontributing to the responsible and sustainable advancement of ML, and\nfostering a deeper understanding of the broader implications of ML models.\n', '  Model stores offer third-party ML models and datasets for easy project\nintegration, minimizing coding efforts. One might hope to find detailed\nspecifications of these models and datasets in the documentation, leveraging\ndocumentation standards such as model and dataset cards. In this study, we use\nstatistical analysis and hybrid card sorting to assess the state of the\npractice of documenting model cards and dataset cards in one of the largest\nmodel stores in use today--Hugging Face (HF). Our findings show that only\n21,902 models (39.62\\%) and 1,925 datasets (28.48\\%) have documentation.\nFurthermore, we observe inconsistency in ethics and transparency-related\ndocumentation for ML models and datasets.\n', ""  Advances in machine learning are closely tied to the creation of datasets.\nWhile data documentation is widely recognized as essential to the reliability,\nreproducibility, and transparency of ML, we lack a systematic empirical\nunderstanding of current dataset documentation practices. To shed light on this\nquestion, here we take Hugging Face -- one of the largest platforms for sharing\nand collaborating on ML models and datasets -- as a prominent case study. By\nanalyzing all 7,433 dataset documentation on Hugging Face, our investigation\nprovides an overview of the Hugging Face dataset ecosystem and insights into\ndataset documentation practices, yielding 5 main findings: (1) The dataset card\ncompletion rate shows marked heterogeneity correlated with dataset popularity.\n(2) A granular examination of each section within the dataset card reveals that\nthe practitioners seem to prioritize Dataset Description and Dataset Structure\nsections, while the Considerations for Using the Data section receives the\nlowest proportion of content. (3) By analyzing the subsections within each\nsection and utilizing topic modeling to identify key topics, we uncover what is\ndiscussed in each section, and underscore significant themes encompassing both\ntechnical and social impacts, as well as limitations within the Considerations\nfor Using the Data section. (4) Our findings also highlight the need for\nimproved accessibility and reproducibility of datasets in the Usage sections.\n(5) In addition, our human annotation evaluation emphasizes the pivotal role of\ncomprehensive dataset content in shaping individuals' perceptions of a dataset\ncard's overall quality. Overall, our study offers a unique perspective on\nanalyzing dataset documentation through large-scale data science analysis and\nunderlines the need for more thorough dataset documentation in machine learning\nresearch.\n""]",Machine Learning Dataset Documentation and Provenance
206,205,43,205_ai_healthcare_medical_biomedical,"['ai', 'healthcare', 'medical', 'biomedical', 'medicine', 'med', 'clinical', 'trustworthy', 'patients', 'mllms']","['healthcare', 'medical', 'clinical', 'patient', 'medicine', 'ethical', 'regulatory', 'imaging', 'care', 'trustworthy']","['ai', 'biomedical', 'patients', 'mllms', 'ethical', 'fda', 'medai', 'explainability', 'trustworthiness', 'sepsislab']","['  This paper explores the significant impact of AI-based medical devices,\nincluding wearables, telemedicine, large language models, and digital twins, on\nclinical decision support systems. It emphasizes the importance of producing\noutcomes that are not only accurate but also interpretable and understandable\nto clinicians, addressing the risk that lack of interpretability poses in terms\nof mistrust and reluctance to adopt these technologies in healthcare. The paper\nreviews interpretable AI processes, methods, applications, and the challenges\nof implementation in healthcare, focusing on quality control to facilitate\nresponsible communication between AI systems and clinicians. It breaks down the\ninterpretability process into data pre-processing, model selection, and\npost-processing, aiming to foster a comprehensive understanding of the crucial\nrole of a robust interpretability approach in healthcare and to guide future\nresearch in this area. with insights for creating responsible clinician-AI\ntools for healthcare, as well as to offer a deeper understanding of the\nchallenges they might face. Our research questions, eligibility criteria and\nprimary goals were identified using Preferred Reporting Items for Systematic\nreviews and Meta-Analyses guideline and PICO method; PubMed, Scopus and Web of\nScience databases were systematically searched using sensitive and specific\nsearch strings. In the end, 52 publications were selected for data extraction\nwhich included 8 existing reviews and 44 related experimental studies. The\npaper offers general concepts of interpretable AI in healthcare and discuss\nthree-levels interpretability process. Additionally, it provides a\ncomprehensive discussion of evaluating robust interpretability AI in\nhealthcare. Moreover, this survey introduces a step-by-step roadmap for\nimplementing responsible AI in healthcare.\n', ""  The recent advancements in artificial intelligence (AI) combined with the\nextensive amount of data generated by today's clinical systems, has led to the\ndevelopment of imaging AI solutions across the whole value chain of medical\nimaging, including image reconstruction, medical image segmentation,\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\nfuture potential of AI in medical imaging, many stakeholders are concerned of\nthe potential risks and ethical implications of imaging AI solutions, which are\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\nin critical clinical applications. Addressing these concerns and risks, the\nFUTURE-AI framework has been proposed, which, sourced from a global\nmulti-domain expert consensus, comprises guiding principles for increased\ntrust, safety, and adoption for AI in healthcare. In this paper, we transform\nthe general FUTURE-AI healthcare principles to a concise and specific AI\nimplementation guide tailored to the needs of the medical imaging community. To\nthis end, we carefully assess each building block of the FUTURE-AI framework\nconsisting of (i) Fairness, (ii) Universality, (iii) Traceability, (iv)\nUsability, (v) Robustness and (vi) Explainability, and respectively define\nconcrete best practices based on accumulated AI implementation experiences from\nfive large European projects on AI in Health Imaging. We accompany our concrete\nstep-by-step medical imaging development guide with a practical AI solution\nmaturity checklist, thus enabling AI development teams to design, evaluate,\nmaintain, and deploy technically, clinically and ethically trustworthy imaging\nAI solutions into clinical practice.\n"", '  Despite major advances in artificial intelligence (AI) for medicine and\nhealthcare, the deployment and adoption of AI technologies remain limited in\nreal-world clinical practice. In recent years, concerns have been raised about\nthe technical, clinical, ethical and legal risks associated with medical AI. To\nincrease real world adoption, it is essential that medical AI tools are trusted\nand accepted by patients, clinicians, health organisations and authorities.\nThis work describes the FUTURE-AI guideline as the first international\nconsensus framework for guiding the development and deployment of trustworthy\nAI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and\ncurrently comprises 118 inter-disciplinary experts from 51 countries\nrepresenting all continents, including AI scientists, clinicians, ethicists,\nand social scientists. Over a two-year period, the consortium defined guiding\nprinciples and best practices for trustworthy AI through an iterative process\ncomprising an in-depth literature review, a modified Delphi survey, and online\nconsensus meetings. The FUTURE-AI framework was established based on 6 guiding\nprinciples for trustworthy AI in healthcare, i.e. Fairness, Universality,\nTraceability, Usability, Robustness and Explainability. Through consensus, a\nset of 28 best practices were defined, addressing technical, clinical, legal\nand socio-ethical dimensions. The recommendations cover the entire lifecycle of\nmedical AI, from design, development and validation to regulation, deployment,\nand monitoring. FUTURE-AI is a risk-informed, assumption-free guideline which\nprovides a structured approach for constructing medical AI tools that will be\ntrusted, deployed and adopted in real-world practice. Researchers are\nencouraged to take the recommendations into account in proof-of-concept stages\nto facilitate future translation towards clinical practice of medical AI.\n']","""Trustworthy AI in Healthcare"""
207,206,43,206_deformation_elastic_modeling_macroscale,"['deformation', 'elastic', 'modeling', 'macroscale', 'microstructures', 'multiscale', 'micromodel', 'neural', 'microscale', 'microstructure']","['constitutive', 'stress', 'material', 'strain', 'materials', 'homogenization', 'microscale', 'inelastic', 'elastic', 'bubble']","['elastic', 'microstructures', 'multiscale', 'micromodel', 'mechanical', 'stresses', 'viscoelastic', 'simulations', 'metamaterials', 'toughness']","['  Identifying constitutive parameters in engineering and biological materials,\nparticularly those with intricate geometries and mechanical behaviors, remains\na longstanding challenge. The recent advent of Physics-Informed Neural Networks\n(PINNs) offers promising solutions, but current frameworks are often limited to\nbasic constitutive laws and encounter practical constraints when combined with\nexperimental data. In this paper, we introduce a robust PINN-based framework\ndesigned to identify material parameters for soft materials, specifically those\nexhibiting complex constitutive behaviors, under large deformation in plane\nstress conditions. Distinctively, our model emphasizes training PINNs with\nmulti-modal synthetic experimental datasets consisting of full-field\ndeformation and loading history, ensuring algorithm robustness even with noisy\ndata. Our results reveal that the PINNs framework can accurately identify\nconstitutive parameters of the incompressible Arruda-Boyce model for samples\nwith intricate geometries, maintaining an error below 5%, even with an\nexperimental noise level of 5%. We believe our framework provides a robust\nmodulus identification approach for complex solids, especially for those with\ngeometrical and constitutive complexity.\n', '  Accurately modeling the mechanical behavior of materials is crucial for\nnumerous engineering applications. The quality of these models depends directly\non the accuracy of the constitutive law that defines the stress-strain\nrelation. Discovering these constitutive material laws remains a significant\nchallenge, in particular when only material deformation data is available. To\naddress this challenge, unsupervised machine learning methods have been\nproposed. However, existing approaches have several limitations: they either\nfail to ensure that the learned constitutive relations are consistent with\nphysical principles, or they rely on a predefined library of constitutive\nrelations or manually crafted input features. These dependencies require\nsignificant expertise and specialized domain knowledge. Here, we introduce a\nmachine learning approach called uLED, which overcomes the limitations by using\nthe input convex neural network (ICNN) as the surrogate constitutive model. We\nimprove the optimization strategy for training ICNN, allowing it to be trained\nend-to-end using direct strain invariants as input across various materials.\nFurthermore, we utilize the nodal force equilibrium at the internal domain as\nthe training objective, which enables us to learn the constitutive relation\nsolely from temporal displacement recordings. We validate the effectiveness of\nthe proposed method on a diverse range of material laws. We demonstrate that it\nis robust to a significant level of noise and that it converges to the ground\ntruth with increasing data resolution. We also show that the model can be\neffectively trained using a displacement field from a subdomain of the test\nspecimen and that the learned constitutive relation from one material sample is\ntransferable to other samples with different geometries. The developed\nmethodology provides an effective tool for discovering constitutive relations.\n', '  Multiscale partial differential equations (PDEs) arise in various\napplications, and several schemes have been developed to solve them\nefficiently. Homogenization theory is a powerful methodology that eliminates\nthe small-scale dependence, resulting in simplified equations that are\ncomputationally tractable while accurately predicting the macroscopic response.\nIn the field of continuum mechanics, homogenization is crucial for deriving\nconstitutive laws that incorporate microscale physics in order to formulate\nbalance laws for the macroscopic quantities of interest. However, obtaining\nhomogenized constitutive laws is often challenging as they do not in general\nhave an analytic form and can exhibit phenomena not present on the microscale.\nIn response, data-driven learning of the constitutive law has been proposed as\nappropriate for this task. However, a major challenge in data-driven learning\napproaches for this problem has remained unexplored: the impact of\ndiscontinuities and corner interfaces in the underlying material. These\ndiscontinuities in the coefficients affect the smoothness of the solutions of\nthe underlying equations. Given the prevalence of discontinuous materials in\ncontinuum mechanics applications, it is important to address the challenge of\nlearning in this context; in particular, to develop underpinning theory that\nestablishes the reliability of data-driven methods in this scientific domain.\nThe paper addresses this unexplored challenge by investigating the learnability\nof homogenized constitutive laws for elliptic operators in the presence of such\ncomplexities. Approximation theory is presented, and numerical experiments are\nperformed which validate the theory in the context of learning the solution\noperator defined by the cell problem arising in homogenization for elliptic\nPDEs.\n']",Material Deformation Modeling
208,207,43,207_ocr_handwriting_handwritten_recognition,"['ocr', 'handwriting', 'handwritten', 'recognition', 'text', 'tesseract', 'manuscripts', 'locr', 'arabic', 'documents']","['handwritten', 'character', 'handwriting', 'recognition', 'characters', 'manuscripts', 'script', 'ancient', 'document', 'line']","['ocr', 'handwriting', 'tesseract', 'manuscripts', 'epigraphy', 'coptic', 'scripts', 'dataset', 'extraction', 'pages']","['  In the fields of Optical Character Recognition (OCR) and Natural Language\nProcessing (NLP), integrating multilingual capabilities remains a critical\nchallenge, especially when considering languages with complex scripts such as\nArabic. This paper introduces the Comprehensive Post-OCR Parsing and Receipt\nUnderstanding Dataset (CORU), a novel dataset specifically designed to enhance\nOCR and information extraction from receipts in multilingual contexts involving\nArabic and English. CORU consists of over 20,000 annotated receipts from\ndiverse retail settings, including supermarkets and clothing stores, alongside\n30,000 annotated images for OCR that were utilized to recognize each detected\nline, and 10,000 items annotated for detailed information extraction. These\nannotations capture essential details such as merchant names, item\ndescriptions, total prices, receipt numbers, and dates. They are structured to\nsupport three primary computational tasks: object detection, OCR, and\ninformation extraction. We establish the baseline performance for a range of\nmodels on CORU to evaluate the effectiveness of traditional methods, like\nTesseract OCR, and more advanced neural network-based approaches. These\nbaselines are crucial for processing the complex and noisy document layouts\ntypical of real-world receipts and for advancing the state of automated\nmultilingual document processing. Our datasets are publicly accessible\n(https://github.com/Update-For-Integrated-Business-AI/CORU).\n', ""  This project undertakes the training and analysis of optical character\nrecognition OCR methods applied to 10th century ancient Tamil inscriptions\ndiscovered on the walls of the Brihadeeswarar Temple.The chosen OCR methods\ninclude Tesseract,a widely used OCR engine,using modern ICR techniques to pre\nprocess the raw data and a box editing software to finetune our model.The\nanalysis with Tesseract aims to evaluate their effectiveness in accurately\ndeciphering the nuances of the ancient Tamil characters.The performance of our\nmodel for the dataset are determined by their accuracy rate where the evaluated\ndataset divided into training set and testing set.By addressing the unique\nchallenges posed by the script's historical context,this study seeks to\ncontribute valuable insights to the broader field of OCR,facilitating improved\npreservation and interpretation of ancient inscriptions\n"", '  Teaching Computer Science (CS) by having students write programs by hand on\npaper has key pedagogical advantages: It allows focused learning and requires\ncareful thinking compared to the use of Integrated Development Environments\n(IDEs) with intelligent support tools or ""just trying things out"". The familiar\nenvironment of pens and paper also lessens the cognitive load of students with\nno prior experience with computers, for whom the mere basic usage of computers\ncan be intimidating. Finally, this teaching approach opens learning\nopportunities to students with limited access to computers.\n  However, a key obstacle is the current lack of teaching methods and support\nsoftware for working with and running handwritten programs. Optical character\nrecognition (OCR) of handwritten code is challenging: Minor OCR errors, perhaps\ndue to varied handwriting styles, easily make code not run, and recognizing\nindentation is crucial for languages like Python but is difficult to do due to\ninconsistent horizontal spacing in handwriting. Our approach integrates two\ninnovative methods. The first combines OCR with an indentation recognition\nmodule and a language model designed for post-OCR error correction without\nintroducing hallucinations. This method, to our knowledge, surpasses all\nexisting systems in handwritten code recognition. It reduces error from 30\\% in\nthe state of the art to 5\\% with minimal hallucination of logical fixes to\nstudent programs. The second method leverages a multimodal language model to\nrecognize handwritten programs in an end-to-end fashion. We hope this\ncontribution can stimulate further pedagogical research and contribute to the\ngoal of making CS education universally accessible. We release a dataset of\nhandwritten programs and code to support future research at\nhttps://github.com/mdoumbouya/codeocr\n']",Optical Character Recognition for Handwritten Text
209,208,43,208_imputations_imputation_missingness_imputing,"['imputations', 'imputation', 'missingness', 'imputing', 'imputed', 'completion', 'missforestpredict', 'datasets', 'imputes', 'impute']","['imputation', 'missing', 'values', 'missingness', 'impute', 'mice', 'incomplete', 'data', 'categorical', 'tabular']","['imputations', 'completion', 'missforest', 'imputer', 'classifier', 'values', 'accurate', 'database', 'selection', 'importance']","['  Missing data is a common problem in practical settings. Various imputation\nmethods have been developed to deal with missing data. However, even though the\nlabel is usually available in the training data, the common practice of\nimputation usually only relies on the input and ignores the label. In this\nwork, we illustrate how stacking the label into the input can significantly\nimprove the imputation of the input. In addition, we propose a classification\nstrategy that initializes the predicted test label with missing values and\nstacks the label with the input for imputation. This allows imputing the label\nand the input at the same time. Also, the technique is capable of handling data\ntraining with missing labels without any prior imputation and is applicable to\ncontinuous, categorical, or mixed-type data. Experiments show promising results\nin terms of accuracy.\n', '  Many datasets suffer from missing values due to various reasons,which not\nonly increases the processing difficulty of related tasks but also reduces the\naccuracy of classification. To address this problem, the mainstream approach is\nto use missing value imputation to complete the dataset. Existing imputation\nmethods estimate the missing parts based on the observed values in the original\nfeature space, and they treat all features as equally important during data\ncompletion, while in fact different features have different importance.\nTherefore, we have designed an imputation method that considers feature\nimportance. This algorithm iteratively performs matrix completion and feature\nimportance learning, and specifically, matrix completion is based on a filling\nloss that incorporates feature importance. Our experimental analysis involves\nthree types of datasets: synthetic datasets with different noisy features and\nmissing values, real-world datasets with artificially generated missing values,\nand real-world datasets originally containing missing values. The results on\nthese datasets consistently show that the proposed method outperforms the\nexisting five imputation algorithms.To the best of our knowledge, this is the\nfirst work that considers feature importance in the imputation model.\n', '  Missing values or data is one popular characteristic of real-world datasets,\nespecially healthcare data. This could be frustrating when using machine\nlearning algorithms on such datasets, simply because most machine learning\nmodels perform poorly in the presence of missing values. The aim of this study\nis to compare the performance of seven imputation techniques, namely Mean\nimputation, Median Imputation, Last Observation carried Forward (LOCF)\nimputation, K-Nearest Neighbor (KNN) imputation, Interpolation imputation,\nMissforest imputation, and Multiple imputation by Chained Equations (MICE), on\nthree healthcare datasets. Some percentage of missing values - 10\\%, 15\\%, 20\\%\nand 25\\% - were introduced into the dataset, and the imputation techniques were\nemployed to impute these missing values. The comparison of their performance\nwas evaluated by using root mean squared error (RMSE) and mean absolute error\n(MAE). The results show that Missforest imputation performs the best followed\nby MICE imputation. Additionally, we try to determine whether it is better to\nperform feature selection before imputation or vice versa by using the\nfollowing metrics - the recall, precision, f1-score and accuracy. Due to the\nfact that there are few literature on this and some debate on the subject among\nresearchers, we hope that the results from this experiment will encourage data\nscientists and researchers to perform imputation first before feature selection\nwhen dealing with data containing missing values.\n']",Missing Data Imputation Methods
210,209,42,209_multimodal_recommender_modality_embeddings,"['multimodal', 'recommender', 'modality', 'embeddings', 'embedding', 'modal', 'personalized', 'recommendation', 'recommendations', 'features']","['recommendation', 'multimodal', 'item', 'modal', 'modality', 'multimedia', 'modalities', 'recommender', 'user', 'items']","['multimodal', 'recommender', 'modality', 'embeddings', 'textbf', 'mp4sr', 'item', 'interests', 'musechat', 'music']","['  Multimodal recommendation aims to model user and item representations\ncomprehensively with the involvement of multimedia content for effective\nrecommendations. Existing research has shown that it is beneficial for\nrecommendation performance to combine (user- and item-) ID embeddings with\nmultimodal salient features, indicating the value of IDs. However, there is a\nlack of a thorough analysis of the ID embeddings in terms of feature semantics\nin the literature. In this paper, we revisit the value of ID embeddings for\nmultimodal recommendation and conduct a thorough study regarding its semantics,\nwhich we recognize as subtle features of \\emph{content} and \\emph{structure}.\nBased on our findings, we propose a novel recommendation model by incorporating\nID embeddings to enhance the salient features of both content and structure.\nSpecifically, we put forward a hierarchical attention mechanism to incorporate\nID embeddings in modality fusing, coupled with contrastive learning, to enhance\ncontent representations. Meanwhile, we propose a lightweight graph convolution\nnetwork for each modality to amalgamate neighborhood and ID embeddings for\nimproving structural representations. Finally, the content and structure\nrepresentations are combined to form the ultimate item embedding for\nrecommendation. Extensive experiments on three real-world datasets (Baby,\nSports, and Clothing) demonstrate the superiority of our method over\nstate-of-the-art multimodal recommendation methods and the effectiveness of\nfine-grained ID embeddings. Our code is available at\nhttps://anonymous.4open.science/r/IDSF-code/.\n', '  With the increasing multimedia information, multimodal recommendation has\nreceived extensive attention. It utilizes multimodal information to alleviate\nthe data sparsity problem in recommendation systems, thus improving\nrecommendation accuracy. However, the reliance on labeled data severely limits\nthe performance of multimodal recommendation models. Recently, self-supervised\nlearning has been used in multimodal recommendations to mitigate the label\nsparsity problem. Nevertheless, the state-of-the-art methods cannot avoid the\nmodality noise when aligning multimodal information due to the large\ndifferences in the distributions of different modalities. To this end, we\npropose a Multi-level sElf-supervised learNing for mulTimOdal Recommendation\n(MENTOR) method to address the label sparsity problem and the modality\nalignment problem. Specifically, MENTOR first enhances the specific features of\neach modality using the graph convolutional network (GCN) and fuses the visual\nand textual modalities. It then enhances the item representation via the item\nsemantic graph for all modalities, including the fused modality. Then, it\nintroduces two multilevel self-supervised tasks: the multilevel cross-modal\nalignment task and the general feature enhancement task. The multilevel\ncross-modal alignment task aligns each modality under the guidance of the ID\nembedding from multiple levels while maintaining the historical interaction\ninformation. The general feature enhancement task enhances the general feature\nfrom both the graph and feature perspectives to improve the robustness of our\nmodel. Extensive experiments on three publicly available datasets demonstrate\nthe effectiveness of our method. Our code is publicly available at\nhttps://github.com/Jinfeng-Xu/MENTOR.\n', '  Multi-modal recommendation greatly enhances the performance of recommender\nsystems by modeling the auxiliary information from multi-modality contents.\nMost existing multi-modal recommendation models primarily exploit multimedia\ninformation propagation processes to enrich item representations and directly\nutilize modal-specific embedding vectors independently obtained from upstream\npre-trained models. However, this might be inappropriate since the abundant\ntask-specific semantics remain unexplored, and the cross-modality semantic gap\nhinders the recommendation performance.\n  Inspired by the recent progress of the cross-modal alignment model CLIP, in\nthis paper, we propose a novel \\textbf{CLIP} \\textbf{E}nhanced\n\\textbf{R}ecommender (\\textbf{CLIPER}) framework to bridge the semantic gap\nbetween modalities and extract fine-grained multi-view semantic information.\nSpecifically, we introduce a multi-view modality-alignment approach for\nrepresentation extraction and measure the semantic similarity between\nmodalities. Furthermore, we integrate the multi-view multimedia representations\ninto downstream recommendation models. Extensive experiments conducted on three\npublic datasets demonstrate the consistent superiority of our model over\nstate-of-the-art multi-modal recommendation models.\n']",Multimodal Recommendation Systems
211,210,41,210_multitask_merging_merge_combine,"['multitask', 'merging', 'merge', 'combine', 'taskonomy', 'tasking', 'models', 'merged', 'tasks', 'unified']","['merging', 'task', 'arithmetic', 'multitask', 'parameter', 'interference', 'tasks', 'multi', 'weights', 'fine']","['multitask', 'merging', 'models', 'fusion', 'adamerging', 'adatask', 'weights', 'additional', 'scalearn', 'learngenes']","['  Model merging has emerged as an effective approach to combine multiple\nsingle-task models, fine-tuned from the same pre-trained model, into a\nmultitask model. This process typically involves computing a weighted average\nof the model parameters without any additional training. Existing model-merging\nmethods focus on enhancing average task accuracy. However, interference and\nconflicts between the objectives of different tasks can lead to trade-offs\nduring model merging. In real-world applications, a set of solutions with\nvarious trade-offs can be more informative, helping practitioners make\ndecisions based on diverse preferences. In this paper, we introduce a novel\nlow-compute algorithm, Model Merging with Amortized Pareto Front (MAP). MAP\nidentifies a Pareto set of scaling coefficients for merging multiple models to\nreflect the trade-offs. The core component of MAP is approximating the\nevaluation metrics of the various tasks using a quadratic approximation\nsurrogate model derived from a pre-selected set of scaling coefficients,\nenabling amortized inference. Experimental results on vision and natural\nlanguage processing tasks show that MAP can accurately identify the Pareto\nfront. To further reduce the required computation of MAP, we propose (1) a\nBayesian adaptive sampling algorithm and (2) a nested merging scheme with\nmultiple stages.\n', ""  In this paper, we introduce a novel approach for large language model merging\nvia black-box multi-objective optimization algorithms. The goal of model\nmerging is to combine multiple models, each excelling in different tasks, into\na single model that outperforms any of the individual source models. However,\nmodel merging faces two significant challenges: First, existing methods rely\nheavily on human intuition and customized strategies to tackle multiple tasks.\nSecond, it's difficult to search for the great model merging configuration in\nlimited evaluations. To address these challenges, we propose a multi-objective\noptimization based model merging method named MM-MO. The proposed method can\nautomatically search merging configurations for multiple tasks with\nmulti-objective optimization algorithms. Moreover, to obtain high-quality model\nmerging configurations within a limited number of evaluation iterations, we\nhave made several improvements to multi-objective Bayesian optimization\nspecifically for model merging scenarios. First, we introduced a weak-to-strong\nmethod to improve the acquisition strategy. Second, we employed Fisher\ninformation to select configurations, further increasing the chances of\ndiscovering superior model merging configurations. Third, we designed a\nsparsity metric as an additional optimization objective to enhance the model's\ngeneralization performance across different tasks. We conducted comprehensive\nexperiments with other mainstream model merging methods, demonstrating that our\nmethod consistently outperforms them. Moreover, performance improvements are\nobserved even on the tasks not explicitly targeted as optimization objectives,\nindicating that our method enhances the overall potential of the model. ...\n"", '  Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.\n']",Model Merging Techniques
212,211,41,211_biases_bias_genderbias_stereotypes,"['biases', 'bias', 'genderbias', 'stereotypes', 'discriminatory', 'biaspainter', 'debiasing', 'demographic', 'counterfactual', 'genders']","['gender', 'biases', 'bias', 'race', 'intersectional', 'attributes', 'image', 'images', 'stereotypes', 'occupation']","['bias', 'stereotypes', 'discriminatory', 'counterfactual', 'genders', 'representational', 'visual', 'versusdebias', 'attributes', 'impression']","[""  The recent advancement of large and powerful models with Text-to-Image (T2I)\ngeneration abilities -- such as OpenAI's DALLE-3 and Google's Gemini -- enables\nusers to generate high-quality images from textual prompts. However, it has\nbecome increasingly evident that even simple prompts could cause T2I models to\nexhibit conspicuous social bias in generated images. Such bias might lead to\nboth allocational and representational harms in society, further marginalizing\nminority groups. Noting this problem, a large body of recent works has been\ndedicated to investigating different dimensions of bias in T2I systems.\nHowever, an extensive review of these studies is lacking, hindering a\nsystematic understanding of current progress and research gaps. We present the\nfirst extensive survey on bias in T2I generative models. In this survey, we\nreview prior studies on dimensions of bias: Gender, Skintone, and Geo-Culture.\nSpecifically, we discuss how these works define, evaluate, and mitigate\ndifferent aspects of bias. We found that: (1) while gender and skintone biases\nare widely studied, geo-cultural bias remains under-explored; (2) most works on\ngender and skintone bias investigated occupational association, while other\naspects are less frequently studied; (3) almost all gender bias works overlook\nnon-binary identities in their studies; (4) evaluation datasets and metrics are\nscattered, with no unified framework for measuring biases; and (5) current\nmitigation methods fail to resolve biases comprehensively. Based on current\nlimitations, we point out future research directions that contribute to\nhuman-centric definitions, evaluations, and mitigation of biases. We hope to\nhighlight the importance of studying biases in T2I systems, as well as\nencourage future efforts to holistically understand and tackle biases, building\nfair and trustworthy T2I technologies for everyone.\n"", '  While vision-language models (VLMs) have achieved remarkable performance\nimprovements recently, there is growing evidence that these models also posses\nharmful biases with respect to social attributes such as gender and race. Prior\nstudies have primarily focused on probing such bias attributes individually\nwhile ignoring biases associated with intersections between social attributes.\nThis could be due to the difficulty of collecting an exhaustive set of\nimage-text pairs for various combinations of social attributes. To address this\nchallenge, we employ text-to-image diffusion models to produce counterfactual\nexamples for probing intersectional social biases at scale. Our approach\nutilizes Stable Diffusion with cross attention control to produce sets of\ncounterfactual image-text pairs that are highly similar in their depiction of a\nsubject (e.g., a given occupation) while differing only in their depiction of\nintersectional social attributes (e.g., race & gender). Through our\nover-generate-then-filter methodology, we produce SocialCounterfactuals, a\nhigh-quality dataset containing 171k image-text pairs for probing\nintersectional biases related to gender, race, and physical characteristics. We\nconduct extensive experiments to demonstrate the usefulness of our generated\ndataset for probing and mitigating intersectional social biases in\nstate-of-the-art VLMs.\n', '  Large Vision-Language Models (LVLMs) have been widely adopted in various\napplications; however, they exhibit significant gender biases. Existing\nbenchmarks primarily evaluate gender bias at the demographic group level,\nneglecting individual fairness, which emphasizes equal treatment of similar\nindividuals. This research gap limits the detection of discriminatory\nbehaviors, as individual fairness offers a more granular examination of biases\nthat group fairness may overlook. For the first time, this paper introduces the\nGenderBias-\\emph{VL} benchmark to evaluate occupation-related gender bias in\nLVLMs using counterfactual visual questions under individual fairness criteria.\nTo construct this benchmark, we first utilize text-to-image diffusion models to\ngenerate occupation images and their gender counterfactuals. Subsequently, we\ngenerate corresponding textual occupation options by identifying stereotyped\noccupation pairs with high semantic similarity but opposite gender proportions\nin real-world statistics. This method enables the creation of large-scale\nvisual question counterfactuals to expose biases in LVLMs, applicable in both\nmultimodal and unimodal contexts through modifying gender attributes in\nspecific modalities. Overall, our GenderBias-\\emph{VL} benchmark comprises\n34,581 visual question counterfactual pairs, covering 177 occupations. Using\nour benchmark, we extensively evaluate 15 commonly used open-source LVLMs (\\eg,\nLLaVA) and state-of-the-art commercial APIs, including GPT-4o and Gemini-Pro.\nOur findings reveal widespread gender biases in existing LVLMs. Our benchmark\noffers: (1) a comprehensive dataset for occupation-related gender bias\nevaluation; (2) an up-to-date leaderboard on LVLM biases; and (3) a nuanced\nunderstanding of the biases presented by these models. \\footnote{The dataset\nand code are available at the \\href{https://genderbiasvl.github.io/}{website}.}\n']",Bias in AI Image Generation
213,212,41,212_unlearning_adversarial_erasing_redacting,"['unlearning', 'adversarial', 'erasing', 'redacting', 'generative', 'concepts', 'copyright', 'memorization', 'erasure', 'models']","['erasure', 'concepts', 'erasing', 'concept', 'prompts', 'image', 'images', 'content', 'safety', 'text']","['unlearning', 'adversarial', 'redacting', 'generative', 'concepts', 'copyright', 'modellock', 'diffusion', 'safegen', 'decof']","['  Large-scale diffusion models, known for their impressive image generation\ncapabilities, have raised concerns among researchers regarding social impacts,\nsuch as the imitation of copyrighted artistic styles. In response, existing\napproaches turn to machine unlearning techniques to eliminate unsafe concepts\nfrom pre-trained models. However, these methods compromise the generative\nperformance and neglect the coupling among multi-concept erasures, as well as\nthe concept restoration problem. To address these issues, we propose a\nSeparable Multi-concept Eraser (SepME), which mainly includes two parts: the\ngeneration of concept-irrelevant representations and the weight decoupling. The\nformer aims to avoid unlearning substantial information that is irrelevant to\nforgotten concepts. The latter separates optimizable model weights, making each\nweight increment correspond to a specific concept erasure without affecting\ngenerative performance on other concepts. Specifically, the weight increment\nfor erasing a specified concept is formulated as a linear combination of\nsolutions calculated based on other known undesirable concepts. Extensive\nexperiments indicate the efficacy of our approach in eliminating concepts,\npreserving model performance, and offering flexibility in the erasure or\nrecovery of various concepts.\n', '  While large-scale text-to-image diffusion models have demonstrated impressive\nimage-generation capabilities, there are significant concerns about their\npotential misuse for generating unsafe content, violating copyright, and\nperpetuating societal biases. Recently, the text-to-image generation community\nhas begun addressing these concerns by editing or unlearning undesired concepts\nfrom pre-trained models. However, these methods often involve data-intensive\nand inefficient fine-tuning or utilize various forms of token remapping,\nrendering them susceptible to adversarial jailbreaks. In this paper, we present\na simple and effective training-free approach, ConceptPrune, wherein we first\nidentify critical regions within pre-trained models responsible for generating\nundesirable concepts, thereby facilitating straightforward concept unlearning\nvia weight pruning. Experiments across a range of concepts including artistic\nstyles, nudity, object erasure, and gender debiasing demonstrate that target\nconcepts can be efficiently erased by pruning a tiny fraction, approximately\n0.12% of total weights, enabling multi-concept erasure and robustness against\nvarious white-box and black-box adversarial attacks.\n', '  Generating images from text has become easier because of the scaling of\ndiffusion models and advancements in the field of vision and language. These\nmodels are trained using vast amounts of data from the Internet. Hence, they\noften contain undesirable content such as copyrighted material. As it is\nchallenging to remove such data and retrain the models, methods for erasing\nspecific concepts from pre-trained models have been investigated. We propose a\nnovel concept-erasure method that updates the text encoder using few-shot\nunlearning in which a few real images are used. The discussion regarding the\ngenerated images after erasing a concept has been lacking. While there are\nmethods for specifying the transition destination for concepts, the validity of\nthe specified concepts is unclear. Our method implicitly achieves this by\ntransitioning to the latent concepts inherent in the model or the images. Our\nmethod can erase a concept within 10 s, making concept erasure more accessible\nthan ever before. Implicitly transitioning to related concepts leads to more\nnatural concept erasure. We applied the proposed method to various concepts and\nconfirmed that concept erasure can be achieved tens to hundreds of times faster\nthan with current methods. By varying the parameters to be updated, we obtained\nresults suggesting that, like previous research, knowledge is primarily\naccumulated in the feed-forward networks of the text encoder.\n']",Removing Undesirable Content from AI Models
214,213,41,213_multimodal_modality_modal_unlearning,"['multimodal', 'modality', 'modal', 'unlearning', 'modalities', 'supervised', 'learnt', 'learning', 'unimodal', 'discriminative']","['modalities', 'multimodal', 'modality', 'unimodal', 'modal', 'uni', 'fusion', 'missing', 'cross', 'information']","['multimodal', 'unlearning', 'modalities', 'discriminative', 'robust', 'multidelete', 'mlem', 'fusion', 'coherency', 'imbalance']","[""  To overcome the imbalanced multimodal learning problem, where models prefer\nthe training of specific modalities, existing methods propose to control the\ntraining of uni-modal encoders from different perspectives, taking the\ninter-modal performance discrepancy as the basis. However, the intrinsic\nlimitation of modality capacity is ignored. The scarcely informative modalities\ncan be recognized as ``worse-learnt'' ones, which could force the model to\nmemorize more noise, counterproductively affecting the multimodal model\nability. Moreover, the current modality modulation methods narrowly concentrate\non selected worse-learnt modalities, even suppressing the training of others.\nHence, it is essential to consider the intrinsic limitation of modality\ncapacity and take all modalities into account during balancing. To this end, we\npropose the Diagnosing \\& Re-learning method. The learning state of each\nmodality is firstly estimated based on the separability of its uni-modal\nrepresentation space, and then used to softly re-initialize the corresponding\nuni-modal encoder. In this way, the over-emphasizing of scarcely informative\nmodalities is avoided. In addition, encoders of worse-learnt modalities are\nenhanced, simultaneously avoiding the over-training of other modalities.\nAccordingly, multimodal learning is effectively balanced and enhanced.\nExperiments covering multiple types of modalities and multimodal frameworks\ndemonstrate the superior performance of our simple-yet-effective method for\nbalanced multimodal learning. The source code and dataset are available at\n\\url{https://github.com/GeWu-Lab/Diagnosing_Relearning_ECCV2024}.\n"", '  Multimodal learning seeks to utilize data from multiple sources to improve\nthe overall performance of downstream tasks. It is desirable for redundancies\nin the data to make multimodal systems robust to missing or corrupted\nobservations in some correlated modalities. However, we observe that the\nperformance of several existing multimodal networks significantly deteriorates\nif one or multiple modalities are absent at test time. To enable robustness to\nmissing modalities, we propose a simple and parameter-efficient adaptation\nprocedure for pretrained multimodal networks. In particular, we exploit\nmodulation of intermediate features to compensate for the missing modalities.\nWe demonstrate that such adaptation can partially bridge performance drop due\nto missing modalities and outperform independent, dedicated networks trained\nfor the available modality combinations in some cases. The proposed adaptation\nrequires extremely small number of parameters (e.g., fewer than 1% of the total\nparameters) and applicable to a wide range of modality combinations and tasks.\nWe conduct a series of experiments to highlight the missing modality robustness\nof our proposed method on five different multimodal tasks across seven\ndatasets. Our proposed method demonstrates versatility across various tasks and\ndatasets, and outperforms existing methods for robust multimodal learning with\nmissing modalities.\n', '  Multimodal learning typically relies on the assumption that all modalities\nare fully available during both the training and inference phases. However, in\nreal-world scenarios, consistently acquiring complete multimodal data presents\nsignificant challenges due to various factors. This often leads to the issue of\nmissing modalities, where data for certain modalities are absent, posing\nconsiderable obstacles not only for the availability of multimodal pretrained\nmodels but also for their fine-tuning and the preservation of robustness in\ndownstream tasks. To address these challenges, we propose a novel framework\nintegrating parameter-efficient fine-tuning of unimodal pretrained models with\na self-supervised joint-embedding learning method. This framework enables the\nmodel to predict the embedding of a missing modality in the representation\nspace during inference. Our method effectively predicts the missing embedding\nthrough prompt tuning, leveraging information from available modalities. We\nevaluate our approach on several multimodal benchmark datasets and demonstrate\nits effectiveness and robustness across various scenarios of missing\nmodalities.\n']",Multimodal Learning and Modality Balancing
215,214,41,214_videos_recognition_pose_surgeons,"['videos', 'recognition', 'pose', 'surgeons', 'laparoscopic', 'neurosurgical', 'camera', 'vision', 'intraoperative', 'endoscope']","['surgical', 'surgery', 'instrument', 'instruments', 'endoscopic', 'surgeons', 'segmentation', 'scene', 'recognition', 'phase']","['videos', 'pose', 'laparoscopic', 'neurosurgical', 'endoscope', 'robotic', 'depth', 'segmentation', 'annotations', 'yolov5']","['  Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction,\nsurgical navigation and augmented reality visualization. Although the\nfoundation model exhibits outstanding performance in many vision tasks,\nincluding depth estimation (e.g., DINOv2), recent works observed its\nlimitations in medical and surgical domain-specific applications. This work\npresents a low-ranked adaptation (LoRA) of the foundation model for surgical\ndepth estimation. Methods: We design a foundation model-based depth estimation\nmethod, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for\ndepth estimation in endoscopic surgery. We build LoRA layers and integrate them\ninto DINO to adapt with surgery-specific domain knowledge instead of\nconventional fine-tuning. During training, we freeze the DINO image encoder,\nwhich shows excellent visual representation capacity, and only optimize the\nLoRA layers and depth decoder to integrate features from the surgical scene.\nResults: Our model is extensively validated on a MICCAI challenge dataset of\nSCARED, which is collected from da Vinci Xi endoscope surgery. We empirically\nshow that Surgical-DINO significantly outperforms all the state-of-the-art\nmodels in endoscopic depth estimation tasks. The analysis with ablation studies\nhas shown evidence of the remarkable effect of our LoRA layers and adaptation.\nConclusion: Surgical-DINO shed some light on the successful adaptation of the\nfoundation models into the surgical domain for depth estimation. There is clear\nevidence in the results that zero-shot prediction on pre-trained weights in\ncomputer vision datasets or naive fine-tuning is not sufficient to use the\nfoundation model in the surgical domain directly. Code is available at\nhttps://github.com/BeileiCui/SurgicalDINO.\n', '  Phase recognition in surgical videos is crucial for enhancing computer-aided\nsurgical systems as it enables automated understanding of sequential procedural\nstages. Existing methods often rely on fixed temporal windows for video\nanalysis to identify dynamic surgical phases. Thus, they struggle to\nsimultaneously capture short-, mid-, and long-term information necessary to\nfully understand complex surgical procedures. To address these issues, we\npropose Multi-Scale Transformers for Surgical Phase Recognition (MuST), a novel\nTransformer-based approach that combines a Multi-Term Frame encoder with a\nTemporal Consistency Module to capture information across multiple temporal\nscales of a surgical video. Our Multi-Term Frame Encoder computes\ninterdependencies across a hierarchy of temporal scales by sampling sequences\nat increasing strides around the frame of interest. Furthermore, we employ a\nlong-term Transformer encoder over the frame embeddings to further enhance\nlong-term reasoning. MuST achieves higher performance than previous\nstate-of-the-art methods on three different public benchmarks.\n', ""  Automation in surgical robotics has the potential to improve patient safety\nand surgical efficiency, but it is difficult to achieve due to the need for\nrobust perception algorithms. In particular, 6D pose estimation of surgical\ninstruments is critical to enable the automatic execution of surgical maneuvers\nbased on visual feedback. In recent years, supervised deep learning algorithms\nhave shown increasingly better performance at 6D pose estimation tasks; yet,\ntheir success depends on the availability of large amounts of annotated data.\nIn household and industrial settings, synthetic data, generated with 3D\ncomputer graphics software, has been shown as an alternative to minimize\nannotation costs of 6D pose datasets. However, this strategy does not translate\nwell to surgical domains as commercial graphics software have limited tools to\ngenerate images depicting realistic instrument-tissue interactions. To address\nthese limitations, we propose an improved simulation environment for surgical\nrobotics that enables the automatic generation of large and diverse datasets\nfor 6D pose estimation of surgical instruments. Among the improvements, we\ndeveloped an automated data generation pipeline and an improved surgical scene.\nTo show the applicability of our system, we generated a dataset of 7.5k images\nwith pose annotations of a surgical needle that was used to evaluate a\nstate-of-the-art pose estimation network. The trained model obtained a mean\ntranslational error of 2.59mm on a challenging dataset that presented varying\nlevels of occlusion. These results highlight our pipeline's success in training\nand evaluating novel vision algorithms for surgical robotics applications.\n""]",Surgical Video Analysis and Instrument Pose Estimation
216,215,40,215_blockchain_blockchains_blockchained_mainchain,"['blockchain', 'blockchains', 'blockchained', 'mainchain', 'federated', 'ledger', 'decentralized', 'privacy', 'ipfs', 'security']","['blockchain', 'decentralized', 'security', 'federated', 'privacy', 'blockchains', 'clients', 'proof', 'smart', 'consensus']","['blockchain', 'federated', 'privacy', 'ipfs', 'trusted', 'verifiable', 'chainfl', 'attacks', 'pow', 'centralized']","['  Generative Artificial Intelligence (GAI) has recently emerged as a promising\nsolution to address critical challenges of blockchain technology, including\nscalability, security, privacy, and interoperability. In this paper, we first\nintroduce GAI techniques, outline their applications, and discuss existing\nsolutions for integrating GAI into blockchains. Then, we discuss emerging\nsolutions that demonstrate the effectiveness of GAI in addressing various\nchallenges of blockchain, such as detecting unknown blockchain attacks and\nsmart contract vulnerabilities, designing key secret sharing schemes, and\nenhancing privacy. Moreover, we present a case study to demonstrate that GAI,\nspecifically the generative diffusion model, can be employed to optimize\nblockchain network performance metrics. Experimental results clearly show that,\ncompared to a baseline traditional AI approach, the proposed generative\ndiffusion model approach can converge faster, achieve higher rewards, and\nsignificantly improve the throughput and latency of the blockchain network.\nAdditionally, we highlight future research directions for GAI in blockchain\napplications, including personalized GAI-enabled blockchains, GAI-blockchain\nsynergy, and privacy and security considerations within blockchain ecosystems.\n', ""  This article aims to study intrusion attacks and then develop a novel\ncyberattack detection framework to detect cyberattacks at the network layer\n(e.g., Brute Password and Flooding of Transactions) of blockchain networks.\nSpecifically, we first design and implement a blockchain network in our\nlaboratory. This blockchain network will serve two purposes, i.e., to generate\nthe real traffic data (including both normal data and attack data) for our\nlearning models and to implement real-time experiments to evaluate the\nperformance of our proposed intrusion detection framework. To the best of our\nknowledge, this is the first dataset that is synthesized in a laboratory for\ncyberattacks in a blockchain network. We then propose a novel collaborative\nlearning model that allows efficient deployment in the blockchain network to\ndetect attacks. The main idea of the proposed learning model is to enable\nblockchain nodes to actively collect data, learn the knowledge from data using\nthe Deep Belief Network, and then share the knowledge learned from its data\nwith other blockchain nodes in the network. In this way, we can not only\nleverage the knowledge from all the nodes in the network but also do not need\nto gather all raw data for training at a centralized node like conventional\ncentralized learning solutions. Such a framework can also avoid the risk of\nexposing local data's privacy as well as excessive network overhead/congestion.\nBoth intensive simulations and real-time experiments clearly show that our\nproposed intrusion detection framework can achieve an accuracy of up to 98.6%\nin detecting attacks.\n"", '  Federated Learning (FL) has recently arisen as a revolutionary approach to\ncollaborative training Machine Learning models. According to this novel\nframework, multiple participants train a global model collaboratively,\ncoordinating with a central aggregator without sharing their local data. As FL\ngains popularity in diverse domains, security, and privacy concerns arise due\nto the distributed nature of this solution. Therefore, integrating this\nstrategy with Blockchain technology has been consolidated as a preferred choice\nto ensure the privacy and security of participants.\n  This paper explores the research efforts carried out by the scientific\ncommunity to define privacy solutions in scenarios adopting Blockchain-Enabled\nFL. It comprehensively summarizes the background related to FL and Blockchain,\nevaluates existing architectures for their integration, and the primary attacks\nand possible countermeasures to guarantee privacy in this setting. Finally, it\nreviews the main application scenarios where Blockchain-Enabled FL approaches\nhave been proficiently applied. This survey can help academia and industry\npractitioners understand which theories and techniques exist to improve the\nperformance of FL through Blockchain to preserve privacy and which are the main\nchallenges and future directions in this novel and still under-explored\ncontext. We believe this work provides a novel contribution respect to the\nprevious surveys and is a valuable tool to explore the current landscape,\nunderstand perspectives, and pave the way for advancements or improvements in\nthis amalgamation of Blockchain and Federated Learning.\n']",Blockchain Technology and Security
217,216,40,216_epidemics_epidemic_outbreak_pandemics,"['epidemics', 'epidemic', 'outbreak', 'pandemics', 'pandemic', 'outbreaks', 'influenza', 'covid', 'infectious', 'pandemicllm']","['epidemic', 'pandemic', 'outbreaks', 'spreading', 'infection', 'spread', 'epidemiological', 'economic', 'epidemics', 'outbreak']","['epidemics', 'pandemics', 'covid', 'epidemiology', 'modeling', 'cdc', 'forecasting', 'hospitalizations', 'interventions', 'hiv']","['  Pandemics involve the high transmission of a disease that impacts global and\nlocal health and economic patterns. The impact of a pandemic can be minimized\nby enforcing certain restrictions on a community. However, while minimizing\ninfection and death rates, these restrictions can also lead to economic crises.\nEpidemiological models help propose pandemic control strategies based on\nnon-pharmaceutical interventions such as social distancing, curfews, and\nlockdowns, reducing the economic impact of these restrictions. However,\ndesigning manual control strategies while considering disease spread and\neconomic status is non-trivial. Optimal strategies can be designed through\nmulti-objective reinforcement learning (MORL) models, which demonstrate how\nrestrictions can be used to optimize the outcome of a pandemic. In this\nresearch, we utilized an epidemiological Susceptible, Exposed, Infected,\nRecovered, Deceased (SEIRD) model: a compartmental model for virtually\nsimulating a pandemic day by day. We combined the SEIRD model with a deep\ndouble recurrent Q-network to train a reinforcement learning agent to enforce\nthe optimal restriction on the SEIRD simulation based on a reward function. We\ntested two agents with unique reward functions and pandemic goals to obtain two\nstrategies. The first agent placed long lockdowns to reduce the initial spread\nof the disease, followed by cyclical and shorter lockdowns to mitigate the\nresurgence of the disease. The second agent provided similar infection rates\nbut an improved economy by implementing a 10-day lockdown and 20-day\nno-restriction cycle. This use of reinforcement learning and epidemiological\nmodeling allowed for both economic and infection mitigation in multiple\npandemic scenarios.\n', '  Since the onset of the COVID-19 pandemic, there has been a growing interest\nin studying epidemiological models. Traditional mechanistic models\nmathematically describe the transmission mechanisms of infectious diseases.\nHowever, they often suffer from limitations of oversimplified or fixed\nassumptions, which could cause sub-optimal predictive power and inefficiency in\ncapturing complex relation information. Consequently, Graph Neural Networks\n(GNNs) have emerged as a progressively popular tool in epidemic research. In\nthis paper, we endeavor to furnish a comprehensive review of GNNs in epidemic\ntasks and highlight potential future directions. To accomplish this objective,\nwe introduce hierarchical taxonomies for both epidemic tasks and methodologies,\noffering a trajectory of development within this domain. For epidemic tasks, we\nestablish a taxonomy akin to those typically employed within the epidemic\ndomain. For methodology, we categorize existing work into Neural Models and\nHybrid Models. Following this, we perform an exhaustive and systematic\nexamination of the methodologies, encompassing both the tasks and their\ntechnical details. Furthermore, we discuss the limitations of existing methods\nfrom diverse perspectives and systematically propose future research\ndirections. This survey aims to bridge literature gaps and promote the\nprogression of this promising field, with a list of relevant papers at\nhttps://github.com/Emory-Melody/awesome-epidemic-modelingpapers. We hope that\nit will facilitate synergies between the communities of GNNs and epidemiology,\nand contribute to their collective progress.\n', '  In this paper, we propose a mathematical framework that governs the evolution\nof epidemic dynamics, encompassing both intra-population dynamics and\ninter-population mobility within a metapopulation network. By linearizing this\ndynamical system, we can identify the spatial starting point(s), namely the\nsource(s) (A) and the initiation time (B) of any epidemic, which we refer to as\nthe ""Big Bang"" of the epidemic. Furthermore, we introduce a novel concept of\neffective distance to track disease spread within the network. Our analysis\nreveals that the contagion geometry can be represented as a line with a\nuniversal slope, independent of disease type (R0) or mobility network\nconfiguration. The mathematical derivations presented in this framework are\ncorroborated by empirical data, including observations from the COVID-19\npandemic in Iran and the US, as well as the H1N1 outbreak worldwide. Within\nthis framework, in order to detect the Big Bang of an epidemic we require two\ntypes of data: A) A snapshot of the active infected cases in each subpopulation\nduring the linear phase. B) A coarse-grained representation of inter-population\nmobility. Also even with access to only type A data, we can still demonstrate\nthe universal contagion geometric pattern. Additionally, we can estimate errors\nand assess the precision of the estimations. This comprehensive approach\nenhances our understanding of when and where epidemics began and how they\nspread, and equips us with valuable insights for developing effective public\nhealth policies and mitigating the impact of infectious diseases on populations\nworldwide.\n']",Epidemics and Pandemics
218,217,40,217_deformation_deforming_registration_pose,"['deformation', 'deforming', 'registration', 'pose', 'imaging', 'deformable', 'images', 'anatomy', 'shapes', '3d']","['registration', 'cardiac', 'anatomical', 'ultrasound', 'meshes', 'shape', 'fetal', 'plane', 'tracking', 'scanning']","['deformation', 'registration', 'pose', 'imaging', '3d', 'correspondence', 'masks', 'sonographers', 'diffeomorphic', 'organs']","['  Affine image registration is a cornerstone of medical image analysis. While\nclassical algorithms can achieve excellent accuracy, they solve a\ntime-consuming optimization for every image pair. Deep-learning (DL) methods\nlearn a function that maps an image pair to an output transform. Evaluating the\nfunction is fast, but capturing large transforms can be challenging, and\nnetworks tend to struggle if a test-image characteristic shifts from the\ntraining domain, such as resolution. Most affine methods are agnostic to the\nanatomy the user wishes to align, meaning the registration will be inaccurate\nif algorithms consider all structures in the image. We address these\nshortcomings with SynthMorph, a fast, symmetric, diffeomorphic, and easy-to-use\nDL tool for joint affine-deformable registration of any brain image without\npreprocessing. First, we leverage a strategy that trains networks with widely\nvarying images synthesized from label maps, yielding robust performance for\nimage types unseen at training. Second, we optimize the spatial overlap of\nselect anatomical labels. This enables networks to distinguish anatomy of\ninterest from irrelevant structures, removing the need for preprocessing that\nexcludes content that may impinge on anatomy-specific registration. Third, we\ncombine the affine model with a deformable hypernetwork that lets users choose\nthe optimal deformation-field regularity for their specific data, at\nregistration time, in a fraction of the time required by classical methods. We\nanalyze how competing architectures learn affine transforms and compare\nstate-of-the-art registration tools across an extremely diverse set of\nneuroimaging data, aiming to truly capture the behavior of methods in the real\nworld. SynthMorph demonstrates high accuracy and is available at\nhttps://w3id.org/synthmorph, as a single complete end-to-end solution for\nregistration of brain MRI.\n', ""  Medical image synthesis remains challenging due to misalignment noise during\ntraining. Existing methods have attempted to address this challenge by\nincorporating a registration-guided module. However, these methods tend to\noverlook the task-specific constraints on the synthetic and registration\nmodules, which may cause the synthetic module to still generate spatially\naligned images with misaligned target images during training, regardless of the\nregistration module's function. Therefore, this paper proposes\nregistration-guided consistency and incorporates disentanglement learning for\nmedical image synthesis. The proposed registration-guided consistency\narchitecture fosters task-specificity within the synthetic and registration\nmodules by applying identical deformation fields before and after synthesis,\nwhile enforcing output consistency through an alignment loss. Moreover, the\nsynthetic module is designed to possess the capability of disentangling\nanatomical structures and specific styles across various modalities. An anatomy\nconsistency loss is introduced to further compel the synthetic module to\npreserve geometrical integrity within latent spaces. Experiments conducted on\nboth an in-house abdominal CECT-CT dataset and a publicly available pelvic\nMR-CT dataset have demonstrated the superiority of the proposed method.\n"", '  Image registration (IR) is a process that deforms images to align them with\nrespect to a reference space, making it easier for medical practitioners to\nexamine various medical images in a standardized reference frame, such as\nhaving the same rotation and scale. This document introduces image registration\nusing a simple numeric example. It provides a definition of image registration\nalong with a space-oriented symbolic representation. This review covers various\naspects of image transformations, including affine, deformable, invertible, and\nbidirectional transformations, as well as medical image registration algorithms\nsuch as Voxelmorph, Demons, SyN, Iterative Closest Point, and SynthMorph. It\nalso explores atlas-based registration and multistage image registration\ntechniques, including coarse-fine and pyramid approaches. Furthermore, this\nsurvey paper discusses medical image registration taxonomies, datasets,\nevaluation measures, such as correlation-based metrics, segmentation-based\nmetrics, processing time, and model size. It also explores applications in\nimage-guided surgery, motion tracking, and tumor diagnosis. Finally, the\ndocument addresses future research directions, including the further\ndevelopment of transformers.\n']",Medical Image Registration
219,218,40,218_transcriptomics_transcriptomic_transcriptome_rna,"['transcriptomics', 'transcriptomic', 'transcriptome', 'rna', 'bioinformatics', 'cell', 'cells', 'gene', 'genes', 'cellular']","['cell', 'seq', 'gene', 'expression', 'transcriptomics', 'single', 'spatial', 'omics', 'biological', 'cellular']","['transcriptomics', 'cells', 'scrna', 'autoencoder', 'sequencing', 'cfgen', 'epigenomic', 'grns', 'variational', 'langcell']","['  Inferring gene regulatory networks (GRNs) from single-cell RNA sequencing\n(scRNA-seq) data is a complex challenge that requires capturing the intricate\nrelationships between genes and their regulatory interactions. In this study,\nwe tackle this challenge by leveraging the single-cell BERT-based pre-trained\ntransformer model (scBERT), trained on extensive unlabeled scRNA-seq data, to\naugment structured biological knowledge from existing GRNs. We introduce a\nnovel joint graph learning approach that combines the rich contextual\nrepresentations learned by pre-trained single-cell language models with the\nstructured knowledge encoded in GRNs using graph neural networks (GNNs). By\nintegrating these two modalities, our approach effectively reasons over boththe\ngene expression level constraints provided by the scRNA-seq data and the\nstructured biological knowledge inherent in GRNs. We evaluate our method on\nhuman cell benchmark datasets from the BEELINE study with cell type-specific\nground truth networks. The results demonstrate superior performance over\ncurrent state-of-the-art baselines, offering a deeper understanding of cellular\nregulatory mechanisms.\n', '  Single-cell RNA sequencing (scRNA-seq) data are important for studying the\nlaws of life at single-cell level. However, it is still challenging to obtain\nenough high-quality scRNA-seq data. To mitigate the limited availability of\ndata, generative models have been proposed to computationally generate\nsynthetic scRNA-seq data. Nevertheless, the data generated with current models\nare not very realistic yet, especially when we need to generate data with\ncontrolled conditions. In the meantime, the Diffusion models have shown their\npower in generating data at high fidelity, providing a new opportunity for\nscRNA-seq generation.\n  In this study, we developed scDiffusion, a generative model combining\ndiffusion model and foundation model to generate high-quality scRNA-seq data\nwith controlled conditions. We designed multiple classifiers to guide the\ndiffusion process simultaneously, enabling scDiffusion to generate data under\nmultiple condition combinations. We also proposed a new control strategy called\nGradient Interpolation. This strategy allows the model to generate continuous\ntrajectories of cell development from a given cell state.\n  Experiments showed that scDiffusion can generate single-cell gene expression\ndata closely resembling real scRNA-seq data. Also, scDiffusion can\nconditionally produce data on specific cell types including rare cell types.\nFurthermore, we could use the multiple-condition generation of scDiffusion to\ngenerate cell type that was out of the training data. Leveraging the Gradient\nInterpolation strategy, we generated a continuous developmental trajectory of\nmouse embryonic cells. These experiments demonstrate that scDiffusion is a\npowerful tool for augmenting the real scRNA-seq data and can provide insights\ninto cell fate research.\n', '  The transformers have achieved significant accomplishments in the natural\nlanguage processing as its outstanding parallel processing capabilities and\nhighly flexible attention mechanism. In addition, increasing studies based on\ntransformers have been proposed to model single-cell data. In this review, we\nattempt to systematically summarize the single-cell language models and\napplications based on transformers. First, we provide a detailed introduction\nabout the structure and principles of transformers. Then, we review the\nsingle-cell language models and large language models for single-cell data\nanalysis. Moreover, we explore the datasets and applications of single-cell\nlanguage models in downstream tasks such as batch correction, cell clustering,\ncell type annotation, gene regulatory network inference and perturbation\nresponse. Further, we discuss the challenges of single-cell language models and\nprovide promising research directions. We hope this review will serve as an\nup-to-date reference for researchers interested in the direction of single-cell\nlanguage models.\n']",Single-Cell RNA Sequencing and Transcriptomics Analysis
220,219,40,219_attributes_products_attribute_descriptions,"['attributes', 'products', 'attribute', 'descriptions', 'items', 'commerce', 'product', 'advertisements', 'assortment', 'amazon']","['commerce', 'product', 'attribute', 'products', 'customer', 'click', 'shopping', 'intentions', 'extraction', 'purchase']","['attributes', 'items', 'advertisements', 'retailers', 'categories', 'extraction', 'dataset', 'receipts', 'ranking', 'consumers']","['  Product offers on e-commerce websites often consist of a product title and a\ntextual product description. In order to enable features such as faceted\nproduct search or to generate product comparison tables, it is necessary to\nextract structured attribute-value pairs from the unstructured product titles\nand descriptions and to normalize the extracted values to a single, unified\nscale for each attribute. This paper explores the potential of using large\nlanguage models (LLMs), such as GPT-3.5 and GPT-4, to extract and normalize\nattribute values from product titles and descriptions. We experiment with\ndifferent zero-shot and few-shot prompt templates for instructing LLMs to\nextract and normalize attribute-value pairs. We introduce the Web Data Commons\n- Product Attribute Value Extraction (WDC-PAVE) benchmark dataset for our\nexperiments. WDC-PAVE consists of product offers from 59 different websites\nwhich provide schema.org annotations. The offers belong to five different\nproduct categories, each with a specific set of attributes. The dataset\nprovides manually verified attribute-value pairs in two forms: (i) directly\nextracted values and (ii) normalized attribute values. The normalization of the\nattribute values requires systems to perform the following types of operations:\nname expansion, generalization, unit of measurement conversion, and string\nwrangling. Our experiments demonstrate that GPT-4 outperforms the PLM-based\nextraction methods SU-OpenTag, AVEQA, and MAVEQA by 10%, achieving an F1-score\nof 91%. For the extraction and normalization of product attribute values, GPT-4\nachieves a similar performance to the extraction scenario, while being\nparticularly strong at string wrangling and name expansion.\n', '  E-commerce platforms rely on structured product descriptions, in the form of\nattribute/value pairs to enable features such as faceted product search and\nproduct comparison. However, vendors on these platforms often provide\nunstructured product descriptions consisting of a title and a textual\ndescription. To process such offers, e-commerce platforms must extract\nattribute/value pairs from the unstructured descriptions. State-of-the-art\nattribute/value extraction methods based on pre-trained language models (PLMs),\nsuch as BERT, face two drawbacks (i) the methods require significant amounts of\ntask-specific training data and (ii) the fine-tuned models have problems to\ngeneralize to attribute values that were not part of the training data. We\nexplore the potential of using large language models (LLMs) as a more training\ndata-efficient and more robust alternative to existing attribute/value\nextraction methods. We propose different prompt templates for instructing LLMs\nabout the target schema of the extraction, covering both zero-shot and few-shot\nscenarios. In the zero-shot scenario, textual and JSON-based approaches for\nrepresenting information about the target attributes are compared. In the\nscenario with training data, we investigate (i) the provision of example\nattribute values, (ii) the selection of in-context demonstrations, (iii)\nshuffled ensembling to prevent position bias, and (iv) fine-tuning the LLM. The\nprompt templates are evaluated in combination with hosted LLMs, such as GPT-3.5\nand GPT-4, and open-source LLMs based on Llama2 which can be run locally. The\nbest average F1-score of 86% was reached by GPT-4 using an ensemble of shuffled\nprompts that combine attribute names, attribute descriptions, example values,\nand demonstrations. Given the same amount of training data, this prompt/model\ncombination outperforms the best PLM baseline by an average of 6% F1.\n', '  Product attribute extraction is an growing field in e-commerce business, with\nseveral applications including product ranking, product recommendation, future\nassortment planning and improving online shopping customer experiences.\nUnderstanding the customer needs is critical part of online business,\nspecifically fashion products. Retailers uses assortment planning to determine\nthe mix of products to offer in each store and channel, stay responsive to\nmarket dynamics and to manage inventory and catalogs. The goal is to offer the\nright styles, in the right sizes and colors, through the right channels. When\nshoppers find products that meet their needs and desires, they are more likely\nto return for future purchases, fostering customer loyalty. Product attributes\nare a key factor in assortment planning. In this paper we present PAE, a\nproduct attribute extraction algorithm for future trend reports consisting text\nand images in PDF format. Most existing methods focus on attribute extraction\nfrom titles or product descriptions or utilize visual information from existing\nproduct images. Compared to the prior works, our work focuses on attribute\nextraction from PDF files where upcoming fashion trends are explained. This\nwork proposes a more comprehensive framework that fully utilizes the different\nmodalities for attribute extraction and help retailers to plan the assortment\nin advance. Our contributions are three-fold: (a) We develop PAE, an efficient\nframework to extract attributes from unstructured data (text and images); (b)\nWe provide catalog matching methodology based on BERT representations to\ndiscover the existing attributes using upcoming attribute values; (c) We\nconduct extensive experiments with several baselines and show that PAE is an\neffective, flexible and on par or superior (avg 92.5% F1-Score) framework to\nexisting state-of-the-art for attribute value extraction task.\n']",Product Attribute Extraction in E-commerce
221,220,40,220_backdoors_backdoor_backdoorbox_catchbackdoor,"['backdoors', 'backdoor', 'backdoorbox', 'catchbackdoor', 'backdoored', 'attacks', 'badnet', 'backdoorbench', 'security', 'adversary']","['backdoor', 'trigger', 'backdoors', 'triggers', 'attacks', 'clean', 'attack', 'defenses', 'defense', 'trojan']","['backdoors', 'badnet', 'security', 'attacker', 'neuron', 'activations', 'dnns', 'trojan', 'stealthy', 'unlearning']","['  Deep anomaly detection on sequential data has garnered significant attention\ndue to the wide application scenarios. However, deep learning-based models face\na critical security threat - their vulnerability to backdoor attacks. In this\npaper, we explore compromising deep sequential anomaly detection models by\nproposing a novel backdoor attack strategy. The attack approach comprises two\nprimary steps, trigger generation and backdoor injection. Trigger generation is\nto derive imperceptible triggers by crafting perturbed samples from the benign\nnormal data, of which the perturbed samples are still normal. The backdoor\ninjection is to properly inject the backdoor triggers to comprise the model\nonly for the samples with triggers. The experimental results demonstrate the\neffectiveness of our proposed attack strategy by injecting backdoors on two\nwell-established one-class anomaly detection models.\n', '  Deep neural networks (DNNs) are susceptible to backdoor attacks, where\nmalicious functionality is embedded to allow attackers to trigger incorrect\nclassifications. Old-school backdoor attacks use strong trigger features that\ncan easily be learned by victim models. Despite robustness against input\nvariation, the robustness however increases the likelihood of unintentional\ntrigger activations. This leaves traces to existing defenses, which find\napproximate replacements for the original triggers that can activate the\nbackdoor without being identical to the original trigger via, e.g., reverse\nengineering and sample overlay.\n  In this paper, we propose and investigate a new characteristic of backdoor\nattacks, namely, backdoor exclusivity, which measures the ability of backdoor\ntriggers to remain effective in the presence of input variation. Building upon\nthe concept of backdoor exclusivity, we propose Backdoor Exclusivity LifTing\n(BELT), a novel technique which suppresses the association between the backdoor\nand fuzzy triggers to enhance backdoor exclusivity for defense evasion.\nExtensive evaluation on three popular backdoor benchmarks validate, our\napproach substantially enhances the stealthiness of four old-school backdoor\nattacks, which, after backdoor exclusivity lifting, is able to evade seven\nstate-of-the-art backdoor countermeasures, at almost no cost of the attack\nsuccess rate and normal utility. For example, one of the earliest backdoor\nattacks BadNet, enhanced by BELT, evades most of the state-of-the-art defenses\nincluding ABS and MOTH which would otherwise recognize the backdoored model.\n', '  Backdoor attacks have emerged as a primary threat to (pre-)training and\ndeployment of deep neural networks (DNNs). While backdoor attacks have been\nextensively studied in a body of works, most of them were focused on\nsingle-trigger attacks that poison a dataset using a single type of trigger.\nArguably, real-world backdoor attacks can be much more complex, e.g., the\nexistence of multiple adversaries for the same dataset if it is of high value.\nIn this work, we investigate the practical threat of backdoor attacks under the\nsetting of \\textbf{multi-trigger attacks} where multiple adversaries leverage\ndifferent types of triggers to poison the same dataset. By proposing and\ninvestigating three types of multi-trigger attacks, including parallel,\nsequential, and hybrid attacks, we provide a set of important understandings of\nthe coexisting, overwriting, and cross-activating effects between different\ntriggers on the same dataset. Moreover, we show that single-trigger attacks\ntend to cause overly optimistic views of the security of current defense\ntechniques, as all examined defense methods struggle to defend against\nmulti-trigger attacks. Finally, we create a multi-trigger backdoor poisoning\ndataset to help future evaluation of backdoor attacks and defenses. Although\nour work is purely empirical, we hope it can help steer backdoor research\ntoward more realistic settings.\n']",Backdoor Attacks on Deep Neural Networks
222,221,40,221_homomorphic_homomorphically_privacy_encryption,"['homomorphic', 'homomorphically', 'privacy', 'encryption', 'cryptographic', 'cryptonets', 'encrypted', 'secure', 'private', 'ciphertexts']","['homomorphic', 'encryption', 'encrypted', 'privacy', 'secure', 'preserving', 'computation', 'party', 'garbled', 'inference']","['homomorphic', 'encrypted', 'private', 'hcnn', 'cloud', 'preserving', 'computing', 'ssnet', 'spencnn', 'dnns']","[""  Transfer learning is a de facto standard method for efficiently training\nmachine learning models for data-scarce problems by adding and fine-tuning new\nclassification layers to a model pre-trained on large datasets. Although\nnumerous previous studies proposed to use homomorphic encryption to resolve the\ndata privacy issue in transfer learning in the machine learning as a service\nsetting, most of them only focused on encrypted inference. In this study, we\npresent HETAL, an efficient Homomorphic Encryption based Transfer Learning\nalgorithm, that protects the client's privacy in training tasks by encrypting\nthe client data using the CKKS homomorphic encryption scheme. HETAL is the\nfirst practical scheme that strictly provides encrypted training, adopting\nvalidation-based early stopping and achieving the accuracy of nonencrypted\ntraining. We propose an efficient encrypted matrix multiplication algorithm,\nwhich is 1.8 to 323 times faster than prior methods, and a highly precise\nsoftmax approximation algorithm with increased coverage. The experimental\nresults for five well-known benchmark datasets show total training times of\n567-3442 seconds, which is less than an hour.\n"", '  Advancements in machine learning (ML) have significantly revolutionized\nmedical image analysis, prompting hospitals to rely on external ML services.\nHowever, the exchange of sensitive patient data, such as chest X-rays, poses\ninherent privacy risks when shared with third parties. Addressing this concern,\nwe propose MedBlindTuner, a privacy-preserving framework leveraging fully\nhomomorphic encryption (FHE) and a data-efficient image transformer (DEiT).\nMedBlindTuner enables the training of ML models exclusively on FHE-encrypted\nmedical images. Our experimental evaluation demonstrates that MedBlindTuner\nachieves comparable accuracy to models trained on non-encrypted images,\noffering a secure solution for outsourcing ML computations while preserving\npatient data privacy. To the best of our knowledge, this is the first work that\nuses data-efficient image transformers and fully homomorphic encryption in this\ndomain.\n', ""  In this paper, we introduce a privacy-preserving stable diffusion framework\nleveraging homomorphic encryption, called HE-Diffusion, which primarily focuses\non protecting the denoising phase of the diffusion process. HE-Diffusion is a\ntailored encryption framework specifically designed to align with the unique\narchitecture of stable diffusion, ensuring both privacy and functionality. To\naddress the inherent computational challenges, we propose a novel\nmin-distortion method that enables efficient partial image encryption,\nsignificantly reducing the overhead without compromising the model's output\nquality. Furthermore, we adopt a sparse tensor representation to expedite\ncomputational operations, enhancing the overall efficiency of the\nprivacy-preserving diffusion process. We successfully implement HE-based\nprivacy-preserving stable diffusion inference. The experimental results show\nthat HE-Diffusion achieves 500 times speedup compared with the baseline method,\nand reduces time cost of the homomorphically encrypted inference to the minute\nlevel. Both the performance and accuracy of the HE-Diffusion are on par with\nthe plaintext counterpart. Our approach marks a significant step towards\nintegrating advanced cryptographic techniques with state-of-the-art generative\nmodels, paving the way for privacy-preserving and efficient image generation in\ncritical applications.\n""]",Homomorphic Encryption for Secure Machine Learning
223,222,40,222_sarcasm_sarcastic_emoticons_sentiment,"['sarcasm', 'sarcastic', 'emoticons', 'sentiment', 'humor', 'irony', 'multimodal', 'emojis', 'linguistic', 'text']","['sarcasm', 'sentiment', 'multimodal', 'modalities', 'modality', 'modal', 'irony', 'sentiments', 'hyperbole', 'analysis']","['sarcastic', 'emoticons', 'sentiment', 'text', 'modality', 'dialogues', 'annotations', 'twitter', 'unimodal', 'arabert']","['  Sarcasm is a way of verbal irony where someone says the opposite of what they\nmean, often to ridicule a person, situation, or idea. It is often difficult to\ndetect sarcasm in the dialogue since detecting sarcasm should reflect the\ncontext (i.e., dialogue history). In this paper, we introduce a new dataset for\nthe Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware\nSarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and\nthe labels for this task on the last response. To build the dataset, we propose\nan efficient sarcasm detection dataset generation pipeline: 1) generating new\nsarcastic dialogues from source dialogues with large language models, 2)\nautomatic and manual filtering of abnormal and toxic dialogues, and 3) human\nannotation for the sarcasm detection task. We also provide a simple but\neffective baseline for the Korean sarcasm detection task trained on our\ndataset. Experimental results on the dataset show that our baseline system\noutperforms strong baselines like large language models, such as GPT-3.5, in\nthe Korean sarcasm detection task. We show that the sarcasm detection task\nrelies deeply on the existence of sufficient context. We will release the\ndataset at https://github.com/Yu-billie/KoCoSa_sarcasm_detection.\n', '  Sarcasm is a type of irony, characterized by an inherent mismatch between the\nliteral interpretation and the intended connotation. Though sarcasm detection\nin text has been extensively studied, there are situations in which textual\ninput alone might be insufficient to perceive sarcasm. The inclusion of\nadditional contextual cues, such as images, is essential to recognize sarcasm\nin social media data effectively. This study presents a novel framework for\nmultimodal sarcasm detection that can process input triplets. Two components of\nthese triplets comprise the input text and its associated image, as provided in\nthe datasets. Additionally, a supplementary modality is introduced in the form\nof descriptive image captions. The motivation behind incorporating this visual\nsemantic representation is to more accurately capture the discrepancies between\nthe textual and visual content, which are fundamental to the sarcasm detection\ntask. The primary contributions of this study are: (1) a robust textual feature\nextraction branch that utilizes a cross-lingual language model; (2) a visual\nfeature extraction branch that incorporates a self-regulated residual ConvNet\nintegrated with a lightweight spatially aware attention module; (3) an\nadditional modality in the form of image captions generated using an\nencoder-decoder architecture capable of reading text embedded in images; (4)\ndistinct attention modules to effectively identify the incongruities between\nthe text and two levels of image representations; (5) multi-level cross-domain\nsemantic incongruity representation achieved through feature fusion. Compared\nwith cutting-edge baselines, the proposed model achieves the best accuracy of\n92.89% and 64.48%, respectively, on the Twitter multimodal sarcasm and\nMultiBully datasets.\n', '  Social media abounds with multimodal sarcasm, and identifying sarcasm targets\nis particularly challenging due to the implicit incongruity not directly\nevident in the text and image modalities. Current methods for Multimodal\nSarcasm Target Identification (MSTI) predominantly focus on superficial\nindicators in an end-to-end manner, overlooking the nuanced understanding of\nmultimodal sarcasm conveyed through both the text and image. This paper\nproposes a versatile MSTI framework with a coarse-to-fine paradigm, by\naugmenting sarcasm explainability with reasoning and pre-training knowledge.\nInspired by the powerful capacity of Large Multimodal Models (LMMs) on\nmultimodal reasoning, we first engage LMMs to generate competing rationales for\ncoarser-grained pre-training of a small language model on multimodal sarcasm\ndetection. We then propose fine-tuning the model for finer-grained sarcasm\ntarget identification. Our framework is thus empowered to adeptly unveil the\nintricate targets within multimodal sarcasm and mitigate the negative impact\nposed by potential noise inherently in LMMs. Experimental results demonstrate\nthat our model far outperforms state-of-the-art MSTI methods, and markedly\nexhibits explainability in deciphering sarcasm as well.\n']",Sarcasm Detection in Multimodal Text
224,223,39,223_processes_process_bpm_mining,"['processes', 'process', 'bpm', 'mining', 'analytics', 'discovering', 'activity', 'discovery', 'activities', 'manufacturing']","['mining', 'process', 'business', 'processes', 'logs', 'patterns', 'event', 'traces', 'discovery', 'pattern']","['processes', 'bpm', 'analytics', 'discovering', 'businesses', 'logs', 'mined', 'behavioral', 'petri', 'incremental']","['  Process mining, as a high-level field in data mining, plays a crucial role in\nenhancing operational efficiency and decision-making across organizations. In\nthis survey paper, we delve into the growing significance and ongoing trends in\nthe field of process mining, advocating a specific viewpoint on its contents,\napplication, and development in modern businesses and process management,\nparticularly in cross-organizational settings. We first summarize the framework\nof process mining, common industrial applications, and the latest advances\ncombined with artificial intelligence, such as workflow optimization,\ncompliance checking, and performance analysis. Then, we propose a holistic\nframework for intelligent process analysis and outline initial methodologies in\ncross-organizational settings, highlighting both challenges and opportunities.\nThis particular perspective aims to revolutionize process mining by leveraging\nartificial intelligence to offer sophisticated solutions for complex,\nmulti-organizational data analysis. By integrating advanced machine learning\ntechniques, we can enhance predictive capabilities, streamline processes, and\nfacilitate real-time decision-making. Furthermore, we pinpoint avenues for\nfuture investigations within the research community, encouraging the\nexploration of innovative algorithms, data integration strategies, and\nprivacy-preserving methods to fully harness the potential of process mining in\ndiverse, interconnected business environments.\n', '  The process mining community has recently recognized the potential of large\nlanguage models (LLMs) for tackling various process mining tasks. Initial\nstudies report the capability of LLMs to support process analysis and even, to\nsome extent, that they are able to reason about how processes work. This latter\nproperty suggests that LLMs could also be used to tackle process mining tasks\nthat benefit from an understanding of process behavior. Examples of such tasks\ninclude (semantic) anomaly detection and next activity prediction, which both\ninvolve considerations of the meaning of activities and their inter-relations.\nIn this paper, we investigate the capabilities of LLMs to tackle such\nsemantics-aware process mining tasks. Furthermore, whereas most works on the\nintersection of LLMs and process mining only focus on testing these models out\nof the box, we provide a more principled investigation of the utility of LLMs\nfor process mining, including their ability to obtain process mining knowledge\npost-hoc by means of in-context learning and supervised fine-tuning.\nConcretely, we define three process mining tasks that benefit from an\nunderstanding of process semantics and provide extensive benchmarking datasets\nfor each of them. Our evaluation experiments reveal that (1) LLMs fail to solve\nchallenging process mining tasks out of the box and when provided only a\nhandful of in-context examples, (2) but they yield strong performance when\nfine-tuned for these tasks, consistently surpassing smaller, encoder-based\nlanguage models.\n', '  In the rapidly evolving field of business process management, there is a\ngrowing need for analytical tools that can transform complex data into\nactionable insights. This research introduces a novel approach by integrating\nLarge Language Models (LLMs), such as ChatGPT, into process mining tools,\nmaking process analytics more accessible to a wider audience. The study aims to\ninvestigate how ChatGPT enhances analytical capabilities, improves user\nexperience, increases accessibility, and optimizes the architectural frameworks\nof process mining tools. The key innovation of this research lies in developing\na tailored prompt engineering strategy for each process mining submodule,\nensuring that the AI-generated outputs are accurate and relevant to the\ncontext. The integration architecture follows an Extract, Transform, Load (ETL)\nprocess, which includes various process mining engine modules and utilizes\nzero-shot and optimized prompt engineering techniques. ChatGPT is connected via\nAPIs and receives structured outputs from the process mining modules, enabling\nconversational interactions. To validate the effectiveness of this approach,\nthe researchers used data from 17 companies that employ BehfaLab\'s Process\nMining Tool. The results showed significant improvements in user experience,\nwith an expert panel rating 72% of the results as ""Good"". This research\ncontributes to the advancement of business process analysis methodologies by\ncombining process mining with artificial intelligence. Future research\ndirections include further optimization of prompt engineering, exploration of\nintegration with other AI technologies, and assessment of scalability across\nvarious business environments. This study paves the way for continuous\ninnovation at the intersection of process mining and artificial intelligence,\npromising to revolutionize the way businesses analyze and optimize their\nprocesses.\n']",Process Mining and Analytics
225,224,39,224_ssl_ssl4ns_supervised_imagenet,"['ssl', 'ssl4ns', 'supervised', 'imagenet', 'learning', 'memorization', 'encoders', 'views', 'labeled', 'learn']","['supervised', 'self', 'augmentations', 'contrastive', 'unlabeled', 'pretraining', 'series', 'representations', 'views', 'learning']","['ssl4ns', 'imagenet', 'memorization', 'encoders', 'views', 'gessl', 'labels', 'self', 'vcreg', 'autoregressive']","[""  In recent years, the rise of generative self-supervised learning (SSL)\nparadigms has exhibited impressive performance across visual, language, and\nmulti-modal domains. While the varied designs of generative SSL objectives lead\nto distinct properties in downstream tasks, a theoretical understanding of\nthese differences remains largely unexplored. In this paper, we establish the\nfirst theoretical comparisons between two leading generative SSL paradigms:\nautoregressive SSL and masked SSL. Through establishing theoretical frameworks,\nwe elucidate the strengths and limitations of autoregressive and masked SSL\nwithin the primary evaluation tasks of classification and content generation.\nOur findings demonstrate that in classification tasks, the flexibility of\ntargeted tokens in masked SSL fosters more inter-sample connections compared to\nthe fixed position of target tokens in autoregressive SSL, which yields\nsuperior clustering performance. In content generation tasks, the misalignment\nbetween the flexible lengths of test samples and the fixed length of unmasked\ntexts in masked SSL (vs. flexible lengths of conditional texts in\nautoregressive SSL) hinders its generation performance. To leverage each\nother's strengths and mitigate weaknesses, we propose diversity-enhanced\nautoregressive and variable-length masked objectives, which substantially\nimprove the classification performance of autoregressive SSL and the generation\nperformance of masked SSL. Code is available at\nhttps://github.com/PKU-ML/LookAheadLookAround.\n"", '  Deep supervised learning algorithms typically require a large volume of\nlabeled data to achieve satisfactory performance. However, the process of\ncollecting and labeling such data can be expensive and time-consuming.\nSelf-supervised learning (SSL), a subset of unsupervised learning, aims to\nlearn discriminative features from unlabeled data without relying on\nhuman-annotated labels. SSL has garnered significant attention recently,\nleading to the development of numerous related algorithms. However, there is a\ndearth of comprehensive studies that elucidate the connections and evolution of\ndifferent SSL variants. This paper presents a review of diverse SSL methods,\nencompassing algorithmic aspects, application domains, three key trends, and\nopen research questions. Firstly, we provide a detailed introduction to the\nmotivations behind most SSL algorithms and compare their commonalities and\ndifferences. Secondly, we explore representative applications of SSL in domains\nsuch as image processing, computer vision, and natural language processing.\nLastly, we discuss the three primary trends observed in SSL research and\nhighlight the open questions that remain. A curated collection of valuable\nresources can be accessed at https://github.com/guijiejie/SSL.\n', '  Self-supervised learning (SSL) has recently achieved impressive performance\non various time series tasks. The most prominent advantage of SSL is that it\nreduces the dependence on labeled data. Based on the pre-training and\nfine-tuning strategy, even a small amount of labeled data can achieve high\nperformance. Compared with many published self-supervised surveys on computer\nvision and natural language processing, a comprehensive survey for time series\nSSL is still missing. To fill this gap, we review current state-of-the-art SSL\nmethods for time series data in this article. To this end, we first\ncomprehensively review existing surveys related to SSL and time series, and\nthen provide a new taxonomy of existing time series SSL methods by summarizing\nthem from three perspectives: generative-based, contrastive-based, and\nadversarial-based. These methods are further divided into ten subcategories\nwith detailed reviews and discussions about their key intuitions, main\nframeworks, advantages and disadvantages. To facilitate the experiments and\nvalidation of time series SSL methods, we also summarize datasets commonly used\nin time series forecasting, classification, anomaly detection, and clustering\ntasks. Finally, we present the future directions of SSL for time series\nanalysis.\n']",Self-Supervised Learning (SSL) Methods and Applications
226,225,39,225_audioset_captioning_captions_audiocaps,"['audioset', 'captioning', 'captions', 'audiocaps', 'caption', 'audio', 'multimodal', 'audios', 'speech', 'clips']","['audio', 'captions', 'speech', 'visual', 'captioning', 'caption', 'clips', 'video', 'acoustic', 'recognition']","['audioset', 'captions', 'audiocaps', 'multimodal', 'clips', 'vocal', 'retrieval', 'encoder', 'recap', 'data2vec']","['  Humans are adept at leveraging visual cues from lip movements for recognizing\nspeech in adverse listening conditions. Audio-Visual Speech Recognition (AVSR)\nmodels follow similar approach to achieve robust speech recognition in noisy\nconditions. In this work, we present a multilingual AVSR model incorporating\nseveral enhancements to improve performance and audio noise robustness.\nNotably, we adapt the recently proposed Fast Conformer model to process both\naudio and visual modalities using a novel hybrid CTC/RNN-T architecture. We\nincrease the amount of audio-visual training data for six distinct languages,\ngenerating automatic transcriptions of unlabelled multilingual datasets\n(VoxCeleb2 and AVSpeech). Our proposed model achieves new state-of-the-art\nperformance on the LRS3 dataset, reaching WER of 0.8%. On the recently\nintroduced MuAViC benchmark, our model yields an absolute average-WER reduction\nof 11.9% in comparison to the original baseline. Finally, we demonstrate the\nability of the proposed model to perform audio-only, visual-only, and\naudio-visual speech recognition at test time.\n', '  Automated audio captioning is a cross-modal translation task for describing\nthe content of audio clips with natural language sentences. This task has\nattracted increasing attention and substantial progress has been made in recent\nyears. Captions generated by existing models are generally faithful to the\ncontent of audio clips, however, these machine-generated captions are often\ndeterministic (e.g., generating a fixed caption for a given audio clip), simple\n(e.g., using common words and simple grammar), and generic (e.g., generating\nthe same caption for similar audio clips). When people are asked to describe\nthe content of an audio clip, different people tend to focus on different sound\nevents and describe an audio clip diversely from various aspects using distinct\nwords and grammar. We believe that an audio captioning system should have the\nability to generate diverse captions, either for a fixed audio clip, or across\nsimilar audio clips. To this end, we propose an adversarial training framework\nbased on a conditional generative adversarial network (C-GAN) to improve\ndiversity of audio captioning systems. A caption generator and two hybrid\ndiscriminators compete and are learned jointly, where the caption generator can\nbe any standard encoder-decoder captioning model used to generate captions, and\nthe hybrid discriminators assess the generated captions from different\ncriteria, such as their naturalness and semantics. We conduct experiments on\nthe Clotho dataset. The results show that our proposed model can generate\ncaptions with better diversity as compared to state-of-the-art methods.\n', '  We present RECAP (REtrieval-Augmented Audio CAPtioning), a novel and\neffective audio captioning system that generates captions conditioned on an\ninput audio and other captions similar to the audio retrieved from a datastore.\nAdditionally, our proposed method can transfer to any domain without the need\nfor any additional fine-tuning. To generate a caption for an audio sample, we\nleverage an audio-text model CLAP to retrieve captions similar to it from a\nreplaceable datastore, which are then used to construct a prompt. Next, we feed\nthis prompt to a GPT-2 decoder and introduce cross-attention layers between the\nCLAP encoder and GPT-2 to condition the audio for caption generation.\nExperiments on two benchmark datasets, Clotho and AudioCaps, show that RECAP\nachieves competitive performance in in-domain settings and significant\nimprovements in out-of-domain settings. Additionally, due to its capability to\nexploit a large text-captions-only datastore in a training-free fashion, RECAP\nshows unique capabilities of captioning novel audio events never seen during\ntraining and compositional audios with multiple events. To promote research in\nthis space, we also release 150,000+ new weakly labeled captions for AudioSet,\nAudioCaps, and Clotho.\n']",Multimodal Audio Captioning
227,226,39,226_deepfake_deepfacegen_deepfakeart_deepfakes,"['deepfake', 'deepfacegen', 'deepfakeart', 'deepfakes', 'detection', 'detecting', 'dataset', 'datasets', 'faceforensics', 'faces']","['deepfake', 'deepfakes', 'fake', 'detection', 'forgeries', 'images', 'forgery', 'facial', 'detectors', 'image']","['deepfacegen', 'dataset', 'faces', 'embedders', 'videos', 'detect', 'ai', 'cocofake', 'forgeries', 'wilddeepfake']","['  We study universal deepfake detection. Our goal is to detect synthetic images\nfrom a range of generative AI approaches, particularly from emerging ones which\nare unseen during training of the deepfake detector. Universal deepfake\ndetection requires outstanding generalization capability. Motivated by recently\nproposed masked image modeling which has demonstrated excellent generalization\nin self-supervised pre-training, we make the first attempt to explore masked\nimage modeling for universal deepfake detection. We study spatial and frequency\ndomain masking in training deepfake detectors. Based on empirical analysis, we\npropose a novel deepfake detector via frequency masking. Our focus on frequency\ndomain is different from the majority, which primarily target spatial domain\ndetection. Our comparative analyses reveal substantial performance gains over\nexisting methods. Code and models are publicly available.\n', '  Deepfake is a generative deep learning algorithm that creates or changes\nfacial features in a very realistic way making it hard to differentiate the\nreal from the fake features It can be used to make movies look better as well\nas to spread false information by imitating famous people In this paper many\ndifferent ways to make a Deepfake are explained analyzed and separated\ncategorically Using Deepfake datasets models are trained and tested for\nreliability through experiments Deepfakes are a type of facial manipulation\nthat allow people to change their entire faces identities attributes and\nexpressions The trends in the available Deepfake datasets are also discussed\nwith a focus on how they have changed Using Deep learning a general Deepfake\ndetection model is made Moreover the problems in making and detecting Deepfakes\nare also mentioned As a result of this survey it is expected that the\ndevelopment of new Deepfake based imaging tools will speed up in the future\nThis survey gives indepth review of methods for manipulating images of face and\nvarious techniques to spot altered face images Four types of facial\nmanipulation are specifically discussed which are attribute manipulation\nexpression swap entire face synthesis and identity swap Across every\nmanipulation category we yield information on manipulation techniques\nsignificant benchmarks for technical evaluation of counterfeit detection\ntechniques available public databases and a summary of the outcomes of all such\nanalyses From all of the topics in the survey we focus on the most recent\ndevelopment of Deepfake showing its advances and obstacles in detecting fake\nimages\n', '  In recent years, the abuse of a face swap technique called deepfake has\nraised enormous public concerns. So far, a large number of deepfake videos\n(known as ""deepfakes"") have been crafted and uploaded to the internet, calling\nfor effective countermeasures. One promising countermeasure against deepfakes\nis deepfake detection. Several deepfake datasets have been released to support\nthe training and testing of deepfake detectors, such as DeepfakeDetection and\nFaceForensics++. While this has greatly advanced deepfake detection, most of\nthe real videos in these datasets are filmed with a few volunteer actors in\nlimited scenes, and the fake videos are crafted by researchers using a few\npopular deepfake softwares. Detectors developed on these datasets may become\nless effective against real-world deepfakes on the internet. To better support\ndetection against real-world deepfakes, in this paper, we introduce a new\ndataset WildDeepfake which consists of 7,314 face sequences extracted from 707\ndeepfake videos collected completely from the internet. WildDeepfake is a small\ndataset that can be used, in addition to existing datasets, to develop and test\nthe effectiveness of deepfake detectors against real-world deepfakes. We\nconduct a systematic evaluation of a set of baseline detection networks on both\nexisting and our WildDeepfake datasets, and show that WildDeepfake is indeed a\nmore challenging dataset, where the detection performance can decrease\ndrastically. We also propose two (eg. 2D and 3D) Attention-based Deepfake\nDetection Networks (ADDNets) to leverage the attention masks on real/fake faces\nfor improved detection. We empirically verify the effectiveness of ADDNets on\nboth existing datasets and WildDeepfake. The dataset is available at:\nhttps://github.com/OpenTAI/wild-deepfake.\n']",Deepfake Detection and Analysis
228,227,39,227_linguistic_communicate_language_emergent,"['linguistic', 'communicate', 'language', 'emergent', 'agents', 'agent', 'communication', 'emergence', 'communicative', 'languages']","['emergent', 'agents', 'social', 'game', 'iterated', 'simulations', 'evolution', 'communication', 'emergence', 'games']","['linguistic', 'agent', 'emergence', 'behavioral', 'cooperation', 'games', 'learnability', 'biases', 'compositional', 'cultural']","[""  Emergent communication studies the development of language between autonomous\nagents, aiming to improve understanding of natural language evolution and\nincrease communication efficiency. While temporal aspects of language have been\nconsidered in computational linguistics, there has been no research on temporal\nreferences in emergent communication. This paper addresses this gap, by\nexploring how agents communicate about temporal relationships. We analyse three\npotential influences for the emergence of temporal references: environmental,\nexternal, and architectural changes. Our experiments demonstrate that altering\nthe loss function is insufficient for temporal references to emerge; rather,\narchitectural changes are necessary. However, a minimal change in agent\narchitecture, using a different batching method, allows the emergence of\ntemporal references. This modified design is compared with the standard\narchitecture in a temporal referential games environment, which emphasises\ntemporal relationships. The analysis indicates that over 95\\% of the agents\nwith the modified batching method develop temporal references, without changes\nto their loss function. We consider temporal referencing necessary for future\nimprovements to the agents' communication efficiency, yielding a closer to\noptimal coding as compared to purely compositional languages. Our readily\ntransferable architectural insights provide the basis for their incorporation\ninto other emergent communication settings.\n"", '  Effective communication requires the ability to refer to specific parts of an\nobservation in relation to others. While emergent communication literature\nshows success in developing various language properties, no research has shown\nthe emergence of such positional references. This paper demonstrates how agents\ncan communicate about spatial relationships within their observations. The\nresults indicate that agents can develop a language capable of expressing the\nrelationships between parts of their observation, achieving over 90% accuracy\nwhen trained in a referential game which requires such communication. Using a\ncollocation measure, we demonstrate how the agents create such references. This\nanalysis suggests that agents use a mixture of non-compositional and\ncompositional messages to convey spatial relationships. We also show that the\nemergent language is interpretable by humans. The translation accuracy is\ntested by communicating with the receiver agent, where the receiver achieves\nover 78% accuracy using parts of this lexicon, confirming that the\ninterpretation of the emergent language was successful.\n', ""  The advances of Large Language Models (LLMs) are expanding their utility in\nboth academic research and practical applications. Recent social science\nresearch has explored the use of these ``black-box'' LLM agents for simulating\ncomplex social systems and potentially substituting human subjects in\nexperiments. Our study delves into this emerging domain, investigating the\nextent to which LLMs exhibit key social interaction principles, such as social\nlearning, social preference, and cooperative behavior (indirect reciprocity),\nin their interactions with humans and other agents. We develop a framework for\nour study, wherein classical laboratory experiments involving human subjects\nare adapted to use LLM agents. This approach involves step-by-step reasoning\nthat mirrors human cognitive processes and zero-shot learning to assess the\ninnate preferences of LLMs. Our analysis of LLM agents' behavior includes both\nthe primary effects and an in-depth examination of the underlying mechanisms.\nFocusing on GPT-4, our analyses suggest that LLM agents appear to exhibit a\nrange of human-like social behaviors such as distributional and reciprocity\npreferences, responsiveness to group identity cues, engagement in indirect\nreciprocity, and social learning capabilities. However, our analysis also\nreveals notable differences: LLMs demonstrate a pronounced fairness preference,\nweaker positive reciprocity, and a more calculating approach in social learning\ncompared to humans. These insights indicate that while LLMs hold great promise\nfor applications in social science research, such as in laboratory experiments\nand agent-based modeling, the subtle behavioral differences between LLM agents\nand humans warrant further investigation. Careful examination and development\nof protocols in evaluating the social behaviors of LLMs are necessary before\ndirectly applying these models to emulate human behavior.\n""]",Emergent Communication in Artificial Intelligence
229,228,39,228_games_chess_subgames_poker,"['games', 'chess', 'subgames', 'poker', 'strategy', 'gameplay', 'game', 'ai', 'hearthstone', 'play']","['games', 'game', 'poker', 'imperfect', 'card', 'players', 'perfect', 'search', 'board', 'player']","['chess', 'subgames', 'strategy', 'pokergpt', 'decks', 'algorithms', 'determinization', 'search', 'agent', 'monte']","['  As a challenging multi-player card game, DouDizhu has recently drawn much\nattention for analyzing competition and collaboration in imperfect-information\ngames. In this paper, we propose PerfectDou, a state-of-the-art DouDizhu AI\nsystem that dominates the game, in an actor-critic framework with a proposed\ntechnique named perfect information distillation. In detail, we adopt a\nperfect-training-imperfect-execution framework that allows the agents to\nutilize the global information to guide the training of the policies as if it\nis a perfect information game and the trained policies can be used to play the\nimperfect information game during the actual gameplay. To this end, we\ncharacterize card and game features for DouDizhu to represent the perfect and\nimperfect information. To train our system, we adopt proximal policy\noptimization with generalized advantage estimation in a parallel training\nparadigm. In experiments we show how and why PerfectDou beats all existing AI\nprograms, and achieves state-of-the-art performance.\n', '  Traditional search algorithms have issues when applied to games of imperfect\ninformation where the number of possible underlying states and trajectories are\nvery large. This challenge is particularly evident in trick-taking card games.\nWhile state sampling techniques such as Perfect Information Monte Carlo (PIMC)\nsearch has shown success in these contexts, they still have major limitations.\n  We present Generative Observation Monte Carlo Tree Search (GO-MCTS), which\nutilizes MCTS on observation sequences generated by a game specific model. This\nmethod performs the search within the observation space and advances the search\nusing a model that depends solely on the agent\'s observations. Additionally, we\ndemonstrate that transformers are well-suited as the generative model in this\ncontext, and we demonstrate a process for iteratively training the transformer\nvia population-based self-play.\n  The efficacy of GO-MCTS is demonstrated in various games of imperfect\ninformation, such as Hearts, Skat, and ""The Crew: The Quest for Planet Nine,""\nwith promising results.\n', '  Poker is in the family of imperfect information games unlike other games such\nas chess, connect four, etc which are perfect information game instead. While\nmany perfect information games have been solved, no non-trivial imperfect\ninformation game has been solved to date. This makes poker a great test bed for\nArtificial Intelligence research. In this paper we firstly compare Game theory\noptimal poker to Exploitative poker. Secondly, we discuss the intricacies of\nabstraction techniques, betting models, and specific strategies employed by\nsuccessful poker bots like Tartanian[1] and Pluribus[6]. Thirdly, we also\nexplore 2-player vs multi-player games and the limitations that come when\nplaying with more players. Finally, this paper discusses the role of machine\nlearning and theoretical approaches in developing winning strategies and\nsuggests future directions for this rapidly evolving field.\n']",Artificial Intelligence in Imperfect Information Games
230,229,39,229_pathfinding_agents_planning_agent,"['pathfinding', 'agents', 'planning', 'agent', 'paths', 'planner', 'robots', 'algorithms', 'robot', 'heuristics']","['path', 'robots', 'paths', 'collision', 'planning', 'finding', 'agents', 'planner', 'conflict', 'makespan']","['pathfinding', 'planning', 'robot', 'heuristics', 'mapf', 'collisions', 'rendezvous', 'algorithm', 'tpgs', 'owrp']","[""  Multi-Agent Path Finding (MAPF), which involves finding collision-free paths\nfor multiple robots, is crucial in various applications. Lifelong MAPF, where\ntargets are reassigned to agents as soon as they complete their initial\ntargets, offers a more accurate approximation of real-world warehouse planning.\nIn this paper, we present a novel mechanism named Caching-Augmented Lifelong\nMAPF (CAL-MAPF), designed to improve the performance of Lifelong MAPF. We have\ndeveloped a new type of map grid called cache for temporary item storage and\nreplacement, and created a locking mechanism to improve the planning solution's\nstability. A task assigner (TA) is designed for CAL-MAPF to allocate target\nlocations to agents and control agent status in different situations. CAL-MAPF\nhas been evaluated using various cache replacement policies and input task\ndistributions. We have identified three main factors significantly impacting\nCAL-MAPF performance through experimentation: suitable input task distribution,\nhigh cache hit rate, and smooth traffic. In general, CAL-MAPF has demonstrated\npotential for performance improvements in certain task distributions, map and\nagent configurations.\n"", '  Multi-agent path finding (MAPF) is the problem of finding paths for multiple\nagents such that they do not collide. This problem manifests in numerous\nreal-world applications such as controlling transportation robots in automated\nwarehouses, moving characters in video games, and coordinating self-driving\ncars in intersections. Finding optimal solutions to MAPF is NP-Hard, yet modern\noptimal solvers can scale to hundreds of agents and even thousands in some\ncases. Different solvers employ different approaches, and there is no single\nstate-of-the-art approach for all problems. Furthermore, there are no clear,\nprovable, guidelines for choosing when each optimal MAPF solver to use. Prior\nwork employed Algorithm Selection (AS) techniques to learn such guidelines from\npast data. A major challenge when employing AS for choosing an optimal MAPF\nalgorithm is how to encode the given MAPF problem. Prior work either used\nhand-crafted features or an image representation of the problem. We explore\ngraph-based encodings of the MAPF problem and show how they can be used\non-the-fly with a modern graph embedding algorithm called FEATHER. Then, we\nshow how this encoding can be effectively joined with existing encodings,\nresulting in a novel AS method we call MAPF Algorithm selection via Graph\nembedding (MAG). An extensive experimental evaluation of MAG on several MAPF\nalgorithm selection tasks reveals that it is either on-par or significantly\nbetter than existing methods.\n', '  Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics that\nasks us to compute collision-free paths for a team of agents, all moving across\na shared map. Although many works appear on this topic, all current algorithms\nstruggle as the number of agents grows. The principal reason is that existing\napproaches typically plan free-flow optimal paths, which creates congestion. To\ntackle this issue, we propose a new approach for MAPF where agents are guided\nto their destination by following congestion-avoiding paths. We evaluate the\nidea in two large-scale settings: one-shot MAPF, where each agent has a single\ndestination, and lifelong MAPF, where agents are continuously assigned new\ndestinations. Empirically, we report large improvements in solution quality for\none-short MAPF and in overall throughput for lifelong MAPF.\n']",Multi-Agent Path Finding and Planning
231,230,38,230_compression_encoder_encoding_compressing,"['compression', 'encoder', 'encoding', 'compressing', 'compressed', 'decoding', 'bitrate', 'coding', 'jpeg', 'compress']","['compression', 'distortion', 'image', 'codecs', 'coding', 'semantic', 'lossy', 'perceptual', 'channel', 'compressed']","['compression', 'encoder', 'bitrate', 'vvc', 'distortion', 'codecs', 'quantization', 'wavelet', 'perceptual', 'modulation']","['  Image compression constitutes a significant challenge amidst the era of\ninformation explosion. Recent studies employing deep learning methods have\ndemonstrated the superior performance of learning-based image compression\nmethods over traditional codecs. However, an inherent challenge associated with\nthese methods lies in their lack of interpretability. Following an analysis of\nthe varying degrees of compression degradation across different frequency\nbands, we propose the end-to-end optimized image compression model facilitated\nby the frequency-oriented transform. The proposed end-to-end image compression\nmodel consists of four components: spatial sampling, frequency-oriented\ntransform, entropy estimation, and frequency-aware fusion. The\nfrequency-oriented transform separates the original image signal into distinct\nfrequency bands, aligning with the human-interpretable concept. Leveraging the\nnon-overlapping hypothesis, the model enables scalable coding through the\nselective transmission of arbitrary frequency components. Extensive experiments\nare conducted to demonstrate that our model outperforms all traditional codecs\nincluding next-generation standard H.266/VVC on MS-SSIM metric. Moreover,\nvisual analysis tasks (i.e., object detection and semantic segmentation) are\nconducted to verify the proposed compression method could preserve semantic\nfidelity besides signal-level precision.\n', '  In the field of neural data compression, the prevailing focus has been on\noptimizing algorithms for either classical distortion metrics, such as PSNR or\nSSIM, or human perceptual quality. With increasing amounts of data consumed by\nmachines rather than humans, a new paradigm of machine-oriented\ncompression$\\unicode{x2013}$which prioritizes the retention of features salient\nfor machine perception over traditional human-centric\ncriteria$\\unicode{x2013}$has emerged, creating several new challenges to the\ndevelopment, evaluation, and deployment of systems utilizing lossy compression.\nIn particular, it is unclear how different approaches to lossy compression will\naffect the performance of downstream machine perception tasks. To address this\nunder-explored area, we evaluate various perception\nmodels$\\unicode{x2013}$including image classification, image segmentation,\nspeech recognition, and music source separation$\\unicode{x2013}$under severe\nlossy compression. We utilize several popular codecs spanning conventional,\nneural, and generative compression architectures. Our results indicate three\nkey findings: (1) using generative compression, it is feasible to leverage\nhighly compressed data while incurring a negligible impact on machine\nperceptual quality; (2) machine perceptual quality correlates strongly with\ndeep similarity metrics, indicating a crucial role of these metrics in the\ndevelopment of machine-oriented codecs; and (3) using lossy compressed\ndatasets, (e.g. ImageNet) for pre-training can lead to counter-intuitive\nscenarios where lossy compression increases machine perceptual quality rather\nthan degrading it. To encourage engagement on this growing area of research,\nour code and experiments are available at:\nhttps://github.com/danjacobellis/MPQ.\n', '  In this age of information, images are a critical medium for storing and\ntransmitting information. With the rapid growth of image data amount, visual\ncompression and visual data perception are two important research topics\nattracting a lot attention. However, those two topics are rarely discussed\ntogether and follow separate research path. Due to the compact compressed\ndomain representation offered by learning-based image compression methods,\nthere exists possibility to have one stream targeting both efficient data\nstorage and compression, and machine perception tasks. In this paper, we\npropose a layered generative image compression model achieving high human\nvision-oriented image reconstructed quality, even at extreme compression\nratios. To obtain analysis efficiency and flexibility, a task-agnostic\nlearning-based compression model is proposed, which effectively supports\nvarious compressed domain-based analytical tasks while reserves outstanding\nreconstructed perceptual quality, compared with traditional and learning-based\ncodecs. In addition, joint optimization schedule is adopted to acquire best\nbalance point among compression ratio, reconstructed image quality, and\ndownstream perception performance. Experimental results verify that our\nproposed compressed domain-based multi-task analysis method can achieve\ncomparable analysis results against the RGB image-based methods with up to\n99.6% bit rate saving (i.e., compared with taking original RGB image as the\nanalysis model input). The practical ability of our model is further justified\nfrom model size and information fidelity aspects.\n']",Image Compression Methods
232,231,38,231_fairness_unfairness_recommender_unfair,"['fairness', 'unfairness', 'recommender', 'unfair', 'discrimination', 'biases', 'incentives', 'personalization', 'rankings', 'recommendation']","['fairness', 'recommender', 'recommendation', 'creators', 'consumer', 'sided', 'recommendations', 'item', 'provider', 'items']","['unfairness', 'recommender', 'rankings', 'fairsync', 'utility', 'disparities', 'preferences', 'consumers', 'sensitive', 'catalog']","[""  In large-scale recommendation systems, the vast array of items makes it\ninfeasible to obtain accurate user preferences for each product, resulting in a\ncommon issue of missing labels. Typically, only items previously recommended to\nusers have associated ground truth data. Although there is extensive research\non fairness concerning fully observed user-item interactions, the challenge of\nfairness in scenarios with missing labels remains underexplored. Previous\nmethods often treat these samples missing labels as negative, which can\nsignificantly deviate from the ground truth fairness metrics. Our study\naddresses this gap by proposing a novel method employing a small randomized\ntraffic to estimate fairness metrics accurately. We present theoretical bounds\nfor the estimation error of our fairness metric and support our findings with\nempirical evidence on real data. Our numerical experiments on synthetic and\nTikTok's real-world data validate our theory and show the efficiency and\neffectiveness of our novel methods. To the best of our knowledge, we are the\nfirst to emphasize the necessity of random traffic in dataset collection for\nrecommendation fairness, the first to publish a fairness-related dataset from\nTikTok and to provide reliable estimates of fairness metrics in the context of\nlarge-scale recommendation systems with missing labels.\n"", '  Efforts in the recommendation community are shifting from the sole emphasis\non utility to considering beyond-utility factors, such as fairness and\nrobustness. Robustness of recommendation models is typically linked to their\nability to maintain the original utility when subjected to attacks. Limited\nresearch has explored the robustness of a recommendation model in terms of\nfairness, e.g., the parity in performance across groups, under attack\nscenarios. In this paper, we aim to assess the robustness of graph-based\nrecommender systems concerning fairness, when exposed to attacks based on\nedge-level perturbations. To this end, we considered four different fairness\noperationalizations, including both consumer and provider perspectives.\nExperiments on three datasets shed light on the impact of perturbations on the\ntargeted fairness notion, uncovering key shortcomings in existing evaluation\nprotocols for robustness. As an example, we observed perturbations affect\nconsumer fairness on a higher extent than provider fairness, with alarming\nunfairness for the former. Source code:\nhttps://github.com/jackmedda/CPFairRobust\n', '  Fairness is an emerging and challenging topic in recommender systems. In\nrecent years, various ways of evaluating and therefore improving fairness have\nemerged. In this study, we examine existing evaluation measures of fairness in\nrecommender systems. Specifically, we focus solely on exposure-based fairness\nmeasures of individual items that aim to quantify the disparity in how\nindividual items are recommended to users, separate from item relevance to\nusers. We gather all such measures and we critically analyse their theoretical\nproperties. We identify a series of limitations in each of them, which\ncollectively may render the affected measures hard or impossible to interpret,\nto compute, or to use for comparing recommendations. We resolve these\nlimitations by redefining or correcting the affected measures, or we argue why\ncertain limitations cannot be resolved. We further perform a comprehensive\nempirical analysis of both the original and our corrected versions of these\nfairness measures, using real-world and synthetic datasets. Our analysis\nprovides novel insights into the relationship between measures based on\ndifferent fairness concepts, and different levels of measure sensitivity and\nstrictness. We conclude with practical suggestions of which fairness measures\nshould be used and when. Our code is publicly available. To our knowledge, this\nis the first critical comparison of individual item fairness measures in\nrecommender systems.\n']",Fairness in Recommendation Systems
233,232,37,232_adapting_adaptation_tta_test,"['adapting', 'adaptation', 'tta', 'test', 'feature', 'cta', 'batches', 'drift', 'predictions', 'batch']","['adaptation', 'test', 'shifts', 'batch', 'entropy', 'distribution', 'domain', 'pseudo', 'time', 'source']","['adapting', 'cta', 'batches', 'drift', 'predictions', 'forgetting', 'classifier', 'unitta', 'regularizer', 'dsp']","['  Given a model trained on source data, Test-Time Adaptation (TTA) enables\nadaptation and inference in test data streams with domain shifts from the\nsource. Current methods predominantly optimize the model for each incoming test\ndata batch using self-training loss. While these methods yield commendable\nresults in ideal test data streams, where batches are independently and\nidentically sampled from the target distribution, they falter under more\npractical test data streams that are not independent and identically\ndistributed (non-i.i.d.). The data batches in a non-i.i.d. stream display\nprominent label shifts relative to each other. It leads to conflicting\noptimization objectives among batches during the TTA process. Given the\ninherent risks of adapting the source model to unpredictable test-time\ndistributions, we reverse the adaptation process and propose a novel\nDistribution Alignment loss for TTA. This loss guides the distributions of\ntest-time features back towards the source distributions, which ensures\ncompatibility with the well-trained source model and eliminates the pitfalls\nassociated with conflicting optimization objectives. Moreover, we devise a\ndomain shift detection mechanism to extend the success of our proposed TTA\nmethod in the continual domain shift scenarios. Our extensive experiments\nvalidate the logic and efficacy of our method. On six benchmark datasets, we\nsurpass existing methods in non-i.i.d. scenarios and maintain competitive\nperformance under the ideal i.i.d. assumption.\n', ""  This paper proposes a novel online evaluation protocol for Test Time\nAdaptation (TTA) methods, which penalizes slower methods by providing them with\nfewer samples for adaptation. TTA methods leverage unlabeled data at test time\nto adapt to distribution shifts. Although many effective methods have been\nproposed, their impressive performance usually comes at the cost of\nsignificantly increased computation budgets. Current evaluation protocols\noverlook the effect of this extra computation cost, affecting their real-world\napplicability. To address this issue, we propose a more realistic evaluation\nprotocol for TTA methods, where data is received in an online fashion from a\nconstant-speed data stream, thereby accounting for the method's adaptation\nspeed. We apply our proposed protocol to benchmark several TTA methods on\nmultiple datasets and scenarios. Extensive experiments show that, when\naccounting for inference speed, simple and fast approaches can outperform more\nsophisticated but slower methods. For example, SHOT from 2020, outperforms the\nstate-of-the-art method SAR from 2023 in this setting. Our results reveal the\nimportance of developing practical TTA methods that are both accurate and\nefficient.\n"", '  Test Time Adaptation (TTA) addresses the problem of distribution shift by\nenabling pretrained models to learn new features on an unseen domain at test\ntime. However, it poses a significant challenge to maintain a balance between\nlearning new features and retaining useful pretrained features. In this paper,\nwe propose Layerwise EArly STopping (LEAST) for TTA to address this problem.\nThe key idea is to stop adapting individual layers during TTA if the features\nbeing learned do not appear beneficial for the new domain. For that purpose, we\npropose using a novel gradient-based metric to measure the relevance of the\ncurrent learnt features to the new domain without the need for supervised\nlabels. More specifically, we propose to use this metric to determine\ndynamically when to stop updating each layer during TTA. This enables a more\nbalanced adaptation, restricted to layers benefiting from it, and only for a\ncertain number of steps. Such an approach also has the added effect of limiting\nthe forgetting of pretrained features useful for dealing with new domains.\nThrough extensive experiments, we demonstrate that Layerwise Early Stopping\nimproves the performance of existing TTA approaches across multiple datasets,\ndomain shifts, model architectures, and TTA losses.\n']",Test-Time Adaptation for Machine Learning Models
234,233,37,233_wav2vec_wav2vec2_wav2code_speech,"['wav2vec', 'wav2vec2', 'wav2code', 'speech', 'supervised', 'encoder', 'wavlm', 'speaker', 'ssft', 'phoneme']","['speech', 'supervised', 'self', 'representations', 'downstream', 'conformer', 'acoustic', 'pre', 'speaker', 'layers']","['wav2code', 'supervised', 'encoder', 'speaker', 'ssft', 'tdnn', 'asr', 'dementiabank', 'mpl', 'tuning']","['  Speech and language models trained through self-supervised learning (SSL)\ndemonstrate strong alignment with brain activity during speech and language\nperception. However, given their distinct training modalities, it remains\nunclear whether they correlate with the same neural aspects. We directly\naddress this question by evaluating the brain prediction performance of two\nrepresentative SSL models, Wav2Vec2.0 and GPT-2, designed for speech and\nlanguage tasks. Our findings reveal that both models accurately predict speech\nresponses in the auditory cortex, with a significant correlation between their\nbrain predictions. Notably, shared speech contextual information between\nWav2Vec2.0 and GPT-2 accounts for the majority of explained variance in brain\nactivity, surpassing static semantic and lower-level acoustic-phonetic\ninformation. These results underscore the convergence of speech contextual\nrepresentations in SSL models and their alignment with the neural network\nunderlying speech perception, offering valuable insights into both SSL models\nand the neural basis of speech and language processing.\n', '  There is a growing interest in cost-effective self-supervised fine-tuning\n(SSFT) of self-supervised learning (SSL)-based speech models to obtain\ntask-specific representations. These task-specific representations are used for\nrobust performance on various downstream tasks by fine-tuning on the labelled\ndata. This work presents a cost-effective SSFT method named Self-supervised\nCorrespondence (SCORE) fine-tuning to adapt the SSL speech representations for\ncontent-related tasks. The proposed method uses a correspondence training\nstrategy, aiming to learn similar representations from perturbed speech and\noriginal speech. Commonly used data augmentation techniques for content-related\ntasks (ASR) are applied to obtain perturbed speech. SCORE fine-tuned HuBERT\noutperforms the vanilla HuBERT on SUPERB benchmark with only a few hours of\nfine-tuning (< 5 hrs) on a single GPU for automatic speech recognition, phoneme\nrecognition, and query-by-example tasks, with relative improvements of 1.09%,\n3.58%, and 12.65%, respectively. SCORE provides competitive results with the\nrecently proposed SSFT method SPIN, using only 1/3 of the processed speech\ncompared to SPIN.\n', '  Self-supervised learning (SSL)-based speech models are extensively used for\nfull-stack speech processing. However, it has been observed that improving\nSSL-based speech representations using unlabeled speech for content-related\ntasks is challenging and computationally expensive. Recent attempts have been\nmade to address this issue with cost-effective self-supervised fine-tuning\n(SSFT) approaches. Continuing in this direction, a cost-effective SSFT method\nnamed ""LASER: Learning by Aligning Self-supervised Representations"" is\npresented. LASER is based on the soft-DTW alignment loss with temporal\nregularisation term. Experiments are conducted with HuBERT and WavLM models and\nevaluated on the SUPERB benchmark for two content-related tasks: automatic\nspeech recognition (ASR) and phoneme recognition (PR). A relative improvement\nof 3.7% and 8.2% for HuBERT, and 4.1% and 11.7% for WavLM are observed, for the\nASR and PR tasks respectively, with only < 3 hours of fine-tuning on a single\nGPU.\n']",Self-Supervised Speech Models and Representations
235,234,37,234_wirelessllm_wireless_networking_communications,"['wirelessllm', 'wireless', 'networking', 'communications', '5g', 'network', 'ai', 'generative', 'networks', 'mobile']","['wireless', 'communication', 'semantic', 'communications', 'networks', 'intelligence', 'network', 'artificial', 'native', 'sixth']","['wirelessllm', 'generative', '6g', 'genai', 'semantic', 'transmission', 'technologies', 'intelligent', 'wdmoe', 'allocation']","['  Intelligent communications have played a pivotal role in shaping the\nevolution of 6G networks. Native artificial intelligence (AI) within green\ncommunication systems must meet stringent real-time requirements. To achieve\nthis, deploying lightweight and resource-efficient AI models is necessary.\nHowever, as wireless networks generate a multitude of data fields and\nindicators during operation, only a fraction of them imposes significant impact\non the network AI models. Therefore, real-time intelligence of communication\nsystems heavily relies on a small but critical set of the data that profoundly\ninfluences the performance of network AI models. These challenges underscore\nthe need for innovative architectures and solutions. In this paper, we propose\na solution, termed the pervasive multi-level (PML) native AI architecture,\nwhich integrates the concept of knowledge graph (KG) into the intelligent\noperational manipulations of mobile networks, resulting in the establishment of\na wireless data KG. Leveraging the wireless data KG, we characterize the\nmassive and complex data collected from wireless communication networks and\nanalyze the relationships among various data fields. The obtained graph of data\nfield relations enables the on-demand generation of minimal and effective\ndatasets, referred to as feature datasets, tailored to specific application\nrequirements. Consequently, this architecture not only enhances AI training,\ninference, and validation processes but also significantly reduces resource\nwastage and overhead for communication networks. To implement this\narchitecture, we have developed a specific solution comprising a\nspatio-temporal heterogeneous graph attention neural network model (STREAM) as\nwell as a feature dataset generation algorithm. Experiments are conducted to\nvalidate the effectiveness of the proposed architecture.\n', '  Generative artificial intelligence (GenAI) and communication networks are\nexpected to have groundbreaking synergies in 6G. Connecting GenAI agents over a\nwireless network can potentially unleash the power of collective intelligence\nand pave the way for artificial general intelligence (AGI). However, current\nwireless networks are designed as a ""data pipe"" and are not suited to\naccommodate and leverage the power of GenAI. In this paper, we propose the\nGenAINet framework in which distributed GenAI agents communicate knowledge\n(high-level concepts or abstracts) to accomplish arbitrary tasks. We first\nprovide a network architecture integrating GenAI capabilities to manage both\nnetwork protocols and applications. Building on this, we investigate effective\ncommunication and reasoning problems by proposing a semantic-native GenAINet.\nSpecifically, GenAI agents extract semantic concepts from multi-modal raw data,\nbuild a knowledgebase representing their semantic relations, which is retrieved\nby GenAI models for planning and reasoning. Under this paradigm, an agent can\nlearn fast from other agents\' experience for making better decisions with\nefficient communications. Furthermore, we conduct two case studies where in\nwireless device query, we show that extracting and transferring knowledge can\nimprove query accuracy with reduced communication; and in wireless power\ncontrol, we show that distributed agents can improve decisions via\ncollaborative reasoning. Finally, we address that developing a hierarchical\nsemantic level Telecom world model is a key path towards network of collective\nintelligence.\n', '  Wireless communications advance hand-in-hand with artificial intelligence\n(AI), indicating an interconnected advancement where each facilitates and\nbenefits from the other. This synergy is particularly evident in the\ndevelopment of the sixth-generation technology standard for mobile networks\n(6G), envisioned to be AI-native. Generative-AI (GenAI), a novel technology\ncapable of producing various types of outputs, including text, images, and\nvideos, offers significant potential for wireless communications, with its\ndistinctive features. Traditionally, conventional AI techniques have been\nemployed for predictions, classifications, and optimization, while GenAI has\nmore to offer. This article introduces the concept of strategic demand-planning\nthrough demand-labeling, demand-shaping, and demand-rescheduling. Accordingly,\nGenAI is proposed as a powerful tool to facilitate demand-shaping in wireless\nnetworks. More specifically, GenAI is used to compress and convert the content\nof various kind (e.g., from a higher bandwidth mode to a lower one, such as\nfrom a video to text), which subsequently enhances performance of wireless\nnetworks in various usage scenarios such as cell-switching, user association\nand load balancing, interference management, and disaster scenarios management.\nTherefore, GenAI can serve a function in saving energy and spectrum in wireless\nnetworks. With recent advancements in AI, including sophisticated algorithms\nlike large-language-models and the development of more powerful hardware built\nexclusively for AI tasks, such as AI accelerators, the concept of\ndemand-planning, particularly demand-shaping through GenAI, becomes\nincreasingly relevant. Furthermore, recent efforts to make GenAI accessible on\ndevices, such as user terminals, make the implementation of this concept even\nmore straightforward and feasible.\n']",Artificial Intelligence in Wireless Networking and Communications
236,235,37,235_supervised_segmentation_classifier_mask2former,"['supervised', 'segmentation', 'classifier', 'mask2former', 'mask', 'classes', 'foreground', 'masks', 'coco', 'detectors']","['segmentation', 'object', 'masks', 'pseudo', 'weakly', 'labels', 'instance', 'pixel', 'supervised', 'objects']","['segmentation', 'mask2former', 'foreground', 'coco', 'detectors', 'annotations', 'instance', 'categories', 'pixel', 'camouflaged']","['  Generating reliable pseudo masks from image-level labels is challenging in\nthe weakly supervised semantic segmentation (WSSS) task due to the lack of\nspatial information. Prevalent class activation map (CAM)-based solutions are\nchallenged to discriminate the foreground (FG) objects from the suspicious\nbackground (BG) pixels (a.k.a. co-occurring) and learn the integral object\nregions. This paper proposes a simple fine-grained background representation\n(FBR) method to discover and represent diverse BG semantics and address the\nco-occurring problems. We abandon using the class prototype or pixel-level\nfeatures for BG representation. Instead, we develop a novel primitive, negative\nregion of interest (NROI), to capture the fine-grained BG semantic information\nand conduct the pixel-to-NROI contrast to distinguish the confusing BG pixels.\nWe also present an active sampling strategy to mine the FG negatives\non-the-fly, enabling efficient pixel-to-pixel intra-foreground contrastive\nlearning to activate the entire object region. Thanks to the simplicity of\ndesign and convenience in use, our proposed method can be seamlessly plugged\ninto various models, yielding new state-of-the-art results under various WSSS\nsettings across benchmarks. Leveraging solely image-level (I) labels as\nsupervision, our method achieves 73.2 mIoU and 45.6 mIoU segmentation results\non Pascal Voc and MS COCO test sets, respectively. Furthermore, by\nincorporating saliency maps as an additional supervision signal (I+S), we\nattain 74.9 mIoU on Pascal Voc test set. Concurrently, our FBR approach\ndemonstrates meaningful performance gains in weakly-supervised instance\nsegmentation (WSIS) tasks, showcasing its robustness and strong generalization\ncapabilities across diverse domains.\n', '  Image-level weakly-supervised semantic segmentation (WSSS) reduces the\nusually vast data annotation cost by surrogate segmentation masks during\ntraining. The typical approach involves training an image classification\nnetwork using global average pooling (GAP) on convolutional feature maps. This\nenables the estimation of object locations based on class activation maps\n(CAMs), which identify the importance of image regions. The CAMs are then used\nto generate pseudo-labels, in the form of segmentation masks, to supervise a\nsegmentation model in the absence of pixel-level ground truth. Our work is\nbased on two techniques for improving CAMs; importance sampling, which is a\nsubstitute for GAP, and the feature similarity loss, which utilizes a heuristic\nthat object contours almost always align with color edges in images. However,\nboth are based on the multinomial posterior with softmax, and implicitly assume\nthat classes are mutually exclusive, which turns out suboptimal in our\nexperiments. Thus, we reformulate both techniques based on binomial posteriors\nof multiple independent binary problems. This has two benefits; their\nperformance is improved and they become more general, resulting in an add-on\nmethod that can boost virtually any WSSS method. This is demonstrated on a wide\nvariety of baselines on the PASCAL VOC dataset, improving the region similarity\nand contour quality of all implemented state-of-the-art methods. Experiments on\nthe MS COCO dataset further show that our proposed add-on is well-suited for\nlarge-scale settings. Our code implementation is available at\nhttps://github.com/arvijj/hfpl.\n', '  Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation\nmodels using image data with only image-level supervision. Since precise\npixel-level annotations are not accessible, existing methods typically focus on\nproducing pseudo masks for training segmentation models by refining CAM-like\nheatmaps. However, the produced heatmaps may capture only the discriminative\nimage regions of object categories or the associated co-occurring backgrounds.\nTo address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS)\nframework, which learns to effectively prompt the CLIP latent space to enhance\nthe semantic alignment between the segmented regions and the target object\ncategories. More specifically, we propose Contrastive Prompt Learning and\nPrompt-guided Semantic Refinement to learn the prompts that adequately describe\nand suppress the co-occurring backgrounds associated with each target object\ncategory. In this way, SemPLeS can perform better semantic alignment between\nobject regions and the associated class labels, resulting in desired pseudo\nmasks for training the segmentation model. The proposed SemPLeS framework\nachieves SOTA performance on the standard WSSS benchmarks, PASCAL VOC and MS\nCOCO, and shows compatibility with other WSSS methods. The source codes are\nprovided in the supplementary.\n']",Weakly Supervised Semantic Segmentation
237,236,37,236_parallelization_unifiednn_distributed_throughput,"['parallelization', 'unifiednn', 'distributed', 'throughput', 'bottleneck', 'cloud', 'streaming', 'memory', 'network', 'deep']","['compression', 'communication', 'bandwidth', 'topologies', 'devices', 'training', 'batch', 'cloud', 'scale', 'parallel']","['parallelization', 'unifiednn', 'bottleneck', 'cloud', 'sgd', 'cluster', 'straggler', 'compression', 'gpu', 'allreduce']","['  With rapidly increasing distributed deep learning workloads in large-scale\ndata centers, efficient distributed deep learning framework strategies for\nresource allocation and workload scheduling have become the key to\nhigh-performance deep learning. The large-scale environment with large volumes\nof datasets, models, and computational and communication resources raises\nvarious unique challenges for resource allocation and workload scheduling in\ndistributed deep learning, such as scheduling complexity, resource and workload\nheterogeneity, and fault tolerance. To uncover these challenges and\ncorresponding solutions, this survey reviews the literature, mainly from 2019\nto 2024, on efficient resource allocation and workload scheduling strategies\nfor large-scale distributed DL. We explore these strategies by focusing on\nvarious resource types, scheduling granularity levels, and performance goals\nduring distributed training and inference processes. We highlight critical\nchallenges for each topic and discuss key insights of existing technologies. To\nillustrate practical large-scale resource allocation and workload scheduling in\nreal distributed deep learning scenarios, we use a case study of training large\nlanguage models. This survey aims to encourage computer science, artificial\nintelligence, and communications researchers to understand recent advances and\nexplore future research directions for efficient framework strategies for\nlarge-scale distributed deep learning.\n', '  The past few years have witnessed the flourishing of large-scale deep neural\nnetwork models with ever-growing parameter numbers. Training such large-scale\nmodels typically requires massive memory and computing resources that exceed\nthose of a single GPU, necessitating distributed training. As GPU performance\nhas rapidly evolved in recent years, computation time has shrunk, thereby\nincreasing the proportion of communication in the overall training time.\nTherefore, optimizing communication for distributed training has become an\nurgent issue. In this article, we briefly introduce the general architecture of\ndistributed deep neural network training and analyze relationships among\nParallelization Strategy, Collective Communication Library, and Network from\nthe perspective of communication optimization, which forms a three-layer\nparadigm. We then review current representative research advances with this\nthree-layer paradigm. We find that layers in the current three-layer paradigm\nare relatively independent, but there is a rich design space for cross-layer\ncollaborative optimization in distributed training scenarios. Therefore, we\nfurther advocate a communication-efficient five-layer paradigm underlining\nopportunities for collaboration designs and look forward to the perspectives of\n""Vertical"", ""Horizontal"", ""Intra-Inter"" and ""Host-Net"" collaboration designs.\nWe hope this article can shed some light on future research on communication\noptimization for distributed training.\n', '  With the rapid growth in the volume of data sets, models, and devices in the\ndomain of deep learning, there is increasing attention on large-scale\ndistributed deep learning. In contrast to traditional distributed deep\nlearning, the large-scale scenario poses new challenges that include fault\ntolerance, scalability of algorithms and infrastructures, and heterogeneity in\ndata sets, models, and resources. Due to intensive synchronization of models\nand sharing of data across GPUs and computing nodes during distributed training\nand inference processes, communication efficiency becomes the bottleneck for\nachieving high performance at a large scale. This article surveys the\nliterature over the period of 2018-2023 on algorithms and technologies aimed at\nachieving efficient communication in large-scale distributed deep learning at\nvarious levels, including algorithms, frameworks, and infrastructures.\nSpecifically, we first introduce efficient algorithms for model synchronization\nand communication data compression in the context of large-scale distributed\ntraining. Next, we introduce efficient strategies related to resource\nallocation and task scheduling for use in distributed training and inference.\nAfter that, we present the latest technologies pertaining to modern\ncommunication infrastructures used in distributed deep learning with a focus on\nexamining the impact of the communication overhead in a large-scale and\nheterogeneous setting. Finally, we conduct a case study on the distributed\ntraining of large language models at a large scale to illustrate how to apply\nthese technologies in real cases. This article aims to offer researchers a\ncomprehensive understanding of the current landscape of large-scale distributed\ndeep learning and to reveal promising future research directions toward\ncommunication-efficient solutions in this scope.\n']",Distributed Deep Learning Optimization
238,237,37,237_mobilitygpt_trajectories_mobility_trips,"['mobilitygpt', 'trajectories', 'mobility', 'trips', 'gps', 'trajcl', 'trip', 'travel', 'trajectory', 'traffic']","['trajectory', 'mobility', 'trajectories', 'travel', 'road', 'airport', 'movement', 'location', 'traffic', 'airports']","['mobilitygpt', 'trajectories', 'trips', 'gps', 'traffic', 'urban', 'geospatial', 'airports', 'datasets', 'semantic']","['  Trajectory computing is a pivotal domain encompassing trajectory data\nmanagement and mining, garnering widespread attention due to its crucial role\nin various practical applications such as location services, urban traffic, and\npublic safety. Traditional methods, focusing on simplistic spatio-temporal\nfeatures, face challenges of complex calculations, limited scalability, and\ninadequate adaptability to real-world complexities. In this paper, we present a\ncomprehensive review of the development and recent advances in deep learning\nfor trajectory computing (DL4Traj). We first define trajectory data and provide\na brief overview of widely-used deep learning models. Systematically, we\nexplore deep learning applications in trajectory management (pre-processing,\nstorage, analysis, and visualization) and mining (trajectory-related\nforecasting, trajectory-related recommendation, trajectory classification,\ntravel time estimation, anomaly detection, and mobility generation). Notably,\nwe encapsulate recent advancements in Large Language Models (LLMs) that hold\nthe potential to augment trajectory computing. Additionally, we summarize\napplication scenarios, public datasets, and toolkits. Finally, we outline\ncurrent challenges in DL4Traj research and propose future directions. Relevant\npapers and open-source resources have been collated and are continuously\nupdated at:\n\\href{https://github.com/yoshall/Awesome-Trajectory-Computing}{DL4Traj Repo}.\n', ""  Understanding human mobility patterns is essential for various applications,\nfrom urban planning to public safety. The individual trajectory such as mobile\nphone location data, while rich in spatio-temporal information, often lacks\nsemantic detail, limiting its utility for in-depth mobility analysis. Existing\nmethods can infer basic routine activity sequences from this data, lacking\ndepth in understanding complex human behaviors and users' characteristics.\nAdditionally, they struggle with the dependency on hard-to-obtain auxiliary\ndatasets like travel surveys. To address these limitations, this paper defines\ntrajectory semantic inference through three key dimensions: user occupation\ncategory, activity sequence, and trajectory description, and proposes the\nTrajectory Semantic Inference with Large Language Models (TSI-LLM) framework to\nleverage LLMs infer trajectory semantics comprehensively and deeply. We adopt\nspatio-temporal attributes enhanced data formatting (STFormat) and design a\ncontext-inclusive prompt, enabling LLMs to more effectively interpret and infer\nthe semantics of trajectory data. Experimental validation on real-world\ntrajectory datasets demonstrates the efficacy of TSI-LLM in deciphering complex\nhuman mobility patterns. This study explores the potential of LLMs in enhancing\nthe semantic analysis of trajectory data, paving the way for more sophisticated\nand accessible human mobility research.\n"", '  Trajectory Representation Learning (TRL) is a powerful tool for\nspatial-temporal data analysis and management. TRL aims to convert complicated\nraw trajectories into low-dimensional representation vectors, which can be\napplied to various downstream tasks, such as trajectory classification,\nclustering, and similarity computation. Existing TRL works usually treat\ntrajectories as ordinary sequence data, while some important spatial-temporal\ncharacteristics, such as temporal regularities and travel semantics, are not\nfully exploited. To fill this gap, we propose a novel Self-supervised\ntrajectory representation learning framework with TemporAl Regularities and\nTravel semantics, namely START. The proposed method consists of two stages. The\nfirst stage is a Trajectory Pattern-Enhanced Graph Attention Network (TPE-GAT),\nwhich converts the road network features and travel semantics into\nrepresentation vectors of road segments. The second stage is a Time-Aware\nTrajectory Encoder (TAT-Enc), which encodes representation vectors of road\nsegments in the same trajectory as a trajectory representation vector,\nmeanwhile incorporating temporal regularities with the trajectory\nrepresentation. Moreover, we also design two self-supervised tasks, i.e.,\nspan-masked trajectory recovery and trajectory contrastive learning, to\nintroduce spatial-temporal characteristics of trajectories into the training\nprocess of our START framework. The effectiveness of the proposed method is\nverified by extensive experiments on two large-scale real-world datasets for\nthree downstream tasks. The experiments also demonstrate that our method can be\ntransferred across different cities to adapt heterogeneous trajectory datasets.\n']",Trajectory Computing and Mobility Analysis
239,238,37,238_welding_laser_fusion_machining,"['welding', 'laser', 'fusion', 'machining', 'melt', 'porosity', 'voxels', 'meltpool', 'surface', 'fabrication']","['manufacturing', 'melt', 'laser', 'printing', 'thermal', 'temperature', 'metal', 'pool', 'roughness', 'additive']","['welding', 'laser', 'machining', 'porosity', 'meltpool', 'cad', 'deepforge', 'extrusion', 'printing', 'temperature']","['  Multi-fidelity (MF) modeling is a powerful statistical approach that can\nintelligently blend data from varied fidelity sources. This approach finds a\ncompelling application in predicting melt pool geometry for laser-directed\nenergy deposition (L-DED). One major challenge in using MF surrogates to merge\na hierarchy of melt pool models is the variability in input spaces. To address\nthis challenge, this paper introduces a novel approach for constructing an MF\nsurrogate for predicting melt pool geometry by integrating models of varying\ncomplexity, that operate on heterogeneous input spaces. The first thermal model\nincorporates five input parameters i.e., laser power, scan velocity, powder\nflow rate, carrier gas flow rate, and nozzle height. In contrast, the second\nthermal model can only handle laser power and scan velocity. A mapping is\nestablished between the heterogeneous input spaces so that the five-dimensional\nspace can be morphed into a pseudo two-dimensional space. Predictions are then\nblended using a Gaussian process-based co-kriging method. The resulting\nheterogeneous multi-fidelity Gaussian process (Het-MFGP) surrogate not only\nimproves predictive accuracy but also offers computational efficiency by\nreducing evaluations required from the high-dimensional, high-fidelity thermal\nmodel. The results underscore the benefits of employing Het-MFGP for modeling\nmelt pool behavior in L-DED. The framework successfully demonstrates how to\nleverage multimodal data and handle scenarios where certain input parameters\nmay be difficult to model or measure.\n', '  Insufficient overlap between the melt pools produced during Laser Powder Bed\nFusion (L-PBF) can lead to lack-of-fusion defects and deteriorated mechanical\nand fatigue performance. In-situ monitoring of the melt pool subsurface\nmorphology requires specialized equipment that may not be readily accessible or\nscalable. Therefore, we introduce a machine learning framework to correlate\nin-situ two-color thermal images observed via high-speed color imaging to the\ntwo-dimensional profile of the melt pool cross-section. Specifically, we employ\na hybrid CNN-Transformer architecture to establish a correlation between single\nbead off-axis thermal image sequences and melt pool cross-section contours\nmeasured via optical microscopy. In this architecture, a ResNet model embeds\nthe spatial information contained within the thermal images to a latent vector,\nwhile a Transformer model correlates the sequence of embedded vectors to\nextract temporal information. Our framework is able to model the curvature of\nthe subsurface melt pool structure, with improved performance in high energy\ndensity regimes compared to analytical melt pool models. The performance of\nthis model is evaluated through dimensional and geometric comparisons to the\ncorresponding experimental melt pool observations.\n', '  A digital twin (DT), with the components of a physics-based model, a\ndata-driven model, and a machine learning (ML) enabled efficient surrogate,\nbehaves as a virtual twin of the real-world physical process. In terms of Laser\nPowder Bed Fusion (L-PBF) based additive manufacturing (AM), a DT can predict\nthe current and future states of the melt pool and the resulting defects\ncorresponding to the input laser parameters, evolve itself by assimilating\nin-situ sensor data, and optimize the laser parameters to mitigate defect\nformation. In this paper, we present a deep neural operator enabled\ncomputational framework of the DT for closed-loop feedback control of the L-PBF\nprocess. This is accomplished by building a high-fidelity computational model\nto accurately represent the melt pool states, an efficient surrogate model to\napproximate the melt pool solution field, followed by an physics-based\nprocedure to extract information from the computed melt pool simulation that\ncan further be correlated to the defect quantities of interest (e.g., surface\nroughness). In particular, we leverage the data generated from the\nhigh-fidelity physics-based model and train a series of Fourier neural operator\n(FNO) based ML models to effectively learn the relation between the input laser\nparameters and the corresponding full temperature field of the melt pool.\nSubsequently, a set of physics-informed variables such as the melt pool\ndimensions and the peak temperature can be extracted to compute the resulting\ndefects. An optimization algorithm is then exercised to control laser input and\nminimize defects. On the other hand, the constructed DT can also evolve with\nthe physical twin via offline finetuning and online material calibration.\nFinally, a probabilistic framework is adopted for uncertainty quantification.\nThe developed DT is envisioned to guide the AM process and facilitate\nhigh-quality manufacturing.\n']",Laser-Based Metal 3D Printing and Machining
240,239,37,239_dermatologist_dermatology_melanoma_dermatologists,"['dermatologist', 'dermatology', 'melanoma', 'dermatologists', 'skin', 'dermatological', 'segmentation', 'classify', 'classifying', 'classification']","['skin', 'lesion', 'melanoma', 'lesions', 'dermatologists', 'dermatological', 'cancer', 'dermoscopic', 'diagnosis', 'dermatology']","['dermatology', 'melanoma', 'segmentation', 'classifying', 'cnn', 'dermoscopy', 'dermsynth3d', 'screening', 'tool', 'validation']","['  Skin lesions are classified in benign or malignant. Among the malignant,\nmelanoma is a very aggressive cancer and the major cause of deaths. So, early\ndiagnosis of skin cancer is very desired. In the last few years, there is a\ngrowing interest in computer aided diagnostic (CAD) using most image and\nclinical data of the lesion. These sources of information present limitations\ndue to their inability to provide information of the molecular structure of the\nlesion. NIR spectroscopy may provide an alternative source of information to\nautomated CAD of skin lesions. The most commonly used techniques and\nclassification algorithms used in spectroscopy are Principal Component Analysis\n(PCA), Partial Least Squares - Discriminant Analysis (PLS-DA), and Support\nVector Machines (SVM). Nonetheless, there is a growing interest in applying the\nmodern techniques of machine and deep learning (MDL) to spectroscopy. One of\nthe main limitations to apply MDL to spectroscopy is the lack of public\ndatasets. Since there is no public dataset of NIR spectral data to skin\nlesions, as far as we know, an effort has been made and a new dataset named\nNIR-SC-UFES, has been collected, annotated and analyzed generating the\ngold-standard for classification of NIR spectral data to skin cancer. Next, the\nmachine learning algorithms XGBoost, CatBoost, LightGBM, 1D-convolutional\nneural network (1D-CNN) were investigated to classify cancer and non-cancer\nskin lesions. Experimental results indicate the best performance obtained by\nLightGBM with pre-processing using standard normal variate (SNV), feature\nextraction providing values of 0.839 for balanced accuracy, 0.851 for recall,\n0.852 for precision, and 0.850 for F-score. The obtained results indicate the\nfirst steps in CAD of skin lesions aiming the automated triage of patients with\nskin lesions in vivo using NIR spectral data.\n', '  Development of artificial intelligence (AI) techniques in medical imaging\nrequires access to large-scale and diverse datasets for training and\nevaluation. In dermatology, obtaining such datasets remains challenging due to\nsignificant variations in patient populations, illumination conditions, and\nacquisition system characteristics. In this work, we propose S-SYNTH, the first\nknowledge-based, adaptable open-source skin simulation framework to rapidly\ngenerate synthetic skin, 3D models and digitally rendered images, using an\nanatomically inspired multi-layer, multi-component skin and growing lesion\nmodel. The skin model allows for controlled variation in skin appearance, such\nas skin color, presence of hair, lesion shape, and blood fraction among other\nparameters. We use this framework to study the effect of possible variations on\nthe development and evaluation of AI models for skin lesion segmentation, and\nshow that results obtained using synthetic data follow similar comparative\ntrends as real dermatologic images, while mitigating biases and limitations\nfrom existing datasets including small dataset size, lack of diversity, and\nunderrepresentation.\n', ""  Skin cancer is a global health concern, necessitating early and accurate\ndiagnosis for improved patient outcomes. This study introduces a groundbreaking\napproach to skin cancer classification, employing the Vision Transformer, a\nstate-of-the-art deep learning architecture renowned for its success in diverse\nimage analysis tasks. Utilizing the HAM10000 dataset of 10,015 meticulously\nannotated skin lesion images, the model undergoes preprocessing for enhanced\nrobustness. The Vision Transformer, adapted to the skin cancer classification\ntask, leverages the self-attention mechanism to capture intricate spatial\ndependencies, achieving superior performance over traditional deep learning\narchitectures. Segment Anything Model aids in precise segmentation of cancerous\nareas, attaining high IOU and Dice Coefficient. Extensive experiments highlight\nthe model's supremacy, particularly the Google-based ViT patch-32 variant,\nwhich achieves 96.15% accuracy and showcases potential as an effective tool for\ndermatologists in skin cancer diagnosis, contributing to advancements in\ndermatological practices.\n""]",Skin Cancer Diagnosis using AI and Machine Learning
241,240,37,240_agent_agents_ai_agentkit,"['agent', 'agents', 'ai', 'agentkit', 'agentgym', 'autonomous', 'intelligent', 'language', 'agentlite', 'autoagents']","['agents', 'agent', 'autonomous', 'multi', 'solving', 'production', 'collaboration', 'systems', 'complex', 'cooperation']","['agentkit', 'autoagents', 'intelligence', 'abstractions', 'autonomy', 'prompting', 'collaborate', 'evoagent', 'agi', 'hypermedia']","[""  Large Language Models (LLMs) have achieved remarkable success across a wide\narray of tasks. Due to the impressive planning and reasoning abilities of LLMs,\nthey have been used as autonomous agents to do many tasks automatically.\nRecently, based on the development of using one LLM as a single planning or\ndecision-making agent, LLM-based multi-agent systems have achieved considerable\nprogress in complex problem-solving and world simulation. To provide the\ncommunity with an overview of this dynamic field, we present this survey to\noffer an in-depth discussion on the essential aspects of multi-agent systems\nbased on LLMs, as well as the challenges. Our goal is for readers to gain\nsubstantial insights on the following questions: What domains and environments\ndo LLM-based multi-agents simulate? How are these agents profiled and how do\nthey communicate? What mechanisms contribute to the growth of agents'\ncapacities? For those interested in delving into this field of study, we also\nsummarize the commonly used datasets or benchmarks for them to have convenient\naccess. To keep researchers updated on the latest studies, we maintain an\nopen-source GitHub repository, dedicated to outlining the research on LLM-based\nmulti-agent systems.\n"", '  The rise of powerful large language models (LLMs) has spurred a new trend in\nbuilding LLM-based autonomous agents for solving complex tasks, especially\nmulti-agent systems. Despite the remarkable progress, we notice that existing\nworks are heavily dependent on human-designed frameworks, which greatly limits\nthe functional scope and scalability of agent systems. How to automatically\nextend the specialized agent to multi-agent systems to improve task-solving\ncapability still remains a significant challenge. In this paper, we introduce\nEvoAgent, a generic method to automatically extend expert agents to multi-agent\nsystems via the evolutionary algorithm, thereby improving the effectiveness of\nLLM-based agents in solving tasks. Specifically, we consider the existing agent\nframeworks as the initial individual and then apply a series of evolutionary\noperators (e.g., mutation, crossover, selection, etc.) to generate multiple\nagents with diverse agent settings. EvoAgent can be generalized to any\nLLM-based agent framework, and can automatically extend the existing agent\nframework to multi-agent systems without any extra human designs. Experimental\nresults across various tasks have shown that EvoAgent can automatically\ngenerate multiple expert agents and significantly enhance the task-solving\ncapabilities of LLM-based agents.\n', '  Intelligent agents stand out as a potential path toward artificial general\nintelligence (AGI). Thus, researchers have dedicated significant effort to\ndiverse implementations for them. Benefiting from recent progress in large\nlanguage models (LLMs), LLM-based agents that use universal natural language as\nan interface exhibit robust generalization capabilities across various\napplications -- from serving as autonomous general-purpose task assistants to\napplications in coding, social, and economic domains, LLM-based agents offer\nextensive exploration opportunities. This paper surveys current research to\nprovide an in-depth overview of LLM-based intelligent agents within\nsingle-agent and multi-agent systems. It covers their definitions, research\nframeworks, and foundational components such as their composition, cognitive\nand planning methods, tool utilization, and responses to environmental\nfeedback. We also delve into the mechanisms of deploying LLM-based agents in\nmulti-agent systems, including multi-role collaboration, message passing, and\nstrategies to alleviate communication issues between agents. The discussions\nalso shed light on popular datasets and application scenarios. We conclude by\nenvisioning prospects for LLM-based agents, considering the evolving landscape\nof AI and natural language processing.\n']",Large Language Model-based Autonomous Agents
242,241,36,241_3d_lidar_points_s3dis,"['3d', 'lidar', 'points', 's3dis', 'supervised', 'segmentation', 'detr3d', '2d', 'scenes', 'slam']","['point', 'cloud', 'clouds', 'pseudo', 'depth', 'segmentation', 'scene', 'object', 'labels', 'region']","['lidar', 'points', 's3dis', 'scenes', 'fusionvision', 'annotations', 'detectors', 'robotics', 'modelnet10', 'segment']","['  3D object detection plays a crucial role in various applications such as\nautonomous vehicles, robotics and augmented reality. However, training 3D\ndetectors requires a costly precise annotation, which is a hindrance to scaling\nannotation to large datasets. To address this challenge, we propose a weakly\nsupervised 3D annotator that relies solely on 2D bounding box annotations from\nimages, along with size priors. One major problem is that supervising a 3D\ndetection model using only 2D boxes is not reliable due to ambiguities between\ndifferent 3D poses and their identical 2D projection. We introduce a simple yet\neffective and generic solution: we build 3D proxy objects with annotations by\nconstruction and add them to the training dataset. Our method requires only\nsize priors to adapt to new classes. To better align 2D supervision with 3D\ndetection, our method ensures depth invariance with a novel expression of the\n2D losses. Finally, to detect more challenging instances, our annotator follows\nan offline pseudo-labelling scheme which gradually improves its 3D\npseudo-labels. Extensive experiments on the KITTI dataset demonstrate that our\nmethod not only performs on-par or above previous works on the Car category,\nbut also achieves performance close to fully supervised methods on more\nchallenging classes. We further demonstrate the effectiveness and robustness of\nour method by being the first to experiment on the more challenging nuScenes\ndataset. We additionally propose a setting where weak labels are obtained from\na 2D detector pre-trained on MS-COCO instead of human annotations.\n', '  State-of-the-art models on contemporary 3D segmentation benchmarks like\nScanNet consume and label dataset-provided 3D point clouds, obtained through\npost processing of sensed multiview RGB-D images. They are typically trained\nin-domain, forego large-scale 2D pre-training and outperform alternatives that\nfeaturize the posed RGB-D multiview images instead. The gap in performance\nbetween methods that consume posed images versus post-processed 3D point clouds\nhas fueled the belief that 2D and 3D perception require distinct model\narchitectures. In this paper, we challenge this view and propose ODIN\n(Omni-Dimensional INstance segmentation), a model that can segment and label\nboth 2D RGB images and 3D point clouds, using a transformer architecture that\nalternates between 2D within-view and 3D cross-view information fusion. Our\nmodel differentiates 2D and 3D feature operations through the positional\nencodings of the tokens involved, which capture pixel coordinates for 2D patch\ntokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art\nperformance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation\nbenchmarks, and competitive performance on ScanNet, S3DIS and COCO. It\noutperforms all previous works by a wide margin when the sensed 3D point cloud\nis used in place of the point cloud sampled from 3D mesh. When used as the 3D\nperception engine in an instructable embodied agent architecture, it sets a new\nstate-of-the-art on the TEACh action-from-dialogue benchmark. Our code and\ncheckpoints can be found at the project website (https://odin-seg.github.io).\n', '  We present a Multimodal Interlaced Transformer (MIT) that jointly considers\n2D and 3D data for weakly supervised point cloud segmentation. Research studies\nhave shown that 2D and 3D features are complementary for point cloud\nsegmentation. However, existing methods require extra 2D annotations to achieve\n2D-3D information fusion. Considering the high annotation cost of point clouds,\neffective 2D and 3D feature fusion based on weakly supervised learning is in\ngreat demand. To this end, we propose a transformer model with two encoders and\none decoder for weakly supervised point cloud segmentation using only\nscene-level class tags. Specifically, the two encoders compute the\nself-attended features for 3D point clouds and 2D multi-view images,\nrespectively. The decoder implements interlaced 2D-3D cross-attention and\ncarries out implicit 2D and 3D feature fusion. We alternately switch the roles\nof queries and key-value pairs in the decoder layers. It turns out that the 2D\nand 3D features are iteratively enriched by each other. Experiments show that\nit performs favorably against existing weakly supervised point cloud\nsegmentation methods by a large margin on the S3DIS and ScanNet benchmarks. The\nproject page will be available at https://jimmy15923.github.io/mit_web/.\n']",3D Point Cloud Segmentation and Detection
243,242,36,242_speechcodes_speechx_voice_voicecraft,"['speechcodes', 'speechx', 'voice', 'voicecraft', 'voices', 'audio', 'speaker', 'speech', 'accents', 'accent']","['speech', 'speaker', 'voice', 'accent', 'shot', 'audio', 'synthesis', 'cloning', 'conversion', 'synthesized']","['speechcodes', 'speechx', 'voicecraft', 'accented', 'recordings', 'prosodic', 'timbre', 'synthesis', 'decoder', 'adaptation']","['  Conventional text-to-speech (TTS) research has predominantly focused on\nenhancing the quality of synthesized speech for speakers in the training\ndataset. The challenge of synthesizing lifelike speech for unseen,\nout-of-dataset speakers, especially those with limited reference data, remains\na significant and unresolved problem. While zero-shot or few-shot\nspeaker-adaptive TTS approaches have been explored, they have many limitations.\nZero-shot approaches tend to suffer from insufficient generalization\nperformance to reproduce the voice of speakers with heavy accents. While\nfew-shot methods can reproduce highly varying accents, they bring a significant\nstorage burden and the risk of overfitting and catastrophic forgetting. In\naddition, prior approaches only provide either zero-shot or few-shot\nadaptation, constraining their utility across varied real-world scenarios with\ndifferent demands. Besides, most current evaluations of speaker-adaptive TTS\nare conducted only on datasets of native speakers, inadvertently neglecting a\nvast portion of non-native speakers with diverse accents. Our proposed\nframework unifies both zero-shot and few-shot speaker adaptation strategies,\nwhich we term as ""instant"" and ""fine-grained"" adaptations based on their\nmerits. To alleviate the insufficient generalization performance observed in\nzero-shot speaker adaptation, we designed two innovative discriminators and\nintroduced a memory mechanism for the speech decoder. To prevent catastrophic\nforgetting and reduce storage implications for few-shot speaker adaptation, we\ndesigned two adapters and a unique adaptation procedure.\n', ""  Recently, zero-shot text-to-speech (TTS) systems, capable of synthesizing any\nspeaker's voice from a short audio prompt, have made rapid advancements.\nHowever, the quality of the generated speech significantly deteriorates when\nthe audio prompt contains noise, and limited research has been conducted to\naddress this issue. In this paper, we explored various strategies to enhance\nthe quality of audio generated from noisy audio prompts within the context of\nflow-matching-based zero-shot TTS. Our investigation includes comprehensive\ntraining strategies: unsupervised pre-training with masked speech denoising,\nmulti-speaker detection and DNSMOS-based data filtering on the pre-training\ndata, and fine-tuning with random noise mixing. The results of our experiments\ndemonstrate significant improvements in intelligibility, speaker similarity,\nand overall audio quality compared to the approach of applying speech\nenhancement to the audio prompt.\n"", '  The zero-shot text-to-speech (TTS) method, based on speaker embeddings\nextracted from reference speech using self-supervised learning (SSL) speech\nrepresentations, can reproduce speaker characteristics very accurately.\nHowever, this approach suffers from degradation in speech synthesis quality\nwhen the reference speech contains noise. In this paper, we propose a\nnoise-robust zero-shot TTS method. We incorporated adapters into the SSL model,\nwhich we fine-tuned with the TTS model using noisy reference speech. In\naddition, to further improve performance, we adopted a speech enhancement (SE)\nfront-end. With these improvements, our proposed SSL-based zero-shot TTS\nachieved high-quality speech synthesis with noisy reference speech. Through the\nobjective and subjective evaluations, we confirmed that the proposed method is\nhighly robust to noise in reference speech, and effectively works in\ncombination with SE.\n']",Text-to-Speech Synthesis and Voice Adaptation
244,243,36,243_semantic_parsing_sparql_knowledge,"['semantic', 'parsing', 'sparql', 'knowledge', 'answering', 'retrieval', 'parse', 'answerability', 'unanswerability', 'schemas']","['question', 'answering', 'questions', 'query', 'parsing', 'knowledge', 'logical', 'forms', 'schema', 'answerable']","['semantic', 'parsing', 'sparql', 'retrieval', 'unanswerability', 'schemas', 'wikidata', 'entity', 'rdf', 'kbs']","['  Knowledge Base Question Answering (KBQA) aims to answer natural language\nquestions over large-scale knowledge bases (KBs), which can be summarized into\ntwo crucial steps: knowledge retrieval and semantic parsing. However, three\ncore challenges remain: inefficient knowledge retrieval, mistakes of retrieval\nadversely impacting semantic parsing, and the complexity of previous KBQA\nmethods. To tackle these challenges, we introduce ChatKBQA, a novel and simple\ngenerate-then-retrieve KBQA framework, which proposes first generating the\nlogical form with fine-tuned LLMs, then retrieving and replacing entities and\nrelations with an unsupervised retrieval method, to improve both generation and\nretrieval more directly. Experimental results show that ChatKBQA achieves new\nstate-of-the-art performance on standard KBQA datasets, WebQSP, and CWQ. This\nwork can also be regarded as a new paradigm for combining LLMs with knowledge\ngraphs (KGs) for interpretable and knowledge-required question answering. Our\ncode is publicly available.\n', '  Existing Knowledge Base Question Answering (KBQA) architectures are hungry\nfor annotated data, which make them costly and time-consuming to deploy. We\nintroduce the problem of few-shot transfer learning for KBQA, where the target\ndomain offers only a few labeled examples, but a large labeled training dataset\nis available in a source domain. We propose a novel KBQA architecture called\nFuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers,\nre-ranks using an LLM and uses this as input for LLM few-shot in-context\nlearning to generate logical forms. These are further refined using\nexecution-guided feedback. Experiments over multiple source-target KBQA pairs\nof varying complexity show that FuSIC-KBQA significantly outperforms\nadaptations of SoTA KBQA models for this setting. Additional experiments show\nthat FuSIC-KBQA also outperforms SoTA KBQA models in the in-domain setting when\ntraining data is limited.\n', '  Recent work integrating Large Language Models (LLMs) has led to significant\nimprovements in the Knowledge Base Question Answering (KBQA) task. However, we\nposit that existing KBQA datasets that either have simple questions, use\nsynthetically generated logical forms, or are based on small knowledge base\n(KB) schemas, do not capture the true complexity of KBQA tasks.\n  To address this, we introduce the SPINACH dataset, an expert-annotated KBQA\ndataset collected from forum discussions on Wikidata\'s ""Request a Query"" forum\nwith 320 decontextualized question-SPARQL pairs. Much more complex than\nexisting datasets, SPINACH calls for strong KBQA systems that do not rely on\ntraining data to learn the KB schema, but can dynamically explore large and\noften incomplete schemas and reason about them.\n  Along with the dataset, we introduce the SPINACH agent, a new KBQA approach\nthat mimics how a human expert would write SPARQLs for such challenging\nquestions. Experiments on existing datasets show SPINACH\'s capability in KBQA,\nachieving a new state of the art on the QALD-7, QALD-9 Plus and QALD-10\ndatasets by 30.1%, 27.0%, and 10.0% in F1, respectively, and coming within 1.6%\nof the fine-tuned LLaMA SOTA model on WikiWebQuestions. On our new SPINACH\ndataset, SPINACH agent outperforms all baselines, including the best\nGPT-4-based KBQA agent, by 38.1% in F1.\n']",Knowledge Base Question Answering with Semantic Parsing
245,244,36,244_designs_generative_artistic_design,"['designs', 'generative', 'artistic', 'design', 'layouts', 'aesthetics', 'architectural', 'layout', 'artworks', 'facades']","['design', 'designers', 'architectural', 'designs', 'artistic', 'typography', 'layout', 'aesthetic', 'generative', 'layouts']","['designs', 'generative', 'artworks', 'creativity', 'stylization', 'facade', 'architects', 'aesthetically', 'imagery', 'typographers']","['  Generative Artificial Intelligence (AI) has pioneered new methodological\nparadigms in architectural design, significantly expanding the innovative\npotential and efficiency of the design process. This paper explores the\nextensive applications of generative AI technologies in architectural design, a\ntrend that has benefited from the rapid development of deep generative models.\nThis article provides a comprehensive review of the basic principles of\ngenerative AI and large-scale models and highlights the applications in the\ngeneration of 2D images, videos, and 3D models. In addition, by reviewing the\nlatest literature from 2020, this paper scrutinizes the impact of generative AI\ntechnologies at different stages of architectural design, from generating\ninitial architectural 3D forms to producing final architectural imagery. The\nmarked trend of research growth indicates an increasing inclination within the\narchitectural design community towards embracing generative AI, thereby\ncatalyzing a shared enthusiasm for research. These research cases and\nmethodologies have not only proven to enhance efficiency and innovation\nsignificantly but have also posed challenges to the conventional boundaries of\narchitectural creativity. Finally, we point out new directions for design\ninnovation and articulate fresh trajectories for applying generative AI in the\narchitectural domain. This article provides the first comprehensive literature\nreview about generative AI for architectural design, and we believe this work\ncan facilitate more research work on this significant topic in architecture.\n', ""  Text-to-image generative models have increasingly been used to assist\ndesigners during concept generation in various creative domains, such as\ngraphic design, user interface design, and fashion design. However, their\napplications in engineering design remain limited due to the models' challenges\nin generating images of feasible designs concepts. To address this issue, this\npaper introduces a method that improves the design feasibility by prompting the\ngeneration with feasible CAD images. In this work, the usefulness of this\nmethod is investigated through a case study with a bike design task using an\noff-the-shelf text-to-image model, Stable Diffusion 2.1. A diverse set of bike\ndesigns are produced in seven different generation settings with varying CAD\nimage prompting weights, and these designs are evaluated on their perceived\nfeasibility and novelty. Results demonstrate that the CAD image prompting\nsuccessfully helps text-to-image models like Stable Diffusion 2.1 create\nvisibly more feasible design images. While a general tradeoff is observed\nbetween feasibility and novelty, when the prompting weight is kept low around\n0.35, the design feasibility is significantly improved while its novelty\nremains on par with those generated by text prompts alone. The insights from\nthis case study offer some guidelines for selecting the appropriate CAD image\nprompting weight for different stages of the engineering design process. When\nutilized effectively, our CAD image prompting method opens doors to a wider\nrange of applications of text-to-image models in engineering design.\n"", '  This paper introduces the WordArt Designer API, a novel framework for\nuser-driven artistic typography synthesis utilizing Large Language Models\n(LLMs) on ModelScope. We address the challenge of simplifying artistic\ntypography for non-professionals by offering a dynamic, adaptive, and\ncomputationally efficient alternative to traditional rigid templates. Our\napproach leverages the power of LLMs to understand and interpret user input,\nfacilitating a more intuitive design process. We demonstrate through various\ncase studies how users can articulate their aesthetic preferences and\nfunctional requirements, which the system then translates into unique and\ncreative typographic designs. Our evaluations indicate significant improvements\nin user satisfaction, design flexibility, and creative expression over existing\nsystems. The WordArt Designer API not only democratizes the art of typography\nbut also opens up new possibilities for personalized digital communication and\ndesign.\n']",Generative Design and Artistic Applications
246,245,35,245_reinforcement_sepsis_interventions_adaptive,"['reinforcement', 'sepsis', 'interventions', 'adaptive', 'medication', 'cpr', 'medications', 'policies', 'outcomes', 'prescribing']","['sepsis', 'treatment', 'health', 'interventions', 'patient', 'care', 'healthcare', 'medication', 'patients', 'dosage']","['sepsis', 'interventions', 'cpr', 'prescribing', 'intensive', 'medt', 'dosage', 'personalized', 'survival', 'simulation']","['  Reinforcement learning (RL) has garnered increasing recognition for its\npotential to optimise dynamic treatment regimes (DTRs) in personalised\nmedicine, particularly for drug dosage prescriptions and medication\nrecommendations. However, a significant challenge persists: the absence of a\nunified framework for simulating diverse healthcare scenarios and a\ncomprehensive analysis to benchmark the effectiveness of RL algorithms within\nthese contexts. To address this gap, we introduce \\textit{DTR-Bench}, a\nbenchmarking platform comprising four distinct simulation environments tailored\nto common DTR applications, including cancer chemotherapy, radiotherapy,\nglucose management in diabetes, and sepsis treatment. We evaluate various\nstate-of-the-art RL algorithms across these settings, particularly highlighting\ntheir performance amidst real-world challenges such as\npharmacokinetic/pharmacodynamic (PK/PD) variability, noise, and missing data.\nOur experiments reveal varying degrees of performance degradation among RL\nalgorithms in the presence of noise and patient variability, with some\nalgorithms failing to converge. Additionally, we observe that using temporal\nobservation representations does not consistently lead to improved performance\nin DTR settings. Our findings underscore the necessity of developing robust,\nadaptive RL algorithms capable of effectively managing these complexities to\nenhance patient-specific healthcare. We have open-sourced our benchmark and\ncode at https://github.com/GilesLuo/DTR-Bench.\n', ""  Offline reinforcement learning has shown promise for solving tasks in\nsafety-critical settings, such as clinical decision support. Its application,\nhowever, has been limited by the lack of interpretability and interactivity for\nclinicians. To address these challenges, we propose the medical decision\ntransformer (MeDT), a novel and versatile framework based on the\ngoal-conditioned reinforcement learning paradigm for sepsis treatment\nrecommendation. MeDT uses the decision transformer architecture to learn a\npolicy for drug dosage recommendation. During offline training, MeDT utilizes\ncollected treatment trajectories to predict administered treatments for each\ntime step, incorporating known treatment outcomes, target acuity scores, past\ntreatment decisions, and current and past medical states. This analysis enables\nMeDT to capture complex dependencies among a patient's medical history,\ntreatment decisions, outcomes, and short-term effects on stability. Our\nproposed conditioning uses acuity scores to address sparse reward issues and to\nfacilitate clinician-model interactions, enhancing decision-making. Following\ntraining, MeDT can generate tailored treatment recommendations by conditioning\non the desired positive outcome (survival) and user-specified short-term\nstability improvements. We carry out rigorous experiments on data from the\nMIMIC-III dataset and use off-policy evaluation to demonstrate that MeDT\nrecommends interventions that outperform or are competitive with existing\noffline reinforcement learning methods while enabling a more interpretable,\npersonalized and clinician-directed approach.\n"", '  We present ICU-Sepsis, an environment that can be used in benchmarks for\nevaluating reinforcement learning (RL) algorithms. Sepsis management is a\ncomplex task that has been an important topic in applied RL research in recent\nyears. Therefore, MDPs that model sepsis management can serve as part of a\nbenchmark to evaluate RL algorithms on a challenging real-world problem.\nHowever, creating usable MDPs that simulate sepsis care in the ICU remains a\nchallenge due to the complexities involved in acquiring and processing patient\ndata. ICU-Sepsis is a lightweight environment that models personalized care of\nsepsis patients in the ICU. The environment is a tabular MDP that is widely\ncompatible and is challenging even for state-of-the-art RL algorithms, making\nit a valuable tool for benchmarking their performance. However, we emphasize\nthat while ICU-Sepsis provides a standardized environment for evaluating RL\nalgorithms, it should not be used to draw conclusions that guide medical\npractice.\n']",Reinforcement Learning for Sepsis Treatment
247,246,35,246_lexical_semantic_lexicon_corpus,"['lexical', 'semantic', 'lexicon', 'corpus', 'wordnet', 'linguistics', 'contextualized', 'contextualised', 'corpora', 'words']","['senses', 'word', 'change', 'sense', 'semantic', 'words', 'lexical', 'usages', 'meaning', 'changes']","['lexicon', 'wordnet', 'contextualized', 'corpora', 'disambiguating', 'changes', 'polysemy', 'metonymy', 'homonyms', 'judgments']","['  Word meanings change over time, and word senses evolve, emerge or die out in\nthe process. For ancient languages, where the corpora are often small and\nsparse, modelling such changes accurately proves challenging, and quantifying\nuncertainty in sense-change estimates consequently becomes important. GASC\n(Genre-Aware Semantic Change) and DiSC (Diachronic Sense Change) are existing\ngenerative models that have been used to analyse sense change for target words\nfrom an ancient Greek text corpus, using unsupervised learning without the help\nof any pre-training. These models represent the senses of a given target word\nsuch as ""kosmos"" (meaning decoration, order or world) as distributions over\ncontext words, and sense prevalence as a distribution over senses. The models\nare fitted using Markov Chain Monte Carlo (MCMC) methods to measure temporal\nchanges in these representations. This paper introduces EDiSC, an Embedded DiSC\nmodel, which combines word embeddings with DiSC to provide superior model\nperformance. It is shown empirically that EDiSC offers improved predictive\naccuracy, ground-truth recovery and uncertainty quantification, as well as\nbetter sampling efficiency and scalability properties with MCMC methods. The\nchallenges of fitting these models are also discussed.\n', '  Despite the predominance of contextualized embeddings in NLP, approaches to\ndetect semantic change relying on these embeddings and clustering methods\nunderperform simpler counterparts based on static word embeddings. This stems\nfrom the poor quality of the clustering methods to produce sense clusters --\nwhich struggle to capture word senses, especially those with low frequency.\nThis issue hinders the next step in examining how changes in word senses in one\nlanguage influence another. To address this issue, we propose a graph-based\nclustering approach to capture nuanced changes in both high- and low-frequency\nword senses across time and languages, including the acquisition and loss of\nthese senses over time. Our experimental results show that our approach\nsubstantially surpasses previous approaches in the SemEval2020 binary\nclassification task across four languages. Moreover, we showcase the ability of\nour approach as a versatile visualization tool to detect semantic changes in\nboth intra-language and inter-language setups. We make our code and data\npublicly available.\n', ""  We use contextualized word definitions generated by large language models as\nsemantic representations in the task of diachronic lexical semantic change\ndetection (LSCD). In short, generated definitions are used as `senses', and the\nchange score of a target word is retrieved by comparing their distributions in\ntwo time periods under comparison. On the material of five datasets and three\nlanguages, we show that generated definitions are indeed specific and general\nenough to convey a signal sufficient to rank sets of words by the degree of\ntheir semantic change over time. Our approach is on par with or outperforms\nprior non-supervised sense-based LSCD methods. At the same time, it preserves\ninterpretability and allows to inspect the reasons behind a specific shift in\nterms of discrete definitions-as-senses. This is another step in the direction\nof explainable semantic change modeling.\n""]","""Lexical Semantic Change Detection"""
248,247,35,247_tools_toolkits_toollens_tooling,"['tools', 'toolkits', 'toollens', 'tooling', 'toolsets', 'tool', 'toolflow', 'toolnet', 'toolkit', 'toolbench']","['tool', 'tools', 'usage', 'calling', 'planning', 'retrieval', 'toolset', 'external', 'queries', 'scenarios']","['toolkits', 'toollens', 'toolflow', 'toolname', 'tasks', 'apis', 'snippets', 'developers', 'documentation', 'rotbench']","['  Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. In general, we hope this paper could inspire\nfuture research in integrating tools with foundation models.\n', '  Large language models (LLMs) have garnered significant attention due to their\nimpressive natural language processing (NLP) capabilities. Recently, many\nstudies have focused on the tool utilization ability of LLMs. They primarily\ninvestigated how LLMs effectively collaborate with given specific tools.\nHowever, in scenarios where LLMs serve as intelligent agents, as seen in\napplications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate\ndecision-making processes that involve deciding whether to employ a tool and\nselecting the most suitable tool(s) from a collection of available tools to\nfulfill user requests. Therefore, in this paper, we introduce MetaTool, a\nbenchmark designed to evaluate whether LLMs have tool usage awareness and can\ncorrectly choose tools. Specifically, we create a dataset called ToolE within\nthe benchmark. This dataset contains various types of user queries in the form\nof prompts that trigger LLMs to use tools, including both single-tool and\nmulti-tool scenarios. Subsequently, we set the tasks for both tool usage\nawareness and tool selection. We define four subtasks from different\nperspectives in tool selection, including tool selection with similar choices,\ntool selection in specific scenarios, tool selection with possible reliability\nissues, and multi-tool selection. We conduct experiments involving eight\npopular LLMs and find that the majority of them still struggle to effectively\nselect tools, highlighting the existing gaps between LLMs and genuine\nintelligent agents. However, through the error analysis, we found there is\nstill significant room for improvement. Finally, we conclude with insights for\ntool developers -- we strongly recommend that tool developers choose an\nappropriate rewrite model for generating new descriptions based on the\ndownstream LLM the tool will apply to. Our code is in\nhttps://github.com/HowieHwong/MetaTool.\n', ""  Tool learning aims to enhance and expand large language models' (LLMs)\ncapabilities with external tools, which has gained significant attention\nrecently. Current methods have shown that LLMs can effectively handle a certain\namount of tools through in-context learning or fine-tuning. However, in\nreal-world scenarios, the number of tools is typically extensive and\nirregularly updated, emphasizing the necessity for a dedicated tool retrieval\ncomponent. Tool retrieval is nontrivial due to the following challenges: 1)\ncomplex user instructions and tool descriptions; 2) misalignment between tool\nretrieval and tool usage models. To address the above issues, we propose to\nenhance tool retrieval with iterative feedback from the large language model.\nSpecifically, we prompt the tool usage model, i.e., the LLM, to provide\nfeedback for the tool retriever model in multi-round, which could progressively\nimprove the tool retriever's understanding of instructions and tools and reduce\nthe gap between the two standalone components. We build a unified and\ncomprehensive benchmark to evaluate tool retrieval models. The extensive\nexperiments indicate that our proposed approach achieves advanced performance\nin both in-domain evaluation and out-of-domain evaluation.\n""]",Tool Learning and Utilization in Artificial Intelligence
249,248,35,248_phishing_cybercrime_spam_cybersecurity,"['phishing', 'cybercrime', 'spam', 'cybersecurity', 'emails', 'malicious', 'phisnet', 'threats', 'phishlang', 'security']","['phishing', 'spam', 'emails', 'email', 'cyber', 'messages', 'websites', 'cybersecurity', 'detection', 'threat']","['phishing', 'cybersecurity', 'emails', 'phishlang', 'idf', 'spamdam', 'webpages', 'cssda', 'knowphish', 'bayes']","['  Phishing email attacks are among the most common and most harmful\ncybersecurity attacks. With the emergence of generative AI, phishing attacks\ncan be based on emails generated automatically, making it more difficult to\ndetect them. That is, instead of a single email format sent to a large number\nof recipients, generative AI can be used to send each potential victim a\ndifferent email, making it more difficult for cybersecurity systems to identify\nthe scam email before it reaches the recipient. Here we describe a corpus of\nAI-generated phishing emails. We also use different machine learning tools to\ntest the ability of automatic text analysis to identify AI-generated phishing\nemails. The results are encouraging, and show that machine learning tools can\nidentify an AI-generated phishing email with high accuracy compared to regular\nemails or human-generated scam email. By applying descriptive analytic, the\nspecific differences between AI-generated emails and manually crafted scam\nemails are profiled, and show that AI-generated emails are different in their\nstyle from human-generated phishing email scams. Therefore, automatic\nidentification tools can be used as a warning for the user. The paper also\ndescribes the corpus of AI-generated phishing emails that is made open to the\npublic, and can be used for consequent studies. While the ability of machine\nlearning to detect AI-generated phishing email is encouraging, AI-generated\nphishing emails are different from regular phishing emails, and therefore it is\nimportant to train machine learning systems also with AI-generated emails in\norder to repel future phishing attacks that are powered by generative AI.\n', '  Phishing email is a serious cyber threat that tries to deceive users by\nsending false emails with the intention of stealing confidential information or\ncausing financial harm. Attackers, often posing as trustworthy entities,\nexploit technological advancements and sophistication to make detection and\nprevention of phishing more challenging. Despite extensive academic research,\nphishing detection remains an ongoing and formidable challenge in the\ncybersecurity landscape. Large Language Models (LLMs) and Masked Language\nModels (MLMs) possess immense potential to offer innovative solutions to\naddress long-standing challenges. In this research paper, we present an\noptimized, fine-tuned transformer-based DistilBERT model designed for the\ndetection of phishing emails. In the detection process, we work with a phishing\nemail dataset and utilize the preprocessing techniques to clean and solve the\nimbalance class issues. Through our experiments, we found that our model\neffectively achieves high accuracy, demonstrating its capability to perform\nwell. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI)\ntechniques such as Local Interpretable Model-Agnostic Explanations (LIME) and\nTransformer Interpret to explain how our model makes predictions in the context\nof text classification for phishing emails.\n', ""  The critical threat of phishing emails has been further exacerbated by the\npotential of LLMs to generate highly targeted, personalized, and automated\nspear phishing attacks. Two critical problems concerning LLM-facilitated\nphishing require further investigation: 1) Existing studies on lateral phishing\nlack specific examination of LLM integration for large-scale attacks targeting\nthe entire organization, and 2) Current anti-phishing infrastructure, despite\nits extensive development, lacks the capability to prevent LLM-generated\nattacks, potentially impacting both employees and IT security incident\nmanagement. However, the execution of such investigative studies necessitates a\nreal-world environment, one that functions during regular business operations\nand mirrors the complexity of a large organizational infrastructure. This\nsetting must also offer the flexibility required to facilitate a diverse array\nof experimental conditions, particularly the incorporation of phishing emails\ncrafted by LLMs. This study is a pioneering exploration into the use of Large\nLanguage Models (LLMs) for the creation of targeted lateral phishing emails,\ntargeting a large tier 1 university's operation and workforce of approximately\n9,000 individuals over an 11-month period. It also evaluates the capability of\nemail filtering infrastructure to detect such LLM-generated phishing attempts,\nproviding insights into their effectiveness and identifying potential areas for\nimprovement. Based on our findings, we propose machine learning-based detection\ntechniques for such emails to detect LLM-generated phishing emails that were\nmissed by the existing infrastructure, with an F1-score of 98.96.\n""]",Phishing and Cybersecurity Threats
250,249,35,249_emissions_energy_carbon_efficiency,"['emissions', 'energy', 'carbon', 'efficiency', 'co2', 'efficient', 'sustainable', 'environmentally', 'models', 'ai']","['energy', 'carbon', 'consumption', 'emissions', 'green', 'footprint', 'sustainable', 'environmental', 'sustainability', 'hardware']","['emissions', 'efficiency', 'ai', 'footprint', 'watt', 'cpu', 'dsnns', 'iot', 'architectures', 'datacenters']","['  DNN inference, known for its significant energy consumption and the resulting\nhigh carbon footprint, can be made more sustainable by adapting model size and\naccuracy to the varying carbon intensity throughout the day. Our heuristic\nalgorithm uses larger, high-accuracy models during low-intensity periods and\nsmaller, lower-accuracy ones during high-intensity periods. We also introduce a\nmetric, carbon-emission efficiency, which quantitatively measures the efficacy\nof adaptive model selection in terms of carbon footprint. The evaluation showed\nthat the proposed approach could improve the carbon emission efficiency in\nimproving the accuracy of vision recognition services by up to 80%.\n', ""  With the increasing usage, scale, and complexity of Deep Learning (DL)\nmodels, their rapidly growing energy consumption has become a critical concern.\nPromoting green development and energy awareness at different granularities is\nthe need of the hour to limit carbon emissions of DL systems. However, the lack\nof standard and repeatable tools to accurately measure and optimize energy\nconsumption at a fine granularity (e.g., at method level) hinders progress in\nthis area. This paper introduces FECoM (Fine-grained Energy Consumption Meter),\na framework for fine-grained DL energy consumption measurement. FECoM enables\nresearchers and developers to profile DL APIs from energy perspective. FECoM\naddresses the challenges of measuring energy consumption at fine-grained level\nby using static instrumentation and considering various factors, including\ncomputational load and temperature stability. We assess FECoM's capability to\nmeasure fine-grained energy consumption for one of the most popular open-source\nDL frameworks, namely TensorFlow. Using FECoM, we also investigate the impact\nof parameter size and execution time on energy consumption, enriching our\nunderstanding of TensorFlow APIs' energy profiles. Furthermore, we elaborate on\nthe considerations, issues, and challenges that one needs to consider while\ndesigning and implementing a fine-grained energy consumption measurement tool.\nThis work will facilitate further advances in DL energy measurement and the\ndevelopment of energy-aware practices for DL systems.\n"", '  Machine learning (ML) has seen tremendous advancements, but its environmental\nfootprint remains a concern. Acknowledging the growing environmental impact of\nML this paper investigates Green ML, examining various model architectures and\nhyperparameters in both training and inference phases to identify\nenergy-efficient practices. Our study leverages software-based power\nmeasurements for ease of replication across diverse configurations, models and\ndatasets. In this paper, we examine multiple models and hardware configurations\nto identify correlations across the various measurements and metrics and key\ncontributors to energy reduction. Our analysis offers practical guidelines for\nconstructing sustainable ML operations, emphasising energy consumption and\ncarbon footprint reductions while maintaining performance. As identified,\nshort-lived profiling can quantify the long-term expected energy consumption.\nMoreover, model parameters can also be used to accurately estimate the expected\ntotal energy without the need for extensive experimentation.\n']",Reducing Carbon Footprint in AI and Machine Learning
251,250,35,250_augmentation_imagenet_learning_transferability,"['augmentation', 'imagenet', 'learning', 'transferability', 'trained', 'classification', 'classifier', 'mixup', 'datasets', 'layers']","['mixup', 'transfer', 'transferability', 'source', 'target', 'transferable', 'generalization', 'domains', 'domain', 'task']","['augmentation', 'imagenet', 'trained', 'layers', 'features', 'mixda', 'activations', 'transferring', 'adatrans', 'downstream']","['  We study the problem of robust data augmentation for regression tasks in the\npresence of noisy data. Data augmentation is essential for generalizing deep\nlearning models, but most of the techniques like the popular Mixup are\nprimarily designed for classification tasks on image data. Recently, there are\nalso Mixup techniques that are specialized to regression tasks like C-Mixup. In\ncomparison to Mixup, which takes linear interpolations of pairs of samples,\nC-Mixup is more selective in which samples to mix based on their label\ndistances for better regression performance. However, C-Mixup does not\ndistinguish noisy versus clean samples, which can be problematic when mixing\nand lead to suboptimal model performance. At the same time, robust training has\nbeen heavily studied where the goal is to train accurate models against noisy\ndata through multiple rounds of model training. We thus propose our data\naugmentation strategy RC-Mixup, which tightly integrates C-Mixup with\nmulti-round robust training methods for a synergistic effect. In particular,\nC-Mixup improves robust training in identifying clean data, while robust\ntraining provides cleaner data to C-Mixup for it to perform better. A key\nadvantage of RC-Mixup is that it is data-centric where the robust model\ntraining algorithm itself does not need to be modified, but can simply benefit\nfrom data mixing. We show in our experiments that RC-Mixup significantly\noutperforms C-Mixup and robust training baselines on noisy data benchmarks and\ncan be integrated with various robust training methods.\n', '  We propose two novel transferability metrics F-OTCE (Fast Optimal Transport\nbased Conditional Entropy) and JC-OTCE (Joint Correspondence OTCE) to evaluate\nhow much the source model (task) can benefit the learning of the target task\nand to learn more transferable representations for cross-domain cross-task\ntransfer learning. Unlike the existing metric that requires evaluating the\nempirical transferability on auxiliary tasks, our metrics are auxiliary-free\nsuch that they can be computed much more efficiently. Specifically, F-OTCE\nestimates transferability by first solving an Optimal Transport (OT) problem\nbetween source and target distributions, and then uses the optimal coupling to\ncompute the Negative Conditional Entropy between source and target labels. It\ncan also serve as a loss function to maximize the transferability of the source\nmodel before finetuning on the target task. Meanwhile, JC-OTCE improves the\ntransferability robustness of F-OTCE by including label distances in the OT\nproblem, though it may incur additional computation cost. Extensive experiments\ndemonstrate that F-OTCE and JC-OTCE outperform state-of-the-art auxiliary-free\nmetrics by 18.85% and 28.88%, respectively in correlation coefficient with the\nground-truth transfer accuracy. By eliminating the training cost of auxiliary\ntasks, the two metrics reduces the total computation time of the previous\nmethod from 43 minutes to 9.32s and 10.78s, respectively, for a pair of tasks.\nWhen used as a loss function, F-OTCE shows consistent improvements on the\ntransfer accuracy of the source model in few-shot classification experiments,\nwith up to 4.41% accuracy gain.\n', '  Transferability estimation is an essential problem in transfer learning to\npredict how good the performance is when transferring a source model (or source\ntask) to a target task. Recent analytical transferability metrics have been\nwidely used for source model selection and multi-task learning. A major\nchallenge is how to make transfereability estimation robust under the\ncross-domain cross-task settings. The recently proposed OTCE score solves this\nproblem by considering both domain and task differences, with the help of\ntransfer experiences on auxiliary tasks, which causes an efficiency overhead.\nIn this work, we propose a practical transferability metric called JC-NCE score\nthat dramatically improves the robustness of the task difference estimation in\nOTCE, thus removing the need for auxiliary tasks. Specifically, we build the\njoint correspondences between source and target data via solving an optimal\ntransport problem with a ground cost considering both the sample distance and\nlabel distance, and then compute the transferability score as the negative\nconditional entropy of the matched labels. Extensive validations under the\nintra-dataset and inter-dataset transfer settings demonstrate that our JC-NCE\nscore outperforms the auxiliary-task free version of OTCE for 7% and 12%,\nrespectively, and is also more robust than other existing transferability\nmetrics on average.\n']",Deep Learning Data Augmentation and Transferability
252,251,35,251_stances_stance_sentiment_semantics,"['stances', 'stance', 'sentiment', 'semantics', 'annotated', 'tweets', 'bias', 'debiasing', 'topics', 'twitter']","['stance', 'detection', 'stances', 'topics', 'media', 'target', 'social', 'topic', 'viewpoint', 'shot']","['stances', 'sentiment', 'annotated', 'tweets', 'debiasing', 'topics', 'attitudes', 'rumour', 'multifaceted', 'political']","[""  Social media platforms are rich sources of opinionated content. Stance\ndetection allows the automatic extraction of users' opinions on various topics\nfrom such content. We focus on zero-shot stance detection, where the model's\nsuccess relies on (a) having knowledge about the target topic; and (b) learning\ngeneral reasoning strategies that can be employed for new topics. We present\nStance Reasoner, an approach to zero-shot stance detection on social media that\nleverages explicit reasoning over background knowledge to guide the model's\ninference about the document's stance on a target. Specifically, our method\nuses a pre-trained language model as a source of world knowledge, with the\nchain-of-thought in-context learning approach to generate intermediate\nreasoning steps. Stance Reasoner outperforms the current state-of-the-art\nmodels on 3 Twitter datasets, including fully supervised models. It can better\ngeneralize across targets, while at the same time providing explicit and\ninterpretable explanations for its predictions.\n"", ""  Stance detection classifies stance relations (namely, Favor, Against, or\nNeither) between comments and targets. Pretrained language models (PLMs) are\nwidely used to mine the stance relation to improve the performance of stance\ndetection through pretrained knowledge. However, PLMs also embed ``bad''\npretrained knowledge concerning stance into the extracted stance relation\nsemantics, resulting in pretrained stance bias. It is not trivial to measure\npretrained stance bias due to its weak quantifiability. In this paper, we\npropose Relative Counterfactual Contrastive Learning (RCCL), in which\npretrained stance bias is mitigated as relative stance bias instead of absolute\nstance bias to overtake the difficulty of measuring bias. Firstly, we present a\nnew structural causal model for characterizing complicated relationships among\ncontext, PLMs and stance relations to locate pretrained stance bias. Then,\nbased on masked language model prediction, we present a target-aware relative\nstance sample generation method for obtaining relative bias. Finally, we use\ncontrastive learning based on counterfactual theory to mitigate pretrained\nstance bias and preserve context stance relation. Experiments show that the\nproposed method is superior to stance detection and debiasing baselines.\n"", '  Stance detection is the view towards a specific target by a given context\n(\\textit{e.g.} tweets, commercial reviews). Target-related knowledge is often\nneeded to assist stance detection models in understanding the target well and\nmaking detection correctly. However, prevailing works for knowledge-infused\nstance detection predominantly incorporate target knowledge from a singular\nsource that lacks knowledge verification in limited domain knowledge. The\nlow-resource training data further increases the challenge for the data-driven\nlarge models in this task. To address those challenges, we propose a\ncollaborative knowledge infusion approach for low-resource stance detection\ntasks, employing a combination of aligned knowledge enhancement and efficient\nparameter learning techniques. Specifically, our stance detection approach\nleverages target background knowledge collaboratively from different knowledge\nsources with the help of knowledge alignment. Additionally, we also introduce\nthe parameter-efficient collaborative adaptor with a staged optimization\nalgorithm, which collaboratively addresses the challenges associated with\nlow-resource stance detection tasks from both network structure and learning\nperspectives. To assess the effectiveness of our method, we conduct extensive\nexperiments on three public stance detection datasets, including low-resource\nand cross-target settings. The results demonstrate significant performance\nimprovements compared to the existing stance detection approaches.\n']",Stance Detection and Sentiment Analysis on Social Media
253,252,34,252_transcription_speech_voice_multilingual,"['transcription', 'speech', 'voice', 'multilingual', 'decoder', 'speechalign', 'translating', 'decoding', 'translation', 'audio']","['speech', 'translation', 'simultaneous', 'cascade', 'end', 'latency', 'streaming', 'transducer', 'decoding', 'parallel']","['transcription', 'voice', 'multilingual', 'decoder', 'speechalign', 'textless', 's2st', 'ast', 'transducer', 'streamspeech']","['  Simultaneous speech-to-speech translation (Simul-S2ST, a.k.a streaming speech\ntranslation) outputs target speech while receiving streaming speech inputs,\nwhich is critical for real-time communication. Beyond accomplishing translation\nbetween speech, Simul-S2ST requires a policy to control the model to generate\ncorresponding target speech at the opportune moment within speech inputs,\nthereby posing a double challenge of translation and policy. In this paper, we\npropose StreamSpeech, a direct Simul-S2ST model that jointly learns translation\nand simultaneous policy in a unified framework of multi-task learning. Adhering\nto a multi-task learning approach, StreamSpeech can perform offline and\nsimultaneous speech recognition, speech translation and speech synthesis via an\n""All-in-One"" seamless model. Experiments on CVSS benchmark demonstrate that\nStreamSpeech achieves state-of-the-art performance in both offline S2ST and\nSimul-S2ST tasks. Besides, StreamSpeech is able to present high-quality\nintermediate results (i.e., ASR or translation results) during simultaneous\ntranslation process, offering a more comprehensive real-time communication\nexperience.\n', '  Existing speech-to-speech translation models fall into two camps: textless\nmodels trained with hundreds of hours of parallel speech data or unsupervised\nmodels that leverage text as an intermediate step. Both approaches limit\nbuilding speech-to-speech translation models for a wide range of languages, as\nthey exclude languages that are primarily spoken and language pairs that lack\nlarge-scale parallel speech data. We present a new framework for training\ntextless low-resource speech-to-speech translation (S2ST) systems that only\nneed dozens of hours of parallel speech data. We reformulate S2ST as a\nunit-to-unit seq2seq translation task, and start by pretraining a model on\nlarge-scale monolingual speech data. Then, we finetune it with a small amount\nof parallel speech data ($20-60$ hours). Lastly, we improve model performance\nthrough an unsupervised backtranslation objective. We train and evaluate our\nmodels for English-to-German, German-to-English and Marathi-to-English\ntranslation on three different domains (European Parliament, Common Voice, and\nAll India Radio) with single-speaker synthesized speech data. Evaluated using\nthe ASR-BLEU metric, our models achieve reasonable performance on all three\ndomains, with some being within 1-2 points of our supervised topline.\n', '  Speech segmentation is an essential part of speech translation (ST) systems\nin real-world scenarios. Since most ST models are designed to process speech\nsegments, long-form audio must be partitioned into shorter segments before\ntranslation. Recently, data-driven approaches for the speech segmentation task\nhave been developed. Although the approaches improve overall translation\nquality, a performance gap exists due to a mismatch between the models and ST\nsystems. In addition, the prior works require large self-supervised speech\nmodels, which consume significant computational resources. In this work, we\npropose a segmentation model that achieves better speech translation quality\nwith a small model size. We propose an ASR-with-punctuation task as an\neffective pre-training strategy for the segmentation model. We also show that\nproper integration of the speech segmentation model into the underlying ST\nsystem is critical to improve overall translation quality at inference time.\n']",Speech-to-Speech Translation
254,253,34,253_spin_lattice_isingnets_lattices,"['spin', 'lattice', 'isingnets', 'lattices', 'renormalization', 'ferromagnetic', 'hamiltonians', 'magnetization', 'networks', 'neural']","['spin', 'lattice', 'gauge', 'glass', 'transition', 'temperature', 'quantum', 'phase', 'symmetry', 'ground']","['lattice', 'isingnets', 'renormalization', 'ferromagnetic', 'hamiltonians', 'phases', 'annealing', 'antiskyrmions', 'gauge', 'symmetry']","['  We propose an RNN-based efficient Ising model solver, the Criticality-ordered\nRecurrent Mean Field (CoRMF), for forward Ising problems. In its core, a\ncriticality-ordered spin sequence of an $N$-spin Ising model is introduced by\nsorting mission-critical edges with greedy algorithm, such that an\nautoregressive mean-field factorization can be utilized and optimized with\nRecurrent Neural Networks (RNNs). Our method has two notable characteristics:\n(i) by leveraging the approximated tree structure of the underlying Ising\ngraph, the newly-obtained criticality order enables the unification between\nvariational mean-field and RNN, allowing the generally intractable Ising model\nto be efficiently probed with probabilistic inference; (ii) it is\nwell-modulized, model-independent while at the same time expressive enough, and\nhence fully applicable to any forward Ising inference problems with minimal\neffort. Computationally, by using a variance-reduced Monte Carlo gradient\nestimator, CoRFM solves the Ising problems in a self-train fashion without\ndata/evidence, and the inference tasks can be executed by directly sampling\nfrom RNN. Theoretically, we establish a provably tighter error bound than naive\nmean-field by using the matrix cut decomposition machineries. Numerically, we\ndemonstrate the utility of this framework on a series of Ising datasets.\n', ""  We explore a one-to-one correspondence between a neural network (NN) and a\nstatistical mechanical spin model where neurons are mapped to Ising spins and\nweights to spin-spin couplings. The process of training an NN produces a family\nof spin Hamiltonians parameterized by training time. We study the magnetic\nphases and the melting transition temperature as training progresses. First, we\nprove analytically that the common initial state before training--an NN with\nindependent random weights--maps to a layered version of the classical\nSherrington-Kirkpatrick spin glass exhibiting a replica symmetry breaking. The\nspin-glass-to-paramagnet transition temperature is calculated. Further, we use\nthe Thouless-Anderson-Palmer (TAP) equations--a theoretical technique to\nanalyze the landscape of energy minima of random systems--to determine the\nevolution of the magnetic phases on two types of NNs (one with continuous and\none with binarized activations) trained on the MNIST dataset. The two NN types\ngive rise to similar results, showing a quick destruction of the spin glass and\nthe appearance of a phase with a hidden order, whose melting transition\ntemperature $T_c$ grows as a power law in training time. We also discuss the\nproperties of the spectrum of the spin system's bond matrix in the context of\nrich vs. lazy learning. We suggest that this statistical mechanical view of NNs\nprovides a useful unifying perspective on the training process, which can be\nviewed as selecting and strengthening a symmetry-broken state associated with\nthe training task.\n"", '  Many deep neural networks have been used to solve Ising models, including\nautoregressive neural networks, convolutional neural networks, recurrent neural\nnetworks, and graph neural networks. Learning a probability distribution of\nenergy configuration or finding the ground states of a disordered, fully\nconnected Ising model is essential for statistical mechanics and NP-hard\nproblems. Despite tremendous efforts, a neural network architecture with the\nability to high-accurately solve these fully connected and extremely\nintractable problems on larger systems is still lacking. Here we propose a\nvariational autoregressive architecture with a message passing mechanism, which\ncan effectively utilize the interactions between spin variables. The new\nnetwork trained under an annealing framework outperforms existing methods in\nsolving several prototypical Ising spin Hamiltonians, especially for larger\nspin systems at low temperatures. The advantages also come from the great\nmitigation of mode collapse during the training process of deep neural\nnetworks. Considering these extremely difficult problems to be solved, our\nmethod extends the current computational limits of unsupervised neural networks\nto solve combinatorial optimization problems.\n']",Ising Models and Neural Networks
255,254,34,254_wireless_decoding_transmit_bandwidth,"['wireless', 'decoding', 'transmit', 'bandwidth', 'channels', 'channel', 'modulation', 'aircomp', 'antennas', 'digital']","['wireless', 'devices', 'communication', 'air', 'uplink', 'channel', 'device', 'scheme', 'heterogeneity', 'edge']","['wireless', 'decoding', 'channel', 'modulation', 'aircomp', 'fl', 'allocation', 'analog', 'minimize', 'latency']","['  Over-the-air (OTA) computation has recently emerged as a\ncommunication-efficient Federated Learning (FL) paradigm to train machine\nlearning models over wireless networks. However, its performance is limited by\nthe device with the worst SNR, resulting in fast yet noisy updates. On the\nother hand, allocating orthogonal resource blocks (RB) to individual devices\nvia digital channels mitigates the noise problem, at the cost of increased\ncommunication latency. In this paper, we address this discrepancy and present\nADFL, a novel Analog-Digital FL scheme: in each round, the parameter server\n(PS) schedules each device to either upload its gradient via the analog OTA\nscheme or transmit its quantized gradient over an orthogonal RB using the\n``digital"" scheme. Focusing on a single FL round, we cast the optimal\nscheduling problem as the minimization of the mean squared error (MSE) on the\nestimated global gradient at the PS, subject to a delay constraint, yielding\nthe optimal device scheduling configuration and quantization bits for the\ndigital devices. Our simulation results show that ADFL, by scheduling most of\nthe devices in the OTA scheme while also occasionally employing the digital\nscheme for a few devices, consistently outperforms OTA-only and digital-only\nschemes, in both i.i.d. and non-i.i.d. settings.\n', '  In this paper, the performance optimization of federated learning (FL), when\ndeployed over a realistic wireless multiple-input multiple-output (MIMO)\ncommunication system with digital modulation and over-the-air computation\n(AirComp) is studied. In particular, a MIMO system is considered in which edge\ndevices transmit their local FL models (trained using their locally collected\ndata) to a parameter server (PS) using beamforming to maximize the number of\ndevices scheduled for transmission. The PS, acting as a central controller,\ngenerates a global FL model using the received local FL models and broadcasts\nit back to all devices. Due to the limited bandwidth in a wireless network,\nAirComp is adopted to enable efficient wireless data aggregation. However,\nfading of wireless channels can produce aggregate distortions in an\nAirComp-based FL scheme. To tackle this challenge, we propose a modified\nfederated averaging (FedAvg) algorithm that combines digital modulation with\nAirComp to mitigate wireless fading while ensuring the communication\nefficiency. This is achieved by a joint transmit and receive beamforming\ndesign, which is formulated as an optimization problem to dynamically adjust\nthe beamforming matrices based on current FL model parameters so as to minimize\nthe transmitting error and ensure the FL performance. To achieve this goal, we\nfirst analytically characterize how the beamforming matrices affect the\nperformance of the FedAvg in different iterations. Based on this relationship,\nan artificial neural network (ANN) is used to estimate the local FL models of\nall devices and adjust the beamforming matrices at the PS for future model\ntransmission. The algorithmic advantages and improved performance of the\nproposed methodologies are demonstrated through extensive numerical\nexperiments.\n', '  In this paper, we quantitatively compare these two effective communication\nschemes, i.e., digital and analog ones, for wireless federated learning (FL)\nover resource-constrained networks, highlighting their essential differences as\nwell as their respective application scenarios. We first examine both digital\nand analog transmission methods, together with a unified and fair comparison\nscheme under practical constraints. A universal convergence analysis under\nvarious imperfections is established for FL performance evaluation in wireless\nnetworks. These analytical results reveal that the fundamental difference\nbetween the two paradigms lies in whether communication and computation are\njointly designed or not. The digital schemes decouple the communication design\nfrom specific FL tasks, making it difficult to support simultaneous uplink\ntransmission of massive devices with limited bandwidth. In contrast, the analog\ncommunication allows over-the-air computation (AirComp), thus achieving\nefficient spectrum utilization. However, computation-oriented analog\ntransmission reduces power efficiency, and its performance is sensitive to\ncomputational errors. Finally, numerical simulations are conducted to verify\nthese theoretical observations.\n']",Wireless Communication for Federated Learning
256,255,34,255_powerflownet_powerflowmultinet_networks_powergraph,"['powerflownet', 'powerflowmultinet', 'networks', 'powergraph', 'grid', 'safepowergraph', 'grids', 'flow', 'blackout', 'optimal']","['power', 'grids', 'grid', 'bus', 'flow', 'contingency', 'operations', 'blackout', 'renewable', 'graph']","['powerflownet', 'networks', 'grid', 'safepowergraph', 'blackouts', 'inverters', 'renewable', 'gnn', 'ev', 'ropf']","['  The AC optimal power flow (AC-OPF) problem is essential for power system\noperations, but its non-convex nature makes it challenging to solve. A widely\nused simplification is the linearized DC optimal power flow (DC-OPF) problem,\nwhich can be solved to global optimality, but whose optimal solution is always\ninfeasible in the original AC-OPF problem. Recently, neural networks (NN) have\nbeen introduced for solving the AC-OPF problem at significantly faster\ncomputation times. However, these methods necessitate extensive datasets, are\ndifficult to train, and are often viewed as black boxes, leading to resistance\nfrom operators who prefer more transparent and interpretable solutions. In this\npaper, we introduce a novel learning-based approach that merges simplicity and\ninterpretability, providing a bridge between traditional approximation methods\nand black-box learning techniques. Our approach not only provides transparency\nfor operators but also achieves competitive accuracy. Numerical results across\nvarious power networks demonstrate that our method provides accuracy comparable\nto, and often surpassing, that of neural networks, particularly when training\ndatasets are limited.\n', '  A DC OPF surrogate modeling framework is developed for Monte Carlo (MC)\nsampling-based risk quantification in power grid operation. MC simulation\nnecessitates solving a large number of DC OPF problems corresponding to the\nsamples of stochastic grid variables (power demand and renewable generation),\nwhich is computationally prohibitive. Computationally inexpensive surrogates of\nOPF provide an attractive alternative for expedited MC simulation. Graph neural\nnetwork (GNN) surrogates of DC OPF, which are especially suitable to\ngraph-structured data, are employed in this work. Previously developed DC OPF\nsurrogate models have focused on accurate operational decision-making and not\non risk quantification. Here, risk quantification-specific aspects of DC OPF\nsurrogate evaluation is the main focus. To this end, the proposed GNN\nsurrogates are evaluated using realistic joint probability distributions,\nquantification of their risk estimation accuracy, and investigation of their\ngeneralizability. Four synthetic grids (Case118, Case300, Case1354pegase, and\nCase2848rte) are used for surrogate model performance evaluation. It is shown\nthat the GNN surrogates are sufficiently accurate for predicting the\n(bus-level, branch-level and system-level) grid state and enable fast as well\nas accurate operational risk quantification for power grids. The article thus\ndevelops tools for fast reliability and risk quantification in real-world power\ngrids using GNN-based surrogates.\n', '  Optimal Power Flow (OPF) refers to a wide range of related optimization\nproblems with the goal of operating power systems efficiently and securely. In\nthe simplest setting, OPF determines how much power to generate in order to\nminimize costs while meeting demand for power and satisfying physical and\noperational constraints. In even the simplest case, power grid operators use\napproximations of the AC-OPF problem because solving the exact problem is\nprohibitively slow with state-of-the-art solvers. These approximations\nsacrifice accuracy and operational feasibility in favor of speed. This\ntrade-off leads to costly ""uplift payments"" and increased carbon emissions,\nespecially for large power grids. In the present work, we train a deep learning\nsystem (CANOS) to predict near-optimal solutions (within 1% of the true AC-OPF\ncost) without compromising speed (running in as little as 33--65 ms).\nImportantly, CANOS scales to realistic grid sizes with promising empirical\nresults on grids containing as many as 10,000 buses. Finally, because CANOS is\na Graph Neural Network, it is robust to changes in topology. We show that CANOS\nis accurate across N-1 topological perturbations of a base grid typically used\nin security-constrained analysis. This paves the way for more efficient\noptimization of more complex OPF problems which alter grid connectivity such as\nunit commitment, topology optimization and security-constrained OPF.\n']",Power Grid Optimization
257,256,34,256_fmri_neuroimaging_networks_brain,"['fmri', 'neuroimaging', 'networks', 'brain', 'connectome', 'connectivity', 'hippocampal', 'hippocampus', 'functional', 'embedding']","['brain', 'functional', 'connectivity', 'resting', 'resonance', 'autism', 'disorder', 'connectome', 'magnetic', 'imaging']","['fmri', 'connectome', 'embedding', 'graph', 'neurodegenerative', 'neurodevelopmental', 'biomarkers', 'brainode', 'gnns', 'autism']","[""  The human brain is a complex, dynamic network, which is commonly studied\nusing functional magnetic resonance imaging (fMRI) and modeled as network of\nRegions of interest (ROIs) for understanding various brain functions. Recent\nstudies utilize deep learning approaches to learn the brain network\nrepresentation based on functional connectivity (FC) profile, broadly falling\ninto two main categories. The Fixed-FC approaches, utilizing the FC profile\nwhich represents the linear temporal relation within the brain network, are\nlimited by failing to capture informative brain temporal dynamics. On the other\nhand, the Dynamic-FC approaches, modeling the evolving FC profile over time,\noften exhibit less satisfactory performance due to challenges in handling the\ninherent noisy nature of fMRI data.\n  To address these challenges, we propose Brain Masked Auto-Encoder (BrainMAE)\nfor learning representations directly from fMRI time-series data. Our approach\nincorporates two essential components: a region-aware graph attention mechanism\ndesigned to capture the relationships between different brain ROIs, and a novel\nself-supervised masked autoencoding framework for effective model pre-training.\nThese components enable the model to capture rich temporal dynamics of brain\nactivity while maintaining resilience to inherent noise in fMRI data. Our\nexperiments demonstrate that BrainMAE consistently outperforms established\nbaseline methods by significant margins in four distinct downstream tasks.\nFinally, leveraging the model's inherent interpretability, our analysis of\nmodel-generated representations reveals findings that resonate with ongoing\nresearch in the field of neuroscience.\n"", '  Brain functional connectivity (FC) reveals biomarkers for identification of\nvarious neuropsychiatric disorders. Recent application of deep neural networks\n(DNNs) to connectome-based classification mostly relies on traditional\nconvolutional neural networks using input connectivity matrices on a regular\nEuclidean grid. We propose a graph deep learning framework to incorporate the\nnon-Euclidean information about graph structure for classifying functional\nmagnetic resonance imaging (fMRI)-derived brain networks in major depressive\ndisorder (MDD). We design a novel graph autoencoder (GAE) architecture based on\nthe graph convolutional networks (GCNs) to embed the topological structure and\nnode content of large-sized fMRI networks into low-dimensional latent\nrepresentations. In network construction, we employ the Ledoit-Wolf (LDW)\nshrinkage method to estimate the high-dimensional FC metrics efficiently from\nfMRI data. We consider both supervised and unsupervised approaches for the\ngraph embedding learning. The learned embeddings are then used as feature\ninputs for a deep fully-connected neural network (FCNN) to discriminate MDD\nfrom healthy controls. Evaluated on two resting-state fMRI (rs-fMRI) MDD\ndatasets, results show that the proposed GAE-FCNN model significantly\noutperforms several state-of-the-art methods for brain connectome\nclassification, achieving the best accuracy using the LDW-FC edges as node\nfeatures. The graph embeddings of fMRI FC networks learned by the GAE also\nreveal apparent group differences between MDD and HC. Our new framework\ndemonstrates feasibility of learning graph embeddings on brain networks to\nprovide discriminative information for diagnosis of brain disorders.\n', '  Resting-state functional magnetic resonance imaging (rs-fMRI) is a\nnoninvasive technique pivotal for understanding human neural mechanisms of\nintricate cognitive processes. Most rs-fMRI studies compute a single static\nfunctional connectivity matrix across brain regions of interest, or dynamic\nfunctional connectivity matrices with a sliding window approach. These\napproaches are at risk of oversimplifying brain dynamics and lack proper\nconsideration of the goal at hand. While deep learning has gained substantial\npopularity for modeling complex relational data, its application to uncovering\nthe spatiotemporal dynamics of the brain is still limited. We propose a novel\ninterpretable deep learning framework that learns goal-specific functional\nconnectivity matrix directly from time series and employs a specialized graph\nneural network for the final classification. Our model, DSAM, leverages\ntemporal causal convolutional networks to capture the temporal dynamics in both\nlow- and high-level feature representations, a temporal attention unit to\nidentify important time points, a self-attention unit to construct the\ngoal-specific connectivity matrix, and a novel variant of graph neural network\nto capture the spatial dynamics for downstream classification. To validate our\napproach, we conducted experiments on the Human Connectome Project dataset with\n1075 samples to build and interpret the model for the classification of sex\ngroup, and the Adolescent Brain Cognitive Development Dataset with 8520 samples\nfor independent testing. Compared our proposed framework with other\nstate-of-art models, results suggested this novel approach goes beyond the\nassumption of a fixed connectivity matrix and provides evidence of\ngoal-specific brain connectivity patterns, which opens up the potential to gain\ndeeper insights into how the human brain adapts its functional connectivity\nspecific to the task at hand.\n']",Brain Network Analysis using Neuroimaging Techniques
258,257,34,257_forecast_forecasting_forecasts_solar,"['forecast', 'forecasting', 'forecasts', 'solar', 'meteorological', 'predicting', 'photovoltaic', 'renewable', 'prediction', 'predict']","['solar', 'wind', 'forecasting', 'power', 'weather', 'forecasts', 'photovoltaic', 'energy', 'renewable', 'farms']","['forecast', 'photovoltaic', 'climatic', 'matnet', 'nwp', 'grid', 'farms', 'accuracy', 'regional', 'pgml']","['  Weather forecasts from numerical weather prediction models play a central\nrole in solar energy forecasting, where a cascade of physics-based models is\nused in a model chain approach to convert forecasts of solar irradiance to\nsolar power production, using additional weather variables as auxiliary\ninformation. Ensemble weather forecasts aim to quantify uncertainty in the\nfuture development of the weather, and can be used to propagate this\nuncertainty through the model chain to generate probabilistic solar energy\npredictions. However, ensemble prediction systems are known to exhibit\nsystematic errors, and thus require post-processing to obtain accurate and\nreliable probabilistic forecasts. The overarching aim of our study is to\nsystematically evaluate different strategies to apply post-processing methods\nin model chain approaches: Not applying any post-processing at all;\npost-processing only the irradiance predictions before the conversion;\npost-processing only the solar power predictions obtained from the model chain;\nor applying post-processing in both steps. In a case study based on a benchmark\ndataset for the Jacumba solar plant in the U.S., we develop statistical and\nmachine learning methods for post-processing ensemble predictions of global\nhorizontal irradiance and solar power generation. Further, we propose a neural\nnetwork-based model for direct solar power forecasting that bypasses the model\nchain. Our results indicate that post-processing substantially improves the\nsolar power generation forecasts, in particular when post-processing is applied\nto the power predictions. The machine learning methods for post-processing\nyield slightly better probabilistic forecasts, and the direct forecasting\napproach performs comparable to the post-processing strategies.\n', '  Electricity generated from renewable energy sources has been established as\nan efficient remedy for both energy shortages and the environmental pollution\nstemming from conventional energy production methods. Solar and wind power are\ntwo of the most dominant renewable energy sources. The accurate forecasting of\nthe energy generation of those sources facilitates their integration into\nelectric grids, by minimizing the negative impact of uncertainty regarding\ntheir management and operation. This paper proposes a novel methodology for\ndeterministic wind and solar energy generation forecasting for multiple\ngeneration sites, utilizing multi-location weather forecasts. The method\nemploys a U-shaped Temporal Convolutional Auto-Encoder (UTCAE) architecture for\ntemporal processing of weather-related and energy-related time-series across\neach site. The Multi-sized Kernels convolutional Spatio-Temporal Attention\n(MKST-Attention), inspired by the multi-head scaled-dot product attention\nmechanism, is also proposed aiming to efficiently transfer temporal patterns\nfrom weather data to energy data, without a priori knowledge of the locations\nof the power stations and the locations of provided weather data. The conducted\nexperimental evaluation on a day-ahead solar and wind energy forecasting\nscenario on five datasets demonstrated that the proposed method achieves top\nresults, outperforming all competitive time-series forecasting state-of-the-art\nmethods.\n', '  The challenges in applications of solar energy lies in its intermittency and\ndependency on meteorological parameters such as; solar radiation, ambient\ntemperature, rainfall, wind-speed etc., and many other physical parameters like\ndust accumulation etc. Hence, it is important to estimate the amount of solar\nphotovoltaic (PV) power generation for a specific geographical location.\nMachine learning (ML) models have gained importance and are widely used for\nprediction of solar power plant performance. In this paper, the impact of\nweather parameters on solar PV power generation is estimated by several\nEnsemble ML (EML) models like Bagging, Boosting, Stacking, and Voting for the\nfirst time. The performance of chosen ML algorithms is validated by field\ndataset of a 10kWp solar PV power plant in Eastern India region. Furthermore, a\ncomplete test-bed framework has been designed for data mining as well as to\nselect appropriate learning models. It also supports feature selection and\nreduction for dataset to reduce space and time complexity of the learning\nmodels. The results demonstrate greater prediction accuracy of around 96% for\nStacking and Voting EML models. The proposed work is a generalized one and can\nbe very useful for predicting the performance of large-scale solar PV power\nplants also.\n']",Solar Energy Forecasting
259,258,33,258_portfolio_portfolios_trading_markets,"['portfolio', 'portfolios', 'trading', 'markets', 'agents', 'investment', 'finance', 'stocks', 'market', 'agent']","['trading', 'market', 'portfolio', 'hedging', 'markets', 'financial', 'asset', 'currency', 'alpha', 'assets']","['portfolios', 'markets', 'agents', 'strategies', 'hedging', 'cryptocurrencies', 'rebalancing', 'btc', 'volatile', 'rl']","['  Exploring complex adaptive financial trading environments through multi-agent\nbased simulation methods presents an innovative approach within the realm of\nquantitative finance. Despite the dominance of multi-agent reinforcement\nlearning approaches in financial markets with observable data, there exists a\nset of systematically significant financial markets that pose challenges due to\ntheir partial or obscured data availability. We, therefore, devise a\nmulti-agent simulation approach employing small-scale meta-heuristic methods.\nThis approach aims to represent the opaque bilateral market for Australian\ngovernment bond trading, capturing the bilateral nature of bank-to-bank\ntrading, also referred to as ""over-the-counter"" (OTC) trading, and commonly\noccurring between ""market makers"". The uniqueness of the bilateral market,\ncharacterized by negotiated transactions and a limited number of agents, yields\nvaluable insights for agent-based modelling and quantitative finance. The\ninherent rigidity of this market structure, which is at odds with the global\nproliferation of multilateral platforms and the decentralization of finance,\nunderscores the unique insights offered by our agent-based model. We explore\nthe implications of market rigidity on market structure and consider the\nelement of stability, in market design. This extends the ongoing discourse on\ncomplex financial trading environments, providing an enhanced understanding of\ntheir dynamics and implications.\n', '  In recent years, deep or reinforcement learning approaches have been applied\nto optimise investment portfolios through learning the spatial and temporal\ninformation under the dynamic financial market. Yet in most cases, the existing\napproaches may produce biased trading signals based on the conventional price\ndata due to a lot of market noises, which possibly fails to balance the\ninvestment returns and risks. Accordingly, a multi-agent and self-adaptive\nportfolio optimisation framework integrated with attention mechanisms and time\nseries, namely the MASAAT, is proposed in this work in which multiple trading\nagents are created to observe and analyse the price series and directional\nchange data that recognises the significant changes of asset prices at\ndifferent levels of granularity for enhancing the signal-to-noise ratio of\nprice series. Afterwards, by reconstructing the tokens of financial data in a\nsequence, the attention-based cross-sectional analysis module and temporal\nanalysis module of each agent can effectively capture the correlations between\nassets and the dependencies between time points. Besides, a portfolio generator\nis integrated into the proposed framework to fuse the spatial-temporal\ninformation and then summarise the portfolios suggested by all trading agents\nto produce a newly ensemble portfolio for reducing biased trading actions and\nbalancing the overall returns and risks. The experimental results clearly\ndemonstrate that the MASAAT framework achieves impressive enhancement when\ncompared with many well-known portfolio optimsation approaches on three\nchallenging data sets of DJIA, S&P 500 and CSI 300. More importantly, our\nproposal has potential strengths in many possible applications for future\nstudy.\n', '  As a model-free algorithm, deep reinforcement learning (DRL) agent learns and\nmakes decisions by interacting with the environment in an unsupervised way. In\nrecent years, DRL algorithms have been widely applied by scholars for portfolio\noptimization in consecutive trading periods, since the DRL agent can\ndynamically adapt to market changes and does not rely on the specification of\nthe joint dynamics across the assets. However, typical DRL agents for portfolio\noptimization cannot learn a policy that is aware of the dynamic correlation\nbetween portfolio asset returns. Since the dynamic correlations among portfolio\nassets are crucial in optimizing the portfolio, the lack of such knowledge\nmakes it difficult for the DRL agent to maximize the return per unit of risk,\nespecially when the target market permits short selling (i.e., the US stock\nmarket). In this research, we propose a hybrid portfolio optimization model\ncombining the DRL agent and the Black-Litterman (BL) model to enable the DRL\nagent to learn the dynamic correlation between the portfolio asset returns and\nimplement an efficacious long/short strategy based on the correlation.\nEssentially, the DRL agent is trained to learn the policy to apply the BL model\nto determine the target portfolio weights. To test our DRL agent, we construct\nthe portfolio based on all the Dow Jones Industrial Average constitute stocks.\nEmpirical results of the experiments conducted on real-world United States\nstock market data demonstrate that our DRL agent significantly outperforms\nvarious comparison portfolio choice strategies and alternative DRL frameworks\nby at least 42% in terms of accumulated return. In terms of the return per unit\nof risk, our DRL agent significantly outperforms various comparative portfolio\nchoice strategies and alternative strategies based on other machine learning\nframeworks.\n']","""Portfolio Optimization using Artificial Intelligence in Financial Markets"""
260,259,33,259_hydrological_hydrology_hydrologic_flooding,"['hydrological', 'hydrology', 'hydrologic', 'flooding', 'forecasting', 'floods', 'forecast', 'runoff', 'flood', 'catchment']","['flood', 'hydrological', 'water', 'streamflow', 'catchments', 'physics', 'runoff', 'ice', 'hydrologic', 'hydrology']","['hydrology', 'forecasting', 'runoff', 'streamflow', 'modeling', 'rnns', 'precipitation', 'lake', 'storms', 'landslide']","['  Current hydrological modeling methods combine data-driven Machine Learning\n(ML) algorithms and traditional physics-based models to address their\nrespective limitations incorrect parameter estimates from rigid physics-based\nmodels and the neglect of physical process constraints by ML algorithms.\nDespite the accuracy of ML in outcome prediction, the integration of scientific\nknowledge is crucial for reliable predictions. This study introduces a Physics\nInformed Machine Learning (PIML) model, which merges the process understanding\nof conceptual hydrological models with the predictive efficiency of ML\nalgorithms. Applied to the Anandapur sub-catchment, the PIML model demonstrates\nsuperior performance in forecasting monthly streamflow and actual\nevapotranspiration over both standalone conceptual models and ML algorithms,\nensuring physical consistency of the outputs. This study replicates the\nmethodologies of Bhasme, P., Vagadiya, J., & Bhatia, U. (2022) from their\npivotal work on Physics Informed Machine Learning for hydrological processes,\nutilizing their shared code and datasets to further explore the predictive\ncapabilities in hydrological modeling.\n', '  In recent years, climate extremes such as floods have created significant\nenvironmental and economic hazards for Australia, causing damage to the\nenvironment and economy and losses of human and animal lives. An efficient\nmethod of forecasting floods is crucial to limit this damage. Techniques for\nflood prediction are currently based on hydrological, and hydrodynamic\n(physically-based) numerical models. Machine learning methods that include deep\nlearning offer certain advantages over conventional physically based\napproaches, including flexibility and accuracy. Deep learning methods have been\npromising for predicting small to medium-sized climate extreme events over a\nshort time horizon; however, large flooding events present a critical\nchallenge. We present an ensemble-based machine learning approach that\naddresses large-scale extreme flooding challenges using a switching mechanism\nmotivated by extreme-value theory for long-short-term-memory (LSTM) deep\nlearning models. We use a multivariate and multi-step time-series prediction\napproach to predict streamflow for multiple days ahead in the major catchments\nof Australia. The ensemble framework also employs static information to enrich\nthe time-series information, allowing for regional modelling across catchments.\nOur results demonstrate enhanced prediction of streamflow extremes, with\nnotable efficacy for large flooding scenarios in the selected Australian\ncatchments. Through comparative analysis, our methodology underscores the\npotential for deep learning models to revolutionise flood forecasting across\ndiverse regions.\n', '  The application of process-based and data-driven hydrological models is\ncrucial in modern hydrological research, especially for predicting key water\ncycle variables such as runoff, evapotranspiration (ET), and soil moisture.\nThese models provide a scientific basis for water resource management, flood\nforecasting, and ecological protection. Process-based models simulate the\nphysical mechanisms of watershed hydrological processes, while data-driven\nmodels leverage large datasets and advanced machine learning algorithms. This\npaper reviewed and compared methods for assessing and enhancing the\nextrapolability of both model types, discussing their prospects and\nlimitations. Key strategies include the use of leave-one-out cross-validation\nand similarity-based methods to evaluate model performance in ungauged regions.\nDeep learning, transfer learning, and domain adaptation techniques are also\npromising in their potential to improve model predictions in data-sparse and\nextreme conditions. Interdisciplinary collaboration and continuous algorithmic\nadvancements are also important to strengthen the global applicability and\nreliability of hydrological models.\n']",Hydrological Modeling and Flood Forecasting
261,260,33,260_perceptrons_neural_networks_imagenet1k,"['perceptrons', 'neural', 'networks', 'imagenet1k', 'neurons', 'perceptron', 'layers', 'kolmogorov', 'learnable', 'convolutional']","['kolmogorov', 'polynomials', 'functions', 'perceptrons', 'spline', 'wavelet', 'basis', 'layer', 'activation', 'alternative']","['perceptrons', 'imagenet1k', 'layers', 'learnable', 'convolutional', 'fcnns', 'wavelets', 'synapses', 'fsck', 'activation']","['  Kolmogorov-Arnold Networks (KANs) offer an efficient and interpretable\nalternative to traditional multi-layer perceptron (MLP) architectures due to\ntheir finite network topology. However, according to the results of Kolmogorov\nand Vitushkin, the representation of generic smooth functions by KAN\nimplementations using analytic functions constrained to a finite number of\ncutoff points cannot be exact. Hence, the convergence of KAN throughout the\ntraining process may be limited. This paper explores the relevance of\nsmoothness in KANs, proposing that smooth, structurally informed KANs can\nachieve equivalence to MLPs in specific function classes. By leveraging\ninherent structural knowledge, KANs may reduce the data required for training\nand mitigate the risk of generating hallucinated predictions, thereby enhancing\nmodel reliability and performance in computational biomedicine.\n', '  Kolmogorov-Arnold Networks (KANs) introduce a paradigm of neural modeling\nthat implements learnable functions on the edges of the networks, diverging\nfrom the traditional node-centric activations in neural networks. This work\nassesses the applicability and efficacy of KANs in visual modeling, focusing on\nthe image recognition task. We mainly analyze the performance and efficiency of\ndifferent network architectures built using KAN concepts along with\nconventional building blocks of convolutional and linear layers, enabling a\ncomparative analysis with the conventional models. Our findings are aimed at\ncontributing to understanding the potential of KANs in computer vision,\nhighlighting both their strengths and areas for further research. Our\nevaluation shows that whereas KAN-based architectures perform in-line with the\noriginal claims of KAN paper for performance and model-complexity in the case\nof simpler vision datasets like MNIST, the advantages seem to diminish even for\nslightly more complex datasets like CIFAR-10.\n', '  Recently, Kolmogorov-Arnold Networks (KANs) have been proposed as an\nalternative to multilayer perceptrons, suggesting advantages in performance and\ninterpretability. We study a typical binary event classification task in\nhigh-energy physics including high-level features and comment on the\nperformance and interpretability of KANs in this context. We find that the\nlearned activation functions of a one-layer KAN resemble the log-likelihood\nratio of the input features. In deeper KANs, the activations in the first KAN\nlayer differ from those in the one-layer KAN, which indicates that the deeper\nKANs learn more complex representations of the data. We study KANs with\ndifferent depths and widths and we compare them to multilayer perceptrons in\nterms of performance and number of trainable parameters. For the chosen\nclassification task, we do not find that KANs are more parameter efficient.\nHowever, small KANs may offer advantages in terms of interpretability that come\nat the cost of only a moderate loss in performance.\n']",Kolmogorov-Arnold Networks in Neural Network Architectures
262,261,33,261_batteryml_batteries_battery_lithium,"['batteryml', 'batteries', 'battery', 'lithium', 'predicting', 'prediction', 'prognostics', 'estimating', 'lifespan', 'degradation']","['battery', 'batteries', 'lithium', 'ion', 'life', 'electrochemical', 'degradation', 'aging', 'voltage', 'health']","['batteryml', 'predicting', 'lifespan', 'electrochemical', 'storage', 'cell', 'graphite', 'li', 'evs', 'hybrid']","['  Batteries are dynamic systems with complicated nonlinear aging, highly\ndependent on cell design, chemistry, manufacturing, and operational conditions.\nPrediction of battery cycle life and estimation of aging states is important to\naccelerate battery R&D, testing, and to further the understanding of how\nbatteries degrade. Beyond testing, battery management systems rely on real-time\nmodels and onboard diagnostics and prognostics for safe operation. Estimating\nthe state of health and remaining useful life of a battery is important to\noptimize performance and use resources optimally.\n  This tutorial begins with an overview of first-principles, machine learning,\nand hybrid battery models. Then, a typical pipeline for the development of\ninterpretable machine learning models is explained and showcased for cycle life\nprediction from laboratory testing data. We highlight the challenges of machine\nlearning models, motivating the incorporation of physics in hybrid modeling\napproaches, which are needed to decipher the aging trajectory of batteries but\nrequire more data and further work on the physics of battery degradation. The\ntutorial closes with a discussion on generalization and further research\ndirections.\n', '  Battery life estimation is critical for optimizing battery performance and\nguaranteeing minimal degradation for better efficiency and reliability of\nbattery-powered systems. The existing methods to predict the Remaining Useful\nLife(RUL) of Lithium-ion Batteries (LiBs) neglect the relational dependencies\nof the battery parameters to model the nonlinear degradation trajectories. We\npresent the Battery GraphNets framework that jointly learns to incorporate a\ndiscrete dependency graph structure between battery parameters to capture the\ncomplex interactions and the graph-learning algorithm to model the intrinsic\nbattery degradation for RUL prognosis. The proposed method outperforms several\npopular methods by a significant margin on publicly available battery datasets\nand achieves SOTA performance. We report the ablation studies to support the\nefficacy of our approach.\n', '  Lithium-ion batteries are pivotal to technological advancements in\ntransportation, electronics, and clean energy storage. The optimal operation\nand safety of these batteries require proper and reliable estimation of battery\ncapacities to monitor the state of health. Current methods for estimating the\ncapacities fail to adequately account for long-term temporal dependencies of\nkey variables (e.g., voltage, current, and temperature) associated with battery\naging and degradation. In this study, we explore the usage of transformer\nnetworks to enhance the estimation of battery capacity. We develop a\ntransformer-based battery capacity prediction model that accounts for both\nlong-term and short-term patterns in battery data. Further, to tackle the data\nscarcity issue, data augmentation is used to increase the data size, which\nhelps to improve the performance of the model. Our proposed method is validated\nwith benchmark datasets. Simulation results show the effectiveness of data\naugmentation and the transformer network in improving the accuracy and\nrobustness of battery capacity prediction.\n']",Battery Performance Prediction and Degradation Analysis
263,262,33,262_hyperspectral_multispectral_spectral_supervised,"['hyperspectral', 'multispectral', 'spectral', 'supervised', 'imagery', 'sensing', 'classification', 'denoising', 'hyperview', 'images']","['hyperspectral', 'spectral', 'unmixing', 'endmembers', 'spatial', 'multispectral', 'pixel', 'abundance', 'bands', 'abundances']","['hyperspectral', 'spectral', 'sensing', 'classification', 'denoising', 'pixels', 'lightgbm', 'pca', 'infrared', 'abundances']","['  Land cover analysis using hyperspectral images (HSI) remains an open problem\ndue to their low spatial resolution and complex spectral information. Recent\nstudies are primarily dedicated to designing Transformer-based architectures\nfor spatial-spectral long-range dependencies modeling, which is computationally\nexpensive with quadratic complexity. Selective structured state space model\n(Mamba), which is efficient for modeling long-range dependencies with linear\ncomplexity, has recently shown promising progress. However, its potential in\nhyperspectral image processing that requires handling numerous spectral bands\nhas not yet been explored. In this paper, we innovatively propose S$^2$Mamba, a\nspatial-spectral state space model for hyperspectral image classification, to\nexcavate spatial-spectral contextual features, resulting in more efficient and\naccurate land cover analysis. In S$^2$Mamba, two selective structured state\nspace models through different dimensions are designed for feature extraction,\none for spatial, and the other for spectral, along with a spatial-spectral\nmixture gate for optimal fusion. More specifically, S$^2$Mamba first captures\nspatial contextual relations by interacting each pixel with its adjacent\nthrough a Patch Cross Scanning module and then explores semantic information\nfrom continuous spectral bands through a Bi-directional Spectral Scanning\nmodule. Considering the distinct expertise of the two attributes in homogenous\nand complicated texture scenes, we realize the Spatial-spectral Mixture Gate by\na group of learnable matrices, allowing for the adaptive incorporation of\nrepresentations learned across different dimensions. Extensive experiments\nconducted on HSI classification benchmarks demonstrate the superiority and\nprospect of S$^2$Mamba. The code will be made available at:\nhttps://github.com/PURE-melo/S2Mamba.\n', '  Hyperspectral images (HSIs) contain rich spectral and spatial information.\nMotivated by the success of transformers in the field of natural language\nprocessing and computer vision where they have shown the ability to learn long\nrange dependencies within input data, recent research has focused on using\ntransformers for HSIs. However, current state-of-the-art hyperspectral\ntransformers only tokenize the input HSI sample along the spectral dimension,\nresulting in the under-utilization of spatial information. Moreover,\ntransformers are known to be data-hungry and their performance relies heavily\non large-scale pretraining, which is challenging due to limited annotated\nhyperspectral data. Therefore, the full potential of HSI transformers has not\nbeen fully realized. To overcome these limitations, we propose a novel\nfactorized spectral-spatial transformer that incorporates factorized\nself-supervised pretraining procedures, leading to significant improvements in\nperformance. The factorization of the inputs allows the spectral and spatial\ntransformers to better capture the interactions within the hyperspectral data\ncubes. Inspired by masked image modeling pretraining, we also devise efficient\nmasking strategies for pretraining each of the spectral and spatial\ntransformers. We conduct experiments on six publicly available datasets for HSI\nclassification task and demonstrate that our model achieves state-of-the-art\nperformance in all the datasets. The code for our model will be made available\nat https://github.com/csiro-robotics/factoformer.\n', '  Contrastive learning has demonstrated great effectiveness in representation\nlearning especially for image classification tasks. However, there is still a\nshortage in the studies targeting regression tasks, and more specifically\napplications on hyperspectral data. In this paper, we propose a contrastive\nlearning framework for the regression tasks for hyperspectral data. To this\nend, we provide a collection of transformations relevant for augmenting\nhyperspectral data, and investigate contrastive learning for regression.\nExperiments on synthetic and real hyperspectral datasets show that the proposed\nframework and transformations significantly improve the performance of\nregression models, achieving better scores than other state-of-the-art\ntransformations.\n']",Hyperspectral Image Analysis
264,263,33,263_adversarial_captioner_multimodal_captioning,"['adversarial', 'captioner', 'multimodal', 'captioning', 'embeddings', 'captions', 'visual', 'adversary', 'attacking', 'safeclip']","['adversarial', 'attack', 'attacks', 'robustness', 'vision', 'image', 'multimodal', 'images', 'dreaming', 'hijacks']","['adversarial', 'captioner', 'multimodal', 'embeddings', 'safeclip', 'vulnerable', 'vision', 'targeted', 'lvlm', 'models']","['  Vision-Language Models (VLMs) are becoming increasingly vulnerable to\nadversarial attacks as various novel attack strategies are being proposed\nagainst these models. While existing defenses excel in unimodal contexts, they\ncurrently fall short in safeguarding VLMs against adversarial threats. To\nmitigate this vulnerability, we propose a novel, yet elegantly simple approach\nfor detecting adversarial samples in VLMs. Our method leverages Text-to-Image\n(T2I) models to generate images based on captions produced by target VLMs.\nSubsequently, we calculate the similarities of the embeddings of both input and\ngenerated images in the feature space to identify adversarial samples.\nEmpirical evaluations conducted on different datasets validate the efficacy of\nour approach, outperforming baseline methods adapted from image classification\ndomains. Furthermore, we extend our methodology to classification tasks,\nshowcasing its adaptability and model-agnostic nature. Theoretical analyses and\nempirical findings also show the resilience of our approach against adaptive\nattacks, positioning it as an excellent defense mechanism for real-world\ndeployment against adversarial threats.\n', ""  Vision-language models (VLMs) have achieved significant strides in recent\ntimes specially in multimodal tasks, yet they remain susceptible to adversarial\nattacks on their vision components. To address this, we propose Sim-CLIP, an\nunsupervised adversarial fine-tuning method that enhances the robustness of the\nwidely-used CLIP vision encoder against such attacks while maintaining semantic\nrichness and specificity. By employing a Siamese architecture with cosine\nsimilarity loss, Sim-CLIP learns semantically meaningful and attack-resilient\nvisual representations without requiring large batch sizes or momentum\nencoders. Our results demonstrate that VLMs enhanced with Sim-CLIP's fine-tuned\nCLIP encoder exhibit significantly enhanced robustness against adversarial\nattacks, while preserving semantic meaning of the perturbed images. Notably,\nSim-CLIP does not require additional training or fine-tuning of the VLM itself;\nreplacing the original vision encoder with our fine-tuned Sim-CLIP suffices to\nprovide robustness. This work underscores the significance of reinforcing\nfoundational models like CLIP to safeguard the reliability of downstream VLM\napplications, paving the way for more secure and effective multimodal systems.\n"", ""  Vision-enabled language models (VLMs) are now used to build autonomous\nmultimodal agents capable of taking actions in real environments. In this\npaper, we show that multimodal agents raise new safety risks, even though\nattacking agents is more challenging than prior attacks due to limited access\nto and knowledge about the environment. Our attacks use adversarial text\nstrings to guide gradient-based perturbation over one trigger image in the\nenvironment: (1) our captioner attack attacks white-box captioners if they are\nused to process images into captions as additional inputs to the VLM; (2) our\nCLIP attack attacks a set of CLIP models jointly, which can transfer to\nproprietary VLMs. To evaluate the attacks, we curated VisualWebArena-Adv, a set\nof adversarial tasks based on VisualWebArena, an environment for web-based\nmultimodal agent tasks. Within an L-infinity norm of $16/256$ on a single\nimage, the captioner attack can make a captioner-augmented GPT-4V agent execute\nthe adversarial goals with a 75% success rate. When we remove the captioner or\nuse GPT-4V to generate its own captions, the CLIP attack can achieve success\nrates of 21% and 43%, respectively. Experiments on agents based on other VLMs,\nsuch as Gemini-1.5, Claude-3, and GPT-4o, show interesting differences in their\nrobustness. Further analysis reveals several key factors contributing to the\nattack's success, and we also discuss the implications for defenses as well.\nProject page: https://chenwu.io/attack-agent Code and data:\nhttps://github.com/ChenWu98/agent-attack\n""]",Adversarial Attacks on Multimodal Vision-Language Models
265,264,33,264_gaze_gazemedseg_eyedentify_eye,"['gaze', 'gazemedseg', 'eyedentify', 'eye', 'eyes', 'saliency', 'ocular', 'attention', 'visual', 'attentional']","['gaze', 'eye', 'tracking', 'pupil', 'scanpath', 'scanpaths', 'fixation', 'driver', 'estimation', 'saccades']","['gaze', 'eyedentify', 'saliency', 'visual', 'saccades', 'webcam', 'heatmaps', 'masks', 'prediction', 'macro']","[""  Predicting human gaze behavior within computer vision is integral for\ndeveloping interactive systems that can anticipate user attention, address\nfundamental questions in cognitive science, and hold implications for fields\nlike human-computer interaction (HCI) and augmented/virtual reality (AR/VR)\nsystems. Despite methodologies introduced for modeling human eye gaze behavior,\napplying these models to medical imaging for scanpath prediction remains\nunexplored. Our proposed system aims to predict eye gaze sequences from\nradiology reports and CXR images, potentially streamlining data collection and\nenhancing AI systems using larger datasets. However, predicting human scanpaths\non medical images presents unique challenges due to the diverse nature of\nabnormal regions. Our model predicts fixation coordinates and durations\ncritical for medical scanpath prediction, outperforming existing models in the\ncomputer vision community. Utilizing a two-stage training process and large\npublicly available datasets, our approach generates static heatmaps and eye\ngaze videos aligned with radiology reports, facilitating comprehensive\nanalysis. We validate our approach by comparing its performance with\nstate-of-the-art methods and assessing its generalizability among different\nradiologists, introducing novel strategies to model radiologists' search\npatterns during CXR image diagnosis. Based on the radiologist's evaluation,\nMedGaze can generate human-like gaze sequences with a high focus on relevant\nregions over the CXR images. It sometimes also outperforms humans in terms of\nredundancy and randomness in the scanpaths.\n"", '  Eye gaze that reveals human observational patterns has increasingly been\nincorporated into solutions for vision tasks. Despite recent explorations on\nleveraging gaze to aid deep networks, few studies exploit gaze as an efficient\nannotation approach for medical image segmentation which typically entails\nheavy annotating costs. In this paper, we propose to collect dense weak\nsupervision for medical image segmentation with a gaze annotation scheme. To\ntrain with gaze, we propose a multi-level framework that trains multiple\nnetworks from discriminative human attention, simulated with a set of\npseudo-masks derived by applying hierarchical thresholds on gaze heatmaps.\nFurthermore, to mitigate gaze noise, a cross-level consistency is exploited to\nregularize overfitting noisy labels, steering models toward clean patterns\nlearned by peer networks. The proposed method is validated on two public\nmedical datasets of polyp and prostate segmentation tasks. We contribute a\nhigh-quality gaze dataset entitled GazeMedSeg as an extension to the popular\nmedical segmentation datasets. To the best of our knowledge, this is the first\ngaze dataset for medical image segmentation. Our experiments demonstrate that\ngaze annotation outperforms previous label-efficient annotation schemes in\nterms of both performance and annotation time. Our collected gaze data and code\nare available at: https://github.com/med-air/GazeMedSeg.\n', '  Eye-tracking applications that utilize the human gaze in video understanding\ntasks have become increasingly important. To effectively automate the process\nof video analysis based on eye-tracking data, it is important to accurately\nreplicate human gaze behavior. However, this task presents significant\nchallenges due to the inherent complexity and ambiguity of human gaze patterns.\nIn this work, we introduce a novel method for simulating human gaze behavior.\nOur approach uses a transformer-based reinforcement learning algorithm to train\nan agent that acts as a human observer, with the primary role of watching\nvideos and simulating human gaze behavior. We employed an eye-tracking dataset\ngathered from videos generated by the VirtualHome simulator, with a primary\nfocus on activity recognition. Our experimental results demonstrate the\neffectiveness of our gaze prediction method by highlighting its capability to\nreplicate human gaze behavior and its applicability for downstream tasks where\nreal human-gaze is used as input.\n']",Eye Gaze Analysis in Medical Imaging and Video Understanding
266,265,33,265_locomotion_stepping_terrain_legged,"['locomotion', 'stepping', 'terrain', 'legged', 'robotics', 'walking', 'jumps', 'humanoid', 'terrains', 'jumping']","['locomotion', 'robots', 'robot', 'legged', 'controller', 'gaits', 'gait', 'bipedal', 'terrain', 'limbs']","['locomotion', 'legged', 'robotics', 'jumps', 'terrains', 'gaits', 'learned', 'biped', 'footsteps', 'actuator']","[""  Autonomous wheeled-legged robots have the potential to transform logistics\nsystems, improving operational efficiency and adaptability in urban\nenvironments. Navigating urban environments, however, poses unique challenges\nfor robots, necessitating innovative solutions for locomotion and navigation.\nThese challenges include the need for adaptive locomotion across varied\nterrains and the ability to navigate efficiently around complex dynamic\nobstacles. This work introduces a fully integrated system comprising adaptive\nlocomotion control, mobility-aware local navigation planning, and large-scale\npath planning within the city. Using model-free reinforcement learning (RL)\ntechniques and privileged learning, we develop a versatile locomotion\ncontroller. This controller achieves efficient and robust locomotion over\nvarious rough terrains, facilitated by smooth transitions between walking and\ndriving modes. It is tightly integrated with a learned navigation controller\nthrough a hierarchical RL framework, enabling effective navigation through\nchallenging terrain and various obstacles at high speed. Our controllers are\nintegrated into a large-scale urban navigation system and validated by\nautonomous, kilometer-scale navigation missions conducted in Zurich,\nSwitzerland, and Seville, Spain. These missions demonstrate the system's\nrobustness and adaptability, underscoring the importance of integrated control\nsystems in achieving seamless navigation in complex environments. Our findings\nsupport the feasibility of wheeled-legged robots and hierarchical RL for\nautonomous navigation, with implications for last-mile delivery and beyond.\n"", '  Recent advances of locomotion controllers utilizing deep reinforcement\nlearning (RL) have yielded impressive results in terms of achieving rapid and\nrobust locomotion across challenging terrain, such as rugged rocks, non-rigid\nground, and slippery surfaces. However, while these controllers primarily\naddress challenges underneath the robot, relatively little research has\ninvestigated legged mobility through confined 3D spaces, such as narrow tunnels\nor irregular voids, which impose all-around constraints. The cyclic gait\npatterns resulted from existing RL-based methods to learn parameterized\nlocomotion skills characterized by motion parameters, such as velocity and body\nheight, may not be adequate to navigate robots through challenging confined 3D\nspaces, requiring both agile 3D obstacle avoidance and robust legged\nlocomotion. Instead, we propose to learn locomotion skills end-to-end from\ngoal-oriented navigation in confined 3D spaces. To address the inefficiency of\ntracking distant navigation goals, we introduce a hierarchical locomotion\ncontroller that combines a classical planner tasked with planning waypoints to\nreach a faraway global goal location, and an RL-based policy trained to follow\nthese waypoints by generating low-level motion commands. This approach allows\nthe policy to explore its own locomotion skills within the entire solution\nspace and facilitates smooth transitions between local goals, enabling\nlong-term navigation towards distant goals. In simulation, our hierarchical\napproach succeeds at navigating through demanding confined 3D environments,\noutperforming both pure end-to-end learning approaches and parameterized\nlocomotion skills. We further demonstrate the successful real-world deployment\nof our simulation-trained controller on a real robot.\n', '  While most recent advancements in legged robot control have been driven by\nmodel-free reinforcement learning, we explore the potential of differentiable\nsimulation. Differentiable simulation promises faster convergence and more\nstable training by computing low-variant first-order gradients using the robot\nmodel, but so far, its use for legged robot control has remained limited to\nsimulation. The main challenge with differentiable simulation lies in the\ncomplex optimization landscape of robotic tasks due to discontinuities in\ncontact-rich environments, e.g., quadruped locomotion. This work proposes a\nnew, differentiable simulation framework to overcome these challenges. The key\nidea involves decoupling the complex whole-body simulation, which may exhibit\ndiscontinuities due to contact, into two separate continuous domains.\nSubsequently, we align the robot state resulting from the simplified model with\na more precise, non-differentiable simulator to maintain sufficient simulation\naccuracy. Our framework enables learning quadruped walking in minutes using a\nsingle simulated robot without any parallelization. When augmented with GPU\nparallelization, our approach allows the quadruped robot to master diverse\nlocomotion skills, including trot, pace, bound, and gallop, on challenging\nterrains in minutes. Additionally, our policy achieves robust locomotion\nperformance in the real world zero-shot. To the best of our knowledge, this\nwork represents the first demonstration of using differentiable simulation for\ncontrolling a real quadruped robot. This work provides several important\ninsights into using differentiable simulations for legged locomotion in the\nreal world.\n']","""Legged Robot Locomotion and Navigation"""
267,266,33,266_decoders_digits_transformers_encodings,"['decoders', 'digits', 'transformers', 'encodings', 'rnns', 'turing', 'digit', 'decoder', 'addition', 'transformer']","['transformers', 'digit', 'arithmetic', 'numbers', 'length', 'transformer', 'multiplication', 'position', 'scratchpad', 'integers']","['decoders', 'rnns', 'turing', 'transformer', 'operations', 'generalizing', 'integers', 'longer', 'automata', 'modular']","['  The poor performance of transformers on arithmetic tasks seems to stem in\nlarge part from their inability to keep track of the exact position of each\ndigit inside of a large span of digits. We mend this problem by adding an\nembedding to each digit that encodes its position relative to the start of the\nnumber. In addition to the boost these embeddings provide on their own, we show\nthat this fix enables architectural modifications such as input injection and\nrecurrent layers to improve performance even further.\n  With positions resolved, we can study the logical extrapolation ability of\ntransformers. Can they solve arithmetic problems that are larger and more\ncomplex than those in their training data? We find that training on only 20\ndigit numbers with a single GPU for one day, we can reach state-of-the-art\nperformance, achieving up to 99% accuracy on 100 digit addition problems.\nFinally, we show that these gains in numeracy also unlock improvements on other\nmulti-step reasoning tasks including sorting and multiplication.\n', '  Despite the success of Transformers on language understanding, code\ngeneration, and logical reasoning, they still fail to generalize over length on\nbasic arithmetic tasks such as addition and multiplication. A major reason\nbehind this failure is the vast difference in structure between numbers and\ntext; For example, the numbers are typically parsed from right to left, and\nthere is a correspondence between digits at the same position across different\nnumbers. In contrast, for text, such symmetries are quite unnatural. In this\nwork, we propose to encode these semantics explicitly into the model via\nmodified number formatting and custom positional encodings. Empirically, our\nmethod allows a Transformer trained on numbers with at most 5-digits for\naddition and multiplication to generalize up to 50-digit numbers, without using\nadditional data for longer sequences. We further demonstrate that traditional\nabsolute positional encodings (APE) fail to generalize to longer sequences,\neven when trained with augmented data that captures task symmetries. To\nelucidate the importance of explicitly encoding structure, we prove that\nexplicit incorporation of structure via positional encodings is necessary for\nout-of-distribution generalization. Finally, we pinpoint other challenges\ninherent to length generalization beyond capturing symmetries, in particular\ncomplexity of the underlying task, and propose changes in the training\ndistribution to address them.\n', '  Length generalization refers to the ability to extrapolate from short\ntraining sequences to long test sequences and is a challenge for current large\nlanguage models. While prior work has proposed some architecture or data format\nchanges to achieve length generalization, these proposals typically apply to a\nlimited set of tasks. Building on prior scratchpad and Chain-of-Thought (CoT)\ntechniques, we propose Turing Programs, a novel CoT strategy that decomposes an\nalgorithmic task into steps mimicking the computation of a Turing Machine. This\nframework is both universal, as it can accommodate any algorithmic task, and\nsimple, requiring only copying text from the context with small modifications.\nWe show that by using Turing Programs, we obtain robust length generalization\non a range of algorithmic tasks: addition, multiplication and in-context SGD.\nWe then demonstrate that transformers achieve length generalization on random\nTuring Programs, suggesting that length generalization is possible for any\nalgorithmic task. Finally, we theoretically prove that transformers can\nimplement Turing Programs, constructing a simple RASP (Weiss et al.) program\nthat simulates an arbitrary Turing machine.\n']",Improving Transformers for Arithmetic and Algorithmic Tasks
268,267,32,267_fairness_discriminatory_graphpar_bias,"['fairness', 'discriminatory', 'graphpar', 'bias', 'networks', 'fairgb', 'biases', 'unfair', 'unfairness', 'fairinv']","['fairness', 'fair', 'sensitive', 'attributes', 'graph', 'groups', 'bias', 'attribute', 'node', 'group']","['discriminatory', 'graphpar', 'bias', 'fairgb', 'unfairness', 'fairmigration', 'datasets', 'fairsample', 'neighbors', 'neural']","['  Fairness-aware graph learning has gained increasing attention in recent\nyears. Nevertheless, there lacks a comprehensive benchmark to evaluate and\ncompare different fairness-aware graph learning methods, which blocks\npractitioners from choosing appropriate ones for broader real-world\napplications. In this paper, we present an extensive benchmark on ten\nrepresentative fairness-aware graph learning methods. Specifically, we design a\nsystematic evaluation protocol and conduct experiments on seven real-world\ndatasets to evaluate these methods from multiple perspectives, including group\nfairness, individual fairness, the balance between different fairness criteria,\nand computational efficiency. Our in-depth analysis reveals key insights into\nthe strengths and limitations of existing methods. Additionally, we provide\npractical guidance for applying fairness-aware graph learning methods in\napplications. To the best of our knowledge, this work serves as an initial step\ntowards comprehensively understanding representative fairness-aware graph\nlearning methods to facilitate future advancements in this area.\n', '  Graph neural networks (GNNs) have emerged as a powerful tool for analyzing\nand learning from complex data structured as graphs, demonstrating remarkable\neffectiveness in various applications, such as social network analysis,\nrecommendation systems, and drug discovery. However, despite their impressive\nperformance, the fairness problem has increasingly gained attention as a\ncrucial aspect to consider. Existing research in graph learning focuses on\neither group fairness or individual fairness. However, since each concept\nprovides unique insights into fairness from distinct perspectives, integrating\nthem into a fair graph neural network system is crucial. To the best of our\nknowledge, no study has yet to comprehensively tackle both individual and group\nfairness simultaneously. In this paper, we propose a new concept of individual\nfairness within groups and a novel framework named Fairness for Group and\nIndividual (FairGI), which considers both group fairness and individual\nfairness within groups in the context of graph learning. FairGI employs the\nsimilarity matrix of individuals to achieve individual fairness within groups,\nwhile leveraging adversarial learning to address group fairness in terms of\nboth Equal Opportunity and Statistical Parity. The experimental results\ndemonstrate that our approach not only outperforms other state-of-the-art\nmodels in terms of group fairness and individual fairness within groups, but\nalso exhibits excellent performance in population-level individual fairness,\nwhile maintaining comparable prediction accuracy.\n', '  Graph Neural Networks (GNNs) have been widely used for various types of graph\ndata processing and analytical tasks in different domains. Training GNNs over\ncentralized graph data can be infeasible due to privacy concerns and regulatory\nrestrictions. Thus, federated learning (FL) becomes a trending solution to\naddress this challenge in a distributed learning paradigm. However, as GNNs may\ninherit historical bias from training data and lead to discriminatory\npredictions, the bias of local models can be easily propagated to the global\nmodel in distributed settings. This poses a new challenge in mitigating bias in\nfederated GNNs. To address this challenge, we propose $\\text{F}^2$GNN, a Fair\nFederated Graph Neural Network, that enhances group fairness of federated GNNs.\nAs bias can be sourced from both data and learning algorithms, $\\text{F}^2$GNN\naims to mitigate both types of bias under federated settings. First, we provide\ntheoretical insights on the connection between data bias in a training graph\nand statistical fairness metrics of the trained GNN models. Based on the\ntheoretical analysis, we design $\\text{F}^2$GNN which contains two key\ncomponents: a fairness-aware local model update scheme that enhances group\nfairness of the local models on the client side, and a fairness-weighted global\nmodel update scheme that takes both data bias and fairness metrics of local\nmodels into consideration in the aggregation process. We evaluate\n$\\text{F}^2$GNN empirically versus a number of baseline methods, and\ndemonstrate that $\\text{F}^2$GNN outperforms these baselines in terms of both\nfairness and model accuracy.\n']",Fairness in Graph Neural Networks
269,268,32,268_solar_flare_geomagnetic_flares,"['solar', 'flare', 'geomagnetic', 'flares', 'solarcnn', 'sun', 'observatory', 'magnetogram', 'eclipse', 'magnetograms']","['solar', 'flares', 'flare', 'magnetic', 'magnetograms', 'coronal', 'weather', 'geomagnetic', 'regions', 'satellites']","['geomagnetic', 'flares', 'solarcnn', 'observatory', 'helioseismic', 'forecasting', 'imager', 'auroral', 'atmospheres', 'mars']","[""  Given the rarity of significant solar flares compared to smaller ones,\ntraining effective machine learning models for solar activity forecasting is\nchallenging due to insufficient data. This study proposes using generative deep\nlearning models, specifically a Denoising Diffusion Probabilistic Model (DDPM),\nto create synthetic images of solar phenomena, including flares of varying\nintensities. By employing a dataset from the AIA instrument aboard the SDO\nspacecraft, focusing on the 171 {\\AA} band that captures various solar\nactivities, and classifying images with GOES X-ray measurements based on flare\nintensity, we aim to address the data scarcity issue. The DDPM's performance is\nevaluated using cluster metrics, Frechet Inception Distance (FID), and\nF1-score, showcasing promising results in generating realistic solar imagery.\nWe conduct two experiments: one to train a supervised classifier for event\nidentification and another for basic flare prediction, demonstrating the value\nof synthetic data in managing imbalanced datasets. This research underscores\nthe potential of DDPMs in solar data analysis and forecasting, suggesting\nfurther exploration into their capabilities for solar flare prediction and\napplication in other deep learning and physical tasks.\n"", '  Solar flares, especially C, M, and X class, pose significant risks to\nsatellite operations, communication systems, and power grids. We present a\nnovel approach for predicting extreme solar flares using HMI intensitygrams and\nmagnetograms. By detecting sunspots from intensitygrams and extracting magnetic\nfield patches from magnetograms, we train a Residual Network (ResNet) to\nclassify extreme class flares. Our model demonstrates high accuracy, offering a\nrobust tool for predicting extreme solar flares and improving space weather\nforecasting. Additionally, we show that HMI magnetograms provide more useful\ndata for deep learning compared to other SDO AIA images by better capturing\nfeatures critical for predicting flare magnitudes. This study underscores the\nimportance of identifying magnetic fields in solar flare prediction, marking a\nsignificant advancement in solar activity prediction with practical\nimplications for mitigating space weather impacts.\n', ""  In this dataset we provide a comprehensive collection of magnetograms (images\nquantifying the strength of the magnetic field) from the National Aeronautics\nand Space Administration's (NASA's) Solar Dynamics Observatory (SDO). The\ndataset incorporates data from three sources and provides SDO Helioseismic and\nMagnetic Imager (HMI) magnetograms of solar active regions (regions of large\nmagnetic flux, generally the source of eruptive events) as well as labels of\ncorresponding flaring activity. This dataset will be useful for image analysis\nor solar physics research related to magnetic structure, its evolution over\ntime, and its relation to solar flares. The dataset will be of interest to\nthose researchers investigating automated solar flare prediction methods,\nincluding supervised and unsupervised machine learning (classical and deep),\nbinary and multi-class classification, and regression. This dataset is a\nminimally processed, user configurable dataset of consistently sized images of\nsolar active regions that can serve as a benchmark dataset for solar flare\nprediction research.\n""]",Solar Flare Prediction and Analysis
270,269,32,269_evaluations_evaluation_language_assessment,"['evaluations', 'evaluation', 'language', 'assessment', 'benchmarking', 'evaluating', 'benchmark', 'benchmarks', 'vocabulary', 'eval']","['evaluation', 'benchmarks', 'benchmark', 'leaderboard', 'instruction', 'capabilities', 'evaluations', 'turn', 'test', 'leakage']","['evaluations', 'assessment', 'benchmarking', 'tool', 'tests', 'instruction', 'rankings', 'lms', 'skill', 'benchbench']","['  Large language models (LLM) have achieved remarkable performance on various\nNLP tasks and are augmented by tools for broader applications. Yet, how to\nevaluate and analyze the tool-utilization capability of LLMs is still\nunder-explored. In contrast to previous works that evaluate models\nholistically, we comprehensively decompose the tool utilization into multiple\nsub-processes, including instruction following, planning, reasoning, retrieval,\nunderstanding, and review. Based on that, we further introduce T-Eval to\nevaluate the tool utilization capability step by step. T-Eval disentangles the\ntool utilization evaluation into several sub-domains along model capabilities,\nfacilitating the inner understanding of both holistic and isolated competency\nof LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of\nvarious LLMs. T-Eval not only exhibits consistency with the outcome-oriented\nevaluation but also provides a more fine-grained analysis of the capabilities\nof LLMs, providing a new perspective in LLM evaluation on tool-utilization\nability. The benchmark will be available at\nhttps://github.com/open-compass/T-Eval.\n', ""  In recent times, substantial advancements have been witnessed in large\nlanguage models (LLMs), exemplified by ChatGPT, showcasing remarkable\nproficiency across a range of complex tasks. However, many mainstream LLMs\n(e.g. LLaMA) are pretrained on English-dominant corpus, which limits their\nperformance in other non-English languages. In this paper, we focus on how to\neffectively transfer the capabilities of language generation and following\ninstructions to a non-English language. To answer this question, we conduct an\nextensive empirical investigation based on LLaMA, accumulating over 1440 GPU\nhours. We analyze the impact of key factors such as vocabulary extension,\nfurther pretraining, and instruction tuning on transfer. To accurately assess\nthe model's level of knowledge, we employ four widely used standardized testing\nbenchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a\ncomprehensive evaluation of the model's response quality is conducted,\nconsidering aspects such as accuracy, fluency, informativeness, logical\ncoherence, and harmlessness, based on LLM-Eval, a benchmarks consisting\ninstruction tasks from 17 diverse categories. Our evaluation results\ndemonstrate that comparable performance to state-of-the-art transfer models can\nbe achieved with less than 1% of the pretraining data, both in terms of\nknowledge alignment and response quality. Furthermore, the experimental\noutcomes across the thirteen low-resource languages also exhibit similar\ntrends. We anticipate that the conclusions revealed by the experiments will aid\nthe community in developing non-English LLMs.\n"", ""  Recent advancements in Language Models (LMs) have catalyzed the creation of\nmultiple benchmarks, designed to assess these models' general capabilities. A\ncrucial task, however, is assessing the validity of the benchmarks themselves.\nThis is most commonly done via Benchmark Agreement Testing (BAT), where new\nbenchmarks are validated against established ones using some agreement metric\n(e.g., rank correlation). Despite the crucial role of BAT for benchmark\nbuilders and consumers, there are no standardized procedures for such agreement\ntesting. This deficiency can lead to invalid conclusions, fostering mistrust in\nbenchmarks and upending the ability to properly choose the appropriate\nbenchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how\nsome overlooked methodological choices can significantly influence BAT results,\npotentially undermining the validity of conclusions. To address these\ninconsistencies, we propose a set of best practices for BAT and demonstrate how\nutilizing these methodologies greatly improves BAT robustness and validity. To\nfoster adoption and facilitate future research,, we introduce BenchBench, a\npython package for BAT, and release the BenchBench-leaderboard, a\nmeta-benchmark designed to evaluate benchmarks using their peers. Our findings\nunderscore the necessity for standardized BAT, ensuring the robustness and\nvalidity of benchmark evaluations in the evolving landscape of language model\nresearch.\n  BenchBench Package: https://github.com/IBM/BenchBench\n  Leaderboard: https://huggingface.co/spaces/per/BenchBench\n""]",Language Model Evaluation and Benchmarking
271,270,32,270_embeddings_memorization_neural_linguistic,"['embeddings', 'memorization', 'neural', 'linguistic', 'embedding', 'memorize', 'representations', 'memorized', 'memory', 'sentences']","['linguistic', 'memorization', 'memorized', 'layers', 'sentences', 'neurons', 'representations', 'concept', 'unmemorized', 'language']","['embeddings', 'memorize', 'sentences', 'neurons', 'attentional', 'encode', 'subnetworks', 'recurrent', 'bert', 'tokens']","['  Large Language Models (LLMs), trained on massive corpora with billions of\nparameters, show unprecedented performance in various fields. Though surprised\nby their excellent performances, researchers also noticed some special\nbehaviors of those LLMs. One of those behaviors is memorization, in which LLMs\ncan generate the same content used to train them. Though previous research has\ndiscussed memorization, the memorization of LLMs still lacks explanation,\nespecially the cause of memorization and the dynamics of generating them. In\nthis research, we comprehensively discussed memorization from various\nperspectives and extended the discussion scope to not only just the memorized\ncontent but also less and unmemorized content. Through various studies, we\nfound that: (1) Through experiments, we revealed the relation of memorization\nbetween model size, continuation size, and context size. Further, we showed how\nunmemorized sentences transition to memorized sentences. (2) Through embedding\nanalysis, we showed the distribution and decoding dynamics across model size in\nembedding space for sentences with different memorization scores. The n-gram\nstatistics analysis presents d (3) An analysis over n-gram and entropy decoding\ndynamics discovered a boundary effect when the model starts to generate\nmemorized sentences or unmemorized sentences. (4)We trained a Transformer model\nto predict the memorization of different models, showing that it is possible to\npredict memorizations by context.\n', ""  This study investigates how BERT processes and represents Argument Structure\nConstructions (ASCs), extending previous LSTM analyses. Using a dataset of 2000\nsentences across four ASC types (transitive, ditransitive, caused-motion,\nresultative), we analyzed BERT's token embeddings across 12 layers.\nVisualizations with MDS and t-SNE and clustering quantified by Generalized\nDiscrimination Value (GDV) were used. Feedforward classifiers (probes)\npredicted construction categories from embeddings. CLS token embeddings\nclustered best in layers 2-4, decreased in intermediate layers, and slightly\nincreased in final layers. DET and SUBJ embeddings showed consistent clustering\nin intermediate layers, VERB embeddings increased in clustering from layer 1 to\n12, and OBJ embeddings peaked in layer 10. Probe accuracies indicated low\nconstruction information in layer 1, with over 90 percent accuracy from layer 2\nonward, revealing latent construction information beyond GDV clustering. Fisher\nDiscriminant Ratio (FDR) analysis of attention weights showed OBJ tokens were\ncrucial for differentiating ASCs, followed by VERB and DET tokens. SUBJ, CLS,\nand SEP tokens had insignificant FDR scores. This study highlights BERT's\nlayered processing of linguistic constructions and its differences from LSTMs.\nFuture research will compare these findings with neuroimaging data to\nunderstand the neural correlates of ASC processing. This research underscores\nneural language models' potential to mirror linguistic processing in the human\nbrain, offering insights into the computational and neural mechanisms\nunderlying language understanding.\n"", ""  Understanding how language and linguistic constructions are processed in the\nbrain is a fundamental question in cognitive computational neuroscience. In\nthis study, we explore the representation and processing of Argument Structure\nConstructions (ASCs) in a recurrent neural language model. We trained a Long\nShort-Term Memory (LSTM) network on a custom-made dataset consisting of 2000\nsentences, generated using GPT-4, representing four distinct ASCs: transitive,\nditransitive, caused-motion, and resultative constructions.\n  We analyzed the internal activations of the LSTM model's hidden layers using\nMultidimensional Scaling (MDS) and t-Distributed Stochastic Neighbor Embedding\n(t-SNE) to visualize the sentence representations. The Generalized\nDiscrimination Value (GDV) was calculated to quantify the degree of clustering\nwithin these representations. Our results show that sentence representations\nform distinct clusters corresponding to the four ASCs across all hidden layers,\nwith the most pronounced clustering observed in the last hidden layer before\nthe output layer. This indicates that even a relatively simple,\nbrain-constrained recurrent neural network can effectively differentiate\nbetween various construction types.\n  These findings are consistent with previous studies demonstrating the\nemergence of word class and syntax rule representations in recurrent language\nmodels trained on next word prediction tasks. In future work, we aim to\nvalidate these results using larger language models and compare them with\nneuroimaging data obtained during continuous speech perception. This study\nhighlights the potential of recurrent neural language models to mirror\nlinguistic processing in the human brain, providing valuable insights into the\ncomputational and neural mechanisms underlying language understanding.\n""]",Neural Language Models and Memorization
272,271,32,271_querysets_queries_relational_entities,"['querysets', 'queries', 'relational', 'entities', 'entity', 'query2gmm', 'reasoninglm', 'query', 'relations', 'databases']","['query', 'queries', 'logical', 'answering', 'reasoning', 'hop', 'graphs', 'entities', 'graph', 'answer']","['querysets', 'relational', 'query2gmm', 'embedding', 'subgraphs', 'ngdbs', 'facts', 'tutorqa', 'ultraquery', 'hop']","['  Answering logical queries on knowledge graphs (KG) poses a significant\nchallenge for machine reasoning. The primary obstacle in this task stems from\nthe inherent incompleteness of KGs. Existing research has predominantly focused\non addressing the issue of missing edges in KGs, thereby neglecting another\naspect of incompleteness: the emergence of new entities. Furthermore, most of\nthe existing methods tend to reason over each logical operator separately,\nrather than comprehensively analyzing the query as a whole during the reasoning\nprocess. In this paper, we propose a query-aware prompt-fused framework named\nPro-QE, which could incorporate existing query embedding methods and address\nthe embedding of emerging entities through contextual information aggregation.\nAdditionally, a query prompt, which is generated by encoding the symbolic\nquery, is introduced to gather information relevant to the query from a\nholistic perspective. To evaluate the efficacy of our model in the inductive\nsetting, we introduce two new challenging benchmarks. Experimental results\ndemonstrate that our model successfully handles the issue of unseen entities in\nlogical queries. Furthermore, the ablation study confirms the efficacy of the\naggregator and prompt components.\n', '  Reasoning over knowledge graphs (KGs) is a challenging task that requires a\ndeep understanding of the complex relationships between entities and the\nunderlying logic of their relations. Current approaches rely on learning\ngeometries to embed entities in vector space for logical query operations, but\nthey suffer from subpar performance on complex queries and dataset-specific\nrepresentations. In this paper, we propose a novel decoupled approach,\nLanguage-guided Abstract Reasoning over Knowledge graphs (LARK), that\nformulates complex KG reasoning as a combination of contextual KG search and\nlogical query reasoning, to leverage the strengths of graph extraction\nalgorithms and large language models (LLM), respectively. Our experiments\ndemonstrate that the proposed approach outperforms state-of-the-art KG\nreasoning methods on standard benchmark datasets across several logical query\nconstructs, with significant performance gain for queries of higher complexity.\nFurthermore, we show that the performance of our approach improves\nproportionally to the increase in size of the underlying LLM, enabling the\nintegration of the latest advancements in LLMs for logical reasoning over KGs.\nOur work presents a new direction for addressing the challenges of complex KG\nreasoning and paves the way for future research in this area.\n', '  Complex logical query answering is a challenging task in knowledge graphs\n(KGs) that has been widely studied. The ability to perform complex logical\nreasoning is essential and supports various graph reasoning-based downstream\ntasks, such as search engines. Recent approaches are proposed to represent KG\nentities and logical queries into embedding vectors and find answers to logical\nqueries from the KGs. However, existing proposed methods mainly focus on\nquerying a single KG and cannot be applied to multiple graphs. In addition,\ndirectly sharing KGs with sensitive information may incur privacy risks, making\nit impractical to share and construct an aggregated KG for reasoning to\nretrieve query answers. Thus, it remains unknown how to answer queries on\nmulti-source KGs. An entity can be involved in various knowledge graphs and\nreasoning on multiple KGs and answering complex queries on multi-source KGs is\nimportant in discovering knowledge cross graphs. Fortunately, federated\nlearning is utilized in knowledge graphs to collaboratively learn\nrepresentations with privacy preserved. Federated knowledge graph embeddings\nenrich the relations in knowledge graphs to improve the representation quality.\nHowever, these methods only focus on one-hop relations and cannot perform\ncomplex reasoning tasks. In this paper, we apply federated learning to complex\nquery-answering tasks to reason over multi-source knowledge graphs while\npreserving privacy. We propose a Federated Complex Query Answering framework\n(FedCQA), to reason over multi-source KGs avoiding sensitive raw data\ntransmission to protect privacy. We conduct extensive experiments on three\nreal-world datasets and evaluate retrieval performance on various types of\ncomplex queries.\n']",Reasoning over Knowledge Graphs
273,272,32,272_survival_predicting_prediction_predict,"['survival', 'predicting', 'prediction', 'predict', 'predictive', 'cox', 'predictions', 'hazards', 'outcomes', 'survmixclust']","['survival', 'event', 'censoring', 'death', 'time', 'analysis', 'markers', 'concordance', 'parametric', 'hazards']","['predicting', 'cox', 'hazards', 'outcomes', 'survmixclust', 'censoring', 'interpretability', 'churn', 'torchsurv', 'forests']","[""  Scoring systems are highly interpretable and widely used to evaluate\ntime-to-event outcomes in healthcare research. However, existing time-to-event\nscores are predominantly created ad-hoc using a few manually selected variables\nbased on clinician's knowledge, suggesting an unmet need for a robust and\nefficient generic score-generating method.\n  AutoScore was previously developed as an interpretable machine learning score\ngenerator, integrated both machine learning and point-based scores in the\nstrong discriminability and accessibility. We have further extended it to\ntime-to-event data and developed AutoScore-Survival, for automatically\ngenerating time-to-event scores with right-censored survival data. Random\nsurvival forest provides an efficient solution for selecting variables, and Cox\nregression was used for score weighting. We illustrated our method in a\nreal-life study of 90-day mortality of patients in intensive care units and\ncompared its performance with survival models (i.e., Cox) and the random\nsurvival forest.\n  The AutoScore-Survival-derived scoring model was more parsimonious than\nsurvival models built using traditional variable selection methods (e.g.,\npenalized likelihood approach and stepwise variable selection), and its\nperformance was comparable to survival models using the same set of variables.\nAlthough AutoScore-Survival achieved a comparable integrated area under the\ncurve of 0.782 (95% CI: 0.767-0.794), the integer-valued time-to-event scores\ngenerated are favorable in clinical applications because they are easier to\ncompute and interpret.\n  Our proposed AutoScore-Survival provides an automated, robust and easy-to-use\nmachine learning-based clinical score generator to studies of time-to-event\noutcomes. It provides a systematic guideline to facilitate the future\ndevelopment of time-to-event scores for clinical applications.\n"", '  Survival analysis stands as a pivotal process in cancer treatment research,\ncrucial for predicting patient survival rates accurately. Recent advancements\nin data collection techniques have paved the way for enhancing survival\npredictions by integrating information from multiple modalities. However,\nreal-world scenarios often present challenges with incomplete data,\nparticularly when dealing with censored survival labels. Prior works have\naddressed missing modalities but have overlooked incomplete labels, which can\nintroduce bias and limit model efficacy. To bridge this gap, we introduce a\nnovel framework that simultaneously handles incomplete data across modalities\nand censored survival labels. Our approach employs advanced foundation models\nto encode individual modalities and align them into a universal representation\nspace for seamless fusion. By generating pseudo labels and incorporating\nuncertainty, we significantly enhance predictive accuracy. The proposed method\ndemonstrates outstanding prediction accuracy in two survival analysis tasks on\nboth employed datasets. This innovative approach overcomes limitations\nassociated with disparate modalities and improves the feasibility of\ncomprehensive survival analysis using multiple large foundation models.\n', '  Kernel survival analysis models estimate individual survival distributions\nwith the help of a kernel function, which measures the similarity between any\ntwo data points. Such a kernel function can be learned using deep kernel\nsurvival models. In this paper, we present a new deep kernel survival model\ncalled a survival kernet, which scales to large datasets in a manner that is\namenable to model interpretation and also theoretical analysis. Specifically,\nthe training data are partitioned into clusters based on a recently developed\ntraining set compression scheme for classification and regression called kernel\nnetting that we extend to the survival analysis setting. At test time, each\ndata point is represented as a weighted combination of these clusters, and each\nsuch cluster can be visualized. For a special case of survival kernets, we\nestablish a finite-sample error bound on predicted survival distributions that\nis, up to a log factor, optimal. Whereas scalability at test time is achieved\nusing the aforementioned kernel netting compression strategy, scalability\nduring training is achieved by a warm-start procedure based on tree ensembles\nsuch as XGBoost and a heuristic approach to accelerating neural architecture\nsearch. On four standard survival analysis datasets of varying sizes (up to\nroughly 3 million data points), we show that survival kernets are highly\ncompetitive compared to various baselines tested in terms of time-dependent\nconcordance index. Our code is available at:\nhttps://github.com/georgehc/survival-kernets\n']",Survival Prediction in Healthcare
274,273,31,273_morality_moral_ethical_ethics,"['morality', 'moral', 'ethical', 'ethics', 'morally', 'ai', 'persuasion', 'judgments', 'unethical', 'judgment']","['moral', 'ethical', 'judgments', 'ethics', 'morality', 'values', 'alignment', 'defeasible', 'value', 'judgment']","['morality', 'ethics', 'ai', 'persuasion', 'judgments', 'languages', 'responses', 'dishonest', 'cultural', 'values']","[""  In this essay, I argue that explicit ethical machines, whose moral principles\nare inferred through a bottom-up approach, are unable to replicate human-like\nmoral reasoning and cannot be considered moral agents. By utilizing Alan\nTuring's theory of computation, I demonstrate that moral reasoning is\ncomputationally intractable by these machines due to the halting problem. I\naddress the frontiers of machine ethics by formalizing moral problems into\n'algorithmic moral questions' and by exploring moral psychology's dual-process\nmodel. While the nature of Turing Machines theoretically allows artificial\nagents to engage in recursive moral reasoning, critical limitations are\nintroduced by the halting problem, which states that it is impossible to\npredict with certainty whether a computational process will halt. A thought\nexperiment involving a military drone illustrates this issue, showing that an\nartificial agent might fail to decide between actions due to the halting\nproblem, which limits the agent's ability to make decisions in all instances,\nundermining its moral agency.\n"", '  As large language models (LLMs) are deployed in more and more real-world\nsituations, it is crucial to understand their decision-making when faced with\nmoral dilemmas. Inspired by a large-scale cross-cultural study of human moral\npreferences, ""The Moral Machine Experiment"", we set up the same set of moral\nchoices for LLMs. We translate 1K vignettes of moral dilemmas, parametrically\nvaried across key axes, into 100+ languages, and reveal the preferences of LLMs\nin each of these languages. We then compare the responses of LLMs to that of\nhuman speakers of those languages, harnessing a dataset of 40 million human\nmoral judgments. We discover that LLMs are more aligned with human preferences\nin languages such as English, Korean, Hungarian, and Chinese, but less aligned\nin languages such as Hindi and Somali (in Africa). Moreover, we characterize\nthe explanations LLMs give for their moral choices and find that fairness is\nthe most dominant supporting reason behind GPT-4\'s decisions and utilitarianism\nby GPT-3. We also discover ""language inequality"" (which we define as the\nmodel\'s different development levels in different languages) in a series of\nmeta-properties of moral decision making.\n', '  Making moral judgments is an essential step toward developing ethical AI\nsystems. Prevalent approaches are mostly implemented in a bottom-up manner,\nwhich uses a large set of annotated data to train models based on crowd-sourced\nopinions about morality. These approaches have been criticized for\novergeneralizing the moral stances of a limited group of annotators and lacking\nexplainability. This work proposes a flexible top-down framework to steer\n(Large) Language Models (LMs) to perform moral reasoning with well-established\nmoral theories from interdisciplinary research. The theory-guided top-down\nframework can incorporate various moral theories. Our experiments demonstrate\nthe effectiveness of the proposed framework on datasets derived from moral\ntheories. Furthermore, we show the alignment between different moral theories\nand existing morality datasets. Our analysis exhibits the potential and flaws\nin existing resources (models and datasets) in developing explainable moral\njudgment-making systems.\n']",Artificial Intelligence and Moral Reasoning
275,274,31,274_ensemble_ensembles_ensembling_diversity,"['ensemble', 'ensembles', 'ensembling', 'diversity', 'classification', 'trained', 'fusionshot', 'fusion', 'training', 'mixmax']","['ensembles', 'ensemble', 'averaging', 'diversity', 'basin', 'soups', 'weight', 'base', 'learners', 'weights']","['ensemble', 'diversity', 'fusionshot', 'mixmax', 'accuracy', 'learners', 'bagging', 'imbalanced', 'soups', 'basin']","['  This paper presents FusionShot, a focal diversity optimized few-shot ensemble\nlearning approach for boosting the robustness and generalization performance of\npre-trained few-shot models. The paper makes three original contributions.\nFirst, we explore the unique characteristics of few-shot learning to ensemble\nmultiple few-shot (FS) models by creating three alternative fusion channels.\nSecond, we introduce the concept of focal error diversity to learn the most\nefficient ensemble teaming strategy, rather than assuming that an ensemble of a\nlarger number of base models will outperform those sub-ensembles of smaller\nsize. We develop a focal-diversity ensemble pruning method to effectively prune\nout the candidate ensembles with low ensemble error diversity and recommend\ntop-$K$ FS ensembles with the highest focal error diversity. Finally, we\ncapture the complex non-linear patterns of ensemble few-shot predictions by\ndesigning the learn-to-combine algorithm, which can learn the diverse weight\nassignments for robust ensemble fusion over different member models. Extensive\nexperiments on representative few-shot benchmarks show that the top-K ensembles\nrecommended by FusionShot can outperform the representative SOTA few-shot\nmodels on novel tasks (different distributions and unknown at training), and\ncan prevail over existing few-shot learners in both cross-domain settings and\nadversarial settings. For reproducibility purposes, FusionShot trained models,\nresults, and code are made available at https://github.com/sftekin/fusionshot\n', '  The performance of deep neural networks is enhanced by ensemble methods,\nwhich average the output of several models. However, this comes at an increased\ncost at inference. Weight averaging methods aim at balancing the generalization\nof ensembling and the inference speed of a single model by averaging the\nparameters of an ensemble of models. Yet, naive averaging results in poor\nperformance as models converge to different loss basins, and aligning the\nmodels to improve the performance of the average is challenging. Alternatively,\ninspired by distributed training, methods like DART and PAPA have been proposed\nto train several models in parallel such that they will end up in the same\nbasin, resulting in good averaging accuracy. However, these methods either\ncompromise ensembling accuracy or demand significant communication between\nmodels during training. In this paper, we introduce WASH, a novel distributed\nmethod for training model ensembles for weight averaging that achieves\nstate-of-the-art image classification accuracy. WASH maintains models within\nthe same basin by randomly shuffling a small percentage of weights during\ntraining, resulting in diverse models and lower communication costs compared to\nstandard parameter averaging methods.\n', '  Classic results establish that encouraging predictive diversity improves\nperformance in ensembles of low-capacity models, e.g. through bagging or\nboosting. Here we demonstrate that these intuitions do not apply to\nhigh-capacity neural network ensembles (deep ensembles), and in fact the\nopposite is often true. In a large scale study of nearly 600 neural network\nclassification ensembles, we examine a variety of interventions that trade off\ncomponent model performance for predictive diversity. While such interventions\ncan improve the performance of small neural network ensembles (in line with\nstandard intuitions), they harm the performance of the large neural network\nensembles most often used in practice. Surprisingly, we also find that\ndiscouraging predictive diversity is often benign in large-network ensembles,\nfully inverting standard intuitions. Even when diversity-promoting\ninterventions do not sacrifice component model performance (e.g. using\nheterogeneous architectures and training paradigms), we observe an opportunity\ncost associated with pursuing increased predictive diversity. Examining over\n1000 ensembles, we observe that the performance benefits of diverse\narchitectures/training procedures are easily dwarfed by the benefits of simply\nusing higher-capacity models, despite the fact that such higher capacity models\noften yield significantly less predictive diversity. Overall, our findings\ndemonstrate that standard intuitions around predictive diversity, originally\ndeveloped for low-capacity ensembles, do not directly apply to modern\nhigh-capacity deep ensembles. This work clarifies fundamental challenges to the\ngoal of improving deep ensembles by making them more diverse, while suggesting\nan alternative path: simply forming ensembles from ever more powerful (and less\ndiverse) component models.\n']",Ensemble Methods for Deep Learning
276,275,31,275_gestures_gestureprint_gesture_touch,"['gestures', 'gestureprint', 'gesture', 'touch', 'touchpad', 'multimodal', 'hands', 'hand', 'recognition', 'classifiers']","['gesture', 'gestures', 'hand', 'recognition', 'electromyography', 'movement', 'muscle', 'tap', 'handwashing', 'forearm']","['gestureprint', 'touchpad', 'multimodal', 'classifiers', 'taps', 'sdcnn', 'handwashing', 'sensor', 'electromyographic', 'walking']","[""  Hand gestures can provide a natural means of human-computer interaction and\nenable people who cannot speak to communicate efficiently. Existing hand\ngesture recognition methods heavily depend on pre-defined gestures, however,\nmotor-impaired individuals require new gestures tailored to each individual's\ngesture motion and style. Gesture samples collected from different persons have\ndistribution shifts due to their health conditions, the severity of the\ndisability, motion patterns of the arms, etc. In this paper, we introduce the\nLatent Embedding Exploitation (LEE) mechanism in our replay-based Few-Shot\nContinual Learning (FSCL) framework that significantly improves the performance\nof fine-tuning a model for out-of-distribution data. Our method produces a\ndiversified latent feature space by leveraging a preserved latent embedding\nknown as gesture prior knowledge, along with intra-gesture divergence derived\nfrom two additional embeddings. Thus, the model can capture latent statistical\nstructure in highly variable gestures with limited samples. We conduct an\nexperimental evaluation using the SmartWatch Gesture and the Motion Gesture\ndatasets. The proposed method results in an average test accuracy of 57.0%,\n64.6%, and 69.3% by using one, three, and five samples for six different\ngestures. Our method helps motor-impaired persons leverage wearable devices,\nand their unique styles of movement can be learned and applied in\nhuman-computer interaction and social communication. Code is available at:\nhttps://github.com/riyadRafiq/wearable-latent-embedding-exploitation\n"", '  As robots are expected to get more involved in people\'s everyday lives,\nframeworks that enable intuitive user interfaces are in demand. Hand gesture\nrecognition systems provide a natural way of communication and, thus, are an\nintegral part of seamless Human-Robot Interaction (HRI). Recent years have\nwitnessed an immense evolution of computational models powered by deep\nlearning. However, state-of-the-art models fall short in expanding across\ndifferent gesture domains, such as emblems and co-speech. In this paper, we\npropose a novel hybrid hand gesture recognition system. Our architecture\nenables learning both static and dynamic gestures: by capturing a so-called\n""snapshot"" of the gesture performance at its peak, we integrate the hand pose\nalong with the dynamic movement. Moreover, we present a method for analyzing\nthe motion profile of a gesture to uncover its dynamic characteristics and\nwhich allows regulating a static channel based on the amount of motion. Our\nevaluation demonstrates the superiority of our approach on two gesture\nbenchmarks compared to a CNNLSTM baseline. We also provide an analysis on a\ngesture class basis that unveils the potential of our Snapture architecture for\nperformance improvements. Thanks to its modular implementation, our framework\nallows the integration of other multimodal data like facial expressions and\nhead tracking, which are important cues in HRI scenarios, into one\narchitecture. Thus, our work contributes both to gesture recognition research\nand machine learning applications for non-verbal communication with robots.\n', '  Artificial intelligence (AI) has made significant advances in recent years\nand opened up new possibilities in exploring applications in various fields\nsuch as biomedical, robotics, education, industry, etc. Among these fields,\nhuman hand gesture recognition is a subject of study that has recently emerged\nas a research interest in robotic hand control using electromyography (EMG).\nSurface electromyography (sEMG) is a primary technique used in EMG, which is\npopular due to its non-invasive nature and is used to capture gesture movements\nusing signal acquisition devices placed on the surface of the forearm.\nMoreover, these signals are pre-processed to extract significant handcrafted\nfeatures through time and frequency domain analysis. These are helpful and act\nas input to machine learning (ML) models to identify hand gestures. However,\nhandling multiple classes and biases are major limitations that can affect the\nperformance of an ML model. Therefore, to address this issue, a new mixture of\nexperts extra tree (MEET) model is proposed to identify more accurate and\neffective hand gesture movements. This model combines individual ML models\nreferred to as experts, each focusing on a minimal class of two. Moreover, a\nfully trained model known as the gate is employed to weigh the output of\nindividual expert models. This amalgamation of the expert models with the gate\nmodel is known as a mixture of experts extra tree (MEET) model. In this study,\nfour subjects with six hand gesture movements have been considered and their\nidentification is evaluated among eleven models, including the MEET classifier.\nResults elucidate that the MEET classifier performed best among other\nalgorithms and identified hand gesture movement accurately.\n']",Hand Gesture Recognition
277,276,30,276_recipes_cuisine_foods_recipe,"['recipes', 'cuisine', 'foods', 'recipe', 'recipemc', 'chef', 'food', 'culinary', 'meals', 'meal']","['food', 'recipes', 'recipe', 'meal', 'ingredients', 'flavor', 'cooking', 'nutritional', 'culinary', 'ingredient']","['recipes', 'cuisine', 'recipemc', 'foodlearner', 'cookingsense', 'foodsky', 'flavorgraph', 'dietetic', 'curated', 'nutribench']","[""  Large Multi-modal Models (LMMs) have significantly advanced a variety of\nvision-language tasks. The scalability and availability of high-quality\ntraining data play a pivotal role in the success of LMMs. In the realm of food,\nwhile comprehensive food datasets such as Recipe1M offer an abundance of\ningredient and recipe information, they often fall short of providing ample\ndata for nutritional analysis. The Recipe1M+ dataset, despite offering a subset\nfor nutritional evaluation, is limited in the scale and accuracy of nutrition\ninformation. To bridge this gap, we introduce Uni-Food, a unified food dataset\nthat comprises over 100,000 images with various food labels, including\ncategories, ingredients, recipes, and ingredient-level nutritional information.\nUni-Food is designed to provide a more holistic approach to food data analysis,\nthereby enhancing the performance and capabilities of LMMs in this domain. To\nmitigate the conflicts arising from multi-task supervision during fine-tuning\nof LMMs, we introduce a novel Linear Rectification Mixture of Diverse Experts\n(RoDE) approach. RoDE utilizes a diverse array of experts to address tasks of\nvarying complexity, thereby facilitating the coordination of trainable\nparameters, i.e., it allocates more parameters for more complex tasks and,\nconversely, fewer parameters for simpler tasks. RoDE implements linear\nrectification union to refine the router's functionality, thereby enhancing the\nefficiency of sparse task allocation. These design choices endow RoDE with\nfeatures that ensure GPU memory efficiency and ease of optimization. Our\nexperimental results validate the effectiveness of our proposed approach in\naddressing the inherent challenges of food-related multitasking.\n"", '  In the early 2000s, renowned chef Heston Blumenthal formulated his ""food\npairing"" hypothesis, positing that if foods share many flavor compounds, then\nthey tend to taste good when eaten together. In 2011, Ahn et al. conducted a\nstudy using a dataset of recipes, ingredients, and flavor compounds, finding\nthat, in Western cuisine, ingredients in recipes often share more flavor\ncompounds than expected by chance, indicating a natural tendency towards food\npairing. Building upon Ahn\'s research, our work applies state-of-the-art\ncollaborative filtering techniques to the dataset, providing a tool that can\nrecommend new foods to add in recipes, retrieve missing ingredients and advise\nagainst certain combinations. We create our recommender in two ways, by taking\ninto account ingredients appearances in recipes or shared flavor compounds\nbetween foods. While our analysis confirms the existence of food pairing, the\nrecipe-based recommender performs significantly better than the flavor-based\none, leading to the conclusion that food pairing is just one of the principles\nto take into account when creating recipes. Furthermore, and more\ninterestingly, we find that food pairing in data is mostly due to trivial\ncouplings of very similar ingredients, leading to a reconsideration of its\ncurrent role in recipes, from being an already existing feature to a key to\nopen up new scenarios in gastronomy. Our flavor-based recommender can thus\nleverage this novel concept and provide a new tool to lead culinary innovation.\n', '  Food computing has emerged as a prominent multidisciplinary field of research\nin recent years. An ambitious goal of food computing is to develop end-to-end\nintelligent systems capable of autonomously producing recipe information for a\nfood image. Current image-to-recipe methods are retrieval-based and their\nsuccess depends heavily on the dataset size and diversity, as well as the\nquality of learned embeddings. Meanwhile, the emergence of powerful\nattention-based vision and language models presents a promising avenue for\naccurate and generalizable recipe generation, which has yet to be extensively\nexplored. This paper proposes FIRE, a novel multimodal methodology tailored to\nrecipe generation in the food computing domain, which generates the food title,\ningredients, and cooking instructions based on input food images. FIRE\nleverages the BLIP model to generate titles, utilizes a Vision Transformer with\na decoder for ingredient extraction, and employs the T5 model to generate\nrecipes incorporating titles and ingredients as inputs. We showcase two\npractical applications that can benefit from integrating FIRE with large\nlanguage model prompting: recipe customization to fit recipes to user\npreferences and recipe-to-code transformation to enable automated cooking\nprocesses. Our experimental findings validate the efficacy of our proposed\napproach, underscoring its potential for future advancements and widespread\nadoption in food computing.\n']",Food and Recipe Analysis
278,277,30,277_gans_inception_gan_dcgan,"['gans', 'inception', 'gan', 'dcgan', 'cyclegan', 'generative', 'echogan', 'adversarial', 'skullgan', 'imaging']","['medical', 'ultrasound', 'images', 'imaging', 'segmentation', 'image', 'synthetic', 'skull', 'generative', 'adversarial']","['gans', 'inception', 'dcgan', 'echogan', 'adversarial', 'mris', 'volumetric', 'segmentation', 'scan', 'augment']","['  Ultrasound imaging is pivotal in various medical diagnoses due to its\nnon-invasive nature and safety. In clinical practice, the accuracy and\nprecision of ultrasound image analysis are critical. Recent advancements in\ndeep learning are showing great capacity of processing medical images. However,\nthe data hungry nature of deep learning and the shortage of high-quality\nultrasound image training data suppress the development of deep learning based\nultrasound analysis methods. To address these challenges, we introduce an\nadvanced deep learning model, dubbed S-CycleGAN, which generates high-quality\nsynthetic ultrasound images from computed tomography (CT) data. This model\nincorporates semantic discriminators within a CycleGAN framework to ensure that\ncritical anatomical details are preserved during the style transfer process.\nThe synthetic images produced are used to augment training datasets for\nsemantic segmentation models and robot-assisted ultrasound scanning system\ndevelopment, enhancing their ability to accurately parse real ultrasound\nimagery.\n', '  Large annotated datasets are required for training deep learning models, but\nin medical imaging data sharing is often complicated due to ethics,\nanonymization and data protection legislation. Generative AI models, such as\ngenerative adversarial networks (GANs) and diffusion models, can today produce\nvery realistic synthetic images, and can potentially facilitate data sharing.\nHowever, in order to share synthetic medical images it must first be\ndemonstrated that they can be used for training different networks with\nacceptable performance. Here, we therefore comprehensively evaluate four GANs\n(progressive GAN, StyleGAN 1-3) and a diffusion model for the task of brain\ntumor segmentation (using two segmentation networks, U-Net and a Swin\ntransformer). Our results show that segmentation networks trained on synthetic\nimages reach Dice scores that are 80% - 90% of Dice scores when training with\nreal images, but that memorization of the training images can be a problem for\ndiffusion models if the original dataset is too small. Our conclusion is that\nsharing synthetic medical images is a viable option to sharing real images, but\nthat further work is required. The trained generative models and the generated\nsynthetic images are shared on AIDA data hub\n', '  Medical imaging is an essential tool for diagnosing and treating diseases.\nHowever, lacking medical images can lead to inaccurate diagnoses and\nineffective treatments. Generative models offer a promising solution for\naddressing medical image shortage problems due to their ability to generate new\ndata from existing datasets and detect anomalies in this data. Data\naugmentation with position augmentation methods like scaling, cropping,\nflipping, padding, rotation, and translation could lead to more overfitting in\ndomains with little data, such as medical image data. This paper proposes the\nGAN-GA, a generative model optimized by embedding a genetic algorithm. The\nproposed model enhances image fidelity and diversity while preserving\ndistinctive features. The proposed medical image synthesis approach improves\nthe quality and fidelity of medical images, an essential aspect of image\ninterpretation. To evaluate synthesized images: Frechet Inception Distance\n(FID) is used. The proposed GAN-GA model is tested by generating Acute\nlymphoblastic leukemia (ALL) medical images, an image dataset, and is the first\ntime to be used in generative models. Our results were compared to those of\nInfoGAN as a baseline model. The experimental results show that the proposed\noptimized GAN-GA enhances FID scores by about 6.8\\%, especially in earlier\ntraining epochs. The source code and dataset will be available at:\nhttps://github.com/Mustafa-AbdulRazek/InfoGAN-GA.\n']",Generative Models for Medical Imaging
279,278,30,278_cloud_virtualization_scheduling_workloads,"['cloud', 'virtualization', 'scheduling', 'workloads', 'workload', 'microservices', 'microservice', 'ai', 'supercomputing', 'service']","['cloud', 'service', 'computing', 'job', 'scheduling', 'microservice', 'microservices', 'resource', 'migration', 'resources']","['virtualization', 'workload', 'microservices', 'aws', 'provisioner', 'infrastructure', 'autoscaling', 'serverless', 'ibm', 'sla']","['  With the continuous expansion of the scale of cloud computing applications,\nartificial intelligence technologies such as Deep Learning and Reinforcement\nLearning have gradually become the key tools to solve the automated task\nscheduling of large-scale cloud computing systems. Aiming at the complexity and\nreal-time requirement of task scheduling in large-scale cloud computing system,\nthis paper proposes an automatic task scheduling scheme based on deep learning\nand reinforcement learning. Firstly, the deep learning technology is used to\nmonitor and predict the parameters in the cloud computing system in real time\nto obtain the system status information. Then, combined with reinforcement\nlearning algorithm, the task scheduling strategy is dynamically adjusted\naccording to the real-time system state and task characteristics to achieve the\noptimal utilization of system resources and the maximum of task execution\nefficiency. This paper verifies the effectiveness and performance advantages of\nthe proposed scheme in experiments, and proves the potential and application\nprospect of deep learning and reinforcement learning in automatic task\nscheduling in large-scale cloud computing systems.\n', '  The paragraph is grammatically correct and logically coherent. It discusses\nthe importance of mobile terminal cloud computing migration technology in\nmeeting the demands of evolving computer and cloud computing technologies. It\nemphasizes the need for efficient data access and storage, as well as the\nutilization of cloud computing migration technology to prevent additional time\ndelays. The paragraph also highlights the contributions of cloud computing\nmigration technology to expanding cloud computing services. Additionally, it\nacknowledges the role of virtualization as a fundamental capability of cloud\ncomputing while emphasizing that cloud computing and virtualization are not\ninherently interconnected. Finally, it introduces machine learning-based\nvirtual machine migration optimization and dynamic resource allocation as a\ncritical research direction in cloud computing, citing the limitations of\nstatic rules or manual settings in traditional cloud computing environments.\nOverall, the paragraph effectively communicates the importance of machine\nlearning technology in addressing resource allocation and virtual machine\nmigration challenges in cloud computing.\n', '  In recent years, cloud computing has been widely used. Cloud computing refers\nto the centralized computing resources, users through the access to the\ncentralized resources to complete the calculation, the cloud computing center\nwill return the results of the program processing to the user. Cloud computing\nis not only for individual users, but also for enterprise users. By purchasing\na cloud server, users do not have to buy a large number of computers, saving\ncomputing costs. According to a report by China Economic News Network, the\nscale of cloud computing in China has reached 209.1 billion yuan. At present,\nthe more mature cloud service providers in China are Ali Cloud, Baidu Cloud,\nHuawei Cloud and so on. Therefore, this paper proposes an innovative approach\nto solve complex problems in cloud computing resource scheduling and management\nusing machine learning optimization techniques. Through in-depth study of\nchallenges such as low resource utilization and unbalanced load in the cloud\nenvironment, this study proposes a comprehensive solution, including\noptimization methods such as deep learning and genetic algorithm, to improve\nsystem performance and efficiency, and thus bring new breakthroughs and\nprogress in the field of cloud computing resource management.Rational\nallocation of resources plays a crucial role in cloud computing. In the\nresource allocation of cloud computing, the cloud computing center has limited\ncloud resources, and users arrive in sequence. Each user requests the cloud\ncomputing center to use a certain number of cloud resources at a specific time.\n']",Cloud Computing Resource Management and Optimization
280,279,30,279_gps_accelerometer_gyroscope_inertial,"['gps', 'accelerometer', 'gyroscope', 'inertial', 'sensor', 'sensors', 'navigation', 'satellite', 'tracking', 'vehicle']","['inertial', 'urban', 'road', 'navigation', 'magnetic', 'angle', 'satellite', 'sensor', 'orientation', 'gyroscope']","['gps', 'accelerometer', 'gyroscope', 'inertial', 'kalman', 'gnss', 'magnetometers', 'orientation', 'spoofing', 'cockpit']","[""  In this paper, we validate the performance of the a sensor fusion-based\nGlobal Navigation Satellite System (GNSS) spoofing attack detection framework\nfor Autonomous Vehicles (AVs). To collect data, a vehicle equipped with a GNSS\nreceiver, along with Inertial Measurement Unit (IMU) is used. The detection\nframework incorporates two strategies: The first strategy involves comparing\nthe predicted location shift, which is the distance traveled between two\nconsecutive timestamps, with the inertial sensor-based location shift. For this\npurpose, data from low-cost in-vehicle inertial sensors such as the\naccelerometer and gyroscope sensor are fused and fed into a long short-term\nmemory (LSTM) neural network. The second strategy employs a Random-Forest\nsupervised machine learning model to detect and classify turns, distinguishing\nbetween left and right turns using the output from the steering angle sensor.\nIn experiments, two types of spoofing attack models: turn-by-turn and wrong\nturn are simulated. These spoofing attacks are modeled as SQL injection\nattacks, where, upon successful implementation, the navigation system perceives\ninjected spoofed location information as legitimate while being unable to\ndetect legitimate GNSS signals. Importantly, the IMU data remains uncompromised\nthroughout the spoofing attack. To test the effectiveness of the detection\nframework, experiments are conducted in Tuscaloosa, AL, mimicking urban road\nstructures. The results demonstrate the framework's ability to detect various\nsophisticated GNSS spoofing attacks, even including slow position drifting\nattacks. Overall, the experimental results showcase the robustness and efficacy\nof the sensor fusion-based spoofing attack detection approach in safeguarding\nAVs against GNSS spoofing threats.\n"", '  Inertial sensing is used in many applications and platforms, ranging from\nday-to-day devices such as smartphones to very complex ones such as autonomous\nvehicles. In recent years, the development of machine learning and deep\nlearning techniques has increased significantly in the field of inertial\nsensing and sensor fusion. This is due to the development of efficient\ncomputing hardware and the accessibility of publicly available sensor data.\nThese data-driven approaches mainly aim to empower model-based inertial sensing\nalgorithms. To encourage further research in integrating deep learning with\ninertial navigation and fusion and to leverage their capabilities, this paper\nprovides an in-depth review of deep learning methods for inertial sensing and\nsensor fusion. We discuss learning methods for calibration and denoising as\nwell as approaches for improving pure inertial navigation and sensor fusion.\nThe latter is done by learning some of the fusion filter parameters. The\nreviewed approaches are classified by the environment in which the vehicles\noperate: land, air, and sea. In addition, we analyze trends and future\ndirections in deep learning-based navigation and provide statistical data on\ncommonly used approaches.\n', ""  This paper presents a novel approach to vehicle positioning that operates\nwithout reliance on the global navigation satellite system (GNSS). Traditional\nGNSS approaches are vulnerable to interference in certain environments,\nrendering them unreliable in situations such as urban canyons, under flyovers,\nor in low reception areas. This study proposes a vehicle positioning method\nbased on learning the road signature from accelerometer and gyroscope\nmeasurements obtained by an inertial measurement unit (IMU) sensor. In our\napproach, the route is divided into segments, each with a distinct signature\nthat the IMU can detect through the vibrations of a vehicle in response to\nsubtle changes in the road surface. The study presents two different\ndata-driven methods for learning the road segment from IMU measurements. One\nmethod is based on convolutional neural networks and the other on ensemble\nrandom forest applied to handcrafted features. Additionally, the authors\npresent an algorithm to deduce the position of a vehicle in real-time using the\nlearned road segment. The approach was applied in two positioning tasks: (i) a\ncar along a 6[km] route in a dense urban area; (ii) an e-scooter on a 1[km]\nroute that combined road and pavement surfaces. The mean error between the\nproposed method's position and the ground truth was approximately 50[m] for the\ncar and 30[m] for the e-scooter. Compared to a solution based on time\nintegration of the IMU measurements, the proposed approach has a mean error of\nmore than 5 times better for e-scooters and 20 times better for cars.\n""]",Inertial Sensor-Based Navigation and Tracking
281,280,30,280_forgetting_continual_retrieval_learning,"['forgetting', 'continual', 'retrieval', 'learning', 'memory', 'learned', 'retaining', 'lifelong', 'forgotten', 'continually']","['continual', 'forgetting', 'catastrophic', 'replay', 'upstream', 'knowledge', 'task', 'lifelong', 'incremental', 'pretraining']","['forgetting', 'retrieval', 'learned', 'retaining', 'lifelong', 'replaying', 'incremental', 'pretraining', 'lms', 'instruction']","['  This paper studies the evolving domain of Continual Learning (CL) in large\nlanguage models (LLMs), with a focus on developing strategies for efficient and\nsustainable training. Our primary emphasis is on continual domain-adaptive\npretraining, a process designed to equip LLMs with the ability to integrate new\ninformation from various domains while retaining previously learned knowledge\nand enhancing cross-domain knowledge transfer without relying on\ndomain-specific identification. Unlike previous studies, which mostly\nconcentrate on a limited selection of tasks or domains and primarily aim to\naddress the issue of forgetting, our research evaluates the adaptability and\ncapabilities of LLMs to changing data landscapes in practical scenarios. To\nthis end, we introduce a new benchmark designed to measure the adaptability of\nLLMs to these evolving data environments, offering a comprehensive framework\nfor evaluation. We examine the impact of model size on learning efficacy and\nforgetting, as well as how the progression and similarity of emerging domains\naffect the knowledge transfer within these models. Our findings uncover several\nkey insights: (i) when the sequence of domains shows semantic similarity,\ncontinual pretraining enables LLMs to better specialize in the current domain\ncompared to stand-alone fine-tuning, (ii) training across a diverse range of\ndomains enhances both backward and forward knowledge transfer, and (iii)\nsmaller models are particularly sensitive to continual pretraining, showing the\nmost significant rates of both forgetting and learning. We posit that our\nresearch marks a shift towards establishing a more realistic benchmark for\ninvestigating CL in LLMs, and has the potential to play a key role in guiding\nthe direction of future research in the field.\n', '  The recent success of large language models (LLMs) trained on static,\npre-collected, general datasets has sparked numerous research directions and\napplications. One such direction addresses the non-trivial challenge of\nintegrating pre-trained LLMs into dynamic data distributions, task structures,\nand user preferences. Pre-trained LLMs, when tailored for specific needs, often\nexperience significant performance degradation in previous knowledge domains --\na phenomenon known as ""catastrophic forgetting"". While extensively studied in\nthe continual learning (CL) community, it presents new manifestations in the\nrealm of LLMs. In this survey, we provide a comprehensive overview of the\ncurrent research progress on LLMs within the context of CL. This survey is\nstructured into four main sections: we first describe an overview of\ncontinually learning LLMs, consisting of two directions of continuity: vertical\ncontinuity (or vertical continual learning), i.e., continual adaptation from\ngeneral to specific capabilities, and horizontal continuity (or horizontal\ncontinual learning), i.e., continual adaptation across time and domains\n(Section 3). We then summarize three stages of learning LLMs in the context of\nmodern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP),\nand Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of\nevaluation protocols for continual learning with LLMs, along with the current\navailable data sources (Section 5). Finally, we discuss intriguing questions\npertaining to continual learning for LLMs (Section 6). The full list of papers\nexamined in this survey is available at\nhttps://github.com/Wang-ML-Lab/llm-continual-learning-survey.\n', '  Recently, foundation language models (LMs) have marked significant\nachievements in the domains of natural language processing (NLP) and computer\nvision (CV). Unlike traditional neural network models, foundation LMs obtain a\ngreat ability for transfer learning by acquiring rich commonsense knowledge\nthrough pre-training on extensive unsupervised datasets with a vast number of\nparameters. However, they still can not emulate human-like continuous learning\ndue to catastrophic forgetting. Consequently, various continual learning\n(CL)-based methodologies have been developed to refine LMs, enabling them to\nadapt to new tasks without forgetting previous knowledge. However, a systematic\ntaxonomy of existing approaches and a comparison of their performance are still\nlacking, which is the gap that our survey aims to fill. We delve into a\ncomprehensive review, summarization, and classification of the existing\nliterature on CL-based approaches applied to foundation language models, such\nas pre-trained language models (PLMs), large language models (LLMs) and\nvision-language models (VLMs). We divide these studies into offline CL and\nonline CL, which consist of traditional methods, parameter-efficient-based\nmethods, instruction tuning-based methods and continual pre-training methods.\nOffline CL encompasses domain-incremental learning, task-incremental learning,\nand class-incremental learning, while online CL is subdivided into hard task\nboundary and blurry task boundary settings. Additionally, we outline the\ntypical datasets and metrics employed in CL research and provide a detailed\nanalysis of the challenges and future work for LMs-based continual learning.\n']",Continual Learning in Large Language Models
282,281,30,281_earthquakes_earthquake_seismic_seismicity,"['earthquakes', 'earthquake', 'seismic', 'seismicity', 'microseismic', 'geotechnical', 'disasters', 'geological', 'tsunami', 'ground']","['earthquake', 'seismic', 'earthquakes', 'ground', 'wave', 'motion', 'magnitude', 'intensity', 'picking', 'velocity']","['earthquakes', 'geotechnical', 'seismograms', 'convolutional', 'wavecastnet', 'coseismic', 'sensor', 'warning', 'faulting', 'ionospheric']","['  Seismograms, the fundamental seismic records, have revolutionized earthquake\nresearch and monitoring. Recent advancements in deep learning have further\nenhanced seismic signal processing, leading to even more precise and effective\nearthquake monitoring capabilities. This paper introduces a foundational deep\nlearning model, the Seismogram Transformer (SeisT), designed for a variety of\nearthquake monitoring tasks. SeisT combines multiple modules tailored to\ndifferent tasks and exhibits impressive out-of-distribution generalization\nperformance, outperforming or matching state-of-the-art models in tasks like\nearthquake detection, seismic phase picking, first-motion polarity\nclassification, magnitude estimation, back-azimuth estimation, and epicentral\ndistance estimation. The performance scores on the tasks are 0.96, 0.96, 0.68,\n0.95, 0.86, 0.55, and 0.81, respectively. The most significant improvements, in\ncomparison to existing models, are observed in phase-P picking, phase-S\npicking, and magnitude estimation, with gains of 1.7%, 9.5%, and 8.0%,\nrespectively. Our study, through rigorous experiments and evaluations, suggests\nthat SeisT has the potential to contribute to the advancement of seismic signal\nprocessing and earthquake research.\n', '  This article surveys the growing interest in utilizing Deep Learning (DL) as\na powerful tool to address challenging problems in earthquake engineering.\nDespite decades of advancement in domain knowledge, issues such as uncertainty\nin earthquake occurrence, unpredictable seismic loads, nonlinear structural\nresponses, and community engagement remain difficult to tackle using\ndomain-specific methods. DL offers promising solutions by leveraging its\ndata-driven capacity for nonlinear mapping, sequential data modeling, automatic\nfeature extraction, dimensionality reduction, optimal decision-making, etc.\nHowever, the literature lacks a comprehensive review that systematically covers\na consistent scope intersecting DL and earthquake engineering. To bridge the\ngap, the article first discusses methodological advances to elucidate various\napplicable DL techniques, such as multi-layer perceptron (MLP), convolutional\nneural network (CNN), recurrent neural network (RNN), generative adversarial\nnetwork (GAN), autoencoder (AE), transfer learning (TL), reinforcement learning\n(RL), and graph neural network (GNN). A thorough research landscape is then\ndisclosed by exploring various DL applications across different research\ntopics, including vision-based seismic damage assessment and structural\ncharacterization, seismic demand and damage state prediction, seismic response\nhistory prediction, regional seismic risk assessment and community resilience,\nground motion (GM) for engineering use, seismic response control, and the\ninverse problem of system/damage identification. Suitable DL techniques for\neach research topic are identified, emphasizing the preeminence of CNN for\nvision-based tasks, RNN for sequential data, RL for community resilience, and\nunsupervised learning for GM analysis. The article also discusses opportunities\nand challenges for leveraging DL in earthquake engineering research and\npractice.\n', '  Predicting high-fidelity ground motions for future earthquakes is crucial for\nseismic hazard assessment and infrastructure resilience. Conventional empirical\nsimulations suffer from sparse sensor distribution and geographically localized\nearthquake locations, while physics-based methods are computationally intensive\nand require accurate representations of Earth structures and earthquake\nsources. We propose a novel artificial intelligence (AI) simulator, Conditional\nGenerative Modeling for Ground Motion (CGM-GM), to synthesize high-frequency\nand spatially continuous earthquake ground motion waveforms. CGM-GM leverages\nearthquake magnitudes and geographic coordinates of earthquakes and sensors as\ninputs, learning complex wave physics and Earth heterogeneities, without\nexplicit physics constraints. This is achieved through a probabilistic\nautoencoder that captures latent distributions in the time-frequency domain and\nvariational sequential models for prior and posterior distributions. We\nevaluate the performance of CGM-GM using small-magnitude earthquake records\nfrom the San Francisco Bay Area, a region with high seismic risks. CGM-GM\ndemonstrates a strong potential for outperforming a state-of-the-art\nnon-ergodic empirical ground motion model and shows great promise in seismology\nand beyond.\n']",Earthquake Research and Seismic Monitoring
283,282,30,282_fairness_discrimination_unfairness_ai,"['fairness', 'discrimination', 'unfairness', 'ai', 'discriminatory', 'ethical', 'bias', 'ethics', 'unfair', 'biases']","['fairness', 'algorithmic', 'biases', 'discrimination', 'bias', 'fair', 'recourse', 'stakeholders', 'definitions', 'discriminatory']","['fairness', 'discrimination', 'ai', 'ethical', 'biases', 'equal', 'neutrality', 'unjust', 'fairer', 'indirect']","[""  The integration of Artificial Intelligence (AI) into education has\ntransformative potential, providing tailored learning experiences and creative\ninstructional approaches. However, the inherent biases in AI algorithms hinder\nthis improvement by unintentionally perpetuating prejudice against specific\ndemographics, especially in human-centered applications like education. This\nsurvey delves deeply into the developing topic of algorithmic fairness in\neducational contexts, providing a comprehensive evaluation of the diverse\nliterature on fairness, bias, and ethics in AI-driven educational applications.\nIt identifies the common forms of biases, such as data-related, algorithmic,\nand user-interaction, that fundamentally undermine the accomplishment of\nfairness in AI teaching aids. By outlining existing techniques for mitigating\nthese biases, ranging from varied data gathering to algorithmic fairness\ninterventions, the survey emphasizes the critical role of ethical\nconsiderations and legal frameworks in shaping a more equitable educational\nenvironment. Furthermore, it guides readers through the complexities of\nfairness measurements, methods, and datasets, shedding light on the way to bias\nreduction. Despite these gains, this survey highlights long-standing issues,\nsuch as achieving a balance between fairness and accuracy, as well as the need\nfor diverse datasets. Overcoming these challenges and ensuring the ethical and\nfair use of AI's promise in education call for a collaborative,\ninterdisciplinary approach.\n"", ""  The rise in the use of AI/ML applications across industries has sparked more\ndiscussions about the fairness of AI/ML in recent times. While prior research\non the fairness of AI/ML exists, there is a lack of empirical studies focused\non understanding the perspectives and experiences of AI practitioners in\ndeveloping a fair AI/ML system. Understanding AI practitioners' perspectives\nand experiences on the fairness of AI/ML systems are important because they are\ndirectly involved in its development and deployment and their insights can\noffer valuable real-world perspectives on the challenges associated with\nensuring fairness in AI/ML systems. We conducted semi-structured interviews\nwith 22 AI practitioners to investigate their understanding of what a 'fair\nAI/ML' is, the challenges they face in developing a fair AI/ML system, the\nconsequences of developing an unfair AI/ML system, and the strategies they\nemploy to ensure AI/ML system fairness. We developed a framework showcasing the\nrelationship between AI practitioners' understanding of 'fair AI/ML' system and\n(i) their challenges in its development, (ii) the consequences of developing an\nunfair AI/ML system, and (iii) strategies used to ensure AI/ML system fairness.\nBy exploring AI practitioners' perspectives and experiences, this study\nprovides actionable insights to enhance AI/ML fairness, which may promote\nfairer systems, reduce bias, and foster public trust in AI technologies.\nAdditionally, we also identify areas for further investigation and offer\nrecommendations to aid AI practitioners and AI companies in navigating\nfairness.\n"", '  Reaching consensus on a commonly accepted definition of AI Fairness has long\nbeen a central challenge in AI ethics and governance. There is a broad spectrum\nof views across society on what the concept of fairness means and how it should\nbest be put to practice. In this workbook, we tackle this challenge by\nexploring how a context-based and society-centred approach to understanding AI\nFairness can help project teams better identify, mitigate, and manage the many\nways that unfair bias and discrimination can crop up across the AI project\nworkflow.\n  We begin by exploring how, despite the plurality of understandings about the\nmeaning of fairness, priorities of equality and non-discrimination have come to\nconstitute the broadly accepted core of its application as a practical\nprinciple. We focus on how these priorities manifest in the form of equal\nprotection from direct and indirect discrimination and from discriminatory\nharassment. These elements form ethical and legal criteria based upon which\ninstances of unfair bias and discrimination can be identified and mitigated\nacross the AI project workflow.\n  We then take a deeper dive into how the different contexts of the AI project\nlifecycle give rise to different fairness concerns. This allows us to identify\nseveral types of AI Fairness (Data Fairness, Application Fairness, Model Design\nand Development Fairness, Metric-Based Fairness, System Implementation\nFairness, and Ecosystem Fairness) that form the basis of a multi-lens approach\nto bias identification, mitigation, and management. Building on this, we\ndiscuss how to put the principle of AI Fairness into practice across the AI\nproject workflow through Bias Self-Assessment and Bias Risk Management as well\nas through the documentation of metric-based fairness criteria in a Fairness\nPosition Statement.\n']",Fairness and Bias in Artificial Intelligence
284,283,30,283_uncertainty_uncertainties_ensembles_prediction,"['uncertainty', 'uncertainties', 'ensembles', 'prediction', 'probabilistic', 'ensemble', 'deep', 'epistemic', 'neural', 'evidential']","['uncertainty', 'epistemic', 'evidential', 'aleatoric', 'ensembles', 'quantification', 'ensemble', 'predictive', 'uncertainties', 'deep']","['uncertainties', 'ensemble', 'evidential', 'bayesian', 'predictions', 'reliable', 'networks', 'estimation', 'confidence', 'dnns']","['  Robust quantification of predictive uncertainty is critical for understanding\nfactors that drive weather and climate outcomes. Ensembles provide predictive\nuncertainty estimates and can be decomposed physically, but both physics and\nmachine learning ensembles are computationally expensive. Parametric deep\nlearning can estimate uncertainty with one model by predicting the parameters\nof a probability distribution but do not account for epistemic uncertainty..\nEvidential deep learning, a technique that extends parametric deep learning to\nhigher-order distributions, can account for both aleatoric and epistemic\nuncertainty with one model. This study compares the uncertainty derived from\nevidential neural networks to those obtained from ensembles. Through\napplications of classification of winter precipitation type and regression of\nsurface layer fluxes, we show evidential deep learning models attaining\npredictive accuracy rivaling standard methods, while robustly quantifying both\nsources of uncertainty. We evaluate the uncertainty in terms of how well the\npredictions are calibrated and how well the uncertainty correlates with\nprediction error. Analyses of uncertainty in the context of the inputs reveal\nsensitivities to underlying meteorological processes, facilitating\ninterpretation of the models. The conceptual simplicity, interpretability, and\ncomputational efficiency of evidential neural networks make them highly\nextensible, offering a promising approach for reliable and practical\nuncertainty quantification in Earth system science modeling. In order to\nencourage broader adoption of evidential deep learning in Earth System Science,\nwe have developed a new Python package, MILES-GUESS\n(https://github.com/ai2es/miles-guess), that enables users to train and\nevaluate both evidential and ensemble deep learning.\n', '  Trustworthy ML systems should not only return accurate predictions, but also\na reliable representation of their uncertainty. Bayesian methods are commonly\nused to quantify both aleatoric and epistemic uncertainty, but alternative\napproaches, such as evidential deep learning methods, have become popular in\nrecent years. The latter group of methods in essence extends empirical risk\nminimization (ERM) for predicting second-order probability distributions over\noutcomes, from which measures of epistemic (and aleatoric) uncertainty can be\nextracted. This paper presents novel theoretical insights of evidential deep\nlearning, highlighting the difficulties in optimizing second-order loss\nfunctions and interpreting the resulting epistemic uncertainty measures. With a\nsystematic setup that covers a wide range of approaches for classification,\nregression and counts, it provides novel insights into issues of\nidentifiability and convergence in second-order loss minimization, and the\nrelative (rather than absolute) nature of epistemic uncertainty measures.\n', '  Epistemic uncertainty quantification (UQ) identifies where models lack\nknowledge. Traditional UQ methods, often based on Bayesian neural networks, are\nnot suitable for pre-trained non-Bayesian models. Our study addresses\nquantifying epistemic uncertainty for any pre-trained model, which does not\nneed the original training data or model modifications and can ensure broad\napplicability regardless of network architectures or training techniques.\nSpecifically, we propose a gradient-based approach to assess epistemic\nuncertainty, analyzing the gradients of outputs relative to model parameters,\nand thereby indicating necessary model adjustments to accurately represent the\ninputs. We first explore theoretical guarantees of gradient-based methods for\nepistemic UQ, questioning the view that this uncertainty is only calculable\nthrough differences between multiple models. We further improve gradient-driven\nUQ by using class-specific weights for integrating gradients and emphasizing\ndistinct contributions from neural network layers. Additionally, we enhance UQ\naccuracy by combining gradient and perturbation methods to refine the\ngradients. We evaluate our approach on out-of-distribution detection,\nuncertainty calibration, and active learning, demonstrating its superiority\nover current state-of-the-art UQ methods for pre-trained models.\n']",Uncertainty Quantification in Deep Learning
285,284,29,284_inferences_bayesian_probabilistic_inference,"['inferences', 'bayesian', 'probabilistic', 'inference', 'graphs', 'bayes', 'nodes', 'causal', 'posterior', 'polytrees']","['graphical', 'variables', 'posterior', 'polytrees', 'independence', 'structure', 'conditional', 'acyclic', 'likelihood', 'undirected']","['inferences', 'bayesian', 'causal', 'polytrees', 'graph', 'dependencies', 'learn', 'parameterisation', 'factorization', 'marginal']","['  Gaussian Process Networks (GPNs) are a class of directed graphical models\nwhich employ Gaussian processes as priors for the conditional expectation of\neach variable given its parents in the network. The model allows the\ndescription of continuous joint distributions in a compact but flexible manner\nwith minimal parametric assumptions on the dependencies between variables.\nBayesian structure learning of GPNs requires computing the posterior over\ngraphs of the network and is computationally infeasible even in low dimensions.\nThis work implements Monte Carlo and Markov Chain Monte Carlo methods to sample\nfrom the posterior distribution of network structures. As such, the approach\nfollows the Bayesian paradigm, comparing models via their marginal likelihood\nand computing the posterior probability of the GPN features. Simulation studies\nshow that our method outperforms state-of-the-art algorithms in recovering the\ngraphical structure of the network and provides an accurate approximation of\nits posterior distribution.\n', '  Bayesian causal structure learning aims to learn a posterior distribution\nover directed acyclic graphs (DAGs), and the mechanisms that define the\nrelationship between parent and child variables. By taking a Bayesian approach,\nit is possible to reason about the uncertainty of the causal model. The notion\nof modelling the uncertainty over models is particularly crucial for causal\nstructure learning since the model could be unidentifiable when given only a\nfinite amount of observational data. In this paper, we introduce a novel method\nto jointly learn the structure and mechanisms of the causal model using\nVariational Bayes, which we call Variational Bayes-DAG-GFlowNet (VBG). We\nextend the method of Bayesian causal structure learning using GFlowNets to\nlearn not only the posterior distribution over the structure, but also the\nparameters of a linear-Gaussian model. Our results on simulated data suggest\nthat VBG is competitive against several baselines in modelling the posterior\nover DAGs and mechanisms, while offering several advantages over existing\nmethods, including the guarantee to sample acyclic graphs, and the flexibility\nto generalize to non-linear causal mechanisms.\n', '  Estimating the structure of a Bayesian network, in the form of a directed\nacyclic graph (DAG), from observational data is a statistically and\ncomputationally hard problem with essential applications in areas such as\ncausal discovery. Bayesian approaches are a promising direction for solving\nthis task, as they allow for uncertainty quantification and deal with\nwell-known identifiability issues. From a probabilistic inference perspective,\nthe main challenges are (i) representing distributions over graphs that satisfy\nthe DAG constraint and (ii) estimating a posterior over the underlying\ncombinatorial space. We propose an approach that addresses these challenges by\nformulating a joint distribution on an augmented space of DAGs and\npermutations. We carry out posterior estimation via variational inference,\nwhere we exploit continuous relaxations of discrete distributions. We show that\nour approach performs competitively when compared with a wide range of Bayesian\nand non-Bayesian benchmarks on a range of synthetic and real datasets.\n']",Bayesian Inference for Graphical Models
286,285,29,285_nlg_evaluations_evaluation_generation,"['nlg', 'evaluations', 'evaluation', 'generation', 'generated', 'evaluating', 'evaluator', 'eval', 'summarization', 'assessment']","['evaluation', 'evaluators', 'metrics', 'references', 'reference', 'human', 'generation', 'judgments', 'natural', 'quality']","['nlg', 'evaluations', 'summarization', 'texts', 'grading', 'critiquellm', 'instruction', '_generative_', 'bias', 'recall']","['  Evaluating natural language generation (NLG) is a vital but challenging\nproblem in artificial intelligence. Traditional evaluation metrics mainly\ncapturing content (e.g. n-gram) overlap between system outputs and references\nare far from satisfactory, and large language models (LLMs) such as ChatGPT\nhave demonstrated great potential in NLG evaluation in recent years. Various\nautomatic evaluation methods based on LLMs have been proposed, including\nmetrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled\nevaluation data. In this survey, we first give a taxonomy of LLM-based NLG\nevaluation methods, and discuss their pros and cons, respectively. We also\ndiscuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several\nopen problems in this area and point out future research directions.\n', ""  Natural Language Generation (NLG) typically involves evaluating the generated\ntext in various aspects (e.g., consistency and naturalness) to obtain a\ncomprehensive assessment. However, multi-aspect evaluation remains challenging\nas it may require the evaluator to generalize to any given evaluation aspect\neven if it's absent during training. In this paper, we introduce X-Eval, a\ntwo-stage instruction tuning framework to evaluate the text in both seen and\nunseen aspects customized by end users. X-Eval consists of two learning stages:\nthe vanilla instruction tuning stage that improves the model's ability to\nfollow evaluation instructions, and an enhanced instruction tuning stage that\nexploits the connections between fine-grained evaluation aspects to better\nassess text quality. To support the training of X-Eval, we collect\nAspectInstruct, the first instruction tuning dataset tailored for multi-aspect\nNLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance\ntask diversity, we devise an augmentation strategy that converts human rating\nannotations into diverse forms of NLG evaluation tasks, including scoring,\ncomparison, ranking, and Boolean question answering. Extensive experiments\nacross three essential categories of NLG tasks: dialogue generation,\nsummarization, and data-to-text coupled with 21 aspects in meta-evaluation,\ndemonstrate that our X-Eval enables even a lightweight language model to\nachieve a comparable if not higher correlation with human judgments compared to\nthe state-of-the-art NLG evaluators, such as GPT-4.\n"", '  The evaluation of natural language generation (NLG) tasks is a significant\nand longstanding research issue. With the recent emergence of powerful large\nlanguage models (LLMs), some studies have turned to LLM-based automatic\nevaluation methods, which demonstrate great potential to become a new\nevaluation paradigm following traditional string-based and model-based metrics.\nHowever, despite the improved performance of existing methods, they still\npossess some deficiencies, such as dependency on references and limited\nevaluation flexibility. Therefore, in this paper, we meticulously construct a\nlarge-scale NLG evaluation corpus NLG-Eval with human and GPT-4 annotations to\nalleviate the lack of relevant data in this field. Furthermore, we propose\nThemis, an LLM dedicated to NLG evaluation, which has been trained with our\ndesigned multi-perspective consistency and rating-oriented preference alignment\nmethods. Themis can conduct flexible and interpretable evaluations without\nreferences, and it exhibits superior evaluation performance on various NLG\ntasks, simultaneously generalizing well to unseen tasks and surpassing other\nevaluation models, including GPT-4.\n']",Natural Language Generation Evaluation
287,286,29,286_forecasting_rnn_temporal_predicting,"['forecasting', 'rnn', 'temporal', 'predicting', 'prediction', 'neural', 'events', 'future', 'autoregressive', 'modeling']","['event', 'events', 'temporal', 'point', 'sequences', 'intensity', 'processes', 'series', 'marks', 'arrival']","['forecasting', 'rnn', 'temporal', 'conditioned', 'tpps', 'diffusion', 'spatio', 'arrival', 'sequences', 'hawkes']","['  Autoregressive neural networks within the temporal point process (TPP)\nframework have become the standard for modeling continuous-time event data.\nEven though these models can expressively capture event sequences in a\none-step-ahead fashion, they are inherently limited for long-term forecasting\napplications due to the accumulation of errors caused by their sequential\nnature. To overcome these limitations, we derive ADD-THIN, a principled\nprobabilistic denoising diffusion model for TPPs that operates on entire event\nsequences. Unlike existing diffusion approaches, ADD-THIN naturally handles\ndata with discrete and continuous components. In experiments on synthetic and\nreal-world datasets, our model matches the state-of-the-art TPP models in\ndensity estimation and strongly outperforms them in forecasting.\n', '  Neural Temporal Point Processes (TPPs) have emerged as the primary framework\nfor predicting sequences of events that occur at irregular time intervals, but\ntheir sequential nature can hamper performance for long-horizon forecasts. To\naddress this, we introduce a novel approach that incorporates a diffusion\ngenerative model. The model facilitates sequence-to-sequence prediction,\nallowing multi-step predictions based on historical event sequences. In\ncontrast to previous approaches, our model directly learns the joint\nprobability distribution of types and inter-arrival times for multiple events.\nThis allows us to fully leverage the high dimensional modeling capability of\nmodern generative models. Our model is composed of two diffusion processes, one\nfor the time intervals and one for the event types. These processes interact\nthrough their respective denoising functions, which can take as input\nintermediate representations from both processes, allowing the model to learn\ncomplex interactions. We demonstrate that our proposal outperforms\nstate-of-the-art baselines for long-horizon forecasting of TPP.\n', ""  Temporal Point Processes (TPPs) hold a pivotal role in modeling event\nsequences across diverse domains, including social networking and e-commerce,\nand have significantly contributed to the advancement of recommendation systems\nand information retrieval strategies. Through the analysis of events such as\nuser interactions and transactions, TPPs offer valuable insights into\nbehavioral patterns, facilitating the prediction of future trends. However,\naccurately forecasting future events remains a formidable challenge due to the\nintricate nature of these patterns. The integration of Neural Networks with\nTPPs has ushered in the development of advanced deep TPP models. While these\nmodels excel at processing complex and nonlinear temporal data, they encounter\nlimitations in modeling intensity functions, grapple with computational\ncomplexities in integral computations, and struggle to capture long-range\ntemporal dependencies effectively. In this study, we introduce the CuFun model,\nrepresenting a novel approach to TPPs that revolves around the Cumulative\nDistribution Function (CDF). CuFun stands out by uniquely employing a monotonic\nneural network for CDF representation, utilizing past events as a scaling\nfactor. This innovation significantly bolsters the model's adaptability and\nprecision across a wide range of data scenarios. Our approach addresses several\ncritical issues inherent in traditional TPP modeling: it simplifies\nlog-likelihood calculations, extends applicability beyond predefined density\nfunction forms, and adeptly captures long-range temporal patterns. Our\ncontributions encompass the introduction of a pioneering CDF-based TPP model,\nthe development of a methodology for incorporating past event information into\nfuture event prediction, and empirical validation of CuFun's effectiveness\nthrough extensive experimentation on synthetic and real-world datasets.\n""]",Temporal Event Forecasting with Neural Networks
288,287,29,287_adversarial_watermarking_watermark_watermarked,"['adversarial', 'watermarking', 'watermark', 'watermarked', 'watermarks', 'unmarker', 'stealing', 'tampering', 'protect', 'security']","['watermarking', 'watermark', 'watermarks', 'ownership', 'suspect', 'intellectual', 'removal', 'attacks', 'owners', 'trigger']","['adversarial', 'watermark', 'unmarker', 'tampering', 'steal', 'protected', 'backdoor', 'hiding', 'deepjudge', 'copyright']","['  As deep learning (DL) models are widely and effectively used in Machine\nLearning as a Service (MLaaS) platforms, there is a rapidly growing interest in\nDL watermarking techniques that can be used to confirm the ownership of a\nparticular model. Unfortunately, these methods usually produce watermarks\nsusceptible to model stealing attacks. In our research, we introduce a novel\ntrigger set-based watermarking approach that demonstrates resilience against\nfunctionality stealing attacks, particularly those involving extraction and\ndistillation. Our approach does not require additional model training and can\nbe applied to any model architecture. The key idea of our method is to compute\nthe trigger set, which is transferable between the source model and the set of\nproxy models with a high probability. In our experimental study, we show that\nif the probability of the set being transferable is reasonably high, it can be\neffectively used for ownership verification of the stolen model. We evaluate\nour method on multiple benchmarks and show that our approach outperforms\ncurrent state-of-the-art watermarking techniques in all considered experimental\nsetups.\n', '  Watermark has been widely deployed by industry to detect AI-generated images.\nThe robustness of such watermark-based detector against evasion attacks in the\nwhite-box and black-box settings is well understood in the literature. However,\nthe robustness in the no-box setting is much less understood. In particular,\nmultiple studies claimed that image watermark is robust in such setting. In\nthis work, we propose a new transfer evasion attack to image watermark in the\nno-box setting. Our transfer attack adds a perturbation to a watermarked image\nto evade multiple surrogate watermarking models trained by the attacker itself,\nand the perturbed watermarked image also evades the target watermarking model.\nOur major contribution is to show that, both theoretically and empirically,\nwatermark-based AI-generated image detector is not robust to evasion attacks\neven if the attacker does not have access to the watermarking model nor the\ndetection API.\n', '  Deep Learning (DL) models have become crucial in digital transformation, thus\nraising concerns about their intellectual property rights. Different\nwatermarking techniques have been developed to protect Deep Neural Networks\n(DNNs) from IP infringement, creating a competitive field for DNN watermarking\nand removal methods. The predominant watermarking schemes use white-box\ntechniques, which involve modifying weights by adding a unique signature to\nspecific DNN layers. On the other hand, existing attacks on white-box\nwatermarking usually require knowledge of the specific deployed watermarking\nscheme or access to the underlying data for further training and fine-tuning.\nWe propose DeepEclipse, a novel and unified framework designed to remove\nwhite-box watermarks. We present obfuscation techniques that significantly\ndiffer from the existing white-box watermarking removal schemes. DeepEclipse\ncan evade watermark detection without prior knowledge of the underlying\nwatermarking scheme, additional data, or training and fine-tuning. Our\nevaluation reveals that DeepEclipse excels in breaking multiple white-box\nwatermarking schemes, reducing watermark detection to random guessing while\nmaintaining a similar model accuracy as the original one. Our framework\nshowcases a promising solution to address the ongoing DNN watermark protection\nand removal challenges.\n']",Adversarial Watermarking Techniques for Deep Learning Models
289,288,29,288_paragraphs_autoregressively_autoregressive_text,"['paragraphs', 'autoregressively', 'autoregressive', 'text', 'diffusion', 'mlms', 'models', 'language', 'predict', 'fluent']","['autoregressive', 'diffusion', 'discrete', 'text', 'generation', 'sampling', 'tokens', 'quality', 'token', 'language']","['paragraphs', 'autoregressive', 'diffusion', 'fluent', 'decoding', 'lms', 'tokens', 'tess', 'pretraining', 'hsd']","['  Autoregressive models for text sometimes generate repetitive and low-quality\noutput because errors accumulate during the steps of generation. This issue is\noften attributed to exposure bias - the difference between how a model is\ntrained, and how it is used during inference. Denoising diffusion models\nprovide an alternative approach in which a model can revisit and revise its\noutput. However, they can be computationally expensive and prior efforts on\ntext have led to models that produce less fluent output compared to\nautoregressive models, especially for longer text and paragraphs. In this\npaper, we propose PLANNER, a model that combines latent semantic diffusion with\nautoregressive generation, to generate fluent text while exercising global\ncontrol over paragraphs. The model achieves this by combining an autoregressive\n""decoding"" module with a ""planning"" module that uses latent diffusion to\ngenerate semantic paragraph embeddings in a coarse-to-fine manner. The proposed\nmethod is evaluated on various conditional generation tasks, and results on\nsemantic generation, text completion and summarization show its effectiveness\nin generating high-quality long-form text in an efficient manner.\n', '  The modern autoregressive Large Language Models (LLMs) have achieved\noutstanding performance on NLP benchmarks, and they are deployed in the real\nworld. However, they still suffer from limitations of the autoregressive\ntraining paradigm. For example, autoregressive token generation is notably slow\nand can be prone to \\textit{exposure bias}. The diffusion-based language models\nwere proposed as an alternative to autoregressive generation to address some of\nthese limitations. We evaluate the recently proposed Score Entropy Discrete\nDiffusion (SEDD) approach and show it is a promising alternative to\nautoregressive generation but it has some short-comings too. We empirically\ndemonstrate the advantages and challenges of SEDD, and observe that SEDD\ngenerally matches autoregressive models in perplexity and on benchmarks such as\nHellaSwag, Arc or WinoGrande. Additionally, we show that in terms of inference\nlatency, SEDD can be up to 4.5$\\times$ more efficient than GPT-2. While SEDD\nallows conditioning on tokens at abitrary positions, SEDD appears slightly\nweaker than GPT-2 for conditional generation given short prompts. Finally, we\nreproduced the main results from the original SEDD paper.\n', '  Despite their groundbreaking performance for many generative modeling tasks,\ndiffusion models have fallen short on discrete data domains such as natural\nlanguage. Crucially, standard diffusion models rely on the well-established\ntheory of score matching, but efforts to generalize this to discrete structures\nhave not yielded the same empirical gains. In this work, we bridge this gap by\nproposing score entropy, a novel loss that naturally extends score matching to\ndiscrete spaces, integrates seamlessly to build discrete diffusion models, and\nsignificantly boosts performance. Experimentally, we test our Score Entropy\nDiscrete Diffusion models (SEDD) on standard language modeling tasks. For\ncomparable model sizes, SEDD beats existing language diffusion paradigms\n(reducing perplexity by $25$-$75$\\%) and is competitive with autoregressive\nmodels, in particular outperforming GPT-2. Furthermore, compared to\nautoregressive mdoels, SEDD generates faithful text without requiring\ndistribution annealing techniques like temperature scaling (around\n$6$-$8\\times$ better generative perplexity than un-annealed GPT-2), can trade\ncompute and quality (similar quality with $32\\times$ fewer network\nevaluations), and enables controllable infilling (matching nucleus sampling\nquality while enabling other strategies besides left to right prompting).\n']","""Advances in Text Generation with Autoregressive and Diffusion Models"""
290,289,29,289_resumes_resume_hiring_jobs,"['resumes', 'resume', 'hiring', 'jobs', 'recruitment', 'workforce', 'employment', 'applicants', 'skills', 'occupations']","['job', 'resume', 'career', 'skill', 'skills', 'resumes', 'occupational', 'recruitment', 'market', 'descriptions']","['resumes', 'recruitment', 'skills', 'dataset', 'occupational', 'extract', 'nlp', 'matching', 'profiles', 'vacancies']","['  [Abridged Abstract]\n  Recent technological advances underscore labor market dynamics, yielding\nsignificant consequences for employment prospects and increasing job vacancy\ndata across platforms and languages. Aggregating such data holds potential for\nvaluable insights into labor market demands, new skills emergence, and\nfacilitating job matching for various stakeholders. However, despite prevalent\ninsights in the private sector, transparent language technology systems and\ndata for this domain are lacking. This thesis investigates Natural Language\nProcessing (NLP) technology for extracting relevant information from job\ndescriptions, identifying challenges including scarcity of training data, lack\nof standardized annotation guidelines, and shortage of effective extraction\nmethods from job ads. We frame the problem, obtaining annotated data, and\nintroducing extraction methodologies. Our contributions include job description\ndatasets, a de-identification dataset, and a novel active learning algorithm\nfor efficient model training. We propose skill extraction using weak\nsupervision, a taxonomy-aware pre-training methodology adapting multilingual\nlanguage models to the job market domain, and a retrieval-augmented model\nleveraging multiple skill extraction datasets to enhance overall performance.\nFinally, we ground extracted information within a designated taxonomy.\n', '  A reliable resume-job matching system helps a company find suitable\ncandidates from a pool of resumes, and helps a job seeker find relevant jobs\nfrom a list of job posts. However, since job seekers apply only to a few jobs,\ninteraction records in resume-job datasets are sparse. Different from many\nprior work that use complex modeling techniques, we tackle this sparsity\nproblem using data augmentations and a simple contrastive learning approach.\nConFit first creates an augmented resume-job dataset by paraphrasing specific\nsections in a resume or a job post. Then, ConFit uses contrastive learning to\nfurther increase training samples from $B$ pairs per batch to $O(B^2)$ per\nbatch. We evaluate ConFit on two real-world datasets and find it outperforms\nprior methods (including BM25 and OpenAI text-ada-002) by up to 19% and 31%\nabsolute in nDCG@10 for ranking jobs and ranking resumes, respectively.\n', ""  Crafting the ideal, job-specific resume is a challenging task for many job\napplicants, especially for early-career applicants. While it is highly\nrecommended that applicants tailor their resume to the specific role they are\napplying for, manually tailoring resumes to job descriptions and role-specific\nrequirements is often (1) extremely time-consuming, and (2) prone to human\nerrors. Furthermore, performing such a tailoring step at scale while applying\nto several roles may result in a lack of quality of the edited resumes. To\ntackle this problem, in this demo paper, we propose ResumeFlow: a Large\nLanguage Model (LLM) aided tool that enables an end user to simply provide\ntheir detailed resume and the desired job posting, and obtain a personalized\nresume specifically tailored to that specific job posting in the matter of a\nfew seconds. Our proposed pipeline leverages the language understanding and\ninformation extraction capabilities of state-of-the-art LLMs such as OpenAI's\nGPT-4 and Google's Gemini, in order to (1) extract details from a job\ndescription, (2) extract role-specific details from the user-provided resume,\nand then (3) use these to refine and generate a role-specific resume for the\nuser. Our easy-to-use tool leverages the user-chosen LLM in a completely\noff-the-shelf manner, thus requiring no fine-tuning. We demonstrate the\neffectiveness of our tool via a video demo and propose novel task-specific\nevaluation metrics to control for alignment and hallucination. Our tool is\navailable at https://job-aligned-resume.streamlit.app.\n""]",Job Market and Resume Optimization
291,290,29,290_disentangling_disentangled_disentanglement_representations,"['disentangling', 'disentangled', 'disentanglement', 'representations', 'autoencoders', 'entangled', 'encoder', 'generative', 'decoder', 'representation']","['disentanglement', 'disentangled', 'factors', 'representation', 'latent', 'representations', 'variation', 'variational', 'definitions', 'variables']","['disentangling', 'representations', 'autoencoder', 'variational', 'regularization', 'multimodal', 'unsupervised', 'mixtures', 'informativeness', 'downstream']","['  Representation learning is an approach that allows to discover and extract\nthe factors of variation from the data. Intuitively, a representation is said\nto be disentangled if it separates the different factors of variation in a way\nthat is understandable to humans. Definitions of disentanglement and metrics to\nmeasure it usually assume that the factors of variation are independent of each\nother. However, this is generally false in the real world, which limits the use\nof these definitions and metrics to very specific and unrealistic scenarios. In\nthis paper we give a definition of disentanglement based on information theory\nthat is also valid when the factors of variation are not independent.\nFurthermore, we relate this definition to the Information Bottleneck Method.\nFinally, we propose a method to measure the degree of disentanglement from the\ngiven definition that works when the factors of variation are not independent.\nWe show through different experiments that the method proposed in this paper\ncorrectly measures disentanglement with non-independent factors of variation,\nwhile other methods fail in this scenario.\n', '  Current autoencoder-based disentangled representation learning methods\nachieve disentanglement by penalizing the (aggregate) posterior to encourage\nstatistical independence of the latent factors. This approach introduces a\ntrade-off between disentangled representation learning and reconstruction\nquality since the model does not have enough capacity to learn correlated\nlatent variables that capture detail information present in most image data. To\novercome this trade-off, we present a novel multi-stage modeling approach where\nthe disentangled factors are first learned using a penalty-based disentangled\nrepresentation learning method; then, the low-quality reconstruction is\nimproved with another deep generative model that is trained to model the\nmissing correlated latent variables, adding detail information while\nmaintaining conditioning on the previously learned disentangled factors. Taken\ntogether, our multi-stage modelling approach results in a single, coherent\nprobabilistic model that is theoretically justified by the principal of\nD-separation and can be realized with a variety of model classes including\nlikelihood-based models such as variational autoencoders, implicit models such\nas generative adversarial networks, and tractable models like normalizing flows\nor mixtures of Gaussians. We demonstrate that our multi-stage model has higher\nreconstruction quality than current state-of-the-art methods with equivalent\ndisentanglement performance across multiple standard benchmarks. In addition,\nwe apply the multi-stage model to generate synthetic tabular datasets,\nshowcasing an enhanced performance over benchmark models across a variety of\nmetrics. The interpretability analysis further indicates that the multi-stage\nmodel can effectively uncover distinct and meaningful features of variations\nfrom which the original distribution can be recovered.\n', '  In representation learning, a disentangled representation is highly desirable\nas it encodes generative factors of data in a separable and compact pattern.\nResearchers have advocated leveraging disentangled representations to complete\ndownstream tasks with encouraging empirical evidence. This paper further\ninvestigates the necessity of disentangled representation in downstream\napplications. Specifically, we show that dimension-wise disentangled\nrepresentations are unnecessary on a fundamental downstream task, abstract\nvisual reasoning. We provide extensive empirical evidence against the necessity\nof disentanglement, covering multiple datasets, representation learning\nmethods, and downstream network architectures. Furthermore, our findings\nsuggest that the informativeness of representations is a better indicator of\ndownstream performance than disentanglement. Finally, the positive correlation\nbetween informativeness and disentanglement explains the claimed usefulness of\ndisentangled representations in previous works. The source code is available at\nhttps://github.com/Richard-coder-Nai/disentanglement-lib-necessity.git.\n']",Disentangled Representation Learning
292,291,29,291_fraud_banking_classification_bank,"['fraud', 'banking', 'classification', 'bank', 'banks', 'credit', 'fraudulent', 'lending', 'card', 'lenders']","['credit', 'fraud', 'financial', 'card', 'banks', 'fraudulent', 'transactions', 'risk', 'transaction', 'default']","['banking', 'fraudulent', 'imbalance', 'ensemble', 'analytics', 'logistic', 'alerts', 'thresholding', 'auditors', 'nigerian']","['  Financial institutions and businesses face an ongoing challenge from\nfraudulent transactions, prompting the need for effective detection methods.\nDetecting credit card fraud is crucial for identifying and preventing\nunauthorized transactions.Timely detection of fraud enables investigators to\ntake swift actions to mitigate further losses. However, the investigation\nprocess is often time-consuming, limiting the number of alerts that can be\nthoroughly examined each day. Therefore, the primary objective of a fraud\ndetection model is to provide accurate alerts while minimizing false alarms and\nmissed fraud cases. In this paper, we introduce a state-of-the-art hybrid\nensemble (ENS) dependable Machine learning (ML) model that intelligently\ncombines multiple algorithms with proper weighted optimization using Grid\nsearch, including Decision Tree (DT), Random Forest (RF), K-Nearest Neighbor\n(KNN), and Multilayer Perceptron (MLP), to enhance fraud identification. To\naddress the data imbalance issue, we employ the Instant Hardness Threshold\n(IHT) technique in conjunction with Logistic Regression (LR), surpassing\nconventional approaches. Our experiments are conducted on a publicly available\ncredit card dataset comprising 284,807 transactions. The proposed model\nachieves impressive accuracy rates of 99.66%, 99.73%, 98.56%, and 99.79%, and a\nperfect 100% for the DT, RF, KNN, MLP and ENS models, respectively. The hybrid\nensemble model outperforms existing works, establishing a new benchmark for\ndetecting fraudulent transactions in high-frequency scenarios. The results\nhighlight the effectiveness and reliability of our approach, demonstrating\nsuperior performance metrics and showcasing its exceptional potential for\nreal-world fraud detection applications.\n', '  Credit card fraud is a major cause of national concern in the Nigerian\nfinancial sector, affecting hundreds of transactions per second and impacting\ninternational ecommerce negatively. Despite the rapid spread and adoption of\nonline marketing, millions of Nigerians are prevented from transacting in\nseveral countries with local credit cards due to bans and policies directed at\nrestricting credit card fraud. Presently, a myriad of technologies exist to\ndetect fraudulent transactions, a few of which are adopted by Nigerian\nfinancial institutions to proactively manage the situation. Fraud detection\nallows institutions to restrict offenders from networks and with a centralized\nbanking identity management system, such as the Bank Verification Number used\nby the Central Bank of Nigeria, offenders who may have stolen other identities\ncan be backtraced and their bank accounts frozen. This paper aims to compare\nthe effectiveness of two fraud detection technologies that are projected to\nwork fully independent of human intervention to possibly predict and detect\nfraudulent credit card transactions. Autoencoders as an unsupervised tensorflow\nbased anomaly detection technique generally offers greater performance in\ndimensionality reduction than the Principal Component Analysis, and this theory\nwas tested out on Nigerian credit card transaction data. Results demonstrate\nthat autoencoders are better suited to analyzing complex and extensive datasets\nand offer more reliable results with minimal mislabeling than the PCA\nalgorithm.\n', '  Credit card fraud detection is a critical challenge in the financial sector,\ndemanding sophisticated approaches to accurately identify fraudulent\ntransactions. This research proposes an innovative methodology combining Neural\nNetworks (NN) and Synthet ic Minority Over-sampling Technique (SMOTE) to\nenhance the detection performance. The study addresses the inherent imbalance\nin credit card transaction data, focusing on technical advancements for robust\nand precise fraud detection. Results demonstrat e that the integration of NN\nand SMOTE exhibits superior precision, recall, and F1-score compared to\ntraditional models, highlighting its potential as an advanced solution for\nhandling imbalanced datasets in credit card fraud detection scenarios. This\nrese arch contributes to the ongoing efforts to develop effective and efficient\nmechanisms for safeguarding financial transactions from fraudulent activities.\n']",Credit Card Fraud Detection
293,292,28,292_embeddings_embedding_visualizing_visualise,"['embeddings', 'embedding', 'visualizing', 'visualise', 'visualize', 'visualizations', 'visualization', 'visualisation', 'sne', 'dimensionality']","['dimensional', 'dimensionality', 'visualization', 'reduction', 'manifold', 'embeddings', 'kernel', 'visualisation', 'structures', 'projections']","['embeddings', 'visualizations', 'sne', 'dimensional', 'datasets', 'clusters', 'eigenmaps', 'slisemap', 'scatter', 'manifold']","['  We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle\ninfinite data streams. The core idea behind S+t-SNE is to update the t-SNE\nembedding incrementally as new data arrives, ensuring scalability and\nadaptability to handle streaming scenarios. By selecting the most important\npoints at each step, the algorithm ensures scalability while keeping\ninformative visualisations. Employing a blind method for drift management\nadjusts the embedding space, facilitating continuous visualisation of evolving\ndata dynamics. Our experimental evaluations demonstrate the effectiveness and\nefficiency of S+t-SNE. The results highlight its ability to capture patterns in\na streaming scenario. We hope our approach offers researchers and practitioners\na real-time tool for understanding and interpreting high-dimensional data.\n', '  Neighbor embedding methods $t$-SNE and UMAP are the de facto standard for\nvisualizing high-dimensional datasets. Motivated from entirely different\nviewpoints, their loss functions appear to be unrelated. In practice, they\nyield strongly differing embeddings and can suggest conflicting interpretations\nof the same data. The fundamental reasons for this and, more generally, the\nexact relationship between $t$-SNE and UMAP have remained unclear. In this\nwork, we uncover their conceptual connection via a new insight into contrastive\nlearning methods. Noise-contrastive estimation can be used to optimize $t$-SNE,\nwhile UMAP relies on negative sampling, another contrastive method. We find the\nprecise relationship between these two contrastive methods and provide a\nmathematical characterization of the distortion introduced by negative\nsampling. Visually, this distortion results in UMAP generating more compact\nembeddings with tighter clusters compared to $t$-SNE. We exploit this new\nconceptual connection to propose and implement a generalization of negative\nsampling, allowing us to interpolate between (and even extrapolate beyond)\n$t$-SNE and UMAP and their respective embeddings. Moving along this spectrum of\nembeddings leads to a trade-off between discrete / local and continuous /\nglobal structures, mitigating the risk of over-interpreting ostensible features\nof any single embedding. We provide a PyTorch implementation.\n', ""  t-Distributed Stochastic Neighbor Embedding (t-SNE) for the visualization of\nmultidimensional data has proven to be a popular approach, with successful\napplications in a wide range of domains. Despite their usefulness, t-SNE\nprojections can be hard to interpret or even misleading, which hurts the\ntrustworthiness of the results. Understanding the details of t-SNE itself and\nthe reasons behind specific patterns in its output may be a daunting task,\nespecially for non-experts in dimensionality reduction. In this work, we\npresent t-viSNE, an interactive tool for the visual exploration of t-SNE\nprojections that enables analysts to inspect different aspects of their\naccuracy and meaning, such as the effects of hyper-parameters, distance and\nneighborhood preservation, densities and costs of specific neighborhoods, and\nthe correlations between dimensions and visual patterns. We propose a coherent,\naccessible, and well-integrated collection of different views for the\nvisualization of t-SNE projections. The applicability and usability of t-viSNE\nare demonstrated through hypothetical usage scenarios with real data sets.\nFinally, we present the results of a user study where the tool's effectiveness\nwas evaluated. By bringing to light information that would normally be lost\nafter running t-SNE, we hope to support analysts in using t-SNE and making its\nresults better understandable.\n""]",Dimensionality Reduction and Visualization Techniques
294,293,28,293_sram_memory_intel_hardware,"['sram', 'memory', 'intel', 'hardware', 'cores', 'throughput', 'accelerator', 'accelerators', 'gpu', 'gpus']","['accelerators', 'hardware', 'chip', 'dataflow', 'memory', 'accelerator', 'systolic', 'energy', 'latency', 'parallelism']","['sram', 'accelerator', 'gpu', 'dataflow', 'cimnet', 'cuda', 'nvsram', 'architectures', 'dnns', 'convolutions']","['  Deep Neural Networks (DNNs) excel in learning hierarchical representations\nfrom raw data, such as images, audio, and text. To compute these DNN models\nwith high performance and energy efficiency, these models are usually deployed\nonto customized hardware accelerators. Among various accelerator designs,\ndataflow architecture has shown promising performance due to its\nlayer-pipelined structure and its scalability in data parallelism.\n  Exploiting weights and activations sparsity can further enhance memory\nstorage and computation efficiency. However, existing approaches focus on\nexploiting sparsity in non-dataflow accelerators, which cannot be applied onto\ndataflow accelerators because of the large hardware design space introduced. As\nsuch, this could miss opportunities to find an optimal combination of sparsity\nfeatures and hardware designs.\n  In this paper, we propose a novel approach to exploit unstructured weights\nand activations sparsity for dataflow accelerators, using software and hardware\nco-optimization. We propose a Hardware-Aware Sparsity Search (HASS) to\nsystematically determine an efficient sparsity solution for dataflow\naccelerators. Over a set of models, we achieve an efficiency improvement\nranging from 1.3$\\times$ to 4.2$\\times$ compared to existing sparse designs,\nwhich are either non-dataflow or non-hardware-aware. Particularly, the\nthroughput of MobileNetV3 can be optimized to 4895 images per second. HASS is\nopen-source: \\url{https://github.com/Yu-Zhewen/HASS}\n', '  Accommodating all the weights on-chip for large-scale NNs remains a great\nchallenge for SRAM based computing-in-memory (SRAM-CIM) with limited on-chip\ncapacity. Previous non-volatile SRAM-CIM (nvSRAM-CIM) addresses this issue by\nintegrating high-density single-level ReRAMs on the top of high-efficiency\nSRAM-CIM for weight storage to eliminate the off-chip memory access. However,\nprevious SL-nvSRAM-CIM suffers from poor scalability for an increased number of\nSL-ReRAMs and limited computing efficiency. To overcome these challenges, this\nwork proposes an ultra-high-density three-level ReRAMs-assisted\ncomputing-in-nonvolatile-SRAM (TL-nvSRAM-CIM) scheme for large NN models. The\nclustered n-selector-n-ReRAM (cluster-nSnRs) is employed for reliable\nweight-restore with eliminated DC power. Furthermore, a ternary SRAM-CIM\nmechanism with differential computing scheme is proposed for energy-efficient\nternary MAC operations while preserving high NN accuracy. The proposed\nTL-nvSRAM-CIM achieves 7.8x higher storage density, compared with the\nstate-of-art works. Moreover, TL-nvSRAM-CIM shows up to 2.9x and 1.9x enhanced\nenergy-efficiency, respectively, compared to the baseline designs of SRAM-CIM\nand ReRAM-CIM, respectively.\n', ""  With the recent growth in demand for large-scale deep neural networks,\ncompute in-memory (CiM) has come up as a prominent solution to alleviate\nbandwidth and on-chip interconnect bottlenecks that constrain Von-Neuman\narchitectures. However, the construction of CiM hardware poses a challenge as\nany specific memory hierarchy in terms of cache sizes and memory bandwidth at\ndifferent interfaces may not be ideally matched to any neural network's\nattributes such as tensor dimension and arithmetic intensity, thus leading to\nsuboptimal and under-performing systems. Despite the success of neural\narchitecture search (NAS) techniques in yielding efficient sub-networks for a\ngiven hardware metric budget (e.g., DNN execution time or latency), it assumes\nthe hardware configuration to be frozen, often yielding sub-optimal\nsub-networks for a given budget. In this paper, we present CiMNet, a framework\nthat jointly searches for optimal sub-networks and hardware configurations for\nCiM architectures creating a Pareto optimal frontier of downstream task\naccuracy and execution metrics (e.g., latency). The proposed framework can\ncomprehend the complex interplay between a sub-network's performance and the\nCiM hardware configuration choices including bandwidth, processing element\nsize, and memory size. Exhaustive experiments on different model architectures\nfrom both CNN and Transformer families demonstrate the efficacy of the CiMNet\nin finding co-optimized sub-networks and CiM hardware configurations.\nSpecifically, for similar ImageNet classification accuracy as baseline ViT-B,\noptimizing only the model architecture increases performance (or reduces\nworkload execution time) by 1.7x while optimizing for both the model\narchitecture and hardware configuration increases it by 3.1x.\n""]",Optimizing Memory and Hardware for Deep Neural Networks
295,294,28,294_autoencoders_autoencoder_transcoders_sparse,"['autoencoders', 'autoencoder', 'transcoders', 'sparse', 'attention', 'features', 'interpretable', 'interpretability', 'activations', 'interpret']","['circuit', 'circuits', 'mechanistic', 'sparse', 'interpretability', 'autoencoders', 'patching', 'dictionary', 'interpretable', 'transcoders']","['autoencoders', 'attention', 'features', 'interpretability', 'circuits', 'gated', 'dictionaries', 'train', 'transformers', 'sublayers']","[""  Identifying the features learned by neural networks is a core challenge in\nmechanistic interpretability. Sparse autoencoders (SAEs), which learn a sparse,\novercomplete dictionary that reconstructs a network's internal activations,\nhave been used to identify these features. However, SAEs may learn more about\nthe structure of the datatset than the computational structure of the network.\nThere is therefore only indirect reason to believe that the directions found in\nthese dictionaries are functionally important to the network. We propose\nend-to-end (e2e) sparse dictionary learning, a method for training SAEs that\nensures the features learned are functionally important by minimizing the KL\ndivergence between the output distributions of the original model and the model\nwith SAE activations inserted. Compared to standard SAEs, e2e SAEs offer a\nPareto improvement: They explain more network performance, require fewer total\nfeatures, and require fewer simultaneously active features per datapoint, all\nwith no cost to interpretability. We explore geometric and qualitative\ndifferences between e2e SAE features and standard SAE features. E2e dictionary\nlearning brings us closer to methods that can explain network behavior\nconcisely and accurately. We release our library for training e2e SAEs and\nreproducing our analysis at https://github.com/ApolloResearch/e2e_sae\n"", ""  Circuit analysis of any certain model behavior is a central task in\nmechanistic interpretability. We introduce our circuit discovery pipeline with\nSparse Autoencoders (SAEs) and a variant called Transcoders. With these two\nmodules inserted into the model, the model's computation graph with respect to\nOV and MLP circuits becomes strictly linear. Our methods do not require linear\napproximation to compute the causal effect of each node. This fine-grained\ngraph identifies both end-to-end and local circuits accounting for either\nlogits or intermediate features. We can scalably apply this pipeline with a\ntechnique called Hierarchical Attribution. We analyze three kinds of circuits\nin GPT-2 Small: bracket, induction, and Indirect Object Identification\ncircuits. Our results reveal new findings underlying existing discoveries.\n"", '  Decomposing model activations into interpretable components is a key open\nproblem in mechanistic interpretability. Sparse autoencoders (SAEs) are a\npopular method for decomposing the internal activations of trained transformers\ninto sparse, interpretable features, and have been applied to MLP layers and\nthe residual stream. In this work we train SAEs on attention layer outputs and\nshow that also here SAEs find a sparse, interpretable decomposition. We\ndemonstrate this on transformers from several model families and up to 2B\nparameters.\n  We perform a qualitative study of the features computed by attention layers,\nand find multiple families: long-range context, short-range context and\ninduction features. We qualitatively study the role of every head in GPT-2\nSmall, and estimate that at least 90% of the heads are polysemantic, i.e. have\nmultiple unrelated roles.\n  Further, we show that Sparse Autoencoders are a useful tool that enable\nresearchers to explain model behavior in greater detail than prior work. For\nexample, we explore the mystery of why models have so many seemingly redundant\ninduction heads, use SAEs to motivate the hypothesis that some are long-prefix\nwhereas others are short-prefix, and confirm this with more rigorous analysis.\nWe use our SAEs to analyze the computation performed by the Indirect Object\nIdentification circuit (Wang et al.), validating that the SAEs find causally\nmeaningful intermediate variables, and deepening our understanding of the\nsemantics of the circuit. We open-source the trained SAEs and a tool for\nexploring arbitrary prompts through the lens of Attention Output SAEs.\n']",Interpretable Machine Learning with Autoencoders
296,295,28,295_facetalk_lip_styletalker_audio,"['facetalk', 'lip', 'styletalker', 'audio', 'portrait', 'animations', 'talkformer', 'audio2rig', 'mouth', 'animation']","['lip', 'talking', 'facial', 'sync', 'audio', 'motion', 'animation', 'synchronization', 'face', 'head']","['facetalk', 'lip', 'styletalker', 'portrait', 'animations', 'talkformer', 'audio2rig', 'expressive', 'speech', 'phonetic']","['  We propose StyleTalker, a novel audio-driven talking head generation model\nthat can synthesize a video of a talking person from a single reference image\nwith accurately audio-synced lip shapes, realistic head poses, and eye blinks.\nSpecifically, by leveraging a pretrained image generator and an image encoder,\nwe estimate the latent codes of the talking head video that faithfully reflects\nthe given audio. This is made possible with several newly devised components:\n1) A contrastive lip-sync discriminator for accurate lip synchronization, 2) A\nconditional sequential variational autoencoder that learns the latent motion\nspace disentangled from the lip movements, such that we can independently\nmanipulate the motions and lip movements while preserving the identity. 3) An\nauto-regressive prior augmented with normalizing flow to learn a complex\naudio-to-motion multi-modal latent space. Equipped with these components,\nStyleTalker can generate talking head videos not only in a motion-controllable\nway when another motion source video is given but also in a completely\naudio-driven manner by inferring realistic motions from the input audio.\nThrough extensive experiments and user studies, we show that our model is able\nto synthesize talking head videos with impressive perceptual quality which are\naccurately lip-synced with the input audios, largely outperforming\nstate-of-the-art baselines.\n', '  Audio-driven lip sync has recently drawn significant attention due to its\nwidespread application in the multimedia domain. Individuals exhibit distinct\nlip shapes when speaking the same utterance, attributed to the unique speaking\nstyles of individuals, posing a notable challenge for audio-driven lip sync.\nEarlier methods for such task often bypassed the modeling of personalized\nspeaking styles, resulting in sub-optimal lip sync conforming to the general\nstyles. Recent lip sync techniques attempt to guide the lip sync for arbitrary\naudio by aggregating information from a style reference video, yet they can not\npreserve the speaking styles well due to their inaccuracy in style aggregation.\nThis work proposes an innovative audio-aware style reference scheme that\neffectively leverages the relationships between input audio and reference audio\nfrom style reference video to address the style-preserving audio-driven lip\nsync. Specifically, we first develop an advanced Transformer-based model adept\nat predicting lip motion corresponding to the input audio, augmented by the\nstyle information aggregated through cross-attention layers from style\nreference video. Afterwards, to better render the lip motion into realistic\ntalking face video, we devise a conditional latent diffusion model, integrating\nlip motion through modulated convolutional layers and fusing reference facial\nimages via spatial cross-attention layers. Extensive experiments validate the\nefficacy of the proposed approach in achieving precise lip sync, preserving\nspeaking styles, and generating high-fidelity, realistic talking face videos.\n', '  Speech-driven facial animation methods usually contain two main classes, 3D\nand 2D talking face, both of which attract considerable research attention in\nrecent years. However, to the best of our knowledge, the research on 3D talking\nface does not go deeper as 2D talking face, in the aspect of\nlip-synchronization (lip-sync) and speech perception. To mind the gap between\nthe two sub-fields, we propose a learning framework named Learn2Talk, which can\nconstruct a better 3D talking face network by exploiting two expertise points\nfrom the field of 2D talking face. Firstly, inspired by the audio-video sync\nnetwork, a 3D sync-lip expert model is devised for the pursuit of lip-sync\nbetween audio and 3D facial motion. Secondly, a teacher model selected from 2D\ntalking face methods is used to guide the training of the audio-to-3D motions\nregression network to yield more 3D vertex accuracy. Extensive experiments show\nthe advantages of the proposed framework in terms of lip-sync, vertex accuracy\nand speech perception, compared with state-of-the-arts. Finally, we show two\napplications of the proposed framework: audio-visual speech recognition and\nspeech-driven 3D Gaussian Splatting based avatar animation.\n']",Audio-Driven Talking Face Generation
297,296,28,296_alignment_annotation_aligned_aligner,"['alignment', 'annotation', 'aligned', 'aligner', 'language', 'aligning', 'models', 'automated', 'unaligned', 'aligncot']","['alignment', 'aligner', 'weak', 'human', 'self', 'constitutions', 'undesirable', 'responses', 'strong', 'specialization']","['annotation', 'aligner', 'human', 'bootstrapping', 'llms', 'iteratively', 'instruction', 'texttt', 'contrans', 'reformulated']","['  Ensuring alignment with human preferences is a crucial characteristic of\nlarge language models (LLMs). Presently, the primary alignment methods, RLHF\nand DPO, require extensive human annotation, which is expensive despite their\nefficacy. The significant expenses associated with current alignment techniques\nmotivate researchers to investigate the development of annotation-free\nalignment training methods. In pursuit of improved alignment without relying on\nexternal annotation, we introduce Latent Distance Guided Alignment Training\n(LD-Align). This approach seeks to align the model with a high-quality\nsupervised fine-tune dataset using guidance from a latent space. The latent\nspace is generated through sample reconstruction, akin to auto-encoding.\nConsequently, we utilize the distance between sample pairs in the latent space\nto guide DPO-based alignment training. Extensive experimentation and evaluation\nshow the efficacy of our proposed method in achieving notable alignment.\n', '  Alignment is the most critical step in building large language models (LLMs)\nthat meet human needs. With the rapid development of LLMs gradually surpassing\nhuman capabilities, traditional alignment methods based on human-annotation are\nincreasingly unable to meet the scalability demands. Therefore, there is an\nurgent need to explore new sources of automated alignment signals and technical\napproaches. In this paper, we systematically review the recently emerging\nmethods of automated alignment, attempting to explore how to achieve effective,\nscalable, automated alignment once the capabilities of LLMs exceed those of\nhumans. Specifically, we categorize existing automated alignment methods into 4\nmajor categories based on the sources of alignment signals and discuss the\ncurrent status and potential development of each category. Additionally, we\nexplore the underlying mechanisms that enable automated alignment and discuss\nthe essential factors that make automated alignment technologies feasible and\neffective from the fundamental role of alignment.\n', ""  The alignment process changes several properties of a large language model's\n(LLM's) output distribution. We analyze two aspects of post-alignment\ndistributional shift of LLM responses. First, we re-examine previously reported\nreductions in response diversity post-alignment. Our analysis suggests that an\napparent drop in the diversity of responses is largely explained by quality\ncontrol and information aggregation. Alignment suppresses irrelevant and\nunhelpful content while shifting the output distribution toward longer\nresponses that cover information spanning several responses from the base LLM,\nessentially presenting diverse information in a single response. Finding little\nevidence that alignment suppresses useful information, it is natural to ask the\nopposite question: do aligned models surface information that cannot be\nrecovered from base models? Our second investigation shows this is not the case\nand the behavior of aligned models is recoverable from base models without\nfine-tuning. A combination of in-context examples and lower-resolution semantic\nhints about response content can elicit responses from base LLMs that are as\nsimilar to alignment-tuned LLM responses as alignment-tuned LLM responses are\nto each other. Taken together, these results indicate that current alignment\ntechniques capture but do not extend the useful subset of assistant-like base\nLLM behavior, providing further evidence for the Superficial Alignment\nHypothesis. They also show that in-context alignment can go surprisingly far as\na strategy for imitating aligned LLMs without fine-tuning. Our code and data is\navailable at https://github.com/thomlake/investigating-alignment.\n""]",Language Model Alignment Techniques
298,297,28,297_graphs_graph_graph_beta_diffusion_embeddings,"['graphs', 'graph', 'graph_beta_diffusion', 'embeddings', 'edges', 'embedding', 'adjacency', 'models', 'edge', 'manifold']","['graphs', 'graph', 'diffusion', 'generation', 'curvature', 'motif', 'generative', 'permutation', 'hyperbolic', 'latent']","['graph_beta_diffusion', 'adjacency', 'models', 'manifold', 'sparsediff', 'pathway', 'distributional', 'molecular', 'walk', 'swingnn']","['  Diffusion models have made significant contributions to computer vision,\nsparking a growing interest in the community recently regarding the application\nof them to graph generation. Existing discrete graph diffusion models exhibit\nheightened computational complexity and diminished training efficiency. A\npreferable and natural way is to directly diffuse the graph within the latent\nspace. However, due to the non-Euclidean structure of graphs is not isotropic\nin the latent space, the existing latent diffusion models effectively make it\ndifficult to capture and preserve the topological information of graphs. To\naddress the above challenges, we propose a novel geometrically latent diffusion\nframework HypDiff. Specifically, we first establish a geometrically latent\nspace with interpretability measures based on hyperbolic geometry, to define\nanisotropic latent diffusion processes for graphs. Then, we propose a\ngeometrically latent diffusion process that is constrained by both radial and\nangular geometric properties, thereby ensuring the preservation of the original\ntopological properties in the generative graphs. Extensive experimental results\ndemonstrate the superior effectiveness of HypDiff for graph generation with\nvarious topologies.\n', '  Diffusion generative models (DMs) have achieved promising results in image\nand graph generation. However, real-world graphs, such as social networks,\nmolecular graphs, and traffic graphs, generally share non-Euclidean topologies\nand hidden hierarchies. For example, the degree distributions of graphs are\nmostly power-law distributions. The current latent diffusion model embeds the\nhierarchical data in a Euclidean space, which leads to distortions and\ninterferes with modeling the distribution. Instead, hyperbolic space has been\nfound to be more suitable for capturing complex hierarchical structures due to\nits exponential growth property. In order to simultaneously utilize the data\ngeneration capabilities of diffusion models and the ability of hyperbolic\nembeddings to extract latent hierarchical distributions, we propose a novel\ngraph generation method called, Hyperbolic Graph Diffusion Model (HGDM), which\nconsists of an auto-encoder to encode nodes into successive hyperbolic\nembeddings, and a DM that operates in the hyperbolic latent space. HGDM\ncaptures the crucial graph structure distributions by constructing a hyperbolic\npotential node space that incorporates edge information. Extensive experiments\nshow that HGDM achieves better performance in generic graph and molecule\ngeneration benchmarks, with a $48\\%$ improvement in the quality of graph\ngeneration with highly hierarchical structures.\n', '  Generation of graphs is a major challenge for real-world tasks that require\nunderstanding the complex nature of their non-Euclidean structures. Although\ndiffusion models have achieved notable success in graph generation recently,\nthey are ill-suited for modeling the topological properties of graphs since\nlearning to denoise the noisy samples does not explicitly learn the graph\nstructures to be generated. To tackle this limitation, we propose a generative\nframework that models the topology of graphs by explicitly learning the final\ngraph structures of the diffusion process. Specifically, we design the\ngenerative process as a mixture of endpoint-conditioned diffusion processes\nwhich is driven toward the predicted graph that results in rapid convergence.\nWe further introduce a simple parameterization of the mixture process and\ndevelop an objective for learning the final graph structure, which enables\nmaximum likelihood training. Through extensive experimental validation on\ngeneral graph and 2D/3D molecule generation tasks, we show that our method\noutperforms previous generative models, generating graphs with correct topology\nwith both continuous (e.g. 3D coordinates) and discrete (e.g. atom types)\nfeatures. Our code is available at https://github.com/harryjo97/GruM.\n']",Graph Generation with Diffusion Models
299,298,28,298_pollution_meteorological_aerosol_environmental,"['pollution', 'meteorological', 'aerosol', 'environmental', 'forecasting', 'predicting', 'pollutants', 'climate', 'ambient', 'pollutant']","['air', 'pollution', 'concentrations', 'pollutants', 'stations', 'concentration', 'outdoor', 'pollutant', 'hourly', 'fires']","['meteorological', 'aerosol', 'predicting', 'pollutants', 'ambient', 'urban', 'fires', 'sensors', 'occupancy', 'satellite']","['  Policymakers frequently analyze air quality and climate change in isolation,\ndisregarding their interactions. This study explores the influence of specific\nclimate factors on air quality by contrasting a regression model with K-Means\nClustering, Hierarchical Clustering, and Random Forest techniques. We employ\nPhysics-based Deep Learning (PBDL) and Long Short-Term Memory (LSTM) to examine\nthe air pollution predictions. Our analysis utilizes ten years (2009-2018) of\ndaily traffic, weather, and air pollution data from three major cities in\nNorway. Findings from feature selection reveal a correlation between rising\nheating degree days and heightened air pollution levels, suggesting increased\nheating activities in Norway are a contributing factor to worsening air\nquality. PBDL demonstrates superior accuracy in air pollution predictions\ncompared to LSTM. This paper contributes to the growing literature on PBDL\nmethods for more accurate air pollution predictions using environmental\nvariables, aiding policymakers in formulating effective data-driven climate\npolicies.\n', ""  Ambient air pollution remains a critical issue in the United Kingdom, where\ndata on air pollution concentrations form the foundation for interventions\naimed at improving air quality. However, the current air pollution monitoring\nstation network in the UK is characterized by spatial sparsity, heterogeneous\nplacement, and frequent temporal data gaps, often due to issues such as power\noutages. We introduce a scalable data-driven supervised machine learning model\nframework designed to address temporal and spatial data gaps by filling missing\nmeasurements. This approach provides a comprehensive dataset for England\nthroughout 2018 at a 1kmx1km hourly resolution. Leveraging machine learning\ntechniques and real-world data from the sparsely distributed monitoring\nstations, we generate 355,827 synthetic monitoring stations across the study\narea, yielding data valued at approximately \\pounds70 billion. Validation was\nconducted to assess the model's performance in forecasting, estimating missing\nlocations, and capturing peak concentrations. The resulting dataset is of\nparticular interest to a diverse range of stakeholders engaged in downstream\nassessments supported by outdoor air pollution concentration data for NO2, O3,\nPM10, PM2.5, and SO2. This resource empowers stakeholders to conduct studies at\na higher resolution than was previously possible.\n"", '  Ambient air pollution is a pervasive issue with wide-ranging effects on human\nhealth, ecosystem vitality, and economic structures. Utilizing data on ambient\nair pollution concentrations, researchers can perform comprehensive analyses to\nuncover the multifaceted impacts of air pollution across society. To this end,\nwe introduce Environmental Insights, an open-source Python package designed to\ndemocratize access to air pollution concentration data. This tool enables users\nto easily retrieve historical air pollution data and employ a Machine Learning\nmodel for forecasting potential future conditions. Moreover, Environmental\nInsights includes a suite of tools aimed at facilitating the dissemination of\nanalytical findings and enhancing user engagement through dynamic\nvisualizations. This comprehensive approach ensures that the package caters to\nthe diverse needs of individuals looking to explore and understand air\npollution trends and their implications.\n']",Air Pollution Analysis and Forecasting
300,299,28,299_feature_features_selection_featureenvi,"['feature', 'features', 'selection', 'featureenvi', 'predictive', 'discriminative', 'datasets', 'embedding', 'evaluator', 'selecting']","['feature', 'selection', 'features', 'subset', 'set', 'prototype', 'space', 'redundancy', 'search', 'combinations']","['features', 'selection', 'featureenvi', 'predictive', 'discriminative', 'datasets', 'evaluator', 'embedded', 'automan', 'importance']","['  Feature selection prepares the AI-readiness of data by eliminating redundant\nfeatures. Prior research falls into two primary categories: i) Supervised\nFeature Selection, which identifies the optimal feature subset based on their\nrelevance to the target variable; ii) Unsupervised Feature Selection, which\nreduces the feature space dimensionality by capturing the essential information\nwithin the feature set instead of using target variable. However, SFS\napproaches suffer from time-consuming processes and limited generalizability\ndue to the dependence on the target variable and downstream ML tasks. UFS\nmethods are constrained by the deducted feature space is latent and\nuntraceable. To address these challenges, we introduce an innovative framework\nfor feature selection, which is guided by knockoff features and optimized\nthrough reinforcement learning, to identify the optimal and effective feature\nsubset. In detail, our method involves generating ""knockoff"" features that\nreplicate the distribution and characteristics of the original features but are\nindependent of the target variable. Each feature is then assigned a pseudo\nlabel based on its correlation with all the knockoff features, serving as a\nnovel metric for feature evaluation. Our approach utilizes these pseudo labels\nto guide the feature selection process in 3 novel ways, optimized by a single\nreinforced agent: 1). A deep Q-network, pre-trained with the original features\nand their corresponding pseudo labels, is employed to improve the efficacy of\nthe exploration process in feature selection. 2). We introduce unsupervised\nrewards to evaluate the feature subset quality based on the pseudo labels and\nthe feature space reconstruction loss to reduce dependencies on the target\nvariable. 3). A new {\\epsilon}-greedy strategy is used, incorporating insights\nfrom the pseudo labels to make the feature selection process more effective.\n', '  Feature selection aims to identify the most pattern-discriminative feature\nsubset. In prior literature, filter (e.g., backward elimination) and embedded\n(e.g., Lasso) methods have hyperparameters (e.g., top-K, score thresholding)\nand tie to specific models, thus, hard to generalize; wrapper methods search a\nfeature subset in a huge discrete space and is computationally costly. To\ntransform the way of feature selection, we regard a selected feature subset as\na selection decision token sequence and reformulate feature selection as a deep\nsequential generative learning task that distills feature knowledge and\ngenerates decision sequences. Our method includes three steps: (1) We develop a\ndeep variational transformer model over a joint of sequential reconstruction,\nvariational, and performance evaluator losses. Our model can distill feature\nselection knowledge and learn a continuous embedding space to map feature\nselection decision sequences into embedding vectors associated with utility\nscores. (2) We leverage the trained feature subset utility evaluator as a\ngradient provider to guide the identification of the optimal feature subset\nembedding;(3) We decode the optimal feature subset embedding to\nautoregressively generate the best feature selection decision sequence with\nautostop. Extensive experimental results show this generative perspective is\neffective and generic, without large discrete search space and expert-specific\nhyperparameters.\n', '  Feature selection aims to identify the optimal feature subset for enhancing\ndownstream models. Effective feature selection can remove redundant features,\nsave computational resources, accelerate the model learning process, and\nimprove the model overall performance. However, existing works are often\ntime-intensive to identify the effective feature subset within high-dimensional\nfeature spaces. Meanwhile, these methods mainly utilize a single downstream\ntask performance as the selection criterion, leading to the selected subsets\nthat are not only redundant but also lack generalizability. To bridge these\ngaps, we reformulate feature selection through a neuro-symbolic lens and\nintroduce a novel generative framework aimed at identifying short and effective\nfeature subsets. More specifically, we found that feature ID tokens of the\nselected subset can be formulated as symbols to reflect the intricate\ncorrelations among features. Thus, in this framework, we first create a data\ncollector to automatically collect numerous feature selection samples\nconsisting of feature ID tokens, model performance, and the measurement of\nfeature subset redundancy. Building on the collected data, an\nencoder-decoder-evaluator learning paradigm is developed to preserve the\nintelligence of feature selection into a continuous embedding space for\nefficient search. Within the learned embedding space, we leverage a\nmulti-gradient search algorithm to find more robust and generalized embeddings\nwith the objective of improving model performance and reducing feature subset\nredundancy. These embeddings are then utilized to reconstruct the feature ID\ntokens for executing the final feature selection. Ultimately, comprehensive\nexperiments and case studies are conducted to validate the effectiveness of the\nproposed framework.\n']",Feature Selection Methods
301,300,27,300_imagenet_detecting_detection_classifier,"['imagenet', 'detecting', 'detection', 'classifier', 'mlod', 'ood', 'detect', 'outlier', 'detectors', 'deep']","['detection', 'distribution', 'samples', 'prototypes', 'detector', 'penultimate', 'modes', 'feature', 'auxiliary', 'diversities']","['imagenet', 'ood', 'detect', 'outlier', 'benchmarks', 'features', 'dnns', 'saliency', 'dcsod', 'cifar10']","['  Deep neural networks are increasingly used in a wide range of technologies\nand services, but remain highly susceptible to out-of-distribution (OOD)\nsamples, that is, drawn from a different distribution than the original\ntraining set. A common approach to address this issue is to endow deep neural\nnetworks with the ability to detect OOD samples. Several benchmarks have been\nproposed to design and validate OOD detection techniques. However, many of them\nare based on far-OOD samples drawn from very different distributions, and thus\nlack the complexity needed to capture the nuances of real-world scenarios. In\nthis work, we introduce a comprehensive benchmark for OOD detection, based on\nImageNet and Places365, that assigns individual classes as in-distribution or\nout-of-distribution depending on the semantic similarity with the training set.\nSeveral techniques can be used to determine which classes should be considered\nin-distribution, yielding benchmarks with varying properties. Experimental\nresults on different OOD detection techniques show how their measured efficacy\ndepends on the selected benchmark and how confidence-based techniques may\noutperform classifier-based ones on near-OOD samples.\n', '  Deep learning models excel when the data distribution during training aligns\nwith testing data. Yet, their performance diminishes when faced with\nout-of-distribution (OOD) samples, leading to great interest in the field of\nOOD detection. Current approaches typically assume that OOD samples originate\nfrom an unconcentrated distribution complementary to the training distribution.\nWhile this assumption is appropriate in the traditional unsupervised OOD\n(U-OOD) setting, it proves inadequate when considering the place of deployment\nof the underlying deep learning model. To better reflect this real-world\nscenario, we introduce the novel setting of continual U-OOD detection. To\ntackle this new setting, we propose a method that starts from a U-OOD detector,\nwhich is agnostic to the OOD distribution, and slowly updates during deployment\nto account for the actual OOD distribution. Our method uses a new U-OOD scoring\nfunction that combines the Mahalanobis distance with a nearest-neighbor\napproach. Furthermore, we design a confidence-scaled few-shot OOD detector that\noutperforms previous methods. We show our method greatly improves upon strong\nbaselines from related fields.\n', '  Existing Out-of-Distribution (OoD) detection methods address to detect OoD\nsamples from In-Distribution (InD) data mainly by exploring differences in\nfeatures, logits and gradients in Deep Neural Networks (DNNs). We in this work\npropose a new perspective upon loss landscape and mode ensemble to investigate\nOoD detection. In the optimization of DNNs, there exist many local optima in\nthe parameter space, or namely modes. Interestingly, we observe that these\nindependent modes, which all reach low-loss regions with InD data (training and\ntest data), yet yield significantly different loss landscapes with OoD data.\nSuch an observation provides a novel view to investigate the OoD detection from\nthe loss landscape, and further suggests significantly fluctuating OoD\ndetection performance across these modes. For instance, FPR values of the\nRankFeat method can range from 46.58% to 84.70% among 5 modes, showing\nuncertain detection performance evaluations across independent modes. Motivated\nby such diversities on OoD loss landscape across modes, we revisit the deep\nensemble method for OoD detection through mode ensemble, leading to improved\nperformance and benefiting the OoD detector with reduced variances. Extensive\nexperiments covering varied OoD detectors and network structures illustrate\nhigh variances across modes and validate the superiority of mode ensemble in\nboosting OoD detection. We hope this work could attract attention in the view\nof independent modes in the loss landscape of OoD data and more reliable\nevaluations on OoD detectors.\n']",Out-of-Distribution Detection in Deep Learning
302,301,27,301_forgetting_memory_forgetful_forget,"['forgetting', 'memory', 'forgetful', 'forget', 'continual', 'languages', 'pretraining', 'language', 'training', 'catastrophic']","['forgetting', 'catastrophic', 'fine', 'tuning', 'continual', 'domain', 'general', 'knowledge', 'languages', 'pre']","['forgetting', 'languages', 'pretraining', 'incrementally', 'resetting', 'instruction', 'transllm', 'plms', 'lll', 'versatility']","['  Recent advances in Large Language Models (LLMs) have exhibited remarkable\nproficiency across various tasks. Given the potent applications of LLMs in\nnumerous fields, there has been a surge in LLM development. In developing LLMs,\na common practice involves continual pre-training on previously fine-tuned\nmodels. However, this can lead to catastrophic forgetting. In our work, we\ninvestigate the phenomenon of forgetting that occurs during continual\npre-training on an existing fine-tuned LLM. We evaluate the impact of\ncontinuous pre-training on the fine-tuned LLM across various dimensions,\nincluding output format, knowledge, and reliability. Experiment results\nhighlight the non-trivial challenge of addressing catastrophic forgetting\nduring continual pre-training, especially the repetition issue.\n', ""  Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\nwhen a model forgets previously learned information while acquiring new\nknowledge. As large language models (LLMs) have demonstrated remarkable\nperformance, it is intriguing to investigate whether CF exists during the\ncontinual instruction tuning of LLMs. This study empirically evaluates the\nforgetting phenomenon in LLMs' knowledge during continual instruction tuning\nfrom the perspectives of domain knowledge, reasoning, and reading\ncomprehension. The experiments reveal that catastrophic forgetting is generally\nobserved in LLMs ranging from 1b to 7b parameters. Moreover, as the model scale\nincreases, the severity of forgetting intensifies. Comparing the decoder-only\nmodel BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less\nforgetting and retains more knowledge. Interestingly, we also observe that LLMs\ncan mitigate language biases, such as gender bias, during continual\nfine-tuning. Furthermore, our findings indicate that ALPACA maintains more\nknowledge and capacity compared to LLAMA during continual fine-tuning,\nsuggesting that general instruction tuning can help alleviate the forgetting\nphenomenon in LLMs during subsequent fine-tuning processes.\n"", '  We study and quantify the problem of forgetting when fine-tuning pre-trained\nlarge language models (LLMs) on a downstream task. We find that\nparameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters\n(LoRA), still suffer from catastrophic forgetting. In particular, we identify a\nstrong inverse linear relationship between the fine-tuning performance and the\namount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise\nscaling laws that show forgetting increases as a shifted power law in the\nnumber of parameters fine-tuned and the number of update steps. We also examine\nthe impact of forgetting on knowledge, reasoning, and the safety guardrails\ntrained into Llama 2 7B chat. Our study suggests that forgetting cannot be\navoided through early stopping or by varying the number of parameters\nfine-tuned. We believe this opens up an important safety-critical direction for\nfuture research to evaluate and develop fine-tuning schemes which mitigate\nforgetting\n']",Catastrophic Forgetting in Large Language Models
303,302,27,302_spellchecking_corpus_speller_spell,"['spellchecking', 'corpus', 'speller', 'spell', 'spelling', 'misspelled', 'pinyin', 'bspell', 'phonetic', 'pronunciations']","['spelling', 'correction', 'word', 'errors', 'character', 'spell', 'phonetic', 'error', 'characters', 'polyphone']","['spellchecking', 'corpus', 'misspelled', 'pinyin', 'pronunciations', 'tokenization', 'persian', 'ocr', 'correction', 'overcorrection']","['  Chinese Spelling Correction (CSC) commonly lacks large-scale high-quality\ncorpora, due to the labor-intensive labeling of spelling errors in real-life\nhuman writing or typing scenarios. Two data augmentation methods are widely\nadopted: (1) \\textit{Random Replacement} with the guidance of confusion sets\nand (2) \\textit{OCR/ASR-based Generation} that simulates character misusing.\nHowever, both methods inevitably introduce noisy data (e.g., false spelling\nerrors), potentially leading to over-correction. By carefully analyzing the two\ntypes of corpora, we find that though the latter achieves more robust\ngeneralization performance, the former yields better-calibrated CSC models. We\nthen provide a theoretical analysis of this empirical observation, based on\nwhich a corpus refining strategy is proposed. Specifically, OCR/ASR-based data\nsamples are fed into a well-calibrated CSC model trained on random\nreplacement-based corpora and then filtered based on prediction confidence. By\nlearning a simple BERT-based model on the refined OCR/ASR-based corpus, we set\nup impressive state-of-the-art performance on three widely-used benchmarks,\nwhile significantly alleviating over-correction (e.g., lowering false positive\npredictions).\n', ""  This research introduces a state-of-the-art Persian spelling correction\nsystem that seamlessly integrates deep learning techniques with phonetic\nanalysis, significantly enhancing the accuracy and efficiency of natural\nlanguage processing (NLP) for Persian. Utilizing a fine-tuned language\nrepresentation model, our methodology effectively combines deep contextual\nanalysis with phonetic insights, adeptly correcting both non-word and real-word\nspelling errors. This strategy proves particularly effective in tackling the\nunique complexities of Persian spelling, including its elaborate morphology and\nthe challenge of homophony. A thorough evaluation on a wide-ranging dataset\nconfirms our system's superior performance compared to existing methods, with\nimpressive F1-Scores of 0.890 for detecting real-word errors and 0.905 for\ncorrecting them. Additionally, the system demonstrates a strong capability in\nnon-word error correction, achieving an F1-Score of 0.891. These results\nillustrate the significant benefits of incorporating phonetic insights into\ndeep learning models for spelling correction. Our contributions not only\nadvance Persian language processing by providing a versatile solution for a\nvariety of NLP applications but also pave the way for future research in the\nfield, emphasizing the critical role of phonetic analysis in developing\neffective spelling correction system.\n"", '  Automatic spelling correction stands as a pivotal challenge within the ambit\nof natural language processing (NLP), demanding nuanced solutions. Traditional\nspelling correction techniques are typically only capable of detecting and\ncorrecting non-word errors, such as typos and misspellings. However,\ncontext-sensitive errors, also known as real-word errors, are more challenging\nto detect because they are valid words that are used incorrectly in a given\ncontext. The Persian language, characterized by its rich morphology and complex\nsyntax, presents formidable challenges to automatic spelling correction\nsystems. Furthermore, the limited availability of Persian language resources\nmakes it difficult to train effective spelling correction models. This paper\nintroduces a cutting-edge approach for precise and efficient real-word error\ncorrection in Persian text. Our methodology adopts a structured, multi-tiered\napproach, employing semantic analysis, feature selection, and advanced\nclassifiers to enhance error detection and correction efficacy. The innovative\narchitecture discovers and stores semantic similarities between words and\nphrases in Persian text. The classifiers accurately identify real-word errors,\nwhile the semantic ranking algorithm determines the most probable corrections\nfor real-word errors, taking into account specific spelling correction and\ncontext properties such as context, semantic similarity, and edit-distance\nmeasures. Evaluations have demonstrated that our proposed method surpasses\nprevious Persian real-word error correction models. Our method achieves an\nimpressive F-measure of 96.6% in the detection phase and an accuracy of 99.1%\nin the correction phase. These results clearly indicate that our approach is a\nhighly promising solution for automatic real-word error correction in Persian\ntext.\n']",Spelling Correction Techniques
304,303,27,303_chatbots_chatbot_conversational_conversation,"['chatbots', 'chatbot', 'conversational', 'conversation', 'chat', 'chatatc', 'ai', 'prompts', 'assistant', 'chatgpt']","['chatbots', 'chatbot', 'conversational', 'responses', 'chat', 'users', 'interfaces', 'user', 'artificial', 'intelligence']","['chatbots', 'conversational', 'prompts', 'agents', 'interact', 'intelligence', 'openai', 'personalized', 'bard', 'innovations']","[""  Large language models (LLMs) provide a new way to build chatbots by accepting\nnatural language prompts. Yet, it is unclear how to design prompts to power\nchatbots to carry on naturalistic conversations while pursuing a given goal,\nsuch as collecting self-report data from users. We explore what design factors\nof prompts can help steer chatbots to talk naturally and collect data reliably.\nTo this aim, we formulated four prompt designs with different structures and\npersonas. Through an online study (N = 48) where participants conversed with\nchatbots driven by different designs of prompts, we assessed how prompt designs\nand conversation topics affected the conversation flows and users' perceptions\nof chatbots. Our chatbots covered 79% of the desired information slots during\nconversations, and the designs of prompts and topics significantly influenced\nthe conversation flows and the data collection performance. We discuss the\nopportunities and challenges of building chatbots with LLMs.\n"", ""  This research provides an in-depth comprehensive review of the progress of\nchatbot technology over time, from the initial basic systems relying on rules\nto today's advanced conversational bots powered by artificial intelligence.\nSpanning many decades, the paper explores the major milestones, innovations,\nand paradigm shifts that have driven the evolution of chatbots. Looking back at\nthe very basic statistical model in 1906 via the early chatbots, such as ELIZA\nand ALICE in the 1960s and 1970s, the study traces key innovations leading to\ntoday's advanced conversational agents, such as ChatGPT and Google Bard. The\nstudy synthesizes insights from academic literature and industry sources to\nhighlight crucial milestones, including the introduction of Turing tests,\ninfluential projects such as CALO, and recent transformer-based models. Tracing\nthe path forward, the paper highlights how natural language processing and\nmachine learning have been integrated into modern chatbots for more\nsophisticated capabilities. This chronological survey of the chatbot landscape\nprovides a holistic reference to understand the technological and historical\nfactors propelling conversational AI. By synthesizing learnings from this\nhistorical analysis, the research offers important context about the\ndevelopmental trajectory of chatbots and their immense future potential across\nvarious field of application which could be the potential take ways for the\nrespective research community and stakeholders.\n"", ""  The past few decades have witnessed an upsurge in data, forming the\nfoundation for data-hungry, learning-based AI technology. Conversational\nagents, often referred to as AI chatbots, rely heavily on such data to train\nlarge language models (LLMs) and generate new content (knowledge) in response\nto user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have\nset new standards in the AI community. This paper presents a complete survey of\nthe evolution and deployment of LLM-based chatbots in various sectors. We first\nsummarize the development of foundational chatbots, followed by the evolution\nof LLMs, and then provide an overview of LLM-based chatbots currently in use\nand those in the development phase. Recognizing AI chatbots as tools for\ngenerating new knowledge, we explore their diverse applications across various\nindustries. We then discuss the open challenges, considering how the data used\nto train the LLMs and the misuse of the generated knowledge can cause several\nissues. Finally, we explore the future outlook to augment their efficiency and\nreliability in numerous applications. By addressing key milestones and the\npresent-day context of LLM-based chatbots, our survey invites readers to delve\ndeeper into this realm, reflecting on how their next generation will reshape\nconversational AI.\n""]",Conversational AI and Chatbots
305,304,27,304_imagenet_gans_robust_dm_generated_image_detection_generative,"['imagenet', 'gans', 'robust_dm_generated_image_detection', 'generative', 'augmentation', 'deepaugment', 'augment', 'imagenet_d', 'augmentations', 'encode']","['diffusion', 'image', 'augmentation', 'images', 'intrinsics', 'diversity', 'generative', 'synthesis', 'synthetic', 'consistency']","['imagenet', 'gans', 'robust_dm_generated_image_detection', 'deepaugment', 'encode', 'trained', 'refine', 'coco', 'classifier', 'sgid']","['  Image data augmentation constitutes a critical methodology in modern computer\nvision tasks, since it can facilitate towards enhancing the diversity and\nquality of training datasets; thereby, improving the performance and robustness\nof machine learning models in downstream tasks. In parallel, augmentation\napproaches can also be used for editing/modifying a given image in a context-\nand semantics-aware way. Diffusion Models (DMs), which comprise one of the most\nrecent and highly promising classes of methods in the field of generative\nArtificial Intelligence (AI), have emerged as a powerful tool for image data\naugmentation, capable of generating realistic and diverse images by learning\nthe underlying data distribution. The current study realizes a systematic,\ncomprehensive and in-depth review of DM-based approaches for image\naugmentation, covering a wide range of strategies, tasks and applications. In\nparticular, a comprehensive analysis of the fundamental principles, model\narchitectures and training strategies of DMs is initially performed.\nSubsequently, a taxonomy of the relevant image augmentation methods is\nintroduced, focusing on techniques regarding semantic manipulation,\npersonalization and adaptation, and application-specific augmentation tasks.\nThen, performance assessment methodologies and respective evaluation metrics\nare analyzed. Finally, current challenges and future research directions in the\nfield are discussed.\n', '  Existing image augmentation methods consist of two categories:\nperturbation-based methods and generative methods. Perturbation-based methods\napply pre-defined perturbations to augment an original image, but only locally\nvary the image, thus lacking image diversity. In contrast, generative methods\nbring more image diversity in the augmented images but may not preserve\nsemantic consistency, thus incorrectly changing the essential semantics of the\noriginal image. To balance image diversity and semantic consistency in\naugmented images, we propose SGID, a Semantic-guided Generative Image\naugmentation method with Diffusion models for image classification.\nSpecifically, SGID employs diffusion models to generate augmented images with\ngood image diversity. More importantly, SGID takes image labels and captions as\nguidance to maintain semantic consistency between the augmented and original\nimages. Experimental results show that SGID outperforms the best augmentation\nbaseline by 1.72% on ResNet-50 (from scratch), 0.33% on ViT (ImageNet-21k), and\n0.14% on CLIP-ViT (LAION-2B). Moreover, SGID can be combined with other image\naugmentation baselines and further improves the overall performance. We\ndemonstrate the semantic consistency and image diversity of SGID through\nquantitative human and automated evaluations, as well as qualitative case\nstudies.\n', ""  In computer vision, it is well-known that a lack of data diversity will\nimpair model performance. In this study, we address the challenges of enhancing\nthe dataset diversity problem in order to benefit various downstream tasks such\nas object detection and instance segmentation. We propose a simple yet\neffective data augmentation approach by leveraging advancements in generative\nmodels, specifically text-to-image synthesis technologies like Stable\nDiffusion. Our method focuses on generating variations of labeled real images,\nutilizing generative object and background augmentation via inpainting to\naugment existing training data without the need for additional annotations. We\nfind that background augmentation, in particular, significantly improves the\nmodels' robustness and generalization capabilities. We also investigate how to\nadjust the prompt and mask to ensure the generated content comply with the\nexisting annotations. The efficacy of our augmentation techniques is validated\nthrough comprehensive evaluations of the COCO dataset and several other key\nobject detection benchmarks, demonstrating notable enhancements in model\nperformance across diverse scenarios. This approach offers a promising solution\nto the challenges of dataset enhancement, contributing to the development of\nmore accurate and robust computer vision models.\n""]",Image Augmentation with Generative Models
306,305,26,305_audioset_audio_multimodal_visual,"['audioset', 'audio', 'multimodal', 'visual', 'videos', 'scenes', 'supervised', 'auditory', 'embeddings', 'encoder']","['audio', 'visual', 'video', 'sound', 'cues', 'colorization', 'localization', 'videos', 'synchronization', 'segmentation']","['audioset', 'multimodal', 'encoder', 'modality', 'cues', 'colorization', 'frames', 'binaural', 'vggsound', 'correspondence']","['  Audio-Visual Segmentation (AVS) aims to identify, at the pixel level, the\nobject in a visual scene that produces a given sound. Current AVS methods rely\non costly fine-grained annotations of mask-audio pairs, making them impractical\nfor scalability. To address this, we introduce unsupervised AVS, eliminating\nthe need for such expensive annotation. To tackle this more challenging\nproblem, we propose an unsupervised learning method, named Modality\nCorrespondence Alignment (MoCA), which seamlessly integrates off-the-shelf\nfoundation models like DINO, SAM, and ImageBind. This approach leverages their\nknowledge complementarity and optimizes their joint usage for multi-modality\nassociation. Initially, we estimate positive and negative image pairs in the\nfeature space. For pixel-level association, we introduce an audio-visual\nadapter and a novel pixel matching aggregation strategy within the image-level\ncontrastive learning framework. This allows for a flexible connection between\nobject appearance and audio signal at the pixel level, with tolerance to\nimaging variations such as translation and rotation. Extensive experiments on\nthe AVSBench (single and multi-object splits) and AVSS datasets demonstrate\nthat our MoCA outperforms strongly designed baseline methods and approaches\nsupervised counterparts, particularly in complex scenarios with multiple\nauditory objects. Notably when comparing mIoU, MoCA achieves a substantial\nimprovement over baselines in both the AVSBench (S4: +17.24%; MS3: +67.64%) and\nAVSS (+19.23%) audio-visual segmentation challenges.\n', '  Audio-Visual Segmentation (AVS) aims to achieve pixel-level localization of\nsound sources in videos, while Audio-Visual Semantic Segmentation (AVSS), as an\nextension of AVS, further pursues semantic understanding of audio-visual\nscenes. However, since the AVSS task requires the establishment of audio-visual\ncorrespondence and semantic understanding simultaneously, we observe that\nprevious methods have struggled to handle this mashup of objectives in\nend-to-end training, resulting in insufficient learning and sub-optimization.\nTherefore, we propose a two-stage training strategy called \\textit{Stepping\nStones}, which decomposes the AVSS task into two simple subtasks from\nlocalization to semantic understanding, which are fully optimized in each stage\nto achieve step-by-step global optimization. This training strategy has also\nproved its generalization and effectiveness on existing methods. To further\nimprove the performance of AVS tasks, we propose a novel framework Adaptive\nAudio Visual Segmentation, in which we incorporate an adaptive audio query\ngenerator and integrate masked attention into the transformer decoder,\nfacilitating the adaptive fusion of visual and audio features. Extensive\nexperiments demonstrate that our methods achieve state-of-the-art results on\nall three AVS benchmarks. The project homepage can be accessed at\nhttps://gewu-lab.github.io/stepping_stones/.\n', '  Recent advances in multimodal LLMs, have led to several video-text models\nbeing proposed for critical video-related tasks. However, most of the previous\nworks support visual input only, essentially muting the audio signal in the\nvideo. Few models that support both audio and visual input, are not explicitly\ntrained on audio data. Hence, the effect of audio towards video understanding\nis largely unexplored. To this end, we propose a model architecture that\nhandles audio-visual inputs explicitly. We train our model with both audio and\nvisual data from a video instruction-tuning dataset. Comparison with\nvision-only baselines, and other audio-visual models showcase that training on\naudio data indeed leads to improved grounding of responses. For better\nevaluation of audio-visual models, we also release a human-annotated benchmark\ndataset, with audio-aware question-answer pairs.\n']",Audio-Visual Understanding
307,306,26,306_audiological_audio_recordings_respiratory,"['audiological', 'audio', 'recordings', 'respiratory', 'recording', 'voice', 'lung', 'acoustic', 'als', 'acoustics']","['respiratory', 'sounds', 'sound', 'lung', 'speech', 'audio', 'voice', 'acoustic', 'audiological', 'cough']","['audiological', 'recordings', 'lungs', 'spectrogram', 'waveforms', 'svm', 'diagnostic', 'stethoscope', 'biomarkers', 'vowels']","['  Compared with invasive examinations that require tissue sampling, respiratory\nsound testing is a non-invasive examination method that is safer and easier for\npatients to accept. In this study, we introduce Rene, a pioneering large-scale\nmodel tailored for respiratory sound recognition. Rene has been rigorously\nfine-tuned with an extensive dataset featuring a broad array of respiratory\naudio samples, targeting disease detection, sound pattern classification, and\nevent identification. Our innovative approach applies a pre-trained speech\nrecognition model to process respiratory sounds, augmented with patient medical\nrecords. The resulting multi-modal deep-learning framework addresses\ninterpretability and real-time diagnostic challenges that have hindered\nprevious respiratory-focused models. Benchmark comparisons reveal that Rene\nsignificantly outperforms existing models, achieving improvements of 10.27%,\n16.15%, 15.29%, and 18.90% in respiratory event detection and audio\nclassification on the SPRSound database. Disease prediction accuracy on the\nICBHI database improved by 23% over the baseline in both mean average and\nharmonic scores. Moreover, we have developed a real-time respiratory sound\ndiscrimination system utilizing the Rene architecture. Employing\nstate-of-the-art Edge AI technology, this system enables rapid and accurate\nresponses for respiratory sound\nauscultation(https://github.com/zpforlove/Rene).\n', '  This study aims to develop an auxiliary diagnostic system for classifying\nabnormal lung respiratory sounds, enhancing the accuracy of automatic abnormal\nbreath sound classification through an innovative multi-label learning approach\nand multi-head attention mechanism. Addressing the issue of class imbalance and\nlack of diversity in existing respiratory sound datasets, our study employs a\nlightweight and highly accurate model, using a two-dimensional label set to\nrepresent multiple respiratory sound characteristics. Our method achieved a\n59.2% ICBHI score in the four-category task on the ICBHI2017 dataset,\ndemonstrating its advantages in terms of lightweight and high accuracy. This\nstudy not only improves the accuracy of automatic diagnosis of lung respiratory\nsound abnormalities but also opens new possibilities for clinical applications.\n', '  Respiratory audio, such as coughing and breathing sounds, has predictive\npower for a wide range of healthcare applications, yet is currently\nunder-explored. The main problem for those applications arises from the\ndifficulty in collecting large labeled task-specific data for model\ndevelopment. Generalizable respiratory acoustic foundation models pretrained\nwith unlabeled data would offer appealing advantages and possibly unlock this\nimpasse. However, given the safety-critical nature of healthcare applications,\nit is pivotal to also ensure openness and replicability for any proposed\nfoundation model solution. To this end, we introduce OPERA, an OPEn Respiratory\nAcoustic foundation model pretraining and benchmarking system, as the first\napproach answering this need. We curate large-scale respiratory audio datasets\n(~136K samples, 440 hours), pretrain three pioneering foundation models, and\nbuild a benchmark consisting of 19 downstream respiratory health tasks for\nevaluation. Our pretrained models demonstrate superior performance (against\nexisting acoustic models pretrained with general audio on 16 out of 19 tasks)\nand generalizability (to unseen datasets and new respiratory audio modalities).\nThis highlights the great promise of respiratory acoustic foundation models and\nencourages more studies using OPERA as an open resource to accelerate research\non respiratory audio for health. The system is accessible from\nhttps://github.com/evelyn0414/OPERA.\n']",Respiratory Sound Analysis and Diagnosis
308,307,26,307_prompts_prompt_prompting_promptintern,"['prompts', 'prompt', 'prompting', 'promptintern', 'language', 'pretrained', 'tuning', 'optimizing', 'tasks', 'monopara']","['prompt', 'tuning', 'prompts', 'fine', 'task', 'downstream', 'instruction', 'tasks', 'subspaces', 'shot']","['prompts', 'pretrained', 'tuning', 'tasks', 'monopara', 'instruction', 'highlighter', 'lms', 'activation', 'texttt']","['  Prompt tuning is a modular and efficient solution for training large language\nmodels (LLMs). One of its main advantages is task modularity, making it\nsuitable for multi-task problems. However, current soft-prompt-based methods\noften sacrifice multi-task modularity, requiring the training process to be\nfully or partially repeated for each newly added task. While recent work on\ntask vectors applied arithmetic operations on full model weights to achieve the\ndesired multi-task performance, a similar approach for soft-prompts is still\nmissing. To this end, we introduce Task Prompt Vectors, created by element-wise\ndifference between weights of tuned soft-prompts and their random\ninitialization. Experimental results on 12 NLU datasets show that task prompt\nvectors can be used in low-resource settings to effectively initialize prompt\ntuning on similar tasks. In addition, we show that task prompt vectors are\nindependent of the random initialization of prompt tuning. This allows prompt\narithmetics with the pre-trained vectors from different tasks. In this way, by\narithmetic addition of task prompt vectors from multiple tasks, we are able to\noutperform a state-of-the-art baseline in some cases.\n', '  Soft prompt tuning is a widely studied parameter-efficient fine-tuning\nmethod. However, it has a clear drawback: many soft tokens must be inserted\ninto the input sequences to guarantee downstream performance. As a result, soft\nprompt tuning is less considered than Low-rank adaptation (LoRA) in the large\nlanguage modeling (LLM) era. In this work, we propose a novel prompt tuning\nmethod, Instruction-Aware Prompt Tuning (IAPT), that requires only four soft\ntokens. First, we install a parameter-efficient soft prompt generator at each\nTransformer layer to generate idiosyncratic soft prompts for each input\ninstruction. The generated soft prompts can be seen as a semantic summary of\nthe input instructions and can effectively guide the output generation. Second,\nthe soft prompt generators are modules with a bottleneck architecture\nconsisting of a self-attention pooling operation, two linear projections, and\nan activation function. Pilot experiments show that prompt generators at\ndifferent Transformer layers require different activation functions. Thus, we\npropose to learn the idiosyncratic activation functions for prompt generators\nautomatically with the help of rational functions. We have conducted\nexperiments on various tasks, and the experimental results demonstrate that (a)\nour IAPT method can outperform the recent baselines with comparable tunable\nparameters. (b) Our IAPT method is more efficient than LoRA under the\nsingle-backbone multi-tenant setting.\n', '  Prompt tuning is a promising method to fine-tune a pre-trained language model\nwithout retraining its large-scale parameters. Instead, it attaches a soft\nprompt to the input text, whereby downstream tasks can be well adapted by\nmerely learning the embeddings of prompt tokens. Nevertheless, existing methods\nstill suffer from two challenges: (i) they are hard to balance accuracy and\nefficiency. A longer (shorter) soft prompt generally leads to a better(worse)\naccuracy but at the cost of more (less) training time. (ii)The performance may\nnot be consistent when adapting to different downstream tasks. We attribute it\nto the same embedding space but responsible for different requirements of\ndownstream tasks. To address these issues, we propose an Efficient Prompt\nTuning method (EPT) by multi-space projection and prompt fusion. Specifically,\nit decomposes a given soft prompt into a shorter prompt and two low-rank\nmatrices, significantly reducing the training time. Accuracy is also enhanced\nby leveraging low-rank matrices and the short prompt as additional knowledge\nsources to enrich the semantics of the original short prompt. In addition, we\nproject the soft prompt into multiple subspaces to improve the performance\nconsistency, and then adaptively learn the combination weights of different\nspaces through a gating network. Experiments on 13 natural language processing\ndownstream tasks show that our method significantly and consistently\noutperforms 11 comparison methods with the relative percentage of improvements\nup to 12.9%, and training time decreased by 14%.\n']",Prompt Tuning for Language Models
309,308,26,308_optimizing_hyperparameters_regularization_learning,"['optimizing', 'hyperparameters', 'regularization', 'learning', 'hyperparameter', 'optimization', 'minimization', 'gradient', 'gradients', 'objectives']","['multi', 'task', 'hyperparameter', 'objective', 'hyperparameters', 'optimization', 'outlier', 'hyper', 'gradient', 'priority']","['optimizing', 'hyperparameters', 'learning', 'gradient', 'regularize', 'gmm', 'ml', 'descent', 'objective', 'hpo']","['  Multi-objective optimization (MOO) is receiving more attention in various\nfields such as multi-task learning. Recent works provide some effective\nalgorithms with theoretical analysis but they are limited by the standard\n$L$-smooth or bounded-gradient assumptions, which are typically unsatisfactory\nfor neural networks, such as recurrent neural networks (RNNs) and transformers.\nIn this paper, we study a more general and realistic class of $\\ell$-smooth\nloss functions, where $\\ell$ is a general non-decreasing function of gradient\nnorm. We develop two novel single-loop algorithms for $\\ell$-smooth MOO\nproblems, Generalized Smooth Multi-objective Gradient descent (GSMGrad) and its\nstochastic variant, Stochastic Generalized Smooth Multi-objective Gradient\ndescent (SGSMGrad), which approximate the conflict-avoidant (CA) direction that\nmaximizes the minimum improvement among objectives. We provide a comprehensive\nconvergence analysis of both algorithms and show that they converge to an\n$\\epsilon$-accurate Pareto stationary point with a guaranteed $\\epsilon$-level\naverage CA distance (i.e., the gap between the updating direction and the CA\ndirection) over all iterations, where totally $\\mathcal{O}(\\epsilon^{-2})$ and\n$\\mathcal{O}(\\epsilon^{-4})$ samples are needed for deterministic and\nstochastic settings, respectively. Our algorithms can also guarantee a tighter\n$\\epsilon$-level CA distance in each iteration using more samples. Moreover, we\npropose a practical variant of GSMGrad named GSMGrad-FA using only\nconstant-level time and space, while achieving the same performance guarantee\nas GSMGrad. Our experiments validate our theory and demonstrate the\neffectiveness of the proposed methods.\n', '  Hyperparameter optimization (HPO) is an important step in machine learning\n(ML) model development, but common practices are archaic -- primarily relying\non manual or grid searches. This is partly because adopting advanced HPO\nalgorithms introduces added complexity to the workflow, leading to longer\ncomputation times. This poses a notable challenge to ML applications, as\nsuboptimal hyperparameter selections curtail the potential of ML model\nperformance, ultimately obstructing the full exploitation of ML techniques. In\nthis article, we present a two-step HPO method as a strategic solution to\ncurbing computational demands and wait times, gleaned from practical\nexperiences in applied ML parameterization work. The initial phase involves a\npreliminary evaluation of hyperparameters on a small subset of the training\ndataset, followed by a re-evaluation of the top-performing candidate models\npost-retraining with the entire training dataset. This two-step HPO method is\nuniversally applicable across HPO search algorithms, and we argue it has\nattractive efficiency gains.\n  As a case study, we present our recent application of the two-step HPO method\nto the development of neural network emulators for aerosol activation. Although\nour primary use case is a data-rich limit with many millions of samples, we\nalso find that using up to 0.0025% of the data (a few thousand samples) in the\ninitial step is sufficient to find optimal hyperparameter configurations from\nmuch more extensive sampling, achieving up to 135-times speedup. The benefits\nof this method materialize through an assessment of hyperparameters and model\nperformance, revealing the minimal model complexity required to achieve the\nbest performance. The assortment of top-performing models harvested from the\nHPO process allows us to choose a high-performing model with a low inference\ncost for efficient use in global climate models (GCMs).\n', ""  The goal of multi-task learning is to enable more efficient learning than\nsingle task learning by sharing model structures for a diverse set of tasks. A\nstandard multi-task learning objective is to minimize the average loss across\nall tasks. While straightforward, using this objective often results in much\nworse final performance for each task than learning them independently. A major\nchallenge in optimizing a multi-task model is the conflicting gradients, where\ngradients of different task objectives are not well aligned so that following\nthe average gradient direction can be detrimental to specific tasks'\nperformance. Previous work has proposed several heuristics to manipulate the\ntask gradients for mitigating this problem. But most of them lack convergence\nguarantee and/or could converge to any Pareto-stationary point. In this paper,\nwe introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the\naverage loss function, while leveraging the worst local improvement of\nindividual tasks to regularize the algorithm trajectory. CAGrad balances the\nobjectives automatically and still provably converges to a minimum over the\naverage loss. It includes the regular gradient descent (GD) and the multiple\ngradient descent algorithm (MGDA) in the multi-objective optimization (MOO)\nliterature as special cases. On a series of challenging multi-task supervised\nlearning and reinforcement learning tasks, CAGrad achieves improved performance\nover prior state-of-the-art multi-objective gradient manipulation methods.\n""]",Optimizing Hyperparameters in Multi-Task Learning
310,309,26,309_birdnet_bioacoustics_recordings_bioacoustic,"['birdnet', 'bioacoustics', 'recordings', 'bioacoustic', 'audio', 'acoustic', 'supervised', 'recognizing', 'vocalization', 'vocalizations']","['bird', 'underwater', 'animal', 'acoustic', 'sounds', 'species', 'bioacoustics', 'bioacoustic', 'sound', 'audio']","['birdnet', 'bioacoustics', 'recordings', 'acoustic', 'vocalizations', 'animal2vec', 'spectrograms', 'classifying', 'biodiversity', 'whales']","['  Bioacoustic sound event detection allows for better understanding of animal\nbehavior and for better monitoring biodiversity using audio. Deep learning\nsystems can help achieve this goal, however it is difficult to acquire\nsufficient annotated data to train these systems from scratch. To address this\nlimitation, the Detection and Classification of Acoustic Scenes and Events\n(DCASE) community has recasted the problem within the framework of few-shot\nlearning and organize an annual challenge for learning to detect animal sounds\nfrom only five annotated examples. In this work, we regularize supervised\ncontrastive pre-training to learn features that can transfer well on new target\ntasks with animal sounds unseen during training, achieving a high F-score of\n61.52%(0.48) when no feature adaptation is applied, and an F-score of\n68.19%(0.75) when we further adapt the learned features for each new target\ntask. This work aims to lower the entry bar to few-shot bioacoustic sound event\ndetection by proposing a simple and yet effective framework for this task, by\nalso providing open-source code.\n', '  This paper presents a novel deep learning approach for analyzing massive\nunderwater acoustic data by leveraging a model trained on a broad spectrum of\nnon-underwater (aerial) sounds. Recognizing the challenge in labeling vast\namounts of underwater data, we propose a two-fold methodology to accelerate\nthis labor-intensive procedure.\n  The first part of our approach involves PCA and UMAP visualization of the\nunderwater data using the feature vectors of an aerial sound recognition model.\nThis enables us to cluster the data in a two dimensional space and listen to\npoints within these clusters to understand their defining characteristics. This\ninnovative method simplifies the process of selecting candidate labels for\nfurther training.\n  In the second part, we train a neural network model using both the selected\nunderwater data and the non-underwater dataset. We conducted a quantitative\nanalysis to measure the precision, recall, and F1 score of our model for\nrecognizing airgun sounds, a common type of underwater sound. The F1 score\nachieved by our model exceeded 84.3%, demonstrating the effectiveness of our\napproach in analyzing underwater acoustic data.\n  The methodology presented in this paper holds significant potential to reduce\nthe amount of labor required in underwater data analysis and opens up new\npossibilities for further research in the field of cross-domain data analysis.\n', ""  Recently, scientists have proposed several deep learning models to monitor\nthe diversity of bird species. These models can detect bird species with high\naccuracy by analyzing acoustic signals. However, traditional deep learning\nalgorithms are black-box models that provide no insight into their\ndecision-making process. For domain experts, such as ornithologists, it is\ncrucial that these models are not only efficient, but also interpretable in\norder to be used as assistive tools. In this study, we present an adaption of\nthe Prototypical Part Network (ProtoPNet) for audio classification that\nprovides inherent interpretability through its model architecture. Our approach\nis based on a ConvNeXt backbone architecture for feature extraction and learns\nprototypical patterns for each bird species using spectrograms of the training\ndata. Classification of new data is done by comparison with these prototypes in\nlatent space, which simultaneously serve as easily understandable explanations\nfor the model's decisions. We evaluated the performance of our model on seven\ndifferent datasets representing bird species from different geographical\nregions. In our experiments, the model showed excellent results, achieving an\naverage AUROC of 0.82 and an average cmAP of 0.37 across the seven datasets,\nmaking it comparable to state-of-the-art black-box models for bird sound\nclassification. Thus, this work demonstrates that even for the challenging task\nof bioacoustic bird classification, powerful yet interpretable deep learning\nmodels can be developed to provide valuable insights to domain experts.\n""]",Bioacoustic Analysis and Classification
311,310,26,310_gflownet_gflownets_generative_flow,"['gflownet', 'gflownets', 'generative', 'flow', 'eflownets', 'flows', 'networks', 'learned', 'reward', 'rewards']","['gfn', 'flow', 'reward', 'proportional', 'objects', 'policy', 'rewards', 'generative', 'backward', 'compositional']","['gflownet', 'generative', 'flow', 'networks', 'reinforcement', 'stochastic', 'sequentially', 'gfns', 'exploration', 'sampling']","['  Generative Flow Networks (GFlowNets) treat sampling from distributions over\ncompositional discrete spaces as a sequential decision-making problem, training\na stochastic policy to construct objects step by step. Recent studies have\nrevealed strong connections between GFlowNets and entropy-regularized\nreinforcement learning. Building on these insights, we propose to enhance\nplanning capabilities of GFlowNets by applying Monte Carlo Tree Search (MCTS).\nSpecifically, we show how the MENTS algorithm (Xiao et al., 2019) can be\nadapted for GFlowNets and used during both training and inference. Our\nexperiments demonstrate that this approach improves the sample efficiency of\nGFlowNet training and the generation fidelity of pre-trained GFlowNet models.\n', '  Generative Flow Networks (GFlowNets, GFNs) are a generative framework for\nlearning unnormalized probability mass functions over discrete spaces. Since\ntheir inception, GFlowNets have proven to be useful for learning generative\nmodels in applications where the majority of the discrete space is unvisited\nduring training. This has inspired some to hypothesize that GFlowNets, when\npaired with deep neural networks (DNNs), have favourable generalization\nproperties. In this work, we empirically verify some of the hypothesized\nmechanisms of generalization of GFlowNets. In particular, we find that the\nfunctions that GFlowNets learn to approximate have an implicit underlying\nstructure which facilitate generalization. We also find that GFlowNets are\nsensitive to being trained offline and off-policy; however, the reward\nimplicitly learned by GFlowNets is robust to changes in the training\ndistribution.\n', '  The Generative Flow Network (GFlowNet) is a probabilistic framework in which\nan agent learns a stochastic policy and flow functions to sample objects with\nprobability proportional to an unnormalized reward function. GFlowNets share a\nstrong resemblance to reinforcement learning (RL), that typically aims to\nmaximize reward, due to their sequential decision-making processes. Recent\nworks have studied connections between GFlowNets and maximum entropy (MaxEnt)\nRL, which modifies the standard objective of RL agents by learning an\nentropy-regularized objective. However, a critical theoretical gap persists:\ndespite the apparent similarities in their sequential decision-making nature, a\ndirect link between GFlowNets and standard RL has yet to be discovered, while\nbridging this gap could further unlock the potential of both fields. In this\npaper, we establish a new connection between GFlowNets and policy evaluation\nfor a uniform policy. Surprisingly, we find that the resulting value function\nfor the uniform policy has a close relationship to the flows in GFlowNets.\nLeveraging these insights, we further propose a novel rectified policy\nevaluation (RPE) algorithm, which achieves the same reward-matching effect as\nGFlowNets, offering a new perspective. We compare RPE, MaxEnt RL, and GFlowNets\nin a number of benchmarks, and show that RPE achieves competitive results\ncompared to previous approaches. This work sheds light on the previously\nunexplored connection between (non-MaxEnt) RL and GFlowNets, potentially\nopening new avenues for future research in both fields.\n']",Generative Flow Networks
312,311,26,311_sparse_memory_gpu_gpus,"['sparse', 'memory', 'gpu', 'gpus', 'throughput', 'cpu', 'optimizer', 'moe', 'efficient', 'runtime']","['memory', 'experts', 'tuning', 'throughput', 'fine', 'adapters', 'runtime', 'computation', 'sparse', 'latency']","['sparse', 'gpu', 'throughput', 'optimizer', 'moe', '24gb', 'accelerators', 'fragmentation', 'llmem', 'cuda']","['  Due to the cost-prohibitive nature of training Large Language Models (LLMs),\nfine-tuning has emerged as an attractive alternative for specializing LLMs for\nspecific tasks using limited compute resources in a cost-effective manner. In\nthis paper, we characterize sparse Mixture of Experts (MoE) based LLM\nfine-tuning to understand their accuracy and runtime performance on a single\nGPU. Our evaluation provides unique insights into the training efficacy of\nsparse and dense versions of MoE models, as well as their runtime\ncharacteristics, including maximum batch size, execution time breakdown,\nend-to-end throughput, GPU hardware utilization, and load distribution. Our\nstudy identifies the optimization of the MoE layer as crucial for further\nimproving the performance of LLM fine-tuning. Using our profiling results, we\nalso develop and validate an analytical model to estimate the cost of LLM\nfine-tuning on the cloud. This model, based on parameters of the model and GPU\narchitecture, estimates LLM throughput and the cost of training, aiding\npractitioners in industry and academia to budget the cost of fine-tuning a\nspecific model.\n', ""  Large language models (LLMs) based on transformers have made significant\nstrides in recent years, the success of which is driven by scaling up their\nmodel size. Despite their high algorithmic performance, the computational and\nmemory requirements of LLMs present unprecedented challenges. To tackle the\nhigh compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture\nwas introduced which is able to scale its model size without proportionally\nscaling up its computational requirements. Unfortunately, MoE's high memory\ndemands and dynamic activation of sparse experts restrict its applicability to\nreal-world problems. Previous solutions that offload MoE's memory-hungry expert\nparameters to CPU memory fall short because the latency to migrate activated\nexperts from CPU to GPU incurs high performance overhead. Our proposed\nPre-gated MoE system effectively tackles the compute and memory challenges of\nconventional MoE architectures using our algorithm-system co-design. Pre-gated\nMoE employs our novel pre-gating function which alleviates the dynamic nature\nof sparse expert activation, allowing our proposed system to address the large\nmemory footprint of MoEs while also achieving high performance. We demonstrate\nthat Pre-gated MoE is able to improve performance, reduce GPU memory\nconsumption, while also maintaining the same level of model quality. These\nfeatures allow our Pre-gated MoE system to cost-effectively deploy large-scale\nLLMs using just a single GPU with high performance.\n"", ""  This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity\n""]",Optimizing Large Language Models for Efficient GPU Performance
313,312,26,312_videos_frames_segmentation_attention,"['videos', 'frames', 'segmentation', 'attention', 'vision', 'gvseg', 'video', 'surveillance', 'tracking', 'youtube']","['video', 'object', 'frames', 'segmentation', 'videos', 'temporal', 'frame', 'spatiotemporal', 'mask', 'objects']","['videos', 'segmentation', 'vision', 'gvseg', 'tracking', 'pixelcnn', 'objects', 'autoencoder', 'spatiotemporal', 'mask']","['  Object detection in videos is an important task in computer vision for\nvarious applications such as object tracking, video summarization and video\nsearch. Although great progress has been made in improving the accuracy of\nobject detection in recent years due to the rise of deep neural networks, the\nstate-of-the-art algorithms are highly computationally intensive. In order to\naddress this challenge, we make two important observations in the context of\nvideos: (i) Objects often occupy only a small fraction of the area in each\nvideo frame, and (ii) There is a high likelihood of strong temporal correlation\nbetween consecutive frames. Based on these observations, we propose Pack and\nDetect (PaD), an approach to reduce the computational requirements of object\ndetection in videos. In PaD, only selected video frames called anchor frames\nare processed at full size. In the frames that lie between anchor frames\n(inter-anchor frames), regions of interest (ROIs) are identified based on the\ndetections in the previous frame. We propose an algorithm to pack the ROIs of\neach inter-anchor frame together into a reduced-size frame. The computational\nrequirements of the detector are reduced due to the lower size of the input. In\norder to maintain the accuracy of object detection, the proposed algorithm\nexpands the ROIs greedily to provide additional background around each object\nto the detector. PaD can use any underlying neural network architecture to\nprocess the full-size and reduced-size frames. Experiments using the ImageNet\nvideo object detection dataset indicate that PaD can potentially reduce the\nnumber of FLOPS required for a frame by $4\\times$. This leads to an overall\nincrease in throughput of $1.25\\times$ on a 2.1 GHz Intel Xeon server with a\nNVIDIA Titan X GPU at the cost of $1.1\\%$ drop in accuracy.\n', '  Recently, video object segmentation (VOS) networks typically use memory-based\nmethods: for each query frame, the mask is predicted by space-time matching to\nmemory frames. Despite these methods having superior performance, they suffer\nfrom two issues: 1) Challenging data can destroy the space-time coherence\nbetween adjacent video frames. 2) Pixel-level matching will lead to undesired\nmismatching caused by the noises or distractors. To address the aforementioned\nissues, we first propose to generate an auxiliary frame between adjacent\nframes, serving as an implicit short-temporal reference for the query one.\nNext, we learn a prototype for each video object and prototype-level matching\ncan be implemented between the query and memory. The experiment demonstrated\nthat our network outperforms the state-of-the-art method on the DAVIS 2017,\nachieving a J&F score of 86.4%, and attains a competitive result 85.0% on\nYouTube VOS 2018. In addition, our network exhibits a high inference speed of\n32+ FPS.\n', '  Video Object Segmentation (VOS) is a vital task in computer vision, focusing\non distinguishing foreground objects from the background across video frames.\nOur work draws inspiration from the Cutie model, and we investigate the effects\nof object memory, the total number of memory frames, and input resolution on\nsegmentation performance. This report validates the effectiveness of our\ninference method on the coMplex video Object SEgmentation (MOSE) dataset, which\nfeatures complex occlusions. Our experimental results demonstrate that our\napproach achieves a J\\&F score of 0.8139 on the test set, securing the third\nposition in the final ranking. These findings highlight the robustness and\naccuracy of our method in handling challenging VOS scenarios.\n']",Video Object Segmentation and Tracking
314,313,26,313_memory_memristor_memristors_neural,"['memory', 'memristor', 'memristors', 'neural', 'hardware', 'memristive', 'spintronics', 'fpga', 'networks', 'memories']","['hardware', 'dropout', 'idealities', 'memristor', 'memory', 'resistive', 'device', 'crossbar', 'energy', 'stochastic']","['memristors', 'spintronics', 'fpga', 'memories', 'neuromorphic', 'nns', 'junctions', 'pcm', 'baynn', 'transistor']","['  Crossbar memory arrays have been touted as the workhorse of in-memory\ncomputing (IMC)-based acceleration of Deep Neural Networks (DNNs), but the\nassociated hardware non-idealities limit their efficacy. To address this,\ncross-layer design solutions that reduce the impact of hardware non-idealities\non DNN accuracy are needed. In Part 1 of this paper, we established the\nco-optimization strategies for various memory technologies and their crossbar\narrays, and conducted a comparative technology evaluation in the context of IMC\nrobustness. In this part, we analyze various design knobs such as array size\nand bit-slice (number of bits per device) and their impact on the performance\nof 8T SRAM, ferroelectric transistor (FeFET), Resistive RAM (ReRAM) and\nspin-orbit-torque magnetic RAM (SOT-MRAM) in the context of inference accuracy\nat 7nm technology node. Further, we study the effect of circuit design\nsolutions such as Partial Wordline Activation (PWA) and custom ADC reference\nlevels that reduce the hardware non-idealities and comparatively analyze the\nresponse of each technology to such accuracy enhancing techniques. Our results\non ResNet-20 (with CIFAR-10) show that PWA increases accuracy by up to 32.56%\nwhile custom ADC reference levels yield up to 31.62% accuracy enhancement. We\nobserve that compared to the other technologies, FeFET, by virtue of its small\nlayout height and high distinguishability of its memory states, is best suited\nfor large arrays. For higher bit-slices and a more complex dataset (ResNet-50\nwith Cifar-100) we found that ReRAM matches the performance of FeFET.\n', '  The performance of deep learning algorithms such as neural networks (NNs) has\nincreased tremendously recently, and they can achieve state-of-the-art\nperformance in many domains. However, due to memory and computation resource\nconstraints, implementing NNs on edge devices is a challenging task. Therefore,\nhardware accelerators such as computation-in-memory (CIM) with memristive\ndevices have been developed to accelerate the most common operations, i.e.,\nmatrix-vector multiplication. However, due to inherent device properties,\nexternal environmental factors such as temperature, and an immature fabrication\nprocess, memristors suffer from various non-idealities, including defects and\nvariations occurring during manufacturing and runtime. Consequently, there is a\nlack of complete confidence in the predictions made by the model. To improve\nconfidence in NN predictions made by hardware accelerators in the presence of\ndevice non-idealities, in this paper, we propose a Bayesian test vector\ngeneration framework that can estimate the model uncertainty of NNs implemented\non memristor-based CIM hardware. Compared to the conventional point estimate\ntest vector generation method, our method is more generalizable across\ndifferent model dimensions and requires storing only one test Bayesian vector\nin the hardware. Our method is evaluated on different model dimensions, tasks,\nfault rates, and variation noise to show that it can consistently achieve\n$100\\%$ coverage with only $0.024$ MB of memory overhead.\n', '  Uncertainty estimation in Neural Networks (NNs) is vital in improving\nreliability and confidence in predictions, particularly in safety-critical\napplications. Bayesian Neural Networks (BayNNs) with Dropout as an\napproximation offer a systematic approach to quantifying uncertainty, but they\ninherently suffer from high hardware overhead in terms of power, memory, and\ncomputation. Thus, the applicability of BayNNs to edge devices with limited\nresources or to high-performance applications is challenging. Some of the\ninherent costs of BayNNs can be reduced by accelerating them in hardware on a\nComputation-In-Memory (CIM) architecture with spintronic memories and\nbinarizing their parameters. However, numerous stochastic units are required to\nimplement conventional dropout-based BayNN. In this paper, we propose the Scale\nDropout, a novel regularization technique for Binary Neural Networks (BNNs),\nand Monte Carlo-Scale Dropout (MC-Scale Dropout)-based BayNNs for efficient\nuncertainty estimation. Our approach requires only one stochastic unit for the\nentire model, irrespective of the model size, leading to a highly scalable\nBayesian NN. Furthermore, we introduce a novel Spintronic memory-based CIM\narchitecture for the proposed BayNN that achieves more than $100\\times$ energy\nsavings compared to the state-of-the-art. We validated our method to show up to\na $1\\%$ improvement in predictive performance and superior uncertainty\nestimates compared to related works.\n']",Memristor-based Neural Network Hardware Acceleration
315,314,26,314_quadrotor_quadrotors_drones_drone,"['quadrotor', 'quadrotors', 'drones', 'drone', 'aerodynamic', 'multirotor', 'controllers', 'aerial', 'flight', 'trajectory']","['quadrotor', 'aerodynamic', 'control', 'flight', 'tracking', 'autonomous', 'quadrotors', 'simulation', 'controller', 'path']","['quadrotors', 'drone', 'aerodynamic', 'controllers', 'trajectory', 'simulator', 'robotics', 'onboard', 'forces', 'grasper']","['  Learning-based methods, particularly Reinforcement Learning (RL), hold great\npromise for streamlining deployment, enhancing performance, and achieving\ngeneralization in the control of autonomous multirotor aerial vehicles. Deep RL\nhas been able to control complex systems with impressive fidelity and agility\nin simulation but the simulation-to-reality transfer often brings a\nhard-to-bridge reality gap. Moreover, RL is commonly plagued by prohibitively\nlong training times. In this work, we propose a novel asymmetric\nactor-critic-based architecture coupled with a highly reliable RL-based\ntraining paradigm for end-to-end quadrotor control. We show how curriculum\nlearning and a highly optimized simulator enhance sample complexity and lead to\nfast training times. To precisely discuss the challenges related to\nlow-level/end-to-end multirotor control, we also introduce a taxonomy that\nclassifies the existing levels of control abstractions as well as\nnon-linearities and domain parameters. Our framework enables\nSimulation-to-Reality (Sim2Real) transfer for direct RPM control after only 18\nseconds of training on a consumer-grade laptop as well as its deployment on\nmicrocontrollers to control a multirotor under real-time guarantees. Finally,\nour solution exhibits competitive performance in trajectory tracking, as\ndemonstrated through various experimental comparisons with existing\nstate-of-the-art control solutions using a real Crazyflie nano quadrotor. We\nopen source the code including a very fast multirotor dynamics simulator that\ncan simulate about 5 months of flight per second on a laptop GPU. The fast\ntraining times and deployment to a cheap, off-the-shelf quadrotor lower the\nbarriers to entry and help democratize the research and development of these\nsystems.\n', '  Ensuring the reliability and validity of data-driven quadrotor model\npredictions is essential for their accepted and practical use. This is\nespecially true for grey- and black-box models wherein the mapping of inputs to\npredictions is not transparent and subsequent reliability notoriously difficult\nto ascertain. Nonetheless, such techniques are frequently and successfully used\nto identify quadrotor models. Prediction intervals (PIs) may be employed to\nprovide insight into the consistency and accuracy of model predictions. This\npaper estimates such PIs for polynomial and Artificial Neural Network (ANN)\nquadrotor aerodynamic models. Two existing ANN PI estimation techniques - the\nbootstrap method and the quality driven method - are validated numerically for\nquadrotor aerodynamic models using an existing high-fidelity quadrotor\nsimulation. Quadrotor aerodynamic models are then identified on real quadrotor\nflight data to demonstrate their utility and explore their sensitivity to model\ninterpolation and extrapolation. It is found that the ANN-based PIs widen\nconsiderably when extrapolating and remain constant, or shrink, when\ninterpolating. While this behaviour also occurs for the polynomial PIs, it is\nof lower magnitude. The estimated PIs establish probabilistic bounds within\nwhich the quadrotor model outputs will likely lie, subject to modelling and\nmeasurement uncertainties that are reflected through the PI widths.\n', '  Motivated by the increasing use of quadrotors for payload delivery, we\nconsider a joint trajectory generation and feedback control design problem for\na quadrotor experiencing aerodynamic wrenches. Unmodeled aerodynamic drag\nforces from carried payloads can lead to catastrophic outcomes. Prior work\nmodel aerodynamic effects as residual dynamics or external disturbances in the\ncontrol problem leading to a reactive policy that could be catastrophic.\nMoreover, redesigning controllers and tuning control gains on hardware\nplatforms is a laborious effort. In this paper, we argue that adapting the\ntrajectory generation component keeping the controller fixed can improve\ntrajectory tracking for quadrotor systems experiencing drag forces. To achieve\nthis, we formulate a drag-aware planning problem by applying a suitable\nrelaxation to an optimal quadrotor control problem, introducing a tracking cost\nfunction which measures the ability of a controller to follow a reference\ntrajectory. This tracking cost function acts as a regularizer in trajectory\ngeneration and is learned from data obtained from simulation. Our experiments\nin both simulation and on the Crazyflie hardware platform show that changing\nthe planner reduces tracking error by as much as 83%. Evaluation on hardware\ndemonstrates that our planned path, as opposed to a baseline, avoids controller\nsaturation and catastrophic outcomes during aggressive maneuvers.\n']",Quadrotor Control and Aerodynamics
316,315,26,315_evaluations_evaluation_evaluator_assessment,"['evaluations', 'evaluation', 'evaluator', 'assessment', 'evaluating', 'assessing', 'evaluators', 'ranking', 'eval', 'judgments']","['evaluators', 'evaluation', 'evaluator', 'comparisons', 'assessment', 'pairwise', 'answers', 'choice', 'leaderboards', 'options']","['evaluations', 'evaluator', 'bias', 'leaderboards', 'nlg', 'responses', 'checkeval', 'scoring', 'rubrics', 'candidates']","['  Large Language Models (LLMs) are increasingly relied upon to evaluate text\noutputs of other LLMs, thereby influencing leaderboards and development\ndecisions. However, concerns persist over the accuracy of these assessments and\nthe potential for misleading conclusions. In this work, we investigate the\neffectiveness of LLMs as evaluators for text generation tasks. We propose FBI,\na novel framework designed to examine the proficiency of Evaluator LLMs in\nassessing four critical abilities in other LLMs: factual accuracy, instruction\nfollowing, coherence in long-form writing, and reasoning proficiency. By\nintroducing targeted perturbations in answers generated by LLMs, that clearly\nimpact one of these key capabilities, we test whether an Evaluator LLM can\ndetect these quality drops. By creating a total of 2400 perturbed answers\ncovering 22 perturbation categories, we conduct a comprehensive study using\ndifferent evaluation strategies on five prominent LLMs commonly used as\nevaluators in the literature. Our findings reveal significant shortcomings in\ncurrent Evaluator LLMs, which failed to identify quality drops in over 50\\% of\ncases on average. Single-answer and pairwise evaluations demonstrated notable\nlimitations, whereas reference-based evaluations showed comparatively better\nperformance. These results underscore the unreliable nature of current\nEvaluator LLMs and advocate for cautious implementation in practical\napplications. Code and data are available at https://github.com/AI4Bharat/FBI.\n', '  Proprietary LMs such as GPT-4 are often employed to assess the quality of\nresponses from various LMs. However, concerns including transparency,\ncontrollability, and affordability strongly motivate the development of\nopen-source LMs specialized in evaluations. On the other hand, existing open\nevaluator LMs exhibit critical shortcomings: 1) they issue scores that\nsignificantly diverge from those assigned by humans, and 2) they lack the\nflexibility to perform both direct assessment and pairwise ranking, the two\nmost prevalent forms of assessment. Additionally, they do not possess the\nability to evaluate based on custom evaluation criteria, focusing instead on\ngeneral attributes like helpfulness and harmlessness. To address these issues,\nwe introduce Prometheus 2, a more powerful evaluator LM than its predecessor\nthat closely mirrors human and GPT-4 judgements. Moreover, it is capable of\nprocessing both direct assessment and pair-wise ranking formats grouped with a\nuser-defined evaluation criteria. On four direct assessment benchmarks and four\npairwise ranking benchmarks, Prometheus 2 scores the highest correlation and\nagreement with humans and proprietary LM judges among all tested open evaluator\nLMs. Our models, code, and data are all publicly available at\nhttps://github.com/prometheus-eval/prometheus-eval.\n', ""  Recently, using a powerful proprietary Large Language Model (LLM) (e.g.,\nGPT-4) as an evaluator for long-form responses has become the de facto\nstandard. However, for practitioners with large-scale evaluation tasks and\ncustom criteria in consideration (e.g., child-readability), using proprietary\nLLMs as an evaluator is unreliable due to the closed-source nature,\nuncontrolled versioning, and prohibitive costs. In this work, we propose\nPrometheus, a fully open-source LLM that is on par with GPT-4's evaluation\ncapabilities when the appropriate reference materials (reference answer, score\nrubric) are accompanied. We first construct the Feedback Collection, a new\ndataset that consists of 1K fine-grained score rubrics, 20K instructions, and\n100K responses and language feedback generated by GPT-4. Using the Feedback\nCollection, we train Prometheus, a 13B evaluator LLM that can assess any given\nlong-form text based on customized score rubric provided by the user.\nExperimental results show that Prometheus scores a Pearson correlation of 0.897\nwith human evaluators when evaluating with 45 customized score rubrics, which\nis on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392).\nFurthermore, measuring correlation with GPT-4 with 1222 customized score\nrubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask\nEval) shows similar trends, bolstering Prometheus's capability as an evaluator\nLLM. Lastly, Prometheus achieves the highest accuracy on two human preference\nbenchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced\nreward models explicitly trained on human preference datasets, highlighting its\npotential as an universal reward model. We open-source our code, dataset, and\nmodel at https://kaistai.github.io/prometheus/.\n""]",Evaluating Language Models
317,316,26,316_soundspaces_recordings_soundscapes_audio,"['soundspaces', 'recordings', 'soundscapes', 'audio', 'microphone', 'acoustics', 'acoustic', 'microphones', 'recording', 'auditory']","['sound', 'acoustic', 'room', 'localization', 'audio', 'events', 'binaural', 'reverberant', 'event', 'target']","['soundspaces', 'recording', 'reverberation', 'binaural', 'listener', 'reconstructing', 'spatialscaper', 'task3', 'spectrogram', 'sight']","['  Deep learning-based sound event localization and classification is an\nemerging research area within wireless acoustic sensor networks. However,\ncurrent methods for sound event localization and classification typically rely\non a single microphone array, making them susceptible to signal attenuation and\nenvironmental noise, which limits their monitoring range. Moreover, methods\nusing multiple microphone arrays often focus solely on source localization,\nneglecting the aspect of sound event classification. In this paper, we propose\na deep learning-based method that employs multiple features and attention\nmechanisms to estimate the location and class of sound source. We introduce a\nSoundmap feature to capture spatial information across multiple frequency\nbands. We also use the Gammatone filter to generate acoustic features more\nsuitable for outdoor environments. Furthermore, we integrate attention\nmechanisms to learn channel-wise relationships and temporal dependencies within\nthe acoustic features. To evaluate our proposed method, we conduct experiments\nusing simulated datasets with different levels of noise and size of monitoring\nareas, as well as different arrays and source positions. The experimental\nresults demonstrate the superiority of our proposed method over\nstate-of-the-art methods in both sound event classification and sound source\nlocalization tasks. And we provide further analysis to explain the reasons for\nthe observed errors.\n', '  Sound Event Detection and Localization (SELD) is a combined task of\nidentifying sound events and their corresponding direction-of-arrival (DOA).\nWhile this task has numerous applications and has been extensively researched\nin recent years, it fails to provide full information about the sound source\nposition. In this paper, we overcome this problem by extending the task to\nSound Event Detection, Localization with Distance Estimation (3D SELD). We\nstudy two ways of integrating distance estimation within the SELD core - a\nmulti-task approach, in which the problem is tackled by a separate model\noutput, and a single-task approach obtained by extending the multi-ACCDOA\nmethod to include distance information. We investigate both methods for the\nAmbisonic and binaural versions of STARSS23: Sony-TAU Realistic Spatial\nSoundscapes 2023. Moreover, our study involves experiments on the loss function\nrelated to the distance estimation part. Our results show that it is possible\nto perform 3D SELD without any degradation of performance in sound event\ndetection and DOA estimation.\n', '  Sound event localization and detection (SELD) is an important task in machine\nlistening. Major advancements rely on simulated data with sound events in\nspecific rooms and strong spatio-temporal labels. SELD data is simulated by\nconvolving spatialy-localized room impulse responses (RIRs) with sound\nwaveforms to place sound events in a soundscape. However, RIRs require manual\ncollection in specific rooms. We present SpatialScaper, a library for SELD data\nsimulation and augmentation. Compared to existing tools, SpatialScaper emulates\nvirtual rooms via parameters such as size and wall absorption. This allows for\nparameterized placement (including movement) of foreground and background sound\nsources. SpatialScaper also includes data augmentation pipelines that can be\napplied to existing SELD data. As a case study, we use SpatialScaper to add\nrooms to the DCASE SELD data. Training a model with our data led to progressive\nperformance improves as a direct function of acoustic diversity. These results\nshow that SpatialScaper is valuable to train robust SELD models.\n']",Sound Event Detection and Localization in Audio Recordings
318,317,25,317_views_view_clustering_multiview,"['views', 'view', 'clustering', 'multiview', 'cluster', 'clusters', 'embedding', 'regularization', 'mvcan', 'factorization']","['view', 'clustering', 'views', 'anchor', 'subspace', 'consensus', 'multi', 'cluster', 'matrix', 'incomplete']","['views', 'clusters', 'embedding', 'regularization', 'mvcan', 'factorization', 'learns', 'tensor', 'mvdscn', 'ccmvc']","[""  Multi-view clustering (MVC) aims at exploring category structures among\nmulti-view data in self-supervised manners. Multiple views provide more\ninformation than single views and thus existing MVC methods can achieve\nsatisfactory performance. However, their performance might seriously degenerate\nwhen the views are noisy in practical multi-view scenarios. In this paper, we\nformally investigate the drawback of noisy views and then propose a\ntheoretically grounded deep MVC method (namely MVCAN) to address this issue.\nSpecifically, we propose a novel MVC objective that enables un-shared\nparameters and inconsistent clustering predictions across multiple views to\nreduce the side effects of noisy views. Furthermore, a two-level multi-view\niterative optimization is designed to generate robust learning targets for\nrefining individual views' representation learning. Theoretical analysis\nreveals that MVCAN works by achieving the multi-view consistency,\ncomplementarity, and noise robustness. Finally, experiments on extensive public\ndatasets demonstrate that MVCAN outperforms state-of-the-art methods and is\nrobust against the existence of noisy views.\n"", '  Multi-view clustering has attracted growing attention owing to its\ncapabilities of aggregating information from various sources and its promising\nhorizons in public affairs. Up till now, many advanced approaches have been\nproposed in recent literature. However, there are several ongoing difficulties\nto be tackled. One common dilemma occurs while attempting to align the features\nof different views. {Moreover, due to the fact that many existing multi-view\nclustering algorithms stem from spectral clustering, this results to cubic time\ncomplexity w.r.t. the number of dataset. However, we propose Anchor-based\nMulti-view Subspace Clustering with Hierarchical Feature Descent(MVSC-HFD) to\ntackle the discrepancy among views through hierarchical feature descent and\nproject to a common subspace( STAGE 1), which reveals dependency of different\nviews. We further reduce the computational complexity to linear time cost\nthrough a unified sampling strategy in the common subspace( STAGE 2), followed\nby anchor-based subspace clustering to learn the bipartite graph collectively(\nSTAGE 3). }Extensive experimental results on public benchmark datasets\ndemonstrate that our proposed model consistently outperforms the\nstate-of-the-art techniques.\n', '  In this paper, we propose a novel multi-view clustering model, named\nDual-space Co-training Large-scale Multi-view Clustering (DSCMC). The main\nobjective of our approach is to enhance the clustering performance by\nleveraging co-training in two distinct spaces. In the original space, we learn\na projection matrix to obtain latent consistent anchor graphs from different\nviews. This process involves capturing the inherent relationships and\nstructures between data points within each view. Concurrently, we employ a\nfeature transformation matrix to map samples from various views to a shared\nlatent space. This transformation facilitates the alignment of information from\nmultiple views, enabling a comprehensive understanding of the underlying data\ndistribution. We jointly optimize the construction of the latent consistent\nanchor graph and the feature transformation to generate a discriminative anchor\ngraph. This anchor graph effectively captures the essential characteristics of\nthe multi-view data and serves as a reliable basis for subsequent clustering\nanalysis. Moreover, the element-wise method is proposed to avoid the impact of\ndiverse information between different views. Our algorithm has an approximate\nlinear computational complexity, which guarantees its successful application on\nlarge-scale datasets. Through experimental validation, we demonstrate that our\nmethod significantly reduces computational complexity while yielding superior\nclustering performance compared to existing approaches.\n']",Multi-View Clustering Methods
319,318,25,318_patents_patentsview_patenting_patent,"['patents', 'patentsview', 'patenting', 'patent', 'patenteval', 'patenthan', 'inventors', 'invention', 'semantic', 'inventions']","['patent', 'patents', 'claim', 'claims', 'assignees', 'landscaping', 'inventors', 'citation', 'similarity', 'technical']","['patents', 'citation', 'texts', 'similarity', 'embedding', 'disambiguation', 'technological', 'intellectual', 'novelty', 'examiners']","['  Recent advancements in Artificial Intelligence (AI) and machine learning have\ndemonstrated transformative capabilities across diverse domains. This progress\nextends to the field of patent analysis and innovation, where AI-based tools\npresent opportunities to streamline and enhance important tasks in the patent\ncycle such as classification, retrieval, and valuation prediction. This not\nonly accelerates the efficiency of patent researchers and applicants but also\nopens new avenues for technological innovation and discovery. Our survey\nprovides a comprehensive summary of recent AI tools in patent analysis from\nmore than 40 papers from 26 venues between 2017 and 2023. Unlike existing\nsurveys, we include methods that work for patent image and text data.\nFurthermore, we introduce a novel taxonomy for the categorization based on the\ntasks in the patent life cycle as well as the specifics of the AI methods. This\ninterdisciplinary survey aims to serve as a resource for researchers and\npractitioners who are working at the intersection of AI and patent analysis as\nwell as the patent offices that are aiming to build efficient patent systems.\n', ""  This paper makes two contributions to the field of text-based patent\nsimilarity. First, it compares the performance of different kinds of\npatent-specific pretrained embedding models, namely static word embeddings\n(such as word2vec and doc2vec models) and contextual word embeddings (such as\ntransformers based models), on the task of patent similarity calculation.\nSecond, it compares specifically the performance of Sentence Transformers\n(SBERT) architectures with different training phases on the patent similarity\ntask. To assess the models' performance, we use information about patent\ninterferences, a phenomenon in which two or more patent claims belonging to\ndifferent patent applications are proven to be overlapping by patent examiners.\nTherefore, we use these interferences cases as a proxy for maximum similarity\nbetween two patents, treating them as ground-truth to evaluate the performance\nof the different embedding models. Our results point out that, first, Patent\nSBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer\narchitecture proposed in this research, outperforms the current\nstate-of-the-art in patent similarity. Second, they show that, in some cases,\nlarge static models performances are still comparable to contextual ones when\ntrained on extensive data; thus, we believe that the superiority in the\nperformance of contextual embeddings may not be related to the actual\narchitecture but rather to the way the training phase is performed.\n"", '  Patents, encapsulating crucial technical and legal information, present a\nrich domain for natural language processing (NLP) applications. As NLP\ntechnologies evolve, large language models (LLMs) have demonstrated outstanding\ncapabilities in general text processing and generation tasks. However, the\napplication of LLMs in the patent domain remains under-explored and\nunder-developed due to the complexity of patent processing. Understanding the\nunique characteristics of patent documents and related research in the patent\ndomain becomes essential for researchers to apply these tools effectively.\nTherefore, this paper aims to equip NLP researchers with the essential\nknowledge to navigate this complex domain efficiently. We introduce the\nrelevant fundamental aspects of patents to provide solid background\ninformation, particularly for readers unfamiliar with the patent system. In\naddition, we systematically break down the structural and linguistic\ncharacteristics unique to patents and map out how NLP can be leveraged for\npatent analysis and generation. Moreover, we demonstrate the spectrum of\ntext-based patent-related tasks, including nine patent analysis and four patent\ngeneration tasks.\n']",Artificial Intelligence in Patent Analysis
320,319,25,319_3d_3ddc_lv3d_3dcompat,"['3d', '3ddc', 'lv3d', '3dcompat', 'scenes', 'llmi3d', 'visual', '3ddet', 'vision', 'concretenet']","['grounding', 'scene', 'object', 'vocabulary', 'segmentation', 'point', 'clouds', 'visual', 'objects', 'vision']","['3ddc', 'lv3d', 'opensun3d', 'captioning', 'annotations', 'camera', 'workshop', 'spatial', 'scannet', 'grounding']","['  We propose a lightweight and scalable Regional Point-Language Contrastive\nlearning framework, namely \\textbf{RegionPLC}, for open-world 3D scene\nunderstanding, aiming to identify and recognize open-set objects and\ncategories. Specifically, based on our empirical studies, we introduce a\n3D-aware SFusion strategy that fuses 3D vision-language pairs derived from\nmultiple 2D foundation models, yielding high-quality, dense region-level\nlanguage descriptions without human 3D annotations. Subsequently, we devise a\nregion-aware point-discriminative contrastive learning objective to enable\nrobust and effective 3D learning from dense regional language supervision. We\ncarry out extensive experiments on ScanNet, ScanNet200, and nuScenes datasets,\nand our model outperforms prior 3D open-world scene understanding approaches by\nan average of 17.2\\% and 9.1\\% for semantic and instance segmentation,\nrespectively, while maintaining greater scalability and lower resource demands.\nFurthermore, our method has the flexibility to be effortlessly integrated with\nlanguage models to enable open-ended grounded 3D reasoning without extra\ntask-specific training. Code is available at https://github.com/CVMI-Lab/PLA.\n', '  Learning to ground natural language queries to target objects or regions in\n3D point clouds is quite essential for 3D scene understanding. Nevertheless,\nexisting 3D visual grounding approaches require a substantial number of\nbounding box annotations for text queries, which is time-consuming and\nlabor-intensive to obtain. In this paper, we propose \\textbf{3D-VLA}, a weakly\nsupervised approach for \\textbf{3D} visual grounding based on \\textbf{V}isual\n\\textbf{L}inguistic \\textbf{A}lignment. Our 3D-VLA exploits the superior\nability of current large-scale vision-language models (VLMs) on aligning the\nsemantics between texts and 2D images, as well as the naturally existing\ncorrespondences between 2D images and 3D point clouds, and thus implicitly\nconstructs correspondences between texts and 3D point clouds with no need for\nfine-grained box annotations in the training procedure. During the inference\nstage, the learned text-3D correspondence will help us ground the text queries\nto the 3D target objects even without 2D images. To the best of our knowledge,\nthis is the first work to investigate 3D visual grounding in a weakly\nsupervised manner by involving large scale vision-language models, and\nextensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our\n3D-VLA achieves comparable and even superior results over the fully supervised\nmethods.\n', '  Large 2D vision-language models (2D-LLMs) have gained significant attention\nby bridging Large Language Models (LLMs) with images using a simple projector.\nInspired by their success, large 3D point cloud-language models (3D-LLMs) also\nintegrate point clouds into LLMs. However, directly aligning point clouds with\nLLM requires expensive training costs, typically in hundreds of GPU-hours on\nA100, which hinders the development of 3D-LLMs. In this paper, we introduce\nMiniGPT-3D, an efficient and powerful 3D-LLM that achieves multiple SOTA\nresults while training for only 27 hours on one RTX 3090. Specifically, we\npropose to align 3D point clouds with LLMs using 2D priors from 2D-LLMs, which\ncan leverage the similarity between 2D and 3D visual information. We introduce\na novel four-stage training strategy for modality alignment in a cascaded way,\nand a mixture of query experts module to adaptively aggregate features with\nhigh efficiency. Moreover, we utilize parameter-efficient fine-tuning methods\nLoRA and Norm fine-tuning, resulting in only 47.8M learnable parameters, which\nis up to 260x fewer than existing methods. Extensive experiments show that\nMiniGPT-3D achieves SOTA on 3D object classification and captioning tasks, with\nsignificantly cheaper training costs. Notably, MiniGPT-3D gains an 8.12\nincrease on GPT-4 evaluation score for the challenging object captioning task\ncompared to ShapeLLM-13B, while the latter costs 160 total GPU-hours on 8 A800.\nWe are the first to explore the efficient 3D-LLM, offering new insights to the\ncommunity. Code and weights are available at\nhttps://github.com/TangYuan96/MiniGPT-3D.\n']",3D Scene Understanding and Visual Language Models
321,320,25,320_imagenet_generative_autoencoder_attention,"['imagenet', 'generative', 'autoencoder', 'attention', 'conditioned', 'images', 'diffusion', 'pixel', 'conditioning', 'denoised']","['diffusion', 'conditional', 'image', 'guidance', 'unconditional', 'generation', 'condition', 'controls', 'super', 'quality']","['imagenet', 'generative', 'autoencoder', 'coco', 'inpainting', 'sdm', 'distillation', 'denoising', 'controllable', 'resolution']","[""  Large generative diffusion models have revolutionized text-to-image\ngeneration and offer immense potential for conditional generation tasks such as\nimage enhancement, restoration, editing, and compositing. However, their\nwidespread adoption is hindered by the high computational cost, which limits\ntheir real-time application. To address this challenge, we introduce a novel\nmethod dubbed CoDi, that adapts a pre-trained latent diffusion model to accept\nadditional image conditioning inputs while significantly reducing the sampling\nsteps required to achieve high-quality results. Our method can leverage\narchitectures such as ControlNet to incorporate conditioning inputs without\ncompromising the model's prior knowledge gained during large scale\npre-training. Additionally, a conditional consistency loss enforces consistent\npredictions across diffusion steps, effectively compelling the model to\ngenerate high-quality images with conditions in a few steps. Our\nconditional-task learning and distillation approach outperforms previous\ndistillation methods, achieving a new state-of-the-art in producing\nhigh-quality images with very few steps (e.g., 1-4) across multiple tasks,\nincluding super-resolution, text-guided image editing, and depth-to-image\ngeneration.\n"", ""  Recent studies have demonstrated that diffusion models are capable of\ngenerating high-quality samples, but their quality heavily depends on sampling\nguidance techniques, such as classifier guidance (CG) and classifier-free\nguidance (CFG). These techniques are often not applicable in unconditional\ngeneration or in various downstream tasks such as image restoration. In this\npaper, we propose a novel sampling guidance, called Perturbed-Attention\nGuidance (PAG), which improves diffusion sample quality across both\nunconditional and conditional settings, achieving this without requiring\nadditional training or the integration of external modules. PAG is designed to\nprogressively enhance the structure of samples throughout the denoising\nprocess. It involves generating intermediate samples with degraded structure by\nsubstituting selected self-attention maps in diffusion U-Net with an identity\nmatrix, by considering the self-attention mechanisms' ability to capture\nstructural information, and guiding the denoising process away from these\ndegraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves\nsample quality in conditional and even unconditional scenarios. Moreover, PAG\nsignificantly improves the baseline performance in various downstream tasks\nwhere existing guidances such as CG or CFG cannot be fully utilized, including\nControlNet with empty prompts and image restoration such as inpainting and\ndeblurring.\n"", '  To enhance the controllability of text-to-image diffusion models, existing\nefforts like ControlNet incorporated image-based conditional controls. In this\npaper, we reveal that existing methods still face significant challenges in\ngenerating images that align with the image conditional controls. To this end,\nwe propose ControlNet++, a novel approach that improves controllable generation\nby explicitly optimizing pixel-level cycle consistency between generated images\nand conditional controls. Specifically, for an input conditional control, we\nuse a pre-trained discriminative reward model to extract the corresponding\ncondition of the generated images, and then optimize the consistency loss\nbetween the input conditional control and extracted condition. A\nstraightforward implementation would be generating images from random noises\nand then calculating the consistency loss, but such an approach requires\nstoring gradients for multiple sampling timesteps, leading to considerable time\nand memory costs. To address this, we introduce an efficient reward strategy\nthat deliberately disturbs the input images by adding noise, and then uses the\nsingle-step denoised images for reward fine-tuning. This avoids the extensive\ncosts associated with image sampling, allowing for more efficient reward\nfine-tuning. Extensive experiments show that ControlNet++ significantly\nimproves controllability under various conditional controls. For example, it\nachieves improvements over ControlNet by 11.1% mIoU, 13.4% SSIM, and 7.6% RMSE,\nrespectively, for segmentation mask, line-art edge, and depth conditions. All\nthe code, models, demo and organized data have been open sourced on our Github\nRepo.\n']",Image Generation with Diffusion Models
322,321,25,321_ai_explainers_explanations_explainability,"['ai', 'explainers', 'explanations', 'explainability', 'understandability', 'dialogue', 'conversational', 'explainer', 'dialogues', 'interactive']","['explanations', 'explanation', 'explainable', 'users', 'explainees', 'user', 'explainee', 'artificial', 'intelligence', 'technical']","['ai', 'explainability', 'conversational', 'dialogues', 'xai', 'explainee', 'interactivity', 'recognising', 'skills', 'novices']","['  Explainable Artificial Intelligence (XAI) aims to improve the transparency of\nautonomous decision-making through explanations. Recent literature has\nemphasised users\' need for holistic ""multi-shot"" explanations and the ability\nto personalise their engagement with XAI systems. We refer to this user-centred\ninteraction as an XAI Experience. Despite advances in creating XAI experiences,\nevaluating them in a user-centred manner has remained challenging. To address\nthis, we introduce the XAI Experience Quality (XEQ) Scale (pronounced ""Seek""\nScale), for evaluating the user-centred quality of XAI experiences.\nFurthermore, XEQ quantifies the quality of experiences across four evaluation\ndimensions: learning, utility, fulfilment and engagement. These contributions\nextend the state-of-the-art of XAI evaluation, moving beyond the\none-dimensional metrics frequently developed to assess single-shot\nexplanations. In this paper, we present the XEQ scale development and\nvalidation process, including content validation with XAI experts as well as\ndiscriminant and construct validation through a large-scale pilot study. Out\npilot study results offer strong evidence that establishes the XEQ Scale as a\ncomprehensive framework for evaluating user-centred XAI experiences.\n', ""  The goal of Explainable AI (XAI) is to design methods to provide insights\ninto the reasoning process of black-box models, such as deep neural networks,\nin order to explain them to humans. Social science research states that such\nexplanations should be conversational, similar to human-to-human explanations.\nIn this work, we show how to incorporate XAI in a conversational agent, using a\nstandard design for the agent comprising natural language understanding and\ngeneration components. We build upon an XAI question bank, which we extend by\nquality-controlled paraphrases, to understand the user's information needs. We\nfurther systematically survey the literature for suitable explanation methods\nthat provide the information to answer those questions, and present a\ncomprehensive list of suggestions. Our work is the first step towards truly\nnatural conversations about machine learning models with an explanation agent.\nThe comprehensive list of XAI questions and the corresponding explanation\nmethods may support other researchers in providing the necessary information to\naddress users' demands. To facilitate future work, we release our source code\nand data.\n"", ""  The field of eXplainable Artificial Intelligence (XAI) is increasingly\nrecognizing the need to personalize and/or interactively adapt the explanation\nto better reflect users' explanation needs. While dialogue-based approaches to\nXAI have been proposed recently, the state-of-the-art in XAI is still\ncharacterized by what we call one-shot, non-personalized and one-way\nexplanations. In contrast, dialogue-based systems that can adapt explanations\nthrough interaction with a user promise to be superior to GUI-based or\ndashboard explanations as they offer a more intuitive way of requesting\ninformation. In general, while interactive XAI systems are often evaluated in\nterms of user satisfaction, there are limited studies that access user's\nobjective model understanding. This is in particular the case for\ndialogue-based XAI approaches. In this paper, we close this gap by carrying out\ncontrolled experiments within a dialogue framework in which we measure\nunderstanding of users in three phases by asking them to simulate the\npredictions of the model they are learning about. By this, we can quantify the\nlevel of (improved) understanding w.r.t. how the model works, comparing the\nstate prior, and after the interaction. We further analyze the data to reveal\npatterns of how the interaction between groups with high vs. low understanding\ngain differ. Overall, our work thus contributes to our understanding about the\neffectiveness of XAI approaches.\n""]",Explainable Artificial Intelligence (XAI)
323,322,25,322_checkpointing_checkpoint_checkpoints_backup,"['checkpointing', 'checkpoint', 'checkpoints', 'backup', 'queue', 'storage', 'writes', 'parallelism', 'saving', 'persistent']","['checkpoint', 'checkpointing', 'checkpoints', 'service', 'storage', 'loading', 'fault', 'tolerance', 'failures', 'parallelism']","['checkpoints', 'backup', 'writes', 'bytecheckpoint', 'serverlessllm', 'cluster', 'autoscaling', 'redundancy', 'ssds', 'kvcache']","['  As large language models continue to scale up, the imperative for fault\ntolerance in distributed deep learning systems intensifies, becoming a focal\narea of AI infrastructure research. Checkpoint has emerged as the predominant\nfault tolerance strategy, with extensive studies dedicated to optimizing its\nefficiency. However, the advent of the sparse Mixture-of-Experts (MoE) model\npresents new challenges for traditional checkpoint techniques due to the\nsubstantial increase in model size, despite comparable computational demands to\ndense models. Breaking new ground in the realm of efficient fault tolerance for\nMoE model training, we introduce a novel Partial Experts Checkpoint (PEC)\nmechanism alongside a corresponding PEC fault-tolerant system. Our approach\nstrategically checkpoints a selected subset of experts, thereby significantly\nreducing the checkpoint size for MoE models to a level comparable with that of\ndense models. The empirical analysis on our 8-expert GPT-MoE model demonstrates\nthat the proposed PEC approach facilitates a substantial 54.2% decrease in the\nsize of non-redundant checkpoint (no data-parallel duplication), without\ncompromising the final model quality. Moreover, our PEC fault-tolerant system\nachieves a 76.9% reduction in checkpoint workload per data-parallel distributed\nrank, thereby correspondingly diminishing the checkpointing time and\nfacilitating complete overlap with the training process.\n', ""  The development of real-world Large Language Models (LLMs) necessitates\ncheckpointing of training states in persistent storage to mitigate potential\nsoftware and hardware failures, as well as to facilitate checkpoint\ntransferring within the training pipeline and across various tasks. Due to the\nimmense size of LLMs, saving and loading checkpoints often incur intolerable\nminute-level stalls, significantly diminishing training efficiency. Besides,\nwhen transferring checkpoints across tasks, checkpoint resharding, defined as\nloading checkpoints into parallel configurations differing from those used for\nsaving, is often required according to the characteristics and resource quota\nof specific tasks. Previous checkpointing systems [16,3,33,6] assume consistent\nparallel configurations, failing to address the complexities of checkpoint\ntransformation during resharding. Furthermore, in the industry platform,\ndevelopers create checkpoints from different training frameworks[23,36,21,11],\neach with its own unique storage and I/O logic. This diversity complicates the\nimplementation of unified checkpoint management and optimization. To address\nthese challenges, we introduce ByteCheckpoint, a PyTorch-native multi-framework\nLLM checkpointing system that supports automatic online checkpoint resharding.\nByteCheckpoint employs a data/metadata disaggregated storage architecture,\ndecoupling checkpoint storage from the adopted parallelism strategies and\ntraining frameworks. We design an efficient asynchronous tensor merging\ntechnique to settle the irregular tensor sharding problem and propose several\nI/O performance optimizations to significantly enhance the efficiency of\ncheckpoint saving and loading. Experimental results demonstrate\nByteCheckpoint's substantial advantages in reducing checkpoint saving (by up to\n529.22X) and loading (by up to 3.51X) costs, compared to baseline methods.\n"", '  Existing checkpointing approaches seem ill-suited for distributed training\neven though hardware limitations make model parallelism, i.e., sharding model\nstate across multiple accelerators, a requirement for model scaling.\nConsolidating distributed model state into a single checkpoint unacceptably\nslows down training, and is impractical at extreme scales. Distributed\ncheckpoints, in contrast, are tightly coupled to the model parallelism and\nhardware configurations of the training run, and thus unusable on different\nconfigurations. To address this problem, we propose Universal Checkpointing, a\ntechnique that enables efficient checkpoint creation while providing the\nflexibility of resuming on arbitrary parallelism strategy and hardware\nconfigurations. Universal Checkpointing unlocks unprecedented capabilities for\nlarge-scale training such as improved resilience to hardware failures through\ncontinued training on remaining healthy hardware, and reduced training time\nthrough opportunistic exploitation of elastic capacity.\n  The key insight of Universal Checkpointing is the selection of the optimal\nrepresentation in each phase of the checkpointing life cycle: distributed\nrepresentation for saving, and consolidated representation for loading. This is\nachieved using two key mechanisms. First, the universal checkpoint format,\nwhich consists of a consolidated representation of each model parameter and\nmetadata for mapping parameter fragments into training ranks of arbitrary\nmodel-parallelism configuration. Second, the universal checkpoint language, a\nsimple but powerful specification language for converting distributed\ncheckpoints into the universal checkpoint format. Our evaluation demonstrates\nthe effectiveness and generality of Universal Checkpointing on state-of-the-art\nmodel architectures and a wide range of parallelism techniques.\n']",Checkpointing Strategies for Large-Scale Deep Learning Systems
324,323,25,323_multiomics_transcriptomics_genomics_genome,"['multiomics', 'transcriptomics', 'genomics', 'genome', 'genomic', 'genomes', 'biomarker', 'gene', 'genes', 'rna']","['cancer', 'omics', 'gene', 'genes', 'biological', 'patient', 'subtypes', 'biomarkers', 'survival', 'genome']","['transcriptomics', 'biomarkers', 'lncrnas', 'deepgene', 'gnn', 'prediction', 'multi', 'adenocarcinoma', 'mirna', 'profiling']","['  The application of machine learning to transcriptomics data has led to\nsignificant advances in cancer research. However, the high dimensionality and\ncomplexity of RNA sequencing (RNA-seq) data pose significant challenges in\npan-cancer studies. This study hypothesizes that gene sets derived from\nsingle-cell RNA sequencing (scRNA-seq) data will outperform those selected\nusing bulk RNA-seq in pan-cancer downstream tasks. We analyzed scRNA-seq data\nfrom 181 tumor biopsies across 13 cancer types. High-dimensional weighted gene\nco-expression network analysis (hdWGCNA) was performed to identify relevant\ngene sets, which were further refined using XGBoost for feature selection.\nThese gene sets were applied to downstream tasks using TCGA pan-cancer RNA-seq\ndata and compared to six reference gene sets and oncogenes from OncoKB\nevaluated with deep learning models, including multilayer perceptrons (MLPs)\nand graph neural networks (GNNs). The XGBoost-refined hdWGCNA gene set\ndemonstrated higher performance in most tasks, including tumor mutation burden\nassessment, microsatellite instability classification, mutation prediction,\ncancer subtyping, and grading. In particular, genes such as DPM1, BAD, and\nFKBP4 emerged as important pan-cancer biomarkers, with DPM1 consistently\nsignificant across tasks. This study presents a robust approach for feature\nselection in cancer genomics by integrating scRNA-seq data and advanced\nanalysis techniques, offering a promising avenue for improving predictive\naccuracy in cancer research.\n', ""  Multi-omics research has enhanced our understanding of cancer heterogeneity\nand progression. Investigating molecular data through multi-omics approaches is\ncrucial for unraveling the complex biological mechanisms underlying cancer,\nthereby enabling effective diagnosis, treatment, and prevention strategies.\nHowever, predicting patient outcomes through integration of all available\nmulti-omics data is an under-study research direction. Here, we present SeNMo\n(Self-normalizing Network for Multi-omics), a deep neural network trained on\nmulti-omics data across 33 cancer types. SeNMo is efficient in handling\nmulti-omics data characterized by high-width (many features) and low-length\n(fewer samples) attributes. We trained SeNMo for the task of overall survival\nusing pan-cancer data involving 33 cancer sites from Genomics Data Commons\n(GDC). The training data includes gene expression, DNA methylation, miRNA\nexpression, DNA mutations, protein expression modalities, and clinical data. We\nevaluated the model's performance in predicting overall survival using\nconcordance index (C-Index). SeNMo performed consistently well in training\nregime, with the validation C-Index of 0.76 on GDC's public data. In the\ntesting regime, SeNMo performed with a C-Index of 0.758 on a held-out test set.\nThe model showed an average accuracy of 99.8% on the task of classifying the\nprimary cancer type on the pan-cancer test cohort. SeNMo proved to be a\nmini-foundation model for multi-omics oncology data because it demonstrated\nrobust performance, and adaptability not only across molecular data types but\nalso on the classification task of predicting the primary cancer type of\npatients. SeNMo can be further scaled to any cancer site and molecular data\ntype. We believe SeNMo and similar models are poised to transform the oncology\nlandscape, offering hope for more effective, efficient, and patient-centric\ncancer care.\n"", '  The recent development of high-throughput sequencing creates a large\ncollection of multi-omics data, which enables researchers to better investigate\ncancer molecular profiles and cancer taxonomy based on molecular subtypes.\nIntegrating multi-omics data has been proven to be effective for building more\nprecise classification models. Current multi-omics integrative models mainly\nuse early fusion by concatenation or late fusion based on deep neural networks.\nDue to the nature of biological systems, graphs are a better representation of\nbio-medical data. Although few graph neural network (GNN) based multi-omics\nintegrative methods have been proposed, they suffer from three common\ndisadvantages. One is most of them use only one type of connection, either\ninter-omics or intra-omic connection; second, they only consider one kind of\nGNN layer, either graph convolution network (GCN) or graph attention network\n(GAT); and third, most of these methods lack testing on a more complex cancer\nclassification task. We propose a novel end-to-end multi-omics GNN framework\nfor accurate and robust cancer subtype classification. The proposed model\nutilizes multi-omics data in the form of heterogeneous multi-layer graphs that\ncombines both inter-omics and intra-omic connections from established\nbiological knowledge. The proposed model incorporates learned graph features\nand global genome features for accurate classification. We test the proposed\nmodel on TCGA Pan-cancer dataset and TCGA breast cancer dataset for molecular\nsubtype and cancer subtype classification, respectively. The proposed model\noutperforms four current state-of-the-art baseline models in multiple\nevaluation metrics. The comparative analysis of GAT-based models and GCN-based\nmodels reveals that GAT-based models are preferred for smaller graphs with less\ninformation and GCN-based models are preferred for larger graphs with extra\ninformation.\n']",Cancer Research using Multi-Omics and Genomics
325,324,25,324_recommender_recommendation_personalized_collaborative,"['recommender', 'recommendation', 'personalized', 'collaborative', 'domains', 'cdsr', 'interests', 'domain', 'cdr', 'hypergraph']","['domain', 'recommendation', 'overlapping', 'domains', 'cross', 'cold', 'user', 'start', 'users', 'target']","['recommender', 'domains', 'interests', 'hypergraph', 'representations', 'shared', 'cdctr', 'cdrs', 'cvpm', 'hgdr']","[""  Cross-domain recommendation (CDR) has been proven as a promising way to\ntackle the user cold-start problem, which aims to make recommendations for\nusers in the target domain by transferring the user preference derived from the\nsource domain. Traditional CDR studies follow the embedding and mapping (EMCDR)\nparadigm, which transfers user representations from the source to target domain\nby learning a user-shared mapping function, neglecting the user-specific\npreference. Recent CDR studies attempt to learn user-specific mapping functions\nin meta-learning paradigm, which regards each user's CDR as an individual task,\nbut neglects the preference correlations among users, limiting the beneficial\ninformation for user representations. Moreover, both of the paradigms neglect\nthe explicit user-item interactions from both domains during the mapping\nprocess. To address the above issues, this paper proposes a novel CDR framework\nwith neural process (NP), termed as CDRNP. Particularly, it develops the\nmeta-learning paradigm to leverage user-specific preference, and further\nintroduces a stochastic process by NP to capture the preference correlations\namong the overlapping and cold-start users, thus generating more powerful\nmapping functions by mapping the user-specific preference and common preference\ncorrelations to a predictive probability distribution. In addition, we also\nintroduce a preference remainer to enhance the common preference from the\noverlapping users, and finally devises an adaptive conditional decoder with\npreference modulation to make prediction for cold-start users with items in the\ntarget domain. Experimental results demonstrate that CDRNP outperforms previous\nSOTA methods in three real-world CDR scenarios.\n"", '  Cross-domain recommendation (CDR) extends conventional recommender systems by\nleveraging user-item interactions from dense domains to mitigate data sparsity\nand the cold start problem. While CDR offers substantial potential for\nenhancing recommendation performance, most existing CDR methods suffer from\nsensitivity to the ratio of overlapping users and intrinsic discrepancy between\nsource and target domains. To overcome these limitations, in this work, we\nexplore the application of graph signal processing (GSP) in CDR scenarios. We\npropose CGSP, a unified CDR framework based on GSP, which employs a\ncross-domain similarity graph constructed by flexibly combining target-only\nsimilarity and source-bridged similarity. By processing personalized graph\nsignals computed for users from either the source or target domain, our\nframework effectively supports both inter-domain and intra-domain\nrecommendations. Our empirical evaluation demonstrates that CGSP consistently\noutperforms various encoder-based CDR approaches in both intra-domain and\ninter-domain recommendation scenarios, especially when the ratio of overlapping\nusers is low, highlighting its significant practical implication in real-world\napplications.\n', ""  Cross-Domain Recommendation (CDR) is a promising paradigm inspired by\ntransfer learning to solve the cold-start problem in recommender systems.\nExisting state-of-the-art CDR methods train an explicit mapping function to\ntransfer the cold-start users from a data-rich source domain to a target\ndomain. However, a limitation of these methods is that the mapping function is\ntrained on overlapping users across domains, while only a small number of\noverlapping users are available for training. By visualizing the loss landscape\nof the existing CDR model, we find that training on a small number of\noverlapping users causes the model to converge to sharp minima, leading to poor\ngeneralization. Based on this observation, we leverage loss-geometry-based\nmachine learning approach and propose a novel CDR method called Sharpness-Aware\nCDR (SCDR). Our proposed method simultaneously optimizes recommendation loss\nand loss sharpness, leading to better generalization with theoretical\nguarantees. Empirical studies on real-world datasets demonstrate that SCDR\nsignificantly outperforms the other CDR models for cold-start recommendation\ntasks, while concurrently enhancing the model's robustness to adversarial\nattacks.\n""]",Cross-Domain Recommendation Systems
326,325,25,325_langevin_mcmc_stochastic_sampling,"['langevin', 'mcmc', 'stochastic', 'sampling', 'diffusion', 'discretization', 'subsampling', 'hessian', 'samplers', 'metropolis']","['sampling', 'concave', 'log', 'divergence', 'distributions', 'target', 'stochastic', 'density', 'gradient', 'smooth']","['langevin', 'mcmc', 'stochastic', 'diffusion', 'discretization', 'subsampling', 'hessian', 'samplers', 'sgldiff', 'guarantees']","['  Stochastic gradients have been widely integrated into Langevin-based methods\nto improve their scalability and efficiency in solving large-scale sampling\nproblems. However, the proximal sampler, which exhibits much faster convergence\nthan Langevin-based algorithms in the deterministic setting Lee et al. (2021),\nhas yet to be explored in its stochastic variants. In this paper, we study the\nStochastic Proximal Samplers (SPS) for sampling from non-log-concave\ndistributions. We first establish a general framework for implementing\nstochastic proximal samplers and establish the convergence theory accordingly.\nWe show that the convergence to the target distribution can be guaranteed as\nlong as the second moment of the algorithm trajectory is bounded and restricted\nGaussian oracles can be well approximated. We then provide two implementable\nvariants based on Stochastic gradient Langevin dynamics (SGLD) and\nMetropolis-adjusted Langevin algorithm (MALA), giving rise to SPS-SGLD and\nSPS-MALA. We further show that SPS-SGLD and SPS-MALA can achieve\n$\\epsilon$-sampling error in total variation (TV) distance within\n$\\tilde{\\mathcal{O}}(d\\epsilon^{-2})$ and\n$\\tilde{\\mathcal{O}}(d^{1/2}\\epsilon^{-2})$ gradient complexities, which\noutperform the best-known result by at least an $\\tilde{\\mathcal{O}}(d^{1/3})$\nfactor. This enhancement in performance is corroborated by our empirical\nstudies on synthetic data with various dimensions, demonstrating the efficiency\nof our proposed algorithm.\n', '  We study the task of efficiently sampling from a Gibbs distribution $d \\pi^*\n= e^{-h} d {vol}_g$ over a Riemannian manifold $M$ via (geometric) Langevin\nMCMC; this algorithm involves computing exponential maps in random Gaussian\ndirections and is efficiently implementable in practice. The key to our\nanalysis of Langevin MCMC is a bound on the discretization error of the\ngeometric Euler-Murayama scheme, assuming $\\nabla h$ is Lipschitz and $M$ has\nbounded sectional curvature. Our error bound matches the error of Euclidean\nEuler-Murayama in terms of its stepsize dependence. Combined with a contraction\nguarantee for the geometric Langevin Diffusion under Kendall-Cranston coupling,\nwe prove that the Langevin MCMC iterates lie within $\\epsilon$-Wasserstein\ndistance of $\\pi^*$ after $\\tilde{O}(\\epsilon^{-2})$ steps, which matches the\niteration complexity for Euclidean Langevin MCMC. Our results apply in general\nsettings where $h$ can be nonconvex and $M$ can have negative Ricci curvature.\nUnder additional assumptions that the Riemannian curvature tensor has bounded\nderivatives, and that $\\pi^*$ satisfies a $CD(\\cdot,\\infty)$ condition, we\nanalyze the stochastic gradient version of Langevin MCMC, and bound its\niteration complexity by $\\tilde{O}(\\epsilon^{-2})$ as well.\n', '  Understanding the dimension dependency of computational complexity in\nhigh-dimensional sampling problem is a fundamental problem, both from a\npractical and theoretical perspective. Compared with samplers with unbiased\nstationary distribution, e.g., Metropolis-adjusted Langevin algorithm (MALA),\nbiased samplers, e.g., Underdamped Langevin Dynamics (ULD), perform better in\nlow-accuracy cases just because a lower dimension dependency in their\ncomplexities. Along this line, Freund et al. (2022) suggest that the modified\nLangevin algorithm with prior diffusion is able to converge dimension\nindependently for strongly log-concave target distributions. Nonetheless, it\nremains open whether such property establishes for more general cases. In this\npaper, we investigate the prior diffusion technique for the target\ndistributions satisfying log-Sobolev inequality (LSI), which covers a much\nbroader class of distributions compared to the strongly log-concave ones. In\nparticular, we prove that the modified Langevin algorithm can also obtain the\ndimension-independent convergence of KL divergence with different step size\nschedules. The core of our proof technique is a novel construction of an\ninterpolating SDE, which significantly helps to conduct a more accurate\ncharacterization of the discrete updates of the overdamped Langevin dynamics.\nOur theoretical analysis demonstrates the benefits of prior diffusion for a\nbroader class of target distributions and provides new insights into developing\nfaster sampling algorithms.\n']",Langevin Monte Carlo Methods for Efficient Sampling
327,326,25,326_drift_drifts_driftlens_drifting,"['drift', 'drifts', 'driftlens', 'drifting', 'streams', 'detection', 'detecting', 'detect', 'stream', 'streaming']","['drift', 'concept', 'drifts', 'streams', 'stream', 'detection', 'detectors', 'leakages', 'detector', 'changes']","['drift', 'streams', 'adaptive', 'unsupervised', 'retraining', 'leakages', 'detectors', 'concept', 'periodically', 'exstream']","[""  Concept Drift is a phenomenon in which the underlying data distribution and\nstatistical properties of a target domain change over time, leading to a\ndegradation of the model's performance. Consequently, models deployed in\nproduction require continuous monitoring through drift detection techniques.\nMost drift detection methods to date are supervised, i.e., based on\nground-truth labels. However, true labels are usually not available in many\nreal-world scenarios. Although recent efforts have been made to develop\nunsupervised methods, they often lack the required accuracy, have a complexity\nthat makes real-time implementation in production environments difficult, or\nare unable to effectively characterize drift. To address these challenges, we\npropose DriftLens, an unsupervised real-time concept drift detection framework.\nIt works on unstructured data by exploiting the distribution distances of deep\nlearning representations. DriftLens can also provide drift characterization by\nanalyzing each label separately. A comprehensive experimental evaluation is\npresented with multiple deep learning classifiers for text, image, and speech.\nResults show that (i) DriftLens performs better than previous methods in\ndetecting drift in $11/13$ use cases; (ii) it runs at least 5 times faster;\n(iii) its detected drift value is very coherent with the amount of drift\n(correlation $\\geq 0.85$); (iv) it is robust to parameter changes.\n"", ""  Concept drift detection is crucial for many AI systems to ensure the system's\nreliability. These systems often have to deal with large amounts of data or\nreact in real-time. Thus, drift detectors must meet computational requirements\nor constraints with a comprehensive performance evaluation. However, so far,\nthe focus of developing drift detectors is on inference quality, e.g. accuracy,\nbut not on computational performance, such as runtime. Many of the previous\nworks consider computational performance only as a secondary objective and do\nnot have a benchmark for such evaluation. Hence, we propose and explain\nperformance engineering for unsupervised concept drift detection that reflects\non computational complexities, benchmarking, and performance analysis. We\nprovide the computational complexities of existing unsupervised drift detectors\nand discuss why further computational performance investigations are required.\nHence, we state and substantiate the aspects of a benchmark for unsupervised\ndrift detection reflecting on inference quality and computational performance.\nFurthermore, we demonstrate performance analysis practices that have proven\ntheir effectiveness in High-Performance Computing, by tracing two drift\ndetectors and displaying their performance data.\n"", '  Uncertain changes in data streams present challenges for machine learning\nmodels to dynamically adapt and uphold performance in real-time. Particularly,\nclassification boundary change, also known as real concept drift, is the major\ncause of classification performance deterioration. However, accurately\ndetecting real concept drift remains challenging because the theoretical\nfoundations of existing drift detection methods - two-sample distribution tests\nand monitoring classification error rate, both suffer from inherent limitations\nsuch as the inability to distinguish virtual drift (changes not affecting the\nclassification boundary, will introduce unnecessary model maintenance), limited\nstatistical power, or high computational cost. Furthermore, no existing\ndetection method can provide information on the trend of the drift, which could\nbe invaluable for model maintenance. This work presents a novel real concept\ndrift detection method based on Neighbor-Searching Discrepancy, a new statistic\nthat measures the classification boundary difference between two samples. The\nproposed method is able to detect real concept drift with high accuracy while\nignoring virtual drift. It can also indicate the direction of the\nclassification boundary change by identifying the invasion or retreat of a\ncertain class, which is also an indicator of separability change between\nclasses. A comprehensive evaluation of 11 experiments is conducted, including\nempirical verification of the proposed theory using artificial datasets, and\nexperimental comparisons with commonly used drift handling methods on\nreal-world datasets. The results show that the proposed theory is robust\nagainst a range of distributions and dimensions, and the drift detection method\noutperforms state-of-the-art alternative methods.\n']",Concept Drift Detection in Machine Learning
328,327,25,327_ai_intelligence_decisions_assisted,"['ai', 'intelligence', 'decisions', 'assisted', 'explanations', 'assistance', 'behavior', 'strategies', 'optimize', 'assistances']","['decision', 'making', 'assistance', 'reliance', 'human', 'advice', 'complementarity', 'assistances', 'umpires', 'recommendations']","['ai', 'explanations', 'assistances', 'misrepresentations', 'behavioral', 'guesser', 'advice', 'policies', 'influence', 'alarms']","[""  Explainability techniques are rapidly being developed to improve human-AI\ndecision-making across various cooperative work settings. Consequently,\nprevious research has evaluated how decision-makers collaborate with imperfect\nAI by investigating appropriate reliance and task performance with the aim of\ndesigning more human-centered computer-supported collaborative tools. Several\nhuman-centered explainable AI (XAI) techniques have been proposed in hopes of\nimproving decision-makers' collaboration with AI; however, these techniques are\ngrounded in findings from previous studies that primarily focus on the impact\nof incorrect AI advice. Few studies acknowledge the possibility of the\nexplanations being incorrect even if the AI advice is correct. Thus, it is\ncrucial to understand how imperfect XAI affects human-AI decision-making. In\nthis work, we contribute a robust, mixed-methods user study with 136\nparticipants to evaluate how incorrect explanations influence humans'\ndecision-making behavior in a bird species identification task, taking into\naccount their level of expertise and an explanation's level of assertiveness.\nOur findings reveal the influence of imperfect XAI and humans' level of\nexpertise on their reliance on AI and human-AI team performance. We also\ndiscuss how explanations can deceive decision-makers during human-AI\ncollaboration. Hence, we shed light on the impacts of imperfect XAI in the\nfield of computer-supported cooperative work and provide guidelines for\ndesigners of human-AI collaboration systems.\n"", '  Imagine if AI decision-support tools not only complemented our ability to\nmake accurate decisions, but also improved our skills, boosted collaboration,\nand elevated the joy we derive from our tasks. Despite the potential to\noptimize a broad spectrum of such human-centric objectives, the design of\ncurrent AI tools remains focused on decision accuracy alone. We propose offline\nreinforcement learning (RL) as a general approach for modeling human-AI\ndecision-making to optimize human-AI interaction for diverse objectives. RL can\noptimize such objectives by tailoring decision support, providing the right\ntype of assistance to the right person at the right time. We instantiated our\napproach with two objectives: human-AI accuracy on the decision-making task and\nhuman learning about the task and learned decision support policies from\nprevious human-AI interaction data. We compared the optimized policies against\nseveral baselines in AI-assisted decision-making. Across two experiments (N=316\nand N=964), our results demonstrated that people interacting with policies\noptimized for accuracy achieve significantly better accuracy -- and even\nhuman-AI complementarity -- compared to those interacting with any other type\nof AI support. Our results further indicated that human learning was more\ndifficult to optimize than accuracy, with participants who interacted with\nlearning-optimized policies showing significant learning improvement only at\ntimes. Our research (1) demonstrates offline RL to be a promising approach to\nmodel human-AI decision-making, leading to policies that may optimize\nhuman-centric objectives and provide novel insights about the AI-assisted\ndecision-making space, and (2) emphasizes the importance of considering\nhuman-centric objectives beyond decision accuracy in AI-assisted\ndecision-making, opening up the novel research challenge of optimizing human-AI\ninteraction for such objectives.\n', ""  With the rapid development of AI-based decision aids, different forms of AI\nassistance have been increasingly integrated into the human decision making\nprocesses. To best support humans in decision making, it is essential to\nquantitatively understand how diverse forms of AI assistance influence humans'\ndecision making behavior. To this end, much of the current research focuses on\nthe end-to-end prediction of human behavior using ``black-box'' models, often\nlacking interpretations of the nuanced ways in which AI assistance impacts the\nhuman decision making process. Meanwhile, methods that prioritize the\ninterpretability of human behavior predictions are often tailored for one\nspecific form of AI assistance, making adaptations to other forms of assistance\ndifficult. In this paper, we propose a computational framework that can provide\nan interpretable characterization of the influence of different forms of AI\nassistance on decision makers in AI-assisted decision making. By\nconceptualizing AI assistance as the ``{\\em nudge}'' in human decision making\nprocesses, our approach centers around modelling how different forms of AI\nassistance modify humans' strategy in weighing different information in making\ntheir decisions. Evaluations on behavior data collected from real human\ndecision makers show that the proposed framework outperforms various baselines\nin accurately predicting human behavior in AI-assisted decision making. Based\non the proposed framework, we further provide insights into how individuals\nwith different cognitive styles are nudged by AI assistance differently.\n""]",Human-AI Collaboration and Decision-Making
329,328,25,328_copyrightability_copyright_copyrights_copyrighted,"['copyrightability', 'copyright', 'copyrights', 'copyrighted', 'infringing', 'infringement', 'copying', 'creators', 'ai', 'generative']","['copyright', 'infringement', 'generative', 'protection', 'originality', 'asset', 'legal', 'intellectual', 'rights', 'owners']","['copyrightability', 'infringement', 'creators', 'ai', 'generative', 'originality', 'artworks', 'copyleft', 'violating', 'resemblance']","['  In the rapidly evolving landscape of generative artificial intelligence (AI),\nthe increasingly pertinent issue of copyright infringement arises as AI\nadvances to generate content from scraped copyrighted data, prompting questions\nabout ownership and protection that impact professionals across various\ncareers. With this in mind, this survey provides an extensive examination of\ncopyright infringement as it pertains to generative AI, aiming to stay abreast\nof the latest developments and open problems. Specifically, it will first\noutline methods of detecting copyright infringement in mediums such as text,\nimage, and video. Next, it will delve an exploration of existing techniques\naimed at safeguarding copyrighted works from generative models. Furthermore,\nthis survey will discuss resources and tools for users to evaluate copyright\nviolations. Finally, insights into ongoing regulations and proposals for AI\nwill be explored and compared. Through combining these disciplines, the\nimplications of AI-driven content and copyright are thoroughly illustrated and\nbrought into question.\n', '  The advent of Generative Artificial Intelligence (GenAI) models, including\nGitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content\ncreation, enabling non-professionals to produce high-quality content across\nvarious domains. This transformative technology has led to a surge of synthetic\ncontent and sparked legal disputes over copyright infringement. To address\nthese challenges, this paper introduces a novel approach that leverages the\nlearning capacity of GenAI models for copyright legal analysis, demonstrated\nwith GPT2 and Stable Diffusion models. Copyright law distinguishes between\noriginal expressions and generic ones (Sc\\`enes \\`a faire), protecting the\nformer and permitting reproduction of the latter. However, this distinction has\nhistorically been challenging to make consistently, leading to over-protection\nof copyrighted works. GenAI offers an unprecedented opportunity to enhance this\nlegal analysis by revealing shared patterns in preexisting works. We propose a\ndata-driven approach to identify the genericity of works created by GenAI,\nemploying ""data-driven bias"" to assess the genericity of expressive\ncompositions. This approach aids in copyright scope determination by utilizing\nthe capabilities of GenAI to identify and prioritize expressive elements and\nrank them according to their frequency in the model\'s dataset. The potential\nimplications of measuring expressive genericity for copyright law are profound.\nSuch scoring could assist courts in determining copyright scope during\nlitigation, inform the registration practices of Copyright Offices, allowing\nregistration of only highly original synthetic works, and help copyright owners\nsignal the value of their works and facilitate fairer licensing deals. More\ngenerally, this approach offers valuable insights to policymakers grappling\nwith adapting copyright law to the challenges posed by the era of GenAI.\n', '  This paper addresses the contentious issue of copyright infringement in\nimages generated by text-to-image models, sparking debates among AI developers,\ncontent creators, and legal entities. State-of-the-art models create\nhigh-quality content without crediting original creators, causing concern in\nthe artistic community. To mitigate this, we propose the \\copyright Plug-in\nAuthorization framework, introducing three operations: addition, extraction,\nand combination. Addition involves training a \\copyright plug-in for specific\ncopyright, facilitating proper credit attribution. Extraction allows creators\nto reclaim copyright from infringing models, and combination enables users to\nmerge different \\copyright plug-ins. These operations act as permits,\nincentivizing fair use and providing flexibility in authorization. We present\ninnovative approaches,""Reverse LoRA"" for extraction and ""EasyMerge"" for\nseamless combination. Experiments in artist-style replication and cartoon IP\nrecreation demonstrate \\copyright plug-ins\' effectiveness, offering a valuable\nsolution for human copyright protection in the age of generative AIs.\n']",Copyright and Generative AI
330,329,24,329_cnn_defects_features_inceptionv3,"['cnn', 'defects', 'features', 'inceptionv3', 'feature', 'defect', 'dataset', 'autoencoder', 'inspection', 'detecting']","['defect', 'defects', 'manufacturing', 'inspection', 'detection', 'industrial', 'semiconductor', 'defective', 'images', 'feature']","['cnn', 'defects', 'features', 'inceptionv3', 'autoencoder', 'yolov5', 'detect', 'pipeline', 'shapefeat', 'wafer']","['  Recent advancements in quality control across various industries have\nincreasingly utilized the integration of video cameras and image processing for\neffective defect detection. A critical barrier to progress is the scarcity of\ncomprehensive datasets featuring annotated defects, which are essential for\ndeveloping and refining automated defect detection models. This systematic\nreview, spanning from 2015 to 2023, identifies 15 publicly available datasets\nand critically examines them to assess their effectiveness and applicability\nfor benchmarking and model development. Our findings reveal a diverse landscape\nof datasets, such as NEU-CLS, NEU-DET, DAGM, KolektorSDD, PCB Defect Dataset,\nand the Hollow Cylindrical Defect Detection Dataset, each with unique strengths\nand limitations in terms of image quality, defect type representation, and\nreal-world applicability. The goal of this systematic review is to consolidate\nthese datasets in a single location, providing researchers who seek such\npublicly available resources with a comprehensive reference.\n', '  Deep learning-based semiconductor defect inspection has gained traction in\nrecent years, offering a powerful and versatile approach that provides high\naccuracy, adaptability, and efficiency in detecting and classifying nano-scale\ndefects. However, semiconductor manufacturing processes are continually\nevolving, leading to the emergence of new types of defects over time. This\npresents a significant challenge for conventional supervised defect detectors,\nas they may suffer from catastrophic forgetting when trained on new defect\ndatasets, potentially compromising performance on previously learned tasks. An\nalternative approach involves the constant storage of previously trained\ndatasets alongside pre-trained model versions, which can be utilized for\n(re-)training from scratch or fine-tuning whenever encountering a new defect\ndataset. However, adhering to such a storage template is impractical in terms\nof size, particularly when considering High-Volume Manufacturing (HVM).\nAdditionally, semiconductor defect datasets, especially those encompassing\nstochastic defects, are often limited and expensive to obtain, thus lacking\nsufficient representation of the entire universal set of defectivity. This work\nintroduces a task-agnostic, meta-learning approach aimed at addressing this\nchallenge, which enables the incremental addition of new defect classes and\nscales to create a more robust and generalized model for semiconductor defect\ninspection. We have benchmarked our approach using real resist-wafer SEM\n(Scanning Electron Microscopy) datasets for two process steps, ADI and AEI,\ndemonstrating its superior performance compared to conventional supervised\ntraining methods.\n', '  As automation advances in manufacturing, the demand for precise and\nsophisticated defect detection technologies grows. Existing vision models for\ndefect recognition methods are insufficient for handling the complexities and\nvariations of defects in contemporary manufacturing settings. These models\nespecially struggle in scenarios involving limited or imbalanced defect data.\nIn this work, we introduce MemoryMamba, a novel memory-augmented state space\nmodel (SSM), designed to overcome the limitations of existing defect\nrecognition models. MemoryMamba integrates the state space model with the\nmemory augmentation mechanism, enabling the system to maintain and retrieve\nessential defect-specific information in training. Its architecture is designed\nto capture dependencies and intricate defect characteristics, which are crucial\nfor effective defect detection. In the experiments, MemoryMamba was evaluated\nacross four industrial datasets with diverse defect types and complexities. The\nmodel consistently outperformed other methods, demonstrating its capability to\nadapt to various defect recognition scenarios.\n']",Defect Detection using Deep Learning
331,330,24,330_variational_unnormalized_inference_posteriors,"['variational', 'unnormalized', 'inference', 'posteriors', 'gradient', 'probabilistic', 'divergence', 'likelihood', 'reparameterization', 'posterior']","['variational', 'posterior', 'inference', 'gradient', 'amortized', 'families', 'variance', 'approximation', 'expectation', 'estimators']","['variational', 'unnormalized', 'posteriors', 'divergence', 'likelihood', 'reparameterization', 'bbvi', 'samplers', 'empirically', 'estimator']","['  Variational families with full-rank covariance approximations are known not\nto work well in black-box variational inference (BBVI), both empirically and\ntheoretically. In fact, recent computational complexity results for BBVI have\nestablished that full-rank variational families scale poorly with the\ndimensionality of the problem compared to e.g. mean-field families. This is\nparticularly critical to hierarchical Bayesian models with local variables;\ntheir dimensionality increases with the size of the datasets. Consequently, one\ngets an iteration complexity with an explicit (\\mathcal{O}(N^2)) dependence on\nthe dataset size (N). In this paper, we explore a theoretical middle ground\nbetween mean-field variational families and full-rank families: structured\nvariational families. We rigorously prove that certain scale matrix structures\ncan achieve a better iteration complexity of (\\mathcal{O}\\left(N\\right)),\nimplying better scaling with respect to (N). We empirically verify our\ntheoretical results on large-scale hierarchical models.\n', '  Estimating a distribution given access to its unnormalized density is pivotal\nin Bayesian inference, where the posterior is generally known only up to an\nunknown normalizing constant. Variational inference and Markov chain Monte\nCarlo methods are the predominant tools for this task; however, both methods\nare often challenging to apply reliably, particularly when the posterior has\ncomplex geometry. Here, we introduce Soft Contrastive Variational Inference\n(SoftCVI), which allows a family of variational objectives to be derived\nthrough a contrastive estimation framework. These objectives have zero variance\ngradient when the variational approximation is exact, without the need for\nspecialized gradient estimators. The approach involves parameterizing a\nclassifier in terms of the variational distribution, which allows the inference\ntask to be reframed as a contrastive estimation problem, aiming to identify a\nsingle true posterior sample among a set of samples. Despite this framing, we\ndo not require positive or negative samples, but rather learn by sampling the\nvariational distribution and computing ground truth soft classification labels\nfrom the unnormalized posterior itself. We empirically investigate the\nperformance on a variety of Bayesian inference tasks, using both using both\nsimple (e.g. normal) and expressive (normalizing flow) variational\ndistributions. We find that SoftCVI objectives often outperform other commonly\nused variational objectives.\n', '  We provide the first convergence guarantee for full black-box variational\ninference (BBVI), also known as Monte Carlo variational inference. While\npreliminary investigations worked on simplified versions of BBVI (e.g., bounded\ndomain, bounded support, only optimizing for the scale, and such), our setup\ndoes not need any such algorithmic modifications. Our results hold for\nlog-smooth posterior densities with and without strong log-concavity and the\nlocation-scale variational family. Also, our analysis reveals that certain\nalgorithm design choices commonly employed in practice, particularly, nonlinear\nparameterizations of the scale of the variational approximation, can result in\nsuboptimal convergence rates. Fortunately, running BBVI with proximal\nstochastic gradient descent fixes these limitations, and thus achieves the\nstrongest known convergence rate guarantees. We evaluate this theoretical\ninsight by comparing proximal SGD against other standard implementations of\nBBVI on large-scale Bayesian inference problems.\n']",Variational Inference Methods
332,331,24,331_personas_persona_personascore_personae,"['personas', 'persona', 'personascore', 'personae', 'personagym', 'profiles', 'agent', 'personality', 'personalities', 'characters']","['persona', 'playing', 'role', 'character', 'personas', 'characters', 'conversational', 'agents', 'agent', 'personalized']","['personas', 'profiles', 'agent', 'personality', 'personalization', 'charactereval', 'traits', 'rolefact', 'dialogues', 'portrayals']","[""  With the recent introduction of Assistants API, it is expected that\ndocument-based language models will be actively used in various domains,\nespecially Role-playing. However, a key challenge lies in utilizing\nprotagonist's persona: Assistants API often fails to achieve with its search\nbecause the information extraction part is different each time and it often\nomits important information such as protagonist's backstory or relationships.\nIt is hard to maintain a consistent persona simply by using the persona\ndocument as input to the Assistants API. To address the challenge of achieving\nstable persona consistency, we propose CharacterGPT, a novel persona\nreconstruction framework to alleviate the shortcomings of the Assistants API.\nOur method involves Character Persona Training (CPT), an effective persona\nrebuilding process that updates the character persona by extracting the\ncharacter's traits from given summary of the novel for each character as if the\nstory in a novel progresses. In our experiments, we ask each character to take\nthe Big Five Inventory personality test in various settings and analyze the\nresults. To assess whether it can think outside the box, we let each character\ngenerate short novels. Extensive experiments and human evaluation demonstrate\nthat CharacterGPT presents new possibilities for role-playing agent research.\nCode and results are available at: https://github.com/Jeiyoon/charactergpt\n"", '  Persona agents, which are LLM agents that act according to an assigned\npersona, have demonstrated impressive contextual response capabilities across\nvarious applications. These persona agents offer significant enhancements\nacross diverse sectors, such as education, healthcare, and entertainment, where\nmodel developers can align agent responses to different user requirements\nthereby broadening the scope of agent applications. However, evaluating persona\nagent performance is incredibly challenging due to the complexity of assessing\npersona adherence in free-form interactions across various environments that\nare relevant to each persona agent. We introduce PersonaGym, the first dynamic\nevaluation framework for assessing persona agents, and PersonaScore, the first\nautomated human-aligned metric grounded in decision theory for comprehensive\nlarge-scale evaluation of persona agents. Our evaluation of 6 open and\nclosed-source LLMs, using a benchmark encompassing 200 personas and 10,000\nquestions, reveals significant opportunities for advancement in persona agent\ncapabilities across state-of-the-art models. For example, Claude 3.5 Sonnet\nonly has a 2.97% relative improvement in PersonaScore than GPT 3.5 despite\nbeing a much more advanced model. Importantly, we find that increased model\nsize and complexity do not necessarily imply enhanced persona agent\ncapabilities thereby highlighting the pressing need for algorithmic and\narchitectural invention towards faithful and performant persona agents.\n', '  This survey explores the burgeoning field of role-playing with language\nmodels, focusing on their development from early persona-based models to\nadvanced character-driven simulations facilitated by Large Language Models\n(LLMs). Initially confined to simple persona consistency due to limited model\ncapabilities, role-playing tasks have now expanded to embrace complex character\nportrayals involving character consistency, behavioral alignment, and overall\nattractiveness. We provide a comprehensive taxonomy of the critical components\nin designing these systems, including data, models and alignment, agent\narchitecture and evaluation. This survey not only outlines the current\nmethodologies and challenges, such as managing dynamic personal profiles and\nachieving high-level persona consistency but also suggests avenues for future\nresearch in improving the depth and realism of role-playing applications. The\ngoal is to guide future research by offering a structured overview of current\nmethodologies and identifying potential areas for improvement. Related\nresources and papers are available at\nhttps://github.com/nuochenpku/Awesome-Role-Play-Papers.\n']",Persona-Based Language Models for Role-Playing Applications
333,332,24,332_genderless_gender_genders_translations,"['genderless', 'gender', 'genders', 'translations', 'gendered', 'multilingual', 'translating', 'masculine', 'translation', 'feminine']","['gender', 'translation', 'pronoun', 'pronouns', 'bias', 'neutral', 'translations', 'gendered', 'masculine', 'neopronouns']","['genderless', 'translations', 'pronouns', 'neopronouns', 'bias', 'turkish', 'referring', 'coreference', 'contexts', 'tokenizer']","['  In machine translation, the problem of ambiguously gendered input has been\npointed out, where the gender of an entity is not available in the source\nsentence. To address this ambiguity issue, the task of controlled translation\nthat takes the gender of the ambiguous entity as additional input have been\nproposed. However, most existing works have only considered a simplified setup\nof one target gender for input. In this paper, we tackle controlled translation\nin a more realistic setting of inputs with multiple entities and propose\nGender-of-Entity (GoE) prompting method for LLMs. Our proposed method instructs\nthe model with fine-grained entity-level gender information to translate with\ncorrect gender inflections. By utilizing four evaluation benchmarks, we\ninvestigate the controlled translation capability of LLMs in multiple\ndimensions and find that LLMs reach state-of-the-art performance in controlled\ntranslation. Furthermore, we discover an emergence of gender interference\nphenomenon when controlling the gender of multiple entities. Finally, we\naddress the limitations of existing gender accuracy evaluation metrics and\npropose leveraging LLMs as an evaluator for gender inflection in machine\ntranslation.\n', '  Gender bias has been a focal point in the study of bias in machine\ntranslation and language models. Existing machine translation gender bias\nevaluations are primarily focused on male and female genders, limiting the\nscope of the evaluation. To assess gender bias accurately, these studies often\nrely on calculating the accuracy of gender pronouns or the masculine and\nfeminine attributes of grammatical gender via the stereotypes triggered by\noccupations or sentiment words ({\\em i.e.}, clear positive or negative\nattitude), which cannot extend to non-binary groups. This study presents a\nbenchmark AmbGIMT (Gender-Inclusive Machine Translation with Ambiguous attitude\nwords), which assesses gender bias beyond binary gender. Meanwhile, we propose\na novel process to evaluate gender bias based on the Emotional Attitude Score\n(EAS), which is used to quantify ambiguous attitude words. In evaluating three\nrecent and effective open-source LLMs and one powerful multilingual\ntranslation-specific model, our main observations are: (1) The translation\nperformance within non-binary gender contexts is markedly inferior in terms of\ntranslation quality and exhibits more negative attitudes than binary-gender\ncontexts. (2) The analysis experiments indicate that incorporating constraint\ncontext in prompts for gender identity terms can substantially reduce\ntranslation bias, while the bias remains evident despite the presence of the\nconstraints. The code is publicly available at\n\\url{https://github.com/pppa2019/ambGIMT}.\n', ""  While machine translation (MT) systems have seen significant improvements, it\nis still common for translations to reflect societal biases, such as gender\nbias. Decoder-only Large Language Models (LLMs) have demonstrated potential in\nMT, albeit with performance slightly lagging behind traditional encoder-decoder\nNeural Machine Translation (NMT) systems. However, LLMs offer a unique\nadvantage: the ability to control the properties of the output through prompts.\nIn this study, we leverage this flexibility to explore LLaMa's capability to\nproduce gender-specific translations. Our results indicate that LLaMa can\ngenerate gender-specific translations with translation accuracy and gender bias\ncomparable to NLLB, a state-of-the-art multilingual NMT system. Furthermore,\nour experiments reveal that LLaMa's gender-specific translations rely on\ncoreference resolution to determine gender, showing higher gender variance in\ngender-ambiguous datasets but maintaining consistency in less ambiguous\ncontexts. This research investigates the potential and challenges of using LLMs\nfor gender-specific translations as an instance of the controllability of\noutputs offered by LLMs.\n""]",Gender Bias in Machine Translation
334,333,24,333_bottleneck_concepts_concept_annotations,"['bottleneck', 'concepts', 'concept', 'annotations', 'features', 'models', 'concept_realignment', 'sparsecbm', 'interpretability', 'saliency']","['concept', 'concepts', 'bottleneck', 'understandable', 'interventions', 'black', 'interpretable', 'localities', 'intervention', 'box']","['bottleneck', 'annotations', 'features', 'concept_realignment', 'sparsecbm', 'saliency', 'prediction', 'interpretable', 'cb2m', 'textbf']","['  There has been considerable recent interest in interpretable concept-based\nmodels such as Concept Bottleneck Models (CBMs), which first predict\nhuman-interpretable concepts and then map them to output classes. To reduce\nreliance on human-annotated concepts, recent works have converted pretrained\nblack-box models into interpretable CBMs post-hoc. However, these approaches\npredefine a set of concepts, assuming which concepts a black-box model encodes\nin its representations. In this work, we eliminate this assumption by\nleveraging unsupervised concept discovery to automatically extract concepts\nwithout human annotations or a predefined set of concepts. We further introduce\nan input-dependent concept selection mechanism that ensures only a small subset\nof concepts is used across all classes. We show that our approach improves\ndownstream performance and narrows the performance gap to black-box models,\nwhile using significantly fewer concepts in the classification. Finally, we\ndemonstrate how large vision-language models can intervene on the final model\nweights to correct model errors.\n', ""  Concept Bottleneck Models (CBMs) are regarded as inherently interpretable\nbecause they first predict a set of human-defined concepts which are used to\npredict a task label. For inherent interpretability to be fully realised, and\nensure trust in a model's output, it's desirable for concept predictions to use\nsemantically meaningful input features. For instance, in an image, pixels\nrepresenting a broken bone should contribute to predicting a fracture. However,\ncurrent literature suggests that concept predictions often rely on irrelevant\ninput features. We hypothesise that this occurs when dataset labels include\ninaccurate concept annotations, or the relationship between input features and\nconcepts is unclear. In general, the effect of dataset labelling on concept\nrepresentations remains an understudied area. In this paper, we demonstrate\nthat CBMs can learn to map concepts to semantically meaningful input features,\nby utilising datasets with a clear link between the input features and the\ndesired concept predictions. This is achieved, for instance, by ensuring\nmultiple concepts do not always co-occur and, therefore provide a clear\ntraining signal for the CBM to distinguish the relevant input features for each\nconcept. We validate our hypothesis on both synthetic and real-world image\ndatasets, and demonstrate under the correct conditions, CBMs can learn to\nattribute semantically meaningful input features to the correct concept\npredictions.\n"", '  Concept Bottleneck Models (CBMs) map the black-box visual representations\nextracted by deep neural networks onto a set of interpretable concepts and use\nthe concepts to make predictions, enhancing the transparency of the\ndecision-making process. Multimodal pre-trained models can match visual\nrepresentations with textual concept embeddings, allowing for obtaining the\ninterpretable concept bottleneck without the expertise concept annotations.\nRecent research has focused on the concept bank establishment and the\nhigh-quality concept selection. However, it is challenging to construct a\ncomprehensive concept bank through humans or large language models, which\nseverely limits the performance of CBMs. In this work, we propose the\nIncremental Residual Concept Bottleneck Model (Res-CBM) to address the\nchallenge of concept completeness. Specifically, the residual concept\nbottleneck model employs a set of optimizable vectors to complete missing\nconcepts, then the incremental concept discovery module converts the\ncomplemented vectors with unclear meanings into potential concepts in the\ncandidate concept bank. Our approach can be applied to any user-defined concept\nbank, as a post-hoc processing method to enhance the performance of any CBMs.\nFurthermore, to measure the descriptive efficiency of CBMs, the Concept\nUtilization Efficiency (CUE) metric is proposed. Experiments show that the\nRes-CBM outperforms the current state-of-the-art methods in terms of both\naccuracy and efficiency and achieves comparable performance to black-box models\nacross multiple datasets.\n']",Concept Bottleneck Models for Interpretable Machine Learning
335,334,24,334_distillation_datasets_dataset_distill,"['distillation', 'datasets', 'dataset', 'distill', 'distilled', 'imagenet', 'data', 'training', 'samples', 'amplification']","['distillation', 'distilled', 'dataset', 'synthetic', 'original', 'compress', 'matching', 'datasets', 'compact', 'storage']","['distillation', 'datasets', 'imagenet', 'training', 'amplification', 'labels', 'infogrowth', 'biases', 'compressing', 'val84']","['  Herein, we propose a novel dataset distillation method for constructing small\ninformative datasets that preserve the information of the large original\ndatasets. The development of deep learning models is enabled by the\navailability of large-scale datasets. Despite unprecedented success,\nlarge-scale datasets considerably increase the storage and transmission costs,\nresulting in a cumbersome model training process. Moreover, using raw data for\ntraining raises privacy and copyright concerns. To address these issues, a new\ntask named dataset distillation has been introduced, aiming to synthesize a\ncompact dataset that retains the essential information from the large original\ndataset. State-of-the-art (SOTA) dataset distillation methods have been\nproposed by matching gradients or network parameters obtained during training\non real and synthetic datasets. The contribution of different network\nparameters to the distillation process varies, and uniformly treating them\nleads to degraded distillation performance. Based on this observation, we\npropose an importance-aware adaptive dataset distillation (IADD) method that\ncan improve distillation performance by automatically assigning importance\nweights to different network parameters during distillation, thereby\nsynthesizing more robust distilled datasets. IADD demonstrates superior\nperformance over other SOTA dataset distillation methods based on parameter\nmatching on multiple benchmark datasets and outperforms them in terms of\ncross-architecture generalization. In addition, the analysis of self-adaptive\nweights demonstrates the effectiveness of IADD. Furthermore, the effectiveness\nof IADD is validated in a real-world medical application such as COVID-19\ndetection.\n', '  Dataset distillation aims at synthesizing a dataset by a small number of\nartificially generated data items, which, when used as training data, reproduce\nor approximate a machine learning (ML) model as if it were trained on the\nentire original dataset. Consequently, data distillation methods are usually\ntied to a specific ML algorithm. While recent literature deals mainly with\ndistillation of large collections of images in the context of neural network\nmodels, tabular data distillation is much less represented and mainly focused\non a theoretical perspective. The current paper explores the potential of a\nsimple distillation technique previously proposed in the context of\nLess-than-one shot learning. The main goal is to push further the performance\nof prototype-based soft-labels distillation in terms of classification\naccuracy, by integrating optimization steps in the distillation process. The\nanalysis is performed on real-world data sets with various degrees of\nimbalance. Experimental studies trace the capability of the method to distill\nthe data, but also the opportunity to act as an augmentation method, i.e. to\ngenerate new data that is able to increase model accuracy when used in\nconjunction with - as opposed to instead of - the original data.\n', '  Data-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.\n']",Dataset Distillation
336,335,24,335_crisistransformers_tweets_twitter_microblogs,"['crisistransformers', 'tweets', 'twitter', 'microblogs', 'disasters', 'crises', 'tweet', 'crisisvit', 'emergencies', 'disaster']","['crisis', 'disaster', 'media', 'social', 'evacuation', 'emergency', 'events', 'informatics', 'emergencies', 'event']","['crisistransformers', 'tweets', 'disasters', 'evacuation', 'incidents1m', 'humanitarian', 'topics', 'lingual', 'geotagged', 'relief']","['  Tasks such as semantic search and clustering on crisis-related social media\ntexts enhance our comprehension of crisis discourse, aiding decision-making and\ntargeted interventions. Pre-trained language models have advanced performance\nin crisis informatics, but their contextual embeddings lack semantic\nmeaningfulness. Although the CrisisTransformers family includes a sentence\nencoder to address the semanticity issue, it remains monolingual, processing\nonly English texts. Furthermore, employing separate models for different\nlanguages leads to embeddings in distinct vector spaces, introducing challenges\nwhen comparing semantic similarities between multi-lingual texts. Therefore, we\npropose multi-lingual sentence encoders (CT-XLMR-SE and CT-mBERT-SE) that embed\ncrisis-related social media texts for over 50 languages, such that texts with\nsimilar meanings are in close proximity within the same vector space,\nirrespective of language diversity. Results in sentence encoding and sentence\nmatching tasks are promising, suggesting these models could serve as robust\nbaselines when embedding multi-lingual crisis-related social media texts. The\nmodels are publicly available at: https://huggingface.co/crisistransformers.\n', '  In the field of crisis/disaster informatics, social media is increasingly\nbeing used for improving situational awareness to inform response and relief\nefforts. Efficient and accurate text classification tools have been a focal\narea of investigation in crisis informatics. However, current methods mostly\nrely on single-label text classification models, which fails to capture\ndifferent insights embedded in dynamic and multifaceted disaster-related social\nmedia data. This study introduces a novel approach to disaster text\nclassification by enhancing a pre-trained Large Language Model (LLM) through\ninstruction fine-tuning targeted for multi-label classification of\ndisaster-related tweets. Our methodology involves creating a comprehensive\ninstruction dataset from disaster-related tweets, which is then used to\nfine-tune an open-source LLM, thereby embedding it with disaster-specific\nknowledge. This fine-tuned model can classify multiple aspects of\ndisaster-related information simultaneously, such as the type of event,\ninformativeness, and involvement of human aid, significantly improving the\nutility of social media data for situational awareness in disasters. The\nresults demonstrate that this approach enhances the categorization of critical\ninformation from social media posts, thereby facilitating a more effective\ndeployment for situational awareness during emergencies. This research paves\nthe way for more advanced, adaptable, and robust disaster management tools,\nleveraging the capabilities of LLMs to improve real-time situational awareness\nand response strategies in disaster scenarios.\n', '  Social media platforms play an essential role in crisis communication, but\nanalyzing crisis-related social media texts is challenging due to their\ninformal nature. Transformer-based pre-trained models like BERT and RoBERTa\nhave shown success in various NLP tasks, but they are not tailored for\ncrisis-related texts. Furthermore, general-purpose sentence encoders are used\nto generate sentence embeddings, regardless of the textual complexities in\ncrisis-related texts. Advances in applications like text classification,\nsemantic search, and clustering contribute to the effective processing of\ncrisis-related texts, which is essential for emergency responders to gain a\ncomprehensive view of a crisis event, whether historical or real-time. To\naddress these gaps in crisis informatics literature, this study introduces\nCrisisTransformers, an ensemble of pre-trained language models and sentence\nencoders trained on an extensive corpus of over 15 billion word tokens from\ntweets associated with more than 30 crisis events, including disease outbreaks,\nnatural disasters, conflicts, and other critical incidents. We evaluate\nexisting models and CrisisTransformers on 18 crisis-specific public datasets.\nOur pre-trained models outperform strong baselines across all datasets in\nclassification tasks, and our best-performing sentence encoder improves the\nstate-of-the-art by 17.43% in sentence encoding tasks. Additionally, we\ninvestigate the impact of model initialization on convergence and evaluate the\nsignificance of domain-specific models in generating semantically meaningful\nsentence embeddings. The models are publicly available at:\nhttps://huggingface.co/crisistransformers\n']",Crisis Informatics and Social Media Analysis
337,336,23,336_braille_accessibility_impaired_blind,"['braille', 'accessibility', 'impaired', 'blind', 'impairments', 'impairment', 'disabilities', 'disability', 'accessible', 'sighted']","['accessibility', 'blind', 'braille', 'people', 'impairments', 'inclusive', 'disabilities', 'descriptions', 'visual', 'individuals']","['braille', 'accessibility', 'impairments', 'sighted', 'recognize', 'assistive', 'auditory', 'worldscribe', 'voiceover', 'banknotes']","['  In recent years, advancements in Natural Language Processing (NLP) techniques\nhave revolutionized the field of accessibility and exclusivity of testing,\nparticularly for visually impaired students (VIS). CBT has shown in years back\nits relevance in terms of administering exams electronically, making the test\nprocess easier, providing quicker and more accurate results, and offering\ngreater flexibility and accessibility for candidates. Yet, its relevance was\nnot felt by the visually impaired students as they cannot access printed\ndocuments. Hence, in this paper, we present an NLP-driven Computer-Based Test\nguide for visually impaired students. It employs a speech technology\npre-trained methods to provide real-time assistance and support to visually\nimpaired students. The system utilizes NLP technologies to convert the\ntext-based questions and the associated options in a machine-readable format.\nSubsequently, the speech technology pre-trained model processes the converted\ntext enabling the VIS to comprehend and analyze the content. Furthermore, we\nvalidated that this pre-trained model is not perverse by testing for accuracy\nusing sample audio datasets labels (A, B, C, D, E, F, G) to compare with the\nvoice recordings obtained from 20 VIS which is been predicted by the system to\nattain values for precision, recall, and F1-scores. These metrics are used to\nassess the performance of the pre-trained model and have indicated that it is\nproficient enough to give its better performance to the evaluated system. The\nmethodology adopted for this system is Object Oriented Analysis and Design\nMethodology (OOADM) where Objects are discussed and built by modeling\nreal-world instances.\n', '  With the increasing need for inclusive and user-friendly technology, web\naccessibility is crucial to ensuring equal access to online content for\nindividuals with disabilities, including visual, auditory, cognitive, or motor\nimpairments. Despite the existence of accessibility guidelines and standards\nsuch as Web Content Accessibility Guidelines (WCAG) and the Web Accessibility\nInitiative (W3C), over 90% of websites still fail to meet the necessary\naccessibility requirements. For web users with disabilities, there exists a\nneed for a tool to automatically fix web page accessibility errors. While\nresearch has demonstrated methods to find and target accessibility errors, no\nresearch has focused on effectively correcting such violations. This paper\npresents a novel approach to correcting accessibility violations on the web by\nmodifying the document object model (DOM) in real time with foundation models.\nLeveraging accessibility error information, large language models (LLMs), and\nprompt engineering techniques, we achieved greater than a 51% reduction in\naccessibility violation errors after corrections on our novel benchmark:\nACCESS. Our work demonstrates a valuable approach toward the direction of\ninclusive web content, and provides directions for future research to explore\nadvanced methods to automate web accessibility.\n', '  Visually impaired people are a large group who can only use braille for\nreading and writing. However, the lack of special educational resources is the\nbottleneck for educating them. Educational equity is a reflection of the level\nof social civilization, cultural equality, and individual dignity. Facilitating\nand improving lifelong learning channels for the visually impaired is of great\nsignificance. Their written braille homework or exam papers cannot be\nunderstood by sighted teachers, because of the lack of a highly accurate\nbraille translation system, especially in Chinese which has tone marks. braille\nwriters often omit tone marks to save space, leading to confusion when braille\nwith the same consonants and vowels is translated into Chinese. Previous\nalgorithms were insufficient in extracting contextual information, resulting in\nlow accuracy of braille translations into Chinese. This project informatively\nfine-tuned the mT5 model with an Encoder-decoder architecture for braille to\nChinese character conversion. This research created a training set of braille\nand corresponding Chinese text from the Leipzig Corpora. This project\nsignificantly reduced the confusion in braille, achieving $62.4$ and $62.3$\nBLEU scores in the validation and test sets, with a curriculum learning\nfine-tuning method. By incorporating the braille recognition algorithm, this\nproject is the first publicly available braille translation system and can\nbenefit lots of visually impaired students and families who are preparing for\nthe Chinese College Test and help to propel their college dreams in the future.\nThere is a demo on our homepage\\footnote{\\url{https://vision-braille.com/}}.\n']","""Accessibility for Visually Impaired Individuals"""
338,337,23,337_spectral_spectrally_graphs_networks,"['spectral', 'spectrally', 'graphs', 'networks', 'multigraph', 'multigraphs', 'graph', 'convolutions', 'filters', 'nodes']","['spectral', 'filters', 'polynomial', 'graph', 'convolution', 'filter', 'polynomials', 'basis', 'expressive', 'convolutions']","['spectral', 'networks', 'multigraphs', 'convolutions', 'filters', 'eigendecomposition', 'sgcns', 'laplacian', 'receptive', 'frequency']","['  With the recent advancements in graph neural networks (GNNs), spectral GNNs\nhave received increasing popularity by virtue of their specialty in capturing\ngraph signals in the frequency domain, demonstrating promising capability in\nspecific tasks. However, few systematic studies have been conducted on\nassessing their spectral characteristics. This emerging family of models also\nvaries in terms of designs and settings, leading to difficulties in comparing\ntheir performance and deciding on the suitable model for specific scenarios,\nespecially for large-scale tasks. In this work, we extensively benchmark\nspectral GNNs with a focus on the frequency perspective. We analyze and\ncategorize over 30 GNNs with 27 corresponding filters. Then, we implement these\nspectral models under a unified framework with dedicated graph computations and\nefficient training schemes. Thorough experiments are conducted on the spectral\nmodels with inclusive metrics on effectiveness and efficiency, offering\npractical guidelines on evaluating and selecting spectral GNNs with desirable\nperformance. Our implementation enables application on larger graphs with\ncomparable performance and less overhead, which is available at:\nhttps://github.com/gdmnl/Spectral-GNN-Benchmark.\n', '  Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded\nin the spectral domain, their practical reliance on polynomial approximation\nimplies a profound linkage to the spatial domain. As previous studies rarely\nexamine spectral GNNs from the spatial perspective, their spatial-domain\ninterpretability remains elusive, e.g., what information is essentially encoded\nby spectral GNNs in the spatial domain? In this paper, to answer this question,\nwe investigate the theoretical connection between spectral filtering and\nspatial aggregation, unveiling an intrinsic interaction that spectral filtering\nimplicitly leads the original graph to an adapted new graph, explicitly\ncomputed for spatial aggregation. Both theoretical and empirical investigations\nreveal that the adapted new graph not only exhibits non-locality but also\naccommodates signed edge weights to reflect label consistency among nodes.\nThese findings thus highlight the interpretable role of spectral GNNs in the\nspatial domain and inspire us to rethink graph spectral filters beyond the\nfixed-order polynomials, which neglect global information. Built upon the\ntheoretical findings, we revisit the state-of-the-art spectral GNNs and propose\na novel Spatially Adaptive Filtering (SAF) framework, which leverages the\nadapted new graph by spectral filtering for an auxiliary non-local aggregation.\nNotably, our SAF comprehensively models both node similarity and dissimilarity\nfrom a global perspective, therefore alleviating persistent deficiencies of\nGNNs related to long-range dependencies and graph heterophily. Extensive\nexperiments over 13 node classification benchmarks demonstrate the superiority\nof our proposed framework to the state-of-the-art methods.\n', '  Spectral Graph Neural Networks (GNNs) have achieved tremendous success in\ngraph learning. As an essential part of spectral GNNs, spectral graph\nconvolution extracts crucial frequency information in graph data, leading to\nsuperior performance of spectral GNNs in downstream tasks. However, in this\npaper, we show that existing spectral GNNs remain critical drawbacks in\nperforming the spectral graph convolution. Specifically, considering the\nspectral graph convolution as a construction operation towards target output,\nwe prove that existing popular convolution paradigms cannot construct the\ntarget output with mild conditions on input graph signals, causing spectral\nGNNs to fall into suboptimal solutions. To address the issues, we rethink the\nspectral graph convolution from a more general two-dimensional (2-D) signal\nconvolution perspective and propose a new convolution paradigm, named 2-D graph\nconvolution. We prove that 2-D graph convolution unifies existing graph\nconvolution paradigms, and is capable to construct arbitrary target output.\nBased on the proposed 2-D graph convolution, we further propose ChebNet2D, an\nefficient and effective GNN implementation of 2-D graph convolution through\napplying Chebyshev interpolation. Extensive experiments on benchmark datasets\ndemonstrate both effectiveness and efficiency of the ChebNet2D.\n']",Spectral Graph Neural Networks
339,338,23,338_twins_twinning_twin_digital,"['twins', 'twinning', 'twin', 'digital', 'cyber', 'ai', 'cybersecurity', 'network', 'replicas', 'technologies']","['twins', 'digital', 'twin', 'wireless', 'twinning', 'physical', 'maintenance', 'smart', 'technologies', 'assets']","['twins', 'digital', 'ai', 'cybersecurity', 'replicas', 'synchronized', 'interoperability', 'iot', 'avatars', 'physical']","[""  The potential of digital twin technology is yet to be fully realized due to\nits diversity and untapped potential. Digital twins enable systems' analysis,\ndesign, optimization, and evolution to be performed digitally or in conjunction\nwith a cyber-physical approach to improve speed, accuracy, and efficiency over\ntraditional engineering methods. Industry 4.0, factories of the future, and\ndigital twins continue to benefit from the technology and provide enhanced\nefficiency within existing systems. Due to the lack of information and security\nstandards associated with the transition to cyber digitization, cybercriminals\nhave been able to take advantage of the situation. Access to a digital twin of\na product or service is equivalent to threatening the entire collection. There\nis a robust interaction between digital twins and artificial intelligence\ntools, which leads to strong interaction between these technologies, so it can\nbe used to improve the cybersecurity of these digital platforms based on their\nintegration with these technologies. This study aims to investigate the role of\nartificial intelligence in providing cybersecurity for digital twin versions of\nvarious industries, as well as the risks associated with these versions. In\naddition, this research serves as a road map for researchers and others\ninterested in cybersecurity and digital security.\n"", '  Digital twin, which enables emulation, evaluation, and optimization of\nphysical entities through synchronized digital replicas, has gained increasing\nattention as a promising technology for intricate wireless networks. For 6G,\nnumerous innovative wireless technologies and network architectures have posed\nnew challenges in establishing wireless network digital twins. To tackle these\nchallenges, artificial intelligence (AI), particularly the flourishing\ngenerative AI, emerges as a potential solution. In this article, we discuss\nemerging prerequisites for wireless network digital twins considering the\ncomplicated network architecture, tremendous network scale, extensive coverage,\nand diversified application scenarios in the 6G era. We further explore the\napplications of generative AI, such as Transformer and diffusion model, to\nempower the 6G digital twin from multiple perspectives including\nphysical-digital modeling, synchronization, and slicing capability.\nSubsequently, we propose a hierarchical generative AI-enabled wireless network\ndigital twin at both the message-level and policy-level, and provide a typical\nuse case with numerical results to validate the effectiveness and efficiency.\nFinally, open research issues for wireless network digital twins in the 6G era\nare discussed.\n', '  In recent years, digital twins have been proposed and implemented in various\nfields with potential applications ranging from prototyping to maintenance.\nGoing forward, they are to enable numerous efficient and sustainable\ntechnologies, among them autonomous cars. However, despite a large body of\nresearch in many fields, academics have yet to agree on what exactly a digital\ntwin is -- and as a result, what its capabilities and limitations might be. To\nfurther our understanding, we explore the capabilities of digital twins\nconcerning diagnosis in the field of transportation. We conduct a systematic\nmapping study including digital twins of vehicles and their components, as well\nas transportation infrastructure. We discovered that few papers on digital\ntwins describe any diagnostic process. Furthermore, most existing approaches\nappear limited to system monitoring or fault detection. These findings suggest\nthat we need more research for diagnostic reasoning utilizing digital twins.\n']",Digital Twins and Cybersecurity
340,339,23,339_adversarial_adversary_attacking_attackers,"['adversarial', 'adversary', 'attacking', 'attackers', 'attacks', 'attacker', 'attack', 'adversaries', 'defenses', 'threat']","['victim', 'attack', 'attacks', 'attacker', 'adversarial', 'agent', 'agents', 'policies', 'backdoor', 'reinforcement']","['adversarial', 'adversary', 'defenses', 'reinforcement', 'agents', 'vulnerabilities', 'traitor', 'strongest', 'robustness', 'stealthier']","['  Most existing works focus on direct perturbations to the victim\'s\nstate/action or the underlying transition dynamics to demonstrate the\nvulnerability of reinforcement learning agents to adversarial attacks. However,\nsuch direct manipulations may not be always realizable. In this paper, we\nconsider a multi-agent setting where a well-trained victim agent $\\nu$ is\nexploited by an attacker controlling another agent $\\alpha$ with an\n\\textit{adversarial policy}. Previous models do not account for the possibility\nthat the attacker may only have partial control over $\\alpha$ or that the\nattack may produce easily detectable ""abnormal"" behaviors. Furthermore, there\nis a lack of provably efficient defenses against these adversarial policies. To\naddress these limitations, we introduce a generalized attack framework that has\nthe flexibility to model to what extent the adversary is able to control the\nagent, and allows the attacker to regulate the state distribution shift and\nproduce stealthier adversarial policies. Moreover, we offer a provably\nefficient defense with polynomial convergence to the most robust victim policy\nthrough adversarial training with timescale separation. This stands in sharp\ncontrast to supervised learning, where adversarial training typically provides\nonly \\textit{empirical} defenses. Using the Robosumo competition experiments,\nwe show that our generalized attack formulation results in much stealthier\nadversarial policies when maintaining the same winning rate as baselines.\nAdditionally, our adversarial training approach yields stable learning dynamics\nand less exploitable victim policies.\n', ""  This study considers the attack on reinforcement learning agents where the\nadversary aims to control the victim's behavior as specified by the adversary\nby adding adversarial modifications to the victim's state observation. While\nsome attack methods reported success in manipulating the victim agent's\nbehavior, these methods often rely on environment-specific heuristics. In\naddition, all existing attack methods require white-box access to the victim's\npolicy. In this study, we propose a novel method for manipulating the victim\nagent in the black-box (i.e., the adversary is allowed to observe the victim's\nstate and action only) and no-box (i.e., the adversary is allowed to observe\nthe victim's state only) setting without requiring environment-specific\nheuristics. Our attack method is formulated as a bi-level optimization problem\nthat is reduced to a distribution matching problem and can be solved by an\nexisting imitation learning algorithm in the black-box and no-box settings.\nEmpirical evaluations on several reinforcement learning benchmarks show that\nour proposed method has superior attack performance to baselines.\n"", ""  To ensure the usefulness of Reinforcement Learning (RL) in real systems, it\nis crucial to ensure they are robust to noise and adversarial attacks. In\nadversarial RL, an external attacker has the power to manipulate the victim\nagent's interaction with the environment. We study the full class of online\nmanipulation attacks, which include (i) state attacks, (ii) observation attacks\n(which are a generalization of perceived-state attacks), (iii) action attacks,\nand (iv) reward attacks. We show the attacker's problem of designing a stealthy\nattack that maximizes its own expected reward, which often corresponds to\nminimizing the victim's value, is captured by a Markov Decision Process (MDP)\nthat we call a meta-MDP since it is not the true environment but a higher level\nenvironment induced by the attacked interaction. We show that the attacker can\nderive optimal attacks by planning in polynomial time or learning with\npolynomial sample complexity using standard RL techniques. We argue that the\noptimal defense policy for the victim can be computed as the solution to a\nstochastic Stackelberg game, which can be further simplified into a\npartially-observable turn-based stochastic game (POTBSG). Neither the attacker\nnor the victim would benefit from deviating from their respective optimal\npolicies, thus such solutions are truly robust. Although the defense problem is\nNP-hard, we show that optimal Markovian defenses can be computed (learned) in\npolynomial time (sample complexity) in many scenarios.\n""]",Adversarial Attacks on Reinforcement Learning
341,340,23,340_contamination_language_corpora_contaminated,"['contamination', 'language', 'corpora', 'contaminated', 'evading', 'benchmark', 'models', 'benchmarks', 'data', 'detection']","['contamination', 'benchmarks', 'evaluation', 'partition', 'benchmark', 'inflated', 'quiz', 'test', 'issue', 'detection']","['contamination', 'corpora', 'evading', 'benchmarks', 'training', 'mitigation', 'leaked', 'textit', 'llmsanitize', 'dice']","['  Data contamination in model evaluation has become increasingly prevalent with\nthe growing popularity of large language models. It allows models to ""cheat""\nvia memorisation instead of displaying true capabilities. Therefore,\ncontamination analysis has become an crucial part of reliable model evaluation\nto validate results. However, existing contamination analysis is usually\nconducted internally by large language model developers and often lacks\ntransparency and completeness. This paper presents an extensive data\ncontamination report for over 15 popular large language models across six\npopular multiple-choice QA benchmarks. We also introduce an open-source\npipeline that enables the community to perform contamination analysis on\ncustomised data and models. Our experiments reveal varying contamination levels\nranging from 1\\% to 45\\% across benchmarks, with the contamination degree\nincreasing rapidly over time. Performance analysis of large language models\nindicates that data contamination does not necessarily lead to increased model\nmetrics: while significant accuracy boosts of up to 14\\% and 7\\% are observed\non contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is\nnoted on contaminated MMLU. We also find larger models seem able to gain more\nadvantages than smaller models on contaminated test sets.\n', ""  Recent statements about the impressive capabilities of large language models\n(LLMs) are usually supported by evaluating on open-access benchmarks.\nConsidering the vast size and wide-ranging sources of LLMs' training data, it\ncould explicitly or implicitly include test data, leading to LLMs being more\nsusceptible to data contamination. However, due to the opacity of training\ndata, the black-box access of models, and the rapid growth of synthetic\ntraining data, detecting and mitigating data contamination for LLMs faces\nsignificant challenges. In this paper, we propose CDD, which stands for\nContamination Detection via output Distribution for LLMs. CDD necessitates only\nthe sampled texts to detect data contamination, by identifying the peakedness\nof LLM's output distribution. To mitigate the impact of data contamination in\nevaluation, we also present TED: Trustworthy Evaluation via output\nDistribution, based on the correction of LLM's output distribution. To\nfacilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval,\nfor data contamination detection and contamination mitigation evaluation tasks.\nExtensive experimental results show that CDD achieves the average relative\nimprovements of 21.8\\%-30.2\\% over other contamination detection approaches in\nterms of Accuracy, F1 Score, and AUC metrics, and can effectively detect\nimplicit contamination. TED substantially mitigates performance improvements up\nto 66.9\\% attributed to data contamination across various contamination setups.\nIn real-world applications, we reveal that ChatGPT exhibits a high potential to\nsuffer from data contamination on HumanEval benchmark.\n"", ""  Language models pre-trained on web-scale corpora demonstrate impressive\ncapabilities on diverse downstream tasks. However, there is increasing concern\nwhether such capabilities might arise from evaluation datasets being included\nin the pre-training corpus -- a phenomenon known as \\textit{data contamination}\n-- in a manner that artificially increases performance. There has been little\nunderstanding of how this potential contamination might influence LMs'\nperformance on downstream tasks. In this paper, we explore the impact of data\ncontamination at the pre-training stage by pre-training a series of GPT-2\nmodels \\textit{from scratch}. We highlight the effect of both text\ncontamination (\\textit{i.e.}\\ input text of the evaluation samples) and\nground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and\nthe desired outputs) from evaluation data. We also investigate the effects of\nrepeating contamination for various downstream tasks. Additionally, we examine\nthe prevailing n-gram-based definitions of contamination within current LLM\nreports, pinpointing their limitations and inadequacy. Our findings offer new\ninsights into data contamination's effects on language model capabilities and\nunderscore the need for independent, comprehensive contamination assessments in\nLLM studies.\n""]",Data Contamination in Language Models
342,341,23,341_timegraphs_temporal_chronological_predicting,"['timegraphs', 'temporal', 'chronological', 'predicting', 'future', 'prediction', 'knowledge', 'relational', 'predict', 'timestamps']","['temporal', 'historical', 'reasoning', 'events', 'timestamps', 'history', 'facts', 'knowledge', 'quadruples', 'graph']","['timegraphs', 'temporal', 'relational', 'predict', 'snapshots', 'timestamp', 'historical', 'hypergraph', 'information', 'tkgrs']","['  Temporal Knowledge Graph (TKG), which characterizes temporally evolving facts\nin the form of (subject, relation, object, timestamp), has attracted much\nattention recently. TKG reasoning aims to predict future facts based on given\nhistorical ones. However, existing TKG reasoning models are unable to abstain\nfrom predictions they are uncertain, which will inevitably bring risks in\nreal-world applications. Thus, in this paper, we propose an abstention\nmechanism for TKG reasoning, which helps the existing models make selective,\ninstead of indiscriminate, predictions. Specifically, we develop a confidence\nestimator, called Confidence Estimator with History (CEHis), to enable the\nexisting TKG reasoning models to first estimate their confidence in making\npredictions, and then abstain from those with low confidence. To do so, CEHis\ntakes two kinds of information into consideration, namely, the certainty of the\ncurrent prediction and the accuracy of historical predictions. Experiments with\nrepresentative TKG reasoning models on two benchmark datasets demonstrate the\neffectiveness of the proposed CEHis.\n', '  Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based\non given histories. Most recent graph-based models excel at capturing\nstructural information within TKGs but lack semantic comprehension abilities.\nNowadays, with the surge of LLMs, the LLM-based TKG prediction model has\nemerged. However, the existing LLM-based model exhibits three shortcomings: (1)\nIt only focuses on the first-order history for prediction while ignoring\nhigh-order historical information, resulting in the provided information for\nLLMs being extremely limited. (2) LLMs struggle with optimal reasoning\nperformance under heavy historical information loads. (3) For TKG prediction,\nthe temporal reasoning capability of LLM alone is limited. To address the first\ntwo challenges, we propose Chain-of-History (CoH) reasoning which explores\nhigh-order histories step-by-step, achieving effective utilization of\nhigh-order historical information for LLMs on TKG prediction. To address the\nthird issue, we design CoH as a plug-and-play module to enhance the performance\nof graph-based models for TKG prediction. Extensive experiments on three\ndatasets and backbones demonstrate the effectiveness of CoH.\n', '  Temporal Knowledge Graph (TKG) reasoning that forecasts future events based\non historical snapshots distributed over timestamps is denoted as extrapolation\nand has gained significant attention. Owing to its extreme versatility and\nvariation in spatial and temporal correlations, TKG reasoning presents a\nchallenging task, demanding efficient capture of concurrent structures and\nevolutional interactions among facts. While existing methods have made strides\nin this direction, they still fall short of harnessing the diverse forms of\nintrinsic expressive semantics of TKGs, which encompass entity correlations\nacross multiple timestamps and periodicity of temporal information. This\nlimitation constrains their ability to thoroughly reflect historical\ndependencies and future trends. In response to these drawbacks, this paper\nproposes an innovative reasoning approach that focuses on Learning Multi-graph\nStructure (LMS). Concretely, it comprises three distinct modules concentrating\non multiple aspects of graph structure knowledge within TKGs, including\nconcurrent and evolutional patterns along timestamps, query-specific\ncorrelations across timestamps, and semantic dependencies of timestamps, which\ncapture TKG features from various perspectives. Besides, LMS incorporates an\nadaptive gate for merging entity representations both along and across\ntimestamps effectively. Moreover, it integrates timestamp semantics into graph\nattention calculations and time-aware decoders, in order to impose temporal\nconstraints on events and narrow down prediction scopes with historical\nstatistics. Extensive experimental results on five event-based benchmark\ndatasets demonstrate that LMS outperforms state-of-the-art extrapolation\nmodels, indicating the superiority of modeling a multi-graph perspective for\nTKG reasoning.\n']",Temporal Knowledge Graph Prediction
343,342,23,342_backpropagation_neural_neuron_forwardgnn,"['backpropagation', 'neural', 'neuron', 'forwardgnn', 'neurons', 'rnns', 'neuronal', 'backward', 'forward', 'backprop']","['forward', 'backpropagation', 'biological', 'plausible', 'propagation', 'backward', 'dendritic', 'algorithm', 'passes', 'cortical']","['backpropagation', 'neural', 'forwardgnn', 'rnns', 'neuromorphic', 'synapses', 'layers', 'propagation', 'descent', 'retrograde']","['  ""Forward-only"" algorithms, which train neural networks while avoiding a\nbackward pass, have recently gained attention as a way of solving the\nbiologically unrealistic aspects of backpropagation. Here, we first address\ncompelling challenges related to the ""forward-only"" rules, which include\nreducing the performance gap with backpropagation and providing an analytical\nunderstanding of their dynamics. To this end, we show that the forward-only\nalgorithm with top-down feedback is well-approximated by an\n""adaptive-feedback-alignment"" algorithm, and we analytically track its\nperformance during learning in a prototype high-dimensional setting. Then, we\ncompare different versions of forward-only algorithms, focusing on the\nForward-Forward and PEPITA frameworks, and we show that they share the same\nlearning principles. Overall, our work unveils the connections between three\nkey neuro-inspired learning rules, providing a link between ""forward-only""\nalgorithms, i.e., Forward-Forward and PEPITA, and an approximation of\nbackpropagation, i.e., Feedback Alignment.\n', '  The Backpropagation algorithm has often been criticised for its lack of\nbiological realism. In an attempt to find a more biologically plausible\nalternative, the recently introduced Forward-Forward algorithm replaces the\nforward and backward passes of Backpropagation with two forward passes. In this\nwork, we show that the internal representations obtained by the Forward-Forward\nalgorithm can organise into category-specific ensembles exhibiting high\nsparsity - composed of a low number of active units. This situation is\nreminiscent of what has been observed in cortical sensory areas, where neuronal\nensembles are suggested to serve as the functional building blocks for\nperception and action. Interestingly, while this sparse pattern does not\ntypically arise in models trained with standard Backpropagation, it can emerge\nin networks trained with Backpropagation on the same objective proposed for the\nForward-Forward algorithm. These results suggest that the learning procedure\nproposed by Forward-Forward may be superior to Backpropagation in modelling\nlearning in the cortex, even when a backward pass is used.\n', '  Graph neural networks (GNNs) have achieved remarkable success across a wide\nrange of applications, such as recommendation, drug discovery, and question\nanswering. Behind the success of GNNs lies the backpropagation (BP) algorithm,\nwhich is the de facto standard for training deep neural networks (NNs).\nHowever, despite its effectiveness, BP imposes several constraints, which are\nnot only biologically implausible, but also limit the scalability, parallelism,\nand flexibility in learning NNs. Examples of such constraints include storage\nof neural activities computed in the forward pass for use in the subsequent\nbackward pass, and the dependence of parameter updates on non-local signals. To\naddress these limitations, the forward-forward algorithm (FF) was recently\nproposed as an alternative to BP in the image classification domain, which\ntrains NNs by performing two forward passes over positive and negative data.\nInspired by this advance, we propose ForwardGNN in this work, a new forward\nlearning procedure for GNNs, which avoids the constraints imposed by BP via an\neffective layer-wise local forward training. ForwardGNN extends the original FF\nto deal with graph data and GNNs, and makes it possible to operate without\ngenerating negative inputs (hence no longer forward-forward). Further,\nForwardGNN enables each layer to learn from both the bottom-up and top-down\nsignals without relying on the backpropagation of errors. Extensive experiments\non real-world datasets show the effectiveness and generality of the proposed\nforward graph learning framework. We release our code at\nhttps://github.com/facebookresearch/forwardgnn.\n']",Alternative Learning Algorithms for Neural Networks
344,343,23,343_unlearning_unlearn_forgetting_erase,"['unlearning', 'unlearn', 'forgetting', 'erase', 'forget', 'erasure', 'remembering', 'forgotten', 'amnesiac', 'retraining']","['unlearning', 'forget', 'forgetting', 'deletion', 'retraining', 'machine', 'amnesiac', 'influence', 'removal', 'exact']","['unlearning', 'forgetting', 'erase', 'amnesiac', 'retrain', 'eliminates', 'retain', 'labelling', 'stored', 'instances']","['  To comply with AI and data regulations, the need to forget private or\ncopyrighted information from trained machine learning models is increasingly\nimportant. The key challenge in unlearning is forgetting the necessary data in\na timely manner, while preserving model performance. In this work, we address\nthe zero-shot unlearning scenario, whereby an unlearning algorithm must be able\nto remove data given only a trained model and the data to be forgotten. We\nexplore unlearning from an information theoretic perspective, connecting the\ninfluence of a sample to the information gain a model receives by observing it.\nFrom this, we derive a simple but principled zero-shot unlearning method based\non the geometry of the model. Our approach takes the form of minimising the\ngradient of a learned function with respect to a small neighbourhood around a\ntarget forget point. This induces a smoothing effect, causing forgetting by\nmoving the boundary of the classifier. We explore the intuition behind why this\napproach can jointly unlearn forget samples while preserving general model\nperformance through a series of low-dimensional experiments. We perform\nextensive empirical evaluation of our method over a range of contemporary\nbenchmarks, verifying that our method is competitive with state-of-the-art\nperformance under the strict constraints of zero-shot unlearning.\n', '  Machine unlearning is an emerging technology that has come to attract\nwidespread attention. A number of factors, including regulations and laws,\nprivacy, and usability concerns, have resulted in this need to allow a trained\nmodel to forget some of its training data. Existing studies of machine\nunlearning mainly focus on unlearning requests that forget a cluster of\ninstances or all instances from one class. While these approaches are effective\nin removing instances, they do not scale to scenarios where partial targets\nwithin an instance need to be forgotten. For example, one would like to only\nunlearn a person from all instances that simultaneously contain the person and\nother targets. Directly migrating instance-level unlearning to target-level\nunlearning will reduce the performance of the model after the unlearning\nprocess, or fail to erase information completely. To address these concerns, we\nhave proposed a more effective and efficient unlearning scheme that focuses on\nremoving partial targets from the model, which we name ""target unlearning"".\nSpecifically, we first construct an essential graph data structure to describe\nthe relationships between all important parameters that are selected based on\nthe model explanation method. After that, we simultaneously filter parameters\nthat are also important for the remaining targets and use the pruning-based\nunlearning method, which is a simple but effective solution to remove\ninformation about the target that needs to be forgotten. Experiments with\ndifferent training models on various datasets demonstrate the effectiveness of\nthe proposed approach.\n', '  In response to recent data regulation requirements, machine unlearning (MU)\nhas emerged as a critical process to remove the influence of specific examples\nfrom a given model. Although exact unlearning can be achieved through complete\nmodel retraining using the remaining dataset, the associated computational\ncosts have driven the development of efficient, approximate unlearning\ntechniques. Moving beyond data-centric MU approaches, our study introduces a\nnovel model-based perspective: model sparsification via weight pruning, which\nis capable of reducing the gap between exact unlearning and approximate\nunlearning. We show in both theory and practice that model sparsity can boost\nthe multi-criteria unlearning performance of an approximate unlearner, closing\nthe approximation gap, while continuing to be efficient. This leads to a new MU\nparadigm, termed prune first, then unlearn, which infuses a sparse model prior\ninto the unlearning process. Building on this insight, we also develop a\nsparsity-aware unlearning method that utilizes sparsity regularization to\nenhance the training process of approximate unlearning. Extensive experiments\nshow that our proposals consistently benefit MU in various unlearning\nscenarios. A notable highlight is the 77% unlearning efficacy gain of\nfine-tuning (one of the simplest unlearning methods) when using sparsity-aware\nunlearning. Furthermore, we demonstrate the practical impact of our proposed MU\nmethods in addressing other machine learning challenges, such as defending\nagainst backdoor attacks and enhancing transfer learning. Codes are available\nat https://github.com/OPTML-Group/Unlearn-Sparse.\n']",Machine Unlearning and Forgetting
345,344,23,344_tokenizers_tokenization_tokenisations_tokenizer,"['tokenizers', 'tokenization', 'tokenisations', 'tokenizer', 'sentencepiece', 'morphemes', 'subwords', 'tokens', 'wordpiece', 'token']","['tokenization', 'tokenizers', 'tokenizer', 'morphological', 'subwords', 'tokens', 'vocabulary', 'words', 'word', 'segmentation']","['tokenizers', 'sentencepiece', 'morphemes', 'subwords', 'subtokens', 'lexically', 'nlp', 'vocabularies', 'tagging', 'encoding']","['  The popular subword tokenizers of current language models, such as Byte-Pair\nEncoding (BPE), are known not to respect morpheme boundaries, which affects the\ndownstream performance of the models. While many improved tokenization\nalgorithms have been proposed, their evaluation and cross-comparison is still\nan open problem. As a solution, we propose a combined intrinsic-extrinsic\nevaluation framework for subword tokenization. Intrinsic evaluation is based on\nour new UniMorph Labeller tool that classifies subword tokenization as either\nmorphological or alien. Extrinsic evaluation, in turn, is performed via the\nOut-of-Vocabulary Generalization Challenge 1.0 benchmark, which consists of\nthree newly specified downstream text classification tasks. Our empirical\nfindings show that the accuracy of UniMorph Labeller is 98%, and that, in all\nlanguage models studied (including ALBERT, BERT, RoBERTa, and DeBERTa), alien\ntokenization leads to poorer generalizations compared to morphological\ntokenization for semantic compositionality of word meanings.\n', '  Subword tokenization has become the prevailing standard in the field of\nnatural language processing (NLP) over recent years, primarily due to the\nwidespread utilization of pre-trained language models. This shift began with\nByte-Pair Encoding (BPE) and was later followed by the adoption of\nSentencePiece and WordPiece. While subword tokenization consistently\noutperforms character and word-level tokenization, the precise factors\ncontributing to its success remain unclear. Key aspects such as the optimal\nsegmentation granularity for diverse tasks and languages, the influence of data\nsources on tokenizers, and the role of morphological information in\nIndo-European languages remain insufficiently explored. This is particularly\npertinent for biomedical terminology, characterized by specific rules governing\nmorpheme combinations. Despite the agglutinative nature of biomedical\nterminology, existing language models do not explicitly incorporate this\nknowledge, leading to inconsistent tokenization strategies for common terms. In\nthis paper, we seek to delve into the complexities of subword tokenization in\nFrench biomedical domain across a variety of NLP tasks and pinpoint areas where\nfurther enhancements can be made. We analyze classical tokenization algorithms,\nincluding BPE and SentencePiece, and introduce an original tokenization\nstrategy that integrates morpheme-enriched word segmentation into existing\ntokenization methods.\n', ""  Tokenization is a foundational step in Natural Language Processing (NLP)\ntasks, bridging raw text and language models. Existing tokenization approaches\nlike Byte-Pair Encoding (BPE) originate from the field of data compression, and\nit has been suggested that the effectiveness of BPE stems from its ability to\ncondense text into a relatively small number of tokens. We test the hypothesis\nthat fewer tokens lead to better downstream performance by introducing\nPathPiece, a new tokenizer that segments a document's text into the minimum\nnumber of tokens for a given vocabulary. Through extensive experimentation we\nfind this hypothesis not to be the case, casting doubt on the understanding of\nthe reasons for effective tokenization. To examine which other factors play a\nrole, we evaluate design decisions across all three phases of tokenization:\npre-tokenization, vocabulary construction, and segmentation, offering new\ninsights into the design of effective tokenizers. Specifically, we illustrate\nthe importance of pre-tokenization and the benefits of using BPE to initialize\nvocabulary construction. We train 64 language models with varying tokenization,\nranging in size from 350M to 2.4B parameters, all of which are made publicly\navailable.\n""]",Subword Tokenization in NLP
346,345,22,345_clicks_recommender_ctr_feature,"['clicks', 'recommender', 'ctr', 'feature', 'click', 'advertiser', 'advertisers', 'prediction', 'cvr', 'advertising']","['advertising', 'click', 'conversions', 'calibration', 'prediction', 'feature', 'auction', 'online', 'interaction', 'conversion']","['clicks', 'recommender', 'feature', 'cvr', 'embedding', 'rate', 'mdctr', 'adsnet', 'mtl', 'auction']","['  Click-through rate (CTR) prediction is widely used in academia and industry.\nMost CTR tasks fall into a feature embedding \\& feature interaction paradigm,\nwhere the accuracy of CTR prediction is mainly improved by designing practical\nfeature interaction structures. However, recent studies have argued that the\nfixed feature embedding learned only through the embedding layer limits the\nperformance of existing CTR models. Some works apply extra modules on top of\nthe embedding layer to dynamically refine feature representations in different\ninstances, making it effective and easy to integrate with existing CTR methods.\nDespite the promising results, there is a lack of a systematic review and\nsummarization of this new promising direction on the CTR task. To fill this\ngap, we comprehensively summarize and define a new module, namely\n\\textbf{feature refinement} (FR) module, that can be applied between feature\nembedding and interaction layers. We extract 14 FR modules from previous works,\nincluding instances where the FR module was proposed but not clearly defined or\nexplained. We fully assess the effectiveness and compatibility of existing FR\nmodules through comprehensive and extensive experiments with over 200 augmented\nmodels and over 4,000 runs for more than 15,000 GPU hours. The results offer\ninsightful guidelines for researchers, and all benchmarking code and\nexperimental results are open-sourced. In addition, we present a new\narchitecture of assigning independent FR modules to separate sub-networks for\nparallel CTR models, as opposed to the conventional method of inserting a\nshared FR module on top of the embedding layer. Our approach is also supported\nby comprehensive experiments demonstrating its effectiveness.\n', '  Click-Through Rate (CTR) prediction is a crucial task in online\nrecommendation platforms as it involves estimating the probability of user\nengagement with advertisements or items by clicking on them. Given the\navailability of various services like online shopping, ride-sharing, food\ndelivery, and professional services on commercial platforms, recommendation\nsystems in these platforms are required to make CTR predictions across multiple\ndomains rather than just a single domain. However, multi-domain click-through\nrate (MDCTR) prediction remains a challenging task in online recommendation due\nto the complex mutual influence between domains. Traditional MDCTR models\ntypically encode domains as discrete identifiers, ignoring rich semantic\ninformation underlying. Consequently, they can hardly generalize to new\ndomains. Besides, existing models can be easily dominated by some specific\ndomains, which results in significant performance drops in the other domains\n(i.e. the ""seesaw phenomenon""). In this paper, we propose a novel solution\nUni-CTR to address the above challenges. Uni-CTR leverages a backbone Large\nLanguage Model (LLM) to learn layer-wise semantic representations that capture\ncommonalities between domains. Uni-CTR also uses several domain-specific\nnetworks to capture the characteristics of each domain. Note that we design a\nmasked loss strategy so that these domain-specific networks are decoupled from\nbackbone LLM. This allows domain-specific networks to remain unchanged when\nincorporating new or removing domains, thereby enhancing the flexibility and\nscalability of the system significantly. Experimental results on three public\ndatasets show that Uni-CTR outperforms the state-of-the-art (SOTA) MDCTR models\nsignificantly. Furthermore, Uni-CTR demonstrates remarkable effectiveness in\nzero-shot prediction. We have applied Uni-CTR in industrial scenarios,\nconfirming its efficiency.\n', '  Click-Through Rate (CTR) prediction holds a pivotal place in online\nadvertising and recommender systems since CTR prediction performance directly\ninfluences the overall satisfaction of the users and the revenue generated by\ncompanies. Even so, CTR prediction is still an active area of research since it\ninvolves accurately modelling the preferences of users based on sparse and\nhigh-dimensional features where the higher-order interactions of multiple\nfeatures can lead to different outcomes.\n  Most CTR prediction models have relied on a single fusion and interaction\nlearning strategy. The few CTR prediction models that have utilized multiple\ninteraction modelling strategies have treated each interaction to be\nself-contained. In this paper, we propose a novel model named STEC that reaps\nthe benefits of multiple interaction learning approaches in a single unified\narchitecture. Additionally, our model introduces residual connections from\ndifferent orders of interactions which boosts the performance by allowing lower\nlevel interactions to directly affect the predictions. Through extensive\nexperiments on four real-world datasets, we demonstrate that STEC outperforms\nexisting state-of-the-art approaches for CTR prediction thanks to its greater\nexpressive capabilities.\n']",Click-Through Rate Prediction in Online Advertising and Recommendation Systems
347,346,22,346_chartinsights_chartmimic_charts_chart,"['chartinsights', 'chartmimic', 'charts', 'chart', 'chartformer', 'visual', 'chartqa', 'flowcharts', 'multimodal', 'chartpali']","['chart', 'charts', 'flowcharts', 'visual', 'question', 'understanding', 'comprehension', 'multimodal', 'elements', 'reasoning']","['chartinsights', 'flowcharts', 'multimodal', 'comprehension', 'plots', 'visualqa', 'unannotated', 'plot2code', 'flowlearn', 'mllms']","['  Charts provide visual representations of data and are widely used for\nanalyzing information, addressing queries, and conveying insights to others.\nVarious chart-related downstream tasks have emerged recently, such as\nquestion-answering and summarization. A common strategy to solve these tasks is\nto fine-tune various models originally trained on vision tasks language.\nHowever, such task-specific models are not capable of solving a wide range of\nchart-related tasks, constraining their real-world applicability. To overcome\nthese challenges, we introduce ChartInstruct: a novel chart-specific\nvision-language Instruction-following dataset comprising 191K instructions\ngenerated with 71K charts. We then present two distinct systems for instruction\ntuning on such datasets: (1) an end-to-end model that connects a vision encoder\nfor chart understanding with a LLM; and (2) a pipeline model that employs a\ntwo-step approach to extract chart data tables and input them into the LLM. In\nexperiments on four downstream tasks, we first show the effectiveness of our\nmodel--achieving a new set of state-of-the-art results. Further evaluation\nshows that our instruction-tuning approach supports a wide array of real-world\nchart comprehension and reasoning scenarios, thereby expanding the scope and\napplicability of our models to new kinds of tasks.\n', ""  Recent studies customizing Multimodal Large Language Models (MLLMs) for\ndomain-specific tasks have yielded promising results, especially in the field\nof scientific chart comprehension. These studies generally utilize visual\ninstruction tuning with specialized datasets to enhance question and answer\n(QA) accuracy within the chart domain. However, they often neglect the\nfundamental discrepancy between natural image-caption pre-training data and\ndigital chart image-QA data, particularly in the models' capacity to extract\nunderlying numeric values from charts. This paper tackles this oversight by\nexploring the training processes necessary to improve MLLMs' comprehension of\ncharts. We present three key findings: (1) Incorporating raw data values in\nalignment pre-training markedly improves comprehension of chart data. (2)\nReplacing images with their textual representation randomly during end-to-end\nfine-tuning transfer the language reasoning capability to chart interpretation\nskills. (3) Requiring the model to first extract the underlying chart data and\nthen answer the question in the fine-tuning can further improve the accuracy.\nConsequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart\ncomprehension. CHOPINLLM effectively interprets various types of charts,\nincluding unannotated ones, while maintaining robust reasoning abilities.\nFurthermore, we establish a new benchmark to evaluate MLLMs' understanding of\ndifferent chart types across various comprehension levels. Experimental results\nshow that CHOPINLLM exhibits strong performance in understanding both annotated\nand unannotated charts across a wide range of types.\n"", '  Data visualization in the form of charts plays a pivotal role in data\nanalysis, offering critical insights and aiding in informed decision-making.\nAutomatic chart understanding has witnessed significant advancements with the\nrise of large foundation models in recent years. Foundation models, such as\nlarge language models, have revolutionized various natural language processing\ntasks and are increasingly being applied to chart understanding tasks. This\nsurvey paper provides a comprehensive overview of the recent developments,\nchallenges, and future directions in chart understanding within the context of\nthese foundation models. We review fundamental building blocks crucial for\nstudying chart understanding tasks. Additionally, we explore various tasks and\ntheir evaluation metrics and sources of both charts and textual inputs. Various\nmodeling strategies are then examined, encompassing both classification-based\nand generation-based approaches, along with tool augmentation techniques that\nenhance chart understanding performance. Furthermore, we discuss the\nstate-of-the-art performance of each task and discuss how we can improve the\nperformance. Challenges and future directions are addressed, highlighting the\nimportance of several topics, such as domain-specific charts, lack of efforts\nin developing evaluation metrics, and agent-oriented settings. This survey\npaper serves as a comprehensive resource for researchers and practitioners in\nthe fields of natural language processing, computer vision, and data analysis,\nproviding valuable insights and directions for future research in chart\nunderstanding leveraging large foundation models. The studies mentioned in this\npaper, along with emerging new research, will be continually updated at:\nhttps://github.com/khuangaf/Awesome-Chart-Understanding.\n']",Chart Understanding and Analysis
348,347,22,347_reinforcement_cooperation_rewards_cooperative,"['reinforcement', 'cooperation', 'rewards', 'cooperative', 'games', 'strategies', 'agents', 'incentives', 'game', 'agent']","['cooperation', 'games', 'agents', 'game', 'opponent', 'cooperative', 'agent', 'sum', 'equilibrium', 'dilemmas']","['reinforcement', 'cooperation', 'incentives', 'reciprocity', 'evolutionary', 'multiplayer', 'dilemmas', 'equilibrium', 'prisoner', 'agentmixer']","[""  The significance of network structures in promoting group cooperation within\nsocial dilemmas has been widely recognized. Prior studies attribute this\nfacilitation to the assortment of strategies driven by spatial interactions.\nAlthough reinforcement learning has been employed to investigate the impact of\ndynamic interaction on the evolution of cooperation, there remains a lack of\nunderstanding about how agents develop neighbour selection behaviours and the\nformation of strategic assortment within an explicit interaction structure. To\naddress this, our study introduces a computational framework based on\nmulti-agent reinforcement learning in the spatial Prisoner's Dilemma game. This\nframework allows agents to select dilemma strategies and interacting neighbours\nbased on their long-term experiences, differing from existing research that\nrelies on preset social norms or external incentives. By modelling each agent\nusing two distinct Q-networks, we disentangle the coevolutionary dynamics\nbetween cooperation and interaction. The results indicate that long-term\nexperience enables agents to develop the ability to identify non-cooperative\nneighbours and exhibit a preference for interaction with cooperative ones. This\nemergent self-organizing behaviour leads to the clustering of agents with\nsimilar strategies, thereby increasing network reciprocity and enhancing group\ncooperation.\n"", '  Multi-agent reinforcement learning (MARL) methods, while effective in\nzero-sum or positive-sum games, often yield suboptimal outcomes in general-sum\ngames where cooperation is essential for achieving globally optimal outcomes.\nMatrix game social dilemmas, which abstract key aspects of general-sum\ninteractions, such as cooperation, risk, and trust, fail to model the temporal\nand spatial dynamics characteristic of real-world scenarios. In response, our\nstudy extends matrix game social dilemmas into more complex, higher-dimensional\nMARL environments. We adapt a gridworld implementation of the Stag Hunt dilemma\nto more closely match the decision-space of a one-shot matrix game while also\nintroducing variable environment complexity. Our findings indicate that as\ncomplexity increases, MARL agents trained in these environments converge to\nsuboptimal strategies, consistent with the risk-dominant Nash equilibria\nstrategies found in matrix games. Our work highlights the impact of environment\ncomplexity on achieving optimal outcomes in higher-dimensional game-theoretic\nMARL environments.\n', ""  Cooperation is fundamental in Multi-Agent Systems (MAS) and Multi-Agent\nReinforcement Learning (MARL), often requiring agents to balance individual\ngains with collective rewards. In this regard, this paper aims to investigate\nstrategies to invoke cooperation in game-theoretic scenarios, namely the\nIterated Prisoner's Dilemma, where agents must optimize both individual and\ngroup outcomes. Existing cooperative strategies are analyzed for their\neffectiveness in promoting group-oriented behavior in repeated games.\nModifications are proposed where encouraging group rewards will also result in\na higher individual gain, addressing real-world dilemmas seen in distributed\nsystems. The study extends to scenarios with exponentially growing agent\npopulations ($N \\longrightarrow +\\infty$), where traditional computation and\nequilibrium determination are challenging. Leveraging mean-field game theory,\nequilibrium solutions and reward structures are established for infinitely\nlarge agent sets in repeated games. Finally, practical insights are offered\nthrough simulations using the Multi Agent-Posthumous Credit Assignment trainer,\nand the paper explores adapting simulation algorithms to create scenarios\nfavoring cooperation for group rewards. These practical implementations bridge\ntheoretical concepts with real-world applications.\n""]",Cooperation in Multi-Agent Systems
349,348,22,348_uav_uavs_unmanned_aerial,"['uav', 'uavs', 'unmanned', 'aerial', 'transmit', 'offloading', 'swarm', 'vehicles', 'mobility', 'optimization']","['unmanned', 'aerial', 'terrestrial', 'transmission', 'transmit', 'vehicles', 'emergency', 'slots', 'energy', 'backlog']","['uav', 'offloading', 'swarm', 'mobility', 'scheduling', 'cubesat', 'qos', 'emergency', 'eavesdropping', 'deepair']","[""  In the past decade, Unmanned Aerial Vehicles (UAVs) have grabbed the\nattention of researchers in academia and industry for their potential use in\ncritical emergency applications, such as providing wireless services to ground\nusers and collecting data from areas affected by disasters, due to their\nadvantages in terms of maneuverability and movement flexibility. The UAVs'\nlimited resources, energy budget, and strict mission completion time have posed\nchallenges in adopting UAVs for these applications. Our system model considers\na UAV swarm that navigates an area collecting data from ground IoT devices\nfocusing on providing better service for strategic locations and allowing UAVs\nto join and leave the swarm (e.g., for recharging) in a dynamic way. In this\nwork, we introduce an optimization model with the aim of minimizing the total\nenergy consumption and provide the optimal path planning of UAVs under the\nconstraints of minimum completion time and transmit power. The formulated\noptimization is NP-hard making it not applicable for real-time decision making.\nTherefore, we introduce a light-weight meta-reinforcement learning solution\nthat can also cope with sudden changes in the environment through fast\nconvergence. We conduct extensive simulations and compare our approach to three\nstate-of-the-art learning models. Our simulation results prove that our\nintroduced approach is better than the three state-of-the-art algorithms in\nproviding coverage to strategic locations with fast convergence.\n"", '  In this paper, the problem of using one active unmanned aerial vehicle (UAV)\nand four passive UAVs to localize a 3D target UAV in real time is investigated.\nIn the considered model, each passive UAV receives reflection signals from the\ntarget UAV, which are initially transmitted by the active UAV. The received\nreflection signals allow each passive UAV to estimate the signal transmission\ndistance which will be transmitted to a base station (BS) for the estimation of\nthe position of the target UAV. Due to the movement of the target UAV, each\nactive/passive UAV must optimize its trajectory to continuously localize the\ntarget UAV. Meanwhile, since the accuracy of the distance estimation depends on\nthe signal-to-noise ratio of the transmission signals, the active UAV must\noptimize its transmit power. This problem is formulated as an optimization\nproblem whose goal is to jointly optimize the transmit power of the active UAV\nand trajectories of both active and passive UAVs so as to maximize the target\nUAV positioning accuracy. To solve this problem, a Z function decomposition\nbased reinforcement learning (ZD-RL) method is proposed. Compared to value\nfunction decomposition based RL (VD-RL), the proposed method can find the\nprobability distribution of the sum of future rewards to accurately estimate\nthe expected value of the sum of future rewards thus finding better transmit\npower of the active UAV and trajectories for both active and passive UAVs and\nimproving target UAV positioning accuracy. Simulation results show that the\nproposed ZD-RL method can reduce the positioning errors by up to 39.4% and\n64.6%, compared to VD-RL and independent deep RL methods, respectively.\n', '  Effective solutions for intelligent data collection in terrestrial cellular\nnetworks are crucial, especially in the context of Internet of Things\napplications. The limited spectrum and coverage area of terrestrial base\nstations pose challenges in meeting the escalating data rate demands of network\nusers. Unmanned aerial vehicles, known for their high agility, mobility, and\nflexibility, present an alternative means to offload data traffic from\nterrestrial BSs, serving as additional access points. This paper introduces a\nnovel approach to efficiently maximize the utilization of multiple UAVs for\ndata traffic offloading from terrestrial BSs. Specifically, the focus is on\nmaximizing user association with UAVs by jointly optimizing UAV trajectories\nand users association indicators under quality of service constraints. Since,\nthe formulated UAVs control problem is nonconvex and combinatorial, this study\nleverages the multi agent reinforcement learning framework. In this framework,\neach UAV acts as an independent agent, aiming to maintain inter UAV cooperative\nbehavior. The proposed approach utilizes the finite state Markov decision\nprocess to account for UAVs velocity constraints and the relationship between\ntheir trajectories and state space. A low complexity distributed state action\nreward state action algorithm is presented to determine UAVs optimal sequential\ndecision making policies over training episodes. The extensive simulation\nresults validate the proposed analysis and offer valuable insights into the\noptimal UAV trajectories. The derived trajectories demonstrate superior average\nUAV association performance compared to benchmark techniques such as Q learning\nand particle swarm optimization.\n']","""UAVs for Data Collection and Communication Optimization"""
350,349,22,349_gaussian_gps_kernels_prediction,"['gaussian', 'gps', 'kernels', 'prediction', 'gpr', 'gp', 'priors', 'covariance', 'predictors', 'lfgp']","['covariance', 'kernels', 'kernel', 'likelihood', 'stationary', 'process', 'posterior', 'processes', 'reverting', 'predictors']","['gaussian', 'gps', 'gpr', 'priors', 'lfgp', 'variational', 'likelihood', 'hyperparameter', 'relaxation', 'geospatial']","['  Gaussian processes (GPs) are the most common formalism for defining\nprobability distributions over spaces of functions. While applications of GPs\nare myriad, a comprehensive understanding of GP sample paths, i.e. the function\nspaces over which they define a probability measure, is lacking. In practice,\nGPs are not constructed through a probability measure, but instead through a\nmean function and a covariance kernel. In this paper we provide necessary and\nsufficient conditions on the covariance kernel for the sample paths of the\ncorresponding GP to attain a given regularity. We use the framework of H\\""older\nregularity as it grants particularly straightforward conditions, which simplify\nfurther in the cases of stationary and isotropic GPs. We then demonstrate that\nour results allow for novel and unusually tight characterisations of the sample\npath regularities of the GPs commonly used in machine learning applications,\nsuch as the Mat\\\'ern GPs.\n', '  Gaussian process (GP) models have received increasingly attentions in recent\nyears due to their superb prediction accuracy and modeling flexibility. To\naddress the computational burdens of GP models for large-scale datasets,\ndistributed learning for GPs are often adopted. Current aggregation models for\ndistributed GPs are not time-efficient when incorporating correlations between\nGP experts. In this work, we propose a novel approach for aggregated prediction\nin distributed GPs. The technique is suitable for both the exact and sparse\nvariational GPs. The proposed method incorporates correlations among experts,\nleading to better prediction accuracy with manageable computational\nrequirements. As demonstrated by empirical studies, the proposed approach\nresults in more stable predictions in less time than state-of-the-art\nconsistent aggregation models.\n', '  We present a new strategy for learning the functional relation between a pair\nof variables, while addressing inhomogeneities in the correlation structure of\nthe available data, by modelling the sought function as a sample function of a\nnon-stationary Gaussian Process (GP), that nests within itself multiple other\nGPs, each of which we prove can be stationary, thereby establishing sufficiency\nof two GP layers. In fact, a non-stationary kernel is envisaged, with each\nhyperparameter set as dependent on the sample function drawn from the outer\nnon-stationary GP, such that a new sample function is drawn at every pair of\ninput values at which the kernel is computed. However, such a model cannot be\nimplemented, and we substitute this by recalling that the average effect of\ndrawing different sample functions from a given GP is equivalent to that of\ndrawing a sample function from each of a set of GPs that are rendered\ndifferent, as updated during the equilibrium stage of the undertaken inference\n(via MCMC). The kernel is fully non-parametric, and it suffices to learn one\nhyperparameter per layer of GP, for each dimension of the input variable. We\nillustrate this new learning strategy on a real dataset.\n']",Gaussian Processes for Prediction and Modeling
351,350,22,350_politically_partisan_political_biases,"['politically', 'partisan', 'political', 'biases', 'elections', 'politicians', 'speeches', 'electoral', 'constituency', 'ideological']","['political', 'leaning', 'democratic', 'parties', 'ideological', 'left', 'partisan', 'constituency', 'elections', 'attitude']","['partisan', 'biases', 'speeches', 'voter', 'constituencies', 'surveys', 'attitudes', 'ideologies', 'german', 'manifestos']","[""  Instruction-finetuned Large Language Models inherit clear political leanings\nthat have been shown to influence downstream task performance. We expand this\nline of research beyond the two-party system in the US and audit Llama Chat in\nthe context of EU politics in various settings to analyze the model's political\nknowledge and its ability to reason in context. We adapt, i.e., further\nfine-tune, Llama Chat on speeches of individual euro-parties from debates in\nthe European Parliament to reevaluate its political leaning based on the EUandI\nquestionnaire. Llama Chat shows considerable knowledge of national parties'\npositions and is capable of reasoning in context. The adapted, party-specific,\nmodels are substantially re-aligned towards respective positions which we see\nas a starting point for using chat-based LLMs as data-driven conversational\nengines to assist research in political science.\n"", '  The assessment of bias within Large Language Models (LLMs) has emerged as a\ncritical concern in the contemporary discourse surrounding Artificial\nIntelligence (AI) in the context of their potential impact on societal\ndynamics. Recognizing and considering political bias within LLM applications is\nespecially important when closing in on the tipping point toward performative\nprediction. Then, being educated about potential effects and the societal\nbehavior LLMs can drive at scale due to their interplay with human operators.\nIn this way, the upcoming elections of the European Parliament will not remain\nunaffected by LLMs. We evaluate the political bias of the currently most\npopular open-source LLMs (instruct or assistant models) concerning political\nissues within the European Union (EU) from a German voter\'s perspective. To do\nso, we use the ""Wahl-O-Mat,"" a voting advice application used in Germany. From\nthe voting advice of the ""Wahl-O-Mat"" we quantize the degree of alignment of\nLLMs with German political parties. We show that larger models, such as\nLlama3-70B, tend to align more closely with left-leaning political parties,\nwhile smaller models often remain neutral, particularly when prompted in\nEnglish. The central finding is that LLMs are similarly biased, with low\nvariances in the alignment concerning a specific party. Our findings underline\nthe importance of rigorously assessing and making bias transparent in LLMs to\nsafeguard the integrity and trustworthiness of applications that employ the\ncapabilities of performative prediction and the invisible hand of machine\nlearning prediction and language generation.\n', ""  I report here a comprehensive analysis about the political preferences\nembedded in Large Language Models (LLMs). Namely, I administer 11 political\norientation tests, designed to identify the political preferences of the test\ntaker, to 24 state-of-the-art conversational LLMs, both closed and open source.\nWhen probed with questions/statements with political connotations, most\nconversational LLMs tend to generate responses that are diagnosed by most\npolitical test instruments as manifesting preferences for left-of-center\nviewpoints. This does not appear to be the case for five additional base (i.e.\nfoundation) models upon which LLMs optimized for conversation with humans are\nbuilt. However, the weak performance of the base models at coherently answering\nthe tests' questions makes this subset of results inconclusive. Finally, I\ndemonstrate that LLMs can be steered towards specific locations in the\npolitical spectrum through Supervised Fine-Tuning (SFT) with only modest\namounts of politically aligned data, suggesting SFT's potential to embed\npolitical orientation in LLMs. With LLMs beginning to partially displace\ntraditional information sources like search engines and Wikipedia, the societal\nimplications of political biases embedded in LLMs are substantial.\n""]","""Language Models and Political Bias"""
352,351,22,351_radar_radars_targets_sar,"['radar', 'radars', 'targets', 'sar', 'doppler', 'range', 'target', 'transmit', 'echoes', 'sensing']","['radar', 'radars', 'waveforms', 'targets', 'clutter', 'resolution', 'range', 'subwavelength', 'band', 'sensing']","['radar', 'targets', 'doppler', 'drones', 'reflectors', 'estimation', 'cfar', 'beamforming', 'vehicular', 'mimo']","['  Radar Automated Target Recognition (RATR) for Unmanned Aerial Vehicles (UAVs)\ninvolves transmitting Electromagnetic Waves (EMWs) and performing target type\nrecognition on the received radar echo, crucial for defense and aerospace\napplications. Previous studies highlighted the advantages of multistatic radar\nconfigurations over monostatic ones in RATR. However, fusion methods in\nmultistatic radar configurations often suboptimally combine classification\nvectors from individual radars probabilistically. To address this, we propose a\nfully Bayesian RATR framework employing Optimal Bayesian Fusion (OBF) to\naggregate classification probability vectors from multiple radars. OBF, based\non expected 0-1 loss, updates a Recursive Bayesian Classification (RBC)\nposterior distribution for target UAV type, conditioned on historical\nobservations across multiple time steps. We evaluate the approach using\nsimulated random walk trajectories for seven drones, correlating target aspect\nangles to Radar Cross Section (RCS) measurements in an anechoic chamber.\nComparing against single radar Automated Target Recognition (ATR) systems and\nsuboptimal fusion methods, our empirical results demonstrate that the OBF\nmethod integrated with RBC significantly enhances classification accuracy\ncompared to other fusion methods and single radar configurations.\n', '  Simulation is an invaluable tool for radio-frequency system designers that\nenables rapid prototyping of various algorithms for imaging, target detection,\nclassification, and tracking. However, simulating realistic radar scans is a\nchallenging task that requires an accurate model of the scene, radio frequency\nmaterial properties, and a corresponding radar synthesis function. Rather than\nspecifying these models explicitly, we propose DART - Doppler Aided Radar\nTomography, a Neural Radiance Field-inspired method which uses radar-specific\nphysics to create a reflectance and transmittance-based rendering pipeline for\nrange-Doppler images. We then evaluate DART by constructing a custom data\ncollection platform and collecting a novel radar dataset together with accurate\nposition and instantaneous velocity measurements from lidar-based localization.\nIn comparison to state-of-the-art baselines, DART synthesizes superior radar\nrange-Doppler images from novel views across all datasets and additionally can\nbe used to generate high quality tomographic images.\n', '  Millimeter-wave (mmWave) radars are indispensable for perception tasks of\nautonomous vehicles, thanks to their resilience in challenging weather\nconditions. Yet, their deployment is often limited by insufficient spatial\nresolution for precise semantic scene interpretation. Classical\nsuper-resolution techniques adapted from optical imaging inadequately address\nthe distinct characteristics of radar signal data. In response, our study\nredefines radar imaging super-resolution as a one-dimensional (1D) signal\nsuper-resolution spectra estimation problem by harnessing the radar signal\nprocessing domain knowledge, introducing innovative data normalization and a\ndomain-informed signal-to-noise ratio (SNR)-guided loss function. Our tailored\ndeep learning network for automotive radar imaging exhibits remarkable\nscalability, parameter efficiency and fast inference speed, alongside enhanced\nperformance in terms of radar imaging quality and resolution. Extensive testing\nconfirms that our SR-SPECNet sets a new benchmark in producing high-resolution\nradar range-azimuth images, outperforming existing methods across varied\nantenna configurations and dataset sizes. Source code and new radar dataset\nwill be made publicly available online.\n']",Radar Technology and Target Recognition
353,352,22,352_dehazing_haze_cnn_colorization,"['dehazing', 'haze', 'cnn', 'colorization', 'blur', 'photography', 'brightness', 'hazy', 'illumination', 'dark']","['dehazing', 'color', 'dark', 'light', 'images', 'enhancement', 'haze', 'image', 'camera', 'brightness']","['haze', 'colorization', 'blur', 'brightness', 'convnext', 'reflectance', 'priornet', 'pixelgen', 'restoration', 'dffn']","['  In recent years, as computer vision tasks have increasingly relied on\nhigh-quality image inputs, the task of image dehazing has received significant\nattention. Previously, many methods based on priors and deep learning have been\nproposed to address the task of image dehazing. Ignoring the domain gap between\ndifferent data, former de-hazing methods usually adopt multiple datasets for\nexplicit training, which often makes the methods themselves be violated. To\naddress this problem, we propose a novel method of internal and external data\naugmentation to improve the existing dehazing methodology. By using cross-data\nexternal augmentor. The dataset inherits samples from different domains that\nare firmly aligned, making the model learn more robust and generalizable\nfeatures. By using the internal data augmentation method, the model can fully\nexploit local information within the images, thereby obtaining more image\ndetails. To demonstrate the effectiveness of our proposed method, we conduct\ntraining on both the Natural Image Dataset (NID) and the Remote Sensing Image\nDataset (RSID). Experimental results show that our method clearly resolves the\ndomain gap in different dehazing datasets and presents a new pipeline for joint\ntraining in the dehazing task. Our approach significantly outperforms other\nadvanced methods in dehazing and produces dehazed images that are closest to\nreal haze-free images. The code will be available at:\nhttps://github.com/wengzp1/ScaleUpDehazing\n', '  Image dehazing has been a popular topic of research for a long time. Previous\ndeep learning-based image dehazing methods have failed to achieve satisfactory\ndehazing effects on both synthetic datasets and real-world datasets, exhibiting\npoor generalization. Moreover, single-stage networks often result in many\nregions with artifacts and color distortion in output images. To address these\nissues, this paper proposes a two-stage image dehazing network called TSNet,\nmainly consisting of the multi-scale fusion module (MSFM) and the adaptive\nlearning module (ALM). Specifically, MSFM and ALM enhance the generalization of\nTSNet. The MSFM can obtain large receptive fields at multiple scales and\nintegrate features at different frequencies to reduce the differences between\ninputs and learning objectives. The ALM can actively learn of regions of\ninterest in images and restore texture details more effectively. Additionally,\nTSNet is designed as a two-stage network, where the first-stage network\nperforms image dehazing, and the second-stage network is employed to improve\nissues such as artifacts and color distortion present in the results of the\nfirst-stage network. We also change the learning objective from ground truth\nimages to opposite fog maps, which improves the learning efficiency of TSNet.\nExtensive experiments demonstrate that TSNet exhibits superior dehazing\nperformance on both synthetic and real-world datasets compared to previous\nstate-of-the-art methods.\n', ""  Hazy images degrade visual quality, and dehazing is a crucial prerequisite\nfor subsequent processing tasks. Most current dehazing methods rely on neural\nnetworks and face challenges such as high computational parameter pressure and\nweak generalization capabilities. This paper introduces PriorNet--a novel,\nlightweight, and highly applicable dehazing network designed to significantly\nimprove the clarity and visual quality of hazy images while avoiding excessive\ndetail extraction issues. The core of PriorNet is the original\nMulti-Dimensional Interactive Attention (MIA) mechanism, which effectively\ncaptures a wide range of haze characteristics, substantially reducing the\ncomputational load and generalization difficulties associated with complex\nsystems. By utilizing a uniform convolutional kernel size and incorporating\nskip connections, we have streamlined the feature extraction process.\nSimplifying the number of layers and architecture not only enhances dehazing\nefficiency but also facilitates easier deployment on edge devices. Extensive\ntesting across multiple datasets has demonstrated PriorNet's exceptional\nperformance in dehazing and clarity restoration, maintaining image detail and\ncolor fidelity in single-image dehazing tasks. Notably, with a model size of\njust 18Kb, PriorNet showcases superior dehazing generalization capabilities\ncompared to other methods. Our research makes a significant contribution to\nadvancing image dehazing technology, providing new perspectives and tools for\nthe field and related domains, particularly emphasizing the importance of\nimproving universality and deployability.\n""]",Image Dehazing Techniques
354,353,22,353_graphinstruct_instructgraph_graphreader_graphwiz,"['graphinstruct', 'instructgraph', 'graphreader', 'graphwiz', 'graphlm', 'graphtoken', 'graphs', 'grapheval2000', 'nlgraph', 'graphqa']","['graph', 'reasoning', 'graphs', 'structured', 'pure', 'abilities', 'prompting', 'tasks', 'instruction', 'hop']","['instructgraph', 'graphreader', 'graphwiz', 'graphtoken', 'chatgraph', 'benchmarkms', 'tasks', 'traversal', 'node', 'knowreason']","[""  Large language models (LLMs) have achieved impressive success across several\nfields, but their proficiency in understanding and resolving complex graph\nproblems is less explored. To bridge this gap, we introduce GraphInstruct, a\nnovel and comprehensive instruction-tuning dataset designed to equip language\nmodels with the ability to tackle a broad spectrum of graph problems using\nexplicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an\nopen-source language model capable of resolving various graph problem types\nwhile generating clear reasoning processes. To enhance the model's capability\nand reliability, we incorporate the Direct Preference Optimization (DPO)\nframework into the graph problem-solving context. The enhanced model,\nGraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with\ndifferent complexity levels, surpassing GPT-4 which has an average accuracy of\n43.8%. Moreover, our research delves into the delicate balance between training\ndata volume and model performance, highlighting the potential for overfitting\nwith increased data. We also explore the transferability of the model's\nreasoning ability across different graph tasks, indicating the model's\nadaptability and practical application potential. Our investigation offers a\nnew blueprint and valuable insights for developing LLMs specialized in graph\nreasoning and problem-solving.\n"", '  Evaluating and enhancing the general capabilities of large language models\n(LLMs) has been an important research topic. Graph is a common data structure\nin the real world, and understanding graph data is a crucial part for advancing\ngeneral intelligence. To evaluate and enhance the graph understanding abilities\nof LLMs, in this paper, we propose a benchmark named GraphInstruct, which\ncomprehensively includes 21 classical graph reasoning tasks, providing diverse\ngraph generation pipelines and detailed reasoning steps. Based on\nGraphInstruct, we further construct GraphLM through efficient\ninstruction-tuning, which shows prominent graph understanding capability. In\norder to enhance the LLM with graph reasoning capability as well, we propose a\nstep mask training strategy, and construct a model named GraphLM+. As one of\nthe pioneering efforts to enhance the graph understanding and reasoning\nabilities of LLMs, extensive experiments have demonstrated the superiority of\nGraphLM and GraphLM+ over other LLMs. We look forward to more researchers\nexploring the potential of LLMs in the graph data mining domain through\nGraphInstruct. Our code for generating GraphInstruct is released publicly at:\nhttps://github.com/CGCL-codes/GraphInstruct.\n', '  Large language models (LLMs) are increasingly adopted for a variety of tasks\nwith implicit graphical structures, such as planning in robotics, multi-hop\nquestion answering or knowledge probing, structured commonsense reasoning, and\nmore. While LLMs have advanced the state-of-the-art on these tasks with\nstructure implications, whether LLMs could explicitly process textual\ndescriptions of graphs and structures, map them to grounded conceptual spaces,\nand perform structured operations remains underexplored. To this end, we\npropose NLGraph (Natural Language Graph), a comprehensive benchmark of\ngraph-based problem solving designed in natural language. NLGraph contains\n29,370 problems, covering eight graph reasoning tasks with varying complexity\nfrom simple tasks such as connectivity and shortest path up to complex problems\nsuch as maximum flow and simulating graph neural networks. We evaluate LLMs\n(GPT-3/4) with various prompting approaches on the NLGraph benchmark and find\nthat 1) language models do demonstrate preliminary graph reasoning abilities,\n2) the benefit of advanced prompting and in-context learning diminishes on more\ncomplex graph problems, while 3) LLMs are also (un)surprisingly brittle in the\nface of spurious correlations in graph and problem settings. We then propose\nBuild-a-Graph Prompting and Algorithmic Prompting, two instruction-based\napproaches to enhance LLMs in solving natural language graph problems.\nBuild-a-Graph and Algorithmic prompting improve the performance of LLMs on\nNLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to\nsolve the most complicated graph reasoning tasks in our setup with language\nmodels remains an open research question. The NLGraph benchmark and evaluation\ncode are available at https://github.com/Arthur-Heng/NLGraph.\n']",Graph Reasoning and Problem-Solving with Large Language Models
355,354,22,354_automl_automate_automated_automating,"['automl', 'automate', 'automated', 'automating', 'automation', 'autorecsys', 'autommlab', 'automatically', 'autorul', 'expertise']","['pipelines', 'pipeline', 'engineering', 'scientists', 'automation', 'machine', 'tools', 'expertise', 'augmentation', 'data']","['automl', 'automate', 'autorecsys', 'autommlab', 'expertise', 'workflow', 'hyperparameters', 'pipelines', 'libraries', 'mlops']","[""  Automated machine learning (AutoML) was formed around the fundamental\nobjectives of automatically and efficiently configuring machine learning (ML)\nworkflows, aiding the research of new ML algorithms, and contributing to the\ndemocratization of ML by making it accessible to a broader audience. Over the\npast decade, commendable achievements in AutoML have primarily focused on\noptimizing predictive performance. This focused progress, while substantial,\nraises questions about how well AutoML has met its broader, original goals. In\nthis position paper, we argue that a key to unlocking AutoML's full potential\nlies in addressing the currently underexplored aspect of user interaction with\nAutoML systems, including their diverse roles, expectations, and expertise. We\nenvision a more human-centered approach in future AutoML research, promoting\nthe collaborative design of ML systems that tightly integrates the\ncomplementary strengths of human expertise and AutoML methodologies.\n"", '  Automated machine learning (AutoML) is envisioned to make ML techniques\naccessible to ordinary users. Recent work has investigated the role of humans\nin enhancing AutoML functionality throughout a standard ML workflow. However,\nit is also critical to understand how users adopt existing AutoML solutions in\ncomplex, real-world settings from a holistic perspective. To fill this gap,\nthis study conducted semi-structured interviews of AutoML users (N=19) focusing\non understanding (1) the limitations of AutoML encountered by users in their\nreal-world practices, (2) the strategies users adopt to cope with such\nlimitations, and (3) how the limitations and workarounds impact their use of\nAutoML. Our findings reveal that users actively exercise user agency to\novercome three major challenges arising from customizability, transparency, and\nprivacy. Furthermore, users make cautious decisions about whether and how to\napply AutoML on a case-by-case basis. Finally, we derive design implications\nfor developing future AutoML solutions.\n', '  Background. Due to the widespread adoption of Artificial Intelligence (AI)\nand Machine Learning (ML) for building software applications, companies are\nstruggling to recruit employees with a deep understanding of such technologies.\nIn this scenario, AutoML is soaring as a promising solution to fill the AI/ML\nskills gap since it promises to automate the building of end-to-end AI/ML\npipelines that would normally be engineered by specialized team members. Aims.\nDespite the growing interest and high expectations, there is a dearth of\ninformation about the extent to which AutoML is currently adopted by teams\ndeveloping AI/ML-enabled systems and how it is perceived by practitioners and\nresearchers. Method. To fill these gaps, in this paper, we present a\nmixed-method study comprising a benchmark of 12 end-to-end AutoML tools on two\nSE datasets and a user survey with follow-up interviews to further our\nunderstanding of AutoML adoption and perception. Results. We found that AutoML\nsolutions can generate models that outperform those trained and optimized by\nresearchers to perform classification tasks in the SE domain. Also, our\nfindings show that the currently available AutoML solutions do not live up to\ntheir names as they do not equally support automation across the stages of the\nML development workflow and for all the team members. Conclusions. We derive\ninsights to inform the SE research community on how AutoML can facilitate their\nactivities and tool builders on how to design the next generation of AutoML\ntechnologies.\n']",Automated Machine Learning (AutoML) Research and Applications
356,355,22,355_autoencoders_autoencoder_masking_imagenet,"['autoencoders', 'autoencoder', 'masking', 'imagenet', 'mask', 'masked', 'supervised', 'training', 'learned', 'vision']","['masking', 'reconstruction', 'patches', 'curriculum', 'supervised', 'autoencoders', 'self', 'pretext', 'downstream', 'modeling']","['autoencoders', 'imagenet', 'vision', 'learn', 'masksub', 'augmentations', 'modeling', 'mae', 'labelling', 'curriculum']","['  Masked Autoencoder (MAE) has demonstrated superior performance on various\nvision tasks via randomly masking image patches and reconstruction. However,\neffective data augmentation strategies for MAE still remain open questions,\ndifferent from those in contrastive learning that serve as the most important\npart. This paper studies the prevailing mixing augmentation for MAE. We first\ndemonstrate that naive mixing will in contrast degenerate model performance due\nto the increase of mutual information (MI). To address, we propose homologous\nrecognition, an auxiliary pretext task, not only to alleviate the MI\nincreasement by explicitly requiring each patch to recognize homologous\npatches, but also to perform object-aware self-supervised pre-training for\nbetter downstream dense perception performance. With extensive experiments, we\ndemonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the\nstate-of-the-art transfer results among masked image modeling (MIM)\naugmentations on different downstream tasks with significant efficiency.\nSpecifically, our MixedAE outperforms MAE by +0.3% accuracy, +1.7 mIoU and +0.9\nAP on ImageNet-1K, ADE20K and COCO respectively with a standard ViT-Base.\nMoreover, MixedAE surpasses iBOT, a strong MIM method combined with instance\ndiscrimination, while accelerating training by 2x. To our best knowledge, this\nis the very first work to consider mixing for MIM from the perspective of\npretext task design. Code will be made available.\n', '  Masked image modeling (MIM) has been recognized as a strong self-supervised\npre-training approach in the vision domain. However, the mechanism and\nproperties of the learned representations by such a scheme, as well as how to\nfurther enhance the representations are so far not well-explored. In this\npaper, we aim to explore an interactive Masked Autoencoders (i-MAE) framework\nto enhance the representation capability from two aspects: (1) employing a\ntwo-way image reconstruction and a latent feature reconstruction with\ndistillation loss to learn better features; (2) proposing a semantics-enhanced\nsampling strategy to boost the learned semantics in MAE. Upon the proposed\ni-MAE architecture, we can address two critical questions to explore the\nbehaviors of the learned representations in MAE: (1) Whether the separability\nof latent representations in Masked Autoencoders is helpful for model\nperformance? We study it by forcing the input as a mixture of two images\ninstead of one. (2) Whether we can enhance the representations in the latent\nfeature space by controlling the degree of semantics during sampling on Masked\nAutoencoders? To this end, we propose a sampling strategy within a mini-batch\nbased on the semantics of training samples to examine this aspect. Extensive\nexperiments are conducted on CIFAR-10/100, Tiny-ImageNet and ImageNet-1K to\nverify the observations we discovered. Furthermore, in addition to\nqualitatively analyzing the characteristics of the latent representations, we\nexamine the existence of linear separability and the degree of semantics in the\nlatent space by proposing two evaluation schemes. The surprising and consistent\nresults demonstrate that i-MAE is a superior framework design for understanding\nMAE frameworks, as well as achieving better representational ability. Code is\navailable at https://github.com/vision-learning-acceleration-lab/i-mae.\n', '  Masked image modeling has been demonstrated as a powerful pretext task for\ngenerating robust representations that can be effectively generalized across\nmultiple downstream tasks. Typically, this approach involves randomly masking\npatches (tokens) in input images, with the masking strategy remaining unchanged\nduring training. In this paper, we propose a curriculum learning approach that\nupdates the masking strategy to continually increase the complexity of the\nself-supervised reconstruction task. We conjecture that, by gradually\nincreasing the task complexity, the model can learn more sophisticated and\ntransferable representations. To facilitate this, we introduce a novel\nlearnable masking module that possesses the capability to generate masks of\ndifferent complexities, and integrate the proposed module into masked\nautoencoders (MAE). Our module is jointly trained with the MAE, while adjusting\nits behavior during training, transitioning from a partner to the MAE\n(optimizing the same reconstruction loss) to an adversary (optimizing the\nopposite loss), while passing through a neutral state. The transition between\nthese behaviors is smooth, being regulated by a factor that is multiplied with\nthe reconstruction loss of the masking module. The resulting training procedure\ngenerates an easy-to-hard curriculum. We train our Curriculum-Learned Masked\nAutoencoder (CL-MAE) on ImageNet and show that it exhibits superior\nrepresentation learning capabilities compared to MAE. The empirical results on\nfive downstream tasks confirm our conjecture, demonstrating that curriculum\nlearning can be successfully used to self-supervise masked autoencoders. We\nrelease our code at https://github.com/ristea/cl-mae.\n']",Masked Autoencoders for Image Representation Learning
357,356,21,356_humordb_funnynet_humor_humour,"['humordb', 'funnynet', 'humor', 'humour', 'humorous', 'jokes', 'subtitles', 'laugh', 'funnier', 'laughter']","['humor', 'funny', 'humour', 'humorous', 'jokes', 'comic', 'moments', 'mischief', 'laugh', 'video']","['humordb', 'funnynet', 'subtitles', 'puns', 'smile', 'embeddings', 'content', 'rating', 'colbert', 'sitcoms']","['  In this paper, we explore the generation of one-liner jokes through\nmulti-step reasoning. Our work involved reconstructing the process behind\ncreating humorous one-liners and developing a working prototype for humor\ngeneration. We conducted comprehensive experiments with human participants to\nevaluate our approach, comparing it with human-created jokes, zero-shot GPT-4\ngenerated humor, and other baselines. The evaluation focused on the quality of\nhumor produced, using human labeling as a benchmark. Our findings demonstrate\nthat the multi-step reasoning approach consistently improves the quality of\ngenerated humor. We present the results and share the datasets used in our\nexperiments, offering insights into enhancing humor generation with artificial\nintelligence.\n', '  Humor understanding is an important and challenging research in natural\nlanguage processing. As the popularity of pre-trained language models (PLMs),\nsome recent work makes preliminary attempts to adopt PLMs for humor recognition\nand generation. However, these simple attempts do not substantially answer the\nquestion: {\\em whether PLMs are capable of humor understanding?} This paper is\nthe first work that systematically investigates the humor understanding ability\nof PLMs. For this purpose, a comprehensive framework with three evaluation\nsteps and four evaluation tasks is designed. We also construct a comprehensive\nChinese humor dataset, which can fully meet all the data requirements of the\nproposed evaluation framework. Our empirical study on the Chinese humor dataset\nyields some valuable observations, which are of great guiding value for future\noptimization of PLMs in humor understanding and generation.\n', ""  Humor is a fundamental facet of human cognition and interaction. Yet, despite\nrecent advances in natural language processing, humor detection remains a\nchallenging task that is complicated by the scarcity of datasets that pair\nhumorous texts with similar non-humorous counterparts. In our work, we\ninvestigate whether large language models (LLMs), can generate synthetic data\nfor humor detection via editing texts. We benchmark LLMs on an existing human\ndataset and show that current LLMs display an impressive ability to 'unfun'\njokes, as judged by humans and as measured on the downstream task of humor\ndetection. We extend our approach to a code-mixed English-Hindi humor dataset,\nwhere we find that GPT-4's synthetic data is highly rated by bilingual\nannotators and provides challenging adversarial examples for humor classifiers.\n""]",Humor Generation and Understanding
358,357,21,357_augmentation_augmenting_augmentations_auggpt,"['augmentation', 'augmenting', 'augmentations', 'auggpt', 'improving', 'improvement', 'augmented', 'improve', 'text', 'autoaugment']","['augmentation', 'augmented', 'augmentations', 'text', 'protein', 'curriculum', 'data', 'cutout', 'natural', 'substitution']","['augmentation', 'improving', 'autoaugment', 'nlp', 'amplify', 'unification', 'labels', 'texts', 'curriculum', 'protein']","['  Data augmentation is one of the regularization strategies for the training of\ndeep learning models, which enhances generalizability and prevents overfitting,\nleading to performance improvement. Although researchers have proposed various\ndata augmentation techniques, they often lack consideration for the difficulty\nof augmented data. Recently, another line of research suggests incorporating\nthe concept of curriculum learning with data augmentation in the field of\nnatural language processing. In this study, we adopt curriculum data\naugmentation for image data augmentation and propose colorful cutout, which\ngradually increases the noise and difficulty introduced in the augmented image.\nOur experimental results highlight the possibility of curriculum data\naugmentation for image data. We publicly released our source code to improve\nthe reproducibility of our study.\n', '  Large models, encompassing large language and diffusion models, have shown\nexceptional promise in approximating human-level intelligence, garnering\nsignificant interest from both academic and industrial spheres. However, the\ntraining of these large models necessitates vast quantities of high-quality\ndata, and with continuous updates to these models, the existing reservoir of\nhigh-quality data may soon be depleted. This challenge has catalyzed a surge in\nresearch focused on data augmentation methods. Leveraging large models, these\ndata augmentation techniques have outperformed traditional approaches. This\npaper offers an exhaustive review of large model-driven data augmentation\nmethods, adopting a comprehensive perspective. We begin by establishing a\nclassification of relevant studies into three main categories: image\naugmentation, text augmentation, and paired data augmentation. Following this,\nwe delve into various data post-processing techniques pertinent to large\nmodel-based data augmentation. Our discussion then expands to encompass the\narray of applications for these data augmentation methods within natural\nlanguage processing, computer vision, and audio signal processing. We proceed\nto evaluate the successes and limitations of large model-based data\naugmentation across different scenarios. Concluding our review, we highlight\nprospective challenges and avenues for future exploration in the field of data\naugmentation. Our objective is to furnish researchers with critical insights,\nultimately contributing to the advancement of more sophisticated large models.\nWe consistently maintain the related open-source materials at:\nhttps://github.com/MLGroup-JLU/LLM-data-aug-survey.\n', '  Augmentation is an effective alternative to utilize the small amount of\nlabeled protein data. However, most of the existing work focuses on design-ing\nnew architectures or pre-training tasks, and relatively little work has studied\ndata augmentation for proteins. This paper extends data augmentation techniques\npreviously used for images and texts to proteins and then benchmarks these\ntechniques on a variety of protein-related tasks, providing the first\ncomprehensive evaluation of protein augmentation. Furthermore, we propose two\nnovel semantic-level protein augmentation methods, namely Integrated Gradients\nSubstitution and Back Translation Substitution, which enable protein\nsemantic-aware augmentation through saliency detection and biological\nknowledge. Finally, we integrate extended and proposed augmentations into an\naugmentation pool and propose a simple but effective framework, namely\nAutomated Protein Augmentation (APA), which can adaptively select the most\nsuitable augmentation combinations for different tasks. Extensive experiments\nhave shown that APA enhances the performance of five protein related tasks by\nan average of 10.55% across three architectures compared to vanilla\nimplementations without augmentation, highlighting its potential to make a\ngreat impact on the field.\n']",Data Augmentation Techniques
359,358,21,358_explanations_counterfactuals_reasonability_implicatures,"['explanations', 'counterfactuals', 'reasonability', 'implicatures', 'explaining', 'counterfactual', 'language', 'predictions', 'attribution', 'implicature']","['explanations', 'faithfulness', 'plausibility', 'explanation', 'rationales', 'rationale', 'implicatures', 'maxims', 'self', 'faithful']","['explanations', 'counterfactuals', 'reasonability', 'implicatures', 'attribution', 'nle', 'redaction', 'faithfulness', 'appropriateness', 'eval']","[""  Large Language Models (LLMs) have become proficient in addressing complex\ntasks by leveraging their extensive internal knowledge and reasoning\ncapabilities. However, the black-box nature of these models complicates the\ntask of explaining their decision-making processes. While recent advancements\ndemonstrate the potential of leveraging LLMs to self-explain their predictions\nthrough natural language (NL) explanations, their explanations may not\naccurately reflect the LLMs' decision-making process due to a lack of fidelity\noptimization on the derived explanations. Measuring the fidelity of NL\nexplanations is a challenging issue, as it is difficult to manipulate the input\ncontext to mask the semantics of these explanations. To this end, we introduce\nFaithLM to explain the decision of LLMs with NL explanations. Specifically,\nFaithLM designs a method for evaluating the fidelity of NL explanations by\nincorporating the contrary explanations to the query process. Moreover, FaithLM\nconducts an iterative process to improve the fidelity of derived explanations.\nExperiment results on three datasets from multiple domains demonstrate that\nFaithLM can significantly improve the fidelity of derived explanations, which\nalso provides a better alignment with the ground-truth explanations.\n"", ""  Instruction-tuned Large Language Models (LLMs) excel at many tasks and will\neven explain their reasoning, so-called self-explanations. However, convincing\nand wrong self-explanations can lead to unsupported confidence in LLMs, thus\nincreasing risk. Therefore, it's important to measure if self-explanations\ntruly reflect the model's behavior. Such a measure is called\ninterpretability-faithfulness and is challenging to perform since the ground\ntruth is inaccessible, and many LLMs only have an inference API. To address\nthis, we propose employing self-consistency checks to measure faithfulness. For\nexample, if an LLM says a set of words is important for making a prediction,\nthen it should not be able to make its prediction without these words. While\nself-consistency checks are a common approach to faithfulness, they have not\npreviously been successfully applied to LLM self-explanations for\ncounterfactual, feature attribution, and redaction explanations. Our results\ndemonstrate that faithfulness is explanation, model, and task-dependent,\nshowing self-explanations should not be trusted in general. For example, with\nsentiment classification, counterfactuals are more faithful for Llama2, feature\nattribution for Mistral, and redaction for Falcon 40B.\n"", '  Large Language Models (LLMs) are deployed as powerful tools for several\nnatural language processing (NLP) applications. Recent works show that modern\nLLMs can generate self-explanations (SEs), which elicit their intermediate\nreasoning steps for explaining their behavior. Self-explanations have seen\nwidespread adoption owing to their conversational and plausible nature.\nHowever, there is little to no understanding of their faithfulness. In this\nwork, we discuss the dichotomy between faithfulness and plausibility in SEs\ngenerated by LLMs. We argue that while LLMs are adept at generating plausible\nexplanations -- seemingly logical and coherent to human users -- these\nexplanations do not necessarily align with the reasoning processes of the LLMs,\nraising concerns about their faithfulness. We highlight that the current trend\ntowards increasing the plausibility of explanations, primarily driven by the\ndemand for user-friendly interfaces, may come at the cost of diminishing their\nfaithfulness. We assert that the faithfulness of explanations is critical in\nLLMs employed for high-stakes decision-making. Moreover, we emphasize the need\nfor a systematic characterization of faithfulness-plausibility requirements of\ndifferent real-world applications and ensure explanations meet those needs.\nWhile there are several approaches to improving plausibility, improving\nfaithfulness is an open challenge. We call upon the community to develop novel\nmethods to enhance the faithfulness of self explanations thereby enabling\ntransparent deployment of LLMs in diverse high-stakes settings.\n']",Evaluating Explainability in Large Language Models
360,359,21,359_forecasting_forecast_forecasts_prediction,"['forecasting', 'forecast', 'forecasts', 'prediction', 'electricity', 'predictions', 'regression', 'markets', 'prices', 'demand']","['electricity', 'price', 'forecasting', 'ahead', 'market', 'day', 'forecasts', 'markets', 'prices', 'quantile']","['forecasts', 'electricity', 'markets', 'seasonalities', 'bidding', 'heteroskedasticity', 'renewables', 'hourly', 'quantile', 'dispatch']","[""  Accurate prediction of electricity day-ahead prices is essential in\ncompetitive electricity markets. Although stationary electricity-price\nforecasting techniques have received considerable attention, research on\nnon-stationary methods is comparatively scarce, despite the common prevalence\nof non-stationary features in electricity markets. Specifically, existing\nnon-stationary techniques will often aim to address individual non-stationary\nfeatures in isolation, leaving aside the exploration of concurrent multiple\nnon-stationary effects. Our overarching objective here is the formulation of a\nframework to systematically model and forecast non-stationary electricity-price\ntime series, encompassing the broader scope of non-stationary behavior. For\nthis purpose we develop a data-driven model that combines an N-dimensional\nLangevin equation (LE) with a neural-ordinary differential equation (NODE). The\nLE captures fine-grained details of the electricity-price behavior in\nstationary regimes but is inadequate for non-stationary conditions. To overcome\nthis inherent limitation, we adopt a NODE approach to learn, and at the same\ntime predict, the difference between the actual electricity-price time series\nand the simulated price trajectories generated by the LE. By learning this\ndifference, the NODE reconstructs the non-stationary components of the time\nseries that the LE is not able to capture. We exemplify the effectiveness of\nour framework using the Spanish electricity day-ahead market as a prototypical\ncase study. Our findings reveal that the NODE nicely complements the LE,\nproviding a comprehensive strategy to tackle both stationary and non-stationary\nelectricity-price behavior. The framework's dependability and robustness is\ndemonstrated through different non-stationary scenarios by comparing it against\na range of basic naive methods.\n"", '  This paper undertakes a comprehensive investigation of electricity price\nforecasting methods, focused on the Irish Integrated Single Electricity Market,\nparticularly on changes during recent periods of high volatility. The primary\nobjective of this research is to evaluate and compare the performance of\nvarious forecasting models, ranging from traditional machine learning models to\nmore complex neural networks, as well as the impact of different lengths of\ntraining periods. The performance metrics, mean absolute error, root mean\nsquare error, and relative mean absolute error, are utilized to assess and\ncompare the accuracy of each model. A comprehensive set of input features was\ninvestigated and selected from data recorded between October 2018 and September\n2022. The paper demonstrates that the daily EU Natural Gas price is a more\nuseful feature for electricity price forecasting in Ireland than the daily\nHenry Hub Natural Gas price. This study also shows that the correlation of\nfeatures to the day-ahead market price has changed in recent years. The price\nof natural gas on the day and the amount of wind energy on the grid that hour\nare significantly more important than any other features. More specifically\nspeaking, the input fuel for electricity has become a more important driver of\nthe price of it, than the total generation or demand. In addition, it can be\nseen that System Non-Synchronous Penetration (SNSP) is highly correlated with\nthe day-ahead market price, and that renewables are pushing down the price of\nelectricity.\n', ""  Trading on the day-ahead electricity markets requires accurate information\nabout the realization of electricity prices and the uncertainty attached to the\npredictions. Deriving accurate forecasting models presents a difficult task due\nto the day-ahead price's non-stationarity resulting from changing market\nconditions, e.g., due to changes resulting from the energy crisis in 2021. We\npresent a probabilistic forecasting approach for day-ahead electricity prices\nusing the fully data-driven deep generative model called normalizing flow. Our\nmodeling approach generates full-day scenarios of day-ahead electricity prices\nbased on conditional features such as residual load forecasts. Furthermore, we\npropose extended feature sets of prior realizations and a periodic retraining\nscheme that allows the normalizing flow to adapt to the changing conditions of\nmodern electricity markets. Our results highlight that the normalizing flow\ngenerates high-quality scenarios that reproduce the true price distribution and\nyield accurate forecasts. Additionally, our analysis highlights how our\nimprovements towards adaptations in changing regimes allow the normalizing flow\nto adapt to changing market conditions and enable continued sampling of\nhigh-quality day-ahead price scenarios.\n""]",Electricity Price Forecasting
361,360,20,360_contracts_optimal_incentive_learns,"['contracts', 'optimal', 'incentive', 'learns', 'agents', 'bandit', 'incentives', 'contract', 'reward', 'agent']","['principal', 'contract', 'contracts', 'agent', 'regret', 'players', 'utility', 'agents', 'game', 'optimal']","['contracts', 'optimal', 'incentive', 'learns', 'bandit', 'agent', 'strategic', 'games', 'equilibrium', 'generalized']","[""  We study a repeated contracting setting in which a Principal adaptively\nchooses amongst $k$ Agents at each of $T$ rounds. The Agents are non-myopic,\nand so a mechanism for the Principal induces a $T$-round extensive form game\namongst the Agents. We give several results aimed at understanding an\nunder-explored aspect of contract theory -- the game induced when choosing an\nAgent to contract with. First, we show that this game admits a pure-strategy\n\\emph{non-responsive} equilibrium amongst the Agents -- informally an\nequilibrium in which the Agent's actions depend on the history of realized\nstates of nature, but not on the history of each other's actions, and so avoids\nthe complexities of collusion and threats. Next, we show that if the Principal\nselects Agents using a \\emph{monotone} bandit algorithm, then for any concave\ncontract, in any such equilibrium, the Principal obtains no regret to\ncontracting with the best Agent in hindsight -- not just given their realized\nactions, but also to the counterfactual world in which they had offered a\nguaranteed $T$-round contract to the best Agent in hindsight, which would have\ninduced a different sequence of actions. Finally, we show that if the Principal\nselects Agents using a monotone bandit algorithm which guarantees no\nswap-regret, then the Principal can additionally offer only limited liability\ncontracts (in which the Agent never needs to pay the Principal) while getting\nno-regret to the counterfactual world in which she offered a linear contract to\nthe best Agent in hindsight -- despite the fact that linear contracts are not\nlimited liability. We instantiate this theorem by demonstrating the existence\nof a monotone no swap-regret bandit algorithm, which to our knowledge has not\npreviously appeared in the literature.\n"", ""  We study principal-agent problems in which a principal commits to an\noutcome-dependent payment scheme -- called contract -- in order to induce an\nagent to take a costly, unobservable action leading to favorable outcomes. We\nconsider a generalization of the classical (single-round) version of the\nproblem in which the principal interacts with the agent by committing to\ncontracts over multiple rounds. The principal has no information about the\nagent, and they have to learn an optimal contract by only observing the outcome\nrealized at each round. We focus on settings in which the size of the agent's\naction space is small. We design an algorithm that learns an\napproximately-optimal contract with high probability in a number of rounds\npolynomial in the size of the outcome space, when the number of actions is\nconstant. Our algorithm solves an open problem by Zhu et al.[2022]. Moreover,\nit can also be employed to provide a $\\tilde{\\mathcal{O}}(T^{4/5})$ regret\nbound in the related online learning setting in which the principal aims at\nmaximizing their cumulative utility, thus considerably improving\npreviously-known regret bounds.\n"", ""  Generalized principal-agent problems, including Stackelberg games, contract\ndesign, and Bayesian persuasion, are a class of economic problems where an\nagent best responds to a principal's committed strategy. We study repeated\ngeneralized principal-agent problems under the assumption that the principal\ndoes not have commitment power and the agent uses algorithms to learn to\nrespond to the principal. We reduce this problem to a one-shot generalized\nprincipal-agent problem with an approximately-best-responding agent. Using this\nreduction, we show that: (1) if the agent uses contextual no-regret learning\nalgorithms, then the principal can guarantee a utility that is at least the\nprincipal's optimal utility in the classic non-learning model minus the square\nroot of the agent's regret; (2) if the agent uses contextual no-swap-regret\nlearning algorithms, then the principal cannot obtain any utility more than the\noptimal utility in the non-learning model plus the agent's swap regret. But (3)\nif the agent uses mean-based learning algorithms (which can be no-regret but\nnot no-swap-regret), then the principal can do significantly better than the\nnon-learning model. These general results not only refine previous results in\nStackelberg games and contract design with learning agents but also lead to new\nresults for Bayesian persuasion with a learning agent.\n""]",Contract Theory and Incentive Design
362,361,20,361_memecraft_memes_multimodal_memeguard,"['memecraft', 'memes', 'multimodal', 'memeguard', 'mememqacorpus', 'meme', 'hateful', 'tweets', 'mememqa', 'embeddings']","['memes', 'meme', 'hateful', 'harmful', 'toxic', 'hate', 'multimodal', 'internet', 'hatefulness', 'content']","['memecraft', 'multimodal', 'memeguard', 'mememqa', 'cyberbullying', 'hatefulness', 'modality', 'humor', 'communities', 'detecting']","['  Recent advances show that two-stream approaches have achieved outstanding\nperformance in hateful meme detection. However, hateful memes constantly evolve\nas new memes emerge by fusing progressive cultural ideas, making existing\nmethods obsolete or ineffective. In this work, we explore the potential of\nLarge Multimodal Models (LMMs) for hateful meme detection. To this end, we\npropose Evolver, which incorporates LMMs via Chain-of-Evolution (CoE)\nPrompting, by integrating the evolution attribute and in-context information of\nmemes. Specifically, Evolver simulates the evolving and expressing process of\nmemes and reasons through LMMs in a step-by-step manner. First, an evolutionary\npair mining module retrieves the top-k most similar memes in the external\ncurated meme set with the input meme. Second, an evolutionary information\nextractor is designed to summarize the semantic regularities between the paired\nmemes for prompting. Finally, a contextual relevance amplifier enhances the\nin-context hatefulness information to boost the search for evolutionary\nprocesses. Extensive experiments on public FHM, MAMI, and HarM datasets show\nthat CoE prompting can be incorporated into existing LMMs to improve their\nperformance. More encouragingly, it can serve as an interpretive tool to\npromote the understanding of the evolution of social memes.\n', '  Warning: This paper contains memes that may be offensive to some readers.\n  Multimodal Internet Memes are now a ubiquitous fixture in online discourse.\nOne strand of meme-based research is the classification of memes according to\nvarious affects, such as sentiment and hate, supported by manually compiled\nmeme datasets. Understanding the unique characteristics of memes is crucial for\nmeme classification. Unlike other user-generated content, memes spread via\nmemetics, i.e. the process by which memes are imitated and transformed into\nsymbols used to create new memes. In effect, there exists an ever-evolving pool\nof visual and linguistic symbols that underpin meme culture and are crucial to\ninterpreting the meaning of individual memes. The current approach of training\nsupervised learning models on static datasets, without taking memetics into\naccount, limits the depth and accuracy of meme interpretation. We argue that\nmeme datasets must contain genuine memes, as defined via memetics, so that\neffective meme classifiers can be built. In this work, we develop a meme\nidentification protocol which distinguishes meme from non-memetic content by\nrecognising the memetics within it. We apply our protocol to random samplings\nof the leading 7 meme classification datasets and observe that more than half\n(50. 4\\%) of the evaluated samples were found to contain no signs of memetics.\nOur work also provides a meme typology grounded in memetics, providing the\nbasis for more effective approaches to the interpretation of memes and the\ncreation of meme datasets.\n', '  Multimedia content on social media is rapidly evolving, with memes gaining\nprominence as a distinctive form. Unfortunately, some malicious users exploit\nmemes to target individuals or vulnerable communities, making it imperative to\nidentify and address such instances of hateful memes. Extensive research has\nbeen conducted to address this issue by developing hate meme detection models.\nHowever, a notable limitation of traditional machine/deep learning models is\nthe requirement for labeled datasets for accurate classification. Recently, the\nresearch community has witnessed the emergence of several visual language\nmodels that have exhibited outstanding performance across various tasks. In\nthis study, we aim to investigate the efficacy of these visual language models\nin handling intricate tasks such as hate meme detection. We use various prompt\nsettings to focus on zero-shot classification of hateful/harmful memes. Through\nour analysis, we observe that large VLMs are still vulnerable for zero-shot\nhate meme detection.\n']",Hateful Meme Detection and Analysis
363,362,20,362_privacy_confidential_privately_private,"['privacy', 'confidential', 'privately', 'private', 'obfuscation', 'secure', 'security', 'public', 'protect', 'untrusted']","['private', 'privacy', 'secure', 'party', 'inference', 'instructions', 'prompts', 'concerns', 'provider', 'services']","['privately', 'obfuscation', 'secure', 'serializing', 'leak', 'sgd', 'llms', 'cloud', 'mpc', 'protocols']","['  In-context learning (ICL) enables large language models (LLMs) to adapt to\nnew tasks by conditioning on demonstrations of question-answer pairs and it has\nbeen shown to have comparable performance to costly model retraining and\nfine-tuning. Recently, ICL has been extended to allow tabular data to be used\nas demonstration examples by serializing individual records into natural\nlanguage formats. However, it has been shown that LLMs can leak information\ncontained in prompts, and since tabular data often contain sensitive\ninformation, understanding how to protect the underlying tabular data used in\nICL is a critical area of research. This work serves as an initial\ninvestigation into how to use differential privacy (DP) -- the long-established\ngold standard for data privacy and anonymization -- to protect tabular data\nused in ICL. Specifically, we investigate the application of DP mechanisms for\nprivate tabular ICL via data privatization prior to serialization and\nprompting. We formulate two private ICL frameworks with provable privacy\nguarantees in both the local (LDP-TabICL) and global (GDP-TabICL) DP scenarios\nvia injecting noise into individual records or group statistics, respectively.\nWe evaluate our DP-based frameworks on eight real-world tabular datasets and\nacross multiple ICL and DP settings. Our evaluations show that DP-based ICL can\nprotect the privacy of the underlying tabular data while achieving comparable\nperformance to non-LLM baselines, especially under high privacy regimes.\n', ""  Large Language Models (LLMs) have emerged as dominant tools for various\ntasks, particularly when tailored for a specific target by prompt tuning.\nNevertheless, concerns surrounding data privacy present obstacles due to the\ntuned prompts' dependency on sensitive private information. A practical\nsolution is to host a local LLM and optimize a soft prompt privately using\ndata. Yet, hosting a local model becomes problematic when model ownership is\nprotected. Alternative methods, like sending data to the model's provider for\ntraining, intensify these privacy issues facing an untrusted provider. In this\npaper, we present a novel solution called Differentially-Private Offsite Prompt\nTuning (DP-OPT) to address this challenge. Our approach involves tuning a\ndiscrete prompt on the client side and then applying it to the desired cloud\nmodels. We demonstrate that prompts suggested by LLMs themselves can be\ntransferred without compromising performance significantly. To ensure that the\nprompts do not leak private information, we introduce the first private prompt\ngeneration mechanism, by a differentially-private (DP) ensemble of in-context\nlearning with private demonstrations. With DP-OPT, generating\nprivacy-preserving prompts by Vicuna-7b can yield competitive performance\ncompared to non-private in-context learning on GPT3.5 or local private prompt\ntuning. Codes are available at https://github.com/VITA-Group/DP-OPT .\n"", '  We study the problem of in-context learning (ICL) with large language models\n(LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leak\nor regurgitate the private examples demonstrated in the prompt. We propose a\nnovel algorithm that generates synthetic few-shot demonstrations from the\nprivate dataset with formal differential privacy (DP) guarantees, and show\nempirically that it can achieve effective ICL. We conduct extensive experiments\non standard benchmarks and compare our algorithm with non-private ICL and\nzero-shot solutions. Our results demonstrate that our algorithm can achieve\ncompetitive performance with strong privacy levels. These results open up new\npossibilities for ICL with privacy protection for a broad range of\napplications.\n']","""Protecting Data Privacy in Large Language Models"""
364,363,20,363_contextual_attention_context_icl,"['contextual', 'attention', 'context', 'icl', 'examples', 'language', 'example', 'pretraining', 'icll', 'input']","['heads', 'context', 'transformers', 'token', 'transformer', 'bigram', 'function', 'attention', 'output', 'induction']","['contextual', 'attention', 'icl', 'example', 'pretraining', 'classes', 'lms', 'layers', 'tokens', 'textsf']","[""  State of the art foundation models such as GPT-4 perform surprisingly well at\nin-context learning (ICL), a variant of meta-learning concerning the learned\nability to solve tasks during a neural network forward pass, exploiting\ncontextual information provided as input to the model. This useful ability\nemerges as a side product of the foundation model's massive pretraining. While\ntransformer models are currently the state of the art in ICL, this work\nprovides empirical evidence that Mamba, a newly proposed state space model\nwhich scales better than transformers w.r.t. the input sequence length, has\nsimilar ICL capabilities. We evaluated Mamba on tasks involving simple function\napproximation as well as more complex natural language processing problems. Our\nresults demonstrate that, across both categories of tasks, Mamba closely\nmatches the performance of transformer models for ICL. Further analysis reveals\nthat, like transformers, Mamba appears to solve ICL problems by incrementally\noptimizing its internal representations. Overall, our work suggests that Mamba\ncan be an efficient alternative to transformers for ICL tasks involving long\ninput sequences. This is an exciting finding in meta-learning and may enable\ngeneralizations of in-context learned AutoML algorithms (like TabPFN or\nOptformer) to long input sequences.\n"", '  In-context learning (ICL) exhibits dual operating modes: task learning, i.e.,\nacquiring a new skill from in-context samples, and task retrieval, i.e.,\nlocating and activating a relevant pretrained skill. Recent theoretical work\ninvestigates various mathematical models to analyze ICL, but existing models\nexplain only one operating mode at a time. We introduce a probabilistic model,\nwith which one can explain the dual operating modes of ICL simultaneously.\nFocusing on in-context learning of linear functions, we extend existing models\nfor pretraining data by introducing multiple task groups and task-dependent\ninput distributions. We then analyze the behavior of the optimally pretrained\nmodel under the squared loss, i.e., the MMSE estimator of the label given\nin-context examples. Regarding pretraining task distribution as prior and\nin-context examples as the observation, we derive the closed-form expression of\nthe task posterior distribution. With the closed-form expression, we obtain a\nquantitative understanding of the two operating modes of ICL. Furthermore, we\nshed light on an unexplained phenomenon observed in practice: under certain\nsettings, the ICL risk initially increases and then decreases with more\nin-context examples. Our model offers a plausible explanation for this ""early\nascent"" phenomenon: a limited number of in-context samples may lead to the\nretrieval of an incorrect skill, thereby increasing the risk, which will\neventually diminish as task learning takes effect with more in-context samples.\nWe also theoretically analyze ICL with biased labels, e.g., zero-shot ICL,\nwhere in-context examples are assigned random labels. Lastly, we validate our\nfindings and predictions via experiments involving Transformers and large\nlanguage models.\n', '  In-context learning (ICL) is one of the surprising and useful features of\nlarge language models and subject of intense research. Recently, stylized\nmeta-learning-like ICL setups have been devised that train transformers on\nsequences of input-output pairs $(x, f(x))$. The function $f$ comes from a\nfunction class and generalization is checked by evaluating on sequences\ngenerated from unseen functions from the same class. One of the main\ndiscoveries in this line of research has been that for several function\nclasses, such as linear regression, transformers successfully generalize to new\nfunctions in the class. However, the inductive biases of these models resulting\nin this behavior are not clearly understood. A model with unlimited training\ndata and compute is a Bayesian predictor: it learns the pretraining\ndistribution. In this paper we empirically examine how far this Bayesian\nperspective can help us understand ICL. To this end, we generalize the previous\nmeta-ICL setup to hierarchical meta-ICL setup which involve unions of multiple\ntask families. We instantiate this setup on a diverse range of linear and\nnonlinear function families and find that transformers can do ICL in this\nsetting as well. Where Bayesian inference is tractable, we find evidence that\nhigh-capacity transformers mimic the Bayesian predictor. The Bayesian\nperspective provides insights into the inductive bias of ICL and how\ntransformers perform a particular task when they are trained on multiple tasks.\nWe also find that transformers can learn to generalize to new function classes\nthat were not seen during pretraining. This involves deviation from the\nBayesian predictor. We examine these deviations in more depth offering new\ninsights and hypotheses.\n']",In-Context Learning in Artificial Intelligence
365,364,20,364_edge_cloud_iot_network,"['edge', 'cloud', 'iot', 'network', 'ai', 'bandwidth', 'devices', 'mobile', 'device', 'offload']","['edge', 'cloud', 'inference', 'intelligence', 'computing', 'collaborative', 'resource', 'sustainable', 'devices', 'latency']","['edge', 'cloud', 'iot', 'dnn', 'latency', 'galaxy', 'intelligent', 'netgpt', '6g', 'energy']","[""  Big Artificial Intelligence (AI) models have emerged as a crucial element in\nvarious intelligent applications at the edge, such as voice assistants in smart\nhomes and autonomous robotics in smart factories. Training big AI models, e.g.,\nfor personalized fine-tuning and continual model refinement, poses significant\nchallenges to edge devices due to the inherent conflict between limited\ncomputing resources and intensive workload associated with training. Despite\nthe constraints of on-device training, traditional approaches usually resort to\naggregating training data and sending it to a remote cloud for centralized\ntraining. Nevertheless, this approach is neither sustainable, which strains\nlong-range backhaul transmission and energy-consuming datacenters, nor safely\nprivate, which shares users' raw data with remote infrastructures. To address\nthese challenges, we alternatively observe that prevalent edge environments\nusually contain a diverse collection of trusted edge devices with untapped idle\nresources, which can be leveraged for edge training acceleration. Motivated by\nthis, in this article, we propose collaborative edge training, a novel training\nmechanism that orchestrates a group of trusted edge devices as a resource pool\nfor expedited, sustainable big AI model training at the edge. As an initial\nstep, we present a comprehensive framework for building collaborative edge\ntraining systems and analyze in-depth its merits and sustainable scheduling\nchoices following its workflow. To further investigate the impact of its\nparallelism design, we empirically study a case of four typical parallelisms\nfrom the perspective of energy demand with realistic testbeds. Finally, we\ndiscuss open challenges for sustainable collaborative edge training to point to\nfuture directions of edge-centric big AI model training.\n"", ""  With the vigorous development of artificial intelligence (AI), the\nintelligent applications based on deep neural network (DNN) change people's\nlifestyles and the production efficiency. However, the huge amount of\ncomputation and data generated from the network edge becomes the major\nbottleneck, and traditional cloud-based computing mode has been unable to meet\nthe requirements of real-time processing tasks. To solve the above problems, by\nembedding AI model training and inference capabilities into the network edge,\nedge intelligence (EI) becomes a cutting-edge direction in the field of AI.\nFurthermore, collaborative DNN inference among the cloud, edge, and end device\nprovides a promising way to boost the EI. Nevertheless, at present, EI oriented\ncollaborative DNN inference is still in its early stage, lacking a systematic\nclassification and discussion of existing research efforts. Thus motivated, we\nhave made a comprehensive investigation on the recent studies about EI oriented\ncollaborative DNN inference. In this paper, we firstly review the background\nand motivation of EI. Then, we classify four typical collaborative DNN\ninference paradigms for EI, and analyze the characteristics and key\ntechnologies of them. Finally, we summarize the current challenges of\ncollaborative DNN inference, discuss the future development trend and provide\nthe future research direction.\n"", '  Artificial intelligence (AI) technologies have emerged as pivotal enablers\nacross a multitude of industries largely due to their significant resurgence\nover the past decade. The transformative power of AI is primarily derived from\nthe utilization of deep neural networks (DNNs), which require extensive data\nfor training and substantial computational resources for processing.\nConsequently, DNN models are typically trained and deployed on resource-rich\ncloud servers. However, due to potential latency issues associated with cloud\ncommunications, deep learning (DL) workflows are increasingly being\ntransitioned to wireless edge networks in proximity to end-user devices (EUDs).\nThis shift is designed to support latency-sensitive applications and has given\nrise to a new paradigm of edge AI, which will play a critical role in upcoming\nsixth-generation (6G) networks to support ubiquitous AI applications. Despite\nits considerable potential, edge AI faces substantial challenges, mostly due to\nthe dichotomy between the resource limitations of wireless edge networks and\nthe resource-intensive nature of DL. Specifically, the acquisition of\nlarge-scale data, as well as the training and inference processes of DNNs, can\nrapidly deplete the battery energy of EUDs. This necessitates an\nenergy-conscious approach to edge AI to ensure both optimal and sustainable\nperformance. In this paper, we present a contemporary survey on green edge AI.\nWe commence by analyzing the principal energy consumption components of edge AI\nsystems to identify the fundamental design principles of green edge AI. Guided\nby these principles, we then explore energy-efficient design methodologies for\nthe three critical tasks in edge AI systems, including training data\nacquisition, edge training, and edge inference. Finally, we underscore\npotential future research directions to further enhance the energy efficiency\nof edge AI.\n']",Edge Computing for Artificial Intelligence and IoT
366,365,20,365_gpus_scheduling_gpu_cluster,"['gpus', 'scheduling', 'gpu', 'cluster', 'scheduler', 'processors', 'schedulers', 'schedulability', 'schedulable', 'workloads']","['scheduling', 'accelerators', 'parallelism', 'scheduler', 'workloads', 'partitioning', 'job', 'cluster', 'gang', 'jobs']","['gpus', 'cluster', 'processors', 'schedulability', 'workloads', 'bottlenecked', 'parallelism', 'training', 'heterogeneous', 'pipelines']","['  Joint consideration of scheduling and adaptive parallelism offers great\nopportunities for improving the training efficiency of large models on\nheterogeneous GPU clusters. However, integrating adaptive parallelism into a\ncluster scheduler expands the cluster scheduling space. The new space is the\nproduct of the original scheduling space and the parallelism exploration space\nof adaptive parallelism (also a product of pipeline, data, and tensor\nparallelism). The exponentially enlarged scheduling space and ever-changing\noptimal parallelism plan from adaptive parallelism together result in the\ncontradiction between low-overhead and accurate performance data acquisition\nfor efficient cluster scheduling. This paper presents Crius, a training system\nfor efficiently scheduling multiple large models with adaptive parallelism in a\nheterogeneous cluster. Crius proposes a novel scheduling granularity called\nCell. It represents a job with deterministic resources and pipeline stages. The\nexploration space of Cell is shrunk to the product of only data and tensor\nparallelism, thus exposing the potential for accurate and low-overhead\nperformance estimation. Crius then accurately estimates Cells and efficiently\nschedules training jobs. When a Cell is selected as a scheduling choice, its\nrepresented job runs with the optimal parallelism plan explored. Experimental\nresults show that Crius reduces job completion time by up to 48.9% and\nschedules large models with up to 1.49x cluster throughput improvement.\n', '  Training large-scale models relies on a vast number of computing resources.\nFor example, training the GPT-4 model (1.8 trillion parameters) requires 25000\nA100 GPUs . It is a challenge to build a large-scale cluster with one type of\nGPU-accelerator. Using multiple types of GPU-accelerators to construct a\nlarge-scale cluster is an effective way to solve the problem of insufficient\nhomogeneous GPU-accelerators. However, the existing distributed training\nsystems for large-scale models only support homogeneous GPU-accelerators, not\nsupport heterogeneous GPU-accelerators. To address the problem, this paper\nproposes a distributed training system with hybrid parallelism, HETHUB, for\nlarge-scale models, which supports heterogeneous cluster, including AMD, Nvidia\nGPU and other types of GPU-accelerators . It introduces a distributed unified\ncommunicator to realize the communication between heterogeneous\nGPU-accelerators, a distributed performance predictor, and an automatic\nparallel planner to develop and train models efficiently with heterogeneous\nGPU-accelerators. Compared to the distributed training system with homogeneous\nGPU-accelerators, our system can support six combinations of heterogeneous\nGPU-accelerators. We train the Llama-140B model on a heterogeneous cluster with\n768 GPU-accelerators(128 AMD and 640 GPU-accelerator A). The experiment results\nshow that the optimal performance of our system in the heterogeneous cluster\nhas achieved up to 97.49% of the theoretical upper bound performance.\n', '  GPU-based heterogeneous architectures are now commonly used in HPC clusters.\nDue to their architectural simplicity specialized for data-level parallelism,\nGPUs can offer much higher computational throughput and memory bandwidth than\nCPUs in the same generation do. However, as the available resources in GPUs\nhave increased exponentially over the past decades, it has become increasingly\ndifficult for a single program to fully utilize them. As a consequence, the\nindustry has started supporting several resource partitioning features in order\nto improve the resource utilization by co-scheduling multiple programs on the\nsame GPU die at the same time. Driven by the technological trend, this paper\nfocuses on hierarchical resource partitioning on modern GPUs, and as an\nexample, we utilize a combination of two different features available on recent\nNVIDIA GPUs in a hierarchical manner: MPS (Multi-Process Service), a\nfiner-grained logical partitioning; and MIG (Multi-Instance GPU), a\ncoarse-grained physical partitioning. We propose a method for comprehensively\nco-optimizing the setup of hierarchical partitioning and the selection of\nco-scheduling groups from a given set of jobs, based on reinforcement learning\nusing their profiles. Our thorough experimental results demonstrate that our\napproach can successfully set up job concurrency, partitioning, and\nco-scheduling group selections simultaneously. This results in a maximum\nthroughput improvement by a factor of 1.87 compared to the time-sharing\nscheduling.\n']",GPU Scheduling and Resource Management
367,366,20,366_longbench_contexts_retrieval_benchmark,"['longbench', 'contexts', 'retrieval', 'benchmark', 'benchmarks', 'longer', 'context', 'texts', 'long', 'longins']","['long', 'context', 'length', '32k', 'middle', 'contexts', 'lengths', 'longer', 'tokens', 'document']","['longbench', 'contexts', 'retrieval', 'benchmarks', 'texts', 'lengths', 'opus', 'comprehensive', 'handle', 'proficiency']","[""  Recently, the large language model (LLM) community has shown increasing\ninterest in enhancing LLMs' capability to handle extremely long documents. As\nvarious long-text techniques and model architectures emerge, the precise and\ndetailed evaluation of models' long-text capabilities has become increasingly\nimportant. Existing long-text evaluation benchmarks, such as L-Eval and\nLongBench, construct long-text test sets based on open-source datasets,\nfocusing mainly on QA and summarization tasks. These datasets include test\nsamples of varying lengths (from 2k to 32k+) entangled together, making it\nchallenging to assess model capabilities across different length ranges.\nMoreover, they do not cover the ultralong settings (100k+ tokens) that the\nlatest LLMs claim to achieve. In this paper, we introduce Ada-LEval, a\nlength-adaptable benchmark for evaluating the long-context understanding of\nLLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which\nenable a more reliable evaluation of LLMs' long context capabilities. These\nbenchmarks support intricate manipulation of the length of test cases, and can\neasily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art\nclosed-source API models and 6 open-source models with Ada-LEval. The\nevaluation results demonstrate the limitations of current LLMs, especially in\nultra-long-context settings. Our code is available at\nhttps://github.com/open-compass/Ada-LEval.\n"", ""  Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks but are constrained by their small context window sizes. Various\nefforts have been proposed to expand the context window to accommodate even up\nto 200K input tokens. Meanwhile, building high-quality benchmarks with much\nlonger text lengths and more demanding tasks to provide comprehensive\nevaluations is of immense practical interest to facilitate long context\nunderstanding research of LLMs. However, prior benchmarks create datasets that\nostensibly cater to long-text comprehension by expanding the input of\ntraditional tasks, which falls short to exhibit the unique characteristics of\nlong-text understanding, including long dependency tasks and longer text length\ncompatible with modern LLMs' context window size. In this paper, we introduce a\nbenchmark for extremely long context understanding with long-range\ndependencies, XL$^2$Bench, which includes three scenarios: Fiction Reading,\nPaper Reading, and Law Reading, and four tasks of increasing complexity: Memory\nRetrieval, Detailed Understanding, Overall Understanding, and Open-ended\nGeneration, covering 27 subtasks in English and Chinese. It has an average\nlength of 100K+ words (English) and 200K+ characters (Chinese). Evaluating six\nleading LLMs on XL$^2$Bench, we find that their performance significantly lags\nbehind human levels. Moreover, the observed decline in performance across both\nthe original and enhanced datasets underscores the efficacy of our approach to\nmitigating data contamination.\n"", ""  Although large language models (LLMs) demonstrate impressive performance for\nmany language tasks, most of them can only handle texts a few thousand tokens\nlong, limiting their applications on longer sequence inputs, such as books,\nreports, and codebases. Recent works have proposed methods to improve LLMs'\nlong context capabilities by extending context windows and more sophisticated\nmemory mechanisms. However, comprehensive benchmarks tailored for evaluating\nlong context understanding are lacking. In this paper, we introduce LongBench,\nthe first bilingual, multi-task benchmark for long context understanding,\nenabling a more rigorous evaluation of long context understanding. LongBench\ncomprises 21 datasets across 6 task categories in both English and Chinese,\nwith an average length of 6,711 words (English) and 13,386 characters\n(Chinese). These tasks cover key long-text application areas including\nsingle-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks,\nand code completion. All datasets in LongBench are standardized into a unified\nformat, allowing for effortless automatic evaluation of LLMs. Upon\ncomprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial\nmodel (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still\nstruggles on longer contexts. (2) Scaled position embedding and fine-tuning on\nlonger sequences lead to substantial improvement on long context understanding.\n(3) Context compression technique such as retrieval brings improvement for\nmodel with weak ability on long contexts, but the performance still lags behind\nmodels that have strong long context understanding capability. The code and\ndatasets are available at https://github.com/THUDM/LongBench.\n""]",Evaluating Large Language Models on Long-Text Understanding
368,367,20,367_ai_intelligence_ethical_humanity,"['ai', 'intelligence', 'ethical', 'humanity', 'alignmentsurvey', 'agent', 'moral', 'ethics', 'agents', 'morality']","['alignment', 'values', 'moral', 'humans', 'value', 'human', 'norms', 'ethics', 'agents', 'symbol']","['ai', 'alignmentsurvey', 'ethics', 'concept', 'empathise', 'values', 'judgments', 'extractivism', 'philosophical', 'behavioral']","['  Discussion of AI alignment (alignment between humans and AI systems) has\nfocused on value alignment, broadly referring to creating AI systems that share\nhuman values. We argue that before we can even attempt to align values, it is\nimperative that AI systems and humans align the concepts they use to understand\nthe world. We integrate ideas from philosophy, cognitive science, and deep\nlearning to explain the need for concept alignment, not just value alignment,\nbetween humans and machines. We summarize existing accounts of how humans and\nmachines currently learn concepts, and we outline opportunities and challenges\nin the path towards shared concepts. Finally, we explain how we can leverage\nthe tools already being developed in cognitive science and AI research to\naccelerate progress towards concept alignment.\n', ""  How can we build AI systems that are aligned with human values to avoid\ncausing harm or violating societal standards for acceptable behavior? We argue\nthat representational alignment between humans and AI agents facilitates value\nalignment. Making AI systems learn human-like representations of the world has\nmany known benefits, including improving generalization, robustness to domain\nshifts, and few-shot learning performance. We propose that this kind of\nrepresentational alignment between machine learning (ML) models and humans can\nalso support value alignment, allowing ML systems to conform to human values\nand societal norms. We focus on ethics as one aspect of value alignment and\ntrain ML agents using a variety of methods in a multi-armed bandit setting,\nwhere rewards reflect the moral acceptability of the chosen action. We use a\nsynthetic experiment to demonstrate that agents' representational alignment\nwith the environment bounds their learning performance. We then repeat this\nprocedure in a realistic setting, using textual action descriptions and\nsimilarity judgments collected from humans and a variety of language models, to\nshow that the results generalize and are model-agnostic when grounded in an\nethically relevant context.\n"", '  Recent advancements in general-purpose AI have highlighted the importance of\nguiding AI systems towards the intended goals, ethical principles, and values\nof individuals and groups, a concept broadly recognized as alignment. However,\nthe lack of clarified definitions and scopes of human-AI alignment poses a\nsignificant obstacle, hampering collaborative efforts across research domains\nto achieve this alignment. In particular, ML- and philosophy-oriented alignment\nresearch often views AI alignment as a static, unidirectional process (i.e.,\naiming to ensure that AI systems\' objectives match humans) rather than an\nongoing, mutual alignment problem. This perspective largely neglects the\nlong-term interaction and dynamic changes of alignment. To understand these\ngaps, we introduce a systematic review of over 400 papers published between\n2019 and January 2024, spanning multiple domains such as Human-Computer\nInteraction (HCI), Natural Language Processing (NLP), Machine Learning (ML). We\ncharacterize, define and scope human-AI alignment. From this, we present a\nconceptual framework of ""Bidirectional Human-AI Alignment"" to organize the\nliterature from a human-centered perspective. This framework encompasses both\n1) conventional studies of aligning AI to humans that ensures AI produces the\nintended outcomes determined by humans, and 2) a proposed concept of aligning\nhumans to AI, which aims to help individuals and society adjust to AI\nadvancements both cognitively and behaviorally. Additionally, we articulate the\nkey findings derived from literature analysis, including literature gaps and\ntrends, human values, and interaction techniques. To pave the way for future\nstudies, we envision three key challenges and give recommendations for future\nresearch.\n']",AI Alignment and Ethics
369,368,20,368_privacy_federated_private_distributed,"['privacy', 'federated', 'private', 'distributed', 'adversary', 'decentralized', 'sharing', 'untrusted', 'protected', 'ederated']","['privacy', 'federated', 'differential', 'protection', 'private', 'federator', 'leakage', 'clients', 'guarantee', 'centralized']","['privacy', 'federated', 'distributed', 'adversary', 'trusted', 'fdp', 'leakage', 'prirfed', 'quantization', 'gradients']","['  Federated learning (FL) enables multiple clients to collaboratively learn a\nshared model without sharing their individual data. Concerns about utility,\nprivacy, and training efficiency in FL have garnered significant research\nattention. Differential privacy has emerged as a prevalent technique in FL,\nsafeguarding the privacy of individual user data while impacting utility and\ntraining efficiency. Within Differential Privacy Federated Learning (DPFL),\nprevious studies have primarily focused on the utility-privacy trade-off,\nneglecting training efficiency, which is crucial for timely completion.\nMoreover, differential privacy achieves privacy by introducing controlled\nrandomness (noise) on selected clients in each communication round. Previous\nwork has mainly examined the impact of noise level ($\\sigma$) and communication\nrounds ($T$) on the privacy-utility dynamic, overlooking other influential\nfactors like the sample ratio ($q$, the proportion of selected clients). This\npaper systematically formulates an efficiency-constrained utility-privacy\nbi-objective optimization problem in DPFL, focusing on $\\sigma$, $T$, and $q$.\nWe provide a comprehensive theoretical analysis, yielding analytical solutions\nfor the Pareto front. Extensive empirical experiments verify the validity and\nefficacy of our analysis, offering valuable guidance for low-cost parameter\ndesign in DPFL.\n', '  Federated learning is gaining increasing popularity, with data heterogeneity\nand privacy being two prominent challenges. In this paper, we address both\nissues within a federated transfer learning framework, aiming to enhance\nlearning on a target data set by leveraging information from multiple\nheterogeneous source data sets while adhering to privacy constraints. We\nrigorously formulate the notion of \\textit{federated differential privacy},\nwhich offers privacy guarantees for each data set without assuming a trusted\ncentral server. Under this privacy constraint, we study three classical\nstatistical problems, namely univariate mean estimation, low-dimensional linear\nregression, and high-dimensional linear regression. By investigating the\nminimax rates and identifying the costs of privacy for these problems, we show\nthat federated differential privacy is an intermediate privacy model between\nthe well-established local and central models of differential privacy. Our\nanalyses incorporate data heterogeneity and privacy, highlighting the\nfundamental costs of both in federated learning and underscoring the benefit of\nknowledge transfer across data sets.\n', '  In recent years, privacy and security concerns in machine learning have\npromoted trusted federated learning to the forefront of research. Differential\nprivacy has emerged as the de facto standard for privacy protection in\nfederated learning due to its rigorous mathematical foundation and provable\nguarantee. Despite extensive research on algorithms that incorporate\ndifferential privacy within federated learning, there remains an evident\ndeficiency in systematic reviews that categorize and synthesize these studies.\n  Our work presents a systematic overview of the differentially private\nfederated learning. Existing taxonomies have not adequately considered objects\nand level of privacy protection provided by various differential privacy models\nin federated learning. To rectify this gap, we propose a new taxonomy of\ndifferentially private federated learning based on definition and guarantee of\nvarious differential privacy models and federated scenarios. Our classification\nallows for a clear delineation of the protected objects across various\ndifferential privacy models and their respective neighborhood levels within\nfederated learning environments. Furthermore, we explore the applications of\ndifferential privacy in federated learning scenarios. Our work provide valuable\ninsights into privacy-preserving federated learning and suggest practical\ndirections for future research.\n']",Federated Learning and Differential Privacy
370,369,19,369_grading_graded_assessments_assessment,"['grading', 'graded', 'assessments', 'assessment', 'grades', 'graders', 'grade', 'exams', 'exam', 'students']","['grading', 'rubrics', 'scoring', 'students', 'student', 'grades', 'answer', 'formative', 'short', 'ended']","['grading', 'assessments', 'marking', 'classrooms', 'automated', 'rubrics', 'raters', 'essays', 'evaluators', 'questions']","[""  We explore the use of deep reinforcement learning to audit an automatic short\nanswer grading (ASAG) model. Automatic grading may decrease the time burden of\nrating open-ended items for educators, but a lack of robust evaluation methods\nfor these models can result in uncertainty of their quality. Current\nstate-of-the-art ASAG models are configured to match human ratings from a\ntraining set, and researchers typically assess their quality with accuracy\nmetrics that signify agreement between model and human scores. In this paper,\nwe show that a high level of agreement to human ratings does not give\nsufficient evidence that an ASAG model is infallible. We train a reinforcement\nlearning agent to revise student responses with the objective of achieving a\nhigh rating from an automatic grading model in the least number of revisions.\nBy analyzing the agent's revised responses that achieve a high grade from the\nASAG model but would not be considered a high scoring responses according to a\nscoring rubric, we discover ways in which the automated grader can be\nexploited, exposing shortcomings in the grading model.\n"", ""  Large language models (LLMs) have demonstrated strong potential in performing\nautomatic scoring for constructed response assessments. While constructed\nresponses graded by humans are usually based on given grading rubrics, the\nmethods by which LLMs assign scores remain largely unclear. It is also\nuncertain how closely AI's scoring process mirrors that of humans, or if it\nadheres to the same grading criteria. To address this gap, this paper uncovers\nthe grading rubrics that LLMs used to score students' written responses to\nscience tasks and their alignment with human scores. We also examine whether\nenhancing the alignments can improve scoring accuracy. Specifically, we prompt\nLLMs to generate analytic rubrics that they use to assign scores and study the\nalignment gap with human grading rubrics. Based on a series of experiments with\nvarious configurations of LLM settings, we reveal a notable alignment gap\nbetween human and LLM graders. While LLMs can adapt quickly to scoring tasks,\nthey often resort to shortcuts, bypassing deeper logical reasoning expected in\nhuman grading. We found that incorporating high-quality analytical rubrics\ndesigned to reflect human grading logic can mitigate this gap and enhance LLMs'\nscoring accuracy. These results caution against the simplistic application of\nLLMs in science education and highlight the importance of aligning LLM outputs\nwith human expectations to ensure efficient and accurate automatic scoring.\n"", ""  While large language models (LLMs) have been used for automated grading, they\nhave not yet achieved the same level of performance as humans, especially when\nit comes to grading complex questions. Existing research on this topic focuses\non a particular step in the grading procedure: grading using predefined\nrubrics. However, grading is a multifaceted procedure that encompasses other\ncrucial steps, such as grading rubrics design and post-grading review. There\nhas been a lack of systematic research exploring the potential of LLMs to\nenhance the entire grading~process.\n  In this paper, we propose an LLM-based grading system that addresses the\nentire grading procedure, including the following key components: 1) Developing\ngrading rubrics that not only consider the questions but also the student\nanswers, which can more accurately reflect students' performance. 2) Under the\nguidance of grading rubrics, providing accurate and consistent scores for each\nstudent, along with customized feedback. 3) Conducting post-grading review to\nbetter ensure accuracy and fairness. Additionally, we collected a new dataset\nnamed OS from a university operating system course and conducted extensive\nexperiments on both our new dataset and the widely used Mohler dataset.\nExperiments demonstrate the effectiveness of our proposed approach, providing\nsome new insights for developing automated grading systems based on LLMs.\n""]",Automatic Grading and Assessment in Education
371,370,19,370_sleepfm_insomnia_sleep_sleepdg,"['sleepfm', 'insomnia', 'sleep', 'sleepdg', 'sleepppg', 'sleepedf', 'wake', 'electroencephalogram', 'sleepyco', 'apnea']","['sleep', 'staging', 'polysomnography', 'apnea', 'oxygen', 'ear', 'desaturations', 'disorders', 'desaturation', 'disorder']","['sleepfm', 'electroencephalogram', 'apnea', 'polysomnography', 'wavesleepnet', 'cortexm4', 'monitoring', 'biomarkers', 'hypnograms', 's4sleep']","['  Monitoring sleep states is essential for evaluating sleep quality and\ndiagnosing sleep disorders. Traditional manual staging is time-consuming and\nprone to subjective bias, often resulting in inconsistent outcomes. Here, we\ndeveloped an automated model for sleep staging and disorder classification to\nenhance diagnostic accuracy and efficiency. Considering the characteristics of\npolysomnography (PSG) multi-lead sleep monitoring, we designed a multimodal\nsleep state classification model, MSSC-BiMamba, that combines an Efficient\nChannel Attention (ECA) mechanism with a Bidirectional State Space Model\n(BSSM). The ECA module allows for weighting data from different sensor\nchannels, thereby amplifying the influence of diverse sensor inputs.\nAdditionally, the implementation of bidirectional Mamba (BiMamba) enables the\nmodel to effectively capture the multidimensional features and long-range\ndependencies of PSG data. The developed model demonstrated impressive\nperformance on sleep stage classification tasks on both the ISRUC-S3 and\nISRUC-S1 datasets, respectively containing data with healthy and unhealthy\nsleep patterns. Also, the model exhibited a high accuracy for sleep health\nprediction when evaluated on a combined dataset consisting of ISRUC and\nSleep-EDF. Our model, which can effectively handle diverse sleep conditions, is\nthe first to apply BiMamba to sleep staging with multimodal PSG data, showing\nsubstantial gains in computational and memory efficiency over traditional\nTransformer-style models. This method enhances sleep health management by\nmaking monitoring more accessible and extending advanced healthcare through\ninnovative technology.\n', ""  Sleep is vital for people's physical and mental health, and sound sleep can\nhelp them focus on daily activities. Therefore, a sleep study that includes\nsleep patterns and disorders is crucial to enhancing our knowledge about\nindividuals' health status. The findings on sleep stages and sleep disorders\nrelied on polysomnography and self-report measures, and then the study went\nthrough clinical assessments by expert physicians. However, the evaluation\nprocess of sleep stage classification and sleep disorder has become more\nconvenient with artificial intelligence applications and numerous\ninvestigations focusing on various datasets with advanced algorithms and\ntechniques that offer improved computational ease and accuracy. This study aims\nto provide a comprehensive, systematic review and meta-analysis of the recent\nliterature to analyze the different approaches and their outcomes in sleep\nstudies, which includes works on sleep stages classification and sleep disorder\ndetection using AI. In this review, 183 articles were initially selected from\ndifferent journals, among which 80 records were enlisted for explicit review,\nranging from 2016 to 2023. Brain waves were the most commonly employed body\nparameters for sleep staging and disorder studies. The convolutional neural\nnetwork, the most widely used of the 34 distinct artificial intelligence\nmodels, comprised 27%. The other models included the long short-term memory,\nsupport vector machine, random forest, and recurrent neural network, which\nconsisted of 11%, 6%, 6%, and 5% sequentially. For performance metrics,\naccuracy was widely used for a maximum of 83.75% of the cases, the F1 score of\n45%, Kappa of 36.25%, Sensitivity of 31.25%, and Specificity of 30% of cases,\nalong with the other metrics. This article would help physicians and\nresearchers get the gist of AI's contribution to sleep studies and the\nfeasibility of their intended work.\n"", '  Traditional sleep staging categorizes sleep and wakefulness into five\ncoarse-grained classes, overlooking subtle variations within each stage. It\nprovides limited information about the probability of arousal and may hinder\nthe diagnosis of sleep disorders, such as insomnia. To address this issue, we\npropose a deep-learning method for automatic and scalable annotation of sleep\ndepth index using existing sleep staging labels. Our approach is validated\nusing polysomnography from over ten thousand recordings across four large-scale\ncohorts. The results show a strong correlation between the decrease in sleep\ndepth index and the increase in arousal likelihood. Several case studies\nindicate that the sleep depth index captures more nuanced sleep structures than\nconventional sleep staging. Sleep biomarkers extracted from the whole-night\nsleep depth index exhibit statistically significant differences with\nmedium-to-large effect sizes across groups of varied subjective sleep quality\nand insomnia symptoms. These sleep biomarkers also promise utility in\npredicting the severity of obstructive sleep apnea, particularly in severe\ncases. Our study underscores the utility of the proposed method for continuous\nsleep depth annotation, which could reveal more detailed structures and\ndynamics within whole-night sleep and yield novel digital biomarkers beneficial\nfor sleep health.\n']",Sleep Disorders and Sleep Stage Classification
372,371,19,371_graphprompt_graphs_subgraph_graph,"['graphprompt', 'graphs', 'subgraph', 'graph', 'networks', 'pretraining', 'pretext', 'pretexts', 'nodes', 'gnn']","['prompt', 'pre', 'graph', 'pretext', 'prompting', 'downstream', 'graphs', 'tuning', 'prompts', 'tasks']","['graphprompt', 'subgraph', 'networks', 'pretexts', 'gnn', 'trained', 'autocompletion', 'prompts', 'tigs', 'tasks']","[""  Graphs have become an important modeling tool for web applications, and Graph\nNeural Networks (GNNs) have achieved great success in graph representation\nlearning. However, the performance of traditional GNNs heavily relies on a\nlarge amount of supervision. Recently, ``pre-train, fine-tune'' has become the\nparadigm to address the issues of label dependency and poor generalization.\nHowever, the pre-training strategies vary for graphs with homophily and\nheterophily, and the objectives for various downstream tasks also differ. This\nleads to a gap between pretexts and downstream tasks, resulting in ``negative\ntransfer'' and poor performance. Inspired by prompt learning in Natural\nLanguage Processing (NLP), many studies turn to bridge the gap and fully\nleverage the pre-trained model. However, existing methods for graph prompting\nare tailored to homophily, neglecting inherent heterophily on graphs.\nMeanwhile, most of them rely on the randomly initialized prompts, which\nnegatively impact on the stability. Therefore, we propose Self-Prompt, a\nprompting framework for graphs based on the model and data itself. We first\nintroduce asymmetric graph contrastive learning for pretext to address\nheterophily and align the objectives of pretext and downstream tasks. Then we\nreuse the component from pre-training phase as the self adapter and introduce\nself-prompts based on graph itself for task adaptation. Finally, we conduct\nextensive experiments on 11 benchmark datasets to demonstrate its superiority.\nWe provide our codes at https://github.com/gongchenghua/Self-Pro.\n"", '  Graphs have emerged as a natural choice to represent and analyze the\nintricate patterns and rich information of the Web, enabling applications such\nas online page classification and social recommendation. The prevailing\n""pre-train, fine-tune"" paradigm has been widely adopted in graph machine\nlearning tasks, particularly in scenarios with limited labeled nodes. However,\nthis approach often exhibits a misalignment between the training objectives of\npretext tasks and those of downstream tasks. This gap can result in the\n""negative transfer"" problem, wherein the knowledge gained from pre-training\nadversely affects performance in the downstream tasks. The surge in\nprompt-based learning within Natural Language Processing (NLP) suggests the\npotential of adapting a ""pre-train, prompt"" paradigm to graphs as an\nalternative. However, existing graph prompting techniques are tailored to\nhomogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To\nbridge this gap, we propose HetGPT, a general post-training prompting framework\nto improve the predictive performance of pre-trained heterogeneous graph neural\nnetworks (HGNNs). The key is the design of a novel prompting function that\nintegrates a virtual class prompt and a heterogeneous feature prompt, with the\naim to reformulate downstream tasks to mirror pretext tasks. Moreover, HetGPT\nintroduces a multi-view neighborhood aggregation mechanism, capturing the\ncomplex neighborhood structure in heterogeneous graphs. Extensive experiments\non three benchmark datasets demonstrate HetGPT\'s capability to enhance the\nperformance of state-of-the-art HGNNs on semi-supervised node classification.\n', '  In recent years, graph prompt learning/tuning has garnered increasing\nattention in adapting pre-trained models for graph representation learning. As\na kind of universal graph prompt learning method, Graph Prompt Feature (GPF)\nhas achieved remarkable success in adapting pre-trained models for Graph Neural\nNetworks (GNNs). By fixing the parameters of a pre-trained GNN model, the aim\nof GPF is to modify the input graph data by adding some (learnable) prompt\nvectors into graph node features to better align with the downstream tasks on\nthe smaller dataset. However, existing GPFs generally suffer from two main\nlimitations. First, GPFs generally focus on node prompt learning which ignore\nthe prompting for graph edges. Second, existing GPFs generally conduct the\nprompt learning on all nodes equally which fails to capture the importances of\ndifferent nodes and may perform sensitively w.r.t noisy nodes in aligning with\nthe downstream tasks. To address these issues, in this paper, we propose a new\nunified Graph Selective Prompt Feature learning (GSPF) for GNN fine-tuning. The\nproposed GSPF integrates the prompt learning on both graph node and edge\ntogether, which thus provides a unified prompt model for the graph data.\nMoreover, it conducts prompt learning selectively on nodes and edges by\nconcentrating on the important nodes and edges for prompting which thus make\nour model be more reliable and compact. Experimental results on many benchmark\ndatasets demonstrate the effectiveness and advantages of the proposed GSPF\nmethod.\n']",Graph Neural Networks and Prompt Learning
373,372,19,372_crowdsourcing_crowdsourced_crowdworker_crowdworkers,"['crowdsourcing', 'crowdsourced', 'crowdworker', 'crowdworkers', 'crowdshipping', 'annotators', 'crowds', 'annotations', 'annotation', 'annotator']","['crowdsourcing', 'workers', 'worker', 'annotation', 'crowdworkers', 'truth', 'labels', 'vote', 'annotations', 'labeling']","['crowdsourced', 'crowdworker', 'crowdshipping', 'annotations', 'datasets', 'biases', 'turk', 'labelaid', 'assignments', 'aggregation']","['  Whether Large Language Models (LLMs) can outperform crowdsourcing on the data\nannotation task is attracting interest recently. Some works verified this issue\nwith the average performance of individual crowd workers and LLM workers on\nsome specific NLP tasks by collecting new datasets. However, on the one hand,\nexisting datasets for the studies of annotation quality in crowdsourcing are\nnot yet utilized in such evaluations, which potentially provide reliable\nevaluations from a different viewpoint. On the other hand, the quality of these\naggregated labels is crucial because, when utilizing crowdsourcing, the\nestimated labels aggregated from multiple crowd labels to the same instances\nare the eventually collected labels. Therefore, in this paper, we first\ninvestigate which existing crowdsourcing datasets can be used for a comparative\nstudy and create a benchmark. We then compare the quality between individual\ncrowd labels and LLM labels and make the evaluations on the aggregated labels.\nIn addition, we propose a Crowd-LLM hybrid label aggregation method and verify\nthe performance. We find that adding LLM labels from good LLMs to existing\ncrowdsourcing datasets can enhance the quality of the aggregated labels of the\ndatasets, which is also higher than the quality of LLM labels themselves.\n', '  Annotation through crowdsourcing draws incremental attention, which relies on\nan effective selection scheme given a pool of workers. Existing methods propose\nto select workers based on their performance on tasks with ground truth, while\ntwo important points are missed. 1) The historical performances of workers in\nother tasks. In real-world scenarios, workers need to solve a new task whose\ncorrelation with previous tasks is not well-known before the training, which is\ncalled cross-domain. 2) The dynamic worker performance as workers will learn\nfrom the ground truth. In this paper, we consider both factors in designing an\nallocation scheme named cross-domain-aware worker selection with training\napproach. Our approach proposes two estimation modules to both statistically\nanalyze the cross-domain correlation and simulate the learning gain of workers\ndynamically. A framework with a theoretical analysis of the worker elimination\nprocess is given. To validate the effectiveness of our methods, we collect two\nnovel real-world datasets and generate synthetic datasets. The experiment\nresults show that our method outperforms the baselines on both real-world and\nsynthetic datasets.\n', ""  For the purpose of efficient and cost-effective large-scale data labeling,\ncrowdsourcing is increasingly being utilized. To guarantee the quality of data\nlabeling, multiple annotations need to be collected for each data sample, and\ntruth inference algorithms have been developed to accurately infer the true\nlabels. Despite previous studies having released public datasets to evaluate\nthe efficacy of truth inference algorithms, these have typically focused on a\nsingle type of crowdsourcing task and neglected the temporal information\nassociated with workers' annotation activities. These limitations significantly\nrestrict the practical applicability of these algorithms, particularly in the\ncontext of long-term and online truth inference. In this paper, we introduce a\nsubstantial crowdsourcing annotation dataset collected from a real-world\ncrowdsourcing platform. This dataset comprises approximately two thousand\nworkers, one million tasks, and six million annotations. The data was gathered\nover a period of approximately six months from various types of tasks, and the\ntimestamps of each annotation were preserved. We analyze the characteristics of\nthe dataset from multiple perspectives and evaluate the effectiveness of\nseveral representative truth inference algorithms on this dataset. We\nanticipate that this dataset will stimulate future research on tracking\nworkers' abilities over time in relation to different types of tasks, as well\nas enhancing online truth inference.\n""]",Crowdsourcing and Data Annotation
374,373,19,373_neural_brains_brain_fmri,"['neural', 'brains', 'brain', 'fmri', 'cognitive', 'language', 'similarity', 'representational', 'comprehension', 'representations']","['brain', 'cognitive', 'alignment', 'processing', 'similarity', 'language', 'encoding', 'activity', 'representations', 'human']","['fmri', 'cognitive', 'similarity', 'comprehension', 'representations', 'iq', 'brainscores', 'auditory', 'syntactic', 'untrained']","['  Instruction-tuning is a widely adopted finetuning method that enables large\nlanguage models (LLMs) to generate output that more closely resembles human\nresponses. However, no studies have shown that instruction-tuning actually\nteaches LLMs to process language in a similar manner as humans. We investigate\nthe effect of instruction-tuning on aligning LLM and human language processing\nmechanisms in two ways: (1) brain alignment, the similarity of LLM internal\nrepresentations to neural activity in the human language system, and (2)\nbehavioral alignment, the similarity of LLM and human behavior on a reading\ntask. We assess 25 vanilla and instruction-tuned LLMs on three datasets\ninvolving humans reading naturalistic stories and sentences, and find that\ninstruction-tuning generally enhances brain alignment (~6%), but has no similar\neffect on behavioral alignment. To identify factors underlying this improvement\nin brain alignment, we compute correlations between brain alignment and various\nLLM properties, such as model size, problem-solving, and world knowledge\nunderstanding. Notably, we find a strong positive correlation between brain\nalignment and model size (r = 0.95), as well as performance on tasks requiring\nworld knowledge (r = 0.81). Our results demonstrate that instruction-tuning\nLLMs improves both world knowledge representations and brain alignment,\nsuggesting that the mechanisms that encode world knowledge in LLMs also improve\nrepresentational alignment to the human brain.\n', ""  Large Language Models (LLMs) have been shown to be effective models of the\nhuman language system, with some models predicting most explainable variance of\nbrain activity in current datasets. Even in untrained models, the\nrepresentations induced by architectural priors can exhibit reasonable\nalignment to brain data. In this work, we investigate the key architectural\ncomponents driving the surprising alignment of untrained models. To estimate\nLLM-to-brain similarity, we first select language-selective units within an\nLLM, similar to how neuroscientists identify the language network in the human\nbrain. We then benchmark the brain alignment of these LLM units across five\ndifferent brain recording datasets. By isolating critical components of the\nTransformer architecture, we identify tokenization strategy and multihead\nattention as the two major components driving brain alignment. A simple form of\nrecurrence further improves alignment. We further demonstrate this quantitative\nbrain alignment of our model by reproducing landmark studies in the language\nneuroscience field, showing that localized model units -- just like language\nvoxels measured empirically in the human brain -- discriminate more reliably\nbetween lexical than syntactic differences, and exhibit similar response\nprofiles under the same experimental conditions. Finally, we demonstrate the\nutility of our model's representations for language modeling, achieving\nimproved sample and parameter efficiency over comparable architectures. Our\nmodel's estimates of surprisal sets a new state-of-the-art in the behavioral\nalignment to human reading times. Taken together, we propose a highly brain-\nand behaviorally-aligned model that conceptualizes the human language system as\nan untrained shallow feature encoder, with structural priors, combined with a\ntrained decoder to achieve efficient and performant language processing.\n"", '  Large Language Models (LLMs) have demonstrated remarkable abilities in text\ncomprehension and logical reasoning, indicating that the text representations\nlearned by LLMs can facilitate their language processing capabilities. In\ncognitive science, brain cognitive processing signals are typically utilized to\nstudy human language processing. Therefore, it is natural to ask how well the\ntext embeddings from LLMs align with the brain cognitive processing signals,\nand how training strategies affect the LLM-brain alignment? In this paper, we\nemploy Representational Similarity Analysis (RSA) to measure the alignment\nbetween 23 mainstream LLMs and fMRI signals of the brain to evaluate how\neffectively LLMs simulate cognitive language processing. We empirically\ninvestigate the impact of various factors (e.g., pre-training data size, model\nscaling, alignment training, and prompts) on such LLM-brain alignment.\nExperimental results indicate that pre-training data size and model scaling are\npositively correlated with LLM-brain similarity, and alignment training can\nsignificantly improve LLM-brain similarity. Explicit prompts contribute to the\nconsistency of LLMs with brain cognitive language processing, while nonsensical\nnoisy prompts may attenuate such alignment. Additionally, the performance of a\nwide range of LLM evaluations (e.g., MMLU, Chatbot Arena) is highly correlated\nwith the LLM-brain similarity.\n']",Brain-Inspired Language Models
375,374,19,374_tensors_tensor_sparse_compilers,"['tensors', 'tensor', 'sparse', 'compilers', 'compiler', 'pytorch', 'optimized', 'cores', 'implementations', 'graphcore']","['tensor', 'compiler', 'sparse', 'operators', 'operations', 'optimizations', 'hardware', 'tensors', 'sparsity', 'algebra']","['tensors', 'sparse', 'compilers', 'pytorch', 'optimized', 'cores', 'graphcore', 'intel', 'gpus', 'batching']","['  Kernel orchestration is the task of mapping the computation defined in\ndifferent operators of a deep neural network (DNN) to the execution of GPU\nkernels on modern hardware platforms. Prior approaches optimize kernel\norchestration by greedily applying operator fusion, which fuses the computation\nof multiple operators into a single kernel, and miss a variety of optimization\nopportunities in kernel orchestration.\n  This paper presents Korch, a tensor program optimizer that discovers optimal\nkernel orchestration strategies for tensor programs. Instead of directly fusing\noperators, Korch first applies operator fission to decompose tensor operators\ninto a small set of basic tensor algebra primitives. This decomposition enables\na diversity of fine-grained, inter-operator optimizations. Next, Korch\noptimizes kernel orchestration by formalizing it as a constrained optimization\nproblem, leveraging an off-the-shelf binary linear programming solver to\ndiscover an optimal orchestration strategy, and generating an executable that\ncan be directly deployed on modern GPU platforms. Evaluation on a variety of\nDNNs shows that Korch outperforms existing tensor program optimizers by up to\n1.7x on V100 GPUs and up to 1.6x on A100 GPUs. Korch is publicly available at\nhttps://github.com/humuyan/Korch.\n', ""  The rapid growth in the size of deep learning models strains the capabilities\nof traditional dense computation paradigms. Leveraging sparse computation has\nbecome increasingly popular for training and deploying large-scale models, but\nexisting deep learning frameworks lack extensive support for sparse operations.\nTo bridge this gap, we introduce Scorch, a library that seamlessly integrates\nefficient sparse tensor computation into the PyTorch ecosystem, with an initial\nfocus on inference workloads on CPUs. Scorch provides a flexible and intuitive\ninterface for sparse tensors, supporting diverse sparse data structures. Scorch\nintroduces a compiler stack that automates key optimizations, including\nautomatic loop ordering, tiling, and format inference. Combined with a runtime\nthat adapts its execution to both dense and sparse data, Scorch delivers\nsubstantial speedups over hand-written PyTorch Sparse (torch.sparse) operations\nwithout sacrificing usability. More importantly, Scorch enables efficient\ncomputation of complex sparse operations that lack hand-optimized PyTorch\nimplementations. This flexibility is crucial for exploring novel sparse\narchitectures. We demonstrate Scorch's ease of use and performance gains on\ndiverse deep learning models across multiple domains. With only minimal code\nchanges, Scorch achieves 1.05-5.78x speedups over PyTorch Sparse on end-to-end\ntasks. Scorch's seamless integration and performance gains make it a valuable\naddition to the PyTorch ecosystem. We believe Scorch will enable wider\nexploration of sparsity as a tool for scaling deep learning and inform the\ndevelopment of other sparse libraries.\n"", '  With the rapid development of deep learning models and hardware support for\ndense computing, the deep learning workload characteristics changed\nsignificantly from a few hot spots on compute-intensive operations to a broad\nrange of operations scattered across the models. Accelerating a few\ncompute-intensive operations using the expert-tuned implementation of\nprimitives does not fully exploit the performance potential of AI hardware.\nVarious efforts have been made to compile a full deep neural network (DNN)\ngraph. One of the biggest challenges is to achieve high-performance tensor\ncompilation by generating expert level performance code for the dense\ncompute-intensive operations and applying compilation optimization at the scope\nof DNN computation graph across multiple compute-intensive operations.\n  We present oneDNN Graph Compiler, a tensor compiler that employs a hybrid\napproach of using techniques from both compiler optimization and expert-tuned\nkernels for high performance code generation of the deep neural network graph.\noneDNN Graph Compiler addresses unique optimization challenges in the deep\nlearning domain, such as low-precision computation, aggressive fusion of graph\noperations, optimization for static tensor shapes and memory layout, constant\nweight optimization, and memory buffer reuse. Experimental results demonstrate\nsignificant performance gains over existing tensor compiler and primitives\nlibrary for performance-critical DNN computation graphs and end-to-end models\non Intel Xeon Scalable Processors.\n']",Optimizing Deep Learning with Tensor Compilers
376,375,19,375_clustering_softmax_predictions_cluster_online_hard_clustering_clustering,"['clustering_softmax_predictions', 'cluster', 'online_hard_clustering', 'clustering', 'clusters', 'supervised', 'embedding', 'deep', 'unsupervised', 'autoencoder']","['clustering', 'cluster', 'deep', 'assignments', 'tabular', 'autoencoder', 'clusters', 'retina', 'unsupervised', 'functional']","['clustering_softmax_predictions', 'cluster', 'embedding', 'unsupervised', 'autoencoder', 'imagenet21k', 'vectorizing', 'centroids', 'macaque', 'retina']","['  Distribution learning finds probability density functions from a set of data\nsamples, whereas clustering aims to group similar data points to form clusters.\nAlthough there are deep clustering methods that employ distribution learning\nmethods, past work still lacks theoretical analysis regarding the relationship\nbetween clustering and distribution learning. Thus, in this work, we provide a\ntheoretical analysis to guide the optimization of clustering via distribution\nlearning. To achieve better results, we embed deep clustering guided by a\ntheoretical analysis. Furthermore, the distribution learning method cannot\nalways be directly applied to data. To overcome this issue, we introduce a\nclustering-oriented distribution learning method called Monte-Carlo\nMarginalization for Clustering. We integrate Monte-Carlo Marginalization for\nClustering into Deep Clustering, resulting in Deep Clustering via Distribution\nLearning (DCDL). Eventually, the proposed DCDL achieves promising results\ncompared to state-of-the-art methods on popular datasets. Considering a\nclustering task, the new distribution learning method outperforms previous\nmethods as well.\n', '  In the face of complex natural images, existing deep clustering algorithms\nfall significantly short in terms of clustering accuracy when compared to\nsupervised classification methods, making them less practical. This paper\nintroduces an image clustering algorithm based on self-supervised pretrained\nmodels and latent feature distribution optimization, substantially enhancing\nclustering performance. It is found that: (1) For complex natural images, we\neffectively enhance the discriminative power of latent features by leveraging\nself-supervised pretrained models and their fine-tuning, resulting in improved\nclustering performance. (2) In the latent feature space, by searching for\nk-nearest neighbor images for each training sample and shortening the distance\nbetween the training sample and its nearest neighbor, the discriminative power\nof latent features can be further enhanced, and clustering performance can be\nimproved. (3) In the latent feature space, reducing the distance between sample\nfeatures and the nearest predefined cluster centroids can optimize the\ndistribution of latent features, therefore further improving clustering\nperformance. Through experiments on multiple datasets, our approach outperforms\nthe latest clustering algorithms and achieves state-of-the-art clustering\nresults. When the number of categories in the datasets is small, such as\nCIFAR-10 and STL-10, and there are significant differences between categories,\nour clustering algorithm has similar accuracy to supervised methods without\nusing pretrained models, slightly lower than supervised methods using\npre-trained models. The code linked algorithm is\nhttps://github.com/LihengHu/semi.\n', '  Deep clustering methods improve the performance of clustering tasks by\njointly optimizing deep representation learning and clustering. While numerous\ndeep clustering algorithms have been proposed, most of them rely on\nartificially constructed pseudo targets for performing clustering. This\nconstruction process requires some prior knowledge, and it is challenging to\ndetermine a suitable pseudo target for clustering. To address this issue, we\npropose a deep embedding clustering algorithm driven by sample stability\n(DECS), which eliminates the requirement of pseudo targets. Specifically, we\nstart by constructing the initial feature space with an autoencoder and then\nlearn the cluster-oriented embedding feature constrained by sample stability.\nThe sample stability aims to explore the deterministic relationship between\nsamples and all cluster centroids, pulling samples to their respective clusters\nand keeping them away from other clusters with high determinacy. We analyzed\nthe convergence of the loss using Lipschitz continuity in theory, which\nverifies the validity of the model. The experimental results on five datasets\nillustrate that the proposed method achieves superior performance compared to\nstate-of-the-art clustering approaches.\n']",Deep Clustering Methods
377,376,19,376_malicious_recommender_adversary_adversarial,"['malicious', 'recommender', 'adversary', 'adversarial', 'recommenders', 'attacks', 'vulnerabilities', 'threats', 'attacker', 'attackers']","['recommender', 'poisoning', 'attacks', 'attack', 'item', 'robustness', 'systems', 'malicious', 'items', 'attackers']","['malicious', 'recommender', 'adversary', 'adversarial', 'vulnerabilities', 'fraudsters', 'targeted', 'profiles', 'poisonfrs', 'uprank']","[""  Modern recommender systems (RS) have profoundly enhanced user experience\nacross digital platforms, yet they face significant threats from poisoning\nattacks. These attacks, aimed at manipulating recommendation outputs for\nunethical gains, exploit vulnerabilities in RS through injecting malicious data\nor intervening model training. This survey presents a unique perspective by\nexamining these threats through the lens of an attacker, offering fresh\ninsights into their mechanics and impacts. Concretely, we detail a systematic\npipeline that encompasses four stages of a poisoning attack: setting attack\ngoals, assessing attacker capabilities, analyzing victim architecture, and\nimplementing poisoning strategies. The pipeline not only aligns with various\nattack tactics but also serves as a comprehensive taxonomy to pinpoint focuses\nof distinct poisoning attacks. Correspondingly, we further classify defensive\nstrategies into two main categories: poisoning data filtering and robust\ntraining from the defender's perspective. Finally, we highlight existing\nlimitations and suggest innovative directions for further exploration in this\nfield.\n"", ""  To make room for privacy and efficiency, the deployment of many recommender\nsystems is experiencing a shift from central servers to personal devices, where\nthe federated recommender systems (FedRecs) and decentralized collaborative\nrecommender systems (DecRecs) are arguably the two most representative\nparadigms. While both leverage knowledge (e.g., gradients) sharing to\nfacilitate learning local models, FedRecs rely on a central server to\ncoordinate the optimization process, yet in DecRecs, the knowledge sharing\ndirectly happens between clients. Knowledge sharing also opens a backdoor for\nmodel poisoning attacks, where adversaries disguise themselves as benign\nclients and disseminate polluted knowledge to achieve malicious goals like\npromoting an item's exposure rate. Although research on such poisoning attacks\nprovides valuable insights into finding security loopholes and corresponding\ncountermeasures, existing attacks mostly focus on FedRecs, and are either\ninapplicable or ineffective for DecRecs. Compared with FedRecs where the\ntampered information can be universally distributed to all clients once\nuploaded to the cloud, each adversary in DecRecs can only communicate with\nneighbor clients of a small size, confining its impact to a limited range. To\nfill the gap, we present a novel attack method named Poisoning with Adaptive\nMalicious Neighbors (PAMN). With item promotion in top-K recommendation as the\nattack objective, PAMN effectively boosts target items' ranks with several\nadversaries that emulate benign clients and transfers adaptively crafted\ngradients conditioned on each adversary's neighbors. Moreover, with the\nvulnerabilities of DecRecs uncovered, a dedicated defensive mechanism based on\nuser-level gradient clipping with sparsified updating is proposed. Extensive\nexperiments demonstrate the effectiveness of the poisoning attack and the\nrobustness of our defensive mechanism.\n"", ""  Recommender systems have become an integral part of online services to help\nusers locate specific information in a sea of data. However, existing studies\nshow that some recommender systems are vulnerable to poisoning attacks,\nparticularly those that involve learning schemes. A poisoning attack is where\nan adversary injects carefully crafted data into the process of training a\nmodel, with the goal of manipulating the system's final recommendations. Based\non recent advancements in artificial intelligence, such attacks have gained\nimportance recently. While numerous countermeasures to poisoning attacks have\nbeen developed, they have not yet been systematically linked to the properties\nof the attacks. Consequently, assessing the respective risks and potential\nsuccess of mitigation strategies is difficult, if not impossible. This survey\naims to fill this gap by primarily focusing on poisoning attacks and their\ncountermeasures. This is in contrast to prior surveys that mainly focus on\nattacks and their detection methods. Through an exhaustive literature review,\nwe provide a novel taxonomy for poisoning attacks, formalise its dimensions,\nand accordingly organise 30+ attacks described in the literature. Further, we\nreview 40+ countermeasures to detect and/or prevent poisoning attacks,\nevaluating their effectiveness against specific types of attacks. This\ncomprehensive survey should serve as a point of reference for protecting\nrecommender systems against poisoning attacks. The article concludes with a\ndiscussion on open issues in the field and impactful directions for future\nresearch. A rich repository of resources associated with poisoning attacks is\navailable at https://github.com/tamlhp/awesome-recsys-poisoning.\n""]","""Adversarial Attacks on Recommender Systems"""
378,377,19,377_outlier_outliers_outlierness_inlier,"['outlier', 'outliers', 'outlierness', 'inlier', 'inliers', 'anomaly', 'supervised', 'robust', 'unsupervised', 'detection']","['outlier', 'outliers', 'inliers', 'inlier', 'detection', 'negative', 'neighbors', 'class', 'likelihood', 'nearest']","['outlierness', 'unsupervised', 'detection', 'view', 'ensemble', 'underfilter', 'dimensionality', 'likelihood', 'neighbor', 'distort']","['  Graph outlier detection is a prominent task of research and application in\nthe realm of graph neural networks. It identifies the outlier nodes that\nexhibit deviation from the majority in the graph. One of the fundamental\nchallenges confronting supervised graph outlier detection algorithms is the\nprevalent issue of class imbalance, where the scarcity of outlier instances\ncompared to normal instances often results in suboptimal performance.\nConventional methods mitigate the imbalance by reweighting instances in the\nestimation of the loss function, assigning higher weights to outliers and lower\nweights to inliers. Nonetheless, these strategies are prone to overfitting and\nunderfitting, respectively. Recently, generative models, especially diffusion\nmodels, have demonstrated their efficacy in synthesizing high-fidelity images.\nDespite their extraordinary generation quality, their potential in data\naugmentation for supervised graph outlier detection remains largely\nunderexplored.\n  To bridge this gap, we introduce GODM, a novel data augmentation for\nmitigating class imbalance in supervised Graph Outlier detection with latent\nDiffusion Models. Specifically, our proposed method consists of three key\ncomponents: (1) Variantioanl Encoder maps the heterogeneous information\ninherent within the graph data into a unified latent space. (2) Graph Generator\nsynthesizes graph data that are statistically similar to real outliers from\nlatent space, and (3) Latent Diffusion Model learns the latent space\ndistribution of real organic data by iterative denoising. Extensive experiments\nconducted on multiple datasets substantiate the effectiveness and efficiency of\nGODM. The case study further demonstrated the generation quality of our\nsynthetic data. To foster accessibility and reproducibility, we encapsulate\nGODM into a plug-and-play package and release it at the Python Package Index\n(PyPI).\n', '  Discriminative learning effectively predicts true object class for image\nclassification. However, it often results in false positives for outliers,\nposing critical concerns in applications like autonomous driving and video\nsurveillance systems. Previous attempts to address this challenge involved\ntraining image classifiers through contrastive learning using actual outlier\ndata or synthesizing outliers for self-supervised learning. Furthermore,\nunsupervised generative modeling of inliers in pixel space has shown limited\nsuccess for outlier detection. In this work, we introduce a quantile-based\nmaximum likelihood objective for learning the inlier distribution to improve\nthe outlier separation during inference. Our approach fits a normalizing flow\nto pre-trained discriminative features and detects the outliers according to\nthe evaluated log-likelihood. The experimental evaluation demonstrates the\neffectiveness of our method as it surpasses the performance of the\nstate-of-the-art unsupervised methods for outlier detection. The results are\nalso competitive compared with a recent self-supervised approach for outlier\ndetection. Our work allows to reduce dependency on well-sampled negative\ntraining data, which is especially important for domains like medical\ndiagnostics or remote sensing.\n', '  In recent years, multi-view outlier detection (MVOD) methods have advanced\nsignificantly, aiming to identify outliers within multi-view datasets. A key\npoint is to better detect class outliers and class-attribute outliers, which\nonly exist in multi-view data. However, existing methods either is not able to\nreduce the impact of outliers when learning view-consistent information, or\nstruggle in cases with varying neighborhood structures. Moreover, most of them\ndo not apply to partial multi-view data in real-world scenarios. To overcome\nthese drawbacks, we propose a novel method named Regularized Contrastive\nPartial Multi-view Outlier Detection (RCPMOD). In this framework, we utilize\ncontrastive learning to learn view-consistent information and distinguish\noutliers by the degree of consistency. Specifically, we propose (1) An\noutlier-aware contrastive loss with a potential outlier memory bank to\neliminate their bias motivated by a theoretical analysis. (2) A neighbor\nalignment contrastive loss to capture the view-shared local structural\ncorrelation. (3) A spreading regularization loss to prevent the model from\noverfitting over outliers. With the Cross-view Relation Transfer technique, we\ncould easily impute the missing view samples based on the features of\nneighbors. Experimental results on four benchmark datasets demonstrate that our\nproposed approach could outperform state-of-the-art competitors under different\nsettings.\n']",Outlier Detection Methods
379,378,19,378_saliency_salient_imagenet_explanations,"['saliency', 'salient', 'imagenet', 'explanations', 'recognition', 'ai', 'classificationmetricsforimageexplanations', 'explainability', 'backpropagation', 'visual']","['saliency', 'maps', 'explanation', 'prototypical', 'interpretability', 'explanations', 'parts', 'gradient', 'interpretation', 'gradients']","['saliency', 'imagenet', 'ai', 'classificationmetricsforimageexplanations', 'gradients', 'interpretability', 'cam', 'attributions', 'lucidppn', 'guided']","['  Deep learning models have performed well on many NLP tasks. However, their\ninternal mechanisms are typically difficult for humans to understand. The\ndevelopment of methods to explain models has become a key issue in the\nreliability of deep learning models in many important applications. Various\nsaliency explanation methods, which give each feature of input a score\nproportional to the contribution of output, have been proposed to determine the\npart of the input which a model values most. Despite a considerable body of\nwork on the evaluation of saliency methods, whether the results of various\nevaluation metrics agree with human cognition remains an open question. In this\nstudy, we propose a new human-based method to evaluate saliency methods in NLP\nby crowdsourcing. We recruited 800 crowd workers and empirically evaluated\nseven saliency methods on two datasets with the proposed method. We analyzed\nthe performance of saliency methods, compared our results with existing\nautomated evaluation methods, and identified notable differences between NLP\nand computer vision (CV) fields when using saliency methods. The instance-level\ndata of our crowdsourced experiments and the code to reproduce the explanations\nare available at https://github.com/xtlu/lreccoling_evaluation.\n', '  Gradient-based saliency maps are widely used to explain deep neural network\ndecisions. However, as models become deeper and more black-box, such as in\nclosed-source APIs like ChatGPT, computing gradients become challenging,\nhindering conventional explanation methods. In this work, we introduce a novel\nunified framework for estimating gradients in black-box settings and generating\nsaliency maps to interpret model decisions. We employ the likelihood ratio\nmethod to estimate output-to-input gradients and utilize them for saliency map\ngeneration. Additionally, we propose blockwise computation techniques to\nenhance estimation accuracy. Extensive experiments in black-box settings\nvalidate the effectiveness of our method, demonstrating accurate gradient\nestimation and explainability of generated saliency maps. Furthermore, we\nshowcase the scalability of our approach by applying it to explain GPT-Vision,\nrevealing the continued relevance of gradient-based explanation methods in the\nera of large, closed-source, and black-box models.\n', '  Input gradients have a pivotal role in a variety of applications, including\nadversarial attack algorithms for evaluating model robustness, explainable AI\ntechniques for generating Saliency Maps, and counterfactual\nexplanations.However, Saliency Maps generated by traditional neural networks\nare often noisy and provide limited insights. In this paper, we demonstrate\nthat, on the contrary, the Saliency Maps of 1-Lipschitz neural networks,\nlearned with the dual loss of an optimal transportation problem, exhibit\ndesirable XAI properties:They are highly concentrated on the essential parts of\nthe image with low noise, significantly outperforming state-of-the-art\nexplanation approaches across various models and metrics. We also prove that\nthese maps align unprecedentedly well with human explanations on ImageNet.To\nexplain the particularly beneficial properties of the Saliency Map for such\nmodels, we prove this gradient encodes both the direction of the transportation\nplan and the direction towards the nearest adversarial attack. Following the\ngradient down to the decision boundary is no longer considered an adversarial\nattack, but rather a counterfactual explanation that explicitly transports the\ninput from one class to another. Thus, Learning with such a loss jointly\noptimizes the classification objective and the alignment of the gradient, i.e.\nthe Saliency Map, to the transportation plan direction.These networks were\npreviously known to be certifiably robust by design, and we demonstrate that\nthey scale well for large problems and models, and are tailored for\nexplainability using a fast and straightforward method.\n']",Explainability in Deep Learning Models
380,379,19,379_bots_botnet_bot_botartist,"['bots', 'botnet', 'bot', 'botartist', 'adversarial', 'sebot', 'twibot', 'botsscl', 'tweets', 'twitter']","['bot', 'bots', 'social', 'accounts', 'detection', 'media', 'content', 'disinformation', 'profile', 'profiles']","['botnet', 'sebot', 'tweets', 'botdgt', 'scamspot', 'profiles', 'influencer', 'gettr', 'campaigns', 'subgraph']","[""  Recent advancements in social bot detection have been driven by the adoption\nof Graph Neural Networks. The social graph, constructed from social network\ninteractions, contains benign and bot accounts that influence each other.\nHowever, previous graph-based detection methods that follow the transductive\nmessage-passing paradigm may not fully utilize hidden graph information and are\nvulnerable to adversarial bot behavior. The indiscriminate message passing\nbetween nodes from different categories and communities results in excessively\nhomogeneous node representations, ultimately reducing the effectiveness of\nsocial bot detectors. In this paper, we propose SEBot, a novel multi-view\ngraph-based contrastive learning-enabled social bot detector. In particular, we\nuse structural entropy as an uncertainty metric to optimize the entire graph's\nstructure and subgraph-level granularity, revealing the implicitly existing\nhierarchical community structure. And we design an encoder to enable message\npassing beyond the homophily assumption, enhancing robustness to adversarial\nbehaviors of social bots. Finally, we employ multi-view contrastive learning to\nmaximize mutual information between different views and enhance the detection\nperformance through multi-task learning. Experimental results demonstrate that\nour approach significantly improves the performance of social bot detection\ncompared with SOTA methods.\n"", '  Social bots remain a major vector for spreading disinformation on social\nmedia and a menace to the public. Despite the progress made in developing\nmultiple sophisticated social bot detection algorithms and tools, bot detection\nremains a challenging, unsolved problem that is fraught with uncertainty due to\nthe heterogeneity of bot behaviors, training data, and detection algorithms.\nDetection models often disagree on whether to label the same account as bot or\nhuman-controlled. However, they do not provide any measure of uncertainty to\nindicate how much we should trust their results. We propose to address both bot\ndetection and the quantification of uncertainty at the account level - a novel\nfeature of this research. This dual focus is crucial as it allows us to\nleverage additional information related to the quantified uncertainty of each\nprediction, thereby enhancing decision-making and improving the reliability of\nbot classifications. Specifically, our approach facilitates targeted\ninterventions for bots when predictions are made with high confidence and\nsuggests caution (e.g., gathering more data) when predictions are uncertain.\n', '  Social bots play a significant role in many online social networks (OSN) as\nthey imitate human behavior. This fact raises difficult questions about their\ncapabilities and potential risks. Given the recent advances in Generative AI\n(GenAI), social bots are capable of producing highly realistic and complex\ncontent that mimics human creativity. As the malicious social bots emerge to\ndeceive people with their unrealistic content, identifying them and\ndistinguishing the content they produce has become an actual challenge for\nnumerous social platforms. Several approaches to this problem have already been\nproposed in the literature, but the proposed solutions have not been widely\nevaluated. To address this issue, we evaluate the behavior of a text-based bot\ndetector in a competitive environment where some scenarios are proposed:\n\\textit{First}, the tug-of-war between a bot and a bot detector is examined. It\nis interesting to analyze which party is more likely to prevail and which\ncircumstances influence these expectations. In this regard, we model the\nproblem as a synthetic adversarial game in which a conversational bot and a bot\ndetector are engaged in strategic online interactions. \\textit{Second}, the bot\ndetection model is evaluated under attack examples generated by a social bot;\nto this end, we poison the dataset with attack examples and evaluate the model\nperformance under this condition. \\textit{Finally}, to investigate the impact\nof the dataset, a cross-domain analysis is performed. Through our comprehensive\nevaluation of different categories of social bots using two benchmark datasets,\nwe were able to demonstrate some achivement that could be utilized in future\nworks.\n']",Social Bot Detection and Analysis
381,380,18,380_lanesegnet_lanes_lane_driving,"['lanesegnet', 'lanes', 'lane', 'driving', 'road', 'trafficvlm', 'vehicles', 'autonomous', 'vehicle', 'maps']","['lane', 'perception', 'maps', 'vehicles', 'map', 'driving', 'interruption', 'camera', 'cooperative', 'ego']","['lanesegnet', 'vehicles', 'hdmaps', 'traffic', 'view', 'gps', 'detection', 'pedestrian', 'accidentblip2', 'maptrv2']","['  As an emerging task that integrates perception and reasoning, topology\nreasoning in autonomous driving scenes has recently garnered widespread\nattention. However, existing work often emphasizes ""perception over reasoning"":\nthey typically boost reasoning performance by enhancing the perception of lanes\nand directly adopt MLP to learn lane topology from lane query. This paradigm\noverlooks the geometric features intrinsic to the lanes themselves and are\nprone to being influenced by inherent endpoint shifts in lane detection.\n  To tackle this issue, we propose an interpretable method for lane topology\nreasoning based on lane geometric distance and lane query similarity, named\nTopoLogic.\n  This method mitigates the impact of endpoint shifts in geometric space, and\nintroduces explicit similarity calculation in semantic space as a complement.\nBy integrating results from both spaces, our methods provides more\ncomprehensive information for lane topology.\n  Ultimately, our approach significantly outperforms the existing\nstate-of-the-art methods on the mainstream benchmark OpenLane-V2 (23.9 v.s.\n10.9 in TOP$_{ll}$ and 44.1 v.s. 39.8 in OLS on subset_A. Additionally, our\nproposed geometric distance topology reasoning method can be incorporated into\nwell-trained models without re-training, significantly boost the performance of\nlane topology reasoning. The code is released at\nhttps://github.com/Franpin/TopoLogic.\n', '  We present MM-AU, a novel dataset for Multi-Modal Accident video\nUnderstanding. MM-AU contains 11,727 in-the-wild ego-view accident videos, each\nwith temporally aligned text descriptions. We annotate over 2.23 million object\nboxes and 58,650 pairs of video-based accident reasons, covering 58 accident\ncategories. MM-AU supports various accident understanding tasks, particularly\nmultimodal video diffusion to understand accident cause-effect chains for safe\ndriving. With MM-AU, we present an Abductive accident Video understanding\nframework for Safe Driving perception (AdVersa-SD). AdVersa-SD performs video\ndiffusion via an Object-Centric Video Diffusion (OAVD) method which is driven\nby an abductive CLIP model. This model involves a contrastive interaction loss\nto learn the pair co-occurrence of normal, near-accident, accident frames with\nthe corresponding text descriptions, such as accident reasons, prevention\nadvice, and accident categories. OAVD enforces the causal region learning while\nfixing the content of the original frame background in video generation, to\nfind the dominant cause-effect chain for certain accidents. Extensive\nexperiments verify the abductive ability of AdVersa-SD and the superiority of\nOAVD against the state-of-the-art diffusion models. Additionally, we provide\ncareful benchmark evaluations for object detection and accident reason\nanswering since AdVersa-SD relies on precise object and accident reason\ninformation.\n', '  Lane detection is a vital task for vehicles to navigate and localize their\nposition on the road. To ensure reliable driving, lane detection models must\nhave robust generalization performance in various road environments. However,\ndespite the advanced performance in the trained domain, their generalization\nperformance still falls short of expectations due to the domain discrepancy. To\nbridge this gap, we propose a novel generative framework using HD Maps for\nSingle-Source Domain Generalization (SSDG) in lane detection. We first generate\nnumerous front-view images from lane markings of HD Maps. Next, we\nstrategically select a core subset among the generated images using (i) lane\nstructure and (ii) road surrounding criteria to maximize their diversity. In\nthe end, utilizing this core set, we train lane detection models to boost their\ngeneralization performance. We validate that our generative framework from HD\nMaps outperforms the Domain Adaptation model MLDA with +3.01%p accuracy\nimprovement, even though we do not access the target domain images.\n']",Autonomous Vehicle Lane Detection and Topology Reasoning
382,381,18,381_grading_essays_essay_scores,"['grading', 'essays', 'essay', 'scores', 'writing', 'learners', 'scoring', 'score', 'automated', 'rubrics']","['essay', 'scoring', 'essays', 'grammatical', 'writing', 'proficiency', 'feedback', 'scores', 'grading', 'learners']","['grading', 'essays', 'learners', 'automated', 'rubrics', 'writers', 'attentionpooling', 'raters', 'examiners', 'mark']","['  Individual feedback can help students improve their essay writing skills.\nHowever, the manual effort required to provide such feedback limits\nindividualization in practice. Automatically-generated essay feedback may serve\nas an alternative to guide students at their own pace, convenience, and desired\nfrequency. Large language models (LLMs) have demonstrated strong performance in\ngenerating coherent and contextually relevant text. Yet, their ability to\nprovide helpful essay feedback is unclear. This work explores several prompting\nstrategies for LLM-based zero-shot and few-shot generation of essay feedback.\nInspired by Chain-of-Thought prompting, we study how and to what extent\nautomated essay scoring (AES) can benefit the quality of generated feedback. We\nevaluate both the AES performance that LLMs can achieve with prompting only and\nthe helpfulness of the generated essay feedback. Our results suggest that\ntackling AES and feedback generation jointly improves AES performance. However,\nwhile our manual evaluation emphasizes the quality of the generated essay\nfeedback, the impact of essay scoring on the generated feedback remains low\nultimately.\n', ""  Automated Essay Scoring (AES) holds significant promise in the field of\neducation, helping educators to mark larger volumes of essays and provide\ntimely feedback. However, Arabic AES research has been limited by the lack of\npublicly available essay data. This study introduces AR-AES, an Arabic AES\nbenchmark dataset comprising 2046 undergraduate essays, including gender\ninformation, scores, and transparent rubric-based evaluation guidelines,\nproviding comprehensive insights into the scoring process. These essays come\nfrom four diverse courses, covering both traditional and online exams.\nAdditionally, we pioneer the use of AraBERT for AES, exploring its performance\non different question types. We find encouraging results, particularly for\nEnvironmental Chemistry and source-dependent essay questions. For the first\ntime, we examine the scale of errors made by a BERT-based AES system, observing\nthat 96.15 percent of the errors are within one point of the first human\nmarker's prediction, on a scale of one to five, with 79.49 percent of\npredictions matching exactly. In contrast, additional human markers did not\nexceed 30 percent exact matches with the first marker, with 62.9 percent within\none mark. These findings highlight the subjectivity inherent in essay grading,\nand underscore the potential for current AES technology to assist human markers\nto grade consistently across large classes.\n"", '  Automated essay scoring (AES) involves predicting a score that reflects the\nwriting quality of an essay. Most existing AES systems produce only a single\noverall score. However, users and L2 learners expect scores across different\ndimensions (e.g., vocabulary, grammar, coherence) for English essays in\nreal-world applications. To address this need, we have developed two models\nthat automatically score English essays across multiple dimensions by employing\nfine-tuning and other strategies on two large datasets. The results demonstrate\nthat our systems achieve impressive performance in evaluation using three\ncriteria: precision, F1 score, and Quadratic Weighted Kappa. Furthermore, our\nsystem outperforms existing methods in overall scoring.\n']",Automated Essay Scoring and Feedback
383,382,18,382_copyrights_copyright_plagiarism_infringement,"['copyrights', 'copyright', 'plagiarism', 'infringement', 'infringing', 'copyrighted', 'copying', 'texts', 'watermarking', 'passphrases']","['copyright', 'literal', 'copying', 'verbatim', 'duplicates', 'leaked', 'content', 'memorization', 'books', 'reproduction']","['copyrights', 'plagiarism', 'watermarking', 'passphrases', 'excerpts', 'corpus', 'memorization', 'deduplication', 'infringe', 'licenses']","['  Memorization in large language models (LLMs) is a growing concern. LLMs have\nbeen shown to easily reproduce parts of their training data, including\ncopyrighted work. This is an important problem to solve, as it may violate\nexisting copyright laws as well as the European AI Act. In this work, we\npropose a systematic analysis to quantify the extent of potential copyright\ninfringements in LLMs using European law as an example. Unlike previous work,\nwe evaluate instruction-finetuned models in a realistic end-user scenario. Our\nanalysis builds on a proposed threshold of 160 characters, which we borrow from\nthe German Copyright Service Provider Act and a fuzzy text matching algorithm\nto identify potentially copyright-infringing textual reproductions. The\nspecificity of countermeasures against copyright infringement is analyzed by\ncomparing model behavior on copyrighted and public domain data. We investigate\nwhat behaviors models show instead of producing protected text (such as refusal\nor hallucination) and provide a first legal assessment of these behaviors. We\nfind that there are huge differences in copyright compliance, specificity, and\nappropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous\nperform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing\na particularly low absolute number of potential copyright violations. Code will\nbe published soon.\n', '  Large Language Models (LLMs) have demonstrated impressive capabilities in\ngenerating diverse and contextually rich text. However, concerns regarding\ncopyright infringement arise as LLMs may inadvertently produce copyrighted\nmaterial. In this paper, we first investigate the effectiveness of watermarking\nLLMs as a deterrent against the generation of copyrighted texts. Through\ntheoretical analysis and empirical evaluation, we demonstrate that\nincorporating watermarks into LLMs significantly reduces the likelihood of\ngenerating copyrighted content, thereby addressing a critical concern in the\ndeployment of LLMs. Additionally, we explore the impact of watermarking on\nMembership Inference Attacks (MIAs), which aim to discern whether a sample was\npart of the pretraining dataset and may be used to detect copyright violations.\nSurprisingly, we find that watermarking adversely affects the success rate of\nMIAs, complicating the task of detecting copyrighted text in the pretraining\ndataset. Finally, we propose an adaptive technique to improve the success rate\nof a recent MIA under watermarking. Our findings underscore the importance of\ndeveloping adaptive methods to study critical problems in LLMs with potential\nlegal implications.\n', '  Large Language Models (LLMs) have transformed machine learning but raised\nsignificant legal concerns due to their potential to produce text that\ninfringes on copyrights, resulting in several high-profile lawsuits. The legal\nlandscape is struggling to keep pace with these rapid advancements, with\nongoing debates about whether generated text might plagiarize copyrighted\nmaterials. Current LLMs may infringe on copyrights or overly restrict\nnon-copyrighted texts, leading to these challenges: (i) the need for a\ncomprehensive evaluation benchmark to assess copyright compliance from multiple\naspects; (ii) evaluating robustness against safeguard bypassing attacks; and\n(iii) developing effective defenses targeted against the generation of\ncopyrighted text. To tackle these challenges, we introduce a curated dataset to\nevaluate methods, test attack strategies, and propose lightweight, real-time\ndefenses to prevent the generation of copyrighted text, ensuring the safe and\nlawful use of LLMs. Our experiments demonstrate that current LLMs frequently\noutput copyrighted text, and that jailbreaking attacks can significantly\nincrease the volume of copyrighted output. Our proposed defense mechanisms\nsignificantly reduce the volume of copyrighted text generated by LLMs by\neffectively refusing malicious requests. Code is publicly available at\nhttps://github.com/xz-liu/SHIELD\n']",Copyright Infringement in AI-Generated Text
384,383,18,383_svm_svms_classifier_classification,"['svm', 'svms', 'classifier', 'classification', 'kernelized', 'svr', 'vector', 'hyperplanes', 'rademacher', 'outliers']","['vector', 'twin', 'support', 'granular', 'loss', 'multiview', 'classifier', 'machines', 'ball', 'margin']","['svm', 'kernelized', 'hyperplanes', 'rademacher', 'outliers', 'norm', 'gbtwsvm', 'mvtpmsvm', 'cone', 'view']","['  Support vector machine (SVM) has achieved many successes in machine learning,\nespecially for a small sample problem. As a famous extension of the traditional\nSVM, the $\\nu$ support vector machine ($\\nu$-SVM) has shown outstanding\nperformance due to its great model interpretability. However, it still faces\nchallenges in training overhead for large-scale problems. To address this\nissue, we propose a safe screening rule with bi-level optimization for\n$\\nu$-SVM (SRBO-$\\nu$-SVM) which can screen out inactive samples before\ntraining and reduce the computational cost without sacrificing the prediction\naccuracy. Our SRBO-$\\nu$-SVM is strictly deduced by integrating the\nKarush-Kuhn-Tucker (KKT) conditions, the variational inequalities of convex\nproblems and the $\\nu$-property. Furthermore, we develop an efficient dual\ncoordinate descent method (DCDM) to further improve computational speed.\nFinally, a unified framework for SRBO is proposed to accelerate many SVM-type\nmodels, and it is successfully applied to one-class SVM. Experimental results\non 6 artificial data sets and 30 benchmark data sets have verified the\neffectiveness and safety of our proposed methods in supervised and unsupervised\ntasks.\n', '  Loss function plays a vital role in supervised learning frameworks. The\nselection of the appropriate loss function holds the potential to have a\nsubstantial impact on the proficiency attained by the acquired model. The\ntraining of supervised learning algorithms inherently adheres to predetermined\nloss functions during the optimization process. In this paper, we present a\nnovel contribution to the realm of supervised machine learning: an asymmetric\nloss function named wave loss. It exhibits robustness against outliers,\ninsensitivity to noise, boundedness, and a crucial smoothness property.\nTheoretically, we establish that the proposed wave loss function manifests the\nessential characteristic of being classification-calibrated. Leveraging this\nbreakthrough, we incorporate the proposed wave loss function into the least\nsquares setting of support vector machines (SVM) and twin support vector\nmachines (TSVM), resulting in two robust and smooth models termed Wave-SVM and\nWave-TSVM, respectively. To address the optimization problem inherent in\nWave-SVM, we utilize the adaptive moment estimation (Adam) algorithm. It is\nnoteworthy that this paper marks the first instance of the Adam algorithm\napplication to solve an SVM model. Further, we devise an iterative algorithm to\nsolve the optimization problems of Wave-TSVM. To empirically showcase the\neffectiveness of the proposed Wave-SVM and Wave-TSVM, we evaluate them on\nbenchmark UCI and KEEL datasets (with and without feature noise) from diverse\ndomains. Moreover, to exemplify the applicability of Wave-SVM in the biomedical\ndomain, we evaluate it on the Alzheimer Disease Neuroimaging Initiative (ADNI)\ndataset. The experimental outcomes unequivocally reveal the prowess of Wave-SVM\nand Wave-TSVM in achieving superior prediction accuracy against the baseline\nmodels.\n', '  The previous support vector machine(SVM) including $0/1$ loss SVM, hinge loss\nSVM, ramp loss SVM, truncated pinball loss SVM, and others, overlooked the\ndegree of penalty for the correctly classified samples within the margin. This\noversight affects the generalization ability of the SVM classifier to some\nextent. To address this limitation, from the perspective of confidence margin,\nwe propose a novel Slide loss function ($\\ell_s$) to construct the support\nvector machine classifier($\\ell_s$-SVM). By introducing the concept of proximal\nstationary point, and utilizing the property of Lipschitz continuity, we derive\nthe first-order optimality conditions for $\\ell_s$-SVM. Based on this, we\ndefine the $\\ell_s$ support vectors and working set of $\\ell_s$-SVM. To\nefficiently handle $\\ell_s$-SVM, we devise a fast alternating direction method\nof multipliers with the working set ($\\ell_s$-ADMM), and provide the\nconvergence analysis. The numerical experiments on real world datasets confirm\nthe robustness and effectiveness of the proposed method.\n']",Support Vector Machines
385,384,18,384_metaphors_metaphor_metaphorical_similes,"['metaphors', 'metaphor', 'metaphorical', 'similes', 'metaphorically', 'chinese_metaphor_explanation', 'simile', 'figurative', 'linguistics', 'linguistically']","['metaphor', 'metaphors', 'figurative', 'infographics', 'metaphorical', 'conceptual', 'tenors', 'inapt', 'grounds', 'literal']","['metaphors', 'chinese_metaphor_explanation', 'linguistically', 'annotated', 'paraphrases', 'corpus', 'juxtapositions', 'personifications', 'subtasks', 'poems']","[""  Metaphors are considered to pose challenges for a wide spectrum of NLP tasks.\nThis gives rise to the area of computational metaphor processing. However, it\nremains unclear what types of metaphors challenge current state-of-the-art\nmodels. In this paper, we test various NLP models on the VUA metaphor dataset\nand quantify to what extent metaphors affect models' performance on various\ndownstream tasks. Analysis reveals that VUA includes a large number of\nmetaphors that pose little difficulty to downstream tasks. We would like to\nshift the attention of researchers away from these metaphors to instead focus\non challenging metaphors. To identify hard metaphors, we propose an automatic\npipeline that identifies metaphors that challenge a particular model. Our\nanalysis demonstrates that our detected hard metaphors contrast significantly\nwith VUA and reduce the accuracy of machine translation by 16\\%, QA performance\nby 4\\%, NLI by 7\\%, and metaphor identification recall by over 14\\% for various\npopular NLP systems.\n"", ""  Metaphors, although occasionally unperceived, are ubiquitous in our everyday\nlanguage. Thus, it is crucial for Language Models to be able to grasp the\nunderlying meaning of this kind of figurative language. In this work, we\npresent Meta4XNLI, a novel parallel dataset for the tasks of metaphor detection\nand interpretation that contains metaphor annotations in both Spanish and\nEnglish. We investigate language models' metaphor identification and\nunderstanding abilities through a series of monolingual and cross-lingual\nexperiments by leveraging our proposed corpus. In order to comprehend how these\nnon-literal expressions affect models' performance, we look over the results\nand perform an error analysis. Additionally, parallel data offers many\npotential opportunities to investigate metaphor transferability between these\nlanguages and the impact of translation on the development of multilingual\nannotated resources.\n"", '  This paper presents the results of the shared task on Chinese metaphor\ngeneration, hosted at the 13th CCF Conference on Natural Language Processing\nand Chinese Computing (NLPCC 2024). The goal of this shared task is to generate\nChinese metaphors using machine learning techniques and effectively identifying\nbasic components of metaphorical sentences. It is divided into two subtasks: 1)\nMetaphor Generation, which involves creating a metaphor from a provided tuple\nconsisting of TENOR, GROUND, and VEHICLE. The goal here is to synthesize a\nmetaphor that connects the subject (i.e. TENOR) with the object (i.e. VEHICLE),\nguided by the concept of the GROUND. 2) Metaphor Components Identification,\nwhich extracts the most fitting TENORs, GROUNDs, and VEHICLEs from a\nmetaphorical sentence. This component requires the identification of the most\nfitting metaphor elements that correspond to the specified grounds. In addition\nto overall results, we report on the setup and insights from the metaphor\ngeneration shared task, which attracted a total of 4 participating teams across\nboth subtasks.\n']",Metaphor Understanding and Processing in NLP
386,385,18,385_discourse_corpus_treebank_parsing,"['discourse', 'corpus', 'treebank', 'parsing', 'parsers', 'rhetorical', 'connectives', 'dialogue', 'corpora', 'annotated']","['discourse', 'rhetorical', 'connectives', 'relations', 'relation', 'parsing', 'structures', 'corpora', 'topic', 'dependency']","['discourse', 'corpus', 'parsing', 'connectives', 'paraphrases', 'microtexts', 'dependencies', 'doc2dial', 'pdtb', 'hierarchical']","['  Existing discourse corpora are annotated based on different frameworks, which\nshow significant dissimilarities in definitions of arguments and relations and\nstructural constraints. Despite surface differences, these frameworks share\nbasic understandings of discourse relations. The relationship between these\nframeworks has been an open research question, especially the correlation\nbetween relation inventories utilized in different frameworks. Better\nunderstanding of this question is helpful for integrating discourse theories\nand enabling interoperability of discourse corpora annotated under different\nframeworks. However, studies that explore correlations between discourse\nrelation inventories are hindered by different criteria of discourse\nsegmentation, and expert knowledge and manual examination are typically needed.\nSome semi-automatic methods have been proposed, but they rely on corpora\nannotated in multiple frameworks in parallel. In this paper, we introduce a\nfully automatic approach to address the challenges. Specifically, we extend the\nlabel-anchored contrastive learning method introduced by Zhang et al. (2022b)\nto learn label embeddings during a classification task. These embeddings are\nthen utilized to map discourse relations from different frameworks. We show\nexperimental results on RST-DT (Carlson et al., 2001) and PDTB 3.0 (Prasad et\nal., 2018).\n', '  In this article we present Enhanced Rhetorical Structure Theory (eRST), a new\ntheoretical framework for computational discourse analysis, based on an\nexpansion of Rhetorical Structure Theory (RST). The framework encompasses\ndiscourse relation graphs with tree-breaking, nonprojective and concurrent\nrelations, as well as implicit and explicit signals which give explainable\nrationales to our analyses. We survey shortcomings of RST and other existing\nframeworks, such as Segmented Discourse Representation Theory (SDRT), the Penn\nDiscourse Treebank (PDTB) and Discourse Dependencies, and address these using\nconstructs in the proposed theory. We provide annotation, search and\nvisualization tools for data, and present and evaluate a freely available\ncorpus of English annotated according to our framework, encompassing 12 spoken\nand written genres with over 200K tokens. Finally, we discuss automatic\nparsing, evaluation metrics and applications for data in our framework.\n', ""  The development of different theories of discourse structure has led to the\nestablishment of discourse corpora based on these theories. However, the\nexistence of discourse corpora established on different theoretical bases\ncreates challenges when it comes to exploring them in a consistent and cohesive\nway. This study has as its primary focus the conversion of PDTB annotations\ninto dependency structures. It employs refined BERT-based discourse parsers to\ntest the validity of the dependency data derived from the PDTB-style corpora in\nEnglish, Chinese, and several other languages. By converting both PDTB and RST\nannotations for the same texts into dependencies, this study also applies\n``dependency distance'' metrics to examine the correlation between RST\ndependencies and PDTB dependencies in English. The results show that the PDTB\ndependency data is valid and that there is a strong correlation between the two\ntypes of dependency distance. This study presents a comprehensive approach for\nanalyzing and evaluating discourse corpora by employing discourse dependencies\nto achieve unified analysis. By applying dependency representations, we can\nextract data from PDTB, RST, and SDRT corpora in a coherent and unified manner.\nMoreover, the cross-linguistic validation establishes the framework's\ngeneralizability beyond English. The establishment of this comprehensive\ndependency framework overcomes limitations of existing discourse corpora,\nsupporting a diverse range of algorithms and facilitating further studies in\ncomputational discourse analysis and language sciences.\n""]",Discourse Analysis and Parsing
387,386,18,386_encoder_decoder_inception_denoising,"['encoder', 'decoder', 'inception', 'denoising', 'imagenet', 'deblurring', 'generative', 'diffusion', 'masking', 'noising']","['restoration', 'diffusion', 'resolution', 'patches', 'image', 'perceptual', 'guidance', 'unconditional', 'autoregressive', 'degradation']","['encoder', 'denoising', 'imagenet', 'deblurring', 'masked', 'transformer', 'rectification', 'dit', 'degradation', 'painting']","['  At the core of both successful generative and self-supervised representation\nlearning models there is a reconstruction objective that incorporates some form\nof image corruption. Diffusion models implement this approach through a\nscheduled Gaussian corruption process, while masked auto-encoder models do so\nby masking patches of the image. Despite their different approaches, the\nunderlying similarity in their methodologies suggests a promising avenue for an\nauto-encoder capable of both de-noising tasks. We propose a unified\nself-supervised objective, dubbed Unified Masked Diffusion (UMD), that combines\npatch-based and noise-based corruption techniques within a single auto-encoding\nframework. Specifically, UMD modifies the diffusion transformer (DiT) training\nprocess by introducing an additional noise-free, high masking representation\nstep in the diffusion noising schedule, and utilizes a mixed masked and noised\nimage for subsequent timesteps. By integrating features useful for diffusion\nmodeling and for predicting masked patch tokens, UMD achieves strong\nperformance in downstream generative and representation learning tasks,\nincluding linear probing and class-conditional generation. This is achieved\nwithout the need for heavy data augmentations, multiple views, or additional\nencoders. Furthermore, UMD improves over the computational efficiency of prior\ndiffusion based methods in total training time. We release our code at\nhttps://github.com/philippe-eecs/small-vision.\n', '  Image restoration is a fundamental problem that involves recovering a\nhigh-quality clean image from its degraded observation. All-In-One image\nrestoration models can effectively restore images from various types and levels\nof degradation using degradation-specific information as prompts to guide the\nrestoration model. In this work, we present the first approach that uses\nhuman-written instructions to guide the image restoration model. Given natural\nlanguage prompts, our model can recover high-quality images from their degraded\ncounterparts, considering multiple degradation types. Our method, InstructIR,\nachieves state-of-the-art results on several restoration tasks including image\ndenoising, deraining, deblurring, dehazing, and (low-light) image enhancement.\nInstructIR improves +1dB over previous all-in-one restoration methods.\nMoreover, our dataset and results represent a novel benchmark for new research\non text-guided image restoration and enhancement. Our code, datasets and models\nare available at: https://github.com/mv-lab/InstructIR\n', '  We propose an efficient approach to train large diffusion models with masked\ntransformers. While masked transformers have been extensively explored for\nrepresentation learning, their application to generative learning is less\nexplored in the vision domain. Our work is the first to exploit masked training\nto reduce the training cost of diffusion models significantly. Specifically, we\nrandomly mask out a high proportion (e.g., 50%) of patches in diffused input\nimages during training. For masked training, we introduce an asymmetric\nencoder-decoder architecture consisting of a transformer encoder that operates\nonly on unmasked patches and a lightweight transformer decoder on full patches.\nTo promote a long-range understanding of full patches, we add an auxiliary task\nof reconstructing masked patches to the denoising score matching objective that\nlearns the score of unmasked patches. Experiments on ImageNet-256x256 and\nImageNet-512x512 show that our approach achieves competitive and even better\ngenerative performance than the state-of-the-art Diffusion Transformer (DiT)\nmodel, using only around 30% of its original training time. Thus, our method\nshows a promising way of efficiently training large transformer-based diffusion\nmodels without sacrificing the generative performance.\n']",Image Denoising and Restoration using Deep Learning
388,387,18,387_autism_autistic_asd_speech,"['autism', 'autistic', 'asd', 'speech', 'voice', 'diagnosing', 'developmental', 'linguistic', 'classifying', 'psychologist']","['autism', 'children', 'disorder', 'autistic', 'spectrum', 'speech', 'diagnosis', 'developmental', 'therapy', 'behaviors']","['autism', 'asd', 'diagnosing', 'developmental', 'paralinguistic', 'skills', 'neurotypical', '3dcnn', 'recordings', 'dld']","['  Autism Spectrum Disorder (ASD) represents a multifaceted neurodevelopmental\ncondition marked by difficulties in social interaction, communication\nimpediments, and repetitive behaviors. Despite progress in understanding ASD,\nits diagnosis and treatment continue to pose significant challenges due to the\nvariability in symptomatology and the necessity for multidisciplinary care\napproaches. This paper investigates the potential of Artificial Intelligence\n(AI) to augment the capabilities of healthcare professionals and caregivers in\nmanaging ASD. We have developed a sophisticated algorithm designed to analyze\nfacial and bodily expressions during daily activities of both autistic and\nnon-autistic children, leading to the development of a powerful deep\nlearning-based autism detection system. Our study demonstrated that AI models,\nspecifically the Xception and ResNet50V2 architectures, achieved high accuracy\nin diagnosing Autism Spectrum Disorder (ASD). This research highlights the\ntransformative potential of AI in improving the diagnosis, treatment, and\ncomprehensive management of ASD. Our study revealed that AI models, notably the\nXception and ResNet50V2 architectures, demonstrated high accuracy in diagnosing\nASD.\n', ""  The diagnosis of autism spectrum disorder (ASD) is a complex, challenging\ntask as it depends on the analysis of interactional behaviors by psychologists\nrather than the use of biochemical diagnostics. In this paper, we present a\nmodeling approach to ASD diagnosis by analyzing acoustic/prosodic and\nlinguistic features extracted from diagnostic conversations between a\npsychologist and children who either are typically developing (TD) or have ASD.\nWe compare the contributions of different features across a range of\nconversation tasks. We focus on finding a minimal set of parameters that\ncharacterize conversational behaviors of children with ASD. Because ASD is\ndiagnosed through conversational interaction, in addition to analyzing the\nbehavior of the children, we also investigate whether the psychologist's\nconversational behaviors vary across diagnostic groups. Our results can\nfacilitate fine-grained analysis of conversation data for children with ASD to\nsupport diagnosis and intervention.\n"", ""  Purpose: Our study explored the use of artificial intelligence (AI) to\ndiagnose autism spectrum disorder (ASD). It focused on machine learning (ML)\nand deep learning (DL) to detect ASD from text inputs on social media,\naddressing challenges in traditional ASD diagnosis.\n  Methods: We used natural language processing (NLP), ML, and DL models\n(including decision trees, XGB, KNN, RNN, LSTM, Bi-LSTM, BERT, and BERTweet) to\nanalyze 404,627 tweets, classifying them based on ASD or non-ASD authors. A\nsubset of 90,000 tweets was used for model training and testing.\n  Results: Our AI models showed high accuracy, with an 88% success rate in\nidentifying texts from individuals with ASD.\n  Conclusion: The study demonstrates AI's potential in improving ASD diagnosis,\nespecially in children, highlighting the importance of early detection.\n""]",Autism Diagnosis and Detection using AI and Machine Learning
389,388,18,388_gestures_gesture_gesturing_multimodal,"['gestures', 'gesture', 'gesturing', 'multimodal', 'nonverbal', 'speech', 'embodied', 'audio', 'cues', 'recordings']","['gestures', 'gesture', 'speech', 'nonverbal', 'motion', 'beat', 'audio', 'body', 'verbal', 'emotion']","['gestures', 'multimodal', 'nonverbal', 'embodied', 'recordings', 'emotiongesture', 'keyframes', 'freetalker', 'psycholinguistic', 'gesticulation']","[""  This paper focuses on enhancing human-agent communication by integrating\nspatial context into virtual agents' non-verbal behaviors, specifically\ngestures. Recent advances in co-speech gesture generation have primarily\nutilized data-driven methods, which create natural motion but limit the scope\nof gestures to those performed in a void. Our work aims to extend these methods\nby enabling generative models to incorporate scene information into\nspeech-driven gesture synthesis. We introduce a novel synthetic gesture dataset\ntailored for this purpose. This development represents a critical step toward\ncreating embodied conversational agents that interact more naturally with their\nenvironment and users.\n"", '  Gesture synthesis has gained significant attention as a critical research\nfield, aiming to produce contextually appropriate and natural gestures\ncorresponding to speech or textual input. Although deep learning-based\napproaches have achieved remarkable progress, they often overlook the rich\nsemantic information present in the text, leading to less expressive and\nmeaningful gestures. In this letter, we propose GesGPT, a novel approach to\ngesture generation that leverages the semantic analysis capabilities of large\nlanguage models , such as ChatGPT. By capitalizing on the strengths of LLMs for\ntext analysis, we adopt a controlled approach to generate and integrate\nprofessional gestures and base gestures through a text parsing script,\nresulting in diverse and meaningful gestures. Firstly, our approach involves\nthe development of prompt principles that transform gesture generation into an\nintention classification problem using ChatGPT. We also conduct further\nanalysis on emphasis words and semantic words to aid in gesture generation.\nSubsequently, we construct a specialized gesture lexicon with multiple semantic\nannotations, decoupling the synthesis of gestures into professional gestures\nand base gestures. Finally, we merge the professional gestures with base\ngestures. Experimental results demonstrate that GesGPT effectively generates\ncontextually appropriate and expressive gestures.\n', ""  Gestures are inherent to human interaction and often complement speech in\nface-to-face communication, forming a multimodal communication system. An\nimportant task in gesture analysis is detecting a gesture's beginning and end.\nResearch on automatic gesture detection has primarily focused on visual and\nkinematic information to detect a limited set of isolated or silent gestures\nwith low variability, neglecting the integration of speech and vision signals\nto detect gestures that co-occur with speech. This work addresses this gap by\nfocusing on co-speech gesture detection, emphasising the synchrony between\nspeech and co-speech hand gestures. We address three main challenges: the\nvariability of gesture forms, the temporal misalignment between gesture and\nspeech onsets, and differences in sampling rate between modalities. We\ninvestigate extended speech time windows and employ separate backbone models\nfor each modality to address the temporal misalignment and sampling rate\ndifferences. We utilize Transformer encoders in cross-modal and early fusion\ntechniques to effectively align and integrate speech and skeletal sequences.\nThe study results show that combining visual and speech information\nsignificantly enhances gesture detection performance. Our findings indicate\nthat expanding the speech buffer beyond visual time segments improves\nperformance and that multimodal integration using cross-modal and early fusion\ntechniques outperforms baseline methods using unimodal and late fusion methods.\nAdditionally, we find a correlation between the models' gesture prediction\nconfidence and low-level speech frequency features potentially associated with\ngestures. Overall, the study provides a better understanding and detection\nmethods for co-speech gestures, facilitating the analysis of multimodal\ncommunication.\n""]",Gesture Recognition and Synthesis in Multimodal Communication
390,389,18,389_privacy_graphpub_graphs_private,"['privacy', 'graphpub', 'graphs', 'private', 'edge', 'graph', 'adjacency', 'gnndelete', 'nodes', 'neighbors']","['privacy', 'graph', 'node', 'edges', 'edge', 'utility', 'attacks', 'protection', 'private', 'leakage']","['privacy', 'graphpub', 'edge', 'adjacency', 'gnndelete', 'neighbors', 'pagerank', 'randomization', 'differentially', 'ldp']","['  Graph neural networks (GNNs) play a key role in learning representations from\ngraph-structured data and are demonstrated to be useful in many applications.\nHowever, the GNN training pipeline has been shown to be vulnerable to node\nfeature leakage and edge extraction attacks. This paper investigates a scenario\nwhere an attacker aims to recover private edge information from a trained GNN\nmodel. Previous studies have employed differential privacy (DP) to add noise\ndirectly to the adjacency matrix or a compact graph representation. The added\nperturbations cause the graph structure to be substantially morphed, reducing\nthe model utility. We propose a new privacy-preserving GNN training algorithm,\nEclipse, that maintains good model utility while providing strong privacy\nprotection on edges. Eclipse is based on two key observations. First, adjacency\nmatrices in graph structures exhibit low-rank behavior. Thus, Eclipse trains\nGNNs with a low-rank format of the graph via singular values decomposition\n(SVD), rather than the original graph. Using the low-rank format, Eclipse\npreserves the primary graph topology and removes the remaining residual edges.\nEclipse adds noise to the low-rank singular values instead of the entire graph,\nthereby preserving the graph privacy while still maintaining enough of the\ngraph structure to maintain model utility. We theoretically show Eclipse\nprovide formal DP guarantee on edges. Experiments on benchmark graph datasets\nshow that Eclipse achieves significantly better privacy-utility tradeoff\ncompared to existing privacy-preserving GNN training methods. In particular,\nunder strong privacy constraints ($\\epsilon$ < 4), Eclipse shows significant\ngains in the model utility by up to 46%. We further demonstrate that Eclipse\nalso has better resilience against common edge attacks (e.g., LPA), lowering\nthe attack AUC by up to 5% compared to other state-of-the-art baselines.\n', ""  Differentially private GNNs (Graph Neural Networks) have been recently\nstudied to provide high accuracy in various tasks on graph data while strongly\nprotecting user privacy. In particular, a recent study proposes an algorithm to\nprotect each user's feature vector in an attributed graph, which includes\nfeature vectors along with node IDs and edges, with LDP (Local Differential\nPrivacy), a strong privacy notion without a trusted third party. However, this\nalgorithm does not protect edges (friendships) in a social graph, hence cannot\nprotect user privacy in unattributed graphs, which include only node IDs and\nedges. How to provide strong privacy with high accuracy in unattributed graphs\nremains open. In this paper, we propose a novel LDP algorithm called the DPRR\n(Degree-Preserving Randomized Response) to provide LDP for edges in GNNs. Our\nDPRR preserves each user's degree hence a graph structure while providing edge\nLDP. Technically, our DPRR uses Warner's RR (Randomized Response) and strategic\nedge sampling, where each user's sampling probability is automatically tuned\nusing the Laplacian mechanism to preserve the degree information under edge\nLDP. We also propose a privacy budget allocation method to make the noise in\nboth Warner's RR and the Laplacian mechanism small. We focus on graph\nclassification as a task of GNNs and evaluate the DPRR using three social graph\ndatasets. Our experimental results show that the DPRR significantly outperforms\nthree baselines and provides accuracy close to a non-private algorithm in all\ndatasets with a reasonable privacy budget, e.g., epsilon=1. Finally, we\nintroduce data poisoning attacks to our DPRR and a defense against the attacks.\nWe evaluate them using the three social graph datasets and discuss the\nexperimental results.\n"", '  Graph Neural Networks (GNNs) have achieved great success in learning with\ngraph-structured data. Privacy concerns have also been raised for the trained\nmodels which could expose the sensitive information of graphs including both\nnode features and the structure information. In this paper, we aim to achieve\nnode-level differential privacy (DP) for training GNNs so that a node and its\nedges are protected. Node DP is inherently difficult for GNNs because all\ndirect and multi-hop neighbors participate in the calculation of gradients for\neach node via layer-wise message passing and there is no bound on how many\ndirect and multi-hop neighbors a node can have, so existing DP methods will\nresult in high privacy cost or poor utility due to high node sensitivity. We\npropose a Decoupled GNN with Differentially Private Approximate Personalized\nPageRank (DPAR) for training GNNs with an enhanced privacy-utility tradeoff.\nThe key idea is to decouple the feature projection and message passing via a DP\nPageRank algorithm which learns the structure information and uses the top-$K$\nneighbors determined by the PageRank for feature aggregation. By capturing the\nmost important neighbors for each node and avoiding the layer-wise message\npassing, it bounds the node sensitivity and achieves improved privacy-utility\ntradeoff compared to layer-wise perturbation based methods. We theoretically\nanalyze the node DP guarantee for the two processes combined together and\nempirically demonstrate better utilities of DPAR with the same level of node DP\ncompared with state-of-the-art methods.\n']",Graph Neural Network Privacy Protection
391,390,18,390_queries_dbmss_dbms_optimizers,"['queries', 'dbmss', 'dbms', 'optimizers', 'databases', 'optimize', 'sql', 'optimizer', 'database', 'query']","['query', 'database', 'workload', 'workloads', 'plans', 'queries', 'plan', 'execution', 'optimizers', 'optimizer']","['queries', 'dbmss', 'optimize', 'memory', 'indexes', 'analytics', 'plans', 'workloads', 'predicates', 'processing']","['  Query optimizers in relational database management systems (RDBMSs) search\nfor execution plans expected to be optimal for a given queries. They use\nparameter estimates, often inaccurate, and make assumptions that may not hold\nin practice. Consequently, they may select execution plans that are suboptimal\nat runtime, when these estimates and assumptions are not valid, which may\nresult in poor query performance. Therefore, query optimizers do not\nsufficiently support robust query optimization. Recent years have seen a surge\nof interest in using machine learning (ML) to improve efficiency of data\nsystems and reduce their maintenance overheads, with promising results obtained\nin the area of query optimization in particular. In this paper, inspired by\nthese advancements, and based on several years of experience of IBM Db2 in this\njourney, we propose Robust Optimization of Queries, (Roq), a holistic framework\nthat enables robust query optimization based on a risk-aware learning approach.\nRoq includes a novel formalization of the notion of robustness in the context\nof query optimization and a principled approach for its quantification and\nmeasurement based on approximate probabilistic ML. It also includes novel\nstrategies and algorithms for query plan evaluation and selection. Roq also\nincludes a novel learned cost model that is designed to predict query execution\ncost and the associated risks and performs query optimization accordingly. We\ndemonstrate experimentally that Roq provides significant improvements to robust\nquery optimization compared to the state-of-the-art.\n', ""  Query optimization in relational database management systems (DBMSs) is\ncritical for fast query processing. The query optimizer relies on precise\nselectivity and cost estimates to effectively optimize queries prior to\nexecution. While this strategy is effective for relational DBMSs, it is not\nsufficient for DBMSs tailored for processing machine learning (ML) queries. In\nML-centric DBMSs, query optimization is challenging for two reasons. First, the\nperformance bottleneck of the queries shifts to user-defined functions (UDFs)\nthat often wrap around deep learning models, making it difficult to accurately\nestimate UDF statistics without profiling the query. This leads to inaccurate\nstatistics and sub-optimal query plans. Second, the optimal query plan for ML\nqueries is data-dependent, necessitating DBMSs to adapt the query plan on the\nfly during execution. So, a static query plan is not sufficient for such\nqueries.\n  In this paper, we present Hydro, an ML-centric DBMS that utilizes adaptive\nquery processing (AQP) for efficiently processing ML queries. Hydro is designed\nto quickly evaluate UDF-based query predicates by ensuring optimal predicate\nevaluation order and improving the scalability of UDF execution. By integrating\nAQP, Hydro continuously monitors UDF statistics, routes data to predicates in\nan optimal order, and dynamically allocates resources for evaluating\npredicates. We demonstrate Hydro's efficacy through four illustrative use\ncases, delivering up to 11.52x speedup over a baseline system.\n"", '  Modern database systems rely on cost-based query optimizers to come up with\ngood execution plans for input queries. Such query optimizers rely on cost\nmodels to estimate the costs of candidate query execution plans. A cost model\nrepresents a function from a set of cost units to query execution cost, where\neach cost unit specifies the unit cost of executing a certain type of query\nprocessing operation (such as table scan or join). These cost units are\ntraditionally viewed as constants, whose values only depend on the platform\nconfiguration where the database system runs on top of but are invariant for\nqueries processed by the database system. In this paper, we challenge this\nclassic view by thinking of these cost units as variables instead. We show\nthat, by varying the cost-unit values one can obtain query plans that\nsignificantly outperform the default query plans returned by the query\noptimizer when viewing the cost units as constants. We term this cost-unit\ntuning process ""query tuning"" (QT) and show that it is similar to the\nwell-known hyper-parameter optimization (HPO) problem in AutoML. As a result,\nany state-of-the-art HPO technologies can be applied to QT. We study the QT\nproblem in the context of anytime tuning, which is desirable in practice by\nconstraining the total time spent on QT within a given budget -- we call this\nproblem budget-aware query tuning. We further extend our study from tuning a\nsingle query to tuning a workload with multiple queries, and we call this\ngeneralized problem budget-aware workload tuning (WT), which aims for\nminimizing the execution time of the entire workload. WT is more challenging as\none needs to further prioritize individual query tuning within the given time\nbudget. We propose solutions to both QT and WT and experimental evaluation\nusing both benchmark and real workloads demonstrates the efficacy of our\nproposed solutions.\n']",Query Optimization in Database Management Systems
392,391,18,391_autoencoders_autoencoder_encoder_autoenocoder,"['autoencoders', 'autoencoder', 'encoder', 'autoenocoder', 'generative', 'decoder', 'variational', 'regularization', 'priors', 'vae']","['variational', 'posterior', 'latent', 'autoencoders', 'mixture', 'collapse', 'distributions', 'autoencoder', 'inference', 'divergence']","['autoencoders', 'generative', 'variational', 'vae', 'divergence', 'entropy', 'likelihood', 'patchgan', 'gibbs', 'dlvms']","['  The central objective function of a variational autoencoder (VAE) is its\nvariational lower bound (the ELBO). Here we show that for standard (i.e.,\nGaussian) VAEs the ELBO converges to a value given by the sum of three\nentropies: the (negative) entropy of the prior distribution, the expected\n(negative) entropy of the observable distribution, and the average entropy of\nthe variational distributions (the latter is already part of the ELBO). Our\nderived analytical results are exact and apply for small as well as for\nintricate deep networks for encoder and decoder. Furthermore, they apply for\nfinitely and infinitely many data points and at any stationary point (including\nlocal maxima and saddle points). The result implies that the ELBO can for\nstandard VAEs often be computed in closed-form at stationary points while the\noriginal ELBO requires numerical approximations of integrals. As a main\ncontribution, we provide the proof that the ELBO for VAEs is at stationary\npoints equal to entropy sums. Numerical experiments then show that the obtained\nanalytical results are sufficiently precise also in those vicinities of\nstationary points that are reached in practice. Furthermore, we discuss how the\nnovel entropy form of the ELBO can be used to analyze and understand learning\nbehavior. More generally, we believe that our contributions can be useful for\nfuture theoretical and practical studies on VAE learning as they provide novel\ninformation on those points in parameters space that optimization of VAEs\nconverges to.\n', '  The posterior collapse phenomenon in variational autoencoder (VAE), where the\nvariational posterior distribution closely matches the prior distribution, can\nhinder the quality of the learned latent variables. As a consequence of\nposterior collapse, the latent variables extracted by the encoder in VAE\npreserve less information from the input data and thus fail to produce\nmeaningful representations as input to the reconstruction process in the\ndecoder. While this phenomenon has been an actively addressed topic related to\nVAE performance, the theory for posterior collapse remains underdeveloped,\nespecially beyond the standard VAE. In this work, we advance the theoretical\nunderstanding of posterior collapse to two important and prevalent yet less\nstudied classes of VAE: conditional VAE and hierarchical VAE. Specifically, via\na non-trivial theoretical analysis of linear conditional VAE and hierarchical\nVAE with two levels of latent, we prove that the cause of posterior collapses\nin these models includes the correlation between the input and output of the\nconditional VAE and the effect of learnable encoder variance in the\nhierarchical VAE. We empirically validate our theoretical findings for linear\nconditional and hierarchical VAE and demonstrate that these results are also\npredictive for non-linear cases with extensive experiments.\n', ""  Traditional Variational Autoencoders (VAEs) are constrained by the\nlimitations of the Evidence Lower Bound (ELBO) formulation, particularly when\nutilizing simplistic, non-analytic, or unknown prior distributions. These\nlimitations inhibit the VAE's ability to generate high-quality samples and\nprovide clear, interpretable latent representations. This work introduces the\nEntropy Decomposed Variational Autoencoder (ED-VAE), a novel re-formulation of\nthe ELBO that explicitly includes entropy and cross-entropy components. This\nreformulation significantly enhances model flexibility, allowing for the\nintegration of complex and non-standard priors. By providing more detailed\ncontrol over the encoding and regularization of latent spaces, ED-VAE not only\nimproves interpretability but also effectively captures the complex\ninteractions between latent variables and observed data, thus leading to better\ngenerative performance.\n""]",Variational Autoencoders
393,392,18,392_gnndrive_gnn_networks_gpus,"['gnndrive', 'gnn', 'networks', 'gpus', 'graphsage', 'graphs', 'tgraph', 'gnns', 'vertex', 'bottleneck']","['partitioning', 'graph', 'mini', 'batch', 'sampling', 'memory', 'server', 'dataloader', 'graphs', 'training']","['gnndrive', 'graphsage', 'bottleneck', 'gpu', 'batch', 'neural', 'partitioner', 'gsplit', 'locality', 'cache']","['  Many Graph Neural Network (GNN) training systems have emerged recently to\nsupport efficient GNN training. Since GNNs embody complex data dependencies\nbetween training samples, the training of GNNs should address distinct\nchallenges different from DNN training in data management, such as data\npartitioning, batch preparation for mini-batch training, and data transferring\nbetween CPUs and GPUs. These factors, which take up a large proportion of\ntraining time, make data management in GNN training more significant. This\npaper reviews GNN training from a data management perspective and provides a\ncomprehensive analysis and evaluation of the representative approaches. We\nconduct extensive experiments on various benchmark datasets and show many\ninteresting and valuable results. We also provide some practical tips learned\nfrom these experiments, which are helpful for designing GNN training systems in\nthe future.\n', '  Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training\n', '  Recently, graph neural networks (GNNs) have gained much attention as a\ngrowing area of deep learning capable of learning on graph-structured data.\nHowever, the computational and memory requirements for training GNNs on\nlarge-scale graphs make it necessary to distribute the training. A prerequisite\nfor distributed GNN training is to partition the input graph into smaller parts\nthat are distributed among multiple machines of a compute cluster. Although\ngraph partitioning has been studied with regard to graph analytics and graph\ndatabases, its effect on GNN training performance is largely unexplored. As a\nconsequence, it is unclear whether investing computational efforts into\nhigh-quality graph partitioning would pay off in GNN training scenarios.\n  In this paper, we study the effectiveness of graph partitioning for\ndistributed GNN training. Our study aims to understand how different factors\nsuch as GNN parameters, mini-batch size, graph type, features size, and\nscale-out factor influence the effectiveness of graph partitioning. We conduct\nexperiments with two different GNN systems using vertex and edge partitioning.\nWe found that high-quality graph partitioning is a very effective optimization\nto speed up GNN training and to reduce memory consumption. Furthermore, our\nresults show that invested partitioning time can quickly be amortized by\nreduced GNN training time, making it a relevant optimization for most GNN\nscenarios. Compared to research on distributed graph processing, our study\nreveals that graph partitioning plays an even more significant role in\ndistributed GNN training, which motivates further research on the graph\npartitioning problem.\n']",Graph Neural Network Training Optimization
394,393,18,393_phylogenies_protolanguage_phylogenetics_phylogenetic,"['phylogenies', 'protolanguage', 'phylogenetics', 'phylogenetic', 'linguistics', 'phonology', 'ancestral', 'linguists', 'ancestor', 'phonetic']","['cognate', 'protoforms', 'sound', 'phylogenetic', 'proto', 'languages', 'reflexes', 'linguistics', 'synonyms', 'words']","['phylogenies', 'protolanguage', 'phonology', 'ancestral', 'linguists', 'cognates', 'morphological', 'lexicostatistics', 'relatedness', 'latin']","['  We describe a set of new methods to partially automate linguistic\nphylogenetic inference given (1) cognate sets with their respective protoforms\nand sound laws, (2) a mapping from phones to their articulatory features and\n(3) a typological database of sound changes. We train a neural network on these\nsound change data to weight articulatory distances between phones and predict\nintermediate sound change steps between historical protoforms and their modern\ndescendants, replacing a linguistic expert in part of a parsimony-based\nphylogenetic inference algorithm. In our best experiments on Tukanoan\nlanguages, this method produces trees with a Generalized Quartet Distance of\n0.12 from a tree that used expert annotations, a significant improvement over\nother semi-automated baselines. We discuss potential benefits and drawbacks to\nour neural approach and parsimony-based tree prediction. We also experiment\nwith a minimal generalization learner for automatic sound law induction,\nfinding it comparably effective to sound laws from expert annotation. Our code\nis publicly available at https://github.com/cmu-llab/aiscp.\n', '  Identification of cognates across related languages is one of the primary\nproblems in historical linguistics. Automated cognate identification is helpful\nfor several downstream tasks including identifying sound correspondences,\nproto-language reconstruction, phylogenetic classification, etc. Previous\nstate-of-the-art methods for cognate identification are mostly based on\ndistributions of phonemes computed across multilingual wordlists and make\nlittle use of the cognacy labels that define links among cognate clusters. In\nthis paper, we present a transformer-based architecture inspired by\ncomputational biology for the task of automated cognate detection. Beyond a\ncertain amount of supervision, this method performs better than the existing\nmethods, and shows steady improvement with further increase in supervision,\nthereby proving the efficacy of utilizing the labeled information. We also\ndemonstrate that accepting multiple sequence alignments as input and having an\nend-to-end architecture with link prediction head saves much computation time\nwhile simultaneously yielding superior performance.\n', '  In traditional studies on language evolution, scholars often emphasize the\nimportance of sound laws and sound correspondences for phylogenetic inference\nof language family trees. However, to date, computational approaches have\ntypically not taken this potential into account. Most computational studies\nstill rely on lexical cognates as major data source for phylogenetic\nreconstruction in linguistics, although there do exist a few studies in which\nauthors praise the benefits of comparing words at the level of sound sequences.\nBuilding on (a) ten diverse datasets from different language families, and (b)\nstate-of-the-art methods for automated cognate and sound correspondence\ndetection, we test, for the first time, the performance of sound-based versus\ncognate-based approaches to phylogenetic reconstruction. Our results show that\nphylogenies reconstructed from lexical cognates are topologically closer, by\napproximately one third with respect to the generalized quartet distance on\naverage, to the gold standard phylogenies than phylogenies reconstructed from\nsound correspondences.\n']",Linguistic Phylogenetics and Language Evolution
395,394,17,394_classifiers_classifier_annotations_annotation,"['classifiers', 'classifier', 'annotations', 'annotation', 'misclassifies', 'labels', 'imagenet', 'bias', 'biases', 'generalization']","['spurious', 'group', 'correlations', 'subgroups', 'biased', 'labels', 'worst', 'bias', 'annotations', 'shortcut']","['classifiers', 'annotations', 'misclassifies', 'labels', 'imagenet', 'bias', 'training', 'attribute', 'accuracy', 'diversity']","['  Deep neural classifiers tend to rely on spurious correlations between\nspurious attributes of inputs and targets to make predictions, which could\njeopardize their generalization capability. Training classifiers robust to\nspurious correlations typically relies on annotations of spurious correlations\nin data, which are often expensive to get. In this paper, we tackle an\nannotation-free setting and propose a self-guided spurious correlation\nmitigation framework. Our framework automatically constructs fine-grained\ntraining labels tailored for a classifier obtained with empirical risk\nminimization to improve its robustness against spurious correlations. The\nfine-grained training labels are formulated with different prediction behaviors\nof the classifier identified in a novel spuriousness embedding space. We\nconstruct the space with automatically detected conceptual attributes and a\nnovel spuriousness metric which measures how likely a class-attribute\ncorrelation is exploited for predictions. We demonstrate that training the\nclassifier to distinguish different prediction behaviors reduces its reliance\non spurious correlations without knowing them a priori and outperforms prior\nmethods on five real-world datasets.\n', '  Machine learning models are known to learn spurious correlations, i.e.,\nfeatures having strong relations with class labels but no causal relation.\nRelying on those correlations leads to poor performance in the data groups\nwithout these correlations and poor generalization ability. To improve the\nrobustness of machine learning models to spurious correlations, we propose an\napproach to extract a subnetwork from a fully trained network that does not\nrely on spurious correlations. The subnetwork is found by the assumption that\ndata points with the same spurious attribute will be close to each other in the\nrepresentation space when training with ERM, then we employ supervised\ncontrastive loss in a novel way to force models to unlearn the spurious\nconnections. The increase in the worst-group performance of our approach\ncontributes to strengthening the hypothesis that there exists a subnetwork in a\nfully trained dense network that is responsible for using only invariant\nfeatures in classification tasks, therefore erasing the influence of spurious\nfeatures even in the setup of multi spurious attributes and no prior knowledge\nof attributes labels.\n', '  Standard empirical risk minimization (ERM) models may prioritize learning\nspurious correlations between spurious features and true labels, leading to\npoor accuracy on groups where these correlations do not hold. Mitigating this\nissue often requires expensive spurious attribute (group) labels or relies on\ntrained ERM models to infer group labels when group information is unavailable.\nHowever, the significant performance gap in worst-group accuracy between using\npseudo group labels and using oracle group labels inspires us to consider\nfurther improving group robustness through preciser group inference. Therefore,\nwe propose GIC, a novel method that accurately infers group labels, resulting\nin improved worst-group performance. GIC trains a spurious attribute classifier\nbased on two key properties of spurious correlations: (1) high correlation\nbetween spurious attributes and true labels, and (2) variability in this\ncorrelation between datasets with different group distributions. Empirical\nstudies on multiple datasets demonstrate the effectiveness of GIC in inferring\ngroup labels, and combining GIC with various downstream invariant learning\nmethods improves worst-group accuracy, showcasing its powerful flexibility.\nAdditionally, through analyzing the misclassifications in GIC, we identify an\ninteresting phenomenon called semantic consistency, which may contribute to\nbetter decoupling the association between spurious attributes and labels,\nthereby mitigating spurious correlation. The code for GIC is available at\nhttps://github.com/yujinhanml/GIC.\n']",Mitigating Spurious Correlations in Machine Learning Classifiers
396,395,17,395_emotions_affective_emotion_sentiment,"['emotions', 'affective', 'emotion', 'sentiment', 'emotional', 'nlp', 'valence', 'textual', 'annotation', 'smallenglishemotions']","['emotion', 'emotions', 'affective', 'emotional', 'languages', 'guilt', 'classification', 'transfer', 'anger', 'sentiment']","['affective', 'nlp', 'annotation', 'smallenglishemotions', 'sociolinguistic', 'classifier', 'expressions', 'annoyance', 'emollms', 'spanish']","['  Emotions are a central aspect of communication. Consequently, emotion\nanalysis (EA) is a rapidly growing field in natural language processing (NLP).\nHowever, there is no consensus on scope, direction, or methods. In this paper,\nwe conduct a thorough review of 154 relevant NLP publications from the last\ndecade. Based on this review, we address four different questions: (1) How are\nEA tasks defined in NLP? (2) What are the most prominent emotion frameworks and\nwhich emotions are modeled? (3) Is the subjectivity of emotions considered in\nterms of demographics and cultural factors? and (4) What are the primary NLP\napplications for EA? We take stock of trends in EA and tasks, emotion\nframeworks used, existing datasets, methods, and applications. We then discuss\nfour lacunae: (1) the absence of demographic and cultural aspects does not\naccount for the variation in how emotions are perceived, but instead assumes\nthey are universally experienced in the same manner; (2) the poor fit of\nemotion categories from the two main emotion theories to the task; (3) the lack\nof standardized EA terminology hinders gap identification, comparison, and\nfuture goals; and (4) the absence of interdisciplinary research isolates EA\nfrom insights in other fields. Our work will enable more focused research into\nEA and a more holistic approach to modeling emotions in NLP.\n', '  Emotion detection in textual data has received growing interest in recent\nyears, as it is pivotal for developing empathetic human-computer interaction\nsystems. This paper introduces a method for categorizing emotions from text,\nwhich acknowledges and differentiates between the diversified similarities and\ndistinctions of various emotions. Initially, we establish a baseline by\ntraining a transformer-based model for standard emotion classification,\nachieving state-of-the-art performance. We argue that not all\nmisclassifications are of the same importance, as there are perceptual\nsimilarities among emotional classes. We thus redefine the emotion labeling\nproblem by shifting it from a traditional classification model to an ordinal\nclassification one, where discrete emotions are arranged in a sequential order\naccording to their valence levels. Finally, we propose a method that performs\nordinal classification in the two-dimensional emotion space, considering both\nvalence and arousal scales. The results show that our approach not only\npreserves high accuracy in emotion prediction but also significantly reduces\nthe magnitude of errors in cases of misclassification.\n', '  We propose leveraging cognitive science research on emotions and\ncommunication to improve language models for emotion analysis. First, we\npresent the main emotion theories in psychology and cognitive science. Then, we\nintroduce the main methods of emotion annotation in natural language processing\nand their connections to psychological theories. We also present the two main\ntypes of analyses of emotional communication in cognitive pragmatics. Finally,\nbased on the cognitive science research presented, we propose directions for\nimproving language models for emotion analysis. We suggest that these research\nefforts pave the way for constructing new annotation schemes and a possible\nbenchmark for emotional understanding, considering different facets of human\nemotion and communication.\n']",Emotion Analysis in Natural Language Processing
397,396,17,396_tracking_best_of_n_trackers_tracker_tracklets,"['tracking', 'best_of_n_trackers', 'tracker', 'tracklets', 'trackers', 'tracklet', 'trackingnet', 'track', 'tracks', 'frames']","['tracking', 'tracker', 'association', 'trackers', 'object', 'event', 'frames', 'frame', 'tracklets', 'motion']","['best_of_n_trackers', 'tracklets', 'trackingnet', 'frames', 'depthtrack', 'felt_sot_benchmark', 'rgbe', 'mocov2', 'performing', 'filter']","['  Current event-/frame-event based trackers undergo evaluation on short-term\ntracking datasets, however, the tracking of real-world scenarios involves\nlong-term tracking, and the performance of existing tracking algorithms in\nthese scenarios remains unclear. In this paper, we first propose a new\nlong-term and large-scale frame-event single object tracking dataset, termed\nFELT. It contains 742 videos and 1,594,474 RGB frames and event stream pairs\nand has become the largest frame-event tracking dataset to date. We re-train\nand evaluate 15 baseline trackers on our dataset for future works to compare.\nMore importantly, we find that the RGB frames and event streams are naturally\nincomplete due to the influence of challenging factors and spatially sparse\nevent flow. In response to this, we propose a novel associative memory\nTransformer network as a unified backbone by introducing modern Hopfield layers\ninto multi-head self-attention blocks to fuse both RGB and event data.\nExtensive experiments on RGB-Event (FELT), RGB-Thermal (RGBT234, LasHeR), and\nRGB-Depth (DepthTrack) datasets fully validated the effectiveness of our model.\nThe dataset and source code can be found at\n\\url{https://github.com/Event-AHU/FELT_SOT_Benchmark}.\n', '  We observe that the performance of SOTA visual trackers surprisingly strongly\nvaries across different video attributes and datasets. No single tracker\nremains the best performer across all tracking attributes and datasets. To\nbridge this gap, for a given video sequence, we predict the ""Best of the N\nTrackers"", called the BofN meta-tracker. At its core, a Tracking Performance\nPrediction Network (TP2N) selects a predicted best performing visual tracker\nfor the given video sequence using only a few initial frames. We also introduce\na frame-level BofN meta-tracker which keeps predicting best performer after\nregular temporal intervals. The TP2N is based on self-supervised learning\narchitectures MocoV2, SwAv, BT, and DINO; experiments show that the DINO with\nViT-S as a backbone performs the best. The video-level BofN meta-tracker\noutperforms, by a large margin, existing SOTA trackers on nine standard\nbenchmarks - LaSOT, TrackingNet, GOT-10K, VOT2019, VOT2021, VOT2022, UAV123,\nOTB100, and WebUAV-3M. Further improvement is achieved by the frame-level BofN\nmeta-tracker effectively handling variations in the tracking scenarios within\nlong sequences. For instance, on GOT-10k, BofN meta-tracker average overlap is\n88.7% and 91.1% with video and frame-level settings respectively. The best\nperforming tracker, RTS, achieves 85.20% AO. On VOT2022, BofN expected average\noverlap is 67.88% and 70.98% with video and frame level settings, compared to\nthe best performing ARTrack, 64.12%. This work also presents an extensive\nevaluation of competitive tracking methods on all commonly used benchmarks,\nfollowing their protocols. The code, the trained models, and the results will\nsoon be made publicly available on\nhttps://github.com/BasitAlawode/Best_of_N_Trackers.\n', '  RGB-Event based tracking is an emerging research topic, focusing on how to\neffectively integrate heterogeneous multi-modal data (synchronized exposure\nvideo frames and asynchronous pulse Event stream). Existing works typically\nemploy Transformer based networks to handle these modalities and achieve decent\naccuracy through input-level or feature-level fusion on multiple datasets.\nHowever, these trackers require significant memory consumption and\ncomputational complexity due to the use of self-attention mechanism. This paper\nproposes a novel RGB-Event tracking framework, Mamba-FETrack, based on the\nState Space Model (SSM) to achieve high-performance tracking while effectively\nreducing computational costs and realizing more efficient tracking.\nSpecifically, we adopt two modality-specific Mamba backbone networks to extract\nthe features of RGB frames and Event streams. Then, we also propose to boost\nthe interactive learning between the RGB and Event features using the Mamba\nnetwork. The fused features will be fed into the tracking head for target\nobject localization. Extensive experiments on FELT and FE108 datasets fully\nvalidated the efficiency and effectiveness of our proposed tracker.\nSpecifically, our Mamba-based tracker achieves 43.5/55.6 on the SR/PR metric,\nwhile the ViT-S based tracker (OSTrack) obtains 40.0/50.9. The GPU memory cost\nof ours and ViT-S based tracker is 13.98GB and 15.44GB, which decreased about\n$9.5\\%$. The FLOPs and parameters of ours/ViT-S based OSTrack are 59GB/1076GB\nand 7MB/60MB, which decreased about $94.5\\%$ and $88.3\\%$, respectively. We\nhope this work can bring some new insights to the tracking field and greatly\npromote the application of the Mamba architecture in tracking. The source code\nof this work will be released on\n\\url{https://github.com/Event-AHU/Mamba_FETrack}.\n']",Visual Object Tracking
398,397,17,397_privacy_recommender_personalized_personalization,"['privacy', 'recommender', 'personalized', 'personalization', 'federated', 'collaborative', 'shared', 'collaboratively', 'sharing', 'private']","['federated', 'recommendation', 'privacy', 'item', 'server', 'users', 'user', 'recommendations', 'private', 'client']","['privacy', 'recommender', 'personalized', 'federated', 'collaborative', 'embedding', 'client', 'gpfedrec', 'fedhgnn', 'dard']","[""  With the growing concerns regarding user data privacy, Federated Recommender\nSystem (FedRec) has garnered significant attention recently due to its\nprivacy-preserving capabilities. Existing FedRecs generally adhere to a\nlearning protocol in which a central server shares a global recommendation\nmodel with clients, and participants achieve collaborative learning by\nfrequently communicating the model's public parameters. Nevertheless, this\nlearning framework has two drawbacks that limit its practical usability: (1) It\nnecessitates a global-sharing recommendation model; however, in real-world\nscenarios, information related to the recommender model, including its\nalgorithm and parameters, constitutes the platforms' intellectual property.\nHence, service providers are unlikely to release such information actively. (2)\nThe communication costs of model parameter transmission are expensive since the\nmodel parameters are usually high-dimensional matrices. With the model size\nincreasing, the communication burden will be the bottleneck for such\ntraditional FedRecs.\n  Given the above limitations, this paper introduces a novel parameter\ntransmission-free federated recommendation framework that balances the\nprotection between users' data privacy and platforms' model privacy, namely\nPTF-FedRec. Specifically, participants in PTF-FedRec collaboratively exchange\nknowledge by sharing their predictions within a privacy-preserving mechanism.\nThrough this way, the central server can learn a recommender model without\ndisclosing its model parameters or accessing clients' raw data, preserving both\nthe server's model privacy and users' data privacy. Besides, since clients and\nthe central server only need to communicate prediction scores which are just a\nfew real numbers, the overhead is significantly reduced compared to traditional\nFedRecs. The code is available\nat\\url{https://github.com/hi-weiyuan/PTF-FedRec}.\n"", '  Federated recommendations (FRs), facilitating multiple local clients to\ncollectively learn a global model without disclosing user private data, have\nemerged as a prevalent architecture for privacy-preserving recommendations. In\nconventional FRs, a dominant paradigm is to utilize discrete identities to\nrepresent users/clients and items, which are subsequently mapped to\ndomain-specific embeddings to participate in model training. Despite\nconsiderable performance, we reveal three inherent limitations that can not be\nignored in federated settings, i.e., non-transferability across domains,\nunavailability in cold-start settings, and potential privacy violations during\nfederated training. To this end, we propose a transferable federated\nrecommendation model with universal textual representations, TransFR, which\ndelicately incorporates the general capabilities empowered by pre-trained\nlanguage models and the personalized abilities by fine-tuning local private\ndata. Specifically, it first learns domain-agnostic representations of items by\nexploiting pre-trained models with public textual corpora. To tailor for\nfederated recommendation, we further introduce an efficient federated\nfine-tuning and a local training mechanism. This facilitates personalized local\nheads for each client by utilizing their private behavior data. By\nincorporating pre-training and fine-tuning within FRs, it greatly improves the\nadaptation efficiency transferring to a new domain and the generalization\ncapacity to address cold-start issues. Through extensive experiments on several\ndatasets, we demonstrate that our TransFR model surpasses several\nstate-of-the-art FRs in terms of accuracy, transferability, and privacy.\n', ""  The federated recommendation system is an emerging AI service architecture\nthat provides recommendation services in a privacy-preserving manner. Using\nuser-relation graphs to enhance federated recommendations is a promising topic.\nHowever, it is still an open challenge to construct the user-relation graph\nwhile preserving data locality-based privacy protection in federated settings.\nInspired by a simple motivation, similar users share a similar vision\n(embeddings) to the same item set, this paper proposes a novel Graph-guided\nPersonalization for Federated Recommendation (GPFedRec). The proposed method\nconstructs a user-relation graph from user-specific personalized item\nembeddings at the server without accessing the users' interaction records. The\npersonalized item embedding is locally fine-tuned on each device, and then a\nuser-relation graph will be constructed by measuring the similarity among\nclient-specific item embeddings. Without accessing users' historical\ninteractions, we embody the data locality-based privacy protection of vanilla\nfederated learning. Furthermore, a graph-guided aggregation mechanism is\ndesigned to leverage the user-relation graph and federated optimization\nframework simultaneously. Extensive experiments on five benchmark datasets\ndemonstrate GPFedRec's superior performance. The in-depth study validates that\nGPFedRec can generally improve existing federated recommendation methods as a\nplugin while keeping user privacy safe. Code is available to ease\nreproducibility\n""]",Federated Recommendation Systems for Privacy-Preserving Personalization
399,398,17,398_memory_devices_ios_mobile,"['memory', 'devices', 'ios', 'mobile', 'device', 'mobilellm', 'cloud', 'gpus', 'hardware', 'os']","['device', 'devices', 'mobile', 'cloud', 'hardware', 'personalized', 'latency', 'execution', 'user', 'personalization']","['devices', 'mobilellm', 'cloud', 'gpus', 'annotation', 'workloads', 'architecture', 'llms', 'billion', 'oses']","['  Computer systems are becoming increasingly heterogeneous with the emergence\nof new memory technologies and compute devices. GPUs alongside CPUs have become\ncommonplace and CXL is poised to be a mainstay of cloud systems. The operating\nsystem is responsible for managing these hardware resources, requiring\nmodification every time a new device is released. Years of research and\ndevelopment are sunk into tuning the OS for high performance with each new\nheterogeneous device. With the recent explosion in memory technologies and\ndomain-specific accelerators, it would be beneficial to have an OS that could\nprovide high performance for new devices without significant effort.\n  We propose LLaMaS which can adapt to new devices easily. LLaMaS uses Large\nLanguage Models (LLMs) to extract the useful features of new devices from their\ntextual description and uses these features to make operating system decisions\nat runtime. Adding support to LLaMaS for a new device is as simple as\ndescribing the system and new device properties in plaintext.\n  LLaMaS reduces the burden on system administrators to enable easy integration\nof new devices into production systems.\n  Preliminary evaluation using ChatGPT shows that LLMs are capable of\nextracting device features from text and make correct OS decisions based on\nthose features.\n', '  Transformers have revolutionized the machine learning landscape, gradually\nmaking their way into everyday tasks and equipping our computers with ""sparks\nof intelligence"". However, their runtime requirements have prevented them from\nbeing broadly deployed on mobile. As personal devices become increasingly\npowerful and prompt privacy becomes an ever more pressing issue, we explore the\ncurrent state of mobile execution of Large Language Models (LLMs). To achieve\nthis, we have created our own automation infrastructure, MELT, which supports\nthe headless execution and benchmarking of LLMs on device, supporting different\nmodels, devices and frameworks, including Android, iOS and Nvidia Jetson\ndevices. We evaluate popular instruction fine-tuned LLMs and leverage different\nframeworks to measure their end-to-end and granular performance, tracing their\nmemory and energy requirements along the way. Our analysis is the first\nsystematic study of on-device LLM execution, quantifying performance, energy\nefficiency and accuracy across various state-of-the-art models and showcases\nthe state of on-device intelligence in the era of hyperscale models. Results\nhighlight the performance heterogeneity across targets and corroborates that\nLLM inference is largely memory-bound. Quantization drastically reduces memory\nrequirements and renders execution viable, but at a non-negligible accuracy\ncost. Drawing from its energy footprint and thermal behavior, the continuous\nexecution of LLMs remains elusive, as both factors negatively affect user\nexperience. Last, our experience shows that the ecosystem is still in its\ninfancy, and algorithmic as well as hardware breakthroughs can significantly\nshift the execution cost. We expect NPU acceleration, and framework-hardware\nco-design to be the biggest bet towards efficient standalone execution, with\nthe alternative of offloading tailored towards edge deployments.\n', ""  On-device large language models (LLMs) are catalyzing novel mobile\napplications such as UI task automation and personalized email auto-reply,\nwithout giving away users' private data. However, on-device LLMs still suffer\nfrom unacceptably long inference latency, especially the time to first token\n(prefill stage) due to the need of long context for accurate, personalized\ncontent generation, as well as the lack of parallel computing capacity of\nmobile CPU/GPU.\n  To enable practical on-device LLM, we present mllm-NPU, the first-of-its-kind\nLLM inference system that efficiently leverages on-device Neural Processing\nUnit (NPU) offloading. Essentially, mllm-NPU is an algorithm-system co-design\nthat tackles a few semantic gaps between the LLM architecture and contemporary\nNPU design. Specifically, it re-constructs the prompt and model in three\nlevels: (1) At prompt level, it divides variable-length prompts into multiple\nfixed-sized chunks while maintaining data dependencies; (2) At tensor level, it\nidentifies and extracts significant outliers to run on the CPU/GPU in parallel\nwith minimal overhead; (3) At block level, it schedules Transformer blocks in\nan out-of-order manner to the CPU/GPU and NPU based on their hardware affinity\nand sensitivity to accuracy. Compared to competitive baselines, mllm-NPU\nachieves 22.4x faster prefill speed and 30.7x energy savings on average, and up\nto 32.8x speedup in an end-to-end real-world application. For the first time,\nmllm-NPU achieves more than 1,000 tokens/sec prefilling for a billion-sized\nmodel (Qwen1.5-1.8B), paving the way towards practical on-device LLM.\n""]",Optimizing Large Language Models for Mobile Devices
400,399,17,399_heuristics_heuristic_optimized_generating,"['heuristics', 'heuristic', 'optimized', 'generating', 'automate', 'evolutionary', 'algorithms', 'automated', 'generates', 'evolving']","['evolutionary', 'evolution', 'heuristics', 'heuristic', 'optimization', 'algorithms', 'parallels', 'prompts', 'genetic', 'fitness']","['heuristics', 'generating', 'automated', 'evolving', 'programming', 'funsearch', 'iteratively', 'prompts', 'crossovers', 'evol']","['  Pre-trained large language models (LLMs) have powerful capabilities for\ngenerating creative natural text. Evolutionary algorithms (EAs) can discover\ndiverse solutions to complex real-world problems. Motivated by the common\ncollective and directionality of text generation and evolution, this paper\nillustrates the parallels between LLMs and EAs, which includes multiple\none-to-one key characteristics: token representation and individual\nrepresentation, position encoding and fitness shaping, position embedding and\nselection, Transformers block and reproduction, and model training and\nparameter adaptation. By examining these parallels, we analyze existing\ninterdisciplinary research, with a specific focus on evolutionary fine-tuning\nand LLM-enhanced EAs. Drawing from these insights, valuable future directions\nare presented for advancing the integration of LLMs and EAs, while highlighting\nkey challenges along the way. These parallels not only reveal the evolution\nmechanism behind LLMs but also facilitate the development of evolved artificial\nagents that approach or surpass biological organisms.\n', '  Heuristics are widely used for dealing with complex search and optimization\nproblems. However, manual design of heuristics can be often very labour\nextensive and requires rich working experience and knowledge. This paper\nproposes Evolution of Heuristic (EoH), a novel evolutionary paradigm that\nleverages both Large Language Models (LLMs) and Evolutionary Computation (EC)\nmethods for Automatic Heuristic Design (AHD). EoH represents the ideas of\nheuristics in natural language, termed thoughts. They are then translated into\nexecutable codes by LLMs. The evolution of both thoughts and codes in an\nevolutionary search framework makes it very effective and efficient for\ngenerating high-performance heuristics. Experiments on three widely studied\ncombinatorial optimization benchmark problems demonstrate that EoH outperforms\ncommonly used handcrafted heuristics and other recent AHD methods including\nFunSearch. Particularly, the heuristic produced by EoH with a low computational\nbudget (in terms of the number of queries to LLMs) significantly outperforms\nwidely-used human hand-crafted baseline algorithms for the online bin packing\nproblem.\n', '  Large Language Models (LLMs) excel in various tasks, but they rely on\ncarefully crafted prompts that often demand substantial human effort. To\nautomate this process, in this paper, we propose a novel framework for discrete\nprompt optimization, called EvoPrompt, which borrows the idea of evolutionary\nalgorithms (EAs) as they exhibit good performance and fast convergence. To\nenable EAs to work on discrete prompts, which are natural language expressions\nthat need to be coherent and human-readable, we connect LLMs with EAs. This\napproach allows us to simultaneously leverage the powerful language processing\ncapabilities of LLMs and the efficient optimization performance of EAs.\nSpecifically, abstaining from any gradients or parameters, EvoPrompt starts\nfrom a population of prompts and iteratively generates new prompts with LLMs\nbased on the evolutionary operators, improving the population based on the\ndevelopment set. We optimize prompts for both closed- and open-source LLMs\nincluding GPT-3.5 and Alpaca, on 31 datasets covering language understanding,\ngeneration tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt\nsignificantly outperforms human-engineered prompts and existing methods for\nautomatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt\ndemonstrates that connecting LLMs with EAs creates synergies, which could\ninspire further research on the combination of LLMs and conventional\nalgorithms.\n']",Evolutionary Algorithms for Optimizing Language Models
401,400,17,400_contrastive_learning_supervised_embeddings,"['contrastive', 'learning', 'supervised', 'embeddings', 'unlearning', 'representations', 'views', 'representation', 'imagenet1k', 'view']","['contrastive', 'views', 'batch', 'positive', 'epochs', 'negative', 'supervised', 'unsupervised', 'similarity', 'learning']","['contrastive', 'embeddings', 'unlearning', 'imagenet1k', 'view', 'minimisers', 'adainf', 'batch', 'negative', 'augmentations']","['  In contrastive learning, two views of an original image, generated by\ndifferent augmentations, are considered a positive pair, and their similarity\nis required to be high. Similarly, two views of distinct images form a negative\npair, with encouraged low similarity. Typically, a single similarity measure,\nprovided by a lone projection head, evaluates positive and negative sample\npairs. However, due to diverse augmentation strategies and varying intra-sample\nsimilarity, views from the same image may not always be similar. Additionally,\nowing to inter-sample similarity, views from different images may be more akin\nthan those from the same image. Consequently, enforcing high similarity for\npositive pairs and low similarity for negative pairs may be unattainable, and\nin some cases, such enforcement could detrimentally impact performance. To\naddress this challenge, we propose using multiple projection heads, each\nproducing a distinct set of features. Our pre-training loss function emerges\nfrom a solution to the maximum likelihood estimation over head-wise posterior\ndistributions of positive samples given observations. This loss incorporates\nthe similarity measure over positive and negative pairs, each re-weighted by an\nindividual adaptive temperature, regulated to prevent ill solutions. Our\napproach, Adaptive Multi-Head Contrastive Learning (AMCL), can be applied to\nand experimentally enhances several popular contrastive learning methods such\nas SimCLR, MoCo, and Barlow Twins. The improvement remains consistent across\nvarious backbones and linear probing epochs, and becomes more significant when\nemploying multiple augmentation methods.\n', '  Contrastive learning is a paradigm for learning representations from\nunlabelled data that has been highly successful for image and text data.\nSeveral recent works have examined contrastive losses to claim that contrastive\nmodels effectively learn spectral embeddings, while few works show relations\nbetween (wide) contrastive models and kernel principal component analysis\n(PCA). However, it is not known if trained contrastive models indeed correspond\nto kernel methods or PCA. In this work, we analyze the training dynamics of\ntwo-layer contrastive models, with non-linear activation, and answer when these\nmodels are close to PCA or kernel methods. It is well known in the supervised\nsetting that neural networks are equivalent to neural tangent kernel (NTK)\nmachines, and that the NTK of infinitely wide networks remains constant during\ntraining. We provide the first convergence results of NTK for contrastive\nlosses, and present a nuanced picture: NTK of wide networks remains almost\nconstant for cosine similarity based contrastive losses, but not for losses\nbased on dot product similarity. We further study the training dynamics of\ncontrastive models with orthogonality constraints on output layer, which is\nimplicitly assumed in works relating contrastive learning to spectral\nembedding. Our deviation bounds suggest that representations learned by\ncontrastive models are close to the principal components of a certain matrix\ncomputed from random features. We empirically show that our theoretical results\npossibly hold beyond two-layer networks.\n', '  In the era of big data and Artificial Intelligence, an emerging paradigm is\nto utilize contrastive self-supervised learning to model large-scale\nheterogeneous data. Many existing foundation models benefit from the\ngeneralization capability of contrastive self-supervised learning by learning\ncompact and high-quality representations without relying on any label\ninformation. Amidst the explosive advancements in foundation models across\nmultiple domains, including natural language processing and computer vision, a\nthorough survey on heterogeneous contrastive learning for the foundation model\nis urgently needed. In response, this survey critically evaluates the current\nlandscape of heterogeneous contrastive learning for foundation models,\nhighlighting the open challenges and future trends of contrastive learning. In\nparticular, we first present how the recent advanced contrastive learning-based\nmethods deal with view heterogeneity and how contrastive learning is applied to\ntrain and fine-tune the multi-view foundation models. Then, we move to\ncontrastive learning methods for task heterogeneity, including pretraining\ntasks and downstream tasks, and show how different tasks are combined with\ncontrastive learning loss for different purposes. Finally, we conclude this\nsurvey by discussing the open challenges and shedding light on the future\ndirections of contrastive learning.\n']",Contrastive Learning for Representation Learning
402,401,17,401_repairagent_patches_repair_repairing,"['repairagent', 'patches', 'repair', 'repairing', 'patch', 'automated', 'repairllama', 'fixing', 'bugs', 'programming']","['repair', 'bug', 'bugs', 'program', 'localization', 'patches', 'fix', 'software', 'programming', 'code']","['repairagent', 'patches', 'automated', 'faults', 'defects4j', 'bugassist', 'developers', 'java', 'debugging', 'quixbugs']","[""  Automated Program Repair (APR) has evolved significantly with the advent of\nLarge Language Models (LLMs). Fine-tuning LLMs for program repair is a recent\navenue of research, with many dimensions which have not been explored. Existing\nwork mostly fine-tune LLMs with naive code representations and does not scale\nto frontier models. To address this problem, we propose RepairLLaMA, a novel\nprogram repair approach that 1) identifies optimal code representations for APR\nwith fine-tuned models, and 2) pioneers state-of-the-art parameter-efficient\nfine-tuning technique (PEFT) for program repair. This results in RepairLLaMA\nproducing a highly effective `program repair adapter' for fixing bugs with AI.\nOur experiments demonstrate the validity of both concepts. First, fine-tuning\nadapters with program repair specific code representations enables the model to\nuse meaningful repair signals and produce better patches. Second,\nparameter-efficient fine-tuning helps fine-tuning to converge and clearly\ncontributes to the effectiveness of RepairLLaMA in fixing bugs outside the\nfine-tuning data distribution. Overall, RepairLLaMA correctly fixes 144\nDefects4J v2 and 109 HumanEval-Java bugs, outperforming all baselines.\n"", '  Research shows that grammatical mistakes in a sentence can be corrected by\ntranslating it to another language and back using neural machine translation\nwith language models. We investigate whether this correction capability of\nLarge Language Models (LLMs) extends to Automatic Program Repair (APR). Current\ngenerative models for APR are pre-trained on source code and fine-tuned for\nrepair. This paper proposes bypassing the fine-tuning step and using Round-Trip\nTranslation (RTT): translation of code from one programming language to another\nprogramming or natural language, and back. We hypothesize that RTT with LLMs\nrestores the most commonly seen patterns in code during pre-training, i.e.,\nperforms a regression toward the mean, which removes bugs as they are a form of\nnoise w.r.t. the more frequent, natural, bug-free code in the training data. To\ntest this hypothesis, we employ eight recent LLMs pre-trained on code,\nincluding the latest GPT versions, and four common program repair benchmarks in\nJava. We find that RTT with English as an intermediate language repaired 101 of\n164 bugs with GPT-4 on the HumanEval-Java dataset. Moreover, 46 of these are\nunique bugs that are not repaired by other LLMs fine-tuned for APR. Our\nfindings highlight the viability of round-trip translation with LLMs as a\ntechnique for automated program repair and its potential for research in\nsoftware engineering.\n  Keywords: automated program repair, large language model, machine translation\n', ""  Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs are capable of locating and repairing bugs end-to-end when using the\nrelated artifacts (e.g., test cases) as input, existing methods regard them as\nseparate tasks and ask LLMs to generate patches at fixed locations. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first performing fault localization. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-then-repair workflow\nwith direct debugging is more effective for LLM-based APR methods. Thus, we\nbelieve this paper introduces a new mindset for harnessing LLMs in APR.\n""]",Automated Program Repair with Large Language Models
403,402,17,402_fuzzy_sets_rough_soft,"['fuzzy', 'sets', 'rough', 'soft', 'vague', 'granular', 'approximations', 'granularly', 'quantification', 'quantifier']","['fuzzy', 'rough', 'hesitant', 'sets', 'induction', 'soft', 'rule', 'theory', 'pythagorean', 'interval']","['fuzzy', 'granular', 'approximations', 'quantifiers', 'hesitant', 'attributes', 'separability', 'selection', 'neighborhoods', 'mereology']","[""  One of the weaknesses of classical (fuzzy) rough sets is their sensitivity to\nnoise, which is particularly undesirable for machine learning applications. One\napproach to solve this issue is by making use of fuzzy quantifiers, as done by\nthe vaguely quantified fuzzy rough set (VQFRS) model. While this idea is\nintuitive, the VQFRS model suffers from both theoretical flaws as well as from\nsuboptimal performance in applications. In this paper, we improve on VQFRS by\nintroducing fuzzy quantifier-based fuzzy rough sets (FQFRS), an intuitive\ngeneralization of fuzzy rough sets that makes use of general unary and binary\nquantification models. We show how several existing models fit in this\ngeneralization as well as how it inspires novel ones. Several binary\nquantification models are proposed to be used with FQFRS. We conduct a\ntheoretical study of their properties, and investigate their potential by\napplying them to classification problems. In particular, we highlight Yager's\nWeighted Implication-based (YWI) binary quantification model, which induces a\nfuzzy rough set model that is both a significant improvement on VQFRS, as well\nas a worthy competitor to the popular ordered weighted averaging based fuzzy\nrough set (OWAFRS) model.\n"", '  Fuzzy rough set theory can be used as a tool for dealing with inconsistent\ndata when there is a gradual notion of indiscernibility between objects. It\ndoes this by providing lower and upper approximations of concepts. In classical\nfuzzy rough sets, the lower and upper approximations are determined using the\nminimum and maximum operators, respectively. This is undesirable for machine\nlearning applications, since it makes these approximations sensitive to\noutlying samples. To mitigate this problem, ordered weighted average (OWA)\nbased fuzzy rough sets were introduced. In this paper, we show how the\nOWA-based approach can be interpreted intuitively in terms of vague\nquantification, and then generalize it to Choquet-based fuzzy rough sets\n(CFRS). This generalization maintains desirable theoretical properties, such as\nduality and monotonicity. Furthermore, it provides more flexibility for machine\nlearning applications. In particular, we show that it enables the seamless\nintegration of outlier detection algorithms, to enhance the robustness of\nmachine learning algorithms based on fuzzy rough sets.\n', '  Rough set theory is a well-known mathematical framework that can deal with\ninconsistent data by providing lower and upper approximations of concepts. A\nprominent property of these approximations is their granular representation:\nthat is, they can be written as unions of simple sets, called granules. The\nlatter can be identified with ""if. . . , then. . . "" rules, which form the\nbackbone of rough set rule induction. It has been shown previously that this\nproperty can be maintained for various fuzzy rough set models, including those\nbased on ordered weighted average (OWA) operators. In this paper, we will focus\non some instances of the general class of fuzzy quantifier-based fuzzy rough\nsets (FQFRS). In these models, the lower and upper approximations are evaluated\nusing binary and unary fuzzy quantifiers, respectively. One of the main targets\nof this study is to examine the granular representation of different models of\nFQFRS. The main findings reveal that Choquet-based fuzzy rough sets can be\nrepresented granularly under the same conditions as OWA-based fuzzy rough sets,\nwhereas Sugeno-based FRS can always be represented granularly. This observation\nhighlights the potential of these models for resolving data inconsistencies and\nmanaging noise.\n']",Fuzzy Rough Sets and Quantification Models
404,403,17,403_submodularity_submodular_knapsack_optimization,"['submodularity', 'submodular', 'knapsack', 'optimization', 'maximizing', 'optimal', 'maximization', 'algorithms', 'subroutine', 'complexity']","['submodular', 'approximation', 'monotone', 'maximization', 'greedy', 'constraint', 'adaptivity', 'algorithm', 'adaptive', 'ratio']","['submodularity', 'knapsack', 'algorithms', 'subroutine', 'greedyml', 'parallelizable', 'cost', 'decomposable', 'stochastic', 'instances']","['  Non-monotone constrained submodular maximization plays a crucial role in\nvarious machine learning applications. However, existing algorithms often\nstruggle with a trade-off between approximation guarantees and practical\nefficiency. The current state-of-the-art is a recent $0.401$-approximation\nalgorithm, but its computational complexity makes it highly impractical. The\nbest practical algorithms for the problem only guarantee $1/e$-approximation.\nIn this work, we present a novel algorithm for submodular maximization subject\nto a cardinality constraint that combines a guarantee of $0.385$-approximation\nwith a low and practical query complexity of $O(n+k^2)$. Furthermore, we\nevaluate the empirical performance of our algorithm in experiments based on\nvarious machine learning applications, including Movie Recommendation, Image\nSummarization, and more. These experiments demonstrate the efficacy of our\napproach.\n', '  Constrained submodular maximization problems encompass a wide variety of\napplications, including personalized recommendation, team formation, and\nrevenue maximization via viral marketing. The massive instances occurring in\nmodern day applications can render existing algorithms prohibitively slow,\nwhile frequently, those instances are also inherently stochastic. Focusing on\nthese challenges, we revisit the classic problem of maximizing a (possibly\nnon-monotone) submodular function subject to a knapsack constraint. We present\na simple randomized greedy algorithm that achieves a $5.83$ approximation and\nruns in $O(n \\log n)$ time, i.e., at least a factor $n$ faster than other\nstate-of-the-art algorithms. The robustness of our approach allows us to\nfurther transfer it to a stochastic version of the problem. There, we obtain a\n9-approximation to the best adaptive policy, which is the first constant\napproximation for non-monotone objectives. Experimental evaluation of our\nalgorithms showcases their improved performance on real and synthetic data.\n', '  Submodular optimization is a fundamental problem with many applications in\nmachine learning, often involving decision-making over datasets with sensitive\nattributes such as gender or age. In such settings, it is often desirable to\nproduce a diverse solution set that is fairly distributed with respect to these\nattributes. Motivated by this, we initiate the study of Fair Submodular Cover\n(FSC), where given a ground set $U$, a monotone submodular function\n$f:2^U\\to\\mathbb{R}_{\\ge 0}$, a threshold $\\tau$, the goal is to find a\nbalanced subset of $S$ with minimum cardinality such that $f(S)\\ge\\tau$. We\nfirst introduce discrete algorithms for FSC that achieve a bicriteria\napproximation ratio of $(\\frac{1}{\\epsilon}, 1-O(\\epsilon))$. We then present a\ncontinuous algorithm that achieves a $(\\ln\\frac{1}{\\epsilon},\n1-O(\\epsilon))$-bicriteria approximation ratio, which matches the best\napproximation guarantee of submodular cover without a fairness constraint.\nFinally, we complement our theoretical results with a number of empirical\nevaluations that demonstrate the effectiveness of our algorithms on instances\nof maximum coverage.\n']",Submodular Optimization
405,404,17,404_debates_debater_debaters_debating,"['debates', 'debater', 'debaters', 'debating', 'agent', 'agents', 'debate', 'discussion', 'agent4debate', 'verbalised']","['debate', 'agent', 'debating', 'discussion', 'debates', 'agents', 'judge', 'consultancy', 'multi', 'reasoning']","['debaters', 'agent4debate', 'verbalised', 'judge', 'confrontational', 'cognition', 'debatrix', 'evaluators', 'rebuttal', 'consultants']","['  Multi-agent debate has proven effective in improving large language models\nquality for reasoning and factuality tasks. While various role-playing\nstrategies in multi-agent debates have been explored, in terms of the\ncommunication among agents, existing approaches adopt a brute force algorithm\n-- each agent can communicate with all other agents. In this paper, we\nsystematically investigate the effect of communication connectivity in\nmulti-agent systems. Our experiments on GPT and Mistral models reveal that\nmulti-agent debates leveraging sparse communication topology can achieve\ncomparable or superior performance while significantly reducing computational\ncosts. Furthermore, we extend the multi-agent debate framework to multimodal\nreasoning and alignment labeling tasks, showcasing its broad applicability and\neffectiveness. Our findings underscore the importance of communication\nconnectivity on enhancing the efficiency and effectiveness of the ""society of\nminds"" approach.\n', ""  Scalable oversight protocols aim to enable humans to accurately supervise\nsuperhuman AI. In this paper we study debate, where two AI's compete to\nconvince a judge; consultancy, where a single AI tries to convince a judge that\nasks questions; and compare to a baseline of direct question-answering, where\nthe judge just answers outright without the AI. We use large language models\n(LLMs) as both AI agents and as stand-ins for human judges, taking the judge\nmodels to be weaker than agent models. We benchmark on a diverse range of\nasymmetries between judges and agents, extending previous work on a single\nextractive QA task with information asymmetry, to also include mathematics,\ncoding, logic and multimodal reasoning asymmetries. We find that debate\noutperforms consultancy across all tasks when the consultant is randomly\nassigned to argue for the correct/incorrect answer. Comparing debate to direct\nquestion answering, the results depend on the type of task: in extractive QA\ntasks with information asymmetry debate outperforms direct question answering,\nbut in other tasks without information asymmetry the results are mixed.\nPrevious work assigned debaters/consultants an answer to argue for. When we\nallow them to instead choose which answer to argue for, we find judges are less\nfrequently convinced by the wrong answer in debate than in consultancy.\nFurther, we find that stronger debater models increase judge accuracy, though\nmore modestly than in previous studies.\n"", '  Competitive debate is a comprehensive and complex computational argumentation\ntask. Large Language Models (LLMs) encounter hallucinations and lack\ncompetitiveness in this task. To address these challenges, we introduce Agent\nfor Debate (Agent4Debate), a dynamic, multi-agent framework based on LLMs\ndesigned to enhance their capabilities in competitive debate. Drawing\ninspiration from human behavior in debate preparation and execution,\nAgent4Debate employs a collaborative architecture where four specialized agents\n(Searcher, Analyzer, Writer, and Reviewer) dynamically interact and cooperate.\nThese agents work throughout the debate process, covering multiple stages from\ninitial research and argument formulation to rebuttal and summary. To\ncomprehensively evaluate framework performance, we construct the Chinese Debate\nArena, comprising 66 carefully selected Chinese debate motions. We recruite ten\nexperienced human debaters and collect records of 200 debates involving\nAgent4Debate, baseline models, and humans. The evaluation employs the Debatrix\nautomatic scoring system and professional human reviewers based on the\nestablished Debatrix-Elo and Human-Elo ranking. Experimental results indicate\nthat the state-of-the-art Agent4Debate exhibits capabilities comparable to\nthose of humans. Furthermore, ablation studies demonstrate the effectiveness of\neach component in the agent structure.\n']",Artificial Intelligence Debate Systems
406,405,17,405_vehicular_supervised_vehicles_vehicle,"['vehicular', 'supervised', 'vehicles', 'vehicle', 'cloud', 'networks', 'federated', 'driving', 'learning', 'autonomous']","['vehicular', 'vehicles', 'vehicle', 'transportation', 'driver', 'aggregation', 'driving', 'privacy', 'convergence', 'autonomous']","['vehicular', 'supervised', 'cloud', 'federated', 'privacy', 'driver', 'openv2v', 'fedbevt', 'tabnet', 'fedpylot']","['  The forthcoming 6G-enabled Intelligent Transportation System (ITS) is set to\nredefine conventional transportation networks with advanced intelligent\nservices and applications. These technologies, including edge computing,\nMachine Learning (ML), and network softwarization, pose stringent requirements\nfor latency, energy efficiency, and user data security. Distributed Learning\n(DL), such as Federated Learning (FL), is essential to meet these demands by\ndistributing the learning process at the network edge. However, traditional FL\napproaches often require substantial resources for satisfactory learning\nperformance. In contrast, Transfer Learning (TL) and Split Learning (SL) have\nshown effectiveness in enhancing learning efficiency in resource-constrained\nwireless scenarios like ITS. Non-terrestrial Networks (NTNs) have recently\nacquired a central place in the 6G vision, especially for boosting the\ncoverage, capacity, and resilience of traditional terrestrial facilities.\nAir-based NTN layers, such as High Altitude Platforms (HAPs), can have added\nadvantages in terms of reduced transmission distances and flexible deployments\nand thus can be exploited to enable intelligent solutions for latency-critical\nvehicular scenarios. With this motivation, in this work, we introduce the\nconcept of Federated Split Transfer Learning (FSTL) in joint air-ground\nnetworks for resource-constrained vehicular scenarios. Simulations carried out\nin vehicular scenarios validate the efficacy of FSTL on HAPs in NTN,\ndemonstrating significant improvements in addressing the demands of ITS\napplications.\n', '  Federated Learning (FL) is an advanced distributed machine learning approach,\nthat protects the privacy of each vehicle by allowing the model to be trained\non multiple devices simultaneously without the need to upload all data to a\nroad side unit (RSU). This enables FL to handle scenarios with sensitive or\nwidely distributed data. However, in these fields, it is well known that the\nlabeling costs can be a significant expense, and models relying on labels are\nnot suitable for these rapidly evolving fields especially in vehicular\nnetworks, or mobile internet of things (MIoT), where new data emerges\nconstantly. To handle this issue, the self-supervised learning paves the way\nfor training without labels. Additionally, for vehicles with high velocity,\nowing to blurred images, simple aggregation not only impacts the accuracy of\nthe aggregated model but also reduces the convergence speed of FL. This paper\nproposes a FL algorithm based on image blur level to aggregation, called\nFLSimCo, which does not require labels and serves as a pre-training stage for\nself-supervised learning in the vehicular environment. Simulation results\ndemonstrate that the proposed algorithm exhibits fast and stable convergence.\n', ""  Vehicular edge intelligence (VEI) is a promising paradigm for enabling future\nintelligent transportation systems by accommodating artificial intelligence\n(AI) at the vehicular edge computing (VEC) system. Federated learning (FL)\nstands as one of the fundamental technologies facilitating collaborative model\ntraining locally and aggregation, while safeguarding the privacy of vehicle\ndata in VEI. However, traditional FL faces challenges in adapting to vehicle\nheterogeneity, training large models on resource-constrained vehicles, and\nremaining susceptible to model weight privacy leakage. Meanwhile, split\nlearning (SL) is proposed as a promising collaborative learning framework which\ncan mitigate the risk of model wights leakage, and release the training\nworkload on vehicles. SL sequentially trains a model between a vehicle and an\nedge cloud (EC) by dividing the entire model into a vehicle-side model and an\nEC-side model at a given cut layer. In this work, we combine the advantages of\nSL and FL to develop an Adaptive Split Federated Learning scheme for Vehicular\nEdge Computing (ASFV). The ASFV scheme adaptively splits the model and\nparallelizes the training process, taking into account mobile vehicle selection\nand resource allocation. Our extensive simulations, conducted on\nnon-independent and identically distributed data, demonstrate that the proposed\nASFV solution significantly reduces training latency compared to existing\nbenchmarks, while adapting to network dynamics and vehicles' mobility.\n""]",Vehicular Networks and Autonomous Driving
407,406,17,406_intrusions_attacks_intrusion_security,"['intrusions', 'attacks', 'intrusion', 'security', 'reinforcement', 'defence', 'defense', 'attack', 'attacker', 'defender']","['defender', 'cyber', 'intrusion', 'attacker', 'infrastructure', 'strategies', 'attack', 'firewalls', 'defense', 'game']","['attacks', 'intrusion', 'reinforcement', 'defender', 'firewalls', 'malware', 'evolve', 'cyber', 'subgames', 'emulation']","[""  This paper addresses a significant gap in Autonomous Cyber Operations (ACO)\nliterature: the absence of effective edge-blocking ACO strategies in dynamic,\nreal-world networks. It specifically targets the cybersecurity vulnerabilities\nof organizational Active Directory (AD) systems. Unlike the existing literature\non edge-blocking defenses which considers AD systems as static entities, our\nstudy counters this by recognizing their dynamic nature and developing advanced\nedge-blocking defenses through a Stackelberg game model between attacker and\ndefender. We devise a Reinforcement Learning (RL)-based attack strategy and an\nRL-assisted Evolutionary Diversity Optimization-based defense strategy, where\nthe attacker and defender improve each other strategy via parallel gameplay. To\naddress the computational challenges of training attacker-defender strategies\non numerous dynamic AD graphs, we propose an RL Training Facilitator that\nprunes environments and neural networks to eliminate irrelevant elements,\nenabling efficient and scalable training for large graphs. We extensively train\nthe attacker strategy, as a sophisticated attacker model is essential for a\nrobust defense. Our empirical results successfully demonstrate that our\nproposed approach enhances defender's proficiency in hardening dynamic AD\ngraphs while ensuring scalability for large-scale AD.\n"", '  We study automated intrusion response and formulate the interaction between\nan attacker and a defender as an optimal stopping game where attack and defense\nstrategies evolve through reinforcement learning and self-play. The\ngame-theoretic modeling enables us to find defender strategies that are\neffective against a dynamic attacker, i.e. an attacker that adapts its strategy\nin response to the defender strategy. Further, the optimal stopping formulation\nallows us to prove that optimal strategies have threshold properties. To obtain\nnear-optimal defender strategies, we develop Threshold Fictitious Self-Play\n(T-FP), a fictitious self-play algorithm that learns Nash equilibria through\nstochastic approximation. We show that T-FP outperforms a state-of-the-art\nalgorithm for our use case. The experimental part of this investigation\nincludes two systems: a simulation system where defender strategies are\nincrementally learned and an emulation system where statistics are collected\nthat drive simulation runs and where learned strategies are evaluated. We argue\nthat this approach can produce effective defender strategies for a practical IT\ninfrastructure.\n', '  We study automated intrusion prevention using reinforcement learning.\nFollowing a novel approach, we formulate the problem of intrusion prevention as\nan (optimal) multiple stopping problem. This formulation gives us insight into\nthe structure of optimal policies, which we show to have threshold properties.\nFor most practical cases, it is not feasible to obtain an optimal defender\npolicy using dynamic programming. We therefore develop a reinforcement learning\napproach to approximate an optimal threshold policy. We introduce T-SPSA, an\nefficient reinforcement learning algorithm that learns threshold policies\nthrough stochastic approximation. We show that T-SPSA outperforms\nstate-of-the-art algorithms for our use case. Our overall method for learning\nand validating policies includes two systems: a simulation system where\ndefender policies are incrementally learned and an emulation system where\nstatistics are produced that drive simulation runs and where learned policies\nare evaluated. We show that this approach can produce effective defender\npolicies for a practical IT infrastructure.\n']",Cybersecurity Defense Strategies
408,407,16,407_ridesharing_taxis_passengers_taxi,"['ridesharing', 'taxis', 'passengers', 'taxi', 'intercity', 'trips', 'passenger', 'dispatching', 'vehicles', 'vehicle']","['demand', 'vehicle', 'ride', 'fleet', 'dispatching', 'rebalancing', 'taxis', 'taxi', 'mile', 'mobility']","['ridesharing', 'passengers', 'taxi', 'intercity', 'trips', 'routing', 'plans', 'multiagent', 'fleets', 'reinforcement']","[""  For solving problems from the domain of Mobility-on-Demand (MoD), we often\nneed to connect vehicle plans into plans spanning longer time, a process we\ncall plan chaining. As we show in this work, chaining of the plans can be used\nto reduce the size of MoD providers' fleet (fleet-sizing problem) but also to\nreduce the total driven distance by providing high-quality vehicle dispatching\nsolutions in MoD systems. Recently, a solution that uses this principle has\nbeen proposed to solve the fleet-sizing problem. The method does not consider\nthe time flexibility of the plans. Instead, plans are fixed in time and cannot\nbe delayed. However, time flexibility is an essential property of all vehicle\nproblems with time windows. This work presents a new plan chaining formulation\nthat considers delays as allowed by the time windows and a solution method for\nsolving it. Moreover, we prove that the proposed plan chaining method is\noptimal, and we analyze its complexity. Finally, we list some practical\napplications and perform a demonstration for one of them: a new heuristic\nvehicle dispatching method for solving the static dial-a-ride problem. The\ndemonstration results show that our proposed method provides a better solution\nthan the two heuristic baselines for the majority of instances that cannot be\nsolved optimally. At the same time, our method does not have the largest\ncomputational time requirements compared to the baselines. Therefore, we\nconclude that the proposed optimal chaining method provides not only\ntheoretically sound results but is also practically applicable.\n"", '  Mobility-on-demand (MoD) systems consist of a fleet of shared vehicles that\ncan be hailed for one-way point-to-point trips. The total distance driven by\nthe vehicles and the fleet size can be reduced by employing ridesharing, i.e.,\nby assigning multiple passengers to one vehicle. However, finding the optimal\npassenger-vehicle assignment in an MoD system is a hard combinatorial problem.\nIn this work, we demonstrate how the VGA method, a recently proposed systematic\nmethod for ridesharing, can be used to compute the optimal passenger-vehicle\nassignments and corresponding vehicle routes in a massive-scale MoD system. In\ncontrast to existing works, we solve all passenger-vehicle assignment problems\nto optimality, regularly dealing with instances containing thousands of\nvehicles and passengers. Moreover, to examine the impact of using optimal\nridesharing assignments, we compare the performance of an MoD system that uses\noptimal assignments against an MoD system that uses assignments computed using\ninsertion heuristic and against an MoD system that uses no ridesharing. We\nfound that the system that uses optimal ridesharing assignments subject to the\nmaximum travel delay of 4 minutes reduces the vehicle distance driven by 57 %\ncompared to an MoD system without ridesharing. Furthermore, we found that the\noptimal assignments result in a 20 % reduction in vehicle distance driven and 5\n% lower average passenger travel delay compared to a system that uses insertion\nheuristic.\n', ""  The emergence of on-demand ride pooling services allows each vehicle to serve\nmultiple passengers at a time, thus increasing drivers' income and enabling\npassengers to travel at lower prices than taxi/car on-demand services (only one\npassenger can be assigned to a car at a time like UberX and Lyft). Although\non-demand ride pooling services can bring so many benefits, ride pooling\nservices need a well-defined matching strategy to maximize the benefits for all\nparties (passengers, drivers, aggregation companies and environment), in which\nthe regional dispatching of vehicles has a significant impact on the matching\nand revenue. Existing algorithms often only consider revenue maximization,\nwhich makes it difficult for requests with unusual distribution to get a ride.\nHow to increase revenue while ensuring a reasonable assignment of requests\nbrings a challenge to ride pooling service companies (aggregation companies).\nIn this paper, we propose a framework for vehicle dispatching for ride pooling\ntasks, which splits the city into discrete dispatching regions and uses the\nreinforcement learning (RL) algorithm to dispatch vehicles in these regions. We\nalso consider the mutual information (MI) between vehicle and order\ndistribution as the intrinsic reward of the RL algorithm to improve the\ncorrelation between their distributions, thus ensuring the possibility of\ngetting a ride for unusually distributed requests. In experimental results on a\nreal-world taxi dataset, we demonstrate that our framework can significantly\nincrease revenue up to an average of 3\\% over the existing best on-demand ride\npooling method.\n""]",Ridesharing and Taxi Services Optimization
409,408,16,408_electroencephalogram_electroencephalography_eeg_rl_eeg,"['electroencephalogram', 'electroencephalography', 'eeg_rl', 'eeg', 'eeg_glt', 'bci', 'neurophysiology', 'graphs', 'graph', 'brainwave']","['brain', 'emotion', 'adjacency', 'electrode', 'emotional', 'physiological', 'graph', 'spatial', 'recognition', 'layouts']","['electroencephalography', 'eeg_glt', 'graphs', 'neurognn', 'electrodes', 'emotions', 'signals', 'gcns', 'ema', 'contextual']","['  Brain-Computer Interfaces connect the brain to external control devices,\nnecessitating the accurate translation of brain signals such as from\nelectroencephalography (EEG) into executable commands. Graph Neural Networks\n(GCN) have been increasingly applied for classifying EEG Motor Imagery signals,\nprimarily because they incorporates the spatial relationships among EEG\nchannels, resulting in improved accuracy over traditional convolutional\nmethods. Recent advances by GCNs-Net in real-time EEG MI signal classification\nutilised Pearson Coefficient Correlation (PCC) for constructing adjacency\nmatrices, yielding significant results on the PhysioNet dataset. Our paper\nintroduces the EEG Graph Lottery Ticket (EEG_GLT) algorithm, an innovative\ntechnique for constructing adjacency matrices for EEG channels. It does not\nrequire pre-existing knowledge of inter-channel relationships, and it can be\ntailored to suit both individual subjects and GCN model architectures. Our\nfindings demonstrated that the PCC method outperformed the Geodesic approach by\n9.65% in mean accuracy, while our EEG_GLT matrix consistently exceeded the\nperformance of the PCC method by a mean accuracy of 13.39%. Also, we found that\nthe construction of the adjacency matrix significantly influenced accuracy, to\na greater extent than GCN model configurations. A basic GCN configuration\nutilising our EEG_GLT matrix exceeded the performance of even the most complex\nGCN setup with a PCC matrix in average accuracy. Our EEG_GLT method also\nreduced MACs by up to 97% compared to the PCC method, while maintaining or\nenhancing accuracy. In conclusion, the EEG_GLT algorithm marks a breakthrough\nin the development of optimal adjacency matrices, effectively boosting both\ncomputational accuracy and efficiency, making it well-suited for real-time\nclassification of EEG MI signals that demand intensive computational resources.\n', '  Compared to other modalities, EEG-based emotion recognition can intuitively\nrespond to the emotional patterns in the human brain and, therefore, has become\none of the most concerning tasks in the brain-computer interfaces field. Since\ndependencies within brain regions are closely related to emotion, a significant\ntrend is to develop Graph Neural Networks (GNNs) for EEG-based emotion\nrecognition. However, brain region dependencies in emotional EEG have\nphysiological bases that distinguish GNNs in this field from those in other\ntime series fields. Besides, there is neither a comprehensive review nor\nguidance for constructing GNNs in EEG-based emotion recognition. In the survey,\nour categorization reveals the commonalities and differences of existing\napproaches under a unified framework of graph construction. We analyze and\ncategorize methods from three stages in the framework to provide clear guidance\non constructing GNNs in EEG-based emotion recognition. In addition, we discuss\nseveral open challenges and future directions, such as Temporal full-connected\ngraph and Graph condensation.\n', '  Brain-Computer Interfaces (BCIs) rely on accurately decoding\nelectroencephalography (EEG) motor imagery (MI) signals for effective device\ncontrol. Graph Neural Networks (GNNs) outperform Convolutional Neural Networks\n(CNNs) in this regard, by leveraging the spatial relationships between EEG\nelectrodes through adjacency matrices. The EEG_GLT-Net framework, featuring the\nstate-of-the-art EEG_GLT adjacency matrix method, has notably enhanced EEG MI\nsignal classification, evidenced by an average accuracy of 83.95% across 20\nsubjects on the PhysioNet dataset. This significantly exceeds the 76.10%\naccuracy rate achieved using the Pearson Correlation Coefficient (PCC) method\nwithin the same framework.\n  In this research, we advance the field by applying a Reinforcement Learning\n(RL) approach to the classification of EEG MI signals. Our innovative method\nempowers the RL agent, enabling not only the classification of EEG MI data\npoints with higher accuracy, but effective identification of EEG MI data points\nthat are less distinct. We present the EEG_RL-Net, an enhancement of the\nEEG_GLT-Net framework, which incorporates the trained EEG GCN Block from\nEEG_GLT-Net at an adjacency matrix density of 13.39% alongside the RL-centric\nDueling Deep Q Network (Dueling DQN) block. The EEG_RL-Net model showcases\nexceptional classification performance, achieving an unprecedented average\naccuracy of 96.40% across 20 subjects within 25 milliseconds. This model\nillustrates the transformative effect of the RL in EEG MI time point\nclassification.\n']",Electroencephalography and Brain-Computer Interfaces
410,409,16,409_text_stylistic_linguistic_nlp,"['text', 'stylistic', 'linguistic', 'nlp', 'styles', 'sentiment', 'sentences', 'style', 'content', 'politeness']","['style', 'transfer', 'formality', 'styles', 'authorship', 'text', 'parallel', 'content', 'stylistic', 'meaning']","['stylistic', 'linguistic', 'sentiment', 'politeness', 'tst', 'lmstyle', 'embedders', 'punjabi', 'malayalam', 'improves']","['  Text style transfer (TST) aims to vary the style polarity of text while\npreserving the semantic content. Although recent advancements have demonstrated\nremarkable progress in short TST, it remains a relatively straightforward task\nwith limited practical applications. The more comprehensive long TST task\npresents two challenges: (1) existing methods encounter difficulties in\naccurately evaluating content attributes in multiple words, leading to content\ndegradation; (2) the conventional vanilla style classifier loss encounters\nobstacles in maintaining consistent style across multiple generated sentences.\n  In this paper, we propose a novel method SC2, where a multilayer Joint\nStyle-Content Weighed (JSCW) module and a Style Consistency loss are designed\nto address the two issues. The JSCW simultaneously assesses the amounts of\nstyle and content attributes within a token, aiming to acquire a lossless\ncontent representation and thereby enhancing content preservation. The multiple\nJSCW layers further progressively refine content representations. We design a\nstyle consistency loss to ensure the generated multiple sentences consistently\nreflect the target style polarity. Moreover, we incorporate a denoising\nnon-autoregressive decoder to accelerate the training. We conduct plentiful\nexperiments and the results show significant improvements of SC2 over\ncompetitive baselines. Our code: https://github.com/jiezhao6/SC2.\n', '  Text style transfer (TST) is an important task in controllable text\ngeneration, which aims to control selected attributes of language use, such as\npoliteness, formality, or sentiment, without altering the style-independent\ncontent of the text. The field has received considerable research attention in\nrecent years and has already been covered in several reviews, but the focus has\nmostly been on the development of new algorithms and learning from different\ntypes of data (supervised, unsupervised, out-of-domain, etc.) and not so much\non the application side. However, TST-related technologies are gradually\nreaching a production- and deployment-ready level, and therefore, the inclusion\nof the application perspective in TST research becomes crucial. Similarly, the\noften overlooked ethical considerations of TST technology have become a\npressing issue. This paper presents a comprehensive review of TST applications\nthat have been researched over the years, using both traditional linguistic\napproaches and more recent deep learning methods. We discuss current\nchallenges, future research directions, and ethical implications of TST\napplications in text generation. By providing a holistic overview of the\nlandscape of TST applications, we hope to stimulate further research and\ncontribute to a better understanding of the potential as well as ethical\nconsiderations associated with TST.\n', '  Text Style Transfer (TST) is a pivotal task in natural language generation to\nmanipulate text style attributes while preserving style-independent content.\nThe attributes targeted in TST can vary widely, including politeness,\nauthorship, mitigation of offensive language, modification of feelings, and\nadjustment of text formality. TST has become a widely researched topic with\nsubstantial advancements in recent years. This paper provides an introductory\noverview of TST, addressing its challenges, existing approaches, datasets,\nevaluation measures, subtasks, and applications. This fundamental overview\nimproves understanding of the background and fundamentals of text style\ntransfer.\n']",Text Style Transfer
411,410,16,410_confounders_confounder_recommender_recommending,"['confounders', 'confounder', 'recommender', 'recommending', 'bias', 'recommendations', 'recommendation', 'unbiasedness', 'causald', 'ratings']","['confounders', 'propensity', 'confounder', 'causal', 'unmeasured', 'recommendation', 'recommender', 'user', 'unbiased', 'bias']","['confounders', 'recommender', 'bias', 'causal', 'imputation', 'preferences', 'mediators', 'ispensable', 'fda', 'textbf']","['  Recommender models aim to capture user preferences from historical feedback\nand then predict user-specific feedback on candidate items. However, the\npresence of various unmeasured confounders causes deviations between the user\npreferences in the historical feedback and the true preferences, resulting in\nmodels not meeting their expected performance. Existing debias models either\n(1) specific to solving one particular bias or (2) directly obtain auxiliary\ninformation from user historical feedback, which cannot identify whether the\nlearned preferences are true user preferences or mixed with unmeasured\nconfounders. Moreover, we find that the former recommender system is not only a\nsuccessor to unmeasured confounders but also acts as an unmeasured confounder\naffecting user preference modeling, which has always been neglected in previous\nstudies. To this end, we incorporate the effect of the former recommender\nsystem and treat it as a proxy for all unmeasured confounders. We propose a\nnovel framework, Separating and Learning Latent Confounders For Recommendation\n(SLFR), which obtains the representation of unmeasured confounders to identify\nthe counterfactual feedback by disentangling user preferences and unmeasured\nconfounders, then guides the target model to capture the true preferences of\nusers. Extensive experiments in five real-world datasets validate the\nadvantages of our method.\n', ""  Recommender systems suffer from confounding biases when there exist\nconfounders affecting both item features and user feedback (e.g., like or not).\nExisting causal recommendation methods typically assume confounders are fully\nobserved and measured, forgoing the possible existence of hidden confounders in\nreal applications. For instance, product quality is a confounder since\naffecting both item prices and user ratings, but is hidden for the third-party\ne-commerce platform due to the difficulty of large-scale quality inspection;\nignoring it could result in the bias effect of over-recommending high-price\nitems. This work analyzes and addresses the problem from a causal perspective.\nThe key lies in modeling the causal effect of item features on a user's\nfeedback. To mitigate hidden confounding effects, it is compulsory but\nchallenging to estimate the causal effect without measuring the confounder.\nTowards this goal, we propose a Hidden Confounder Removal (HCR) framework that\nleverages front-door adjustment to decompose the causal effect into two partial\neffects, according to the mediators between item features and user feedback.\nThe partial effects are independent from the hidden confounder and\nidentifiable. During training, HCR performs multi-task learning to infer the\npartial effects from historical interactions. We instantiate HCR for two\nscenarios and conduct experiments on three real-world datasets. Empirical\nresults show that the HCR framework provides more accurate recommendations,\nespecially for less-active users. We will release the code once accepted.\n"", ""  In recent years, dual-target Cross-Domain Recommendation (CDR) has been\nproposed to capture comprehensive user preferences in order to ultimately\nenhance the recommendation accuracy in both data-richer and data-sparser\ndomains simultaneously. However, in addition to users' true preferences, the\nuser-item interactions might also be affected by confounders (e.g., free\nshipping, sales promotion). As a result, dual-target CDR has to meet two\nchallenges: (1) how to effectively decouple observed confounders, including\nsingle-domain confounders and cross-domain confounders, and (2) how to preserve\nthe positive effects of observed confounders on predicted interactions, while\neliminating their negative effects on capturing comprehensive user preferences.\nTo address the above two challenges, we propose a Causal Deconfounding\nframework via Confounder Disentanglement for dual-target Cross-Domain\nRecommendation, called CD2CDR. In CD2CDR, we first propose a confounder\ndisentanglement module to effectively decouple observed single-domain and\ncross-domain confounders. We then propose a causal deconfounding module to\npreserve the positive effects of such observed confounders and eliminate their\nnegative effects via backdoor adjustment, thereby enhancing the recommendation\naccuracy in each domain. Extensive experiments conducted on five real-world\ndatasets demonstrate that CD2CDR significantly outperforms the state-of-the-art\nmethods.\n""]",Debiasing Recommender Systems
412,411,16,411_interpretability_explainability_interpretable_explanations,"['interpretability', 'explainability', 'interpretable', 'explanations', 'nlp', 'semantics', 'interpreting', 'interpretml', 'summarize', 'ai']","['explainability', 'interpretability', 'mechanistic', 'mechanisms', 'interpretation', 'explanation', 'explanations', 'transparency', 'interpretable', 'abstract']","['interpretability', 'nlp', 'interpretml', 'summarize', 'ai', 'architecturally', 'conceptual', 'stakeholders', 'fields', 'talktoebm']","['  Large language models (LLMs) have led to breakthroughs in language tasks, yet\nthe internal mechanisms that enable their remarkable generalization and\nreasoning abilities remain opaque. This lack of transparency presents\nchallenges such as hallucinations, toxicity, and misalignment with human\nvalues, hindering the safe and beneficial deployment of LLMs. This paper aims\nto uncover the mechanisms underlying LLM functionality through the lens of\nexplainability. First, we review how knowledge is architecturally composed\nwithin LLMs and encoded in their internal parameters via mechanistic\ninterpretability techniques. Then, we summarize how knowledge is embedded in\nLLM representations by leveraging probing techniques and representation\nengineering. Additionally, we investigate the training dynamics through a\nmechanistic perspective to explain phenomena such as grokking and memorization.\nLastly, we explore how the insights gained from these explanations can enhance\nLLM performance through model editing, improve efficiency through pruning, and\nbetter align with human values.\n', ""  Large Language Models such as GPTs (Generative Pre-trained Transformers)\nexhibit remarkable capabilities across a broad spectrum of applications.\nNevertheless, due to their intrinsic complexity, these models present\nsubstantial challenges in interpreting their internal decision-making\nprocesses. This lack of transparency poses critical challenges when it comes to\ntheir adaptation by financial institutions, where concerns and accountability\nregarding bias, fairness, and reliability are of paramount importance.\nMechanistic interpretability aims at reverse engineering complex AI models such\nas transformers. In this paper, we are pioneering the use of mechanistic\ninterpretability to shed some light on the inner workings of large language\nmodels for use in financial services applications. We offer several examples of\nhow algorithmic tasks can be designed for compliance monitoring purposes. In\nparticular, we investigate GPT-2 Small's attention pattern when prompted to\nidentify potential violation of Fair Lending laws. Using direct logit\nattribution, we study the contributions of each layer and its corresponding\nattention heads to the logit difference in the residual stream. Finally, we\ndesign clean and corrupted prompts and use activation patching as a causal\nintervention method to localize our task completion components further. We\nobserve that the (positive) heads $10.2$ (head $2$, layer $10$), $10.7$, and\n$11.3$, as well as the (negative) heads $9.6$ and $10.6$ play a significant\nrole in the task completion.\n"", '  The necessity for interpretability in natural language processing (NLP) has\nrisen alongside the growing prominence of large language models. Among the\nmyriad tasks within NLP, text generation stands out as a primary objective of\nautoregressive models. The NLP community has begun to take a keen interest in\ngaining a deeper understanding of text generation, leading to the development\nof model-agnostic explainable artificial intelligence (xAI) methods tailored to\nthis task. The design and evaluation of explainability methods are non-trivial\nsince they depend on many factors involved in the text generation process,\ne.g., the autoregressive model and its stochastic nature. This paper outlines\n17 challenges categorized into three groups that arise during the development\nand assessment of attribution-based explainability methods. These challenges\nencompass issues concerning tokenization, defining explanation similarity,\ndetermining token importance and prediction change metrics, the level of human\nintervention required, and the creation of suitable test datasets. The paper\nillustrates how these challenges can be intertwined, showcasing new\nopportunities for the community. These include developing probabilistic\nword-level explainability methods and engaging humans in the explainability\npipeline, from the data design to the final evaluation, to draw robust\nconclusions on xAI methods.\n']",Interpretability and Explainability in AI and NLP
413,412,16,412_corrections_grammar_evaluations_linguistic,"['corrections', 'grammar', 'evaluations', 'linguistic', 'sentences', 'corpora', 'evaluation', 'annotated', 'fluently', 'correction']","['grammatical', 'correction', 'error', 'corrections', 'errors', 'grammar', 'sentences', 'languages', 'commercial', 'metrics']","['corrections', 'grammar', 'evaluations', 'corpora', 'annotated', 'fluency', 'fluent', 'recall', 'ged', 'insertions']","['  The paper focuses on improving the interpretability of Grammatical Error\nCorrection (GEC) metrics, which receives little attention in previous studies.\nTo bridge the gap, we propose CLEME2.0, a reference-based evaluation strategy\nthat can describe four elementary dimensions of GEC systems, namely\nhit-correction, error-correction, under-correction, and over-correction. They\ncollectively contribute to revealing the critical characteristics and locating\ndrawbacks of GEC systems. Evaluating systems by Combining these dimensions\nleads to high human consistency over other reference-based and reference-less\nmetrics. Extensive experiments on 2 human judgement datasets and 6 reference\ndatasets demonstrate the effectiveness and robustness of our method. All the\ncodes will be released after the peer review.\n', '  Thanks to recent advances in generative AI, we are able to prompt large\nlanguage models (LLMs) to produce texts which are fluent and grammatical. In\naddition, it has been shown that we can elicit attempts at grammatical error\ncorrection (GEC) from LLMs when prompted with ungrammatical input sentences. We\nevaluate how well LLMs can perform at GEC by measuring their performance on\nestablished benchmark datasets. We go beyond previous studies, which only\nexamined GPT* models on a selection of English GEC datasets, by evaluating\nseven open-source and three commercial LLMs on four established GEC benchmarks.\nWe investigate model performance and report results against individual error\ntypes. Our results indicate that LLMs do not always outperform supervised\nEnglish GEC models except in specific contexts -- namely commercial LLMs on\nbenchmarks annotated with fluency corrections as opposed to minimal edits. We\nfind that several open-source models outperform commercial ones on minimal edit\nbenchmarks, and that in some settings zero-shot prompting is just as\ncompetitive as few-shot prompting.\n', '  This paper investigates the application of GPT-3.5 for Grammatical Error\nCorrection (GEC) in multiple languages in several settings: zero-shot GEC,\nfine-tuning for GEC, and using GPT-3.5 to re-rank correction hypotheses\ngenerated by other GEC models. In the zero-shot setting, we conduct automatic\nevaluations of the corrections proposed by GPT-3.5 using several methods:\nestimating grammaticality with language models (LMs), the Scribendi test, and\ncomparing the semantic embeddings of sentences. GPT-3.5 has a known tendency to\nover-correct erroneous sentences and propose alternative corrections. For\nseveral languages, such as Czech, German, Russian, Spanish, and Ukrainian,\nGPT-3.5 substantially alters the source sentences, including their semantics,\nwhich presents significant challenges for evaluation with reference-based\nmetrics. For English, GPT-3.5 demonstrates high recall, generates fluent\ncorrections, and generally preserves sentence semantics. However, human\nevaluation for both English and Russian reveals that, despite its strong\nerror-detection capabilities, GPT-3.5 struggles with several error types,\nincluding punctuation mistakes, tense errors, syntactic dependencies between\nwords, and lexical compatibility at the sentence level.\n']",Grammatical Error Correction Evaluation
414,413,16,413_robust_minimizes_optimization_distributional,"['robust', 'minimizes', 'optimization', 'distributional', 'distributionally', 'minimization', 'nonparametrics', 'regularized', 'minimax', 'nonparametric']","['risk', 'robust', 'convex', 'estimators', 'multistage', 'optimization', 'distributional', 'numerical', 'measurable', 'covariate']","['robust', 'distributionally', 'nonparametrics', 'minimax', 'risk', 'estimators', 'convex', 'wasserstein', 'cvar', 'formulation']","['  We present a general duality result for Wasserstein distributionally robust\noptimization that holds for any Kantorovich transport cost, measurable loss\nfunction, and nominal probability distribution. Assuming an interchangeability\nprinciple inherent in existing duality results, our proof only uses\none-dimensional convex analysis. Furthermore, we demonstrate that the\ninterchangeability principle holds if and only if certain measurable projection\nand weak measurable selection conditions are satisfied. To illustrate the\nbroader applicability of our approach, we provide a rigorous treatment of\nduality results in distributionally robust Markov decision processes and\ndistributionally robust multistage stochastic programming. Additionally, we\nextend our analysis to other problems such as infinity-Wasserstein\ndistributionally robust optimization, risk-averse optimization, and globalized\ndistributionally robust counterpart.\n', '  Distributionally robust optimization (DRO) is a powerful framework for\ntraining robust models against data distribution shifts. This paper focuses on\nconstrained DRO, which has an explicit characterization of the robustness\nlevel. Existing studies on constrained DRO mostly focus on convex loss\nfunction, and exclude the practical and challenging case with non-convex loss\nfunction, e.g., neural network. This paper develops a stochastic algorithm and\nits performance analysis for non-convex constrained DRO. The computational\ncomplexity of our stochastic algorithm at each iteration is independent of the\noverall dataset size, and thus is suitable for large-scale applications. We\nfocus on the general Cressie-Read family divergence defined uncertainty set\nwhich includes $\\chi^2$-divergences as a special case. We prove that our\nalgorithm finds an $\\epsilon$-stationary point with a computational complexity\nof $\\mathcal O(\\epsilon^{-3k_*-5})$, where $k_*$ is the parameter of the\nCressie-Read divergence. The numerical results indicate that our method\noutperforms existing methods.} Our method also applies to the smoothed\nconditional value at risk (CVaR) DRO.\n', '  We consider the penalized distributionally robust optimization (DRO) problem\nwith a closed, convex uncertainty set, a setting that encompasses the $f$-DRO,\nWasserstein-DRO, and spectral/$L$-risk formulations used in practice. We\npresent Drago, a stochastic primal-dual algorithm that achieves a\nstate-of-the-art linear convergence rate on strongly convex-strongly concave\nDRO problems. The method combines both randomized and cyclic components with\nmini-batching, which effectively handles the unique asymmetric nature of the\nprimal and dual problems in DRO. We support our theoretical results with\nnumerical benchmarks in classification and regression.\n']",Distributionally Robust Optimization
415,414,16,414_sparse_cnns_encoders_sparsely,"['sparse', 'cnns', 'encoders', 'sparsely', 'deep', 'features', 'representations', 'networks', 'locality', 'feature']","['sparse', 'dictionary', 'features', 'locality', 'hierarchy', 'representations', 'simplified', 'proxies', 'receptive', 'atoms']","['cnns', 'encoders', 'deep', 'features', 'locality', 'dimensionality', 'sparsity', 'receptive', 'dictionaries', 'dsd']","['  The classical sparse coding (SC) model represents visual stimuli as a linear\ncombination of a handful of learned basis functions that are Gabor-like when\ntrained on natural image data. However, the Gabor-like filters learned by\nclassical sparse coding far overpredict well-tuned simple cell receptive field\nprofiles observed empirically. While neurons fire sparsely, neuronal\npopulations are also organized in physical space by their sensitivity to\ncertain features. In V1, this organization is a smooth progression of\norientations along the cortical sheet. A number of subsequent models have\neither discarded the sparse dictionary learning framework entirely or whose\nupdates have yet to take advantage of the surge in unrolled, neural dictionary\nlearning architectures. A key missing theme of these updates is a stronger\nnotion of \\emph{structured sparsity}. We propose an autoencoder architecture\n(WLSC) whose latent representations are implicitly, locally organized for\nspectral clustering through a Laplacian quadratic form of a bipartite graph,\nwhich generates a diverse set of artificial receptive fields that match primate\ndata in V1 as faithfully as recent contrastive frameworks like Local Low\nDimensionality, or LLD \\citep{lld} that discard sparse dictionary learning. By\nunifying sparse and smooth coding in models of the early visual cortex through\nour autoencoder, we also show that our regularization can be interpreted as\nearly-stage specialization of receptive fields to certain classes of stimuli;\nthat is, we induce a weak clustering bias for later stages of cortex where\nfunctional and spatial segregation (i.e. topography) are known to occur. The\nresults show an imperative for \\emph{spatial regularization} of both the\nreceptive fields and firing rates to begin to describe feature disentanglement\nin V1 and beyond.\n', '  Vision tasks are characterized by the properties of locality and translation\ninvariance. The superior performance of convolutional neural networks (CNNs) on\nthese tasks is widely attributed to the inductive bias of locality and weight\nsharing baked into their architecture. Existing attempts to quantify the\nstatistical benefits of these biases in CNNs over locally connected\nconvolutional neural networks (LCNs) and fully connected neural networks (FCNs)\nfall into one of the following categories: either they disregard the optimizer\nand only provide uniform convergence upper bounds with no separating lower\nbounds, or they consider simplistic tasks that do not truly mirror the locality\nand translation invariance as found in real-world vision tasks. To address\nthese deficiencies, we introduce the Dynamic Signal Distribution (DSD)\nclassification task that models an image as consisting of $k$ patches, each of\ndimension $d$, and the label is determined by a $d$-sparse signal vector that\ncan freely appear in any one of the $k$ patches. On this task, for any\northogonally equivariant algorithm like gradient descent, we prove that CNNs\nrequire $\\tilde{O}(k+d)$ samples, whereas LCNs require $\\Omega(kd)$ samples,\nestablishing the statistical advantages of weight sharing in translation\ninvariant tasks. Furthermore, LCNs need $\\tilde{O}(k(k+d))$ samples, compared\nto $\\Omega(k^2d)$ samples for FCNs, showcasing the benefits of locality in\nlocal tasks. Additionally, we develop information theoretic tools for analyzing\nrandomized algorithms, which may be of interest for statistical research.\n', '  In this paper, we provide a theoretical analysis of the inductive biases in\nconvolutional neural networks (CNNs). We start by examining the universality of\nCNNs, i.e., the ability to approximate any continuous functions. We prove that\na depth of $\\mathcal{O}(\\log d)$ suffices for deep CNNs to achieve this\nuniversality, where $d$ in the input dimension. Additionally, we establish that\nlearning sparse functions with CNNs requires only\n$\\widetilde{\\mathcal{O}}(\\log^2d)$ samples, indicating that deep CNNs can\nefficiently capture {\\em long-range} sparse correlations. These results are\nmade possible through a novel combination of the multichanneling and\ndownsampling when increasing the network depth. We also delve into the distinct\nroles of weight sharing and locality in CNNs. To this end, we compare the\nperformance of CNNs, locally-connected networks (LCNs), and fully-connected\nnetworks (FCNs) on a simple regression task, where LCNs can be viewed as CNNs\nwithout weight sharing. On the one hand, we prove that LCNs require\n${\\Omega}(d)$ samples while CNNs need only $\\widetilde{\\mathcal{O}}(\\log^2d)$\nsamples, highlighting the critical role of weight sharing. On the other hand,\nwe prove that FCNs require $\\Omega(d^2)$ samples, whereas LCNs need only\n$\\widetilde{\\mathcal{O}}(d)$ samples, underscoring the importance of locality.\nThese provable separations quantify the difference between the two biases, and\nthe major observation behind our proof is that weight sharing and locality\nbreak different symmetries in the learning process.\n']",Sparse Representations in Deep Neural Networks
416,415,16,415_surfaces_surface_curvature_curvatures,"['surfaces', 'surface', 'curvature', 'curvatures', 'neural', 'mesh', '3d', 'shapes', 'geometry', 'meshes']","['surface', 'surfaces', 'shape', 'distance', 'implicit', 'geometric', 'geometry', 'occupancy', 'clouds', 'fields']","['surfaces', 'curvature', '3d', 'edges', 'multilayer', 'intrinsic', 'rendering', 'representations', 'perceptrons', 'implicit']","['  Neural shape representation generally refers to representing 3D geometry\nusing neural networks, e.g., to compute a signed distance or occupancy value at\na specific spatial position. In this paper, we present a novel encoder-decoder\nneural network for embedding 3D shapes in a single forward pass. Our\narchitecture is based on a multi-scale hybrid system incorporating graph-based\nand voxel-based components, as well as a continuously differentiable decoder.\nFurthermore, the network is trained to solve the Eikonal equation and only\nrequires knowledge of the zero-level set for training and inference. This means\nthat in contrast to most previous work, our network is able to output valid\nsigned distance fields without explicit prior knowledge of non-zero distance\nvalues or shape occupancy. We further propose a modification of the loss\nfunction in case that surface normals are not well defined, e.g., in the\ncontext of non-watertight surfaces and non-manifold geometry. Overall, this can\nhelp reduce the computational overhead of training and evaluating neural\ndistance fields, as well as enabling the application to difficult shapes. We\nfinally demonstrate the efficacy, generalizability and scalability of our\nmethod on datasets consisting of deforming shapes, both based on simulated data\nand raw 3D scans. We further show single-class and multi-class encoding, on\nboth fixed and variable vertex-count inputs, showcasing a wide range of\npossible applications.\n', '  In the field of computer vision, the numerical encoding of 3D surfaces is\ncrucial. It is classical to represent surfaces with their Signed Distance\nFunctions (SDFs) or Unsigned Distance Functions (UDFs). For tasks like\nrepresentation learning, surface classification, or surface reconstruction,\nthis function can be learned by a neural network, called Neural Distance\nFunction. This network, and in particular its weights, may serve as a\nparametric and implicit representation for the surface. The network must\nrepresent the surface as accurately as possible. In this paper, we propose a\nmethod for learning UDFs that improves the fidelity of the obtained Neural UDF\nto the original 3D surface. The key idea of our method is to concentrate the\nlearning effort of the Neural UDF on surface edges. More precisely, we show\nthat sampling more training points around surface edges allows better local\naccuracy of the trained Neural UDF, and thus improves the global expressiveness\nof the Neural UDF in terms of Hausdorff distance. To detect surface edges, we\npropose a new statistical method based on the calculation of a $p$-value at\neach point on the surface. Our method is shown to detect surface edges more\naccurately than a commonly used local geometric descriptor.\n', '  Neural surfaces (e.g., neural map encoding, deep implicits and neural\nradiance fields) have recently gained popularity because of their generic\nstructure (e.g., multi-layer perceptron) and easy integration with modern\nlearning-based setups. Traditionally, we have a rich toolbox of geometry\nprocessing algorithms designed for polygonal meshes to analyze and operate on\nsurface geometry. However, neural representations are typically discretized and\nconverted into a mesh, before applying any geometry processing algorithm. This\nis unsatisfactory and, as we demonstrate, unnecessary. In this work, we propose\na spherical neural surface representation (a spherical parametrization) for\ngenus-0 surfaces and demonstrate how to compute core geometric operators\ndirectly on this representation. Namely, we show how to construct the normals\nand the first and second fundamental forms of the surface, and how to compute\nthe surface gradient, surface divergence and Laplace Beltrami operator on\nscalar/vector fields defined on the surface. These operators, in turn, enable\nus to create geometry processing tools that act directly on the neural\nrepresentations without any unnecessary meshing. We demonstrate illustrative\napplications in (neural) spectral analysis, heat flow and mean curvature flow,\nand our method shows robustness to isometric shape variations. We both propose\ntheoretical formulations and validate their numerical estimates. By\nsystematically linking neural surface representations with classical geometry\nprocessing algorithms, we believe this work can become a key ingredient in\nenabling neural geometry processing.\n']",Neural Representation of 3D Surfaces and Geometry
417,416,16,416_multimodal_fusioninn_fusion_multinpe,"['multimodal', 'fusioninn', 'fusion', 'multinpe', 'imaging', 'deep', 'autoprognosis', 'facial', 'fused', 'biomedical']","['fusion', 'facial', 'multimodal', 'syndrome', 'clinical', 'modalities', 'modality', 'palsy', 'genetic', 'imaging']","['multimodal', 'fusioninn', 'multinpe', 'deep', 'autoprognosis', 'facial', 'biomedical', 'aiforcovid', 'genome', 'macro']","['  Algorithmic detection of facial palsy offers the potential to improve current\npractices, which usually involve labor-intensive and subjective assessment by\nclinicians. In this paper, we present a multimodal fusion-based deep learning\nmodel that utilizes unstructured data (i.e. an image frame with facial line\nsegments) and structured data (i.e. features of facial expressions) to detect\nfacial palsy. We then contribute to a study to analyze the effect of different\ndata modalities and the benefits of a multimodal fusion-based approach using\nvideos of 21 facial palsy patients. Our experimental results show that among\nvarious data modalities (i.e. unstructured data - RGB images and images of\nfacial line segments and structured data - coordinates of facial landmarks and\nfeatures of facial expressions), the feed-forward neural network using features\nof facial expression achieved the highest precision of 76.22 while the\nResNet-based model using images of facial line segments achieved the highest\nrecall of 83.47. When we leveraged both images of facial line segments and\nfeatures of facial expressions, our multimodal fusion-based deep learning model\nslightly improved the precision score to 77.05 at the expense of a decrease in\nthe recall score.\n', '  Multimodal medical imaging plays a pivotal role in clinical diagnosis and\nresearch, as it combines information from various imaging modalities to provide\na more comprehensive understanding of the underlying pathology. Recently, deep\nlearning-based multimodal fusion techniques have emerged as powerful tools for\nimproving medical image classification. This review offers a thorough analysis\nof the developments in deep learning-based multimodal fusion for medical\nclassification tasks. We explore the complementary relationships among\nprevalent clinical modalities and outline three main fusion schemes for\nmultimodal classification networks: input fusion, intermediate fusion\n(encompassing single-level fusion, hierarchical fusion, and attention-based\nfusion), and output fusion. By evaluating the performance of these fusion\ntechniques, we provide insight into the suitability of different network\narchitectures for various multimodal fusion scenarios and application domains.\nFurthermore, we delve into challenges related to network architecture\nselection, handling incomplete multimodal data management, and the potential\nlimitations of multimodal fusion. Finally, we spotlight the promising future of\nTransformer-based multimodal fusion techniques and give recommendations for\nfuture research in this rapidly evolving field.\n', '  Individuals with suspected rare genetic disorders often undergo multiple\nclinical evaluations, imaging studies, laboratory tests and genetic tests, to\nfind a possible answer over a prolonged period of time. Addressing this\n""diagnostic odyssey"" thus has substantial clinical, psychosocial, and economic\nbenefits. Many rare genetic diseases have distinctive facial features, which\ncan be used by artificial intelligence algorithms to facilitate clinical\ndiagnosis, in prioritizing candidate diseases to be further examined by lab\ntests or genetic assays, or in helping the phenotype-driven reinterpretation of\ngenome/exome sequencing data. Existing methods using frontal facial photos were\nbuilt on conventional Convolutional Neural Networks (CNNs), rely exclusively on\nfacial images, and cannot capture non-facial phenotypic traits and demographic\ninformation essential for guiding accurate diagnoses. Here we introduce\nGestaltMML, a multimodal machine learning (MML) approach solely based on the\nTransformer architecture. It integrates facial images, demographic information\n(age, sex, ethnicity), and clinical notes (optionally, a list of Human\nPhenotype Ontology terms) to improve prediction accuracy. Furthermore, we also\nevaluated GestaltMML on a diverse range of datasets, including 528 diseases\nfrom the GestaltMatcher Database, several in-house datasets of\nBeckwith-Wiedemann syndrome (BWS, over-growth syndrome with distinct facial\nfeatures), Sotos syndrome (overgrowth syndrome with overlapping features with\nBWS), NAA10-related neurodevelopmental syndrome, Cornelia de Lange syndrome\n(multiple malformation syndrome), and KBG syndrome (multiple malformation\nsyndrome). Our results suggest that GestaltMML effectively incorporates\nmultiple modalities of data, greatly narrowing candidate genetic diagnoses of\nrare diseases and may facilitate the reinterpretation of genome/exome\nsequencing data.\n']",Multimodal Fusion in Medical Imaging and Diagnosis
418,417,16,417_visualizations_visualization_visual_nl2vis,"['visualizations', 'visualization', 'visual', 'nl2vis', 'nlqs', 'interactive', 'text', 'leveragingllms', 'utterances', 'nlp']","['visualization', 'visualizations', 'notebook', 'notebooks', 'natural', 'charts', 'queries', 'interesting', 'plots', 'relevant']","['visualizations', 'nl2vis', 'nlqs', 'leveragingllms', 'utterances', 'datavist5', 'chartcheck', 'insights', 'schema', 'autohistograms']","['  Data visualization (DV) is the fundamental and premise tool to improve the\nefficiency in conveying the insights behind the big data, which has been widely\naccepted in existing data-driven world. Task automation in DV, such as\nconverting natural language queries to visualizations (i.e., text-to-vis),\ngenerating explanations from visualizations (i.e., vis-to-text), answering\nDV-related questions in free form (i.e. FeVisQA), and explicating tabular data\n(i.e., table-to-text), is vital for advancing the field. Despite their\npotential, the application of pre-trained language models (PLMs) like T5 and\nBERT in DV has been limited by high costs and challenges in handling\ncross-modal information, leading to few studies on PLMs for DV. We introduce\n\\textbf{DataVisT5}, a novel PLM tailored for DV that enhances the T5\narchitecture through a hybrid objective pre-training and multi-task fine-tuning\nstrategy, integrating text and DV datasets to effectively interpret cross-modal\nsemantics. Extensive evaluations on public datasets show that DataVisT5\nconsistently outperforms current state-of-the-art models on various DV-related\ntasks. We anticipate that DataVisT5 will not only inspire further research on\nvertical PLMs but also expand the range of applications for PLMs.\n', ""  NL2VIS (natural language to visualization) is a promising and recent research\narea that involves interpreting natural language queries and translating them\ninto visualizations that accurately represent the underlying data. As we\nnavigate the era of big data, NL2VIS holds considerable application potential\nsince it greatly facilitates data exploration by non-expert users. Following\nthe increasingly widespread usage of generative AI in NL2VIS applications, in\nthis paper we present V-RECS, the first LLM-based Visual Recommender augmented\nwith explanations(E), captioning(C), and suggestions(S) for further data\nexploration. V-RECS' visualization narratives facilitate both response\nverification and data exploration by non-expert users. Furthermore, our\nproposed solution mitigates computational, controllability, and cost issues\nassociated with using powerful LLMs by leveraging a methodology to effectively\nfine-tune small models. To generate insightful visualization narratives, we use\nChain-of-Thoughts (CoT), a prompt engineering technique to help LLM identify\nand generate the logical steps to produce a correct answer. Since CoT is\nreported to perform poorly with small LLMs, we adopted a strategy in which a\nlarge LLM (GPT-4), acting as a Teacher, generates CoT-based instructions to\nfine-tune a small model, Llama-2-7B, which plays the role of a Student.\nExtensive experiments-based on a framework for the quantitative evaluation of\nAI-based visualizations and on manual assessment by a group of\nparticipants-show that V-RECS achieves performance scores comparable to GPT-4,\nat a much lower cost. The efficacy of the V-RECS teacher-student paradigm is\nalso demonstrated by the fact that the un-tuned Llama fails to perform the task\nin the vast majority of test cases. We release V-RECS for the visualization\ncommunity to assist visualization designers throughout the entire visualization\ngeneration process.\n"", ""  Data visualization (DV) systems are increasingly recognized for their\nprofound capability to uncover insights from vast datasets, gaining attention\nacross both industry and academia. Crafting data queries is an essential\nprocess within certain declarative visualization languages (DVLs, e.g.,\nVega-Lite, EChart.). The evolution of natural language processing (NLP)\ntechnologies has streamlined the use of natural language interfaces to\nvisualize tabular data, offering a more accessible and intuitive user\nexperience. However, current methods for converting natural language questions\ninto data visualization queries, such as Seq2Vis, ncNet, and RGVisNet, despite\nutilizing complex neural network architectures, still fall short of\nexpectations and have great room for improvement.\n  Large language models (LLMs) such as ChatGPT and GPT-4, have established new\nbenchmarks in a variety of NLP tasks, fundamentally altering the landscape of\nthe field. Inspired by these advancements, we introduce a novel framework,\nPrompt4Vis, leveraging LLMs and in-context learning to enhance the performance\nof generating data visualization from natural language. Prompt4Vis comprises\ntwo key components: (1) a multi-objective example mining module, designed to\nfind out the truly effective examples that strengthen the LLM's in-context\nlearning capabilities for text-to-vis; (2) a schema filtering module, which is\nproposed to simplify the schema of the database. Extensive experiments through\n5-fold cross-validation on the NVBench dataset demonstrate the superiority of\nPrompt4Vis, which notably surpasses the state-of-the-art (SOTA) RGVisNet by\napproximately 35.9% and 71.3% on dev and test sets, respectively. To the best\nof our knowledge, Prompt4Vis is the first work that introduces in-context\nlearning into the text-to-vis for generating data visualization queries.\n""]",Natural Language to Visualization
419,418,16,418_testing_tests_codegen2_tester,"['testing', 'tests', 'codegen2', 'tester', 'fuzzing', 'test', 'methods2test', 'testsuite', 'testers', 'testchain']","['test', 'tests', 'coverage', 'unit', 'software', 'code', 'flakiness', 'fix', 'generation', 'flaky']","['testing', 'codegen2', 'fuzzing', 'testsuite', 'testers', 'testchain', 'trickybugs', 'development', 'oracles', 'python']","['  Rigorous software testing is crucial for developing and maintaining\nhigh-quality code, making automated test generation a promising avenue for both\nimproving software quality and boosting the effectiveness of code generation\nmethods. However, while code generation with Large Language Models (LLMs) is an\nextraordinarily active research area, test generation remains relatively\nunexplored. We address this gap and investigate the capability of LLM-based\nCode Agents for formalizing user issues into test cases. To this end, we\npropose a novel benchmark based on popular GitHub repositories, containing\nreal-world issues, ground-truth patches, and golden tests. We find that LLMs\ngenerally perform surprisingly well at generating relevant test cases with Code\nAgents designed for code repair exceeding the performance of systems designed\nspecifically for test generation. Further, as test generation is a similar but\nmore structured task than code generation, it allows for a more fine-grained\nanalysis using fail-to-pass rate and coverage metrics, providing a dual metric\nfor analyzing systems designed for code repair. Finally, we find that generated\ntests are an effective filter for proposed code fixes, doubling the precision\nof SWE-Agent.\n', ""  Unit tests represent the most basic level of testing within the software\ntesting lifecycle and are crucial to ensuring software correctness. Designing\nand creating unit tests is a costly and labor-intensive process that is ripe\nfor automation. Recently, Large Language Models (LLMs) have been applied to\nvarious aspects of software development, including unit test generation.\nAlthough several empirical studies evaluating LLMs' capabilities in test code\ngeneration exist, they primarily focus on simple scenarios, such as the\nstraightforward generation of unit tests for individual methods. These\nevaluations often involve independent and small-scale test units, providing a\nlimited view of LLMs' performance in real-world software development scenarios.\nMoreover, previous studies do not approach the problem at a suitable scale for\nreal-life applications. Generated unit tests are often evaluated via manual\nintegration into the original projects, a process that limits the number of\ntests executed and reduces overall efficiency. To address these gaps, we have\ndeveloped an approach for generating and evaluating more real-life complexity\ntest suites. Our approach focuses on class-level test code generation and\nautomates the entire process from test generation to test assessment. In this\nwork, we present \\textsc{AgoneTest}: an automated system for generating test\nsuites for Java projects and a comprehensive and principled methodology for\nevaluating the generated test suites. Starting from a state-of-the-art dataset\n(i.e., \\textsc{Methods2Test}), we built a new dataset for comparing\nhuman-written tests with those generated by LLMs. Our key contributions include\na scalable automated software system, a new dataset, and a detailed methodology\nfor evaluating test quality.\n"", ""  Testing is an essential part of modern software engineering to build reliable\nprograms. As testing the software is important but expensive, automatic test\ncase generation methods have become popular in software development. Unlike\ntraditional search-based coverage-guided test generation like fuzzing, neural\ntest generation backed by large language models can write tests that are\nsemantically meaningful and can be understood by other maintainers. However,\ncompared to regular code corpus, unit tests in the datasets are limited in\namount and diversity. In this paper, we present a novel data augmentation\ntechnique **FuzzAug**, that combines the advantages of fuzzing and large\nlanguage models. FuzzAug not only keeps valid program semantics in the\naugmented data, but also provides more diverse inputs to the function under\ntest, helping the model to associate correct inputs embedded with the\nfunction's dynamic behaviors with the function under test. We evaluate\nFuzzAug's benefits by using it on a neural test generation dataset to train\nstate-of-the-art code generation models. By augmenting the training set, our\nmodel generates test cases with $11\\%$ accuracy increases. Models trained with\nFuzzAug generate unit test functions with double the branch coverage compared\nto those without it. FuzzAug can be used across various datasets to train\nadvanced code generation models, enhancing their utility in automated software\ntesting. Our work shows the benefits of using dynamic analysis results to\nenhance neural test generation. Code and data will be publicly available.\n""]",Automated Software Testing with AI
420,419,16,419_incidents_automated_faultprofit_incident,"['incidents', 'automated', 'faultprofit', 'incident', 'alerts', 'microservices', 'faults', 'logs', 'service', 'services']","['incident', 'incidents', 'alerts', 'root', 'cloud', 'microservice', 'cause', 'alert', 'services', 'service']","['incidents', 'automated', 'faultprofit', 'alerts', 'microservices', 'telemetry', 'mitigation', 'triage', 'clouda', 'ontology']","[""  Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis\nprocess for cloud services, requiring on-call engineers to identify the primary\nissues and implement corrective actions to prevent future recurrences.\nImproving the incident RCA process is vital for minimizing service downtime,\ncustomer impact and manual toil. Recent advances in artificial intelligence\nhave introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which\nhave proven effective in tackling various AIOps problems, ranging from code\nauthoring to incident management. Nonetheless, the GPT-4 model's immense size\npresents challenges when trying to fine-tune it on user data because of the\nsignificant GPU resource demand and the necessity for continuous model\nfine-tuning with the emergence of new data. To address the high cost of\nfine-tuning LLM, we propose an in-context learning approach for automated root\ncausing, which eliminates the need for fine-tuning. We conduct extensive study\nover 100,000 production incidents, comparing several large language models\nusing multiple metrics. The results reveal that our in-context learning\napproach outperforms the previous fine-tuned large language models such as\nGPT-3 by an average of 24.8\\% across all metrics, with an impressive 49.7\\%\nimprovement over the zero-shot model. Moreover, human evaluation involving\nactual incident owners demonstrates its superiority over the fine-tuned model,\nachieving a 43.5\\% improvement in correctness and an 8.7\\% enhancement in\nreadability. The impressive results demonstrate the viability of utilizing a\nvanilla GPT model for the RCA task, thereby avoiding the high computational and\nmaintenance costs associated with a fine-tuned model.\n"", ""  Despite significant reliability efforts, large-scale cloud services\ninevitably experience production incidents that can significantly impact\nservice availability and customer's satisfaction. Worse, in many cases one\nincident can lead to multiple downstream failures due to cascading effects that\ncreates several related incidents across different dependent services. Often\ntime On-call Engineers (OCEs) examine these incidents in silos that lead to\nsignificant amount of manual toil and increase the overall time-to-mitigate\nincidents. Therefore, developing efficient incident linking models is of\nparamount importance for grouping related incidents into clusters so as to\nquickly resolve major outages and reduce on-call fatigue. Existing incident\nlinking methods mostly leverages textual and contextual information of\nincidents (e.g., title, description, severity, impacted components), thus\nfailing to leverage the inter-dependencies between services. In this paper, we\npropose the dependency-aware incident linking (DiLink) framework which\nleverages both textual and service dependency graph information to improve the\naccuracy and coverage of incident links not only coming from same service, but\nalso from different services and workloads. Furthermore, we propose a novel\nmethod to align the embeddings of multi-modal (i.e., textual and graphical)\ndata using Orthogonal Procrustes. Extensive experimental results on real-world\nincidents from 5 workloads of Microsoft demonstrate that our alignment method\nhas an F1-score of 0.96 (14% gain over current state-of-the-art methods). We\nare also in the process of deploying this solution across 610 services from\nthese 5 workloads for continuously supporting OCEs improving incident\nmanagement and reducing manual toil.\n"", ""  The growing complexity of cloud based software systems has resulted in\nincident management becoming an integral part of the software development\nlifecycle. Root cause analysis (RCA), a critical part of the incident\nmanagement process, is a demanding task for on-call engineers, requiring deep\ndomain knowledge and extensive experience with a team's specific services.\nAutomation of RCA can result in significant savings of time, and ease the\nburden of incident management on on-call engineers. Recently, researchers have\nutilized Large Language Models (LLMs) to perform RCA, and have demonstrated\npromising results. However, these approaches are not able to dynamically\ncollect additional diagnostic information such as incident related logs,\nmetrics or databases, severely restricting their ability to diagnose root\ncauses. In this work, we explore the use of LLM based agents for RCA to address\nthis limitation. We present a thorough empirical evaluation of a ReAct agent\nequipped with retrieval tools, on an out-of-distribution dataset of production\nincidents collected at Microsoft. Results show that ReAct performs\ncompetitively with strong retrieval and reasoning baselines, but with highly\nincreased factual accuracy. We then extend this evaluation by incorporating\ndiscussions associated with incident reports as additional inputs for the\nmodels, which surprisingly does not yield significant performance improvements.\nLastly, we conduct a case study with a team at Microsoft to equip the ReAct\nagent with tools that give it access to external diagnostic services that are\nused by the team for manual RCA. Our results show how agents can overcome the\nlimitations of prior work, and practical considerations for implementing such a\nsystem in practice.\n""]",Automated Incident Management in Cloud Services
421,420,16,420_conversational_language_dialogpt_textsc,"['conversational', 'language', 'dialogpt', 'textsc', 'nlp', 'chat', 'chatgpt', 'corpora', 'lexical', 'it_colombiarac_fullycurated_format_chatml_v1']","['chat', 'fine', 'richness', 'lexical', 'centric', 'understanding', 'tuning', 'language', 'synthesis', 'capabilities']","['language', 'dialogpt', 'textsc', 'chatgpt', 'it_colombiarac_fullycurated_format_chatml_v1', 'survey', 'llms2', 'telechat', 'importing', 'tuning']","['  With the emergence of numerous Large Language Models (LLM), the usage of such\nmodels in various Natural Language Processing (NLP) applications is increasing\nextensively. Counterspeech generation is one such key task where efforts are\nmade to develop generative models by fine-tuning LLMs with hatespeech -\ncounterspeech pairs, but none of these attempts explores the intrinsic\nproperties of large language models in zero-shot settings. In this work, we\npresent a comprehensive analysis of the performances of four LLMs namely GPT-2,\nDialoGPT, ChatGPT and FlanT5 in zero-shot settings for counterspeech\ngeneration, which is the first of its kind. For GPT-2 and DialoGPT, we further\ninvestigate the deviation in performance with respect to the sizes (small,\nmedium, large) of the models. On the other hand, we propose three different\nprompting strategies for generating different types of counterspeech and\nanalyse the impact of such strategies on the performance of the models. Our\nanalysis shows that there is an improvement in generation quality for two\ndatasets (17%), however the toxicity increase (25%) with increase in model\nsize. Considering type of model, GPT-2 and FlanT5 models are significantly\nbetter in terms of counterspeech quality but also have high toxicity as\ncompared to DialoGPT. ChatGPT are much better at generating counter speech than\nother models across all metrics. In terms of prompting, we find that our\nproposed strategies help in improving counter speech generation across all the\nmodels.\n', ""  This paper explores the potential of large language models (LLMs) to make the\nAeronautical Regulations of Colombia (RAC) more accessible. Given the\ncomplexity and extensive technicality of the RAC, this study introduces a novel\napproach to simplifying these regulations for broader understanding. By\ndeveloping the first-ever RAC database, which contains 24,478 expertly labeled\nquestion-and-answer pairs, and fine-tuning LLMs specifically for RAC\napplications, the paper outlines the methodology for dataset assembly,\nexpert-led annotation, and model training. Utilizing the Gemma1.1 2b model\nalong with advanced techniques like Unsloth for efficient VRAM usage and flash\nattention mechanisms, the research aims to expedite training processes. This\ninitiative establishes a foundation to enhance the comprehensibility and\naccessibility of RAC, potentially benefiting novices and reducing dependence on\nexpert consultations for navigating the aviation industry's regulatory\nlandscape.\n  You can visit the dataset\n(https://huggingface.co/somosnlp/gemma-1.1-2b-it_ColombiaRAC_FullyCurated_format_chatML_V1)\nand the model\n(https://huggingface.co/datasets/somosnlp/ColombiaRAC_FullyCurated) here.\n"", '  We open-source a state-of-the-art 4B-parameter generative model series for\nVietnamese, which includes the base pre-trained monolingual model PhoGPT-4B and\nits chat variant, PhoGPT-4B-Chat. The base model, PhoGPT-4B, with exactly 3.7B\nparameters, is pre-trained from scratch on a Vietnamese corpus of 102B tokens,\nwith an 8192 context length, employing a vocabulary of 20480 token types. The\nchat variant, PhoGPT-4B-Chat, is the modeling output obtained by fine-tuning\nPhoGPT-4B on a dataset of 70K instructional prompts and their responses, along\nwith an additional 290K conversations. In addition, we also demonstrate its\nsuperior performance compared to previous open-source models. Our PhoGPT models\nare available at: https://github.com/VinAIResearch/PhoGPT\n']",Conversational AI and Large Language Models
422,421,16,421_entities_entity_embeddings_alignments,"['entities', 'entity', 'embeddings', 'alignments', 'semantic', 'embedding', 'alignment', 'similarities', 'relations', 'subgraph']","['entity', 'alignment', 'entities', 'relation', 'equivalent', 'graphs', 'embeddings', 'aggregation', 'alignments', 'triple']","['entity', 'embeddings', 'semantic', 'alignment', 'similarities', 'subgraph', 'datasets', 'lingual', 'reranked', 'dbp15k']","['  The flourishing of knowledge graph applications has driven the need for\nentity alignment (EA) across KGs. However, the heterogeneity of practical KGs,\ncharacterized by differing scales, structures, and limited overlapping\nentities, greatly surpasses that of existing EA datasets. This discrepancy\nhighlights an oversimplified heterogeneity in current EA datasets, which\nobstructs a full understanding of the advancements achieved by recent EA\nmethods. In this paper, we study the performance of EA methods in practical\nsettings, specifically focusing on the alignment of highly heterogeneous KGs\n(HHKGs). Firstly, we address the oversimplified heterogeneity settings of\ncurrent datasets and propose two new HHKG datasets that closely mimic practical\nEA scenarios. Then, based on these datasets, we conduct extensive experiments\nto evaluate previous representative EA methods. Our findings reveal that, in\naligning HHKGs, valuable structure information can hardly be exploited through\nmessage-passing and aggregation mechanisms. This phenomenon leads to inferior\nperformance of existing EA methods, especially those based on GNNs. These\nfindings shed light on the potential problems associated with the conventional\napplication of GNN-based methods as a panacea for all EA datasets.\nConsequently, in light of these observations and to elucidate what EA\nmethodology is genuinely beneficial in practical scenarios, we undertake an\nin-depth analysis by implementing a simple but effective approach: Simple-HHEA.\nThis method adaptly integrates entity name, structure, and temporal information\nto navigate the challenges posed by HHKGs. Our experiment results conclude that\nthe key to the future EA model design in practice lies in their adaptability\nand efficiency to varying information quality conditions, as well as their\ncapability to capture patterns across HHKGs.\n', ""  Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent\nentities across diverse knowledge graphs (KGs) using only a limited number of\nseed alignments. Despite substantial advances in aggregation-based weakly\nsupervised EA, the underlying mechanisms in this setting remain unexplored. In\nthis paper, we present a propagation perspective to analyze weakly supervised\nEA and explain the existing aggregation-based EA models. Our theoretical\nanalysis reveals that these models essentially seek propagation operators for\npairwise entity similarities. We further prove that, despite the structural\nheterogeneity of different KGs, the potentially aligned entities within\naggregation-based EA models have isomorphic subgraphs, which is the core\npremise of EA but has not been investigated. Leveraging this insight, we\nintroduce a potential isomorphism propagation operator to enhance the\npropagation of neighborhood information across KGs. We develop a general EA\nframework, PipEA, incorporating this operator to improve the accuracy of every\ntype of aggregation-based model without altering the learning process.\nExtensive experiments substantiate our theoretical findings and demonstrate\nPipEA's significant performance gains over state-of-the-art weakly supervised\nEA methods. Our work not only advances the field but also enhances our\ncomprehension of aggregation-based weakly supervised EA.\n"", ""  Entity Alignment (EA) aims to match equivalent entities in different\nKnowledge Graphs (KGs), which is essential for knowledge fusion and\nintegration. Recently, embedding-based EA has attracted significant attention\nand many approaches have been proposed. Early approaches primarily focus on\nlearning entity embeddings from the structural features of KGs, defined by\nrelation triples. Later methods incorporated entities' names and attributes as\nauxiliary information to enhance embeddings for EA. However, these approaches\noften used different techniques to encode structural and attribute information,\nlimiting their interaction and mutual enhancement. In this work, we propose a\ndense entity retrieval framework for EA, leveraging language models to\nuniformly encode various features of entities and facilitate nearest entity\nsearch across KGs. Alignment candidates are first generated through entity\nretrieval, which are subsequently reranked to determine the final alignments.\nWe conduct comprehensive experiments on both cross-lingual and monolingual EA\ndatasets, demonstrating that our approach achieves state-of-the-art performance\ncompared to existing EA methods.\n""]",Entity Alignment in Knowledge Graphs
423,422,16,422_toxicity_frenchtoxicityprompts_toxic_corpus,"['toxicity', 'frenchtoxicityprompts', 'toxic', 'corpus', 'annotated', 'harmful', 'language', 'languages', 'harm', 'polyglotoxicityprompts']","['toxicity', 'toxic', 'mitigation', 'languages', 'bias', 'multilingual', 'content', 'harmful', 'attribute', 'audio']","['toxicity', 'frenchtoxicityprompts', 'corpus', 'multilingual', 'wordlist', 'voice', 'safety', 'mitigates', 'ukrainian', 'bias']","[""  Large language models (LLMs) are increasingly popular but are also prone to\ngenerating bias, toxic or harmful language, which can have detrimental effects\non individuals and communities. Although most efforts is put to assess and\nmitigate toxicity in generated content, it is primarily concentrated on\nEnglish, while it's essential to consider other languages as well. For\naddressing this issue, we create and release FrenchToxicityPrompts, a dataset\nof 50K naturally occurring French prompts and their continuations, annotated\nwith toxicity scores from a widely used toxicity classifier. We evaluate 14\ndifferent models from four prevalent open-sourced families of LLMs against our\ndataset to assess their potential toxicity across various dimensions. We hope\nthat our contribution will foster future research on toxicity detection and\nmitigation beyond Englis\n"", '  Toxicity classification for voice heavily relies on the semantic content of\nspeech. We propose a novel framework that utilizes cross-modal learning to\nintegrate the semantic embedding of text into a multilabel speech toxicity\nclassifier during training. This enables us to incorporate textual information\nduring training while still requiring only audio during inference. We evaluate\nthis classifier on large-scale datasets with real-world characteristics to\nvalidate the effectiveness of this framework. Through ablation studies, we\ndemonstrate that general-purpose semantic text embeddings are rich and aligned\nwith speech for toxicity classification purposes. Conducting experiments across\nmultiple languages at scale, we show improvements in voice toxicity\nclassification across five languages and different toxicity categories.\n', ""  In the pursuit of developing Large Language Models (LLMs) that adhere to\nsocietal standards, it is imperative to detect the toxicity in the generated\ntext. The majority of existing toxicity metrics rely on encoder models trained\non specific toxicity datasets, which are susceptible to out-of-distribution\n(OOD) problems and depend on the dataset's definition of toxicity. In this\npaper, we introduce a robust metric grounded on LLMs to flexibly measure\ntoxicity according to the given definition. We first analyze the toxicity\nfactors, followed by an examination of the intrinsic toxic attributes of LLMs\nto ascertain their suitability as evaluators. Finally, we evaluate the\nperformance of our metric with detailed analysis. Our empirical results\ndemonstrate outstanding performance in measuring toxicity within verified\nfactors, improving on conventional metrics by 12 points in the F1 score. Our\nfindings also indicate that upstream toxicity significantly influences\ndownstream metrics, suggesting that LLMs are unsuitable for toxicity\nevaluations within unverified factors.\n""]",Toxic Language Detection in AI Models
424,423,15,423_adversarial_attacks_cyberattacks_attack,"['adversarial', 'attacks', 'cyberattacks', 'attack', 'security', 'supervised', 'mitigation', 'threats', 'vulnerability', 'grid']","['grid', 'grids', 'cyber', 'smart', 'power', 'attacks', 'attack', 'eventful', 'proactive', 'energy']","['adversarial', 'cyberattacks', 'attack', 'mitigation', 'grid', 'detect', 'ensemble', 'renewable', 'ieee', 'anomalous']","['  In this study, we conduct a comprehensive review of smart grid security,\nexploring system architectures, attack methodologies, defense strategies, and\nfuture research opportunities. We provide an in-depth analysis of various\nattack vectors, focusing on new attack surfaces introduced by advanced\ncomponents in smart grids. The review particularly includes an extensive\nanalysis of coordinated attacks that incorporate multiple attack strategies and\nexploit vulnerabilities across various smart grid components to increase their\nadverse impact, demonstrating the complexity and potential severity of these\nthreats. Following this, we examine innovative detection and mitigation\nstrategies, including game theory, graph theory, blockchain, and machine\nlearning, discussing their advancements in counteracting evolving threats and\nassociated research challenges. In particular, our review covers a thorough\nexamination of widely used machine learning-based mitigation strategies,\nanalyzing their applications and research challenges spanning across\nsupervised, unsupervised, semi-supervised, ensemble, and reinforcement\nlearning. Further, we outline future research directions and explore new\ntechniques and concerns. We first discuss the research opportunities for\nexisting and emerging strategies, and then explore the potential role of new\ntechniques, such as large language models (LLMs), and the emerging threat of\nadversarial machine learning in the future of smart grid security.\n', '  In smart electrical grids, fault detection tasks may have a high impact on\nsociety due to their economic and critical implications. In the recent years,\nnumerous smart grid applications, such as defect detection and load\nforecasting, have embraced data-driven methodologies. The purpose of this study\nis to investigate the challenges associated with the security of machine\nlearning (ML) applications in the smart grid scenario. Indeed, the robustness\nand security of these data-driven algorithms have not been extensively studied\nin relation to all power grid applications. We demonstrate first that the deep\nneural network method used in the smart grid is susceptible to adversarial\nperturbation. Then, we highlight how studies on fault localization and type\nclassification illustrate the weaknesses of present ML algorithms in smart\ngrids to various adversarial attacks\n', '  Modern power grids are undergoing significant changes driven by information\nand communication technologies (ICTs), and evolving into smart grids with\nhigher efficiency and lower operation cost. Using ICTs, however, comes with an\ninevitable side effect that makes the power system more vulnerable to cyber\nattacks. In this paper, we propose a self-supervised learning-based framework\nto detect and identify various types of cyber attacks. Different from existing\napproaches, the proposed framework does not rely on large amounts of\nwell-curated labeled data but makes use of the massive unlabeled data in the\nwild which are easily accessible. Specifically, the proposed framework adopts\nthe BERT model from the natural language processing domain and learns\ngeneralizable and effective representations from the unlabeled sensing data,\nwhich capture the distinctive patterns of different attacks. Using the learned\nrepresentations, together with a very small amount of labeled data, we can\ntrain a task-specific classifier to detect various types of cyber attacks.\nMeanwhile, real-world training datasets are usually imbalanced, i.e., there are\nonly a limited number of data samples containing attacks. In order to cope with\nsuch data imbalance, we propose a new loss function, separate mean error (SME),\nwhich pays equal attention to the large and small categories to better train\nthe model. Experiment results in a 5-area power grid system with 37 buses\ndemonstrate the superior performance of our framework over existing approaches,\nespecially when a very limited portion of labeled data are available, e.g., as\nlow as 0.002\\%. We believe such a framework can be easily adopted to detect a\nvariety of cyber attacks in other power grid scenarios.\n']",Cybersecurity Threats in Smart Grids
425,424,15,424_recommender_recommenders_reinforcement_recommendation,"['recommender', 'recommenders', 'reinforcement', 'recommendation', 'recommendations', 'rewards', 'reward', 'critic', 'agent', 'optimizing']","['recommender', 'recommendation', 'offline', 'user', 'exploration', 'items', 'reward', 'term', 'reinforcement', 'item']","['recommender', 'reinforcement', 'agent', 'optimizing', 'rl', 'offline', 'rss', 'cache', 'ecoc', 'max']","['  Offline reinforcement learning (RL) is an effective tool for real-world\nrecommender systems with its capacity to model the dynamic interest of users\nand its interactive nature. Most existing offline RL recommender systems focus\non model-based RL through learning a world model from offline data and building\nthe recommendation policy by interacting with this model. Although these\nmethods have made progress in the recommendation performance, the effectiveness\nof model-based offline RL methods is often constrained by the accuracy of the\nestimation of the reward model and the model uncertainties, primarily due to\nthe extreme discrepancy between offline logged data and real-world data in user\ninteractions with online platforms. To fill this gap, a more accurate reward\nmodel and uncertainty estimation are needed for the model-based RL methods. In\nthis paper, a novel model-based Reward Shaping in Offline Reinforcement\nLearning for Recommender Systems, ROLeR, is proposed for reward and uncertainty\nestimation in recommendation systems. Specifically, a non-parametric reward\nshaping method is designed to refine the reward model. In addition, a flexible\nand more representative uncertainty penalty is designed to fit the needs of\nrecommendation systems. Extensive experiments conducted on four benchmark\ndatasets showcase that ROLeR achieves state-of-the-art performance compared\nwith existing baselines. The source code can be downloaded at\nhttps://github.com/ArronDZhang/ROLeR.\n', ""  In recent years, there has been a growing interest in utilizing reinforcement\nlearning (RL) to optimize long-term rewards in recommender systems. Since\nindustrial recommender systems are typically designed as multi-stage systems,\nRL methods with a single agent face challenges when optimizing multiple stages\nsimultaneously. The reason is that different stages have different observation\nspaces, and thus cannot be modeled by a single agent. To address this issue, we\npropose a novel UNidirectional-EXecution-based multi-agent Reinforcement\nLearning (UNEX-RL) framework to reinforce the long-term rewards in multi-stage\nrecommender systems. We show that the unidirectional execution is a key feature\nof multi-stage recommender systems, bringing new challenges to the applications\nof multi-agent reinforcement learning (MARL), namely the observation dependency\nand the cascading effect. To tackle these challenges, we provide a cascading\ninformation chain (CIC) method to separate the independent observations from\naction-dependent observations and use CIC to train UNEX-RL effectively. We also\ndiscuss practical variance reduction techniques for UNEX-RL. Finally, we show\nthe effectiveness of UNEX-RL on both public datasets and an online recommender\nsystem with over 100 million users. Specifically, UNEX-RL reveals a 0.558%\nincrease in users' usage time compared with single-agent RL algorithms in\nonline A/B experiments, highlighting the effectiveness of UNEX-RL in industrial\nrecommender systems.\n"", ""  As the last critical stage of RSs, Multi-Task Fusion (MTF) is responsible for\ncombining multiple scores outputted by Multi-Task Learning (MTL) into a final\nscore to maximize user satisfaction, which determines the ultimate\nrecommendation results. Recently, to optimize long-term user satisfaction\nwithin a recommendation session, Reinforcement Learning (RL) is used for MTF in\nthe industry. However, the off-policy RL algorithms used for MTF so far have\nthe following severe problems: 1) to avoid out-of-distribution (OOD) problem,\ntheir constraints are overly strict, which seriously damage their performance;\n2) they are unaware of the exploration policy used for producing training data\nand never interact with real environment, so only suboptimal policy can be\nlearned; 3) the traditional exploration policies are inefficient and hurt user\nexperience. To solve the above problems, we propose a novel method named\nIntegratedRL-MTF customized for MTF in large-scale RSs. IntegratedRL-MTF\nintegrates off-policy RL model with our online exploration policy to relax\noverstrict and complicated constraints, which significantly improves its\nperformance. We also design an extremely efficient exploration policy, which\neliminates low-value exploration space and focuses on exploring potential\nhigh-value state-action pairs. Moreover, we adopt progressive training mode to\nfurther enhance our model's performance with the help of our exploration\npolicy. We conduct extensive offline and online experiments in the short video\nchannel of Tencent News. The results demonstrate that our model outperforms\nother models remarkably. IntegratedRL-MTF has been fully deployed in our RS and\nother large-scale RSs in Tencent, which have achieved significant improvements.\n""]",Reinforcement Learning in Recommender Systems
426,425,15,425_probabilistic_circuits_inferences_learnspn,"['probabilistic', 'circuits', 'inferences', 'learnspn', 'inference', 'inceptionpcs', 'tractability', 'tractable', 'circuit', 'pgcs']","['circuits', 'probabilistic', 'tractable', 'circuit', 'projective', 'monotone', 'polynomial', 'distributions', 'variables', 'succinct']","['probabilistic', 'circuits', 'learnspn', 'inceptionpcs', 'tractability', 'pgc', 'relational', 'marginalization', 'qpcs', 'binary']","['  Probabilistic Circuits (PCs) have emerged as an efficient framework for\nrepresenting and learning complex probability distributions. Nevertheless, the\nexisting body of research on PCs predominantly concentrates on data-driven\nparameter learning, often neglecting the potential of knowledge-intensive\nlearning, a particular issue in data-scarce/knowledge-rich domains such as\nhealthcare. To bridge this gap, we propose a novel unified framework that can\nsystematically integrate diverse domain knowledge into the parameter learning\nprocess of PCs. Experiments on several benchmarks as well as real world\ndatasets show that our proposed framework can both effectively and efficiently\nleverage domain knowledge to achieve superior performance compared to purely\ndata-driven learning approaches.\n', '  We present a comprehensive survey of the advancements and techniques in the\nfield of tractable probabilistic generative modeling, primarily focusing on\nProbabilistic Circuits (PCs). We provide a unified perspective on the inherent\ntrade-offs between expressivity and tractability, highlighting the design\nprinciples and algorithmic extensions that have enabled building expressive and\nefficient PCs, and provide a taxonomy of the field. We also discuss recent\nefforts to build deep and hybrid PCs by fusing notions from deep neural models,\nand outline the challenges and open questions that can guide future research in\nthis evolving field.\n', '  Zhang et al. (ICML 2021, PLMR 139, pp. 12447-1245) introduced probabilistic\ngenerating circuits (PGCs) as a probabilistic model to unify probabilistic\ncircuits (PCs) and determinantal point processes (DPPs). At a first glance,\nPGCs store a distribution in a very different way, they compute the probability\ngenerating polynomial instead of the probability mass function and it seems\nthat this is the main reason why PGCs are more powerful than PCs or DPPs.\nHowever, PGCs also allow for negative weights, whereas classical PCs assume\nthat all weights are nonnegative. One of the main insights of our paper is that\nthe negative weights are responsible for the power of PGCs and not the\ndifferent representation. PGCs are PCs in disguise, in particular, we show how\nto transform any PGC into a PC with negative weights with only polynomial\nblowup.\n  PGCs were defined by Zhang et al. only for binary random variables. As our\nsecond main result, we show that there is a good reason for this: we prove that\nPGCs for categorial variables with larger image size do not support tractable\nmarginalization unless NP = P. On the other hand, we show that we can model\ncategorial variables with larger image size as PC with negative weights\ncomputing set-multilinear polynomials. These allow for tractable\nmarginalization. In this sense, PCs with negative weights strictly subsume\nPGCs.\n']",Probabilistic Circuits and Inference Models
427,426,15,426_paraphrases_paraphrasing_paraphrastic_paraphrase,"['paraphrases', 'paraphrasing', 'paraphrastic', 'paraphrase', 'paraphrased', 'sentences', 'linguistic', 'nlg', 'corpora', 'lexical']","['paraphrase', 'paraphrases', 'paraphrasing', 'paraphrastic', 'diversity', 'syntactic', 'lexical', 'linguistic', 'generation', 'text']","['paraphrasing', 'nlg', 'corpora', 'dialog', 'similarity', 'paranlu', 'dversarial', 'araphraser', 'spans', 'types']","['  Since paraphrasing is an ill-defined task, the term ""paraphrasing"" covers\ntext transformation tasks with different characteristics. Consequently,\nexisting paraphrasing studies have applied quite different (explicit and\nimplicit) criteria as to when a pair of texts is to be considered a paraphrase,\nall of which amount to postulating a certain level of semantic or lexical\nsimilarity. In this paper, we conduct a literature review and propose a\ntaxonomy to organize the 25~identified paraphrasing (sub-)tasks. Using\nclassifiers trained to identify the tasks that a given paraphrasing instance\nfits, we find that the distributions of task-specific instances in the known\nparaphrase corpora vary substantially. This means that the use of these\ncorpora, without the respective paraphrase conditions being clearly defined\n(which is the normal case), must lead to incomparable and misleading results.\n', ""  Paraphrases represent a human's intuitive ability to understand expressions\npresented in various different ways. Current paraphrase evaluations of language\nmodels primarily use binary approaches, offering limited interpretability of\nspecific text changes. Atomic paraphrase types (APT) decompose paraphrases into\ndifferent linguistic changes and offer a granular view of the flexibility in\nlinguistic expression (e.g., a shift in syntax or vocabulary used). In this\nstudy, we assess the human preferences towards ChatGPT in generating English\nparaphrases with ten APTs and five prompting techniques. We introduce APTY\n(Atomic Paraphrase TYpes), a dataset of 500 sentence-level and word-level\nannotations by 15 annotators. The dataset also provides a human preference\nranking of paraphrases with different types that can be used to fine-tune\nmodels with RLHF and DPO methods. Our results reveal that ChatGPT can generate\nsimple APTs, such as additions and deletions, but struggle with complex\nstructures (e.g., subordination changes). This study contributes to\nunderstanding which aspects of paraphrasing language models have already\nsucceeded at understanding and what remains elusive. In addition, our curated\ndatasets can be used to develop language models with specific linguistic\ncapabilities.\n"", '  Current approaches in paraphrase generation and detection heavily rely on a\nsingle general similarity score, ignoring the intricate linguistic properties\nof language. This paper introduces two new tasks to address this shortcoming by\nconsidering paraphrase types - specific linguistic perturbations at particular\ntext positions. We name these tasks Paraphrase Type Generation and Paraphrase\nType Detection. Our results suggest that while current techniques perform well\nin a binary classification scenario, i.e., paraphrased or not, the inclusion of\nfine-grained paraphrase types poses a significant challenge. While most\napproaches are good at generating and detecting general semantic similar\ncontent, they fail to understand the intrinsic linguistic variables they\nmanipulate. Models trained in generating and identifying paraphrase types also\nshow improvements in tasks without them. In addition, scaling these models\nfurther improves their ability to understand paraphrase types. We believe\nparaphrase types can unlock a new paradigm for developing paraphrase models and\nsolving tasks in the future.\n']",Paraphrasing in Natural Language Generation
428,427,15,427_cultural_culturalvqa_culturally_cultures,"['cultural', 'culturalvqa', 'culturally', 'cultures', 'culture', 'cultureadapt', 'captioning', 'visual', 'captions', 'multilingual']","['cultural', 'culture', 'cultures', 'countries', 'visual', 'vision', 'images', 'understanding', 'captions', 'questions']","['cultural', 'cultureadapt', 'captions', 'multilingual', 'visual_diversity_budget', 'multimodal', 'diverse', 'inclusiveness', 'korean', 'descriptive']","[""  Foundation models and vision-language pre-training have notably advanced\nVision Language Models (VLMs), enabling multimodal processing of visual and\nlinguistic data. However, their performance has been typically assessed on\ngeneral scene understanding - recognizing objects, attributes, and actions -\nrather than cultural comprehension. This study introduces CulturalVQA, a visual\nquestion-answering benchmark aimed at assessing VLM's geo-diverse cultural\nunderstanding. We curate a collection of 2,378 image-question pairs with 1-5\nanswers per question representing cultures from 11 countries across 5\ncontinents. The questions probe understanding of various facets of culture such\nas clothing, food, drinks, rituals, and traditions. Benchmarking VLMs on\nCulturalVQA, including GPT-4V and Gemini, reveals disparity in their level of\ncultural understanding across regions, with strong cultural understanding\ncapabilities for North America while significantly lower performance for\nAfrica. We observe disparity in their performance across cultural facets too,\nwith clothing, rituals, and traditions seeing higher performances than food and\ndrink. These disparities help us identify areas where VLMs lack cultural\nunderstanding and demonstrate the potential of CulturalVQA as a comprehensive\nevaluation set for gauging VLM progress in understanding diverse cultures.\n"", '  Image Captioning generates descriptive sentences from images using\nVision-Language Pre-trained models (VLPs) such as BLIP, which has improved\ngreatly. However, current methods lack the generation of detailed descriptive\ncaptions for the cultural elements depicted in the images, such as the\ntraditional clothing worn by people from Asian cultural groups. In this paper,\nwe propose a new framework, \\textbf{Culturally-aware Image Captioning (CIC)},\nthat generates captions and describes cultural elements extracted from cultural\nvisual elements in images representing cultures. Inspired by methods combining\nvisual modality and Large Language Models (LLMs) through appropriate prompts,\nour framework (1) generates questions based on cultural categories from images,\n(2) extracts cultural visual elements from Visual Question Answering (VQA)\nusing generated questions, and (3) generates culturally-aware captions using\nLLMs with the prompts. Our human evaluation conducted on 45 participants from 4\ndifferent cultural groups with a high understanding of the corresponding\nculture shows that our proposed framework generates more culturally descriptive\ncaptions when compared to the image captioning baseline based on VLPs. Our code\nand dataset will be made publicly available upon acceptance.\n', ""  Despite recent advancements in vision-language models, their performance\nremains suboptimal on images from non-western cultures due to\nunderrepresentation in training datasets. Various benchmarks have been proposed\nto test models' cultural inclusivity, but they have limited coverage of\ncultures and do not adequately assess cultural diversity across universal as\nwell as culture-specific local concepts. To address these limitations, we\nintroduce the GlobalRG benchmark, comprising two challenging tasks: retrieval\nacross universals and cultural visual grounding. The former task entails\nretrieving culturally diverse images for universal concepts from 50 countries,\nwhile the latter aims at grounding culture-specific concepts within images from\n15 countries. Our evaluation across a wide range of models reveals that the\nperformance varies significantly across cultures -- underscoring the necessity\nfor enhancing multicultural understanding in vision-language models.\n""]",Cultural Understanding in Vision-Language Models
429,428,15,428_distributional_reinforcement_learns_quantile,"['distributional', 'reinforcement', 'learns', 'quantile', 'distributions', 'learning', 'distribution', 'reward', 'dqn', 'minimax']","['distributional', 'quantile', 'return', 'returns', 'reinforcement', 'distribution', 'policy', 'difference', 'distributions', 'value']","['distributional', 'reinforcement', 'learns', 'quantile', 'dqn', 'minimax', 'expectile', 'qtd', 'atari', 'sinkhorndrl']","['  We propose a new algorithm for model-based distributional reinforcement\nlearning (RL), and prove that it is minimax-optimal for approximating return\ndistributions with a generative model (up to logarithmic factors), resolving an\nopen question of Zhang et al. (2023). Our analysis provides new theoretical\nresults on categorical approaches to distributional RL, and also introduces a\nnew distributional Bellman equation, the stochastic categorical CDF Bellman\nequation, which we expect to be of independent interest. We also provide an\nexperimental study comparing several model-based distributional RL algorithms,\nwith several takeaways for practitioners.\n', '  Distributional Reinforcement Learning (RL) estimates return distribution\nmainly by learning quantile values via minimizing the quantile Huber loss\nfunction, entailing a threshold parameter often selected heuristically or via\nhyperparameter search, which may not generalize well and can be suboptimal.\nThis paper introduces a generalized quantile Huber loss function derived from\nWasserstein distance (WD) calculation between Gaussian distributions, capturing\nnoise in predicted (current) and target (Bellman-updated) quantile values.\nCompared to the classical quantile Huber loss, this innovative loss function\nenhances robustness against outliers. Notably, the classical Huber loss\nfunction can be seen as an approximation of our proposed loss, enabling\nparameter adjustment by approximating the amount of noise in the data during\nthe learning process. Empirical tests on Atari games, a common application in\ndistributional RL, and a recent hedging strategy using distributional RL,\nvalidate the effectiveness of our proposed loss function and its potential for\nparameter adjustments in distributional RL. The implementation of the proposed\nloss function is available here.\n', '  Distributional reinforcement learning (RL) has proven useful in multiple\nbenchmarks as it enables approximating the full distribution of returns and\nmakes a better use of environment samples. The commonly used quantile\nregression approach to distributional RL -- based on asymmetric $L_1$ losses --\nprovides a flexible and effective way of learning arbitrary return\ndistributions. In practice, it is often improved by using a more efficient,\nhybrid asymmetric $L_1$-$L_2$ Huber loss for quantile regression. However, by\ndoing so, distributional estimation guarantees vanish, and we empirically\nobserve that the estimated distribution rapidly collapses to its mean. Indeed,\nasymmetric $L_2$ losses, corresponding to expectile regression, cannot be\nreadily used for distributional temporal difference learning. Motivated by the\nefficiency of $L_2$-based learning, we propose to jointly learn expectiles and\nquantiles of the return distribution in a way that allows efficient learning\nwhile keeping an estimate of the full distribution of returns. We prove that\nour approach approximately learns the correct return distribution, and we\nbenchmark a practical implementation on a toy example and at scale. On the\nAtari benchmark, our approach matches the performance of the Huber-based IQN-1\nbaseline after $200$M training frames but avoids distributional collapse and\nkeeps estimates of the full distribution of returns.\n']",Distributional Reinforcement Learning
430,429,15,429_imputations_imputation__imputation_missingness,"['imputations', 'imputation', '_imputation', 'missingness', 'imputed', 'imputing', 'forecasting', 'forecasts', 'impute', 'incomplete']","['imputation', 'missing', 'series', 'hinge', 'multivariate', 'imputed', 'values', 'time', 'forecasting', 'impute']","['imputations', 'forecasts', 'datasets', 'overlooked', 'timestamps', 'rbfnn', 'nonparametric', 'gps', 'tsi', 'channel']","[""  Time series imputation plays a crucial role in various real-world systems and\nhas been extensively explored. Models for time series imputation often require\nspecialization, necessitating distinct designs for different domains and\nmissing patterns. In this study, we introduce NuwaTS, a framework to repurpose\nPre-trained Language Model (PLM) for general time series imputation. Once\ntrained, this model can be applied to imputation tasks on incomplete time\nseries from any domain with any missing patterns. We begin by devising specific\nembeddings for each sub-series patch of the incomplete time series. These\nembeddings encapsulate information about the patch itself, the missing data\npatterns within the patch, and the patch's statistical characteristics. To\nenhance the model's adaptability to different missing patterns, we propose a\ncontrastive learning approach to make representations of the same patch more\nsimilar across different missing patterns. By combining this contrastive loss\nwith the missing data imputation task, we train PLMs to obtain a one-for-all\nimputation model. Furthermore, we utilize a plug-and-play layer-wise\nfine-tuning approach to train domain-specific models. Experimental results\ndemonstrate that leveraging a dataset of over seventeen million time series\nfrom diverse domains, we obtain a one-for-all imputation model which\noutperforms existing domain-specific models across various datasets and missing\npatterns. Additionally, we find that NuwaTS can be generalized to other time\nseries tasks such as forecasting. Our codes are available at\nhttps://github.com/Chengyui/NuwaTS.\n"", '  Time series imputation is one of the most fundamental tasks for time series.\nReal-world time series datasets are frequently incomplete (or irregular with\nmissing observations), in which case imputation is strongly required. Many\ndifferent time series imputation methods have been proposed. Recent\nself-attention-based methods show the state-of-the-art imputation performance.\nHowever, it has been overlooked for a long time to design an imputation method\nbased on continuous-time recurrent neural networks (RNNs), i.e., neural\ncontrolled differential equations (NCDEs). To this end, we redesign time series\n(variational) autoencoders based on NCDEs. Our method, called continuous-time\nautoencoder (CTA), encodes an input time series sample into a continuous hidden\npath (rather than a hidden vector) and decodes it to reconstruct and impute the\ninput. In our experiments with 4 datasets and 19 baselines, our method shows\nthe best imputation performance in almost all cases.\n', '  Time series classification with missing data is a prevalent issue in time\nseries analysis, as temporal data often contain missing values in practical\napplications. The traditional two-stage approach, which handles imputation and\nclassification separately, can result in sub-optimal performance as label\ninformation is not utilized in the imputation process. On the other hand, a\none-stage approach can learn features under missing information, but feature\nrepresentation is limited as imputed errors are propagated in the\nclassification process. To overcome these challenges, this study proposes an\nend-to-end neural network that unifies data imputation and representation\nlearning within a single framework, allowing the imputation process to take\nadvantage of label information. Differing from previous methods, our approach\nplaces less emphasis on the accuracy of imputation data and instead prioritizes\nclassification performance. A specifically designed multi-scale feature\nlearning module is implemented to extract useful information from the\nnoise-imputation data. The proposed model is evaluated on 68 univariate time\nseries datasets from the UCR archive, as well as a multivariate time series\ndataset with various missing data ratios and 4 real-world datasets with missing\ninformation. The results indicate that the proposed model outperforms\nstate-of-the-art approaches for incomplete time series classification,\nparticularly in scenarios with high levels of missing data.\n']",Time Series Imputation and Missing Data Analysis
431,430,15,430_hopfield_memory_retrieval_sparse,"['hopfield', 'memory', 'retrieval', 'sparse', 'hop', '_softmax', 'efficient', 'storage', 'hop82', 'boltzmann']","['modern', 'prototype', 'memory', 'associative', 'replica', 'capacity', 'diagram', 'patterns', 'outlier', 'retrieval']","['hopfield', 'memory', '_softmax', 'boltzmann', 'synaptic', 'attractors', 'boltmzann', 'archetypes', 'louk97', 'ferromagnetic']","['  We investigate the computational limits of the memory retrieval dynamics of\nmodern Hopfield models from the fine-grained complexity analysis. Our key\ncontribution is the characterization of a phase transition behavior in the\nefficiency of all possible modern Hopfield models based on the norm of\npatterns. Specifically, we establish an upper bound criterion for the norm of\ninput query patterns and memory patterns. Only below this criterion,\nsub-quadratic (efficient) variants of the modern Hopfield model exist, assuming\nthe Strong Exponential Time Hypothesis (SETH). To showcase our theory, we\nprovide a formal example of efficient constructions of modern Hopfield models\nusing low-rank approximation when the efficient criterion holds. This includes\na derivation of a lower bound on the computational time, scaling linearly with\n$\\max\\{$# of stored memory patterns, length of input query sequence$\\}$. In\naddition, we prove its memory retrieval error bound and exponential memory\ncapacity.\n', '  We present a nonparametric construction for deep learning compatible modern\nHopfield models and utilize this framework to debut an efficient variant. Our\nkey contribution stems from interpreting the memory storage and retrieval\nprocesses in modern Hopfield models as a nonparametric regression problem\nsubject to a set of query-memory pairs. Crucially, our framework not only\nrecovers the known results from the original dense modern Hopfield model but\nalso fills the void in the literature regarding efficient modern Hopfield\nmodels, by introducing \\textit{sparse-structured} modern Hopfield models with\nsub-quadratic complexity. We establish that this sparse model inherits the\nappealing theoretical properties of its dense analogue -- connection with\ntransformer attention, fixed point convergence and exponential memory capacity\n-- even without knowing details of the Hopfield energy function. Additionally,\nwe showcase the versatility of our framework by constructing a family of modern\nHopfield models as extensions, including linear, random masked, top-$K$ and\npositive random feature modern Hopfield models. Empirically, we validate the\nefficacy of our framework in both synthetic and realistic settings.\n', '  We propose a two-stage memory retrieval dynamics for modern Hopfield models,\ntermed $\\mathtt{U\\text{-}Hop}$, with enhanced memory capacity. Our key\ncontribution is a learnable feature map $\\Phi$ which transforms the Hopfield\nenergy function into kernel space. This transformation ensures convergence\nbetween the local minima of energy and the fixed points of retrieval dynamics\nwithin the kernel space. Consequently, the kernel norm induced by $\\Phi$ serves\nas a novel similarity measure. It utilizes the stored memory patterns as\nlearning data to enhance memory capacity across all modern Hopfield models.\nSpecifically, we accomplish this by constructing a separation loss\n$\\mathcal{L}_\\Phi$ that separates the local minima of kernelized energy by\nseparating stored memory patterns in kernel space. Methodologically,\n$\\mathtt{U\\text{-}Hop}$ memory retrieval process consists of: (Stage I)\nminimizing separation loss for a more uniform memory (local minimum)\ndistribution, followed by (Stage II) standard Hopfield energy minimization for\nmemory retrieval. This results in a significant reduction of possible\nmetastable states in the Hopfield energy function, thus enhancing memory\ncapacity by preventing memory confusion. Empirically, with real-world datasets,\nwe demonstrate that $\\mathtt{U\\text{-}Hop}$ outperforms all existing modern\nHopfield models and state-of-the-art similarity measures, achieving substantial\nimprovements in both associative memory retrieval and deep learning tasks. Code\nis available at https://github.com/MAGICS-LAB/UHop ; future updates are on\narXiv:2404.03827\n']",Modern Hopfield Models for Efficient Memory Retrieval
432,431,15,431_analogies_analogical_analogy_analogykb,"['analogies', 'analogical', 'analogy', 'analogykb', 'semantic', 'cognition', 'analogous', 'cognitive', 'reasoning', 'thinking']","['analogies', 'analogical', 'analogy', 'reasoning', 'humans', 'analogous', 'distractors', 'abilities', 'experiences', 'concepts']","['analogies', 'analogykb', 'semantic', 'analogous', 'thinking', 'concepts', 'humanlike', 'distractors', 'narratives', 'shot']","['  Analogical reasoning is a fundamental cognitive ability of humans. However,\ncurrent language models (LMs) still struggle to achieve human-like performance\nin analogical reasoning tasks due to a lack of resources for model training. In\nthis work, we address this gap by proposing ANALOGYKB, a million-scale analogy\nknowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB\nidentifies two types of analogies from the KGs: 1) analogies of the same\nrelations, which can be directly extracted from the KGs, and 2) analogies of\nanalogous relations, which are identified with a selection and filtering\npipeline enabled by large language models (LLMs), followed by minor human\nefforts for data quality control. Evaluations on a series of datasets of two\nanalogical reasoning tasks (analogy recognition and generation) demonstrate\nthat ANALOGYKB successfully enables both smaller LMs and LLMs to gain better\nanalogical reasoning capabilities.\n', '  While analogies are a common way to evaluate word embeddings in NLP, it is\nalso of interest to investigate whether or not analogical reasoning is a task\nin itself that can be learned. In this paper, we test several ways to learn\nbasic analogical reasoning, specifically focusing on analogies that are more\ntypical of what is used to evaluate analogical reasoning in humans than those\nin commonly used NLP benchmarks. Our experiments find that models are able to\nlearn analogical reasoning, even with a small amount of data. We additionally\ncompare our models to a dataset with a human baseline, and find that after\ntraining, models approach human performance.\n', '  As a core cognitive skill that enables the transferability of information\nacross domains, analogical reasoning has been extensively studied for both\nhumans and computational models. However, while cognitive theories of analogy\noften focus on narratives and study the distinction between surface,\nrelational, and system similarities, existing work in natural language\nprocessing has a narrower focus as far as relational analogies between word\npairs. This gap brings a natural question: can state-of-the-art large language\nmodels (LLMs) detect system analogies between narratives? To gain insight into\nthis question and extend word-based relational analogies to relational system\nanalogies, we devise a comprehensive computational framework that\noperationalizes dominant theories of analogy, using narrative elements to\ncreate surface and system mappings. Leveraging the interplay between these\nmappings, we create a binary task and benchmark for Analogical Reasoning on\nNarratives (ARN), covering four categories of far (cross-domain)/near\n(within-domain) analogies and disanalogies. We show that while all LLMs can\nlargely recognize near analogies, even the largest ones struggle with far\nanalogies in a zero-shot setting, with GPT4.0 scoring below random. Guiding the\nmodels through solved examples and chain-of-thought reasoning enhances their\nanalogical reasoning ability. Yet, since even in the few-shot setting, the best\nmodel only performs halfway between random and humans, ARN opens exciting\ndirections for computational analogical reasoners.\n']",Analogical Reasoning in Artificial Intelligence
433,432,15,432_testing_tests_nonparametric_statistical,"['testing', 'tests', 'nonparametric', 'statistical', 'test', 'statistics', 'distributions', 'kernel', 'statistic', 'distance']","['kernel', 'test', 'tests', 'divergences', 'testing', 'chi', 'distance', 'independence', 'null', 'statistics']","['tests', 'nonparametric', 'distributions', 'kernel', 'likelihood', 'stein', 'ksd', 'unnormalised', 'hellinger', 'discrepancy']","['  This paper introduces and investigates the utilization of maximum and average\ndistance correlations for multivariate independence testing. We characterize\ntheir consistency properties in high-dimensional settings with respect to the\nnumber of marginally dependent dimensions, assess the advantages of each test\nstatistic, examine their respective null distributions, and present a fast\nchi-square-based testing procedure. The resulting tests are non-parametric and\napplicable to both Euclidean distance and the Gaussian kernel as the underlying\nmetric. To better understand the practical use cases of the proposed tests, we\nevaluate the empirical performance of the maximum distance correlation, average\ndistance correlation, and the original distance correlation across various\nmultivariate dependence scenarios, as well as conduct a real data experiment to\ntest the presence of various cancer types and peptide levels in human plasma.\n', '  Distance correlation has gained much recent attention in the data science\ncommunity: the sample statistic is straightforward to compute and\nasymptotically equals zero if and only if independence, making it an ideal\nchoice to discover any type of dependency structure given sufficient sample\nsize. One major bottleneck is the testing process: because the null\ndistribution of distance correlation depends on the underlying random variables\nand metric choice, it typically requires a permutation test to estimate the\nnull and compute the p-value, which is very costly for large amount of data. To\novercome the difficulty, in this paper we propose a chi-square test for\ndistance correlation. Method-wise, the chi-square test is non-parametric,\nextremely fast, and applicable to bias-corrected distance correlation using any\nstrong negative type metric or characteristic kernel. The test exhibits a\nsimilar testing power as the standard permutation test, and can be utilized for\nK-sample and partial testing. Theory-wise, we show that the underlying\nchi-square distribution well approximates and dominates the limiting null\ndistribution in upper tail, prove the chi-square test can be valid and\nuniversally consistent for testing independence, and establish a testing power\ninequality with respect to the permutation test.\n', '  Over the last decade, an approach that has gained a lot of popularity to\ntackle nonparametric testing problems on general (i.e., non-Euclidean) domains\nis based on the notion of reproducing kernel Hilbert space (RKHS) embedding of\nprobability distributions. The main goal of our work is to understand the\noptimality of two-sample tests constructed based on this approach. First, we\nshow the popular MMD (maximum mean discrepancy) two-sample test to be not\noptimal in terms of the separation boundary measured in Hellinger distance.\nSecond, we propose a modification to the MMD test based on spectral\nregularization by taking into account the covariance information (which is not\ncaptured by the MMD test) and prove the proposed test to be minimax optimal\nwith a smaller separation boundary than that achieved by the MMD test. Third,\nwe propose an adaptive version of the above test which involves a data-driven\nstrategy to choose the regularization parameter and show the adaptive test to\nbe almost minimax optimal up to a logarithmic factor. Moreover, our results\nhold for the permutation variant of the test where the test threshold is chosen\nelegantly through the permutation of the samples. Through numerical experiments\non synthetic and real data, we demonstrate the superior performance of the\nproposed test in comparison to the MMD test and other popular tests in the\nliterature.\n']",Nonparametric Statistical Testing
434,433,15,433_influence_influential_influence_analysis_papers_models,"['influence', 'influential', 'influence_analysis_papers', 'models', 'annotated', 'attribution', 'predictions', 'evaluations', 'gradient', 'outlier']","['influence', 'attribution', 'functions', 'influential', 'samples', 'instance', 'silver', 'synset', 'training', 'gradient']","['influential', 'influence_analysis_papers', 'annotated', 'attribution', 'outlier', 'synset', 'hessian', 'datamodeling', 'assessing', 'silver']","[""  Good models require good training data. For overparameterized deep models,\nthe causal relationship between training data and model predictions is\nincreasingly opaque and poorly understood. Influence analysis partially\ndemystifies training's underlying interactions by quantifying the amount each\ntraining instance alters the final model. Measuring the training data's\ninfluence exactly can be provably hard in the worst case; this has led to the\ndevelopment and use of influence estimators, which only approximate the true\ninfluence. This paper provides the first comprehensive survey of training data\ninfluence analysis and estimation. We begin by formalizing the various, and in\nplaces orthogonal, definitions of training data influence. We then organize\nstate-of-the-art influence analysis methods into a taxonomy; we describe each\nof these methods in detail and compare their underlying assumptions, asymptotic\ncomplexities, and overall strengths and weaknesses. Finally, we propose future\nresearch directions to make influence analysis more useful in practice as well\nas more theoretically and empirically sound. A curated, up-to-date list of\nresources related to influence analysis is available at\nhttps://github.com/ZaydH/influence_analysis_papers.\n"", '  Large-scale black-box models have become ubiquitous across numerous\napplications. Understanding the influence of individual training data sources\non predictions made by these models is crucial for improving their\ntrustworthiness. Current influence estimation techniques involve computing\ngradients for every training point or repeated training on different subsets.\nThese approaches face obvious computational challenges when scaled up to large\ndatasets and models.\n  In this paper, we introduce and explore the Mirrored Influence Hypothesis,\nhighlighting a reciprocal nature of influence between training and test data.\nSpecifically, it suggests that evaluating the influence of training data on\ntest predictions can be reformulated as an equivalent, yet inverse problem:\nassessing how the predictions for training samples would be altered if the\nmodel were trained on specific test samples. Through both empirical and\ntheoretical validations, we demonstrate the wide applicability of our\nhypothesis. Inspired by this, we introduce a new method for estimating the\ninfluence of training data, which requires calculating gradients for specific\ntest samples, paired with a forward pass for each training point. This approach\ncan capitalize on the common asymmetry in scenarios where the number of test\nsamples under concurrent examination is much smaller than the scale of the\ntraining dataset, thus gaining a significant improvement in efficiency compared\nto existing approaches.\n  We demonstrate the applicability of our method across a range of scenarios,\nincluding data attribution in diffusion models, data leakage detection,\nanalysis of memorization, mislabeled data detection, and tracing behavior in\nlanguage models. Our code will be made available at\nhttps://github.com/ruoxi-jia-group/Forward-INF.\n', '  Influence functions serve as crucial tools for assessing sample influence in\nmodel interpretation, subset training set selection, noisy label detection, and\nmore. By employing the first-order Taylor extension, influence functions can\nestimate sample influence without the need for expensive model retraining.\nHowever, applying influence functions directly to deep models presents\nchallenges, primarily due to the non-convex nature of the loss function and the\nlarge size of model parameters. This difficulty not only makes computing the\ninverse of the Hessian matrix costly but also renders it non-existent in some\ncases. Various approaches, including matrix decomposition, have been explored\nto expedite and approximate the inversion of the Hessian matrix, with the aim\nof making influence functions applicable to deep models. In this paper, we\nrevisit a specific, albeit naive, yet effective approximation method known as\nTracIn. This method substitutes the inverse of the Hessian matrix with an\nidentity matrix. We provide deeper insights into why this simple approximation\nmethod performs well. Furthermore, we extend its applications beyond measuring\nmodel utility to include considerations of fairness and robustness. Finally, we\nenhance TracIn through an ensemble strategy. To validate its effectiveness, we\nconduct experiments on synthetic data and extensive evaluations on noisy label\ndetection, sample selection for large language model fine-tuning, and defense\nagainst adversarial attacks.\n']",Influence Analysis in Machine Learning Models
435,434,15,434_bitrate_streaming_bitstream_videos,"['bitrate', 'streaming', 'bitstream', 'videos', 'encoder', 'compression', 'encoding', 'vvc', 'decoding', 'decoder']","['bitrate', 'video', 'codecs', 'transcoding', 'codec', 'compression', 'ladder', 'streaming', 'distortion', 'quality']","['bitrate', 'bitstream', 'videos', 'encoder', 'compression', 'codecs', 'dvc', 'pixel', 'distortion', 'boosting']","['  Volumetric video based on Neural Radiance Field (NeRF) holds vast potential\nfor various 3D applications, but its substantial data volume poses significant\nchallenges for compression and transmission. Current NeRF compression lacks the\nflexibility to adjust video quality and bitrate within a single model for\nvarious network and device capacities. To address these issues, we propose HPC,\na novel hierarchical progressive volumetric video coding framework achieving\nvariable bitrate using a single model. Specifically, HPC introduces a\nhierarchical representation with a multi-resolution residual radiance field to\nreduce temporal redundancy in long-duration sequences while simultaneously\ngenerating various levels of detail. Then, we propose an end-to-end progressive\nlearning approach with a multi-rate-distortion loss function to jointly\noptimize both hierarchical representation and compression. Our HPC trained only\nonce can realize multiple compression levels, while the current methods need to\ntrain multiple fixed-bitrate models for different rate-distortion (RD)\ntradeoffs. Extensive experiments demonstrate that HPC achieves flexible quality\nlevels with variable bitrate by a single model and exhibits competitive RD\nperformance, even outperforming fixed-bitrate models across various datasets.\n', '  Providing high-quality video with efficient bitrate is a main challenge in\nvideo industry. The traditional one-size-fits-all scheme for bitrate ladders is\ninefficient and reaching the best content-aware decision computationally\nimpractical due to extensive encodings required. To mitigate this, we propose a\nbitrate and complexity efficient bitrate ladder prediction method using\ntransfer learning and spatio-temporal features. We propose: (1) using feature\nmaps from well-known pre-trained DNNs to predict rate-quality behavior with\nlimited training data; and (2) improving highest quality rung efficiency by\npredicting minimum bitrate for top quality and using it for the top rung. The\nmethod tested on 102 video scenes demonstrates 94.1% reduction in complexity\nversus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning\nwas thoroughly studied through four networks and ablation studies.\n', '  Adaptive video streaming requires efficient bitrate ladder construction to\nmeet heterogeneous network conditions and end-user demands. Per-title optimized\nencoding typically traverses numerous encoding parameters to search the\nPareto-optimal operating points for each video. Recently, researchers have\nattempted to predict the content-optimized bitrate ladder for pre-encoding\noverhead reduction. However, existing methods commonly estimate the encoding\nparameters on the Pareto front and still require subsequent pre-encodings. In\nthis paper, we propose to directly predict the optimal transcoding resolution\nat each preset bitrate for efficient bitrate ladder construction. We adopt a\nTemporal Attentive Gated Recurrent Network to capture spatial-temporal features\nand predict transcoding resolutions as a multi-task classification problem. We\ndemonstrate that content-optimized bitrate ladders can thus be efficiently\ndetermined without any pre-encoding. Our method well approximates the\nground-truth bitrate-resolution pairs with a slight Bj{\\o}ntegaard Delta rate\nloss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.\n']",Video Compression and Bitrate Optimization
436,435,15,435_voice_vocals_voices_vocal,"['voice', 'vocals', 'voices', 'vocal', 'singing', 'karaoker', 'singers', 'singer', 'singfake', 'songs']","['singing', 'voice', 'singer', 'voices', 'pitch', 'singers', 'vocal', 'song', 'conversion', 'synthesis']","['vocals', 'voices', 'karaoker', 'singfake', 'songbsab', 'utterances', 'psychoacoustic', 'clips', 'conversion', 'timbre']","[""  Singing voice conversion (SVC) automates song covers by converting one\nsinger's singing voice into another target singer's singing voice with the\noriginal lyrics and melody. However, it raises serious concerns about copyright\nand civil right infringements to multiple entities. This work proposes\nSongBsAb, the first proactive approach to mitigate unauthorized SVC-based\nillegal song covers. SongBsAb introduces human-imperceptible perturbations to\nsinging voices before releasing them, so that when they are used, the\ngeneration process of SVC will be interfered, resulting in unexpected singing\nvoices. SongBsAb features a dual prevention effect by causing both (singer)\nidentity disruption and lyric disruption, namely, the SVC-covered singing voice\nneither imitates the target singer nor preserves the original lyrics. To\nimprove the imperceptibility of perturbations, we refine a psychoacoustic\nmodel-based loss with the backing track as an additional masker, a unique\naccompanying element for singing voices compared to ordinary speech voices. To\nenhance the transferability, we propose to utilize a frame-level interaction\nreduction-based loss. We demonstrate the prevention effectiveness, utility, and\nrobustness of SongBsAb on three SVC models and two datasets using both\nobjective and human study-based subjective metrics. Our work fosters an\nemerging research direction for mitigating illegal automated song covers.\n"", '  Significant strides have been made in creating voice identity representations\nusing speech data. However, the same level of progress has not been achieved\nfor singing voices. To bridge this gap, we suggest a framework for training\nsinger identity encoders to extract representations suitable for various\nsinging-related tasks, such as singing voice similarity and synthesis. We\nexplore different self-supervised learning techniques on a large collection of\nisolated vocal tracks and apply data augmentations during training to ensure\nthat the representations are invariant to pitch and content variations. We\nevaluate the quality of the resulting representations on singer similarity and\nidentification tasks across multiple datasets, with a particular emphasis on\nout-of-domain generalization. Our proposed framework produces high-quality\nembeddings that outperform both speaker verification and wav2vec 2.0\npre-trained baselines on singing voice while operating at 44.1 kHz. We release\nour code and trained models to facilitate further research on singing voice and\nrelated areas.\n', '  Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses\non generating high-quality singing voices with unseen styles (such as timbre,\nemotion, pronunciation, and articulation skills) derived from reference singing\nvoice samples. However, the endeavor to model the intricate nuances of singing\nvoice styles is an arduous task, as singing voices possess a remarkable degree\nof expressiveness. Moreover, existing SVS methods encounter a decline in the\nquality of synthesized singing voices in OOD scenarios, as they rest upon the\nassumption that the target vocal attributes are discernible during the training\nphase. To overcome these challenges, we propose StyleSinger, the first singing\nvoice synthesis model for zero-shot style transfer of out-of-domain reference\nsinging voice samples. StyleSinger incorporates two critical approaches for\nenhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a\nresidual quantization module to capture diverse style characteristics in\nsinging voices, and 2) the Uncertainty Modeling Layer Normalization (UMLN) to\nperturb the style attributes within the content representation during the\ntraining phase and thus improve the model generalization. Our extensive\nevaluations in zero-shot style transfer undeniably establish that StyleSinger\noutperforms baseline models in both audio quality and similarity to the\nreference singing voice samples. Access to singing voice samples can be found\nat https://stylesinger.github.io/.\n']",Singing Voice Technology
437,436,15,436_forgetting_continual_graphs_learning,"['forgetting', 'continual', 'graphs', 'learning', 'memory', 'graph', 'forget', 'nodes', 'replaying', 'continually']","['continual', 'replay', 'graph', 'incremental', 'memory', 'topological', 'nodes', 'buffer', 'catastrophic', 'forgetting']","['forgetting', 'continual', 'graphs', 'replaying', 'incrementally', 'learnability', 'streaming', 'gcl', 'rememorize', 'tasks']","[""  Continual learning (CL) is the research field that aims to build machine\nlearning models that can accumulate knowledge continuously over different tasks\nwithout retraining from scratch. Previous studies have shown that pre-training\ngraph neural networks (GNN) may lead to negative transfer (Hu et al., 2020)\nafter fine-tuning, a setting which is closely related to CL. Thus, we focus on\nstudying GNN in the continual graph learning (CGL) setting. We propose the\nfirst continual graph learning benchmark for spatio-temporal graphs and use it\nto benchmark well-known CGL methods in this novel setting. The benchmark is\nbased on the N-UCLA and NTU-RGB+D datasets for skeleton-based action\nrecognition. Beyond benchmarking for standard performance metrics, we study the\nclass and task-order sensitivity of CGL methods, i.e., the impact of learning\norder on each class/task's performance, and the architectural sensitivity of\nCGL methods with backbone GNN at various widths and depths. We reveal that\ntask-order robust methods can still be class-order sensitive and observe\nresults that contradict previous empirical observations on architectural\nsensitivity in CL.\n"", '  When handling streaming graphs, existing graph representation learning models\nencounter a catastrophic forgetting problem, where previously learned knowledge\nof these models is easily overwritten when learning with newly incoming graphs.\nIn response, Continual Graph Learning (CGL) emerges as a novel paradigm\nenabling graph representation learning from streaming graphs. Our prior work,\nCondense and Train (CaT) is a replay-based CGL framework with a balanced\ncontinual learning procedure, which designs a small yet effective memory bankn\nfor replaying. Although the CaT alleviates the catastrophic forgetting problem,\nthere exist three issues: (1) The graph condensation only focuses on labelled\nnodes while neglecting abundant information carried by unlabelled nodes; (2)\nThe continual training scheme of the CaT overemphasises on the previously\nlearned knowledge, limiting the model capacity to learn from newly added\nmemories; (3) Both the condensation process and replaying process of the CaT\nare time-consuming. In this paper, we propose a PsUdo-label guided Memory bAnk\n(PUMA) CGL framework, extending from the CaT to enhance its efficiency and\neffectiveness by overcoming the above-mentioned weaknesses and limits. To fully\nexploit the information in a graph, PUMA expands the coverage of nodes during\ngraph condensation with both labelled and unlabelled nodes. Furthermore, a\ntraining-from-scratch strategy is proposed to upgrade the previous continual\nlearning scheme for a balanced training between the historical and the new\ngraphs. Besides, PUMA uses a one-time prorogation and wide graph encoders to\naccelerate the graph condensation and the graph encoding process in the\ntraining stage to improve the efficiency of the whole framework. Extensive\nexperiments on six datasets for the node classification task demonstrate the\nstate-of-the-art performance and efficiency over existing methods.\n', '  Continual learning on graph data has recently attracted paramount attention\nfor its aim to resolve the catastrophic forgetting problem on existing tasks\nwhile adapting the sequentially updated model to newly emerged graph tasks.\nWhile there have been efforts to summarize progress on continual learning\nresearch over Euclidean data, e.g., images and texts, a systematic review of\nprogress in continual learning on graphs, a.k.a, continual graph learning (CGL)\nor lifelong graph learning, is still demanding. Graph data are far more complex\nin terms of data structures and application scenarios, making CGL task\nsettings, model designs, and applications extremely challenging. To bridge the\ngap, we provide a comprehensive review of existing continual graph learning\n(CGL) algorithms by elucidating the different task settings and categorizing\nthe existing methods based on their characteristics. We compare the CGL methods\nwith traditional continual learning techniques and analyze the applicability of\nthe traditional continual learning techniques to CGL tasks. Additionally, we\nreview the benchmark works that are crucial to CGL research. Finally, we\ndiscuss the remaining challenges and propose several future directions. We will\nmaintain an up-to-date GitHub repository featuring a comprehensive list of CGL\nalgorithms, accessible at\nhttps://github.com/UConn-DSIS/Survey-of-Continual-Learning-on-Graphs.\n']",Continual Graph Learning
438,437,15,437_learning_classification_shot_classifier,"['learning', 'classification', 'shot', 'classifier', 'learn', 'trained', 'features', 'representations', 'classes', 'outlier']","['shot', 'outlier', 'meta', 'classes', 'base', 'examples', 'combiner', 'representations', 'inlier', 'classification']","['classification', 'infomax', 'adapting', 'miniimagenet', 'causalfewshot', 'inlier', 'sgpt', 'transductive', 'target', 'fsl']","['  In cross-domain few-shot classification, \\emph{nearest centroid classifier}\n(NCC) aims to learn representations to construct a metric space where few-shot\nclassification can be performed by measuring the similarities between samples\nand the prototype of each class. An intuition behind NCC is that each sample is\npulled closer to the class centroid it belongs to while pushed away from those\nof other classes. However, in this paper, we find that there exist high\nsimilarities between NCC-learned representations of two samples from different\nclasses. In order to address this problem, we propose a bi-level optimization\nframework, \\emph{maximizing optimized kernel dependence} (MOKD) to learn a set\nof class-specific representations that match the cluster structures indicated\nby labeled data of the given task. Specifically, MOKD first optimizes the\nkernel adopted in \\emph{Hilbert-Schmidt independence criterion} (HSIC) to\nobtain the optimized kernel HSIC (opt-HSIC) that can capture the dependence\nmore precisely. Then, an optimization problem regarding the opt-HSIC is\naddressed to simultaneously maximize the dependence between representations and\nlabels and minimize the dependence among all samples. Extensive experiments on\nMeta-Dataset demonstrate that MOKD can not only achieve better generalization\nperformance on unseen domains in most cases but also learn better data\nrepresentation clusters. The project repository of MOKD is available at:\n\\href{https://github.com/tmlr-group/MOKD}{https://github.com/tmlr-group/MOKD}.\n', '  In Few-Shot Learning (FSL), models are trained to recognise unseen objects\nfrom a query set, given a few labelled examples from a support set. In standard\nFSL, models are evaluated on query instances sampled from the same class\ndistribution of the support set. In this work, we explore the more nuanced and\npractical challenge of Open-Set Few-Shot Recognition (OSFSL). Unlike standard\nFSL, OSFSL incorporates unknown classes into the query set, thereby requiring\nthe model not only to classify known classes but also to identify outliers.\nBuilding on the groundwork laid by previous studies, we define a novel\ntransductive inference technique that leverages the InfoMax principle to\nexploit the unlabelled query set. We called our approach the Enhanced Outlier\nLogit (EOL) method. EOL refines class prototype representations through model\ncalibration, effectively balancing the inlier-outlier ratio. This calibration\nenhances pseudo-label accuracy for the query set and improves the optimisation\nobjective within the transductive inference process. We provide a comprehensive\nempirical evaluation demonstrating that EOL consistently surpasses traditional\nmethods, recording performance improvements ranging from approximately $+1.3%$\nto $+6.3%$ across a variety of classification and outlier detection metrics and\nbenchmarks, even in the presence of inlier-outlier imbalance.\n', '  Few-shot-learning (FSL) commonly requires a model to identify images\n(queries) that belong to classes unseen during training, based on a few labeled\nsamples of the new classes (support set) as reference. So far, plenty of\nalgorithms involve training data augmentation to improve the generalization\ncapability of FSL models, but outlier queries or support images during\ninference can still pose great generalization challenges. In this work, to\nreduce the bias caused by the outlier samples, we generate additional\ntest-class samples by combining original samples with suitable train-class\nsamples via a generative image combiner. Then, we obtain averaged features via\nan augmentor, which leads to more typical representations through the\naveraging. We experimentally and theoretically demonstrate the effectiveness of\nour method, e.g., obtaining a test accuracy improvement proportion of around\n10% (e.g., from 46.86% to 53.28%) for trained FSL models. Importantly, given\npretrained image combiner, our method is training-free for off-the-shelf FSL\nmodels, whose performance can be improved without extra datasets nor further\ntraining of the models themselves.\n']",Few-Shot Classification and Representation Learning
439,438,15,438_valuation_valuing_shapley_dataset,"['valuation', 'valuing', 'shapley', 'dataset', 'value', 'data', 'values', 'ml', 'quantifying', 'usefulness']","['valuation', 'value', 'data', 'points', 'values', 'detrimental', 'utility', 'deletions', 'asymptotic', 'solid']","['valuation', 'shapley', 'dataset', 'ml', 'usefulness', 'inflation', 'validation', 'decremental', 'ecoval', 'indicators']","['  As data plays an increasingly pivotal role in decision-making, the emergence\nof data markets underscores the growing importance of data valuation. Within\nthe machine learning landscape, Data Shapley stands out as a widely embraced\nmethod for data valuation. However, a limitation of Data Shapley is its\nassumption of a fixed dataset, contrasting with the dynamic nature of\nreal-world applications where data constantly evolves and expands. This paper\nestablishes the relationship between Data Shapley and infinite-order\nU-statistics and addresses this limitation by quantifying the uncertainty of\nData Shapley with changes in data distribution from the perspective of\nU-statistics. We make statistical inferences on data valuation to obtain\nconfidence intervals for the estimations. We construct two different algorithms\nto estimate this uncertainty and provide recommendations for their applicable\nsituations. We also conduct a series of experiments on various datasets to\nverify asymptotic normality and propose a practical trading scenario enabled by\nthis method.\n', '  Data valuation has garnered increasing attention in recent years, given the\ncritical role of high-quality data in various applications, particularly in\nmachine learning tasks. There are diverse technical avenues to quantify the\nvalue of data within a corpus. While Shapley value-based methods are among the\nmost widely used techniques in the literature due to their solid theoretical\nfoundation, the accurate calculation of Shapley values is often intractable,\nleading to the proposal of numerous approximated calculation methods. Despite\nsignificant progress, nearly all existing methods overlook the utilization of\ndistribution information of values within a data corpus. In this paper, we\ndemonstrate that both global and local statistical information of value\ndistributions hold significant potential for data valuation within the context\nof machine learning. Firstly, we explore the characteristics of both global and\nlocal value distributions across several simulated and real data corpora.\nUseful observations and clues are obtained. Secondly, we propose a new data\nvaluation method that estimates Shapley values by incorporating the explored\ndistribution characteristics into an existing method, AME. Thirdly, we present\na new path to address the dynamic data valuation problem by formulating an\noptimization problem that integrates information of both global and local value\ndistributions. Extensive experiments are conducted on Shapley value estimation,\nvalue-based data removal/adding, mislabeled data detection, and\nincremental/decremental data valuation. The results showcase the effectiveness\nand efficiency of our proposed methodologies, affirming the significant\npotential of global and local value distributions in data valuation.\n', '  Measuring the value of individual samples is critical for many data-driven\ntasks, e.g., the training of a deep learning model. Recent literature witnesses\nthe substantial efforts in developing data valuation methods. The primary data\nvaluation methodology is based on the Shapley value from game theory, and\nvarious methods are proposed along this path. {Even though Shapley value-based\nvaluation has solid theoretical basis, it is entirely an experiment-based\napproach and no valuation model has been constructed so far.} In addition,\ncurrent data valuation methods ignore the interpretability of the output\nvalues, despite an interptable data valuation method is of great helpful for\napplications such as data pricing. This study aims to answer an important\nquestion: is data valuation learnable and interpretable? A learned valuation\nmodel have several desirable merits such as fixed number of parameters and\nknowledge reusability. An intrepretable data valuation model can explain why a\nsample is valuable or invaluable. To this end, two new data value modeling\nframeworks are proposed, in which a multi-layer perception~(MLP) and a new\nregression tree are utilized as specific base models for model training and\ninterpretability, respectively. Extensive experiments are conducted on\nbenchmark datasets. {The experimental results provide a positive answer for the\nquestion.} Our study opens up a new technical path for the assessing of data\nvalues. Large data valuation models can be built across many different\ndata-driven tasks, which can promote the widespread application of data\nvaluation.\n']",Data Valuation Methods
440,439,15,439_tactics_uav_combat_maneuvers,"['tactics', 'uav', 'combat', 'maneuvers', 'uavs', 'evader', 'drones', 'dogfighting', 'tactical', 'dogfight']","['dogfight', 'combat', 'aircraft', 'aerial', 'maneuvers', 'flight', 'air', 'evader', 'drones', 'pursuit']","['uav', 'maneuvers', 'evader', 'dogfighting', 'tactical', 'pursuit', 'pilot', 'strategies', 'agility', 'simulator']","['  This paper proposes a three-layer unmanned combat aerial vehicle (UCAV)\ndogfight frame where Deep reinforcement learning (DRL) is responsible for\nhigh-level maneuver decision. A four-channel low-level control law is firstly\nconstructed, followed by a library containing eight basic flight maneuvers\n(BFMs). Double deep Q network (DDQN) is applied for BFM selection in UCAV\ndogfight, where the opponent strategy during the training process is\nconstructed with DT. Our simulation result shows that, the agent can achieve a\nwin rate of 85.75% against the DT strategy, and positive results when facing\nvarious unseen opponents. Based on the proposed frame, interpretability of the\nDRL-based dogfight is significantly improved. The agent performs yo-yo to\nadjust its turn rate and gain higher maneuverability. Emergence of ""Dive and\nChase"" behavior also indicates the agent can generate a novel tactic that\nutilizes the drawback of its opponent.\n', '  Dogfighting is a challenging scenario in aerial applications that requires a\ncomprehensive understanding of both strategic maneuvers and the aerodynamics of\nagile aircraft. The aerial agent needs to not only understand tactically\nevolving maneuvers of fighter jets from a long-term perspective but also react\nto rapidly changing aerodynamics of aircraft from a short-term viewpoint. In\nthis paper, we introduce TempFuser, a novel long short-term temporal fusion\ntransformer architecture that can learn agile, tactical, and acrobatic flight\nmaneuvers in complex dogfight problems. Our approach integrates two distinct\ntemporal transition embeddings into a transformer-based network to\ncomprehensively capture both the long-term tactics and short-term agility of\naerial agents. By incorporating these perspectives, our policy network\ngenerates end-to-end flight commands that secure dominant positions over the\nlong term and effectively outmaneuver agile opponents. After training in a\nhigh-fidelity flight simulator, our model successfully learns to execute\nstrategic maneuvers, outperforming baseline policy models against various types\nof opponent aircraft. Notably, our model exhibits human-like acrobatic\nmaneuvers even when facing adversaries with superior specifications, all\nwithout relying on explicit prior knowledge. Moreover, it demonstrates robust\npursuit performance in challenging supersonic and low-altitude situations. Demo\nvideos are available at https://sites.google.com/view/tempfuser.\n', ""  Unmanned Combat Aerial Vehicle (UCAV) dogfight, which refers to a fight\nbetween two or more UCAVs usually at close quarters, plays a decisive role on\nthe aerial battlefields. With the evolution of artificial intelligence,\ndogfight progressively transits towards intelligent and autonomous modes.\nHowever, the development of autonomous dogfight policy learning is hindered by\nchallenges such as weak exploration capabilities, low learning efficiency, and\nunrealistic simulated environments. To overcome these challenges, this paper\nproposes a novel imitative reinforcement learning framework, which efficiently\nleverages expert data while enabling autonomous exploration. The proposed\nframework not only enhances learning efficiency through expert imitation, but\nalso ensures adaptability to dynamic environments via autonomous exploration\nwith reinforcement learning. Therefore, the proposed framework can learn a\nsuccessful dogfight policy of 'pursuit-lock-launch' for UCAVs. To support\ndata-driven learning, we establish a dogfight environment based on the\nHarfang3D sandbox, where we conduct extensive experiments. The results indicate\nthat the proposed framework excels in multistage dogfight, significantly\noutperforms state-of-the-art reinforcement learning and imitation learning\nmethods. Thanks to the ability of imitating experts and autonomous exploration,\nour framework can quickly learn the critical knowledge in complex aerial combat\ntasks, achieving up to a 100% success rate and demonstrating excellent\nrobustness.\n""]","""UAV Combat Tactics and Maneuvers"""
441,440,14,440_modeling_autoencoder_nonlinear_nonlinearly,"['modeling', 'autoencoder', 'nonlinear', 'nonlinearly', 'parameterized', 'neural', 'simulating', 'dynamical', 'pdes', 'dynamics']","['equations', 'latent', 'differential', 'nonlinear', 'manifold', 'space', 'adjoint', 'dynamics', 'autoencoder', 'equation']","['autoencoder', 'nonlinear', 'parameterized', 'simulating', 'dynamical', 'pdes', 'dimensionality', 'advection', 'aerospace', 'lasdi']","['  A new knowledge-based and machine learning hybrid modeling approach, called\nconditional Gaussian neural stochastic differential equation (CGNSDE), is\ndeveloped to facilitate modeling complex dynamical systems and implementing\nanalytic formulae of the associated data assimilation (DA). In contrast to the\nstandard neural network predictive models, the CGNSDE is designed to\neffectively tackle both forward prediction tasks and inverse state estimation\nproblems. The CGNSDE starts by exploiting a systematic causal inference via\ninformation theory to build a simple knowledge-based nonlinear model that\nnevertheless captures as much explainable physics as possible. Then, neural\nnetworks are supplemented to the knowledge-based model in a specific way, which\nnot only characterizes the remaining features that are challenging to model\nwith simple forms but also advances the use of analytic formulae to efficiently\ncompute the nonlinear DA solution. These analytic formulae are used as an\nadditional computationally affordable loss to train the neural networks that\ndirectly improve the DA accuracy. This DA loss function promotes the CGNSDE to\ncapture the interactions between state variables and thus advances its modeling\nskills. With the DA loss, the CGNSDE is more capable of estimating extreme\nevents and quantifying the associated uncertainty. Furthermore, crucial\nphysical properties in many complex systems, such as the translate-invariant\nlocal dependence of state variables, can significantly simplify the neural\nnetwork structures and facilitate the CGNSDE to be applied to high-dimensional\nsystems. Numerical experiments based on chaotic systems with intermittency and\nstrong non-Gaussian features indicate that the CGNSDE outperforms\nknowledge-based regression models, and the DA loss further enhances the\nmodeling skills of the CGNSDE.\n', '  Numerical solvers of partial differential equations (PDEs) have been widely\nemployed for simulating physical systems. However, the computational cost\nremains a major bottleneck in various scientific and engineering applications,\nwhich has motivated the development of reduced-order models (ROMs). Recently,\nmachine-learning-based ROMs have gained significant popularity and are\npromising for addressing some limitations of traditional ROM methods,\nespecially for advection dominated systems. In this chapter, we focus on a\nparticular framework known as Latent Space Dynamics Identification (LaSDI),\nwhich transforms the high-fidelity data, governed by a PDE, to simpler and\nlow-dimensional latent-space data, governed by ordinary differential equations\n(ODEs). These ODEs can be learned and subsequently interpolated to make ROM\npredictions. Each building block of LaSDI can be easily modulated depending on\nthe application, which makes the LaSDI framework highly flexible. In\nparticular, we present strategies to enforce the laws of thermodynamics into\nLaSDI models (tLaSDI), enhance robustness in the presence of noise through the\nweak form (WLaSDI), select high-fidelity training data efficiently through\nactive learning (gLaSDI, GPLaSDI), and quantify the ROM prediction uncertainty\nthrough Gaussian processes (GPLaSDI). We demonstrate the performance of\ndifferent LaSDI approaches on Burgers equation, a non-linear heat conduction\nproblem, and a plasma physics problem, showing that LaSDI algorithms can\nachieve relative errors of less than a few percent and up to thousands of times\nspeed-ups.\n', '  Numerically solving partial differential equations (PDEs) can be challenging\nand computationally expensive. This has led to the development of reduced-order\nmodels (ROMs) that are accurate but faster than full order models (FOMs).\nRecently, machine learning advances have enabled the creation of non-linear\nprojection methods, such as Latent Space Dynamics Identification (LaSDI). LaSDI\nmaps full-order PDE solutions to a latent space using autoencoders and learns\nthe system of ODEs governing the latent space dynamics. By interpolating and\nsolving the ODE system in the reduced latent space, fast and accurate ROM\npredictions can be made by feeding the predicted latent space dynamics into the\ndecoder. In this paper, we introduce GPLaSDI, a novel LaSDI-based framework\nthat relies on Gaussian process (GP) for latent space ODE interpolations. Using\nGPs offers two significant advantages. First, it enables the quantification of\nuncertainty over the ROM predictions. Second, leveraging this prediction\nuncertainty allows for efficient adaptive training through a greedy selection\nof additional training data points. This approach does not require prior\nknowledge of the underlying PDEs. Consequently, GPLaSDI is inherently\nnon-intrusive and can be applied to problems without a known PDE or its\nresidual. We demonstrate the effectiveness of our approach on the Burgers\nequation, Vlasov equation for plasma physics, and a rising thermal bubble\nproblem. Our proposed method achieves between 200 and 100,000 times speed-up,\nwith up to 7% relative error.\n']",Machine Learning for Nonlinear Dynamical Systems Modeling
442,441,14,441_knowledge_contexts_facts_factuality,"['knowledge', 'contexts', 'facts', 'factuality', 'language', 'factual', 'answering', 'entity', 'learned', 'wikidata']","['factual', 'knowledge', 'facts', 'entity', 'plausible', 'gaps', 'factuality', 'abstain', 'internal', 'subject']","['knowledge', 'contexts', 'factuality', 'entity', 'wikidata', 'pretraining', 'llms', 'qa', 'relations', 'coherency']","['  In this work, we study the impact of QA fine-tuning data on downstream\nfactuality. We show that fine-tuning on lesser-known facts that are poorly\nstored during pretraining yields significantly worse factuality than\nfine-tuning on well-known facts, even when all facts are seen during\npretraining. We prove this phenomenon theoretically, showing that training on\nlesser-known facts can lead the model to ignore subject entity names and\ninstead output a generic plausible response even when the relevant factual\nknowledge is encoded in the model. On three question answering benchmarks\n(PopQA, Entity Questions, and MMLU) and two language models (Llama-2-7B and\nMistral-7B), we find that (i) finetuning on a completely factual but\nlesser-known subset of the data deteriorates downstream factuality (5-10%) and\n(ii) finetuning on a subset of better-known examples matches or outperforms\nfinetuning on the entire dataset. Ultimately, our results shed light on the\ninteraction between pretrained knowledge and finetuning data and demonstrate\nthe importance of taking into account how facts are stored in the pretrained\nmodel when fine-tuning for knowledge-intensive tasks.\n', ""  Large language models (LLMs) face issues in handling factual knowledge,\nmaking it vital to evaluate their true ability to understand facts. In this\nstudy, we introduce knowledge probing frameworks, BELIEF(-ICL), to evaluate the\nknowledge understanding ability of not only encoder-based PLMs but also\ndecoder-based PLMs from diverse perspectives. BELIEFs utilize a multi-prompt\ndataset to evaluate PLM's accuracy, consistency, and reliability in factual\nknowledge understanding. To provide a more reliable evaluation with BELIEFs, we\nsemi-automatically create MyriadLAMA, which has more diverse prompts than\nexisting datasets. We validate the effectiveness of BELIEFs in correctly and\ncomprehensively evaluating PLM's factual understanding ability through\nextensive evaluations. We further investigate key factors in learning facts in\nLLMs, and reveal the limitation of the prompt-based knowledge probing. The\ndataset is anonymously publicized.\n"", '  Factual knowledge encoded in Pre-trained Language Models (PLMs) enriches\ntheir representations and justifies their use as knowledge bases. Previous work\nhas focused on probing PLMs for factual knowledge by measuring how often they\ncan correctly predict an object entity given a subject and a relation, and\nimproving fact retrieval by optimizing the prompts used for querying PLMs. In\nthis work, we consider a complementary aspect, namely the coherency of factual\nknowledge in PLMs, i.e., how often can PLMs predict the subject entity given\nits initial prediction of the object entity. This goes beyond evaluating how\nmuch PLMs know, and focuses on the internal state of knowledge inside them. Our\nresults indicate that PLMs have low coherency using manually written, optimized\nand paraphrased prompts, but including an evidence paragraph leads to\nsubstantial improvement. This shows that PLMs fail to model inverse relations\nand need further enhancements to be able to handle retrieving facts from their\nparameters in a coherent manner, and to be considered as knowledge bases.\n']",Evaluating Factual Knowledge in Language Models
443,442,14,442_throughput_streaming_batching_workloads,"['throughput', 'streaming', 'batching', 'workloads', 'parallelization', 'chatbots', 'bottlenecks', 'serving', 'scheduling', 'concurrent']","['throughput', 'request', 'latency', 'requests', 'token', 'batching', 'eloquent', 'streaming', 'scheduling', 'prefill']","['streaming', 'batching', 'workloads', 'chatbots', 'concurrent', 'sglang', 'tokens', 'vllm', 'stall', 'gpus']","['  High-demand LLM inference services (e.g., ChatGPT and BARD) support a wide\nrange of requests from short chat conversations to long document reading. To\nensure that all client requests are processed fairly, most major LLM inference\nservices have request rate limits, to ensure that no client can dominate the\nrequest queue. However, this rudimentary notion of fairness also results in\nunder-utilization of the resources and poor client experience when there is\nspare capacity. While there is a rich literature on fair scheduling, serving\nLLMs presents new challenges due to their unpredictable request lengths and\ntheir unique batching characteristics on parallel accelerators. This paper\nintroduces the definition of LLM serving fairness based on a cost function that\naccounts for the number of input and output tokens processed. To achieve\nfairness in serving, we propose a novel scheduling algorithm, the Virtual Token\nCounter (VTC), a fair scheduler based on the continuous batching mechanism. We\nprove a 2x tight upper bound on the service difference between two backlogged\nclients, adhering to the requirement of work-conserving. Through extensive\nexperiments, we demonstrate the superior performance of VTC in ensuring\nfairness, especially in contrast to other baseline methods, which exhibit\nshortcomings under various conditions. The reproducible code is available at\nhttps://github.com/Ying1123/VTC-artifact\n', '  The complexity of large language model (LLM) serving workloads has\nsubstantially increased due to the integration with external tool invocations,\nsuch as ChatGPT plugins. In this paper, we identify a new opportunity for\nefficient LLM serving for requests that trigger tools: tool partial execution\nalongside LLM decoding. To this end, we design Conveyor, an efficient LLM\nserving system optimized for handling requests involving external tools. We\nintroduce a novel interface for tool developers to expose partial execution\nopportunities to the LLM serving system and a request scheduler that\nfacilitates partial tool execution. Our results demonstrate that tool partial\nexecution can improve request completion latency by up to 38.8%.\n', '  Each LLM serving request goes through two phases. The first is prefill which\nprocesses the entire input prompt and produces the first output token and the\nsecond is decode which generates the rest of output tokens, one-at-a-time.\nPrefill iterations have high latency but saturate GPU compute due to parallel\nprocessing of the input prompt. In contrast, decode iterations have low latency\nbut also low compute utilization because a decode iteration processes only a\nsingle token per request. This makes batching highly effective for decodes and\nconsequently for overall throughput. However, batching multiple requests leads\nto an interleaving of prefill and decode iterations which makes it challenging\nto achieve both high throughput and low latency.\n  We introduce an efficient LLM inference scheduler, Sarathi-Serve, to address\nthis throughput-latency tradeoff. Sarathi-Serve introduces chunked-prefills\nwhich splits a prefill request into near equal sized chunks and creates\nstall-free schedules that adds new requests in a batch without pausing ongoing\ndecodes. Stall-free scheduling unlocks the opportunity to improve throughput\nwith large batch sizes while minimizing the effect of batching on latency.\nFurthermore, uniform batches in Sarathi-Serve ameliorate the imbalance between\niterations resulting in minimal pipeline bubbles.\n  Our techniques yield significant improvements in inference performance across\nmodels and hardware under tail latency constraints. For Mistral-7B on single\nA100 GPUs, we achieve 2.6x higher serving capacity and up to 3.7x higher\nserving capacity for the Yi-34B model on two A100 GPUs as compared to vLLM.\nWhen used with pipeline parallelism on Falcon-180B, Sarathi-Serve provides up\nto 5.6x gain in the end-to-end serving capacity. The source code for\nSarathi-Serve is available at https://github.com/microsoft/sarathi-serve.\n']",Optimizing Large Language Model Serving Systems
444,443,14,443_depressive_depression_depressed_distress,"['depressive', 'depression', 'depressed', 'distress', 'multimodal', 'speech', 'suicide', 'psychiatric', 'recordings', 'emotions']","['depression', 'speech', 'interview', 'mental', 'suicide', 'detection', 'vlogs', 'symptoms', 'acoustic', 'interviews']","['depressive', 'distress', 'multimodal', 'speech', 'recordings', 'interviews', 'psychiatrists', 'comorbidity', 'vlogs', 'diagnosing']","['  Depression is a critical concern in global mental health, prompting extensive\nresearch into AI-based detection methods. Among various AI technologies, Large\nLanguage Models (LLMs) stand out for their versatility in mental healthcare\napplications. However, their primary limitation arises from their exclusive\ndependence on textual input, which constrains their overall capabilities.\nFurthermore, the utilization of LLMs in identifying and analyzing depressive\nstates is still relatively untapped. In this paper, we present an innovative\napproach to integrating acoustic speech information into the LLMs framework for\nmultimodal depression detection. We investigate an efficient method for\ndepression detection by integrating speech signals into LLMs utilizing Acoustic\nLandmarks. By incorporating acoustic landmarks, which are specific to the\npronunciation of spoken words, our method adds critical dimensions to text\ntranscripts. This integration also provides insights into the unique speech\npatterns of individuals, revealing the potential mental states of individuals.\nEvaluations of the proposed approach on the DAIC-WOZ dataset reveal\nstate-of-the-art results when compared with existing Audio-Text baselines. In\naddition, this approach is not only valuable for the detection of depression\nbut also represents a new perspective in enhancing the ability of LLMs to\ncomprehend and process speech signals.\n', '  Current automatic depression detection systems provide predictions directly\nwithout relying on the individual symptoms/items of depression as denoted in\nthe clinical depression rating scales. In contrast, clinicians assess each item\nin the depression rating scale in a clinical setting, thus implicitly providing\na more detailed rationale for a depression diagnosis. In this work, we make a\nfirst step towards using the acoustic features of speech to predict individual\nitems of the depression rating scale before obtaining the final depression\nprediction. For this, we use convolutional (CNN) and recurrent (long short-term\nmemory (LSTM)) neural networks. We consider different approaches to learning\nthe temporal context of speech. Further, we analyze two variants of voting\nschemes for individual item prediction and depression detection. We also\ninclude an animated visualization that shows an example of item prediction over\ntime as the speech progresses.\n', ""  The utilization of automated depression detection significantly enhances\nearly intervention for individuals experiencing depression. Despite numerous\nproposals on automated depression detection using recorded clinical interview\nvideos, limited attention has been paid to considering the hierarchical\nstructure of the interview questions. In clinical interviews for diagnosing\ndepression, clinicians use a structured questionnaire that includes routine\nbaseline questions and follow-up questions to assess the interviewee's\ncondition. This paper introduces HiQuE (Hierarchical Question Embedding\nnetwork), a novel depression detection framework that leverages the\nhierarchical relationship between primary and follow-up questions in clinical\ninterviews. HiQuE can effectively capture the importance of each question in\ndiagnosing depression by learning mutual information across multiple\nmodalities. We conduct extensive experiments on the widely-used clinical\ninterview data, DAIC-WOZ, where our model outperforms other state-of-the-art\nmultimodal depression detection models and emotion recognition models,\nshowcasing its clinical utility in depression detection.\n""]",Multimodal Depression Detection using Speech and AI
445,444,14,444_llm_self_bias_selfreflection_responses_corrections,"['llm_self_bias', 'selfreflection', 'responses', 'corrections', 'self', 'mistakes', 'revise', 'wording', 'improve', 'bias']","['correction', 'self', 'reflection', 'intrinsic', 'feedback', 'mistakes', 'guideline', 'responses', 'external', 'correct']","['llm_self_bias', 'selfreflection', 'mistakes', 'feedback', 'confidence', 'intrinsic', 'remark', 'remedy', 'latent', 'idiosyncrasies']","['  Large Language Models (LLMs) can improve their responses when instructed to\ndo so, a capability known as self-correction. When these instructions lack\nspecific details about the issues in the response, this is referred to as\nleveraging the intrinsic self-correction capability. The empirical success of\nself-correction can be found in various applications, e.g., text detoxification\nand social bias mitigation. However, leveraging this self-correction capability\nmay not always be effective, as it has the potential to revise an initially\ncorrect response into an incorrect one. In this paper, we endeavor to\nunderstand how and why leveraging the self-correction capability is effective.\nWe identify that appropriate instructions can guide LLMs to a convergence\nstate, wherein additional self-correction steps do not yield further\nperformance improvements. We empirically demonstrate that model uncertainty and\nactivated latent concepts jointly characterize the effectiveness of\nself-correction. Furthermore, we provide a mathematical formulation indicating\nthat the activated latent concept drives the convergence of the model\nuncertainty and self-correction performance. Our analysis can also be\ngeneralized to the self-correction behaviors observed in Vision-Language Models\n(VLMs). Moreover, we highlight that task-agnostic debiasing can benefit from\nour principle in terms of selecting effective fine-tuning samples. Such initial\nsuccess demonstrates the potential extensibility for better instruction tuning\nand safety alignment.\n', ""  Large language models (LLMs) have attracted significant attention for their\nremarkable abilities in various natural language processing tasks, but they\nsuffer from hallucinations that will cause performance degradation. One\npromising solution to improve the LLMs' performance is to ask LLMs to revise\ntheir answer after generation, a technique known as self-correction. Among the\ntwo types of self-correction, intrinsic self-correction is considered a\npromising direction because it does not utilize external knowledge. However,\nrecent works doubt the validity of LLM's ability to conduct intrinsic\nself-correction. In this paper, we present a novel perspective on the intrinsic\nself-correction capabilities of LLMs through theoretical analyses and empirical\nexperiments. In addition, we identify two critical factors for successful\nself-correction: zero temperature and fair prompts. Leveraging these factors,\nwe demonstrate that intrinsic self-correction ability is exhibited across\nmultiple existing LLMs. Our findings offer insights into the fundamental\ntheories underlying the self-correction behavior of LLMs and remark on the\nimportance of unbiased prompts and zero temperature settings in harnessing\ntheir full potential.\n"", '  Self-correction is an approach to improving responses from large language\nmodels (LLMs) by refining the responses using LLMs during inference. Prior work\nhas proposed various self-correction frameworks using different sources of\nfeedback, including self-evaluation and external feedback. However, there is\nstill no consensus on the question of when LLMs can correct their own mistakes,\nas recent studies also report negative results. In this work, we critically\nsurvey broad papers and discuss the conditions required for successful\nself-correction. We first find that prior studies often do not define their\nresearch questions in detail and involve impractical frameworks or unfair\nevaluations that over-evaluate self-correction. To tackle these issues, we\ncategorize research questions in self-correction research and provide a\nchecklist for designing appropriate experiments. Our critical survey based on\nthe newly categorized research questions shows that (1) no prior work\ndemonstrates successful self-correction with feedback from prompted LLMs in\ngeneral tasks, (2) self-correction works well in tasks that can use reliable\nexternal feedback, and (3) large-scale fine-tuning enables self-correction.\n']",Self-Correction in Large Language Models
446,445,14,445_dentistry_dentist_dental_dentists,"['dentistry', 'dentist', 'dental', 'dentists', 'tooth', 'teeth', 'teethseg', 'segmentation', 'cnns', 'segmenting']","['dental', 'teeth', 'tooth', 'dentistry', 'fluorosis', 'intraoral', 'panoramic', 'segmentation', 'occlusal', 'dentists']","['dental', 'segmentation', 'cnns', 'deep', 'radiographic', 'photos', 'yolov8', 'panoramic', 'cone', 'masks']","['  Cone beam computed tomography (CBCT) is a common way of diagnosing dental\nrelated diseases. Accurate segmentation of 3D tooth is of importance for the\ntreatment. Although deep learning based methods have achieved convincing\nresults in medical image processing, they need a large of annotated data for\nnetwork training, making it very time-consuming in data collection and\nannotation. Besides, domain shift widely existing in the distribution of data\nacquired by different devices impacts severely the model generalization. To\nresolve the problem, we propose a multi-stage framework for 3D tooth\nsegmentation in dental CBCT, which achieves the third place in the\n""Semi-supervised Teeth Segmentation"" 3D (STS-3D) challenge. The experiments on\nvalidation set compared with other semi-supervised segmentation methods further\nindicate the validity of our approach.\n', '  Teeth segmentation and recognition are critical in various dental\napplications and dental diagnosis. Automatic and accurate segmentation\napproaches have been made possible by integrating deep learning models.\nAlthough teeth segmentation has been studied in the past, only some techniques\nwere able to effectively classify and segment teeth simultaneously. This\narticle offers a pipeline of two deep learning models, U-Net and YOLOv8, which\nresults in BB-UNet, a new architecture for the classification and segmentation\nof teeth on panoramic X-rays that is efficient and reliable. We have improved\nthe quality and reliability of teeth segmentation by utilising the YOLOv8 and\nU-Net capabilities. The proposed networks have been evaluated using the mean\naverage precision (mAP) and dice coefficient for YOLOv8 and BB-UNet,\nrespectively. We have achieved a 3\\% increase in mAP score for teeth\nclassification compared to existing methods, and a 10-15\\% increase in dice\ncoefficient for teeth segmentation compared to U-Net across different\ncategories of teeth. A new Dental dataset was created based on UFBA-UESC\ndataset with Bounding-Box and Polygon annotations of 425 dental panoramic\nX-rays. The findings of this research pave the way for a wider adoption of\nobject detection models in the field of dental diagnosis.\n', '  Accurate identification, localization, and segregation of teeth from Cone\nBeam Computed Tomography (CBCT) images are essential for analyzing dental\npathologies. Modeling an individual tooth can be challenging and intricate to\naccomplish, especially when fillings and other restorations introduce\nartifacts. This paper proposes a method for automatically detecting,\nidentifying, and extracting teeth from CBCT images. Our approach involves\ndividing the three-dimensional images into axial slices for image detection.\nTeeth are pinpointed and labeled using a single-stage object detector.\nSubsequently, bounding boxes are delineated and identified to create\nthree-dimensional representations of each tooth. The proposed solution has been\nsuccessfully integrated into the dental analysis tool Dentomo.\n']",Dental Image Analysis and Segmentation
447,446,14,446_privacy_private_federated_secure,"['privacy', 'private', 'federated', 'secure', 'secureboost', 'obfuscation', 'secret', 'sharing', 'protect', 'encryption']","['privacy', 'secure', 'clients', 'aggregation', 'server', 'federated', 'dropout', 'homomorphic', 'protection', 'attacks']","['privacy', 'federated', 'secureboost', 'homomorphic', 'colluding', 'decentralized', 'sybil', 'thwart', 'peer', 'leakage']","['  Most work in privacy-preserving federated learning (FL) has focused on\nhorizontally partitioned datasets where clients hold the same features and\ntrain complete client-level models independently. However, individual data\npoints are often scattered across different institutions, known as clients, in\nvertical FL (VFL) settings. Addressing this category of FL necessitates the\nexchange of intermediate outputs and gradients among participants, resulting in\npotential privacy leakage risks and slow convergence rates. Additionally, in\nmany real-world scenarios, VFL training also faces the acute issue of client\nstragglers and drop-outs, a serious challenge that can significantly hinder the\ntraining process but has been largely overlooked in existing studies. In this\nwork, we present vFedSec, a first dropout-tolerant VFL protocol, which can\nsupport the most generalized vertical framework. It achieves secure and\nefficient model training by using an innovative Secure Layer alongside an\nembedding-padding technique. We provide theoretical proof that our design\nattains enhanced security while maintaining training performance. Empirical\nresults from extensive experiments also demonstrate vFedSec is robust to client\ndropout and provides secure training with negligible computation and\ncommunication overhead. Compared to widely adopted homomorphic encryption (HE)\nmethods, our approach achieves a remarkable > 690x speedup and reduces\ncommunication costs significantly by > 9.6x.\n', ""  Decentralized learning (DL) offers a novel paradigm in machine learning by\ndistributing training across clients without central aggregation, enhancing\nscalability and efficiency. However, DL's peer-to-peer model raises challenges\nin protecting against inference attacks and privacy leaks. By forgoing central\nbottlenecks, DL demands privacy-preserving aggregation methods to protect data\nfrom 'honest but curious' clients and adversaries, maintaining network-wide\nprivacy. Privacy-preserving DL faces the additional hurdle of client dropout,\nclients not submitting updates due to connectivity problems or unavailability,\nfurther complicating aggregation.\n  This work proposes three secret sharing-based dropout resilience approaches\nfor privacy-preserving DL. Our study evaluates the efficiency, performance, and\naccuracy of these protocols through experiments on datasets such as MNIST,\nFashion-MNIST, SVHN, and CIFAR-10. We compare our protocols with traditional\nsecret-sharing solutions across scenarios, including those with up to 1000\nclients. Evaluations show that our protocols significantly outperform\nconventional methods, especially in scenarios with up to 30% of clients dropout\nand model sizes of up to $10^6$ parameters. Our approaches demonstrate markedly\nhigh efficiency with larger models, higher dropout rates, and extensive client\nnetworks, highlighting their effectiveness in enhancing decentralized learning\nsystems' privacy and dropout robustness.\n"", ""  Federated learning (FL) aims to protect data privacy by enabling clients to\nbuild machine learning models collaboratively without sharing their private\ndata. Recent works demonstrate that information exchanged during FL is subject\nto gradient-based privacy attacks, and consequently, a variety of\nprivacy-preserving methods have been adopted to thwart such attacks. However,\nthese defensive methods either introduce orders of magnitude more computational\nand communication overheads (e.g., with homomorphic encryption) or incur\nsubstantial model performance losses in terms of prediction accuracy (e.g.,\nwith differential privacy). In this work, we propose $\\textsc{FedCG}$, a novel\nfederated learning method that leverages conditional generative adversarial\nnetworks to achieve high-level privacy protection while still maintaining\ncompetitive model performance. $\\textsc{FedCG}$ decomposes each client's local\nnetwork into a private extractor and a public classifier and keeps the\nextractor local to protect privacy. Instead of exposing extractors,\n$\\textsc{FedCG}$ shares clients' generators with the server for aggregating\nclients' shared knowledge, aiming to enhance the performance of each client's\nlocal networks. Extensive experiments demonstrate that $\\textsc{FedCG}$ can\nachieve competitive model performance compared with FL baselines, and privacy\nanalysis shows that $\\textsc{FedCG}$ has a high-level privacy-preserving\ncapability. Code is available at https://github.com/yankang18/FedCG\n""]",Federated Learning and Privacy Protection
448,447,14,447_automata_ltl_automaton_solvers,"['automata', 'ltl', 'automaton', 'solvers', 'programming', 'algorithmics', 'constraints', 'logics', 'specification', 'monadic']","['logics', 'logic', 'specification', 'synthesis', 'resp', 'specifications', 'reactive', 'programming', 'temporal', 'equilibrium']","['automata', 'solvers', 'constraints', 'monadic', 'ltlf', 'planning', 'synthesis', 'reactive', 'safetyltl', 'satisfiability']","[""  For combinatorial optimization problems, model-based approaches such as\nmixed-integer programming (MIP) and constraint programming (CP) aim to decouple\nmodeling and solving a problem: the 'holy grail' of declarative problem\nsolving. We propose domain-independent dynamic programming (DIDP), a new\nmodel-based paradigm based on dynamic programming (DP). While DP is not new, it\nhas typically been implemented as a problem-specific method. We propose Dynamic\nProgramming Description Language (DyPDL), a formalism to define DP models, and\ndevelop Cost-Algebraic A* Solver for DyPDL (CAASDy), a generic solver for DyPDL\nusing state space search. We formalize existing problem-specific DP and state\nspace search methods for combinatorial optimization problems as DP models in\nDyPDL. Using CAASDy and commercial MIP and CP solvers, we experimentally\ncompare the DP models with existing MIP and CP models, showing that, despite\nits nascent nature, CAASDy outperforms MIP and CP on a number of common problem\nclasses.\n"", '  Reactive synthesis is the process of generating correct controllers from\ntemporal logic specifications. Classical LTL reactive synthesis handles\n(propositional) LTL as a specification language. Boolean abstractions allow\nreducing LTLt specifications (i.e., LTL with propositions replaced by literals\nfrom a theory calT), into equi-realizable LTL specifications. In this paper we\nextend these results into a full static synthesis procedure. The synthesized\nsystem receives from the environment valuations of variables from a rich theory\ncalT and outputs valuations of system variables from calT. We use the\nabstraction method to synthesize a reactive Boolean controller from the LTL\nspecification, and we combine it with functional synthesis to obtain a static\ncontroller for the original LTLt specification. We also show that our method\nallows responses in the sense that the controller can optimize its outputs in\norder to e.g., always provide the smallest safe values. This is the first full\nstatic synthesis method for LTLt, which is a deterministic program (hence\npredictable and efficient).\n', ""  Linear Temporal Logic (LTL) is one of the most popular temporal logics, that\ncomes into play in a variety of branches of computer science. Among the various\nreasons of its widespread use there are its strong foundational properties: LTL\nis equivalent to counter-free omega-automata, to star-free omega-regular\nexpressions, and (by Kamp's theorem) to the First-Order Theory of Linear Orders\n(FO-TLO). Safety and co-safety languages, where a finite prefix suffices to\nestablish whether a word does not belong or belongs to the language,\nrespectively, play a crucial role in lowering the complexity of problems like\nmodel checking and reactive synthesis for LTL. SafetyLTL (resp., coSafetyLTL)\nis a fragment of LTL where only universal (resp., existential) temporal\nmodalities are allowed, that recognises safety (resp., co-safety) languages\nonly. The main contribution of this paper is the introduction of a fragment of\nFO-TLO, called SafetyFO, and of its dual coSafetyFO, which are expressively\ncomplete with respect to the LTL-definable safety and co-safety languages. We\nprove that they exactly characterize SafetyLTL and coSafetyLTL, respectively, a\nresult that joins Kamp's theorem, and provides a clearer view of the\ncharacterization of (fragments of) LTL in terms of first-order languages. In\naddition, it gives a direct, compact, and self-contained proof that any safety\nlanguage definable in LTL is definable in SafetyLTL as well. As a by-product,\nwe obtain some interesting results on the expressive power of the weak tomorrow\noperator of SafetyLTL, interpreted over finite and infinite words. Moreover, we\nprove that, when interpreted over finite words, SafetyLTL (resp. coSafetyLTL)\ndevoid of the tomorrow (resp., weak tomorrow) operator captures the safety\n(resp., co-safety) fragment of LTL over finite words.\n""]",Temporal Logic and Automata for Problem Solving
449,448,14,448_causal_causalities_causality_reinforcement,"['causal', 'causalities', 'causality', 'reinforcement', 'robot', 'robots', 'discovering', 'guided', 'interventions', 'discovery']","['causal', 'discovery', 'causality', 'stationarity', 'variables', 'environments', 'relationships', 'cause', 'origin', 'exploration']","['causal', 'reinforcement', 'robot', 'discovering', 'guided', 'observational', 'imitation', 'environment', 'atari', 'rl']","[""  Imitation learning, which learns agent policy by mimicking expert\ndemonstration, has shown promising results in many applications such as medical\ntreatment regimes and self-driving vehicles. However, it remains a difficult\ntask to interpret control policies learned by the agent. Difficulties mainly\ncome from two aspects: 1) agents in imitation learning are usually implemented\nas deep neural networks, which are black-box models and lack interpretability;\n2) the latent causal mechanism behind agents' decisions may vary along the\ntrajectory, rather than staying static throughout time steps. To increase\ntransparency and offer better interpretability of the neural agent, we propose\nto expose its captured knowledge in the form of a directed acyclic causal\ngraph, with nodes being action and state variables and edges denoting the\ncausal relations behind predictions. Furthermore, we design this causal\ndiscovery process to be state-dependent, enabling it to model the dynamics in\nlatent causal graphs. Concretely, we conduct causal discovery from the\nperspective of Granger causality and propose a self-explainable imitation\nlearning framework, {\\method}. The proposed framework is composed of three\nparts: a dynamic causal discovery module, a causality encoding module, and a\nprediction module, and is trained in an end-to-end manner. After the model is\nlearned, we can obtain causal relations among states and action variables\nbehind its decisions, exposing policies learned by it. Experimental results on\nboth synthetic and real-world datasets demonstrate the effectiveness of the\nproposed {\\method} in learning the dynamic causal graphs for understanding the\ndecision-making of imitation learning meanwhile maintaining high prediction\naccuracy.\n"", ""  Deploying robots in human-shared spaces requires understanding interactions\namong nearby agents and objects. Modelling cause-and-effect relations through\ncausal inference aids in predicting human behaviours and anticipating robot\ninterventions. However, a critical challenge arises as existing causal\ndiscovery methods currently lack an implementation inside the ROS ecosystem,\nthe standard de facto in robotics, hindering effective utilisation in robotics.\nTo address this gap, this paper introduces ROS-Causal, a ROS-based framework\nfor onboard data collection and causal discovery in human-robot spatial\ninteractions. An ad-hoc simulator, integrated with ROS, illustrates the\napproach's effectiveness, showcasing the robot onboard generation of causal\nmodels during data collection. ROS-Causal is available on GitHub:\nhttps://github.com/lcastri/roscausal.git.\n"", '  Deploying robots in human-shared environments requires a deep understanding\nof how nearby agents and objects interact. Employing causal inference to model\ncause-and-effect relationships facilitates the prediction of human behaviours\nand enables the anticipation of robot interventions. However, a significant\nchallenge arises due to the absence of implementation of existing causal\ndiscovery methods within the ROS ecosystem, the standard de-facto framework in\nrobotics, hindering effective utilisation on real robots. To bridge this gap,\nin our previous work we proposed ROS-Causal, a ROS-based framework designed for\nonboard data collection and causal discovery in human-robot spatial\ninteractions. In this work, we present an experimental evaluation of ROS-Causal\nboth in simulation and on a new dataset of human-robot spatial interactions in\na lab scenario, to assess its performance and effectiveness. Our analysis\ndemonstrates the efficacy of this approach, showcasing how causal models can be\nextracted directly onboard by robots during data collection. The online causal\nmodels generated from the simulation are consistent with those from lab\nexperiments. These findings can help researchers to enhance the performance of\nrobotic systems in shared environments, firstly by studying the causal\nrelations between variables in simulation without real people, and then\nfacilitating the actual robot deployment in real human environments.\nROS-Causal: https://lcastri.github.io/roscausal\n']",Causal Discovery in Robotics and Imitation Learning
450,449,14,449_videos_motionctrl_animation_filmmaking,"['videos', 'motionctrl', 'animation', 'filmmaking', 'motions', 'camera', 'motion', 'cinematographic', '3d', 'mofa_video']","['motion', 'camera', 'video', 'videos', 'object', 'frames', 'animation', 'control', 'controllable', 'frame']","['motionctrl', 'animation', 'filmmaking', '3d', 'mofa_video', 'tracking', 'frames', 'camanimate', 'boximator', 'imageconductor']","['  Human image animation involves generating videos from a character photo,\nallowing user control and unlocking potential for video and movie production.\nWhile recent approaches yield impressive results using high-quality training\ndata, the inaccessibility of these datasets hampers fair and transparent\nbenchmarking. Moreover, these approaches prioritize 2D human motion and\noverlook the significance of camera motions in videos, leading to limited\ncontrol and unstable video generation. To demystify the training data, we\npresent HumanVid, the first large-scale high-quality dataset tailored for human\nimage animation, which combines crafted real-world and synthetic data. For the\nreal-world data, we compile a vast collection of copyright-free real-world\nvideos from the internet. Through a carefully designed rule-based filtering\nstrategy, we ensure the inclusion of high-quality videos, resulting in a\ncollection of 20K human-centric videos in 1080P resolution. Human and camera\nmotion annotation is accomplished using a 2D pose estimator and a SLAM-based\nmethod. For the synthetic data, we gather 2,300 copyright-free 3D avatar assets\nto augment existing available 3D assets. Notably, we introduce a rule-based\ncamera trajectory generation method, enabling the synthetic pipeline to\nincorporate diverse and precise camera motion annotation, which can rarely be\nfound in real-world data. To verify the effectiveness of HumanVid, we establish\na baseline model named CamAnimate, short for Camera-controllable Human\nAnimation, that considers both human and camera motions as conditions. Through\nextensive experimentation, we demonstrate that such simple baseline training on\nour HumanVid achieves state-of-the-art performance in controlling both human\npose and camera motions, setting a new benchmark. Code and data will be\npublicly available at https://github.com/zhenzhiwang/HumanVid/.\n', ""  Filmmaking and animation production often require sophisticated techniques\nfor coordinating camera transitions and object movements, typically involving\nlabor-intensive real-world capturing. Despite advancements in generative AI for\nvideo creation, achieving precise control over motion for interactive video\nasset generation remains challenging. To this end, we propose Image Conductor,\na method for precise control of camera transitions and object movements to\ngenerate video assets from a single image. An well-cultivated training strategy\nis proposed to separate distinct camera and object motion by camera LoRA\nweights and object LoRA weights. To further address cinematographic variations\nfrom ill-posed trajectories, we introduce a camera-free guidance technique\nduring inference, enhancing object movements while eliminating camera\ntransitions. Additionally, we develop a trajectory-oriented video motion data\ncuration pipeline for training. Quantitative and qualitative experiments\ndemonstrate our method's precision and fine-grained control in generating\nmotion-controllable videos from images, advancing the practical application of\ninteractive video synthesis. Project webpage available at\nhttps://liyaowei-stu.github.io/project/ImageConductor/\n"", '  Motions in a video primarily consist of camera motion, induced by camera\nmovement, and object motion, resulting from object movement. Accurate control\nof both camera and object motion is essential for video generation. However,\nexisting works either mainly focus on one type of motion or do not clearly\ndistinguish between the two, limiting their control capabilities and diversity.\nTherefore, this paper presents MotionCtrl, a unified and flexible motion\ncontroller for video generation designed to effectively and independently\ncontrol camera and object motion. The architecture and training strategy of\nMotionCtrl are carefully devised, taking into account the inherent properties\nof camera motion, object motion, and imperfect training data. Compared to\nprevious methods, MotionCtrl offers three main advantages: 1) It effectively\nand independently controls camera motion and object motion, enabling more\nfine-grained motion control and facilitating flexible and diverse combinations\nof both types of motion. 2) Its motion conditions are determined by camera\nposes and trajectories, which are appearance-free and minimally impact the\nappearance or shape of objects in generated videos. 3) It is a relatively\ngeneralizable model that can adapt to a wide array of camera poses and\ntrajectories once trained. Extensive qualitative and quantitative experiments\nhave been conducted to demonstrate the superiority of MotionCtrl over existing\nmethods. Project Page: https://wzhouxiff.github.io/projects/MotionCtrl/\n']",Video Animation and Motion Control
451,450,14,450_estimation_estimators_lasso_estimator,"['estimation', 'estimators', 'lasso', 'estimator', 'identifiability', 'deterministic', 'nonlinearly', 'identification', 'complexity', 'nonlinear']","['asymptotic', 'identification', 'linear', 'matrices', 'switching', 'identifiability', 'squares', 'timescale', 'systems', 'finite']","['estimators', 'lasso', 'identifiability', 'deterministic', 'stability', 'parallelizability', 'controllers', 'hanson', 'trajectory', 'squares']","['  Ordinary Differential Equations (ODEs) have recently gained a lot of\nattention in machine learning. However, the theoretical aspects, e.g.,\nidentifiability and asymptotic properties of statistical estimation are still\nobscure. This paper derives a sufficient condition for the identifiability of\nhomogeneous linear ODE systems from a sequence of equally-spaced error-free\nobservations sampled from a single trajectory. When observations are disturbed\nby measurement noise, we prove that under mild conditions, the parameter\nestimator based on the Nonlinear Least Squares (NLS) method is consistent and\nasymptotic normal with $n^{-1/2}$ convergence rate. Based on the asymptotic\nnormality property, we construct confidence sets for the unknown system\nparameters and propose a new method to infer the causal structure of the ODE\nsystem, i.e., inferring whether there is a causal link between system\nvariables. Furthermore, we extend the results to degraded observations,\nincluding aggregated and time-scaled ones. To the best of our knowledge, our\nwork is the first systematic study of the identifiability and asymptotic\nproperties in learning linear ODE systems. We also construct simulations with\nvarious system dimensions to illustrate the established theoretical results.\n', ""  Machine Learning (ML) and linear System Identification (SI) have been\nhistorically developed independently. In this paper, we leverage\nwell-established ML tools - especially the automatic differentiation framework\n- to introduce SIMBa, a family of discrete linear multi-step-ahead state-space\nSI methods using backpropagation. SIMBa relies on a novel\nLinear-Matrix-Inequality-based free parametrization of Schur matrices to ensure\nthe stability of the identified model.\n  We show how SIMBa generally outperforms traditional linear state-space SI\nmethods, and sometimes significantly, although at the price of a higher\ncomputational burden. This performance gap is particularly remarkable compared\nto other SI methods with stability guarantees, where the gain is frequently\nabove 25% in our investigations, hinting at SIMBa's ability to simultaneously\nachieve state-of-the-art fitting performance and enforce stability.\nInterestingly, these observations hold for a wide variety of input-output\nsystems and on both simulated and real-world data, showcasing the flexibility\nof the proposed approach. We postulate that this new SI paradigm presents a\ngreat extension potential to identify structured nonlinear models from data,\nand we hence open-source SIMBa on https://github.com/Cemempamoi/simba.\n"", '  The focus of this paper is on linear system identification in the setting\nwhere it is known that the underlying partially-observed linear dynamical\nsystem lies within a finite collection of known candidate models. We first\nconsider the problem of identification from a given trajectory, which in this\nsetting reduces to identifying the index of the true model with high\nprobability. We characterize the finite-time sample complexity of this problem\nby leveraging recent advances in the non-asymptotic analysis of linear\nleast-square methods in the literature. In comparison to the earlier results\nthat assume no prior knowledge of the system, our approach takes advantage of\nthe smaller hypothesis class and leads to the design of a learner with a\ndimension-free sample complexity bound. Next, we consider the switching control\nof linear systems, where there is a candidate controller for each of the\ncandidate models and data is collected through interaction of the system with a\ncollection of potentially destabilizing controllers. We develop a\ndimension-dependent criterion that can detect those destabilizing controllers\nin finite time. By leveraging these results, we propose a data-driven switching\nstrategy that identifies the unknown parameters of the underlying system. We\nthen provide a non-asymptotic analysis of its performance and discuss its\nimplications on the classical method of estimator-based supervisory control.\n']",System Identification and Estimation in Dynamical Systems
452,451,14,451_eigenfunctions_spectral_linearization_eigenfunction,"['eigenfunctions', 'spectral', 'linearization', 'eigenfunction', 'operators', 'observables', 'linearizations', 'koopman', 'operator', 'observable']","['operator', 'observables', 'eigenfunctions', 'mode', 'decomposition', 'nonlinear', 'operators', 'dynamical', 'pseudospectra', 'spectral']","['eigenfunctions', 'spectral', 'linearization', 'operators', 'observables', 'koopman', 'modes', 'hermitian', 'numerical', 'compressions']","[""  This paper presents a novel approach for estimating the Koopman operator\ndefined on a reproducing kernel Hilbert space (RKHS) and its spectra. We\npropose an estimation method, what we call Jet Dynamic Mode Decomposition\n(JetDMD), leveraging the intrinsic structure of RKHS and the geometric notion\nknown as jets to enhance the estimation of the Koopman operator. This method\nrefines the traditional Extended Dynamic Mode Decomposition (EDMD) in accuracy,\nespecially in the numerical estimation of eigenvalues. This paper proves\nJetDMD's superiority through explicit error bounds and convergence rate for\nspecial positive definite kernels, offering a solid theoretical foundation for\nits performance. We also delve into the spectral analysis of the Koopman\noperator, proposing the notion of extended Koopman operator within a framework\nof rigged Hilbert space. This notion leads to a deeper understanding of\nestimated Koopman eigenfunctions and capturing them outside the original\nfunction space. Through the theory of rigged Hilbert space, our study provides\na principled methodology to analyze the estimated spectrum and eigenfunctions\nof Koopman operators, and enables eigendecomposition within a rigged RKHS. We\nalso propose a new effective method for reconstructing the dynamical system\nfrom temporally-sampled trajectory data of the dynamical system with solid\ntheoretical guarantee. We conduct several numerical simulations using the van\nder Pol oscillator, the Duffing oscillator, the H\\'enon map, and the Lorenz\nattractor, and illustrate the performance of JetDMD with clear numerical\ncomputations of eigenvalues and accurate predictions of the dynamical systems.\n"", ""  Data-driven approximations of the Koopman operator are promising for\npredicting the time evolution of systems characterized by complex dynamics.\nAmong these methods, the approach known as extended dynamic mode decomposition\nwith dictionary learning (EDMD-DL) has garnered significant attention. Here we\npresent a modification of EDMD-DL that concurrently determines both the\ndictionary of observables and the corresponding approximation of the Koopman\noperator. This innovation leverages automatic differentiation to facilitate\ngradient descent computations through the pseudoinverse. We also address the\nperformance of several alternative methodologies. We assess a 'pure' Koopman\napproach, which involves the direct time-integration of a linear,\nhigh-dimensional system governing the dynamics within the space of observables.\nAdditionally, we explore a modified approach where the system alternates\nbetween spaces of states and observables at each time step -- this approach no\nlonger satisfies the linearity of the true Koopman operator representation. For\nfurther comparisons, we also apply a state space approach (neural ODEs). We\nconsider systems encompassing two and three-dimensional ordinary differential\nequation systems featuring steady, oscillatory, and chaotic attractors, as well\nas partial differential equations exhibiting increasingly complex and intricate\nbehaviors. Our framework significantly outperforms EDMD-DL. Furthermore, the\nstate space approach offers superior performance compared to the 'pure' Koopman\napproach where the entire time evolution occurs in the space of observables.\nWhen the temporal evolution of the Koopman approach alternates between states\nand observables at each time step, however, its predictions become comparable\nto those of the state space approach.\n"", '  The Koopman operator provides a linear perspective on non-linear dynamics by\nfocusing on the evolution of observables in an invariant subspace. Observables\nof interest are typically linearly reconstructed from the Koopman\neigenfunctions. Despite the broad use of Koopman operators over the past few\nyears, there exist some misconceptions about the applicability of Koopman\noperators to dynamical systems with more than one disjoint invariant sets\n(e.g., basins of attractions from isolated fixed points). In this work, we\nfirst provide a simple explanation for the mechanism of linear\nreconstruction-based Koopman operators of nonlinear systems with multiple\ndisjoint invariant sets. Next, we discuss the use of discrete symmetry among\nsuch invariant sets to construct Koopman eigenfunctions in a data efficient\nmanner. Finally, several numerical examples are provided to illustrate the\nbenefits of exploiting symmetry for learning the Koopman operator.\n']",Koopman Operators and Eigenfunctions in Dynamical Systems
453,452,14,452_maintenance_prognostics_prognostic_ai,"['maintenance', 'prognostics', 'prognostic', 'ai', 'prediction', 'predict', 'predictive', 'forecast', 'degradation', 'reliable']","['maintenance', 'failure', 'prognostic', 'prognostics', 'industrial', 'degradation', 'failures', 'predictive', 'turbofan', 'equipment']","['maintenance', 'prognostics', 'ai', 'forecast', 'lstm', 'globalized', 'roadmaps', 'xpm', 'deepfmea', 'sustainably']","['  The landscape of maintenance in distributed systems is rapidly evolving with\nthe integration of Artificial Intelligence (AI). Also, as the complexity of\ncomputing continuum systems intensifies, the role of AI in predictive\nmaintenance (Pd.M.) becomes increasingly pivotal. This paper presents a\ncomprehensive survey of the current state of Pd.M. in the computing continuum,\nwith a focus on the combination of scalable AI technologies. Recognizing the\nlimitations of traditional maintenance practices in the face of increasingly\ncomplex and heterogenous computing continuum systems, the study explores how\nAI, especially machine learning and neural networks, is being used to enhance\nPd.M. strategies. The survey encompasses a thorough review of existing\nliterature, highlighting key advancements, methodologies, and case studies in\nthe field. It critically examines the role of AI in improving prediction\naccuracy for system failures and in optimizing maintenance schedules, thereby\ncontributing to reduced downtime and enhanced system longevity. By synthesizing\nfindings from the latest advancements in the field, the article provides\ninsights into the effectiveness and challenges of implementing AI-driven\npredictive maintenance. It underscores the evolution of maintenance practices\nin response to technological advancements and the growing complexity of\ncomputing continuum systems. The conclusions drawn from this survey are\ninstrumental for practitioners and researchers in understanding the current\nlandscape and future directions of Pd.M. in distributed systems. It emphasizes\nthe need for continued research and development in this area, pointing towards\na trend of more intelligent, efficient, and cost-effective maintenance\nsolutions in the era of AI.\n', ""  Machine Learning (ML) based prognostics and health monitoring (PHM) tools\nprovide new opportunities for manufacturers to operate and maintain their\nequipment in a risk-optimized manner and utilize it more sustainably along its\nlifecycle. Yet, in most industrial settings, data is often limited in quantity,\nand its quality can be inconsistent - both critical for developing and\noperating reliable ML models. To bridge this gap in practice, successfully\nindustrialized PHM tools rely on the introduction of domain expertise as a\nprior, to enable sufficiently accurate predictions, while enhancing their\ninterpretability.\n  Thus, a key challenge while developing data-driven PHM tools involves\ntranslating the experience and process knowledge of maintenance personnel,\ndevelopment, and service engineers into a data structure. This structure must\nnot only capture the diversity and variability of the expertise but also render\nthis knowledge accessible for various data-driven algorithms. This results in\ndata models that are heavily tailored towards a specific application and the\nfailure modes the development team aims to detect or predict. The lack of a\nstandardized approach limits developments' extensibility to new failure modes,\ntheir transferability to new applications, and it inhibits the utilization of\nstandard data management and MLOps tools, increasing the burden on the\ndevelopment team.\n  DeepFMEA draws inspiration from the Failure Mode and Effects Analysis (FMEA)\nin its structured approach to the analysis of any technical system and the\nresulting standardized data model, while considering aspects that are crucial\nto capturing process and maintenance expertise in a way that is both intuitive\nto domain experts and the resulting information can be introduced as priors to\nML algorithms.\n"", '  Industrial Cyber-Physical Systems (ICPS) integrate the disciplines of\ncomputer science, communication technology, and engineering, and have emerged\nas integral components of contemporary manufacturing and industries. However,\nICPS encounters various challenges in long-term operation, including equipment\nfailures, performance degradation, and security threats. To achieve efficient\nmaintenance and management, prognostics and health management (PHM) finds\nwidespread application in ICPS for critical tasks, including failure\nprediction, health monitoring, and maintenance decision-making. The emergence\nof large-scale foundation models (LFMs) like BERT and GPT signifies a\nsignificant advancement in AI technology, and ChatGPT stands as a remarkable\naccomplishment within this research paradigm, harboring potential for General\nArtificial Intelligence. Considering the ongoing enhancement in data\nacquisition technology and data processing capability, LFMs are anticipated to\nassume a crucial role in the PHM domain of ICPS. However, at present, a\nconsensus is lacking regarding the application of LFMs to PHM in ICPS,\nnecessitating systematic reviews and roadmaps to elucidate future directions.\nTo bridge this gap, this paper elucidates the key components and recent\nadvances in the underlying model.A comprehensive examination and comprehension\nof the latest advances in grand modeling for PHM in ICPS can offer valuable\nreferences for decision makers and researchers in the industrial field while\nfacilitating further enhancements in the reliability, availability, and safety\nof ICPS.\n']",Predictive Maintenance using Artificial Intelligence
454,453,14,453_modality_entities_entity_multimodal,"['modality', 'entities', 'entity', 'multimodal', 'modal', 'commonality', 'semantic', 'relational', 'semantics', 'knowledge']","['modal', 'entity', 'modality', 'multi', 'entities', 'relational', 'modalities', 'fusion', 'alignment', 'triples']","['modality', 'entities', 'multimodal', 'semantic', 'knowledge', 'hypergraph', 'adamf', 'encoders', 'bilingual', 'mm4kg']","['  Multi-modal entity alignment (MMEA) aims to identify equivalent entity pairs\nacross different multi-modal knowledge graphs (MMKGs). Existing approaches\nfocus on how to better encode and aggregate information from different\nmodalities. However, it is not trivial to leverage multi-modal knowledge in\nentity alignment due to the modal heterogeneity. In this paper, we propose a\nMulti-Grained Interaction framework for Multi-Modal Entity Alignment (MIMEA),\nwhich effectively realizes multi-granular interaction within the same modality\nor between different modalities. MIMEA is composed of four modules: i) a\nMulti-modal Knowledge Embedding module, which extracts modality-specific\nrepresentations with multiple individual encoders; ii) a Probability-guided\nModal Fusion module, which employs a probability guided approach to integrate\nuni-modal representations into joint-modal embeddings, while considering the\ninteraction between uni-modal representations; iii) an Optimal Transport Modal\nAlignment module, which introduces an optimal transport mechanism to encourage\nthe interaction between uni-modal and joint-modal embeddings; iv) a\nModal-adaptive Contrastive Learning module, which distinguishes the embeddings\nof equivalent entities from those of non-equivalent ones, for each modality.\nExtensive experiments conducted on two real-world datasets demonstrate the\nstrong performance of MIMEA compared to the SoTA. Datasets and code have been\nsubmitted as supplementary materials.\n', '  Multi-modal knowledge graphs (MMKG) store structured world knowledge\ncontaining rich multi-modal descriptive information. To overcome their inherent\nincompleteness, multi-modal knowledge graph completion (MMKGC) aims to discover\nunobserved knowledge from given MMKGs, leveraging both structural information\nfrom the triples and multi-modal information of the entities. Existing MMKGC\nmethods usually extract multi-modal features with pre-trained models and employ\na fusion module to integrate multi-modal features with triple prediction.\nHowever, this often results in a coarse handling of multi-modal data,\noverlooking the nuanced, fine-grained semantic details and their interactions.\nTo tackle this shortfall, we introduce a novel framework MyGO to process, fuse,\nand augment the fine-grained modality information from MMKGs. MyGO tokenizes\nmulti-modal raw data as fine-grained discrete tokens and learns entity\nrepresentations with a cross-modal entity encoder. To further augment the\nmulti-modal representations, MyGO incorporates fine-grained contrastive\nlearning to highlight the specificity of the entity representations.\nExperiments on standard MMKGC benchmarks reveal that our method surpasses 20 of\nthe latest models, underlining its superior performance. Code and data are\navailable at https://github.com/zjukg/MyGO\n', '  Multi-modal entity alignment (MMEA) aims to identify equivalent entities\nbetween multi-modal knowledge graphs (MMKGs), where the entities can be\nassociated with related images. Most existing studies integrate multi-modal\ninformation heavily relying on the automatically-learned fusion module, rarely\nsuppressing the redundant information for MMEA explicitly. To this end, we\nexplore variational information bottleneck for multi-modal entity alignment\n(IBMEA), which emphasizes the alignment-relevant information and suppresses the\nalignment-irrelevant information in generating entity representations.\nSpecifically, we devise multi-modal variational encoders to generate\nmodal-specific entity representations as probability distributions. Then, we\npropose four modal-specific information bottleneck regularizers, limiting the\nmisleading clues in refining modal-specific entity representations. Finally, we\npropose a modal-hybrid information contrastive regularizer to integrate all the\nrefined modal-specific representations, enhancing the entity similarity between\nMMKGs to achieve MMEA. We conduct extensive experiments on two cross-KG and\nthree bilingual MMEA datasets. Experimental results demonstrate that our model\nconsistently outperforms previous state-of-the-art methods, and also shows\npromising and robust performance in low-resource and high-noise data scenarios.\n']",Multi-Modal Entity Alignment and Knowledge Graphs
455,454,14,454_gerrymandering_redistricting_gerrymanderer_candidates,"['gerrymandering', 'redistricting', 'gerrymanderer', 'candidates', 'elections', 'voting', 'candidate', 'electoral', 'ensemble', 'ranking']","['voting', 'redistricting', 'ordinal', 'candidates', 'gerrymanderer', 'voters', 'planar', 'manipulation', 'gerrymandering', 'partisan']","['gerrymandering', 'candidates', 'ensemble', 'delegation', 'sortition', 'utility', 'district', 'proportional', 'volunteer', 'mining']","['  By classic results in social choice theory, any reasonable preferential\nvoting method sometimes gives individuals an incentive to report an insincere\npreference. The extent to which different voting methods are more or less\nresistant to such strategic manipulation has become a key consideration for\ncomparing voting methods. Here we measure resistance to manipulation by whether\nneural networks of varying sizes can learn to profitably manipulate a given\nvoting method in expectation, given different types of limited information\nabout how other voters will vote. We trained over 70,000 neural networks of 26\nsizes to manipulate against 8 different voting methods, under 6 types of\nlimited information, in committee-sized elections with 5-21 voters and 3-6\ncandidates. We find that some voting methods, such as Borda, are highly\nmanipulable by networks with limited information, while others, such as Instant\nRunoff, are not, despite being quite profitably manipulated by an ideal\nmanipulator with full information. For the two probability models for elections\nthat we use, the overall least manipulable of the 8 methods we study are\nCondorcet methods, namely Minimax and Split Cycle.\n', ""  Role mining is a technique used to derive a role-based authorization policy\nfrom an existing policy. Given a set of users $U$, a set of permissions $P$ and\na user-permission authorization relation $\\mahtit{UPA}\\subseteq U\\times P$, a\nrole mining algorithm seeks to compute a set of roles $R$, a user-role\nauthorization relation $\\mathit{UA}\\subseteq U\\times R$ and a permission-role\nauthorization relation $\\mathit{PA}\\subseteq R\\times P$, such that the\ncomposition of $\\mathit{UA}$ and $\\mathit{PA}$ is close (in some appropriate\nsense) to $\\mathit{UPA}$.\n  In this paper, we first introduce the Generalized Noise Role Mining problem\n(GNRM) -- a generalization of the MinNoise Role Mining problem -- which we\nbelieve has considerable practical relevance. Extending work of Fomin et al.,\nwe show that GNRM is fixed parameter tractable, with parameter $r + k$, where\n$r$ is the number of roles in the solution and $k$ is the number of\ndiscrepancies between $\\mathit{UPA}$ and the relation defined by the\ncomposition of $\\mathit{UA}$ and $\\mathit{PA}$. We further introduce a\nbi-objective optimization variant of GNRM, where we wish to minimize both $r$\nand $k$ subject to upper bounds $r\\le \\bar{r}$ and $k\\le \\bar{k}$, where\n$\\bar{r}$ and $\\bar{k}$ are constants. We show that the Pareto front of this\nbi-objective optimization problem (BO-GNRM) can be computed in fixed-parameter\ntractable time with parameter $\\bar{r}+\\bar{k}$.\n  We then report the results of our experimental work using the integer\nprogramming solver Gurobi to solve instances of BO-GNRM. Our key findings are\nthat (a) we obtained strong support that Gurobi's performance is\nfixed-parameter tractable, (b) our results suggest that our techniques may be\nuseful for role mining in practice, based on our experiments in the context of\nthree well-known real-world authorization policies.\n"", '  We study the computational complexity of the map redistricting problem\n(gerrymandering). Mathematically, the electoral district designer\n(gerrymanderer) attempts to partition a weighted graph into $k$ connected\ncomponents (districts) such that its candidate (party) wins as many districts\nas possible. Prior work has principally concerned the special cases where the\ngraph is a path or a tree. Our focus concerns the realistic case where the\ngraph is planar. We prove that the gerrymandering problem is solvable in\npolynomial time in $\\lambda$-outerplanar graphs, when the number of candidates\nand $\\lambda$ are constants and the vertex weights (voting weights) are\npolynomially bounded. In contrast, the problem is NP-complete in general planar\ngraphs even with just two candidates. This motivates the study of approximation\nalgorithms for gerrymandering planar graphs. However, when the number of\ncandidates is large, we prove it is hard to distinguish between instances where\nthe gerrymanderer cannot win a single district and instances where the\ngerrymanderer can win at least one district. This immediately implies that the\nredistricting problem is inapproximable in polynomial time in planar graphs,\nunless P=NP. This conclusion appears terminal for the design of good\napproximation algorithms -- but it is not. The inapproximability bound can be\ncircumvented as it only applies when the maximum number of districts the\ngerrymanderer can win is extremely small, say one. Indeed, for a fixed number\nof candidates, our main result is that there is a constant factor approximation\nalgorithm for redistricting unweighted planar graphs, provided the optimal\nvalue is a large enough constant.\n']",Election Manipulation and Redistricting
456,455,14,455_linguistic_semantics_linguists_languaging,"['linguistic', 'semantics', 'linguists', 'languaging', 'lexical', 'language', 'sentences', 'cognitive', 'interlingual', 'words']","['grammatical', 'pragmatic', 'inferences', 'supradiegetic', 'linguists', 'words', 'word', 'linguistic', 'laypeople', 'cognitive']","['linguistic', 'languaging', 'prototypicality', 'nativelike', 'implicature', 'chatgpt', 'psychometric', 'nlss', 'palindromes', 'human']","['  Large Language Models (LLMs) like ChatGPT reflect profound changes in the\nfield of Artificial Intelligence, achieving a linguistic fluency that is\nimpressively, even shockingly, human-like. The extent of their current and\npotential capabilities is an active area of investigation by no means limited\nto scientific researchers. It is common for people to frame the training data\nfor LLMs as ""text"" or even ""language"". We examine the details of this framing\nusing ideas from several areas, including linguistics, embodied cognition,\ncognitive science, mathematics, and history. We propose that considering what\nit is like to be an LLM like ChatGPT, as Nagel might have put it, can help us\ngain insight into its capabilities in general, and in particular, that its\nexposure to linguistic training data can be productively reframed as exposure\nto the diegetic information encoded in language, and its deficits can be\nreframed as ignorance of extradiegetic information, including supradiegetic\nlinguistic information. Supradiegetic linguistic information consists of those\narbitrary aspects of the physical form of language that are not derivable from\nthe one-dimensional relations of context -- frequency, adjacency, proximity,\nco-occurrence -- that LLMs like ChatGPT have access to. Roughly speaking, the\ndiegetic portion of a word can be thought of as its function, its meaning, as\nthe information in a theoretical vector in a word embedding, while the\nsupradiegetic portion of the word can be thought of as its form, like the\nshapes of its letters or the sounds of its syllables. We use these concepts to\ninvestigate why LLMs like ChatGPT have trouble handling palindromes, the visual\ncharacteristics of symbols, translating Sumerian cuneiform, and continuing\ninteger sequences.\n', '  In native speakers\' lexical choices, a concept can be more readily expressed\nby one expression over another grammatical one, a phenomenon known as\nnativelike selection (NLS). In previous research, arbitrary chunks such as\ncollocations have been considered crucial for this phenomenon. However, this\nstudy examines the possibility of analyzing the semantic motivation and\ndeducibility behind some NLSs by exploring the correlation between NLS and\nprototypicality, specifically the onomasiological hypothesis of Grondelaers and\nGeeraerts (2003, Towards a pragmatic model of cognitive onomasiology. In Hubert\nCuyckens, Ren\\\'e Dirven & John R. Taylor (eds.), Cognitive approaches to\nlexical semantics, 67-92. Berlin: De Gruyter Mouton). They hypothesized that\n""[a] referent is more readily named by a lexical item if it is a salient member\nof the category denoted by that item"". To provide a preliminary investigation\nof this important but rarely explored phenomenon, a series of innovative\nmethods and procedures, including the use of semantic embedding and\ninterlingual comparisons, is designed. Specifically, potential NLSs are\nefficiently discovered through an automatic exploratory analysis using topic\nmodeling techniques, and then confirmed by manual inspection through frame\nsemantics. Finally, to account for the NLS in question, cluster analysis and\nbehavioral profile analysis are conducted to uncover a language-specific\nprototype for the Chinese verb shang \'harm\', providing supporting evidence for\nthe correlation between NLS and prototypicality.\n', ""  Large language models (LLMs) have demonstrated exceptional performance across\nvarious linguistic tasks. However, it remains uncertain whether LLMs have\ndeveloped human-like fine-grained grammatical intuition. This preregistered\nstudy (https://osf.io/t5nes) presents the first large-scale investigation of\nChatGPT's grammatical intuition, building upon a previous study that collected\nlaypeople's grammatical judgments on 148 linguistic phenomena that linguists\njudged to be grammatical, ungrammatical, or marginally grammatical (Sprouse,\nSchutze, & Almeida, 2013). Our primary focus was to compare ChatGPT with both\nlaypeople and linguists in the judgement of these linguistic constructions. In\nExperiment 1, ChatGPT assigned ratings to sentences based on a given reference\nsentence. Experiment 2 involved rating sentences on a 7-point scale, and\nExperiment 3 asked ChatGPT to choose the more grammatical sentence from a pair.\nOverall, our findings demonstrate convergence rates ranging from 73% to 95%\nbetween ChatGPT and linguists, with an overall point-estimate of 89%.\nSignificant correlations were also found between ChatGPT and laypeople across\nall tasks, though the correlation strength varied by task. We attribute these\nresults to the psychometric nature of the judgment tasks and the differences in\nlanguage processing styles between humans and LLMs.\n""]",Linguistic Semantics and Language Models
457,456,14,456_gans_generative_gan_autoencoders,"['gans', 'generative', 'gan', 'autoencoders', 'adversarial', 'data', 'ehrs', 'healthcare', 'ehr', 'generate']","['synthetic', 'healthcare', 'records', 'electronic', 'generative', 'health', 'clinical', 'underrepresented', 'generation', 'visits']","['gans', 'autoencoders', 'healthcare', 'ehr', 'generate', 'pddpm', 'transcripts', 'privacy', 'mimic', 'diffusion']","['  Synthetic data generation offers a promising solution to enhance the\nusefulness of Electronic Healthcare Records (EHR) by generating realistic\nde-identified data. However, the existing literature primarily focuses on the\nquality of synthetic health data, neglecting the crucial aspect of fairness in\ndownstream predictions. Consequently, models trained on synthetic EHR have\nfaced criticism for producing biased outcomes in target tasks. These biases can\narise from either spurious correlations between features or the failure of\nmodels to accurately represent sub-groups. To address these concerns, we\npresent Bias-transforming Generative Adversarial Networks (Bt-GAN), a GAN-based\nsynthetic data generator specifically designed for the healthcare domain. In\norder to tackle spurious correlations (i), we propose an\ninformation-constrained Data Generation Process that enables the generator to\nlearn a fair deterministic transformation based on a well-defined notion of\nalgorithmic fairness. To overcome the challenge of capturing exact sub-group\nrepresentations (ii), we incentivize the generator to preserve sub-group\ndensities through score-based weighted sampling. This approach compels the\ngenerator to learn from underrepresented regions of the data manifold. We\nconduct extensive experiments using the MIMIC-III database. Our results\ndemonstrate that Bt-GAN achieves SOTA accuracy while significantly improving\nfairness and minimizing bias amplification. We also perform an in-depth\nexplainability analysis to provide additional evidence supporting the validity\nof our study. In conclusion, our research introduces a novel and professional\napproach to addressing the limitations of synthetic data generation in the\nhealthcare domain. By incorporating fairness considerations and leveraging\nadvanced techniques such as GANs, we pave the way for more reliable and\nunbiased predictions in healthcare applications.\n', '  Electronic health records (EHRs) are a pivotal data source that enables\nnumerous applications in computational medicine, e.g., disease progression\nprediction, clinical trial design, and health economics and outcomes research.\nDespite wide usability, their sensitive nature raises privacy and\nconfidentially concerns, which limit potential use cases. To tackle these\nchallenges, we explore the use of generative models to synthesize artificial,\nyet realistic EHRs. While diffusion-based methods have recently demonstrated\nstate-of-the-art performance in generating other data modalities and overcome\nthe training instability and mode collapse issues that plague previous\nGAN-based approaches, their applications in EHR generation remain\nunderexplored. The discrete nature of tabular medical code data in EHRs poses\nchallenges for high-quality data generation, especially for continuous\ndiffusion models. To this end, we introduce a novel tabular EHR generation\nmethod, EHR-D3PM, which enables both unconditional and conditional generation\nusing the discrete diffusion model. Our experiments demonstrate that EHR-D3PM\nsignificantly outperforms existing generative baselines on comprehensive\nfidelity and utility metrics while maintaining less attribute and membership\nvulnerability risks. Furthermore, we show EHR-D3PM is effective as a data\naugmentation method and enhances performance on downstream tasks when combined\nwith real data.\n', '  Electronic health records (EHR) contain a wealth of biomedical information,\nserving as valuable resources for the development of precision medicine\nsystems. However, privacy concerns have resulted in limited access to\nhigh-quality and large-scale EHR data for researchers, impeding progress in\nmethodological development. Recent research has delved into synthesizing\nrealistic EHR data through generative modeling techniques, where a majority of\nproposed methods relied on generative adversarial networks (GAN) and their\nvariants for EHR synthesis. Despite GAN-based methods attaining\nstate-of-the-art performance in generating EHR data, these approaches are\ndifficult to train and prone to mode collapse. Recently introduced in\ngenerative modeling, diffusion models have established cutting-edge performance\nin image generation, but their efficacy in EHR data synthesis remains largely\nunexplored. In this study, we investigate the potential of diffusion models for\nEHR data synthesis and introduce a novel method, EHRDiff. Through extensive\nexperiments, EHRDiff establishes new state-of-the-art quality for synthetic EHR\ndata, protecting private information in the meanwhile.\n']",Generative Models for Synthetic Electronic Health Records
458,457,14,457_backdoors_adversarial_backdoor_exploitable,"['backdoors', 'adversarial', 'backdoor', 'exploitable', 'triggers', 'vulnerabilities', 'attacker', 'backdoored', 'attacks', 'threats']","['backdoor', 'triggers', 'trigger', 'poisoned', 'poisoning', 'attacks', 'defense', 'backdoors', 'attack', 'deceptive']","['backdoors', 'adversarial', 'vulnerabilities', 'trigger', 'textual', 'defenses', 'injections', 'trojan', 'integrity', 'tokens']","['  Recently, various parameter-efficient fine-tuning (PEFT) strategies for\napplication to language models have been proposed and successfully implemented.\nHowever, this raises the question of whether PEFT, which only updates a limited\nset of model parameters, constitutes security vulnerabilities when confronted\nwith weight-poisoning backdoor attacks. In this study, we show that PEFT is\nmore susceptible to weight-poisoning backdoor attacks compared to the\nfull-parameter fine-tuning method, with pre-defined triggers remaining\nexploitable and pre-defined targets maintaining high confidence, even after\nfine-tuning. Motivated by this insight, we developed a Poisoned Sample\nIdentification Module (PSIM) leveraging PEFT, which identifies poisoned samples\nthrough confidence, providing robust defense against weight-poisoning backdoor\nattacks. Specifically, we leverage PEFT to train the PSIM with randomly reset\nsample labels. During the inference process, extreme confidence serves as an\nindicator for poisoned samples, while others are clean. We conduct experiments\non text classification tasks, five fine-tuning strategies, and three\nweight-poisoning backdoor attack methods. Experiments show near 100% success\nrates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore,\nour defensive approach exhibits overall competitive performance in mitigating\nweight-poisoning backdoor attacks.\n', '  Language models are often at risk of diverse backdoor attacks, especially\ndata poisoning. Thus, it is important to investigate defense solutions for\naddressing them. Existing backdoor defense methods mainly focus on backdoor\nattacks with explicit triggers, leaving a universal defense against various\nbackdoor attacks with diverse triggers largely unexplored. In this paper, we\npropose an end-to-end ensemble-based backdoor defense framework, DPoE (Denoised\nProduct-of-Experts), which is inspired by the shortcut nature of backdoor\nattacks, to defend various backdoor attacks. DPoE consists of two models: a\nshallow model that captures the backdoor shortcuts and a main model that is\nprevented from learning the backdoor shortcuts. To address the label flip\ncaused by backdoor attackers, DPoE incorporates a denoising design. Experiments\non SST-2 dataset show that DPoE significantly improves the defense performance\nagainst various types of backdoor triggers including word-level,\nsentence-level, and syntactic triggers. Furthermore, DPoE is also effective\nunder a more challenging but practical setting that mixes multiple types of\ntrigger.\n', ""  The prompt-based learning paradigm, which bridges the gap between\npre-training and fine-tuning, achieves state-of-the-art performance on several\nNLP tasks, particularly in few-shot settings. Despite being widely applied,\nprompt-based learning is vulnerable to backdoor attacks. Textual backdoor\nattacks are designed to introduce targeted vulnerabilities into models by\npoisoning a subset of training samples through trigger injection and label\nmodification. However, they suffer from flaws such as abnormal natural language\nexpressions resulting from the trigger and incorrect labeling of poisoned\nsamples. In this study, we propose ProAttack, a novel and efficient method for\nperforming clean-label backdoor attacks based on the prompt, which uses the\nprompt itself as a trigger. Our method does not require external triggers and\nensures correct labeling of poisoned samples, improving the stealthy nature of\nthe backdoor attack. With extensive experiments on rich-resource and few-shot\ntext classification tasks, we empirically validate ProAttack's competitive\nperformance in textual backdoor attacks. Notably, in the rich-resource setting,\nProAttack achieves state-of-the-art attack success rates in the clean-label\nbackdoor attack benchmark without external triggers.\n""]",Backdoor Attacks on Language Models
459,458,14,458_robot_robots_planning_autonomous,"['robot', 'robots', 'planning', 'autonomous', 'robotic', 'pedestrians', 'agent', 'navigate', 'reinforcement', 'navigation']","['navigation', 'robot', 'crowd', 'crowded', 'social', 'pedestrians', 'planner', 'norms', 'pedestrian', 'robots']","['robot', 'pedestrians', 'reinforcement', 'navigating', 'planner', 'obstacle', 'autonomy', 'wheelchairs', 'mobility', 'simulated']","[""  Robots navigating in crowded areas should negotiate free space with humans\nrather than fully controlling collision avoidance, as this can lead to freezing\nbehavior. Game theory provides a framework for the robot to reason about\npotential cooperation from humans for collision avoidance during path planning.\nIn particular, the mixed strategy Nash equilibrium captures the negotiation\nbehavior under uncertainty, making it well suited for crowd navigation.\nHowever, computing the mixed strategy Nash equilibrium is often prohibitively\nexpensive for real-time decision-making. In this paper, we propose an iterative\nBayesian update scheme over probability distributions of trajectories. The\nalgorithm simultaneously generates a stochastic plan for the robot and\nprobabilistic predictions of other pedestrians' paths. We prove that the\nproposed algorithm is equivalent to solving a mixed strategy game for crowd\nnavigation, and the algorithm guarantees the recovery of the global Nash\nequilibrium of the game. We name our algorithm Bayes' Rule Nash Equilibrium\n(BRNE) and develop a real-time model prediction crowd navigation framework.\nSince BRNE is not solving a general-purpose mixed strategy Nash equilibrium but\na tailored formula specifically for crowd navigation, it can compute the\nsolution in real-time on a low-power embedded computer. We evaluate BRNE in\nboth simulated environments and real-world pedestrian datasets. BRNE\nconsistently outperforms non-learning and learning-based methods regarding\nsafety and navigation efficiency. It also reaches human-level crowd navigation\nperformance in the pedestrian dataset benchmark. Lastly, we demonstrate the\npracticality of our algorithm with real humans on an untethered quadruped robot\nwith fully onboard perception and computation.\n"", '  Learning robot navigation strategies among pedestrian is crucial for domain\nbased applications. Combining perception, planning and prediction allows us to\nmodel the interactions between robots and pedestrians, resulting in impressive\noutcomes especially with recent approaches based on deep reinforcement learning\n(RL). However, these works do not consider multi-robot scenarios. In this\npaper, we present MultiSoc, a new method for learning multi-agent socially\naware navigation strategies using RL. Inspired by recent works on multi-agent\ndeep RL, our method leverages graph-based representation of agent interactions,\ncombining the positions and fields of view of entities (pedestrians and\nagents). Each agent uses a model based on two Graph Neural Network combined\nwith attention mechanisms. First an edge-selector produces a sparse graph, then\na crowd coordinator applies node attention to produce a graph representing the\ninfluence of each entity on the others. This is incorporated into a model-free\nRL framework to learn multi-agent policies. We evaluate our approach on\nsimulation and provide a series of experiments in a set of various conditions\n(number of agents / pedestrians). Empirical results show that our method learns\nfaster than social navigation deep RL mono-agent techniques, and enables\nefficient multi-agent implicit coordination in challenging crowd navigation\nwith multiple heterogeneous humans. Furthermore, by incorporating customizable\nmeta-parameters, we can adjust the neighborhood density to take into account in\nour navigation strategy.\n', ""  Mobile robots are being used on a large scale in various crowded situations\nand become part of our society. The socially acceptable navigation behavior of\na mobile robot with individual human consideration is an essential requirement\nfor scalable applications and human acceptance. Deep Reinforcement Learning\n(DRL) approaches are recently used to learn a robot's navigation policy and to\nmodel the complex interactions between robots and humans. We propose to divide\nexisting DRL-based navigation approaches based on the robot's exhibited social\nbehavior and distinguish between social collision avoidance with a lack of\nsocial behavior and socially aware approaches with explicit predefined social\nbehavior. In addition, we propose a novel socially integrated navigation\napproach where the robot's social behavior is adaptive and emerges from the\ninteraction with humans. The formulation of our approach is derived from a\nsociological definition, which states that social acting is oriented toward the\nacting of others. The DRL policy is trained in an environment where other\nagents interact socially integrated and reward the robot's behavior\nindividually. The simulation results indicate that the proposed socially\nintegrated navigation approach outperforms a socially aware approach in terms\nof ego navigation performance while significantly reducing the negative impact\non all agents within the environment.\n""]",Robot Navigation in Crowded Areas
460,459,14,459_dysarthric_dysarthria_speech_speaker,"['dysarthric', 'dysarthria', 'speech', 'speaker', 'wav2vec2', 'impaired', 'lstm', 'dsr', 'transcription', 'recognition']","['dysarthric', 'dysarthria', 'speech', 'speaker', 'intelligibility', 'elderly', 'recognition', 'adaptation', 'impaired', 'disfluency']","['dysarthric', 'wav2vec2', 'impaired', 'dsr', 'transcription', 'recognition', 'dementiabank', 'gan', 'neuromotor', 'ned']","['  Despite the rapid progress of automatic speech recognition (ASR) technologies\ntargeting normal speech, accurate recognition of dysarthric and elderly speech\nremains highly challenging tasks to date. It is difficult to collect large\nquantities of such data for ASR system development due to the mobility issues\noften found among these users. To this end, data augmentation techniques play a\nvital role. In contrast to existing data augmentation techniques only modifying\nthe speaking rate or overall shape of spectral contour, fine-grained\nspectro-temporal differences between dysarthric, elderly and normal speech are\nmodelled using a novel set of speaker dependent (SD) generative adversarial\nnetworks (GAN) based data augmentation approaches in this paper. These flexibly\nallow both: a) temporal or speed perturbed normal speech spectra to be modified\nand closer to those of an impaired speaker when parallel speech data is\navailable; and b) for non-parallel data, the SVD decomposed normal speech\nspectral basis features to be transformed into those of a target elderly\nspeaker before being re-composed with the temporal bases to produce the\naugmented data for state-of-the-art TDNN and Conformer ASR system training.\nExperiments are conducted on four tasks: the English UASpeech and TORGO\ndysarthric speech corpora; the English DementiaBank Pitt and Cantonese JCCOCC\nMoCA elderly speech datasets. The proposed GAN based data augmentation\napproaches consistently outperform the baseline speed perturbation method by up\nto 0.91% and 3.0% absolute (9.61% and 6.4% relative) WER reduction on the TORGO\nand DementiaBank data respectively. Consistent performance improvements are\nretained after applying LHUC based speaker adaptation.\n', '  Automating dysarthria assessments offers the opportunity to develop\npractical, low-cost tools that address the current limitations of manual and\nsubjective assessments. Nonetheless, the small size of most dysarthria datasets\nmakes it challenging to develop automated assessment. Recent research showed\nthat speech representations from models pre-trained on large unlabelled data\ncan enhance Automatic Speech Recognition (ASR) performance for dysarthric\nspeech. We are the first to evaluate the representations from pre-trained\nstate-of-the-art Self-Supervised models across three downstream tasks on\ndysarthric speech: disease classification, word recognition and intelligibility\nclassification, and under three noise scenarios on the UA-Speech dataset. We\nshow that HuBERT is the most versatile feature extractor across dysarthria\nclassification, word recognition, and intelligibility classification, achieving\nrespectively $+24.7\\%, +61\\%, \\text{and} +7.2\\%$ accuracy compared to classical\nacoustic features.\n', '  Disordered speech recognition profound implications for improving the quality\nof life for individuals afflicted with, for example, dysarthria. Dysarthric\nspeech recognition encounters challenges including limited data, substantial\ndissimilarities between dysarthric and non-dysarthric speakers, and significant\nspeaker variations stemming from the disorder. This paper introduces\nPerceiver-Prompt, a method for speaker adaptation that utilizes P-Tuning on the\nWhisper large-scale model. We first fine-tune Whisper using LoRA and then\nintegrate a trainable Perceiver to generate fixed-length speaker prompts from\nvariable-length inputs, to improve model recognition of Chinese dysarthric\nspeech. Experimental results from our Chinese dysarthric speech dataset\ndemonstrate consistent improvements in recognition performance with\nPerceiver-Prompt. Relative reduction up to 13.04% in CER is obtained over the\nfine-tuned Whisper.\n']",Speech Recognition for Dysarthric Speakers
461,460,14,460_entities_entity_matching_matched,"['entities', 'entity', 'matching', 'matched', 'joins', 'records', 'match', 'record', 'matchers', 'linking']","['matching', 'entity', 'records', 'matchers', 'fuzzy', 'entities', 'prompt', 'match', 'sources', 'samples']","['entities', 'matching', 'joins', 'records', 'linking', 'libem', 'redundant', 'gralmatch', 'crms', 'chase']","['  Entity Matching is the task of deciding whether two entity descriptions refer\nto the same real-world entity and is a central step in most data integration\npipelines. Many state-of-the-art entity matching methods rely on pre-trained\nlanguage models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these\nmodels for entity matching are that (i) the models require significant amounts\nof task-specific training data and (ii) the fine-tuned models are not robust\nconcerning out-of-distribution entities. This paper investigates using\ngenerative large language models (LLMs) as a less task-specific training\ndata-dependent and more robust alternative to PLM-based matchers. Our study\ncovers hosted and open-source LLMs, which can be run locally. We evaluate these\nmodels in a zero-shot scenario and a scenario where task-specific training data\nis available. We compare different prompt designs and the prompt sensitivity of\nthe models and show that there is no single best prompt but needs to be tuned\nfor each model/dataset combination. We further investigate (i) the selection of\nin-context demonstrations, (ii) the generation of matching rules, as well as\n(iii) fine-tuning a hosted LLM using the same pool of training data. Our\nexperiments show that the best LLMs require no or only a few training examples\nto perform similarly to PLMs that were fine-tuned using thousands of examples.\nLLM-based matchers further exhibit higher robustness to unseen entities. We\nshow that GPT4 can generate structured explanations for matching decisions. The\nmodel can automatically identify potential causes of matching errors by\nanalyzing explanations of wrong decisions. We demonstrate that the model can\ngenerate meaningful textual descriptions of the identified error classes, which\ncan help data engineers improve entity matching pipelines.\n', '  In this paper, we present an end-to-end multi-source Entity Matching problem,\nwhich we call entity group matching, where the goal is to assign to the same\ngroup, records originating from multiple data sources but representing the same\nreal-world entity. We focus on the effects of transitively matched records,\ni.e. the records connected by paths in the graph G = (V,E) whose nodes and\nedges represent the records and whether they are a match or not. We present a\nreal-world instance of this problem, where the challenge is to match records of\ncompanies and financial securities originating from different data providers.\nWe also introduce two new multi-source benchmark datasets that present similar\nmatching challenges as real-world records. A distinctive characteristic of\nthese records is that they are regularly updated following real-world events,\nbut updates are not applied uniformly across data sources. This phenomenon\nmakes the matching of certain groups of records only possible through the use\nof transitive information.\n  In our experiments, we illustrate how considering transitively matched\nrecords is challenging since a limited amount of false positive pairwise match\npredictions can throw off the group assignment of large quantities of records.\nThus, we propose GraLMatch, a method that can partially detect and remove false\npositive pairwise predictions through graph-based properties. Finally, we\nshowcase how fine-tuning a Transformer-based model (DistilBERT) on a reduced\nnumber of labeled samples yields a better final entity group matching than\ntraining on more samples and/or incorporating fine-tuning optimizations,\nillustrating how precision becomes the deciding factor in the entity group\nmatching of large volumes of records.\n', '  Entity matching is the task of linking records from different sources that\nrefer to the same real-world entity. Past work has primarily treated entity\nlinking as a standard supervised learning problem. However, supervised entity\nmatching models often do not generalize well to new data, and collecting\nexhaustive labeled training data is often cost prohibitive. Further, recent\nefforts have adopted LLMs for this task in few/zero-shot settings, exploiting\ntheir general knowledge. But LLMs are prohibitively expensive for performing\ninference at scale for real-world entity matching tasks.\n  As an efficient alternative, we re-cast entity matching as a conditional\ngeneration task as opposed to binary classification. This enables us to\n""distill"" LLM reasoning into smaller entity matching models via natural\nlanguage explanations. This approach achieves strong performance, especially on\nout-of-domain generalization tests (10.85% F-1) where standalone generative\nmethods struggle. We perform ablations that highlight the importance of\nexplanations, both for performance and model robustness.\n']",Entity Matching and Linking
462,461,14,461_ctm_remote_sensing_change_detection_changes_changeanywhere_change,"['ctm_remote_sensing_change_detection', 'changes', 'changeanywhere', 'change', 'detection', 'features', 'sensing', 'imagery', 'mapchange', 'land']","['change', 'remote', 'sensing', 'land', 'temporal', 'detection', 'bi', 'changes', 'optical', 'cover']","['ctm_remote_sensing_change_detection', 'change', 'sensing', 'imagery', 'land', 'segmentation', 'decoder', 'dsifn', 'bda', 'cdd']","[""  Remote sensing change detection is crucial for understanding the dynamics of\nour planet's surface, facilitating the monitoring of environmental changes,\nevaluating human impact, predicting future trends, and supporting\ndecision-making. In this work, we introduce a novel approach for change\ndetection that can leverage off-the-shelf, unlabeled remote sensing images in\nthe training process by pre-training a Denoising Diffusion Probabilistic Model\n(DDPM) - a class of generative models used in image synthesis. DDPMs learn the\ntraining data distribution by gradually converting training images into a\nGaussian distribution using a Markov chain. During inference (i.e., sampling),\nthey can generate a diverse set of samples closer to the training distribution,\nstarting from Gaussian noise, achieving state-of-the-art image synthesis\nresults. However, in this work, our focus is not on image synthesis but on\nutilizing it as a pre-trained feature extractor for the downstream application\nof change detection. Specifically, we fine-tune a lightweight change classifier\nutilizing the feature representations produced by the pre-trained DDPM\nalongside change labels. Experiments conducted on the LEVIR-CD, WHU-CD,\nDSIFN-CD, and CDD datasets demonstrate that the proposed DDPM-CD method\nsignificantly outperforms the existing state-of-the-art change detection\nmethods in terms of F1 score, IoU, and overall accuracy, highlighting the\npivotal role of pre-trained DDPM as a feature extractor for downstream\napplications. We have made both the code and pre-trained models available at\nhttps://github.com/wgcban/ddpm-cd\n"", '  Remote sensing change detection (CD) is a pivotal technique that pinpoints\nchanges on a global scale based on multi-temporal images. With the recent\nexpansion of deep learning, supervised deep learning-based CD models have shown\nsatisfactory performance. However, CD sample labeling is very time-consuming as\nit is densely labeled and requires expert knowledge. To alleviate this problem,\nwe introduce ChangeAnywhere, a novel CD sample generation method using the\nsemantic latent diffusion model and single-temporal images. Specifically,\nChangeAnywhere leverages the relative ease of acquiring large single-temporal\nsemantic datasets to generate large-scale, diverse, and semantically annotated\nbi-temporal CD datasets. ChangeAnywhere captures the two essentials of CD\nsamples, i.e., change implies semantically different, and non-change implies\nreasonable change under the same semantic constraints. We generated\nChangeAnywhere-100K, the largest synthesis CD dataset with 100,000 pairs of CD\nsamples based on the proposed method. The ChangeAnywhere-100K significantly\nimproved both zero-shot and few-shot performance on two CD benchmark datasets\nfor various deep learning-based CD models, as demonstrated by transfer\nexperiments. This paper delineates the enormous potential of ChangeAnywhere for\nCD sample generation and demonstrates the subsequent enhancement of model\nperformance. Therefore, ChangeAnywhere offers a potent tool for remote sensing\nCD. All codes and pre-trained models will be available at\nhttps://github.com/tangkai-RS/ChangeAnywhere.\n', '  The existing change detection(CD) methods can be summarized as the\nvisual-first change detection (ViFi-CD) paradigm, which first extracts change\nfeatures from visual differences and then assigns them specific semantic\ninformation. However, CD is essentially dependent on change regions of interest\n(CRoIs), meaning that the CD results are directly determined by the semantics\nchanges of interest, making its primary image factor semantic of interest\nrather than visual. The ViFi-CD paradigm can only assign specific semantics of\ninterest to specific change features extracted from visual differences, leading\nto the inevitable omission of potential CRoIs and the inability to adapt to\ndifferent CRoI CD tasks. In other words, changes in other CRoIs cannot be\ndetected by the ViFi-CD method without retraining the model or significantly\nmodifying the method. This paper introduces a new CD paradigm, the\nsemantic-first CD (SeFi-CD) paradigm. The core idea of SeFi-CD is to first\nperceive the dynamic semantics of interest and then visually search for change\nfeatures related to the semantics. Based on the SeFi-CD paradigm, we designed\nAnything You Want Change Detection (AUWCD). Experiments on public datasets\ndemonstrate that the AUWCD outperforms the current state-of-the-art CD methods,\nachieving an average F1 score 5.01\\% higher than that of these advanced\nsupervised baselines on the SECOND dataset, with a maximum increase of 13.17\\%.\nThe proposed SeFi-CD offers a novel CD perspective and approach.\n']",Remote Sensing Change Detection
463,462,14,462_communities_social_networks_community,"['communities', 'social', 'networks', 'community', 'socially', 'influence', 'network', 'behaviors', 'behavior', 'communication']","['social', 'opinion', 'coordinated', 'opinions', 'influence', 'interactions', 'reciprocity', 'communities', 'dynamics', 'online']","['communities', 'influence', 'behavior', 'communication', 'cohesion', 'graphene', 'neighbor', 'distrust', 'coevolve', 'share']","[""  The advent of online social networks has led to the development of an\nabundant literature on the study of online social groups and their relationship\nto individuals' personalities as revealed by their textual productions. Social\nstructures are inferred from a wide range of social interactions. Those\ninteractions form complex -- sometimes multi-layered -- networks, on which\ncommunity detection algorithms are applied to extract higher order structures.\nThe choice of the community detection algorithm is however hardily questioned\nin relation with the cultural production of the individual they classify. In\nthis work, we assume the entangled nature of social networks and their cultural\nproduction to propose a definition of cultural based online social groups as\nsets of individuals whose online production can be categorized as social\ngroup-related. We take advantage of this apparently self-referential\ndescription of online social groups with a hybrid methodology that combines a\ncommunity detection algorithm and a natural language processing classification\nalgorithm. A key result of this analysis is the possibility to score community\ndetection algorithms using their agreement with the natural language processing\nclassification. A second result is that we can assign the opinion of a random\nuser at >85% accuracy.\n"", '  This note considers an innovative interdisciplinary methodology that bridges\nthe gap between the fundamental principles of quantum mechanics applied to the\nstudy of materials such as tellurium nanoparticles (TeNPs) and graphene and the\ncomplex dynamics of social systems. The basis for this approach lies in the\nmetaphorical parallels drawn between the structural features of TeNPs and\ngraphene and the behavioral patterns of social groups in the face of\nmisinformation. TeNPs exhibit unique properties such as the strengthening of\ncovalent bonds within telluric chains and the disruption of secondary structure\nleading to the separation of these chains. This is analogous to increased\ncohesion within social groups and disruption of information flow between\ndifferent subgroups, respectively. . Similarly, the outstanding properties of\ngraphene, such as high electrical conductivity, strength, and flexibility,\nprovide additional aspects for understanding the resilience and adaptability of\nsocial structures in response to external stimuli such as fake news. This\nresearch note proposes a novel metaphorical framework for analyzing the spread\nof fake news within social groups, analogous to the structural features of\ntelluric nanoparticles (TeNPs). We investigate how the strengthening of\ncovalent bonds within TeNPs reflects the strengthening of social cohesion in\ngroups that share common beliefs and values. This paper is partially an attempt\nto utilize ""Generative AI"" and was written with educational intent. There are\ncurrently no plans for it to become a peer-reviewed paper.\n', ""  As people's opinions change, their social networks typically coevolve with\nthem. People are often more susceptible to influence by people with similar\nopinions than by people with dissimilar opinions. In a bounded-confidence model\n(BCM) of opinion dynamics, interacting individuals influence each other through\ndyadic influence if and only if their opinions are sufficiently similar to each\nother. We introduce `neighborhood BCMs' (NBCMs) that include both the usual\ndyadic influence and a transitive influence, which models the effect of friends\nof a friend when determining whether or not an interaction with a friend\ninfluences an individual. In this transitive influence, an individual's opinion\nis influenced by a neighbor when, on average, the opinions of the neighbor's\nneighbors are sufficiently similar to their own opinion. We formulate\nneighborhood Deffuant--Weisbuch (NDW) and neighborhood Hegselmann--Krause (NHK)\nBCMs. We simulate our NDW model on time-independent networks and observe\ninteresting opinion states that cannot occur in an associated baseline DW\nmodel. We also simulate our NDW model on adaptive networks that coevolve with\nopinions by changing its structure through `transitive homophily'. An\nindividual that breaks a tie to one of its neighbors and then rewires that tie\nto a new individual, with a preference for individuals with a mean neighbor\nopinion that is closer to that individual's opinion. We explore how the\nqualitative opinion dynamics and network properties of our time-independent and\nadaptive NDWM models change as we adjust the relative proportions of dyadic and\ntransitive influence. Finally, we study a two-layer opinion--disease model in\nwhich we couple our NDW model with disease spread through a shared adaptive\nnetwork that can change both on the opinion layer and on the disease layer and\nwe examine how the opinion dynamics affect disease spread.\n""]",Social Network Analysis and Community Dynamics
464,463,13,463_forecasting_neural_microgrid_appliance,"['forecasting', 'neural', 'microgrid', 'appliance', 'appliances', 'transformer', 'electricity', 'cnn', 'prediction', 'energy']","['load', 'forecasting', 'electricity', 'interpretive', 'power', 'disaggregation', 'appliance', 'grid', 'microgrid', 'seq2']","['forecasting', 'microgrid', 'appliance', 'transformer', 'cnn', 'powerpm', 'encoder', 'neuralprophet', 'cloud', 'disaggregation']","['  As modern power systems continue to evolve, accurate power load forecasting\nremains a critical issue in energy management. The phase space reconstruction\nmethod can effectively retain the inner chaotic property of power load from a\nsystem dynamics perspective and thus is a promising knowledge-based\npreprocessing method for short-term forecasting. In order to fully utilize the\ncapability of PSR method to model the non-stationary characteristics within\npower load, and to solve the problem of the difficulty in applying traditional\nPSR prediction methods to form a general multi-step forecasting scheme, this\nstudy proposes a novel multi-step forecasting approach by delicately\nintegrating the PSR with neural networks to establish an end-to-end learning\nsystem. Firstly, the useful features in the phase trajectory are discussed in\ndetail. Through mathematical derivation, the equivalent characterization of the\nPSR and another time series preprocessing method, patch segmentation, is\ndemonstrated for the first time. Based on this knowledge, an image-based\nmodeling perspective is introduced. Subsequently, a novel deep learning model,\nnamely PSR-GALIEN, is designed, in which the Transformer Encoder and 2D-CNN are\nemployed for the extraction of the global and local patterns in the image, and\na MLP-based predictor is used for the efficient correlation modeling. Then,\nextensive experiments are conducted on five real-world benchmark datasets to\nverify the effectiveness of the PSR-GALIEN. The results show that, compared\nwith six state-of-the-art deep learning models, the forecasting performance of\nPSR-GALIEN consistently surpasses these baselines, achieving superior accuracy\nin both intra-day and day-ahead forecasting scenarios. At the same time, the\nattributions of its forecasting results can be explained through the\nvisualization-based method, which significantly increases the interpretability.\n', '  The emergence of abundant electricity time series (ETS) data provides ample\nopportunities for various applications in the power systems, including\ndemand-side management, grid stability, and consumer behavior analysis. Deep\nlearning models have advanced ETS modeling by effectively capturing sequence\ndependence. Nevertheless, learning a generic representation of ETS data for\nvarious applications remains challenging due to the inherently complex\nhierarchical structure of ETS data. Moreover, ETS data exhibits intricate\ntemporal dependencies and is suscepti ble to the influence of exogenous\nvariables. Furthermore, different instances exhibit diverse electricity\nconsumption behavior. In this paper, we propose a foundation model PowerPM to\nmodel ETS data, providing a large-scale, off-the-shelf model for power systems.\nPowerPM consists of a temporal encoder and a hierarchical encoder. The temporal\nencoder captures both temporal dependencies in ETS data, considering exogenous\nvariables. The hierarchical encoder models the correlation between hierarchy.\nFurthermore, PowerPM leverages a novel self-supervised pretraining framework\nconsisting of masked ETS modeling and dual-view contrastive learning, which\nenable PowerPM to capture temporal dependency within ETS windows and aware the\ndiscrepancy across ETS windows, providing two different perspectives to learn\ngeneric representation. Our experiments involve five real world scenario\ndatasets, comprising private and public data. Through pre-training on massive\nETS data, PowerPM achieves SOTA performance on diverse downstream tasks within\nthe private dataset. Impressively, when transferred to the public datasets,\nPowerPM maintains its superiority, showcasing its remarkable generalization\nability across various tasks and domains. Moreover, ablation studies, few-shot\nexperiments provide additional evidence of the effectiveness of our model.\n', '  Rapid progress in machine learning and deep learning has enabled a wide range\nof applications in the electricity load forecasting of power systems, for\ninstance, univariate and multivariate short-term load forecasting. Though the\nstrong capabilities of learning the non-linearity of the load patterns and the\nhigh prediction accuracy have been achieved, the interpretability of typical\ndeep learning models for electricity load forecasting is less studied. This\npaper proposes an interpretable deep learning method, which learns a linear\ncombination of neural networks that each attends to an input time feature. We\nalso proposed a multi-scale time series decomposition method to deal with the\ncomplex time patterns. Case studies have been carried out on the Belgium\ncentral grid load dataset and the proposed model demonstrated better accuracy\ncompared to the frequently applied baseline model. Specifically, the proposed\nmulti-scale temporal decomposition achieves the best MSE, MAE and RMSE of 0.52,\n0.57 and 0.72 respectively. As for interpretability, on one hand, the proposed\nmethod displays generalization capability. On the other hand, it can\ndemonstrate not only the feature but also the temporal interpretability\ncompared to other baseline methods. Besides, the global time feature\ninterpretabilities are also obtained. Obtaining global feature\ninterpretabilities allows us to catch the overall patterns, trends, and\ncyclicality in load data while also revealing the significance of various\ntime-related features in forming the final outputs.\n']","""Electricity Load Forecasting with Neural Networks"""
465,464,13,464_reinforcement_imitation_adversarial_learning,"['reinforcement', 'imitation', 'adversarial', 'learning', 'reward', 'exploration', 'demonstrations', 'learner', 'learn', 'inverse']","['imitation', 'reward', 'expert', 'inverse', 'policy', 'reinforcement', 'demonstrations', 'function', 'environment', 'discriminator']","['reinforcement', 'adversarial', 'exploration', 'demonstrations', 'learner', 'inverse', 'ollie', 'agent', 'subroutine', 'ail']","['  One of the main challenges in imitation learning is determining what action\nan agent should take when outside the state distribution of the demonstrations.\nInverse reinforcement learning (IRL) can enable generalization to new states by\nlearning a parameterized reward function, but these approaches still face\nuncertainty over the true reward function and corresponding optimal policy.\nExisting safe imitation learning approaches based on IRL deal with this\nuncertainty using a maxmin framework that optimizes a policy under the\nassumption of an adversarial reward function, whereas risk-neutral IRL\napproaches either optimize a policy for the mean or MAP reward function. While\ncompletely ignoring risk can lead to overly aggressive and unsafe policies,\noptimizing in a fully adversarial sense is also problematic as it can lead to\noverly conservative policies that perform poorly in practice. To provide a\nbridge between these two extremes, we propose Bayesian Robust Optimization for\nImitation Learning (BROIL). BROIL leverages Bayesian reward function inference\nand a user specific risk tolerance to efficiently optimize a robust policy that\nbalances expected return and conditional value at risk. Our empirical results\nshow that BROIL provides a natural way to interpolate between return-maximizing\nand risk-minimizing behaviors and outperforms existing risk-sensitive and\nrisk-neutral inverse reinforcement learning algorithms. Code is available at\nhttps://github.com/dsbrown1331/broil.\n', '  Recovering reward function from expert demonstrations is a fundamental\nproblem in reinforcement learning. The recovered reward function captures the\nmotivation of the expert. Agents can imitate experts by following these reward\nfunctions in their environment, which is known as apprentice learning. However,\nthe agents may face environments different from the demonstrations, and\ntherefore, desire transferable reward functions. Classical reward learning\nmethods such as inverse reinforcement learning (IRL) or, equivalently,\nadversarial imitation learning (AIL), recover reward functions coupled with\ntraining dynamics, which are hard to be transferable. Previous\ndynamics-agnostic reward learning methods rely on assumptions such as that the\nreward function has to be state-only, restricting their applicability. In this\nwork, we present a dynamics-agnostic discriminator-ensemble reward learning\nmethod (DARL) within the AIL framework, capable of learning both state-action\nand state-only reward functions. DARL achieves this by decoupling the reward\nfunction from training dynamics, employing a dynamics-agnostic discriminator on\na latent space derived from the original state-action space. This latent space\nis optimized to minimize information on the dynamics. We moreover discover the\npolicy-dependency issue of the AIL framework that reduces the transferability.\nDARL represents the reward function as an ensemble of discriminators during\ntraining to eliminate policy dependencies. Empirical studies on MuJoCo tasks\nwith changed dynamics show that DARL better recovers the reward function and\nresults in better imitation performance in transferred environments, handling\nboth state-only and state-action reward scenarios.\n', '  Many imitation learning (IL) algorithms employ inverse reinforcement learning\n(IRL) to infer the intrinsic reward function that an expert is implicitly\noptimizing for based on their demonstrated behaviors. However, in practice,\nIRL-based IL can fail to accomplish the underlying task due to a misalignment\nbetween the inferred reward and the objective of the task. In this paper, we\naddress the susceptibility of IL to such misalignment by introducing a\nsemi-supervised reward design paradigm called Protagonist Antagonist Guided\nAdversarial Reward (PAGAR). PAGAR-based IL trains a policy to perform well\nunder mixed reward functions instead of a single reward function as in\nIRL-based IL. We identify the theoretical conditions under which PAGAR-based IL\ncan avoid the task failures caused by reward misalignment. We also present a\npractical on-and-off policy approach to implementing PAGAR-based IL.\nExperimental results show that our algorithm outperforms standard IL baselines\nin complex tasks and challenging transfer settings.\n']",Adversarial Imitation Learning in Reinforcement Learning
466,465,13,465_linguistic_psycholinguistic_linguistics_sentences,"['linguistic', 'psycholinguistic', 'linguistics', 'sentences', 'syntactic', 'lexical', 'language', 'grammaticality', 'languages', 'biases']","['priming', 'word', 'surprisal', 'syntactic', 'reading', 'universals', 'illusion', 'verb', 'order', 'predictability']","['linguistic', 'psycholinguistic', 'grammaticality', 'biases', 'predictability', 'cognitively', 'priming', 'psychometric', 'probing_by_analogy', 'clauses']","[""  To date, most investigations on surprisal and entropy effects in reading have\nbeen conducted on the group level, disregarding individual differences. In this\nwork, we revisit the predictive power of surprisal and entropy measures\nestimated from a range of language models (LMs) on data of human reading times\nas a measure of processing effort by incorporating information of language\nusers' cognitive capacities. To do so, we assess the predictive power of\nsurprisal and entropy estimated from generative LMs on reading data obtained\nfrom individuals who also completed a wide range of psychometric tests.\nSpecifically, we investigate if modulating surprisal and entropy relative to\ncognitive scores increases prediction accuracy of reading times, and we examine\nwhether LMs exhibit systematic biases in the prediction of reading times for\ncognitively high- or low-performing groups, revealing what type of\npsycholinguistic subject a given LM emulates. Our study finds that in most\ncases, incorporating cognitive capacities increases predictive power of\nsurprisal and entropy on reading times, and that generally, high performance in\nthe psychometric tests is associated with lower sensitivity to predictability\neffects. Finally, our results suggest that the analyzed LMs emulate readers\nwith lower verbal intelligence, suggesting that for a given target group (i.e.,\nindividuals with high verbal intelligence), these LMs provide less accurate\npredictability estimates.\n"", '  Structural priming is a widely used psycholinguistic paradigm to study human\nsentence representations. In this work we propose a framework for using\nempirical priming patterns to build a theory characterizing the structural\nrepresentations humans construct when processing sentences. This framework uses\na new cognitively motivated parser, SPAWN, to generate quantitative priming\npredictions from theoretical syntax and evaluate these predictions with\nempirical human behavior. As a case study, we apply this framework to study\nreduced relative clause representations in English. We use SPAWN to generate\npriming predictions from two theoretical accounts which make different\nassumptions about the structure of relative clauses. We find that the\npredictions from only one of these theories (Participial-Phase) align with\nempirical priming patterns, thus highlighting which assumptions about relative\nclause better capture human sentence representations.\n', ""  The effect of syntactic priming exhibits three well-documented empirical\nproperties: the lexical boost, the inverse frequency effect, and the\nasymmetrical decay. We aim to show how these three empirical phenomena can be\nreconciled in a general learning framework, the hierarchical Bayesian model\n(HBM). The model represents syntactic knowledge in a hierarchical structure of\nsyntactic statistics, where a lower level represents the verb-specific biases\nof syntactic decisions, and a higher level represents the abstract bias as an\naggregation of verb-specific biases. This knowledge is updated in response to\nexperience by Bayesian inference. In simulations, we show that the HBM captures\nthe above-mentioned properties of syntactic priming. The results indicate that\nsome properties of priming which are usually explained by a residual activation\naccount can also be explained by an implicit learning account. We also discuss\nthe model's implications for the lexical basis of syntactic priming.\n""]",Psycholinguistics of Sentence Processing
467,466,13,466_argumentation_argumentative_semantics_arguments,"['argumentation', 'argumentative', 'semantics', 'arguments', 'prolog', 'formalisms', 'formalise', 'logic', 'reasoning', 'acceptability']","['argumentation', 'arguments', 'semantics', 'gradual', 'acceptability', 'argument', 'bipolar', 'flat', 'strength', 'preferences']","['argumentation', 'semantics', 'prolog', 'formalise', 'acceptability', 'negation', 'entailed', 'counterfactual', 'explanations', 'programming']","['  The relation between (a fragment of) assumption-based argumentation (ABA) and\nlogic programs (LPs) under stable model semantics is well-studied. However, for\nobtaining this relation, the ABA framework needs to be restricted to being\nflat, i.e., a fragment where the (defeasible) assumptions can never be\nentailed, only assumed to be true or false. Here, we remove this restriction\nand show a correspondence between non-flat ABA and LPs with negation as failure\nin their head. We then extend this result to so-called set-stable ABA\nsemantics, originally defined for the fragment of non-flat ABA called bipolar\nABA. We showcase how to define set-stable semantics for LPs with negation as\nfailure in their head and show the correspondence to set-stable ABA semantics.\n', '  We present an extension-based approach for computing and verifying\npreferences in an abstract argumentation system. Although numerous\nargumentation semantics have been developed previously for identifying\nacceptable sets of arguments from an argumentation framework, there is a lack\nof justification behind their acceptability based on implicit argument\npreferences. Preference-based argumentation frameworks allow one to determine\nwhat arguments are justified given a set of preferences. Our research considers\nthe inverse of the standard reasoning problem, i.e., given an abstract\nargumentation framework and a set of justified arguments, we compute what the\npossible preferences over arguments are. Furthermore, there is a need to verify\n(i.e., assess) that the computed preferences would lead to the acceptable sets\nof arguments. This paper presents a novel approach and algorithm for\nexhaustively computing and enumerating all possible sets of preferences\n(restricted to three identified cases) for a conflict-free set of arguments in\nan abstract argumentation framework. We prove the soundness, completeness and\ntermination of the algorithm. The research establishes that preferences are\ndetermined using an extension-based approach after the evaluation phase\n(acceptability of arguments) rather than stated beforehand. In this work, we\nfocus our research study on grounded, preferred and stable semantics. We show\nthat the complexity of computing sets of preferences is exponential in the\nnumber of arguments, and thus, describe an approximate approach and algorithm\nto compute the preferences. Furthermore, we present novel algorithms for\nverifying (i.e., assessing) the computed preferences. We provide details of the\nimplementation of the algorithms (source code has been made available), various\nexperiments performed to evaluate the algorithms and the analysis of the\nresults.\n', '  Most existing computational tools for assumption-based argumentation (ABA)\nfocus on so-called flat frameworks, disregarding the more general case. In this\npaper, we study an instantiation-based approach for reasoning in possibly\nnon-flat ABA. We make use of a semantics-preserving translation between ABA and\nbipolar argumentation frameworks (BAFs). By utilizing compilability theory, we\nestablish that the constructed BAFs will in general be of exponential size. In\norder to keep the number of arguments and computational cost low, we present\nthree ways of identifying redundant arguments. Moreover, we identify fragments\nof ABA which admit a poly-sized instantiation. We propose two algorithmic\napproaches for reasoning in possibly non-flat ABA. The first approach utilizes\nthe BAF instantiation while the second works directly without constructing\narguments. An empirical evaluation shows that the former outperforms the latter\non many instances, reflecting the lower complexity of BAF reasoning. This\nresult is in contrast to flat ABA, where direct approaches dominate\ninstantiation-based approaches.\n']",Argumentation Frameworks and Semantics
468,467,13,467_cnns_cracknet_cracks_convolutional,"['cnns', 'cracknet', 'cracks', 'convolutional', 'crack', 'efficientnetv2', 'pavement', 'cracking', 'pavements', 'yolov5']","['crack', 'pavement', 'cracks', 'damage', 'cracking', 'road', 'segmentation', 'defect', 'detection', 'severity']","['cnns', 'cracknet', 'efficientnetv2', 'pavement', 'yolov5', 'cyclegan', 'pixel', 'multiscale', 'epochs', 'uav']","[""  Anomalous crack region detection is a typical binary semantic segmentation\ntask, which aims to detect pixels representing cracks on pavement surface\nimages automatically by algorithms. Although existing deep learning-based\nmethods have achieved outcoming results on specific public pavement datasets,\nthe performance would deteriorate dramatically on imbalanced datasets. The\ninput datasets used in such tasks suffer from severely between-class imbalanced\nproblems, hence, it is a core challenge to obtain a robust performance on\ndiverse pavement datasets with generic deep learning models. To address this\nproblem, in this work, we propose a deep learning framework based on\nconditional Generative Adversarial Networks (cGANs) for the anomalous crack\nregion detection tasks at the pixel level. In particular, the proposed\nframework containing a cGANs and a novel auxiliary network is developed to\nenhance and stabilize the generator's performance under two alternative\ntraining stages, when estimating a multiscale probability feature map from\nheterogeneous and imbalanced inputs iteratively. Moreover, several attention\nmechanisms and entropy strategies are incorporated into the cGANs architecture\nand the auxiliary network separately to mitigate further the performance\ndeterioration of model training on severely imbalanced datasets. We implement\nextensive experiments on six accessible pavement datasets. The experimental\nresults from both visual and quantitative evaluation show that the proposed\nframework can achieve state-of-the-art results on these datasets efficiently\nand robustly without acceleration of computation complexity.\n"", '  Due to the varying intensity of pavement cracks, the complexity of\ntopological structure, and the noise of texture background, image\nclassification for asphalt pavement cracking has proven to be a challenging\nproblem. Fatigue cracking, also known as alligator cracking, is one of the\ncommon distresses of asphalt pavement. It is thus important to detect and\nmonitor the condition of alligator cracking on roadway pavements. Most research\nin this area has typically focused on pixel-level detection of cracking using\nlimited datasets. A novel deep convolutional neural network that can achieve\ntwo objectives is proposed. The first objective of the proposed neural network\nis to classify presence of fatigue cracking based on pavement surface images.\nThe second objective is to classify the fatigue cracking severity level based\non the Distress Identification Manual (DIM) standard. In this paper, a databank\nof 4484 high-resolution pavement surface images is established in which images\nare taken locally in the Town of Blacksburg, Virginia, USA. In the data\npre-preparation, over 4000 images are labeled into 4 categories manually\naccording to DIM standards. A four-layer convolutional neural network model is\nthen built to achieve the goal of classification of images by pavement crack\nseverity category. The trained model reached the highest accuracy among all\nexisting methods. After only 30 epochs of training, the model achieved a crack\nexistence classification accuracy of 96.23% and a severity level classification\naccuracy of 96.74%. After 20 epochs of training, the model achieved a pavement\nmarking presence classification accuracy of 97.64%.\n', ""  Cracks pose safety risks to infrastructure and cannot be overlooked. The\nprevailing structures in existing crack segmentation networks predominantly\nconsist of CNNs or Transformers. However, CNNs exhibit a deficiency in global\nmodeling capability, hindering the representation to entire crack features.\nTransformers can capture long-range dependencies but suffer from high and\nquadratic complexity. Recently, Mamba has garnered extensive attention due to\nits linear spatial and computational complexity and its powerful global\nperception. This study explores the representation capabilities of Mamba to\ncrack features. Specifically, this paper uncovers the connection between Mamba\nand the attention mechanism, providing a profound insight, an attention\nperspective, into interpreting Mamba and devising a novel Mamba module\nfollowing the principles of attention blocks, namely CrackMamba. We compare\nCrackMamba with the most prominent visual Mamba modules, Vim and Vmamba, on two\ndatasets comprising asphalt pavement and concrete pavement cracks, and steel\ncracks, respectively. The quantitative results show that CrackMamba stands out\nas the sole Mamba block consistently enhancing the baseline model's performance\nacross all evaluation measures, while reducing its parameters and computational\ncosts. Moreover, this paper substantiates that Mamba can achieve global\nreceptive fields through both theoretical analysis and visual interpretability.\nThe discoveries of this study offer a dual contribution. First, as a\nplug-and-play and simple yet effective Mamba module, CrackMamba exhibits\nimmense potential for integration into various crack segmentation models.\nSecond, the proposed innovative Mamba design concept, integrating Mamba with\nthe attention mechanism, holds significant reference value for all Mamba-based\ncomputer vision models, not limited to crack segmentation networks, as\ninvestigated in this study.\n""]",Crack Detection on Pavement Surfaces using Deep Learning
469,468,13,468_ranking_rankings_rank_fairness,"['ranking', 'rankings', 'rank', 'fairness', 'bias', 'merit', 'unfair', 'selection', 'utility', 'predictors']","['ranking', 'fairness', 'rankings', 'fair', 'items', 'group', 'groups', 'utility', 'stochastic', 'relevance']","['ranking', 'fairness', 'merit', 'utility', 'tax', 'hessian', 'quotas', 'recaptcha', 'ltr', 'owa']","['  Stochastic learning to rank (LTR) is a recent branch in the LTR field that\nconcerns the optimization of probabilistic ranking models. Their probabilistic\nbehavior enables certain ranking qualities that are impossible with\ndeterministic models. For example, they can increase the diversity of displayed\ndocuments, increase fairness of exposure over documents, and better balance\nexploitation and exploration through randomization. A core difficulty in LTR is\ngradient estimation, for this reason, existing stochastic LTR methods have been\nlimited to differentiable ranking models (e.g., neural networks). This is in\nstark contrast with the general field of LTR where Gradient Boosted Decision\nTrees (GBDTs) have long been considered the state-of-the-art. In this work, we\naddress this gap by introducing the first stochastic LTR method for GBDTs. Our\nmain contribution is a novel estimator for the second-order derivatives, i.e.,\nthe Hessian matrix, which is a requirement for effective GBDTs. To efficiently\ncompute both the first and second-order derivatives simultaneously, we\nincorporate our estimator into the existing PL-Rank framework, which was\noriginally designed for first-order derivatives only. Our experimental results\nindicate that stochastic LTR without the Hessian has extremely poor\nperformance, whilst the performance is competitive with the current\nstate-of-the-art with our estimated Hessian. Thus, through the contribution of\nour novel Hessian estimation method, we have successfully introduced GBDTs to\nstochastic LTR.\n', ""  When learning to rank from user interactions, search and recommender systems\nmust address biases in user behavior to provide a high-quality ranking. One\ntype of bias that has recently been studied in the ranking literature is when\nsensitive attributes, such as gender, have an impact on a user's judgment about\nan item's utility. For example, in a search for an expertise area, some users\nmay be biased towards clicking on male candidates over female candidates. We\ncall this type of bias group membership bias. Increasingly, we seek rankings\nthat are fair to individuals and sensitive groups. Merit-based fairness\nmeasures rely on the estimated utility of the items. With group membership\nbias, the utility of the sensitive groups is under-estimated, hence, without\ncorrecting for this bias, a supposedly fair ranking is not truly fair. In this\npaper, first, we analyze the impact of group membership bias on ranking quality\nas well as merit-based fairness metrics and show that group membership bias can\nhurt both ranking and fairness. Then, we provide a correction method for group\nbias that is based on the assumption that the utility score of items in\ndifferent groups comes from the same distribution. This assumption has two\npotential issues of sparsity and equality-instead-of-equity; we use an\namortized approach to address these. We show that our correction method can\nconsistently compensate for the negative impact of group membership bias on\nranking quality and fairness metrics.\n"", '  Learning to Rank (LTR) methods are vital in online economies, affecting users\nand item providers. Fairness in LTR models is crucial to allocate exposure\nproportionally to item relevance. Widely used deterministic LTR models can lead\nto unfair exposure distribution, especially when items with the same relevance\nreceive slightly different ranking scores. Stochastic LTR models, incorporating\nthe Plackett-Luce (PL) ranking model, address fairness issues but suffer from\nhigh training cost. In addition, they cannot provide guarantees on the utility\nor fairness, which can lead to dramatic degraded utility when optimized for\nfairness. To overcome these limitations, we propose Inference-time Stochastic\nRanking with Risk Control (ISRR), a novel method that performs stochastic\nranking at inference time with guanranteed utility or fairness given pretrained\nscoring functions from deterministic or stochastic LTR models. Comprehensive\nexperimental results on three widely adopted datasets demonstrate that our\nproposed method achieves utility and fairness comparable to existing stochastic\nranking methods with much lower computational cost. In addition, results verify\nthat our method provides finite-sample guarantee on utility and fairness. This\nadvancement represents a significant contribution to the field of stochastic\nranking and fair LTR with promising real-world applications.\n']",Fairness in Learning to Rank Models
470,469,13,469_multilingual_languages_answering_language,"['multilingual', 'languages', 'answering', 'language', 'translating', 'annotation', 'translation', 'linguistic', 'annotated', 'translated']","['languages', 'answering', 'question', 'resource', 'dataset', 'extractive', 'low', 'paragraphs', 'translation', 'multilingual']","['multilingual', 'translating', 'annotation', 'nlp', 'dataset', 'serbian', 'resources', 'openqa', 'urdu', 'marathinlp']","['  Question Answering (QA) datasets have been instrumental in developing and\nevaluating Large Language Model (LLM) capabilities. However, such datasets are\nscarce for languages other than English due to the cost and difficulties of\ncollection and manual annotation. This means that producing novel models and\nmeasuring the performance of multilingual LLMs in low-resource languages is\nchallenging. To mitigate this, we propose $\\textbf{S}$yn$\\textbf{DAR}$in, a\nmethod for generating and validating QA datasets for low-resource languages. We\nutilize parallel content mining to obtain $\\textit{human-curated}$ paragraphs\nbetween English and the target language. We use the English data as context to\n$\\textit{generate}$ synthetic multiple-choice (MC) question-answer pairs, which\nare automatically translated and further validated for quality. Combining these\nwith their designated non-English $\\textit{human-curated}$ paragraphs form the\nfinal QA dataset. The method allows to maintain the content quality, reduces\nthe likelihood of factual errors, and circumvents the need for costly\nannotation. To test the method, we created a QA dataset with $1.2$K samples for\nthe Armenian language. The human evaluation shows that $98\\%$ of the generated\nEnglish data maintains quality and diversity in the question types and topics,\nwhile the translation validation pipeline can filter out $\\sim70\\%$ of data\nwith poor quality. We use the dataset to benchmark state-of-the-art LLMs,\nshowing their inability to achieve human accuracy with some model performances\ncloser to random chance. This shows that the generated dataset is non-trivial\nand can be used to evaluate reasoning capabilities in low-resource language.\n', '  Question answering (QA) is the task of answering questions posed in natural\nlanguage with free-form natural language answers extracted from a given\npassage. In the OpenQA variant, only a question text is given, and the system\nmust retrieve relevant passages from an unstructured knowledge source and use\nthem to provide answers, which is the case in the mainstream QA systems on the\nWeb. QA systems currently are mostly limited to the English language due to the\nlack of large-scale labeled QA datasets in non-English languages. In this\npaper, we show that effective, low-cost OpenQA systems can be developed for\nlow-resource contexts. The key ingredients are (1) weak supervision using\nmachine-translated labeled datasets and (2) a relevant unstructured knowledge\nsource in the target language context. Furthermore, we show that only a few\nhundred gold assessment examples are needed to reliably evaluate these systems.\nWe apply our method to Turkish as a challenging case study, since English and\nTurkish are typologically very distinct and Turkish has limited resources for\nQA. We present SQuAD-TR, a machine translation of SQuAD2.0, and we build our\nOpenQA system by adapting ColBERT-QA and retraining it over Turkish resources\nand SQuAD-TR using two versions of Wikipedia dumps spanning two years. We\nobtain a performance improvement of 24-32% in the Exact Match (EM) score and\n22-29% in the F1 score compared to the BM25-based and DPR-based baseline QA\nreader models. Our results show that SQuAD-TR makes OpenQA feasible for\nTurkish, which we hope encourages researchers to build OpenQA systems in other\nlow-resource languages. We make all the code, models, and the dataset publicly\navailable at https://github.com/boun-tabi/SQuAD-TR.\n', ""  Large Language Models (LLMs) have demonstrated remarkable zero-shot and\nfew-shot capabilities in unseen tasks, including context-grounded question\nanswering (QA) in English. However, the evaluation of LLMs' capabilities in\nnon-English languages for context-based QA is limited by the scarcity of\nbenchmarks in non-English languages. To address this gap, we introduce\nIndic-QA, the largest publicly available context-grounded question-answering\ndataset for 11 major Indian languages from two language families. The dataset\ncomprises both extractive and abstractive question-answering tasks and includes\nexisting datasets as well as English QA datasets translated into Indian\nlanguages. Additionally, we generate a synthetic dataset using the Gemini model\nto create question-answer pairs given a passage, which is then manually\nverified for quality assurance. We evaluate various multilingual Large Language\nModels and their instruction-fine-tuned variants on the benchmark and observe\nthat their performance is subpar, particularly for low-resource languages. We\nhope that the release of this dataset will stimulate further research on the\nquestion-answering abilities of LLMs for low-resource languages.\n""]",Multilingual Question Answering
471,470,13,470_logs_parsers_log_logevals,"['logs', 'parsers', 'log', 'logevals', 'logeval', 'openlogparser', 'parsing', 'parser', 'logprompt', 'loghub']","['log', 'logs', 'parsers', 'templates', 'software', 'anomalies', 'parser', 'anomaly', 'messages', 'maintenance']","['logs', 'parsers', 'logevals', 'openlogparser', 'hadoop', 'eclipse', 'occurrence', 'maintenance', 'thunderbird', 'templates']","['  Logs are important in modern software development with runtime information.\nLog parsing is the first step in many log-based analyses, that involve\nextracting structured information from unstructured log data. Traditional log\nparsers face challenges in accurately parsing logs due to the diversity of log\nformats, which directly impacts the performance of downstream log-analysis\ntasks. In this paper, we explore the potential of using Large Language Models\n(LLMs) for log parsing and propose LLMParser, an LLM-based log parser based on\ngenerative LLMs and few-shot tuning. We leverage four LLMs, Flan-T5-small,\nFlan-T5-base, LLaMA-7B, and ChatGLM-6B in LLMParsers. Our evaluation of 16\nopen-source systems shows that LLMParser achieves statistically significantly\nhigher parsing accuracy than state-of-the-art parsers (a 96% average parsing\naccuracy). We further conduct a comprehensive empirical analysis on the effect\nof training size, model size, and pre-training LLM on log parsing accuracy. We\nfind that smaller LLMs may be more effective than more complex LLMs; for\ninstance where Flan-T5-base achieves comparable results as LLaMA-7B with a\nshorter inference time. We also find that using LLMs pre-trained using logs\nfrom other systems does not always improve parsing accuracy. While using\npre-trained Flan-T5-base shows an improvement in accuracy, pre-trained LLaMA\nresults in a decrease (decrease by almost 55% in group accuracy). In short, our\nstudy provides empirical evidence for using LLMs for log parsing and highlights\nthe limitations and future research direction of LLM-based log parsers.\n', ""  Logs are a first-hand source of information for software maintenance and\nfailure diagnosis. Log parsing, which converts semi-structured log messages\ninto structured templates, is a prerequisite for automated log analysis tasks\nsuch as anomaly detection, troubleshooting, and root cause analysis. However,\nexisting log parsers fail in real-world systems for three main reasons. First,\ntraditional heuristics-based parsers require handcrafted features and domain\nknowledge, which are difficult to generalize at scale. Second, existing large\nlanguage model-based parsers rely on periodic offline processing, limiting\ntheir effectiveness in real-time use cases. Third, existing online parsing\nalgorithms are susceptible to log drift, where slight log changes create false\npositives that drown out real anomalies. To address these challenges, we\npropose HELP, a Hierarchical Embeddings-based Log Parser. HELP is the first\nonline semantic-based parser to leverage LLMs for performant and cost-effective\nlog parsing. We achieve this through a novel hierarchical embeddings module,\nwhich fine-tunes a text embedding model to cluster logs before parsing,\nreducing querying costs by multiple orders of magnitude. To combat log drift,\nwe also develop an iterative rebalancing module, which periodically updates\nexisting log groupings. We evaluate HELP extensively on 14 public large-scale\ndatasets, showing that HELP achieves significantly higher F1-weighted grouping\nand parsing accuracy than current state-of-the-art online log parsers. We also\nimplement HELP into Iudex's production observability platform, confirming\nHELP's practicality in a production environment. Our results show that HELP is\neffective and efficient for high-throughput real-world log parsing.\n"", ""  Logs are an essential source of information for people to understand the\nrunning status of a software system. Due to the evolving modern software\narchitecture and maintenance methods, more research efforts have been devoted\nto automated log analysis. In particular, machine learning (ML) has been widely\nused in log analysis tasks. In ML-based log analysis tasks, converting textual\nlog data into numerical feature vectors is a critical and indispensable step.\nHowever, the impact of using different log representation techniques on the\nperformance of the downstream models is not clear, which limits researchers and\npractitioners' opportunities of choosing the optimal log representation\ntechniques in their automated log analysis workflows. Therefore, this work\ninvestigates and compares the commonly adopted log representation techniques\nfrom previous log analysis research. Particularly, we select six log\nrepresentation techniques and evaluate them with seven ML models and four\npublic log datasets (i.e., HDFS, BGL, Spirit and Thunderbird) in the context of\nlog-based anomaly detection. We also examine the impacts of the log parsing\nprocess and the different feature aggregation approaches when they are employed\nwith log representation techniques. From the experiments, we provide some\nheuristic guidelines for future researchers and developers to follow when\ndesigning an automated log analysis workflow. We believe our comprehensive\ncomparison of log representation techniques can help researchers and\npractitioners better understand the characteristics of different log\nrepresentation techniques and provide them with guidance for selecting the most\nsuitable ones for their ML-based log analysis workflow.\n""]",Log Parsing and Analysis
472,471,13,471_imputations_imputation_missingness_data,"['imputations', 'imputation', 'missingness', 'data', 'personalized', 'ehrs', 'ehr', 'health', 'healthcare', 'prediction']","['missing', 'imputation', 'missingness', 'electronic', 'health', 'patient', 'values', 'patients', 'records', 'longitudinal']","['imputations', 'data', 'personalized', 'ehrs', 'healthcare', 'missing', 'prognostic', 'sepsislab', 'longitudinal', 'embedding']","[""  Analyzing the health status of patients based on Electronic Health Records\n(EHR) is a fundamental research problem in medical informatics. The presence of\nextensive missing values in EHR makes it challenging for deep neural networks\nto directly model the patient's health status based on EHR. Existing deep\nlearning training protocols require the use of statistical information or\nimputation models to reconstruct missing values; however, the protocols inject\nnon-realistic data into downstream EHR analysis models, significantly limiting\nmodel performance. This paper introduces Learnable Prompt as Pseudo Imputation\n(PAI) as a new training protocol. PAI no longer introduces any imputed data but\nconstructs a learnable prompt to model the implicit preferences of the\ndownstream model for missing values, resulting in a significant performance\nimprovement for all EHR analysis models. Additionally, our experiments show\nthat PAI exhibits higher robustness in situations of data insufficiency and\nhigh missing rates. More importantly, in a real-world application involving\ncross-institutional data with zero-shot evaluation, PAI demonstrates stronger\nmodel generalization capabilities for non-overlapping features.\n"", '  Electronic health record (EHR) data has emerged as a valuable resource for\nanalyzing patient health status. However, the prevalence of missing data in EHR\nposes significant challenges to existing methods, leading to spurious\ncorrelations and suboptimal predictions. While various imputation techniques\nhave been developed to address this issue, they often obsess unnecessary\ndetails and may introduce additional noise when making clinical predictions. To\ntackle this problem, we propose SMART, a Self-Supervised Missing-Aware\nRepresenTation Learning approach for patient health status prediction, which\nencodes missing information via elaborated attentions and learns to impute\nmissing values through a novel self-supervised pre-training approach that\nreconstructs missing data representations in the latent space. By adopting\nmissing-aware attentions and focusing on learning higher-order representations,\nSMART promotes better generalization and robustness to missing data. We\nvalidate the effectiveness of SMART through extensive experiments on six EHR\ntasks, demonstrating its superiority over state-of-the-art methods.\n', ""  Electronic Health Records present a valuable modality for driving\npersonalized medicine, where treatment is tailored to fit individual-level\ndifferences. For this purpose, many data-driven machine learning and\nstatistical models rely on the wealth of longitudinal EHRs to study patients'\nphysiological and treatment effects. However, longitudinal EHRs tend to be\nsparse and highly missing, where missingness could also be informative and\nreflect the underlying patient's health status. Therefore, the success of\ndata-driven models for personalized medicine highly depends on how the EHR data\nis represented from physiological data, treatments, and the missing values in\nthe data. To this end, we propose a novel deep-learning model that learns the\nunderlying patient dynamics over time across multivariate data to generate\npersonalized realistic values conditioning on an individual's demographic\ncharacteristics and treatments. Our proposed model, IGNITE (Individualized\nGeNeration of Imputations in Time-series Electronic health records), utilises a\nconditional dual-variational autoencoder augmented with dual-stage attention to\ngenerate missing values for an individual. In IGNITE, we further propose a\nnovel individualized missingness mask (IMM), which helps our model generate\nvalues based on the individual's observed data and missingness patterns. We\nfurther extend the use of IGNITE from imputing missingness to a personalized\ndata synthesizer, where it generates missing EHRs that were never observed\nprior or even generates new patients for various applications. We validate our\nmodel on three large publicly available datasets and show that IGNITE\noutperforms state-of-the-art approaches in missing data reconstruction and task\nprediction.\n""]","""Imputation Methods for Electronic Health Records"""
473,472,13,472_exploit_adversarial_vulnerabilities_attacker,"['exploit', 'adversarial', 'vulnerabilities', 'attacker', 'malicious', 'vulnerability', 'attacks', 'retrieval', 'security', 'vulnerable']","['retrieval', 'attacker', 'database', 'passages', 'attack', 'documents', 'attacks', 'security', 'malicious', 'queries']","['exploit', 'adversarial', 'vulnerabilities', 'malicious', 'retrievers', 'securely', 'targeted', 'backdoor', 'defenses', 'robustrag']","['  Retrieval Augmented Generation (RAG) systems have shown great promise in\nnatural language processing. However, their reliance on data stored in a\nretrieval database, which may contain proprietary or sensitive information,\nintroduces new privacy concerns. Specifically, an attacker may be able to infer\nwhether a certain text passage appears in the retrieval database by observing\nthe outputs of the RAG system, an attack known as a Membership Inference Attack\n(MIA). Despite the significance of this threat, MIAs against RAG systems have\nyet remained under-explored. This study addresses this gap by introducing an\nefficient and easy-to-use method for conducting MIA against RAG systems. We\ndemonstrate the effectiveness of our attack using two benchmark datasets and\nmultiple generative models, showing that the membership of a document in the\nretrieval database can be efficiently determined through the creation of an\nappropriate prompt in both black-box and gray-box settings. Moreover, we\nintroduce an initial defense strategy based on adding instructions to the RAG\ntemplate, which shows high effectiveness for some datasets and models. Our\nfindings highlight the importance of implementing security countermeasures in\ndeployed RAG systems and developing more advanced defenses to protect the\nprivacy and security of retrieval databases.\n', '  Large language models (LLMs) have achieved remarkable success due to their\nexceptional generative capabilities. Despite their success, they also have\ninherent limitations such as a lack of up-to-date knowledge and hallucination.\nRetrieval-Augmented Generation (RAG) is a state-of-the-art technique to\nmitigate these limitations. The key idea of RAG is to ground the answer\ngeneration of an LLM on external knowledge retrieved from a knowledge database.\nExisting studies mainly focus on improving the accuracy or efficiency of RAG,\nleaving its security largely unexplored. We aim to bridge the gap in this work.\nWe find that the knowledge database in a RAG system introduces a new and\npractical attack surface. Based on this attack surface, we propose PoisonedRAG,\nthe first knowledge corruption attack to RAG, where an attacker could inject a\nfew malicious texts into the knowledge database of a RAG system to induce an\nLLM to generate an attacker-chosen target answer for an attacker-chosen target\nquestion. We formulate knowledge corruption attacks as an optimization problem,\nwhose solution is a set of malicious texts. Depending on the background\nknowledge (e.g., black-box and white-box settings) of an attacker on a RAG\nsystem, we propose two solutions to solve the optimization problem,\nrespectively. Our results show PoisonedRAG could achieve a 90% attack success\nrate when injecting five malicious texts for each target question into a\nknowledge database with millions of texts. We also evaluate several defenses\nand our results show they are insufficient to defend against PoisonedRAG,\nhighlighting the need for new defenses.\n', ""  Retrieval Augmented Generation (RAG) expands the capabilities of modern large\nlanguage models (LLMs) in chatbot applications, enabling developers to adapt\nand personalize the LLM output without expensive training or fine-tuning. RAG\nsystems use an external knowledge database to retrieve the most relevant\ndocuments for a given query, providing this context to the LLM generator. While\nRAG achieves impressive utility in many applications, its adoption to enable\npersonalized generative models introduces new security risks. In this work, we\npropose new attack surfaces for an adversary to compromise a victim's RAG\nsystem, by injecting a single malicious document in its knowledge database. We\ndesign Phantom, general two-step attack framework against RAG augmented LLMs.\nThe first step involves crafting a poisoned document designed to be retrieved\nby the RAG system within the top-k results only when an adversarial trigger, a\nspecific sequence of words acting as backdoor, is present in the victim's\nqueries. In the second step, a specially crafted adversarial string within the\npoisoned document triggers various adversarial attacks in the LLM generator,\nincluding denial of service, reputation damage, privacy violations, and harmful\nbehaviors. We demonstrate our attacks on multiple LLM architectures, including\nGemma, Vicuna, and Llama.\n""]","""Attacks on Retrieval-Augmented Language Models"""
474,473,13,473_classifiers_classification_classifier_classifying,"['classifiers', 'classification', 'classifier', 'classifying', 'misclassify', 'classes', 'recognition', 'softmax', 'openness', 'wiseopen']","['classes', 'open', 'set', 'unknown', 'recognition', 'closed', 'samples', 'class', 'classification', 'rejection']","['classifiers', 'misclassify', 'softmax', 'wiseopen', 'openmax', 'osc', 'discriminability', 'trained', 'unknowns', 'ova']","['  Classifying patterns of known classes and rejecting ambiguous and novel (also\ncalled as out-of-distribution (OOD)) inputs are involved in open world pattern\nrecognition. Deep neural network models usually excel in closed-set\nclassification while performs poorly in rejecting OOD inputs. To tackle this\nproblem, numerous methods have been designed to perform open set recognition\n(OSR) or OOD rejection/detection tasks. Previous methods mostly take\npost-training score transformation or hybrid models to ensure low scores on OOD\ninputs while separating known classes. In this paper, we attempt to build a\nunified framework for building open set classifiers for both classification and\nOOD rejection. We formulate the open set recognition of $ K $-known-class as a\n$ (K+1) $-class classification problem with model trained on known-class\nsamples only. By decomposing the $ K $-class problem into $ K $ one-versus-all\n(OVA) binary classification tasks and binding some parameters, we show that\ncombining the scores of OVA classifiers can give $ (K+1) $-class posterior\nprobabilities, which enables classification and OOD rejection in a unified\nframework. To maintain the closed-set classification accuracy of the OVA\ntrained classifier, we propose a hybrid training strategy combining OVA loss\nand multi-class cross-entropy loss. We implement the OVA framework and hybrid\ntraining strategy on the recently proposed convolutional prototype network and\nprototype classifier on vision transformer (ViT) backbone. Experiments on\npopular OSR and OOD detection datasets demonstrate that the proposed framework,\nusing a single multi-class classifier, yields competitive performance in\nclosed-set classification, OOD detection, and misclassification detection.\n', ""  In open-set recognition, existing methods generally learn statically fixed\ndecision boundaries using known classes to reject unknown classes. Though they\nhave achieved promising results, such decision boundaries are evidently\ninsufficient for universal unknown classes in dynamic and open scenarios as\nthey can potentially appear at any position in the feature space. Moreover,\nthese methods just simply reject unknown class samples during testing without\nany effective utilization for them. In fact, such samples completely can\nconstitute the true instantiated representation of the unknown classes to\nfurther enhance the model's performance. To address these issues, this paper\nproposes a novel dynamic against dynamic idea, i.e., dynamic method against\ndynamic changing open-set world, where an open-set self-learning (OSSL)\nframework is correspondingly developed. OSSL starts with a good closed-set\nclassifier trained by known classes and utilizes available test samples for\nmodel adaptation during testing, thus gaining the adaptability to changing data\ndistributions. In particular, a novel self-matching module is designed for\nOSSL, which can achieve the adaptation in automatically identifying known class\nsamples while rejecting unknown class samples which are further utilized to\nenhance the discriminability of the model as the instantiated representation of\nunknown classes. Our method establishes new performance milestones respectively\nin almost all standard and cross-data benchmarks.\n"", ""  Open-set Semi-supervised Learning (OSSL) holds a realistic setting that\nunlabeled data may come from classes unseen in the labeled set, i.e.,\nout-of-distribution (OOD) data, which could cause performance degradation in\nconventional SSL models. To handle this issue, except for the traditional\nin-distribution (ID) classifier, some existing OSSL approaches employ an extra\nOOD detection module to avoid the potential negative impact of the OOD data.\nNevertheless, these approaches typically employ the entire set of open-set data\nduring their training process, which may contain data unfriendly to the OSSL\ntask that can negatively influence the model performance. This inspires us to\ndevelop a robust open-set data selection strategy for OSSL. Through a\ntheoretical understanding from the perspective of learning theory, we propose\nWise Open-set Semi-supervised Learning (WiseOpen), a generic OSSL framework\nthat selectively leverages the open-set data for training the model. By\napplying a gradient-variance-based selection mechanism, WiseOpen exploits a\nfriendly subset instead of the whole open-set dataset to enhance the model's\ncapability of ID classification. Moreover, to reduce the computational expense,\nwe also propose two practical variants of WiseOpen by adopting low-frequency\nupdate and loss-based selection respectively. Extensive experiments demonstrate\nthe effectiveness of WiseOpen in comparison with the state-of-the-art.\n""]",Open-Set Classification and Recognition
475,474,13,474_reidentification_matching_discriminative_identification,"['reidentification', 'matching', 'discriminative', 'identification', 'pedestrian', 'pedestrians', 'feature', 'datasets', 'person', 'identity']","['person', 'modality', 'old', 'infrared', 'camera', 'visible', 'identification', 'pedestrian', 'cross', 'instance']","['reidentification', 'matching', 'discriminative', 'identification', 'pedestrians', 'datasets', 'cameras', 'luperson', 'labels', 'ccsp']","['  Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)\naims at learning modality-invariant features from unlabeled cross-modality\ndataset, which is crucial for practical applications in video surveillance\nsystems. The key to essentially address the USL-VI-ReID task is to solve the\ncross-modality data association problem for further heterogeneous joint\nlearning. To address this issue, we propose a Dual Optimal Transport Label\nAssignment (DOTLA) framework to simultaneously assign the generated labels from\none modality to its counterpart modality. The proposed DOTLA mechanism\nformulates a mutual reinforcement and efficient solution to cross-modality data\nassociation, which could effectively reduce the side-effects of some\ninsufficient and noisy label associations. Besides, we further propose a\ncross-modality neighbor consistency guided label refinement and regularization\nmodule, to eliminate the negative effects brought by the inaccurate supervised\nsignals, under the assumption that the prediction or label distribution of each\nexample should be similar to its nearest neighbors. Extensive experimental\nresults on the public SYSU-MM01 and RegDB datasets demonstrate the\neffectiveness of the proposed method, surpassing existing state-of-the-art\napproach by a large margin of 7.76% mAP on average, which even surpasses some\nsupervised VI-ReID methods.\n', '  Existing person re-identification (Re-ID) methods principally deploy the\nImageNet-1K dataset for model initialization, which inevitably results in\nsub-optimal situations due to the large domain gap. One of the key challenges\nis that building large-scale person Re-ID datasets is time-consuming. Some\nprevious efforts address this problem by collecting person images from the\ninternet e.g., LUPerson, but it struggles to learn from unlabeled,\nuncontrollable, and noisy data. In this paper, we present a novel paradigm\nDiffusion-ReID to efficiently augment and generate diverse images based on\nknown identities without requiring any cost of data collection and annotation.\nTechnically, this paradigm unfolds in two stages: generation and filtering.\nDuring the generation stage, we propose Language Prompts Enhancement (LPE) to\nensure the ID consistency between the input image sequence and the generated\nimages. In the diffusion process, we propose a Diversity Injection (DI) module\nto increase attribute diversity. In order to make the generated data have\nhigher quality, we apply a Re-ID confidence threshold filter to further remove\nthe low-quality images. Benefiting from our proposed paradigm, we first create\na new large-scale person Re-ID dataset Diff-Person, which consists of over 777K\nimages from 5,183 identities. Next, we build a stronger person Re-ID backbone\npre-trained on our Diff-Person. Extensive experiments are conducted on four\nperson Re-ID benchmarks in six widely used settings. Compared with other\npre-training and self-supervised competitors, our approach shows significant\nsuperiority.\n', '  Text-based person re-identification (Re-ID) is a challenging topic in the\nfield of complex multimodal analysis, its ultimate aim is to recognize specific\npedestrians by scrutinizing attributes/natural language descriptions. Despite\nthe wide range of applicable areas such as security surveillance, video\nretrieval, person tracking, and social media analytics, there is a notable\nabsence of comprehensive reviews dedicated to summarizing the text-based person\nRe-ID from a technical perspective. To address this gap, we propose to\nintroduce a taxonomy spanning Evaluation, Strategy, Architecture, and\nOptimization dimensions, providing a comprehensive survey of the text-based\nperson Re-ID task. We start by laying the groundwork for text-based person\nRe-ID, elucidating fundamental concepts related to attribute/natural\nlanguage-based identification. Then a thorough examination of existing\nbenchmark datasets and metrics is presented. Subsequently, we further delve\ninto prevalent feature extraction strategies employed in text-based person\nRe-ID research, followed by a concise summary of common network architectures\nwithin the domain. Prevalent loss functions utilized for model optimization and\nmodality alignment in text-based person Re-ID are also scrutinized. To\nconclude, we offer a concise summary of our findings, pinpointing challenges in\ntext-based person Re-ID. In response to these challenges, we outline potential\navenues for future open-set text-based person Re-ID and present a baseline\narchitecture for text-based pedestrian image generation-guided\nre-identification(TBPGR).\n']",Person Re-identification
476,475,12,475_byzantine_distributed_mlmc_decentralized,"['byzantine', 'distributed', 'mlmc', 'decentralized', 'robustscgmm', 'robust', 'federated', 'aggregators', 'adversaries', 'outliers']","['byzantine', 'tolerant', 'workers', 'resilient', 'fault', 'aggregation', 'clipping', 'failures', 'tolerance', 'convex']","['byzantine', 'mlmc', 'robustscgmm', 'federated', 'aggregators', 'trusted', 'attacks', 'resilient', 'consensus', 'validator']","['  In Federated Reinforcement Learning (FRL), agents aim to collaboratively\nlearn a common task, while each agent is acting in its local environment\nwithout exchanging raw trajectories. Existing approaches for FRL either (a) do\nnot provide any fault-tolerance guarantees (against misbehaving agents), or (b)\nrely on a trusted central agent (a single point of failure) for aggregating\nupdates. We provide the first decentralized Byzantine fault-tolerant FRL\nmethod. Towards this end, we first propose a new centralized Byzantine\nfault-tolerant policy gradient (PG) algorithm that improves over existing\nmethods by relying only on assumptions standard for non-fault-tolerant PG.\nThen, as our main contribution, we show how a combination of robust aggregation\nand Byzantine-resilient agreement methods can be leveraged in order to\neliminate the need for a trusted central entity. Since our results represent\nthe first sample complexity analysis for Byzantine fault-tolerant decentralized\nfederated non-convex optimization, our technical contributions may be of\nindependent interest. Finally, we corroborate our theoretical results\nexperimentally for common RL environments, demonstrating the speed-up of\ndecentralized federations w.r.t. the number of participating agents and\nresilience against various Byzantine attacks.\n', '  This paper proposes two split-and-conquer (SC) learning estimators for finite\nmixture models that are tolerant to Byzantine failures. In SC learning,\nindividual machines obtain local estimates, which are then transmitted to a\ncentral server for aggregation. During this communication, the server may\nreceive malicious or incorrect information from some local machines, a scenario\nknown as Byzantine failures. While SC learning approaches have been devised to\nmitigate Byzantine failures in statistical models with Euclidean parameters,\ndeveloping Byzantine-tolerant methods for finite mixture models with\nnon-Euclidean parameters requires a distinct strategy. Our proposed\ndistance-based methods are hyperparameter tuning free, unlike existing methods,\nand are resilient to Byzantine failures while achieving high statistical\nefficiency. We validate the effectiveness of our methods both theoretically and\nempirically via experiments on simulated and real data from machine learning\napplications for digit recognition. The code for the experiment can be found at\nhttps://github.com/SarahQiong/RobustSCGMM.\n', '  Distributed learning has emerged as a leading paradigm for training large\nmachine learning models. However, in real-world scenarios, participants may be\nunreliable or malicious, posing a significant challenge to the integrity and\naccuracy of the trained models. Byzantine fault tolerance mechanisms have been\nproposed to address these issues, but they often assume full participation from\nall clients, which is not always practical due to the unavailability of some\nclients or communication constraints. In our work, we propose the first\ndistributed method with client sampling and provable tolerance to Byzantine\nworkers. The key idea behind the developed method is the use of gradient\nclipping to control stochastic gradient differences in recursive variance\nreduction. This allows us to bound the potential harm caused by Byzantine\nworkers, even during iterations when all sampled clients are Byzantine.\nFurthermore, we incorporate communication compression into the method to\nenhance communication efficiency. Under general assumptions, we prove\nconvergence rates for the proposed method that match the existing\nstate-of-the-art (SOTA) theoretical results. We also propose a heuristic on\nadjusting any Byzantine-robust method to a partial participation scenario via\nclipping.\n']",Byzantine Fault Tolerance in Distributed Machine Learning
477,476,12,476_tokamaks_tokamak_plasma_plasmas,"['tokamaks', 'tokamak', 'plasma', 'plasmas', 'fusion', 'simulations', 'modeling', 'magnetohydrodynamics', 'modelling', 'dynamics']","['plasma', 'tokamak', 'plasmas', 'kinetic', 'dynamics', 'tokamaks', 'evolution', 'fusion', 'surrogate', 'divertor']","['tokamaks', 'plasma', 'simulations', 'magnetohydrodynamics', 'turbulence', 'thruster', 'reactors', 'sciml', 'dimensional', 'discharges']","['  We explore the possibility of fully replacing a plasma physics kinetic\nsimulator with a graph neural network-based simulator. We focus on this class\nof surrogate models given the similarity between their message-passing update\nmechanism and the traditional physics solver update, and the possibility of\nenforcing known physical priors into the graph construction and update. We show\nthat our model learns the kinetic plasma dynamics of the one-dimensional plasma\nmodel, a predecessor of contemporary kinetic plasma simulation codes, and\nrecovers a wide range of well-known kinetic plasma processes, including plasma\nthermalization, electrostatic fluctuations about thermal equilibrium, and the\ndrag on a fast sheet and Landau damping. We compare the performance against the\noriginal plasma model in terms of run-time, conservation laws, and temporal\nevolution of key physical quantities. The limitations of the model are\npresented and possible directions for higher-dimensional surrogate models for\nkinetic plasmas are discussed.\n', '  Reduced-order plasma models that can efficiently predict plasma behavior\nacross various settings and configurations are highly sought after yet elusive.\nThe demand for such models has surged in the past decade due to their potential\nto facilitate scientific research and expedite the development of plasma\ntechnologies. In line with the advancements in computational power and\ndata-driven methods, we introduce the ""Phi Method"" in this two-part article.\nPart I presents this novel algorithm, which employs constrained regression on a\ncandidate term library informed by numerical discretization schemes to discover\ndiscretized systems of differential equations. We demonstrate Phi Method\'s\nefficacy in deriving reliable and robust reduced-order models (ROMs) for three\ntest cases: the Lorenz attractor, flow past a cylinder, and a 1D\nHall-thruster-representative plasma. Part II will delve into the method\'s\napplication for parametric dynamics discovery. Our results show that ROMs\nderived from the Phi Method provide remarkably accurate predictions of systems\'\nbehavior, whether derived from steady-state or transient-state data. This\nunderscores the method\'s potential for transforming plasma system modeling.\n', '  Predicting plasma evolution within a Tokamak reactor is crucial to realizing\nthe goal of sustainable fusion. Capabilities in forecasting the spatio-temporal\nevolution of plasma rapidly and accurately allow us to quickly iterate over\ndesign and control strategies on current Tokamak devices and future reactors.\nModelling plasma evolution using numerical solvers is often expensive,\nconsuming many hours on supercomputers, and hence, we need alternative\ninexpensive surrogate models. We demonstrate accurate predictions of plasma\nevolution both in simulation and experimental domains using deep learning-based\nsurrogate modelling tools, viz., Fourier Neural Operators (FNO). We show that\nFNO has a speedup of six orders of magnitude over traditional solvers in\npredicting the plasma dynamics simulated from magnetohydrodynamic models, while\nmaintaining a high accuracy (MSE in the normalised domain $\\approx$ $10^{-5}$).\nOur modified version of the FNO is capable of solving multi-variable Partial\nDifferential Equations (PDE), and can capture the dependence among the\ndifferent variables in a single model. FNOs can also predict plasma evolution\non real-world experimental data observed by the cameras positioned within the\nMAST Tokamak, i.e., cameras looking across the central solenoid and the\ndivertor in the Tokamak. We show that FNOs are able to accurately forecast the\nevolution of plasma and have the potential to be deployed for real-time\nmonitoring. We also illustrate their capability in forecasting the plasma\nshape, the locations of interactions of the plasma with the central solenoid\nand the divertor for the full (available) duration of the plasma shot within\nMAST. The FNO offers a viable alternative for surrogate modelling as it is\nquick to train and infer, and requires fewer data points, while being able to\ndo zero-shot super-resolution and getting high-fidelity solutions.\n']",Plasma Simulation and Modeling in Tokamaks
478,477,12,477_documentunderstanding_layoutllm_text_doclaynet,"['documentunderstanding', 'layoutllm', 'text', 'doclaynet', 'layouts', 'layout', 'texts', 'document', 'textual', 'ocr']","['layout', 'document', 'layouts', 'documents', 'textual', 'understanding', 'text', 'spatial', 'extraction', 'visual']","['documentunderstanding', 'layoutllm', 'doclaynet', 'texts', 'ocr', 'structured', 'multimodal', 'enrichment', 'instruction', 'benchmarks']","[""  This paper proposes LayoutLLM, a more flexible document analysis method for\nunderstanding imaged documents. Visually Rich Document Understanding tasks,\nsuch as document image classification and information extraction, have gained\nsignificant attention due to their importance. Existing methods have been\ndeveloped to enhance document comprehension by incorporating pre-training\nawareness of images, text, and layout structure. However, these methods require\nfine-tuning for each task and dataset, and the models are expensive to train\nand operate. To overcome this limitation, we propose a new LayoutLLM that\nintegrates these with large-scale language models (LLMs). By leveraging the\nstrengths of existing research in document image understanding and LLMs'\nsuperior language understanding capabilities, the proposed model, fine-tuned\nwith multimodal instruction datasets, performs an understanding of document\nimages in a single model. Our experiments demonstrate improvement over the\nbaseline model in various document analysis tasks.\n"", '  Recent advances in training large language models (LLMs) using massive\namounts of solely textual data lead to strong generalization across many\ndomains and tasks, including document-specific tasks. Opposed to that there is\na trend to train multi-modal transformer architectures tailored for document\nunderstanding that are designed specifically to fuse textual inputs with the\ncorresponding document layout. This involves a separate fine-tuning step for\nwhich additional training data is required. At present, no document\ntransformers with comparable generalization to LLMs are available That raises\nthe question which type of model is to be preferred for document understanding\ntasks. In this paper we investigate the possibility to use purely text-based\nLLMs for document-specific tasks by using layout enrichment. We explore drop-in\nmodifications and rule-based methods to enrich purely textual LLM prompts with\nlayout information. In our experiments we investigate the effects on the\ncommercial ChatGPT model and the open-source LLM Solar. We demonstrate that\nusing our approach both LLMs show improved performance on various standard\ndocument benchmarks. In addition, we study the impact of noisy OCR and layout\nerrors, as well as the limitations of LLMs when it comes to utilizing document\nlayout. Our results indicate that layout enrichment can improve the performance\nof purely text-based LLMs for document understanding by up to 15% compared to\njust using plain document text. In conclusion, this approach should be\nconsidered for the best model choice between text-based LLM or multi-modal\ndocument transformers.\n', '  Recently, leveraging large language models (LLMs) or multimodal large\nlanguage models (MLLMs) for document understanding has been proven very\npromising. However, previous works that employ LLMs/MLLMs for document\nunderstanding have not fully explored and utilized the document layout\ninformation, which is vital for precise document understanding. In this paper,\nwe propose LayoutLLM, an LLM/MLLM based method for document understanding. The\ncore of LayoutLLM is a layout instruction tuning strategy, which is specially\ndesigned to enhance the comprehension and utilization of document layouts. The\nproposed layout instruction tuning strategy consists of two components:\nLayout-aware Pre-training and Layout-aware Supervised Fine-tuning. To capture\nthe characteristics of document layout in Layout-aware Pre-training, three\ngroups of pre-training tasks, corresponding to document-level, region-level and\nsegment-level information, are introduced. Furthermore, a novel module called\nlayout chain-of-thought (LayoutCoT) is devised to enable LayoutLLM to focus on\nregions relevant to the question and generate accurate answers. LayoutCoT is\neffective for boosting the performance of document understanding. Meanwhile, it\nbrings a certain degree of interpretability, which could facilitate manual\ninspection and correction. Experiments on standard benchmarks show that the\nproposed LayoutLLM significantly outperforms existing methods that adopt\nopen-source 7B LLMs/MLLMs for document understanding. The training data of the\nLayoutLLM is publicly available at\nhttps://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/LayoutLLM\n']",Document Understanding with Layout-Aware Language Models
479,478,12,478_translations_decoding_translation_mbr,"['translations', 'decoding', 'translation', 'mbr', 'mbrs', 'bayes', 'mbmbr', 'text', 'cbmbr', 'risk']","['decoding', 'translation', 'translations', 'hypotheses', 'utility', 'mbrs', 'risk', 'quality', 'reranking', 'probability']","['translations', 'decoding', 'mbr', 'bayes', 'reranking', 'mllms', 'quality', 'wmt22', 'ambr', 'nmt']","['  Minimum Bayes Risk (MBR) decoding has been shown to be a powerful alternative\nto beam search decoding in a variety of text generation tasks. MBR decoding\nselects a hypothesis from a pool of hypotheses that has the least expected risk\nunder a probability model according to a given utility function. Since it is\nimpractical to compute the expected risk exactly over all possible hypotheses,\ntwo approximations are commonly used in MBR. First, it integrates over a\nsampled set of hypotheses rather than over all possible hypotheses. Second, it\nestimates the probability of each hypothesis using a Monte Carlo estimator.\nWhile the first approximation is necessary to make it computationally feasible,\nthe second is not essential since we typically have access to the model\nprobability at inference time. We propose Model-Based MBR (MBMBR), a variant of\nMBR that uses the model probability itself as the estimate of the probability\ndistribution instead of the Monte Carlo estimate. We show analytically and\nempirically that the model-based estimate is more promising than the Monte\nCarlo estimate in text generation tasks. Our experiments show that MBMBR\noutperforms MBR in several text generation tasks, both with encoder-decoder\nmodels and with large language models.\n', '  Minimum Bayes Risk (MBR) decoding is a powerful decoding strategy widely used\nfor text generation tasks, but its quadratic computational complexity limits\nits practical application. This paper presents a novel approach for\napproximating MBR decoding using matrix completion techniques, focusing on the\ntask of machine translation. We formulate MBR decoding as a matrix completion\nproblem, where the utility metric scores between candidate hypotheses and\npseudo-reference translations form a low-rank matrix. First, we empirically\nshow that the scores matrices indeed have a low-rank structure. Then, we\nexploit this by only computing a random subset of the scores and efficiently\nrecover the missing entries in the matrix by applying the Alternating Least\nSquares (ALS) algorithm, thereby enabling a fast approximation of the MBR\ndecoding process. Our experimental results on machine translation tasks\ndemonstrate that the proposed method requires 1/16 utility metric computations\ncompared to vanilla MBR decoding while achieving equal translation quality\nmeasured by COMET22 on the WMT22 dataset (en<>de and en<>ru). We also benchmark\nour method against other approximation methods and we show gains in quality\nwhen comparing to them.\n', '  This paper explores Minimum Bayes Risk (MBR) decoding for self-improvement in\nmachine translation (MT), particularly for domain adaptation and low-resource\nlanguages. We implement the self-improvement process by fine-tuning the model\non its MBR-decoded forward translations. By employing COMET as the MBR utility\nmetric, we aim to achieve the reranking of translations that better aligns with\nhuman preferences. The paper explores the iterative application of this\napproach and the potential need for language-specific MBR utility metrics. The\nresults demonstrate significant enhancements in translation quality for all\nexamined language pairs, including successful application to domain-adapted\nmodels and generalisation to low-resource settings. This highlights the\npotential of COMET-guided MBR for efficient MT self-improvement in various\nscenarios.\n']",Minimum Bayes Risk Decoding for Machine Translation
480,479,12,479_classification_surrogates_prediction_surrogate,"['classification', 'surrogates', 'prediction', 'surrogate', 'losses', 'loss', 'multiclass', 'predictor', 'learner', 'minimizing']","['surrogate', 'losses', 'abstention', 'consistency', 'bounds', 'predictor', 'rejector', 'loss', 'multiclass', 'comp']","['classification', 'surrogates', 'losses', 'multiclass', 'duality', 'logistic', 'deferral', 'cost', 'minimizability', 'simplex']","['  We present a study of surrogate losses and algorithms for the general problem\nof learning to defer with multiple experts. We first introduce a new family of\nsurrogate losses specifically tailored for the multiple-expert setting, where\nthe prediction and deferral functions are learned simultaneously. We then prove\nthat these surrogate losses benefit from strong $H$-consistency bounds. We\nillustrate the application of our analysis through several examples of\npractical surrogate losses, for which we give explicit guarantees. These loss\nfunctions readily lead to the design of new learning to defer algorithms based\non their minimization. While the main focus of this work is a theoretical\nanalysis, we also report the results of several experiments on SVHN and\nCIFAR-10 datasets.\n', '  We study the key framework of learning with abstention in the multi-class\nclassification setting. In this setting, the learner can choose to abstain from\nmaking a prediction with some pre-defined cost. We present a series of new\ntheoretical and algorithmic results for this learning problem in the\npredictor-rejector framework. We introduce several new families of surrogate\nlosses for which we prove strong non-asymptotic and hypothesis set-specific\nconsistency guarantees, thereby resolving positively two existing open\nquestions. These guarantees provide upper bounds on the estimation error of the\nabstention loss function in terms of that of the surrogate loss. We analyze\nboth a single-stage setting where the predictor and rejector are learned\nsimultaneously and a two-stage setting crucial in applications, where the\npredictor is learned in a first stage using a standard surrogate loss such as\ncross-entropy. These guarantees suggest new multi-class abstention algorithms\nbased on minimizing these surrogate losses. We also report the results of\nextensive experiments comparing these algorithms to the current\nstate-of-the-art algorithms on CIFAR-10, CIFAR-100 and SVHN datasets. Our\nresults demonstrate empirically the benefit of our new surrogate losses and\nshow the remarkable performance of our broadly applicable two-stage abstention\nalgorithm.\n', '  Learning with abstention is a key scenario where the learner can abstain from\nmaking a prediction at some cost. In this paper, we analyze the score-based\nformulation of learning with abstention in the multi-class classification\nsetting. We introduce new families of surrogate losses for the abstention loss\nfunction, which include the state-of-the-art surrogate losses in the\nsingle-stage setting and a novel family of loss functions in the two-stage\nsetting. We prove strong non-asymptotic and hypothesis set-specific consistency\nguarantees for these surrogate losses, which upper-bound the estimation error\nof the abstention loss function in terms of the estimation error of the\nsurrogate loss. Our bounds can help compare different score-based surrogates\nand guide the design of novel abstention algorithms by minimizing the proposed\nsurrogate losses. We experimentally evaluate our new algorithms on CIFAR-10,\nCIFAR-100, and SVHN datasets and the practical significance of our new\nsurrogate losses and two-stage abstention algorithms. Our results also show\nthat the relative performance of the state-of-the-art score-based surrogate\nlosses can vary across datasets.\n']",Machine Learning with Abstention and Surrogate Losses
481,480,12,480_uav_unmanned_uavs_swarm,"['uav', 'unmanned', 'uavs', 'swarm', 'flying', 'swarms', 'autonomous', 'planning', 'aerial', 'aircraft']","['swarms', 'unmanned', 'flight', 'jamming', 'vehicle', 'vehicles', 'swarm', 'aerial', 'path', 'vertiport']","['uav', 'swarm', 'planning', 'flights', 'mission', 'trajectory', 'remotely', 'hopping', 'optimizer', 'ariel']","['  Unmanned Ariel Vehicle (UAV) services with 5G connectivity is an emerging\nfield with numerous applications. Operator-controlled UAV flights and manual\nstatic flight configurations are major limitations for the wide adoption of\nscalability of UAV services. Several services depend on excellent UAV\nconnectivity with a cellular network and maintaining it is challenging in\npredetermined flight paths. This paper addresses these limitations by proposing\na Deep Reinforcement Learning (DRL) framework for UAV path planning with\nassured connectivity (DUPAC). During UAV flight, DUPAC determines the best\nroute from a defined source to the destination in terms of distance and signal\nquality. The viability and performance of DUPAC are evaluated under simulated\nreal-world urban scenarios using the Unity framework. The results confirm that\nDUPAC achieves an autonomous UAV flight path similar to base method with only\n2% increment while maintaining an average 9% better connection quality\nthroughout the flight.\n', ""  This paper addresses the increasing significance of UAVs (Unmanned Aerial\nVehicles) and the emergence of UAV swarms for collaborative operations in\nvarious domains. However, the effectiveness of UAV swarms can be severely\ncompromised by jamming technology, necessitating robust antijamming strategies.\nWhile existing methods such as frequency hopping and physical path planning\nhave been explored, there remains a gap in research on path planning for UAV\nswarms when the jammer's location is unknown. To address this, a novel\napproach, where UAV swarms leverage collective intelligence to predict jamming\nareas, evade them, and efficiently reach target destinations, is proposed. This\napproach utilizes Graph Convolutional Networks (GCN) to predict the location\nand intensity of jamming areas based on information gathered from each UAV. A\nmulti-agent control algorithm is then employed to disperse the UAV swarm, avoid\njamming, and regroup upon reaching the target. Through simulations, the\neffectiveness of the proposed method is demonstrated, showcasing accurate\nprediction of jamming areas and successful evasion through obstacle avoidance\nalgorithms, ultimately achieving the mission objective. Proposed method offers\nrobustness, scalability, and computational efficiency, making it applicable\nacross various scenarios where UAV swarms operate in potentially hostile\nenvironments.\n"", '  With the impact of artificial intelligence on the traditional UAV industry,\nautonomous UAV flight has become a current hot research field. Based on the\ndemand for research on critical technologies for autonomous flying UAVs, this\npaper addresses the field of flight state recognition and trajectory prediction\nof UAVs. This paper proposes a method to improve the accuracy of UAV trajectory\nprediction based on UAV flight state recognition and verifies it using two\nprediction models. Firstly, UAV flight data acquisition and data preprocessing\nare carried out; secondly, UAV flight trajectory features are extracted based\non data fusion and a UAV flight state recognition model based on PCA-DAGSVM\nmodel is established; finally, two UAV flight trajectory prediction models are\nestablished and the trajectory prediction errors of the two prediction models\nare compared and analyzed after flight state recognition. The results show\nthat: 1) the UAV flight state recognition model based on PCA-DAGSVM has good\nrecognition effect. 2) compared with the traditional UAV trajectory prediction\nmodel, the prediction model based on flight state recognition can effectively\nreduce the prediction error.\n']","""Autonomous Unmanned Aerial Vehicles (UAVs) and Swarms"""
482,481,12,481_counterfactuals_counterfactual_interventions_counterfactual_causality,"['counterfactuals', 'counterfactual_interventions', 'counterfactual', 'causality', 'causal', 'causation', 'inference', 'incentive', 'consistency', 'semantics']","['counterfactual', 'counterfactuals', 'causal', 'structural', 'outcomes', 'curvature', 'causation', 'intervention', 'interventions', 'variables']","['counterfactual_interventions', 'causal', 'incentive', 'consistency', 'outcomes', 'contradicting', 'reframed', 'informative', 'nondeterministic', 'quantile']","['  The capacity to address counterfactual ""what if"" inquiries is crucial for\nunderstanding and making use of causal influences. Traditional counterfactual\ninference, under Pearls\' counterfactual framework, typically depends on having\naccess to or estimating a structural causal model. Yet, in practice, this\ncausal model is often unknown and might be challenging to identify. Hence, this\npaper aims to perform reliable counterfactual inference based solely on\nobservational data and the (learned) qualitative causal structure, without\nnecessitating a predefined causal model or even direct estimations of\nconditional distributions. To this end, we establish a novel connection between\ncounterfactual inference and quantile regression and show that counterfactual\ninference can be reframed as an extended quantile regression problem. Building\non this insight, we propose a practical framework for efficient and effective\ncounterfactual inference implemented with neural networks under a bi-level\noptimization scheme. The proposed approach enhances the capacity to generalize\nestimated counterfactual outcomes to unseen data, thereby providing an upper\nbound on the generalization error. Furthermore, empirical evidence demonstrates\nits superior statistical efficiency in comparison to existing methods.\nEmpirical results conducted on multiple datasets offer compelling support for\nour theoretical assertions.\n', ""  In the field of causal modeling, potential outcomes (PO) and structural\ncausal models (SCMs) stand as the predominant frameworks. However, these\nframeworks face notable challenges in practically modeling counterfactuals,\nformalized as parameters of the joint distribution of potential outcomes.\nCounterfactual reasoning holds paramount importance in contemporary\ndecision-making processes, especially in scenarios that demand personalized\nincentives based on the joint values of $(Y(0), Y(1))$. This paper begins with\nan investigation of the PO and SCM frameworks for modeling counterfactuals.\nThrough the analysis, we identify an inherent model capacity limitation, termed\nas the ``degenerative counterfactual problem'', emerging from the consistency\nrule that is the cornerstone of both frameworks. To address this limitation, we\nintroduce a novel \\textit{distribution-consistency} assumption, and in\nalignment with it, we propose the Distribution-consistency Structural Causal\nModels (DiscoSCMs) offering enhanced capabilities to model counterfactuals. To\nconcretely reveal the enhanced model capacity, we introduce a new identifiable\ncausal parameter, \\textit{the probability of consistency}, which holds\npractical significance within DiscoSCM alone, showcased with a personalized\nincentive example. Furthermore, we provide a comprehensive set of theoretical\nresults about the ``Ladder of Causation'' within the DiscoSCM framework. We\nhope it opens new avenues for future research of counterfactual modeling,\nultimately enhancing our understanding of causality and its real-world\napplications.\n"", '  In the realm of causal inference, Potential Outcomes (PO) and Structural\nCausal Models (SCM) are recognized as the principal frameworks.However, when it\ncomes to Layer 3 valuations -- counterfactual queries deeply entwined with\nindividual-level semantics -- both frameworks encounter limitations due to the\ndegenerative issues brought forth by the consistency rule. This paper advocates\nfor the Distribution-consistency Structural Causal Models (DiscoSCM) framework\nas a pioneering approach to counterfactual inference, skillfully integrating\nthe strengths of both PO and SCM. The DiscoSCM framework distinctively\nincorporates a unit selection variable $U$ and embraces the concept of\nuncontrollable exogenous noise realization. Through personalized incentive\nscenarios, we demonstrate the inadequacies of PO and SCM frameworks in\nrepresenting the probability of a user being a complier (a Layer 3 event)\nwithout degeneration, an issue adeptly resolved by adopting the assumption of\nindependent counterfactual noises within DiscoSCM. This innovative assumption\nbroadens the foundational counterfactual theory, facilitating the extension of\nnumerous theoretical results regarding the probability of causation to an\nindividual granularity level and leading to a comprehensive set of theories on\nheterogeneous counterfactual bounds. Ultimately, our paper posits that if one\nacknowledges and wishes to leverage the ubiquitous heterogeneity, understanding\ncausality as invariance across heterogeneous units, then DiscoSCM stands as a\nsignificant advancement in the methodology of counterfactual inference.\n']",Counterfactual Inference and Causal Modeling
483,482,12,482_policies_policy_estimators_unbiasedness,"['policies', 'policy', 'estimators', 'unbiasedness', 'estimation', 'unbiasedly', 'unbiased', 'evaluation', 'estimate', 'estimator']","['estimator', 'logged', 'policy', 'estimators', 'variance', 'policies', 'risk', 'counterfactual', 'propensity', 'evaluation']","['policies', 'unbiasedness', 'estimate', 'hyperparameters', 'ope', 'offline', 'regularized', 'selection', 'stakes', 'portfolios']","['  Off-Policy Evaluation (OPE) aims to assess the effectiveness of\ncounterfactual policies using only offline logged data and is often used to\nidentify the top-k promising policies for deployment in online A/B tests.\nExisting evaluation metrics for OPE estimators primarily focus on the\n""accuracy"" of OPE or that of downstream policy selection, neglecting\nrisk-return tradeoff in the subsequent online policy deployment. To address\nthis issue, we draw inspiration from portfolio evaluation in finance and\ndevelop a new metric, called SharpeRatio@k, which measures the risk-return\ntradeoff of policy portfolios formed by an OPE estimator under varying online\nevaluation budgets (k). We validate our metric in two example scenarios,\ndemonstrating its ability to effectively distinguish between low-risk and\nhigh-risk estimators and to accurately identify the most efficient one.\nEfficiency of an estimator is characterized by its capability to form the most\nadvantageous policy portfolios, maximizing returns while minimizing risks\nduring online deployment, a nuance that existing metrics typically overlook. To\nfacilitate a quick, accurate, and consistent evaluation of OPE via\nSharpeRatio@k, we have also integrated this metric into an open-source\nsoftware, SCOPE-RL (https://github.com/hakuhodo-technologies/scope-rl).\nEmploying SharpeRatio@k and SCOPE-RL, we conduct comprehensive benchmarking\nexperiments on various estimators and RL tasks, focusing on their risk-return\ntradeoff. These experiments offer several interesting directions and\nsuggestions for future OPE research.\n', '  The Off-Policy Evaluation (OPE) problem consists of evaluating the\nperformance of counterfactual policies with data collected by another one. This\nproblem is of utmost importance for various application domains, e.g.,\nrecommendation systems, medical treatments, and many others. To solve the OPE\nproblem, we resort to estimators, which aim to estimate in the most accurate\nway possible the performance that the counterfactual policies would have had if\nthey were deployed in place of the logging policy. In the literature, several\nestimators have been developed, all with different characteristics and\ntheoretical guarantees. Therefore, there is no dominant estimator, and each\nestimator may be the best one for different OPE problems, depending on the\ncharacteristics of the dataset at hand. While the selection of the estimator is\na crucial choice for an accurate OPE, this problem has been widely overlooked\nin the literature. We propose an automated data-driven OPE estimator selection\nmethod based on machine learning. In particular, the core idea we propose in\nthis paper is to create several synthetic OPE tasks and use a machine learning\nmodel trained to predict the best estimator for those synthetic tasks. We\nempirically show how our method is able to generalize to unseen tasks and make\na better estimator selection compared to a baseline method on several\nreal-world datasets, with a computational cost significantly lower than the one\nof the baseline.\n', ""  Offline policy evaluation (OPE) allows us to evaluate and estimate a new\nsequential decision-making policy's performance by leveraging historical\ninteraction data collected from other policies. Evaluating a new policy online\nwithout a confident estimate of its performance can lead to costly, unsafe, or\nhazardous outcomes, especially in education and healthcare. Several OPE\nestimators have been proposed in the last decade, many of which have\nhyperparameters and require training. Unfortunately, choosing the best OPE\nalgorithm for each task and domain is still unclear. In this paper, we propose\na new algorithm that adaptively blends a set of OPE estimators given a dataset\nwithout relying on an explicit selection using a statistical procedure. We\nprove that our estimator is consistent and satisfies several desirable\nproperties for policy evaluation. Additionally, we demonstrate that when\ncompared to alternative approaches, our estimator can be used to select\nhigher-performing policies in healthcare and robotics. Our work contributes to\nimproving ease of use for a general-purpose, estimator-agnostic, off-policy\nevaluation framework for offline RL.\n""]",Off-Policy Evaluation and Estimation in Decision Making
484,483,12,483_mentalizing_minds_mind_semantics,"['mentalizing', 'minds', 'mind', 'semantics', 'reasoning', 'cognition', 'mental', 'inferences', 'cognitive', 'dialogs']","['beliefs', 'mind', 'belief', 'mental', 'theory', 'states', 'adult', 'social', 'reasoning', 'mentalizing']","['mentalizing', 'semantics', 'cognition', 'inferences', 'dialogs', 'tom', 'representations', 'elicits', 'epistemic', 'lms']","['  Theory of Mind (ToM) refers to the ability of individuals to attribute mental\nstates to others. While Large Language Models (LLMs) have shown some promise\nwith ToM ability, they still struggle with complex ToM reasoning. Our approach\nleverages an external symbolic executor, specifically the SMCDEL model checker,\nand fine-tuning to improve the ToM reasoning ability of LLMs. In our approach,\nan LLM is first fine-tuned through pairs of natural language and symbolic\nformulation representation of ToM problems and is then instructed to generate\nthe symbolic formulation with a one-shot in-context example. The generated\nsymbolic formulation is then executed by the SMCDEL model checker to perform\ntransparent and verifiable ToM reasoning and give the final result. We\ndemonstrate that our approach, ToM-LM, shows a significant improvement over all\nthe constructed baselines. Our study proposes a novel view about externalizing\na particular component of ToM reasoning, mainly reasoning about beliefs, and\nsuggests generalizing it to other aspects of ToM reasoning.\n', '  Evaluating the theory of mind (ToM) capabilities of language models (LMs) has\nrecently received a great deal of attention. However, many existing benchmarks\nrely on synthetic data, which risks misaligning the resulting experiments with\nhuman behavior. We introduce the first ToM dataset based on naturally occurring\nspoken dialogs, Common-ToM, and show that LMs struggle to demonstrate ToM. We\nthen show that integrating a simple, explicit representation of beliefs\nimproves LM performance on Common-ToM.\n', ""  Theory of Mind (ToM)-the cognitive ability to reason about mental states of\nourselves and others, is the foundation of social interaction. Although ToM\ncomes naturally to humans, it poses a significant challenge to even the most\nadvanced Large Language Models (LLMs). Due to the complex logical chains in ToM\nreasoning, especially in higher-order ToM questions, simply utilizing reasoning\nmethods like Chain of Thought (CoT) will not improve the ToM capabilities of\nLLMs. We present TimeToM, which constructs a temporal space and uses it as the\nfoundation to improve the ToM capabilities of LLMs in multiple scenarios.\nSpecifically, within the temporal space, we construct Temporal Belief State\nChain (TBSC) for each character and inspired by the cognition perspective of\nthe social world model, we divide TBSC into self-world beliefs and social world\nbeliefs, aligning with first-order ToM (first-order beliefs) and higher-order\nToM (higher-order beliefs) questions, respectively. Moreover, we design a novel\ntool-belief solver that, by considering belief communication between characters\nin temporal space, can transform a character's higher-order beliefs into\nanother character's first-order beliefs under belief communication period.\nExperimental results indicate that TimeToM can dramatically improve the\nreasoning performance of LLMs on ToM questions while taking a big step towards\ncoherent and robust ToM reasoning.\n""]",Theory of Mind in Artificial Intelligence
485,484,12,484_diarization_speaker_speech_audio,"['diarization', 'speaker', 'speech', 'audio', 'transcription', 'acoustically', 'speakers', 'segmentation', 'conversations', 'recording']","['diarization', 'speaker', 'latency', 'audio', 'clustering', 'speech', 'speakers', 'pyannote', 'end', 'online']","['transcription', 'acoustically', 'segmentation', 'conversations', 'corpora', 'embeddings', 'diart', 'misclassify', 'latency', 'spectral']","['  End-to-end neural speaker diarization systems are able to address the speaker\ndiarization task while effectively handling speech overlap. This work explores\nthe incorporation of speaker information embeddings into the end-to-end systems\nto enhance the speaker discriminative capabilities, while maintaining their\noverlap handling strengths. To achieve this, we propose several methods for\nincorporating these embeddings along the acoustic features. Furthermore, we\ndelve into an analysis of the correct handling of silence frames, the window\nlength for extracting speaker embeddings and the transformer encoder size. The\neffectiveness of our proposed approach is thoroughly evaluated on the CallHome\ndataset for the two-speaker diarization task, with results that demonstrate a\nsignificant reduction in diarization error rates achieving a relative\nimprovement of a 10.78% compared to the baseline end-to-end model.\n', ""  Speaker diarization has gained considerable attention within speech\nprocessing research community. Mainstream speaker diarization rely primarily on\nspeakers' voice characteristics extracted from acoustic signals and often\noverlook the potential of semantic information. Considering the fact that\nspeech signals can efficiently convey the content of a speech, it is of our\ninterest to fully exploit these semantic cues utilizing language models. In\nthis work we propose a novel approach to effectively leverage semantic\ninformation in clustering-based speaker diarization systems. Firstly, we\nintroduce spoken language understanding modules to extract speaker-related\nsemantic information and utilize these information to construct pairwise\nconstraints. Secondly, we present a novel framework to integrate these\nconstraints into the speaker diarization pipeline, enhancing the performance of\nthe entire system. Extensive experiments conducted on the public dataset\ndemonstrate the consistent superiority of our proposed approach over\nacoustic-only speaker diarization systems.\n"", '  Speaker diarization provides the answer to the question ""who spoke when?"" for\nan audio file. This information can be used to complete audio transcripts for\nfurther processing steps. Most speaker diarization systems assume that the\naudio file is available as a whole. However, there are scenarios in which the\nspeaker labels are needed immediately after the arrival of an audio segment.\nSpeaker diarization with a correspondingly low latency is referred to as online\nspeaker diarization. This paper provides an overview. First the history of\nonline speaker diarization is briefly presented. Next a taxonomy and datasets\nfor training and evaluation are given. In the sections that follow, online\ndiarization methods and systems are discussed in detail. This paper concludes\nwith the presentation of challenges that still need to be solved by future\nresearch in the field of online speaker diarization.\n']",Speaker Diarization in Audio Recordings
486,485,12,485_language_generalization_representational_representations,"['language', 'generalization', 'representational', 'representations', 'numbers', 'digits', 'addition', 'numeric', 'digit', 'gram']","['digit', 'numeric', 'arithmetic', 'numbers', 'transformers', 'additions', 'birthyear', 'mechanisms', 'generalization', 'unseen']","['language', 'generalization', 'representations', 'gram', 'arithmetic', 'primitives', 'hundreds', 'lms', 'addends', 'strings']","['  Large language models (LLMs) have achieved remarkable proficiency on solving\ndiverse problems. However, their generalization ability is not always\nsatisfying and the generalization problem is common for generative transformer\nmodels in general. Researchers take basic mathematical tasks like n-digit\naddition or multiplication as important perspectives for investigating their\ngeneralization behaviors. It is observed that when training models on n-digit\noperations (e.g., additions) in which both input operands are n-digit in\nlength, models generalize successfully on unseen n-digit inputs\n(in-distribution (ID) generalization), but fail miserably on longer, unseen\ncases (out-of-distribution (OOD) generalization). We bring this unexplained\nperformance drop into attention and ask whether there is systematic OOD\ngeneralization. Towards understanding LLMs, we train various smaller language\nmodels which may share the same underlying mechanism. We discover that the\nstrong ID generalization stems from structured representations, while behind\nthe unsatisfying OOD performance, the models still exhibit clear learned\nalgebraic structures. Specifically, these models map unseen OOD inputs to\noutputs with learned equivalence relations in the ID domain, which we call the\nequivalence generalization. These findings deepen our knowledge regarding the\ngeneralizability of generative models including LLMs, and provide insights into\npotential avenues for improvement.\n', '  Large language models (LLMs) have demonstrated impressive versatility across\nnumerous tasks, yet their generalization capabilities remain poorly understood.\nTo investigate these behaviors, arithmetic tasks serve as important venues. In\nprevious studies, seemingly unrelated mysteries still exist -- (1) models with\nappropriate positional embeddings can correctly perform longer unseen\narithmetic operations such as addition, but their effectiveness varies in more\ncomplex tasks like multiplication; (2) models perform well for longer unseen\ncases in modular addition under specific moduli (e.g., modulo 100) but struggle\nunder very close moduli (e.g., modulo 101), regardless of the positional\nencoding used. We believe previous studies have been treating the symptoms\nrather than addressing the root cause -- they have paid excessive attention to\nimproving model components, while overlooking the differences in task\nproperties that may be the real drivers. This is confirmed by our unified\ntheoretical framework for different arithmetic scenarios. For example, unlike\nmultiplication, the digital addition task has the property of translation\ninvariance which naturally aligns with the relative positional encoding, and\nthis combination leads to successful generalization of addition to unseen\nlonger domains. The discrepancy in operations modulo 100 and 101 arises from\nthe base. Modulo 100, unlike 101, is compatible with the decimal system (base\n10), such that unseen information in digits beyond the units digit and the tens\ndigit is actually not needed for the task. Extensive experiments with GPT-like\nmodels validate our theoretical predictions. These findings deepen our\nunderstanding of the generalization mechanisms, and facilitate more\ndata-efficient model training and objective-oriented AI alignment.\n', '  Language models (LMs) can express factual knowledge involving numeric\nproperties such as Karl Popper was born in 1902. However, how this information\nis encoded in the model\'s internal representations is not understood well.\nHere, we introduce a simple method for finding and editing representations of\nnumeric properties such as an entity\'s birth year. Empirically, we find\nlow-dimensional subspaces that encode numeric properties monotonically, in an\ninterpretable and editable fashion. When editing representations along\ndirections in these subspaces, LM output changes accordingly. For example, by\npatching activations along a ""birthyear"" direction we can make the LM express\nan increasingly late birthyear: Karl Popper was born in 1929, Karl Popper was\nborn in 1957, Karl Popper was born in 1968. Property-encoding directions exist\nacross several numeric properties in all models under consideration, suggesting\nthe possibility that monotonic representation of numeric properties\nconsistently emerges during LM pretraining. Code:\nhttps://github.com/bheinzerling/numeric-property-repr\n']",Language Model Generalization and Representation of Numbers
487,486,12,486_tweets_twitter_sentiment_vaccinations,"['tweets', 'twitter', 'sentiment', 'vaccinations', 'vaccination', 'pandemic', 'vaccine', 'coronavirus', 'covid', 'vaccines']","['vaccination', 'media', 'vaccine', 'vaccines', 'public', 'stances', 'pandemic', 'social', 'tweets', 'misinformation']","['twitter', 'vaccinations', 'pandemic', 'coronavirus', 'trends', 'attitudes', 'newspaper', 'topics', 'policymakers', 'fda']","['  During the COVID-19 pandemic, the news media coverage encompassed a wide\nrange of topics that includes viral transmission, allocation of medical\nresources, and government response measures. There have been studies on\nsentiment analysis of social media platforms during COVID-19 to understand the\npublic response given the rise of cases and government strategies implemented\nto control the spread of the virus. Sentiment analysis can provide a better\nunderstanding of changes in societal opinions and emotional trends during the\npandemic. Apart from social media, newspapers have played a vital role in the\ndissemination of information, including information from the government,\nexperts, and also the public about various topics. A study of sentiment\nanalysis of newspaper sources during COVID-19 for selected countries can give\nan overview of how the media covered the pandemic. In this study, we select The\nGuardian newspaper and provide a sentiment analysis during various stages of\nCOVID-19 that includes initial transmission, lockdowns and vaccination. We\nemploy novel large language models (LLMs) and refine them with expert-labelled\nsentiment analysis data. We also provide an analysis of sentiments experienced\npre-pandemic for comparison. The results indicate that during the early\npandemic stages, public sentiment prioritised urgent crisis response, later\nshifting focus to addressing the impact on health and the economy. In\ncomparison with related studies about social media sentiment analyses, we found\na discrepancy between The Guardian with dominance of negative sentiments (sad,\nannoyed, anxious and denial), suggesting that social media offers a more\ndiversified emotional reflection. We found a grim narrative in The Guardian\nwith overall dominance of negative sentiments, pre and during COVID-19 across\nnews sections including Australia, UK, World News, and Opinion\n', ""  The Covid-19 pandemic had an enormous effect on our lives, especially on\npeople's interactions. By introducing Covid-19 vaccines, both positive and\nnegative opinions were raised over the subject of taking vaccines or not. In\nthis paper, using data gathered from Twitter, including tweets and user\nprofiles, we offer a comprehensive analysis of public opinion in Iran about the\nCoronavirus vaccines. For this purpose, we applied a search query technique\ncombined with a topic modeling approach to extract vaccine-related tweets. We\nutilized transformer-based models to classify the content of the tweets and\nextract themes revolving around vaccination. We also conducted an emotion\nanalysis to evaluate the public happiness and anger around this topic. Our\nresults demonstrate that Covid-19 vaccination has attracted considerable\nattention from different angles, such as governmental issues, safety or\nhesitancy, and side effects. Moreover, Coronavirus-relevant phenomena like\npublic vaccination and the rate of infection deeply impacted public emotional\nstatus and users' interactions.\n"", ""  Previous studies have highlighted the importance of vaccination as an\neffective strategy to control the transmission of the COVID-19 virus. It is\ncrucial for policymakers to have a comprehensive understanding of the public's\nstance towards vaccination on a large scale. However, attitudes towards\nCOVID-19 vaccination, such as pro-vaccine or vaccine hesitancy, have evolved\nover time on social media. Thus, it is necessary to account for possible\ntemporal shifts when analysing these stances. This study aims to examine the\nimpact of temporal concept drift on stance detection towards COVID-19\nvaccination on Twitter. To this end, we evaluate a range of transformer-based\nmodels using chronological (splitting the training, validation, and test sets\nin order of time) and random splits (randomly splitting these three sets) of\nsocial media data. Our findings reveal significant discrepancies in model\nperformance between random and chronological splits in several existing\nCOVID-19-related datasets; specifically, chronological splits significantly\nreduce the accuracy of stance classification. Therefore, real-world stance\ndetection approaches need to be further refined to incorporate temporal factors\nas a key consideration.\n""]",Public Sentiment on COVID-19 Vaccination
488,487,12,487_coreference_annotation_ontonotes_texts,"['coreference', 'annotation', 'ontonotes', 'texts', 'corpora', 'translations', 'multilingual', 'nlp', 'referring', 'mentions']","['coreference', 'resolution', 'mentions', 'literary', 'mention', 'singleton', 'entity', 'discourse', 'rhetorical', 'singletons']","['coreference', 'annotation', 'ontonotes', 'texts', 'multilingual', 'mentions', 'parses', 'speeches', 'anaphora', 'transmucores']","[""  In this paper, we present KoCoNovel, a novel character coreference dataset\nderived from Korean literary texts, complete with detailed annotation\nguidelines. Comprising 178K tokens from 50 modern and contemporary novels,\nKoCoNovel stands as one of the largest public coreference resolution corpora in\nKorean, and the first to be based on literary texts. KoCoNovel offers four\ndistinct versions to accommodate a wide range of literary coreference analysis\nneeds. These versions are designed to support perspectives of the omniscient\nauthor or readers, and to manage multiple entities as either separate or\noverlapping, thereby broadening its applicability. One of KoCoNovel's\ndistinctive features is that 24% of all character mentions are single common\nnouns, lacking possessive markers or articles. This feature is particularly\ninfluenced by the nuances of Korean address term culture, which favors the use\nof terms denoting social relationships and kinship over personal names. In\nexperiments with a BERT-based coreference model, we observe notable performance\nenhancements with KoCoNovel in character coreference tasks within literary\ntexts, compared to a larger non-literary coreference dataset. Such findings\nunderscore KoCoNovel's potential to significantly enhance coreference\nresolution models through the integration of Korean cultural and linguistic\ndynamics.\n"", '  Coreference resolution is the task of identifying and grouping mentions\nreferring to the same real-world entity. Previous neural models have mainly\nfocused on learning span representations and pairwise scores for coreference\ndecisions. However, current methods do not explicitly capture the referential\nchoice in the hierarchical discourse, an important factor in coreference\nresolution. In this study, we propose a new approach that incorporates\nrhetorical information into neural coreference resolution models. We collect\nrhetorical features from automated discourse parses and examine their impact.\nAs a base model, we implement an end-to-end span-based coreference resolver\nusing a partially fine-tuned multilingual entity-aware language model LUKE. We\nevaluate our method on the RuCoCo-23 Shared Task for coreference resolution in\nRussian. Our best model employing rhetorical distance between mentions has\nranked 1st on the development set (74.6% F1) and 2nd on the test set (73.3% F1)\nof the Shared Task. We hope that our work will inspire further research on\nincorporating discourse information in neural coreference resolution models.\n', '  Coreference resolution involves the task of identifying text spans within a\ndiscourse that pertain to the same real-world entity. While this task has been\nextensively explored in the English language, there has been a notable scarcity\nof publicly accessible resources and models for coreference resolution in South\nAsian languages. We introduce a Translated dataset for Multilingual Coreference\nResolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools\nfor translation and word-alignment. Nearly all of the predicted translations\nsuccessfully pass a sanity check, and 75% of English references align with\ntheir predicted translations. Using multilingual encoders, two off-the-shelf\ncoreference resolution models were trained on a concatenation of TransMuCoRes\nand a Hindi coreference resolution dataset with manual annotations. The best\nperforming model achieved a score of 64 and 68 for LEA F1 and CoNLL F1,\nrespectively, on our test-split of Hindi golden set. This study is the first to\nevaluate an end-to-end coreference resolution model on a Hindi golden set.\nFurthermore, this work underscores the limitations of current coreference\nevaluation metrics when applied to datasets with split antecedents, advocating\nfor the development of more suitable evaluation metrics.\n']",Coreference Resolution in Multilingual Texts
489,488,12,488_changepoints_changepoint_observations_detection,"['changepoints', 'changepoint', 'observations', 'detection', 'detecting', 'cusum', 'detect', 'outliers', 'changes', 'cumulative']","['change', 'point', 'changes', 'detection', 'abrupt', 'changepoint', 'streams', 'points', 'changepoints', 'cumulative']","['changepoints', 'observations', 'cusum', 'detect', 'outliers', 'cumulative', 'streams', 'qcd', 'arima', 'alarms']","[""  The objective of change point detection is to identify abrupt changes at\npotentially multiple points within a data sequence. This task is particularly\nchallenging in the online setting where various types of changes can occur,\nincluding shifts in both the marginal and joint distributions of the data. This\npaper tackles these challenges by sequentially tracking correlation matrices on\nthe Riemannian geometry, where the geodesic distances accurately capture the\ndevelopment of correlations. We propose Rio-CPD, a non-parametric\ncorrelation-aware online change point detection framework that combines the\nRiemannian geometry of the manifold of symmetric positive definite matrices and\nthe cumulative sum statistic (CUSUM) for detecting change points. Rio-CPD\nenhances CUSUM by computing the geodesic distance from present observations to\nthe Fr\\'echet mean of previous observations. With careful choice of metrics\nequipped to the Riemannian geometry, Rio-CPD is simple and computationally\nefficient. Experimental results on both synthetic and real-world datasets\ndemonstrate that Rio-CPD outperforms existing methods in detection accuracy and\nefficiency.\n"", '  In the problem of quickest change detection (QCD), a change occurs at some\nunknown time in the distribution of a sequence of independent observations.\nThis work studies a QCD problem where the change is either a bad change, which\nwe aim to detect, or a confusing change, which is not of our interest. Our\nobjective is to detect a bad change as quickly as possible while avoiding\nraising a false alarm for pre-change or a confusing change. We identify a\nspecific set of pre-change, bad change, and confusing change distributions that\npose challenges beyond the capabilities of standard Cumulative Sum (CuSum)\nprocedures. Proposing novel CuSum-based detection procedures, S-CuSum and\nJ-CuSum, leveraging two CuSum statistics, we offer solutions applicable across\nall kinds of pre-change, bad change, and confusing change distributions. For\nboth S-CuSum and J-CuSum, we provide analytical performance guarantees and\nvalidate them by numerical results. Furthermore, both procedures are\ncomputationally efficient as they only require simple recursive updates.\n', '  Change-point detection, detecting an abrupt change in the data distribution\nfrom sequential data, is a fundamental problem in statistics and machine\nlearning. CUSUM is a popular statistical method for online change-point\ndetection due to its efficiency from recursive computation and constant memory\nrequirement, and it enjoys statistical optimality. CUSUM requires knowing the\nprecise pre- and post-change distribution. However, post-change distribution is\nusually unknown a priori since it represents anomaly and novelty. Classic CUSUM\ncan perform poorly when there is a model mismatch with actual data. While\nlikelihood ratio-based methods encounter challenges facing high dimensional\ndata, neural networks have become an emerging tool for change-point detection\nwith computational efficiency and scalability. In this paper, we introduce a\nneural network CUSUM (NN-CUSUM) for online change-point detection. We also\npresent a general theoretical condition when the trained neural networks can\nperform change-point detection and what losses can achieve our goal. We further\nextend our analysis by combining it with the Neural Tangent Kernel theory to\nestablish learning guarantees for the standard performance metrics, including\nthe average run length (ARL) and expected detection delay (EDD). The strong\nperformance of NN-CUSUM is demonstrated in detecting change-point in\nhigh-dimensional data using both synthetic and real-world data.\n']",Change Point Detection Methods
490,489,12,489_dialogue_dialogues_conversational_conversation,"['dialogue', 'dialogues', 'conversational', 'conversation', 'persona', 'personachat', 'conversations', 'personas', 'personalizing', 'profiles']","['persona', 'dialogue', 'personas', 'personalized', 'commonsense', 'extraction', 'responses', 'conversations', 'response', 'dialogues']","['dialogue', 'personachat', 'personalizing', 'profiles', 'responses', 'topics', 'nli', 'supplementary', 'portrayal', 'generation']","['  Persona-based dialogue systems aim to generate consistent responses based on\nhistorical context and predefined persona. Unlike conventional dialogue\ngeneration, the persona-based dialogue needs to consider both dialogue context\nand persona, posing a challenge for coherent training. Specifically, this\nrequires a delicate weight balance between context and persona. To achieve\nthat, in this paper, we propose an effective framework with Persona-Adaptive\nAttention (PAA), which adaptively integrates the weights from the persona and\ncontext information via our designed attention. In addition, a dynamic masking\nmechanism is applied to the PAA to not only drop redundant information in\ncontext and persona but also serve as a regularization mechanism to avoid\noverfitting. Experimental results demonstrate the superiority of the proposed\nPAA framework compared to the strong baselines in both automatic and human\nevaluation. Moreover, the proposed PAA approach can perform equivalently well\nin a low-resource regime compared to models trained in a full-data setting,\nwhich achieve a similar result with only 20% to 30% of data compared to the\nlarger models trained in the full-data setting. To fully exploit the\neffectiveness of our design, we designed several variants for handling the\nweighted information in different ways, showing the necessity and sufficiency\nof our weighting and masking designs.\n', ""  Providing emotional support through dialogue systems is becoming increasingly\nimportant in today's world, as it can support both mental health and social\ninteractions in many conversation scenarios. Previous works have shown that\nusing persona is effective for generating empathetic and supportive responses.\nThey have often relied on pre-provided persona rather than inferring them\nduring conversations. However, it is not always possible to obtain a user\npersona before the conversation begins. To address this challenge, we propose\nPESS (Persona Extraction through Semantic Similarity), a novel framework that\ncan automatically infer informative and consistent persona from dialogues. We\ndevise completeness loss and consistency loss based on semantic similarity\nscores. The completeness loss encourages the model to generate missing persona\ninformation, and the consistency loss guides the model to distinguish between\nconsistent and inconsistent persona. Our experimental results demonstrate that\nhigh-quality persona information inferred by PESS is effective in generating\nemotionally supportive responses.\n"", '  While valuable datasets such as PersonaChat provide a foundation for training\npersona-grounded dialogue agents, they lack diversity in conversational and\nnarrative settings, primarily existing in the ""real"" world. To develop dialogue\nagents with unique personas, models are trained to converse given a specific\npersona, but hand-crafting these persona can be time-consuming, thus methods\nexist to automatically extract persona information from existing\ncharacter-specific dialogue. However, these persona-extraction models are also\ntrained on datasets derived from PersonaChat and struggle to provide\nhigh-quality persona information from conversational settings that do not take\nplace in the real world, such as the fantasy-focused dataset, LIGHT. Creating\nnew data to train models on a specific setting is human-intensive, thus\nprohibitively expensive. To address both these issues, we introduce a natural\nlanguage inference method for post-hoc adapting a trained persona extraction\nmodel to a new setting. We draw inspiration from the literature of dialog\nnatural language inference (NLI), and devise NLI-reranking methods to extract\nstructured persona information from dialogue. Compared to existing persona\nextraction models, our method returns higher-quality extracted persona and\nrequires less human annotation.\n']",Persona-Based Dialogue Systems
491,490,12,490_privacy_private_randomization_federated,"['privacy', 'private', 'randomization', 'federated', 'distributed', 'udp', 'randomness', 'protection', 'protect', 'quantization']","['privacy', 'silo', 'utility', 'differential', 'protection', 'record', 'guarantees', 'noise', 'randomness', 'server']","['privacy', 'federated', 'udp', 'randomness', 'protect', 'quantization', 'leakage', 'ldp', 'dropouts', 'minimax']","['  Federated Learning (FL) with Secure Aggregation (SA) has gained significant\nattention as a privacy preserving framework for training machine learning\nmodels while preventing the server from learning information about users\' data\nfrom their individual encrypted model updates. Recent research has extended\nprivacy guarantees of FL with SA by bounding the information leakage through\nthe aggregate model over multiple training rounds thanks to leveraging the\n""noise"" from other users\' updates. However, the privacy metric used in that\nwork (mutual information) measures the on-average privacy leakage, without\nproviding any privacy guarantees for worse-case scenarios. To address this, in\nthis work we study the conditions under which FL with SA can provide worst-case\ndifferential privacy guarantees. Specifically, we formally identify the\nnecessary condition that SA can provide DP without addition noise. We then\nprove that when the randomness inside the aggregated model update is Gaussian\nwith non-singular covariance matrix, SA can provide differential privacy\nguarantees with the level of privacy $\\epsilon$ bounded by the reciprocal of\nthe minimum eigenvalue of the covariance matrix. However, we further\ndemonstrate that in practice, these conditions are almost unlikely to hold and\nhence additional noise added in model updates is still required in order for SA\nin FL to achieve DP. Lastly, we discuss the potential solution of leveraging\ninherent randomness inside aggregated model update to reduce the amount of\naddition noise required for DP guarantee.\n', ""  Federated Learning (FL) is an emerging paradigm that holds great promise for\nprivacy-preserving machine learning using distributed data. To enhance privacy,\nFL can be combined with Differential Privacy (DP), which involves adding\nGaussian noise to the model weights. However, FL faces a significant challenge\nin terms of large communication overhead when transmitting these model weights.\nTo address this issue, quantization is commonly employed. Nevertheless, the\npresence of quantized Gaussian noise introduces complexities in understanding\nprivacy protection. This research paper investigates the impact of quantization\non privacy in FL systems. We examine the privacy guarantees of quantized\nGaussian mechanisms using R\\'enyi Differential Privacy (RDP). By deriving the\nprivacy budget of quantized Gaussian mechanisms, we demonstrate that lower\nquantization bit levels provide improved privacy protection. To validate our\ntheoretical findings, we employ Membership Inference Attacks (MIA), which gauge\nthe accuracy of privacy leakage. The numerical results align with our\ntheoretical analysis, confirming that quantization can indeed enhance privacy\nprotection. This study not only enhances our understanding of the correlation\nbetween privacy and communication in FL but also underscores the advantages of\nquantization in preserving privacy.\n"", '  Differentially private distributed mean estimation (DP-DME) is a fundamental\nbuilding block in privacy-preserving federated learning, where a central server\nestimates the mean of $d$-dimensional vectors held by $n$ users while ensuring\n$(\\epsilon,\\delta)$-DP. Local differential privacy (LDP) and distributed DP\nwith secure aggregation (SecAgg) are the most common notions of DP used in\nDP-DME settings with an untrusted server. LDP provides strong resilience to\ndropouts, colluding users, and malicious server attacks, but suffers from poor\nutility. In contrast, SecAgg-based DP-DME achieves an $O(n)$ utility gain over\nLDP in DME, but requires increased communication and computation overheads and\ncomplex multi-round protocols to handle dropouts and malicious attacks. In this\nwork, we propose CorDP-DME, a novel DP-DME mechanism that spans the gap between\nDME with LDP and distributed DP, offering a favorable balance between utility\nand resilience to dropout and collusion. CorDP-DME is based on correlated\nGaussian noise, ensuring DP without the perfect conditional privacy guarantees\nof SecAgg-based approaches. We provide an information-theoretic analysis of\nCorDP-DME, and derive theoretical guarantees for utility under any given\nprivacy parameters and dropout/colluding user thresholds. Our results\ndemonstrate that (anti) correlated Gaussian DP mechanisms can significantly\nimprove utility in mean estimation tasks compared to LDP -- even in adversarial\nsettings -- while maintaining better resilience to dropouts and attacks\ncompared to distributed DP.\n']",Federated Learning and Differential Privacy
492,491,12,491_poetry2image_poetry_poems_poets,"['poetry2image', 'poetry', 'poems', 'poets', 'rhyme', 'rhyming', 'poem', 'syllables', 'literary', 'poetic']","['poetry', 'poems', 'poetic', 'poem', 'rhyme', 'lyric', 'luc', 'meter', 'translation', 'genres']","['poetry2image', 'rhyming', 'nlg', 'alliteration', 'lyric', 'genres', 'write', 'subwords', 'tokenization', 'shakespeare']","[""  Large language models (LLMs) can now generate and recognize text in a wide\nrange of styles and genres, including highly specialized, creative genres like\npoetry. But what do LLMs really know about poetry? What can they know about\npoetry? We develop a task to evaluate how well LLMs recognize a specific aspect\nof poetry, poetic form, for more than 20 forms and formal elements in the\nEnglish language. Poetic form captures many different poetic features,\nincluding rhyme scheme, meter, and word or line repetition. We use this task to\nreflect on LLMs' current poetic capabilities, as well as the challenges and\npitfalls of creating NLP benchmarks for poetry and for other creative tasks. In\nparticular, we use this task to audit and reflect on the poems included in\npopular pretraining datasets. Our findings have implications for NLP\nresearchers interested in model evaluation, digital humanities and cultural\nanalytics scholars, and cultural heritage professionals.\n"", '  Natural Language Generation (NLG), and more generally generative AI, are\namong the currently most impactful research fields. Creative NLG, such as\nautomatic poetry generation, is a fascinating niche in this area. While most\nprevious research has focused on forms of the Turing test when evaluating\nautomatic poetry generation - can humans distinguish between automatic and\nhuman generated poetry - we evaluate the diversity of automatically generated\npoetry, by comparing distributions of generated poetry to distributions of\nhuman poetry along structural, lexical, semantic and stylistic dimensions,\nassessing different model types (word vs. character-level, general purpose LLMs\nvs. poetry-specific models), including the very recent LLaMA3, and types of\nfine-tuning (conditioned vs. unconditioned). We find that current automatic\npoetry systems are considerably underdiverse along multiple dimensions - they\noften do not rhyme sufficiently, are semantically too uniform and even do not\nmatch the length distribution of human poetry. Our experiments reveal, however,\nthat style-conditioning and character-level modeling clearly increases\ndiversity across virtually all dimensions we explore. Our identified\nlimitations may serve as the basis for more genuinely diverse future poetry\ngeneration models.\n', '  Text-to-image generation models often struggle with key element loss or\nsemantic confusion in tasks involving Chinese classical poetry.Addressing this\nissue through fine-tuning models needs considerable training costs.\nAdditionally, manual prompts for re-diffusion adjustments need professional\nknowledge. To solve this problem, we propose Poetry2Image, an iterative\ncorrection framework for images generated from Chinese classical poetry.\nUtilizing an external poetry dataset, Poetry2Image establishes an automated\nfeedback and correction loop, which enhances the alignment between poetry and\nimage through image generation models and subsequent re-diffusion modifications\nsuggested by large language models (LLM). Using a test set of 200 sentences of\nChinese classical poetry, the proposed method--when integrated with five\npopular image generation models--achieves an average element completeness of\n70.63%, representing an improvement of 25.56% over direct image generation. In\ntests of semantic correctness, our method attains an average semantic\nconsistency of 80.09%. The study not only promotes the dissemination of ancient\npoetry culture but also offers a reference for similar non-fine-tuning methods\nto enhance LLM generation.\n']",Poetry Generation and Analysis
493,492,11,492_astroinformatics_astronomy_astronomical_metadata,"['astroinformatics', 'astronomy', 'astronomical', 'metadata', 'astrophysical', 'curated', 'astropt', 'datasets', 'scholarly', 'dataset']","['astronomy', 'astronomical', 'webinar', 'metadata', 'adherence', 'universe', 'scientific', 'abstracts', 'presidency', 'observations']","['astroinformatics', 'astronomy', 'metadata', 'curated', 'astropt', 'datasets', 'galaxies', 'universetbd', 'scientists', '2023']","['  Policy Brief on ""Long Term Space Data and Informatics Needs"", distilled from\nthe corresponding panel that was part of the discussions during S20 Policy\nWebinar on Astroinformatics for Sustainable Development held on 6-7 July 2023.\n  Persistent space data gathering, retention, transmission, and analysis play a\npivotal role in deepening our grasp of the Universe and fostering the\nachievement of global sustainable development goals. Long-term data storage and\ncuration is crucial not only to make the wide range of burgeoning data sets\navailable to the global science community, but also to stabilize those data\nsets, enabling new science in the future to analyse long-term trends over\nunprecedented time spans. In addition to this, over the long-term, the\nimperative to store all data on the ground should be ameliorated by use of\nspace-based data stores --maintained and seen to be as reliable as any other\ndata archive. This concept is sometimes referred to as Memory of the Sky.\nStoring the data must be accompanied by the ability to analyse them. Several\nconcepts covered below acknowledge roots and inspiration based in the Virtual\nObservatory effort. Within this policy document, we delve into the complexities\nsurrounding the long-term utilization of space data and informatics, shedding\nlight on the challenges and opportunities inherent in this endeavour. Further,\nwe present a series of pragmatic recommendations designed to address these\nchallenges proactively.\n  The policy webinar took place during the G20 presidency in India (2023). A\nsummary based on the seven panels can be found here: arxiv:2401.04623.\n', '  We present a comprehensive evaluation of proprietary and open-weights large\nlanguage models using the first astronomy-specific benchmarking dataset. This\ndataset comprises 4,425 multiple-choice questions curated from the Annual\nReview of Astronomy and Astrophysics, covering a broad range of astrophysical\ntopics. Our analysis examines model performance across various astronomical\nsubfields and assesses response calibration, crucial for potential deployment\nin research environments. Claude-3.5-Sonnet outperforms competitors by up to\n4.6 percentage points, achieving 85.0% accuracy. For proprietary models, we\nobserved a universal reduction in cost every 3-to-12 months to achieve similar\nscore in this particular astronomy benchmark. Open-source models have rapidly\nimproved, with LLaMA-3-70b (80.6%) and Qwen-2-72b (77.7%) now competing with\nsome of the best proprietary models. We identify performance variations across\ntopics, with non-English-focused models generally struggling more in\nexoplanet-related fields, stellar astrophysics, and instrumentation related\nquestions. These challenges likely stem from less abundant training data,\nlimited historical context, and rapid recent developments in these areas. This\npattern is observed across both open-weights and proprietary models, with\nregional dependencies evident, highlighting the impact of training data\ndiversity on model performance in specialized scientific domains.\nTop-performing models demonstrate well-calibrated confidence, with correlations\nabove 0.9 between confidence and correctness, though they tend to be slightly\nunderconfident. The development for fast, low-cost inference of open-weights\nmodels presents new opportunities for affordable deployment in astronomy. The\nrapid progress observed suggests that LLM-driven research in astronomy may\nbecome feasible in the near future.\n', '  Policy Brief on ""Global Data in Astronomy: Challenges and Opportunities"",\ndistilled from the corresponding panel that was part of the discussions during\nS20 Policy Webinar on Astroinformatics for Sustainable Development held on 6-7\nJuly 2023.\n  Astronomy is increasingly becoming a data-driven science. Advances in our\nunderstanding of the physical mechanisms at work in the Universe require\nbuilding ever-more sensitive telescopes to gather observations of the cosmos to\ntest and advance our theoretical models of how the universe works. To confront\nthe observed data with our theoretical models we require data hosting,\narchiving and storage and high-performance computing resources to run the\ntheoretical calculations and compare our simulated and observed universe. We\nalso require the sophisticated development of highly skilled human resources.\nNewer large projects are often run through international collaborations and\npartnerships, driving a need for \'open science\' and collaborative structure\nacross national boundaries. While astronomical data are useful scientifically,\nthe data do not come with the same ethical/privacy-related restrictions as\nmedical/biological data. Moreover, the ability to use data for new scientific\nanalysis extends and expands the impact and reach of scientific surveys -- this\nis a strength that national funding agencies should capitalize on. We discuss\nthe management and analysis of such large volumes of data and the corresponding\nsignificant challenges that require policy-level preparations.\n  The policy webinar took place during the G20 presidency in India (2023). A\nsummary based on the seven panels can be found here: arxiv:2401.04623.\n']",Astroinformatics and Data Management in Astronomy
494,493,11,493_fairness_federated_incentives_incentive,"['fairness', 'federated', 'incentives', 'incentive', 'collaborative', 'shared', 'equitable', 'collaboratively', 'aggregation', 'incentivizing']","['fairness', 'client', 'federated', 'clients', 'contributions', 'incentive', 'contribution', 'metaverse', 'participation', 'post']","['fairness', 'federated', 'incentives', 'equitable', 'aggregation', 'cooperative', 'fedsac', 'metaverse', 'shapley', 'reputation']","[""  Federated Learning (FL) is a distributed machine learning framework in which\na set of local communities collaboratively learn a shared global model while\nretaining all training data locally within each community. Two notions of\nfairness have recently emerged as important issues for federated learning:\ngroup fairness and community fairness. Group fairness requires that a model's\ndecisions do not favor any particular group based on a set of legally protected\nattributes such as race or gender. Community fairness requires that global\nmodels exhibit similar levels of performance (accuracy) across all\ncollaborating communities. Both fairness concepts can coexist within an FL\nframework, but the existing literature has focused on either one concept or the\nother. This paper proposes and analyzes a post-processing fair federated\nlearning (FFL) framework called post-FFL. Post-FFL uses a linear program to\nsimultaneously enforce group and community fairness while maximizing the\nutility of the global model. Because Post-FFL is a post-processing approach, it\ncan be used with existing FL training pipelines whose convergence properties\nare well understood. This paper uses post-FFL on real-world datasets to mimic\nhow hospital networks, for example, use federated learning to deliver community\nhealth care. Theoretical results bound the accuracy lost when post-FFL enforces\nboth notion of fairness. Experimental results illustrate that post-FFL\nsimultaneously improves both group and community fairness in FL. Moreover,\npost-FFL outperforms the existing in-processing fair federated learning in\nterms of improving both notions of fairness, communication efficiency and\ncomputation cost.\n"", ""  Federated Learning (FL) is a privacy-enhancing technology for distributed ML.\nBy training models locally and aggregating updates - a federation learns\ntogether, while bypassing centralised data collection. FL is increasingly\npopular in healthcare, finance and personal computing. However, it inherits\nfairness challenges from classical ML and introduces new ones, resulting from\ndifferences in data quality, client participation, communication constraints,\naggregation methods and underlying hardware. Fairness remains an unresolved\nissue in FL and the community has identified an absence of succinct definitions\nand metrics to quantify fairness; to address this, we propose Federated\nFairness Analytics - a methodology for measuring fairness. Our definition of\nfairness comprises four notions with novel, corresponding metrics. They are\nsymptomatically defined and leverage techniques originating from XAI,\ncooperative game-theory and networking engineering. We tested a range of\nexperimental settings, varying the FL approach, ML task and data settings. The\nresults show that statistical heterogeneity and client participation affect\nfairness and fairness conscious approaches such as Ditto and q-FedAvg\nmarginally improve fairness-performance trade-offs. Using our techniques, FL\npractitioners can uncover previously unobtainable insights into their system's\nfairness, at differing levels of granularity in order to address fairness\nchallenges in FL. We have open-sourced our work at:\nhttps://github.com/oscardilley/federated-fairness.\n"", ""  Federated learning (FL) has emerged as a prospective solution for\ncollaboratively learning a shared model across clients without sacrificing\ntheir data privacy. However, the federated learned model tends to be biased\nagainst certain demographic groups (e.g., racial and gender groups) due to the\ninherent FL properties, such as data heterogeneity and party selection. Unlike\ncentralized learning, mitigating bias in FL is particularly challenging as\nprivate training datasets and their sensitive attributes are typically not\ndirectly accessible. Most prior research in this field only focuses on global\nfairness while overlooking the local fairness of individual clients. Moreover,\nexisting methods often require sensitive information about the client's local\ndatasets to be shared, which is not desirable. To address these issues, we\npropose GLOCALFAIR, a client-server co-design fairness framework that can\njointly improve global and local group fairness in FL without the need for\nsensitive statistics about the client's private datasets. Specifically, we\nutilize constrained optimization to enforce local fairness on the client side\nand adopt a fairness-aware clustering-based aggregation on the server to\nfurther ensure the global model fairness across different sensitive groups\nwhile maintaining high utility. Experiments on two image datasets and one\ntabular dataset with various state-of-the-art fairness baselines show that\nGLOCALFAIR can achieve enhanced fairness under both global and local data\ndistributions while maintaining a good level of utility and client fairness.\n""]",Fairness in Federated Learning
495,494,11,494_hypergraph_hypergraphs_hyperedge_hyperedges,"['hypergraph', 'hypergraphs', 'hyperedge', 'hyperedges', 'nodes', 'adjacency', 'networks', 'neural', 'ypergraph', 'hypernode']","['hypergraph', 'hyperedges', 'hypergraphs', 'hyperedge', 'node', 'message', 'nodes', 'order', 'interactions', 'structural']","['hypergraph', 'hyperedges', 'nodes', 'neural', 'hypernode', 'embeddings', 'hyperlink', 'propagation', 'hgnn', 'connectivities']","['  Hypergraphs are widely employed to represent complex higher-order\nrelationships in real-world applications. Most hypergraph learning research\nfocuses on node- or edge-level tasks. A practically relevant but more\nchallenging task, edge-dependent node classification (ENC), is only recently\nproposed. In ENC, a node can have different labels across different hyperedges,\nwhich requires the modeling of node-hyperedge pairs instead of single nodes or\nhyperedges. Existing solutions for this task are based on message passing and\nmodel within-edge and within-node interactions as multi-input single-output\nfunctions. This brings three limitations: (1) non-adaptive representation size,\n(2) node/edge agnostic messages, and (3) insufficient interactions among nodes\nor hyperedges. To tackle these limitations, we develop CoNHD, a new solution\nbased on hypergraph diffusion. Specifically, we first extend hypergraph\ndiffusion using node-hyperedge co-representations. This extension explicitly\nmodels both within-edge and within-node interactions as multi-input\nmulti-output functions using two equivariant diffusion operators. To avoid\nhandcrafted regularization functions, we propose a neural implementation for\nthe co-representation hypergraph diffusion process. Extensive experiments\ndemonstrate the effectiveness and efficiency of the proposed CoNHD model.\n', '  Hypergraphs play a pivotal role in the modelling of data featuring\nhigher-order relations involving more than two entities. Hypergraph neural\nnetworks emerge as a powerful tool for processing hypergraph-structured data,\ndelivering remarkable performance across various tasks, e.g., hypergraph node\nclassification. However, these models struggle to capture global structural\ninformation due to their reliance on local message passing. To address this\nchallenge, we propose a novel hypergraph learning framework, HyperGraph\nTransformer (HyperGT). HyperGT uses a Transformer-based neural network\narchitecture to effectively consider global correlations among all nodes and\nhyperedges. To incorporate local structural information, HyperGT has two\ndistinct designs: i) a positional encoding based on the hypergraph incidence\nmatrix, offering valuable insights into node-node and hyperedge-hyperedge\ninteractions; and ii) a hypergraph structure regularization in the loss\nfunction, capturing connectivities between nodes and hyperedges. Through these\ndesigns, HyperGT achieves comprehensive hypergraph representation learning by\neffectively incorporating global interactions while preserving local\nconnectivity patterns. Extensive experiments conducted on real-world hypergraph\nnode classification tasks showcase that HyperGT consistently outperforms\nexisting methods, establishing new state-of-the-art benchmarks. Ablation\nstudies affirm the effectiveness of the individual designs of our model.\n', '  Hypergraphs are vital in modelling data with higher-order relations\ncontaining more than two entities, gaining prominence in machine learning and\nsignal processing. Many hypergraph neural networks leverage message passing\nover hypergraph structures to enhance node representation learning, yielding\nimpressive performances in tasks like hypergraph node classification. However,\nthese message-passing-based models face several challenges, including\noversmoothing as well as high latency and sensitivity to structural\nperturbations at inference time. To tackle those challenges, we propose an\nalternative approach where we integrate the information about hypergraph\nstructures into training supervision without explicit message passing, thus\nalso removing the reliance on it at inference. Specifically, we introduce\nHypergraph-MLP, a novel learning framework for hypergraph-structured data,\nwhere the learning model is a straightforward multilayer perceptron (MLP)\nsupervised by a loss function based on a notion of signal smoothness on\nhypergraphs. Experiments on hypergraph node classification tasks demonstrate\nthat Hypergraph-MLP achieves competitive performance compared to existing\nbaselines, and is considerably faster and more robust against structural\nperturbations at inference.\n']",Hypergraph Learning and Neural Networks
496,495,11,495_customers_ecommerce_clustering_retailing,"['customers', 'ecommerce', 'clustering', 'retailing', 'retail', 'marketing', 'customer', 'promotions', 'consumer', 'purchases']","['customer', 'customers', 'ecommerce', 'marketing', 'retail', 'financial', 'business', 'telecallers', 'product', 'institutions']","['customers', 'ecommerce', 'clustering', 'retailing', 'categories', 'self_organizing', 'gmm', 'willingness', 'behavioural', 'multinomial']","['  Financial inclusion ensures that individuals have access to financial\nproducts and services that meet their needs. As a key contributing factor to\neconomic growth and investment opportunity, financial inclusion increases\nconsumer spending and consequently business development. It has been shown that\ninstitutions are more profitable when they provide marginalised social groups\naccess to financial services. Customer segmentation based on consumer\ntransaction data is a well-known strategy used to promote financial inclusion.\nWhile the required data is available to modern institutions, the challenge\nremains that segment annotations are usually difficult and/or expensive to\nobtain. This prevents the usage of time series classification models for\ncustomer segmentation based on domain expert knowledge. As a result, clustering\nis an attractive alternative to partition customers into homogeneous groups\nbased on the spending behaviour encoded within their transaction data. In this\npaper, we present a solution to one of the key challenges preventing modern\nfinancial institutions from providing financially inclusive credit, savings and\ninsurance products: the inability to understand consumer financial behaviour,\nand hence risk, without the introduction of restrictive conventional credit\nscoring techniques. We present a novel time series clustering algorithm that\nallows institutions to understand the financial behaviour of their customers.\nThis enables unique product offerings to be provided based on the needs of the\ncustomer, without reliance on restrictive credit practices.\n', ""  Problem definition. In retailing, discrete choice models (DCMs) are commonly\nused to capture the choice behavior of customers when offered an assortment of\nproducts. When estimating DCMs using transaction data, flexible models (such as\nmachine learning models or nonparametric models) are typically not\ninterpretable and hard to estimate, while tractable models (such as the\nmultinomial logit model) tend to misspecify the complex behavior represeted in\nthe data. Methodology/results. In this study, we use a forest of binary\ndecision trees to represent DCMs. This approach is based on random forests, a\npopular machine learning algorithm. The resulting model is interpretable: the\ndecision trees can explain the decision-making process of customers during the\npurchase. We show that our approach can predict the choice probability of any\nDCM consistently and thus never suffers from misspecification. Moreover, our\nalgorithm predicts assortments unseen in the training data. The mechanism and\nerrors can be theoretically analyzed. We also prove that the random forest can\nrecover preference rankings of customers thanks to the splitting criterion such\nas the Gini index and information gain ratio. Managerial implications. The\nframework has unique practical advantages. It can capture customers' behavioral\npatterns such as irrationality or sequential searches when purchasing a\nproduct. It handles nonstandard formats of training data that result from\naggregation. It can measure product importance based on how frequently a random\ncustomer would make decisions depending on the presence of the product. It can\nalso incorporate price information and customer features. Our numerical\nexperiments using synthetic and real data show that using random forests to\nestimate customer choices can outperform existing methods.\n"", '  Recently, peoples awareness of online purchases has significantly risen. This\nhas given rise to online retail platforms and the need for a better\nunderstanding of customer purchasing behaviour. Retail companies are pressed\nwith the need to deal with a high volume of customer purchases, which requires\nsophisticated approaches to perform more accurate and efficient customer\nsegmentation. Customer segmentation is a marketing analytical tool that aids\ncustomer-centric service and thus enhances profitability. In this paper, we aim\nto develop a customer segmentation model to improve decision-making processes\nin the retail market industry. To achieve this, we employed a UK-based online\nretail dataset obtained from the UCI machine learning repository. The retail\ndataset consists of 541,909 customer records and eight features. Our study\nadopted the RFM (recency, frequency, and monetary) framework to quantify\ncustomer values. Thereafter, we compared several state-of-the-art (SOTA)\nclustering algorithms, namely, K-means clustering, the Gaussian mixture model\n(GMM), density-based spatial clustering of applications with noise (DBSCAN),\nagglomerative clustering, and balanced iterative reducing and clustering using\nhierarchies (BIRCH). The results showed the GMM outperformed other approaches,\nwith a Silhouette Score of 0.80.\n']",Customer Segmentation in E-commerce Retail
497,496,11,496_satisfiability_satcomp_satformer_sat,"['satisfiability', 'satcomp', 'satformer', 'sat', 'unsatisfiability', 'unsatisfiable', 'neurosat', 'unsat', 'linsatnet', 'solvers']","['satisfiability', 'solvers', 'clause', 'solver', 'solving', 'unsatisfiable', 'runtime', 'unsatisfiability', 'problems', 'industrial']","['satisfiability', 'neurosat', 'unsat', 'linsatnet', 'solvers', 'g4satbench', 'boolean', 'hardsatgen', 'clauses', 'grns']","['  Propositional satisfiability (SAT) is an NP-complete problem that impacts\nmany research fields, such as planning, verification, and security. Mainstream\nmodern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL)\nalgorithm. Recent work aimed to enhance CDCL SAT solvers using Graph Neural\nNetworks (GNNs). However, so far this approach either has not made solving more\neffective, or required substantial GPU resources for frequent online model\ninferences. Aiming to make GNN improvements practical, this paper proposes an\napproach called NeuroBack, which builds on two insights: (1) predicting phases\n(i.e., values) of variables appearing in the majority (or even all) of the\nsatisfying assignments are essential for CDCL SAT solving, and (2) it is\nsufficient to query the neural model only once for the predictions before the\nSAT solving starts. Once trained, the offline model inference allows NeuroBack\nto execute exclusively on the CPU, removing its reliance on GPU resources. To\ntrain NeuroBack, a new dataset called DataBack containing 120,286 data samples\nis created. NeuroBack is implemented as an enhancement to a state-of-the-art\nSAT solver called Kissat. As a result, it allowed Kissat to solve up to 5.2%\nand 7.4% more problems on two recent SAT competition problem sets, SATCOMP-2022\nand SATCOMP-2023, respectively. NeuroBack therefore shows how machine learning\ncan be harnessed to improve SAT solving in an effective and practical manner.\n', '  Graph neural networks (GNNs) have recently emerged as a promising approach\nfor solving the Boolean Satisfiability Problem (SAT), offering potential\nalternatives to traditional backtracking or local search SAT solvers. However,\ndespite the growing volume of literature in this field, there remains a notable\nabsence of a unified dataset and a fair benchmark to evaluate and compare\nexisting approaches. To address this crucial gap, we present G4SATBench, the\nfirst benchmark study that establishes a comprehensive evaluation framework for\nGNN-based SAT solvers. In G4SATBench, we meticulously curate a large and\ndiverse set of SAT datasets comprising 7 problems with 3 difficulty levels and\nbenchmark a broad range of GNN models across various prediction tasks, training\nobjectives, and inference algorithms. To explore the learning abilities and\ncomprehend the strengths and limitations of GNN-based SAT solvers, we also\ncompare their solving processes with the heuristics in search-based SAT\nsolvers. Our empirical results provide valuable insights into the performance\nof GNN-based SAT solvers and further suggest that existing GNN models can\neffectively learn a solving strategy akin to greedy local search but struggle\nto learn backtracking search in the latent space. Our codebase is available at\nhttps://github.com/zhaoyu-li/G4SATBench.\n', '  Boolean satisfiability (SAT) problems are routinely solved by SAT solvers in\nreal-life applications, yet solving time can vary drastically between solvers\nfor the same instance. This has motivated research into machine learning models\nthat can predict, for a given SAT instance, which solver to select among\nseveral options. Existing SAT solver selection methods all rely on some\nhand-picked instance features, which are costly to compute and ignore the\nstructural information in SAT graphs. In this paper we present GraSS, a novel\napproach for automatic SAT solver selection based on tripartite graph\nrepresentations of instances and a heterogeneous graph neural network (GNN)\nmodel. While GNNs have been previously adopted in other SAT-related tasks, they\ndo not incorporate any domain-specific knowledge and ignore the runtime\nvariation introduced by different clause orders. We enrich the graph\nrepresentation with domain-specific decisions, such as novel node feature\ndesign, positional encodings for clauses in the graph, a GNN architecture\ntailored to our tripartite graphs and a runtime-sensitive loss function.\nThrough extensive experiments, we demonstrate that this combination of raw\nrepresentations and domain-specific choices leads to improvements in runtime\nfor a pool of seven state-of-the-art solvers on both an industrial circuit\ndesign benchmark, and on instances from the 20-year Anniversary Track of the\n2022 SAT Competition.\n']",Satisfiability Problem Solving with Neural Networks
498,497,11,497_detoxify_detoxifying_detoxified_detoxifier,"['detoxify', 'detoxifying', 'detoxified', 'detoxifier', 'detoxifies', 'detoxification', 'detox', 'detoxifiable', 'detoxifiability', 'corpus']","['detoxification', 'toxic', 'toxicity', 'detoxifier', 'detoxifying', 'detoxified', 'text', 'parallel', 'corpus', 'generator']","['detoxifiable', 'corpus', 'toxic', 'fusing', 'rephrasing', 'offensive', 'content', 'counterfactual', 'mitigate', 'subtoxicity']","['  Prior works on detoxification are scattered in the sense that they do not\ncover all aspects of detoxification needed in a real-world scenario. Notably,\nprior works restrict the task of developing detoxification models to only a\nseen subset of platforms, leaving the question of how the models would perform\non unseen platforms unexplored. Additionally, these works do not address\nnon-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified\nwithout altering the meaning. We propose GreenLLaMA, the first comprehensive\nend-to-end detoxification framework, which attempts to alleviate the\naforementioned limitations. We first introduce a cross-platform pseudo-parallel\ncorpus applying multi-step data processing and generation strategies leveraging\nChatGPT. We then train a suite of detoxification models with our cross-platform\ncorpus. We show that our detoxification models outperform the SoTA model\ntrained with human-annotated parallel corpus. We further introduce explanation\nto promote transparency and trustworthiness. GreenLLaMA additionally offers a\nunique paraphrase detector especially dedicated for the detoxification task to\ntackle the non-detoxifiable cases. Through experimental analysis, we\ndemonstrate the effectiveness of our cross-platform corpus and the robustness\nof GreenLLaMA against adversarial toxicity.\n', '  Text detoxification aims to minimize the risk of language models producing\ntoxic content. Existing detoxification methods of directly constraining the\nmodel output or further training the model on the non-toxic corpus fail to\nachieve a decent balance between detoxification effectiveness and generation\nquality. This issue stems from the neglect of constrain imposed by the context\nsince language models are designed to generate output that closely matches the\ncontext while detoxification methods endeavor to ensure the safety of the\noutput even if it semantically deviates from the context. In view of this, we\nintroduce a Context-aware Model self-Detoxification~(CMD) framework that pays\nattention to both the context and the detoxification process, i.e., first\ndetoxifying the context and then making the language model generate along the\nsafe context. Specifically, CMD framework involves two phases: utilizing\nlanguage models to synthesize data and applying these data for training. We\nalso introduce a toxic contrastive loss that encourages the model generation\naway from the negative toxic samples. Experiments on various LLMs have verified\nthe effectiveness of our MSD framework, which can yield the best performance\ncompared to baselines.\n', '  Toxicity mitigation consists in rephrasing text in order to remove offensive\nor harmful meaning. Neural natural language processing (NLP) models have been\nwidely used to target and mitigate textual toxicity. However, existing methods\nfail to detoxify text while preserving the initial non-toxic meaning at the\nsame time. In this work, we propose to apply counterfactual generation methods\nfrom the eXplainable AI (XAI) field to target and mitigate textual toxicity. In\nparticular, we perform text detoxification by applying local feature importance\nand counterfactual generation methods to a toxicity classifier distinguishing\nbetween toxic and non-toxic texts. We carry out text detoxification through\ncounterfactual generation on three datasets and compare our approach to three\ncompetitors. Automatic and human evaluations show that recently developed NLP\ncounterfactual generators can mitigate toxicity accurately while better\npreserving the meaning of the initial text as compared to classical\ndetoxification methods. Finally, we take a step back from using automated\ndetoxification tools, and discuss how to manage the polysemous nature of\ntoxicity and the risk of malicious use of detoxification tools. This work is\nthe first to bridge the gap between counterfactual generation and text\ndetoxification and paves the way towards more practical application of XAI\nmethods.\n']",Text Detoxification
499,498,11,498_multimodal_multimodality_linking_entity,"['multimodal', 'multimodality', 'linking', 'entity', 'richpedia', 'entities', 'disambiguating', 'semantic', 'semantics', 'modality']","['entity', 'linking', 'mentions', 'multimodal', 'entities', 'ambiguous', 'grained', 'textual', 'base', 'visual']","['multimodality', 'linking', 'richpedia', 'disambiguating', 'semantic', 'wikimel', 'multilingual', 'wikidiverse', 'twitter2015', 'enhanced']","['  Multimodal Entity Linking (MEL) is a crucial task that aims at linking\nambiguous mentions within multimodal contexts to the referent entities in a\nmultimodal knowledge base, such as Wikipedia. Existing methods focus heavily on\nusing complex mechanisms and extensive model tuning methods to model the\nmultimodal interaction on specific datasets. However, these methods\novercomplicate the MEL task and overlook the visual semantic information, which\nmakes them costly and hard to scale. Moreover, these methods can not solve the\nissues like textual ambiguity, redundancy, and noisy images, which severely\ndegrade their performance. Fortunately, the advent of Large Language Models\n(LLMs) with robust capabilities in text understanding and reasoning,\nparticularly Multimodal Large Language Models (MLLMs) that can process\nmultimodal inputs, provides new insights into addressing this challenge.\nHowever, how to design a universally applicable LLMs-based MEL approach remains\na pressing challenge. To this end, we propose UniMEL, a unified framework which\nestablishes a new paradigm to process multimodal entity linking tasks using\nLLMs. In this framework, we employ LLMs to augment the representation of\nmentions and entities individually by integrating textual and visual\ninformation and refining textual information. Subsequently, we employ the\nembedding-based method for retrieving and re-ranking candidate entities. Then,\nwith only ~0.26% of the model parameters fine-tuned, LLMs can make the final\nselection from the candidate entities. Extensive experiments on three public\nbenchmark datasets demonstrate that our solution achieves state-of-the-art\nperformance, and ablation studies verify the effectiveness of all modules. Our\ncode is available at https://anonymous.4open.science/r/UniMEL/.\n', '  Multimodal entity linking (MEL) aims to utilize multimodal information\n(usually textual and visual information) to link ambiguous mentions to\nunambiguous entities in knowledge base. Current methods facing main issues:\n(1)treating the entire image as input may contain redundant information. (2)the\ninsufficient utilization of entity-related information, such as attributes in\nimages. (3)semantic inconsistency between the entity in knowledge base and its\nrepresentation. To this end, we propose DWE+ for multimodal entity linking.\nDWE+ could capture finer semantics and dynamically maintain semantic\nconsistency with entities. This is achieved by three aspects: (a)we introduce a\nmethod for extracting fine-grained image features by partitioning the image\ninto multiple local objects. Then, hierarchical contrastive learning is used to\nfurther align semantics between coarse-grained information(text and image) and\nfine-grained (mention and visual objects). (b)we explore ways to extract visual\nattributes from images to enhance fusion feature such as facial features and\nidentity. (c)we leverage Wikipedia and ChatGPT to capture the entity\nrepresentation, achieving semantic enrichment from both static and dynamic\nperspectives, which better reflects the real-world entity semantics.\nExperiments on Wikimel, Richpedia, and Wikidiverse datasets demonstrate the\neffectiveness of DWE+ in improving MEL performance. Specifically, we optimize\nthese datasets and achieve state-of-the-art performance on the enhanced\ndatasets. The code and enhanced datasets are released on\nhttps://github.com/season1blue/DWET\n', '  Multimodal Entity Linking (MEL) aims at linking ambiguous mentions with\nmultimodal information to entity in Knowledge Graph (KG) such as Wikipedia,\nwhich plays a key role in many applications. However, existing methods suffer\nfrom shortcomings, including modality impurity such as noise in raw image and\nambiguous textual entity representation, which puts obstacles to MEL. We\nformulate multimodal entity linking as a neural text matching problem where\neach multimodal information (text and image) is treated as a query, and the\nmodel learns the mapping from each query to the relevant entity from candidate\nentities. This paper introduces a dual-way enhanced (DWE) framework for MEL:\n(1) our model refines queries with multimodal data and addresses semantic gaps\nusing cross-modal enhancers between text and image information. Besides, DWE\ninnovatively leverages fine-grained image attributes, including facial\ncharacteristic and scene feature, to enhance and refine visual features. (2)By\nusing Wikipedia descriptions, DWE enriches entity semantics and obtains more\ncomprehensive textual representation, which reduces between textual\nrepresentation and the entities in KG. Extensive experiments on three public\nbenchmarks demonstrate that our method achieves state-of-the-art (SOTA)\nperformance, indicating the superiority of our model. The code is released on\nhttps://github.com/season1blue/DWE\n']",Multimodal Entity Linking
500,499,11,499_mixtures_mixture_likelihood_gaussian,"['mixtures', 'mixture', 'likelihood', 'gaussian', 'clustering', 'clusterability', 'estimating', 'mixing', 'gmms', 'gmm']","['mixture', 'mixtures', 'components', 'covariances', 'exponential', 'g_0', 'likelihood', 'clustering', 'mislabeling', 'separation']","['likelihood', 'gaussian', 'clusterability', 'mixing', 'gmms', 'parameterized', 'outliers', 'subexponential', 'sgmm', 'fitting']","['  We investigate the landscape of the negative log-likelihood function of\nGaussian Mixture Models (GMMs) with a general number of components in the\npopulation limit. As the objective function is non-convex, there can be\nmultiple local minima that are not globally optimal, even for well-separated\nmixture models. Our study reveals that all local minima share a common\nstructure that partially identifies the cluster centers (i.e., means of the\nGaussian components) of the true location mixture. Specifically, each local\nminimum can be represented as a non-overlapping combination of two types of\nsub-configurations: fitting a single mean estimate to multiple Gaussian\ncomponents or fitting multiple estimates to a single true component. These\nresults apply to settings where the true mixture components satisfy a certain\nseparation condition, and are valid even when the number of components is over-\nor under-specified. We also present a more fine-grained analysis for the\nsetting of one-dimensional GMMs with three components, which provide sharper\napproximation error bounds with improved dependence on the separation.\n', ""  Clustering is a pivotal challenge in unsupervised machine learning and is\noften investigated through the lens of mixture models. The optimal error rate\nfor recovering cluster labels in Gaussian and sub-Gaussian mixture models\ninvolves ad hoc signal-to-noise ratios. Simple iterative algorithms, such as\nLloyd's algorithm, attain this optimal error rate. In this paper, we first\nestablish a universal lower bound for the error rate in clustering any mixture\nmodel, expressed through a Chernoff divergence, a more versatile measure of\nmodel information than signal-to-noise ratios. We then demonstrate that\niterative algorithms attain this lower bound in mixture models with\nsub-exponential tails, notably emphasizing location-scale mixtures featuring\nLaplace-distributed errors. Additionally, for datasets better modelled by\nPoisson or Negative Binomial mixtures, we study mixture models whose\ndistributions belong to an exponential family. In such mixtures, we establish\nthat Bregman hard clustering, a variant of Lloyd's algorithm employing a\nBregman divergence, is rate optimal.\n"", '  We consider the parameter estimation problem in the deviated Gaussian mixture\nof experts in which the data are generated from $(1 - \\lambda^{\\ast}) g_0(Y|\nX)+ \\lambda^{\\ast} \\sum_{i = 1}^{k_{\\ast}} p_{i}^{\\ast}\nf(Y|(a_{i}^{\\ast})^{\\top}X+b_i^{\\ast},\\sigma_{i}^{\\ast})$, where $X, Y$ are\nrespectively a covariate vector and a response variable, $g_{0}(Y|X)$ is a\nknown function, $\\lambda^{\\ast} \\in [0, 1]$ is true but unknown mixing\nproportion, and $(p_{i}^{\\ast}, a_{i}^{\\ast}, b_{i}^{\\ast}, \\sigma_{i}^{\\ast})$\nfor $1 \\leq i \\leq k^{\\ast}$ are unknown parameters of the Gaussian mixture of\nexperts. This problem arises from the goodness-of-fit test when we would like\nto test whether the data are generated from $g_{0}(Y|X)$ (null hypothesis) or\nthey are generated from the whole mixture (alternative hypothesis). Based on\nthe algebraic structure of the expert functions and the distinguishability\nbetween $g_0$ and the mixture part, we construct novel Voronoi-based loss\nfunctions to capture the convergence rates of maximum likelihood estimation\n(MLE) for our models. We further demonstrate that our proposed loss functions\ncharacterize the local convergence rates of parameter estimation more\naccurately than the generalized Wasserstein, a loss function being commonly\nused for estimating parameters in the Gaussian mixture of experts.\n']",Gaussian Mixture Models and Clustering Analysis
501,500,11,500_driving_pedestrian_driver_pedestrians,"['driving', 'pedestrian', 'driver', 'pedestrians', 'vision', 'vehicles', 'visual', 'attention', 'vehicle', 'recognition']","['driver', 'pedestrian', 'autonomous', 'intention', 'driving', 'road', 'safety', 'vehicle', 'vehicles', 'scenes']","['driving', 'pedestrians', 'visual', 'recognition', 'crosswalk', 'encoder', 'intentions', 'hdvs', 'iccv2021', 'vlms']","[""  Predicting pedestrian behavior is the key to ensure safety and reliability of\nautonomous vehicles. While deep learning methods have been promising by\nlearning from annotated video frame sequences, they often fail to fully grasp\nthe dynamic interactions between pedestrians and traffic, crucial for accurate\npredictions. These models also lack nuanced common sense reasoning. Moreover,\nthe manual annotation of datasets for these models is expensive and challenging\nto adapt to new situations. The advent of Vision Language Models (VLMs)\nintroduces promising alternatives to these issues, thanks to their advanced\nvisual and causal reasoning skills. To our knowledge, this research is the\nfirst to conduct both quantitative and qualitative evaluations of VLMs in the\ncontext of pedestrian behavior prediction for autonomous driving. We evaluate\nGPT-4V(ision) on publicly available pedestrian datasets: JAAD and WiDEVIEW. Our\nquantitative analysis focuses on GPT-4V's ability to predict pedestrian\nbehavior in current and future frames. The model achieves a 57% accuracy in a\nzero-shot manner, which, while impressive, is still behind the state-of-the-art\ndomain-specific models (70%) in predicting pedestrian crossing actions.\nQualitatively, GPT-4V shows an impressive ability to process and interpret\ncomplex traffic scenarios, differentiate between various pedestrian behaviors,\nand detect and analyze groups. However, it faces challenges, such as difficulty\nin detecting smaller pedestrians and assessing the relative motion between\npedestrians and the ego vehicle.\n"", '  Accurate behavior prediction for vehicles is essential but challenging for\nautonomous driving. Most existing studies show satisfying performance under\nregular scenarios, but most neglected safety-critical scenarios. In this study,\na spatio-temporal dual-encoder network named STDA for safety-critical scenarios\nwas developed. Considering the exceptional capabilities of human drivers in\nterms of situational awareness and comprehending risks, driver attention was\nincorporated into STDA to facilitate swift identification of the critical\nregions, which is expected to improve both performance and interpretability.\nSTDA contains four parts: the driver attention prediction module, which\npredicts driver attention; the fusion module designed to fuse the features\nbetween driver attention and raw images; the temporary encoder module used to\nenhance the capability to interpret dynamic scenes; and the behavior prediction\nmodule to predict the behavior. The experiment data are used to train and\nvalidate the model. The results show that STDA improves the G-mean from 0.659\nto 0.719 when incorporating driver attention and adopting a temporal encoder\nmodule. In addition, extensive experimentation has been conducted to validate\nthat the proposed module exhibits robust generalization capabilities and can be\nseamlessly integrated into other mainstream models.\n', ""  Accurate pedestrian intention prediction (PIP) by Autonomous Vehicles (AVs)\nis one of the current research challenges in this field. In this article, we\nintroduce PIP-Net, a novel framework designed to predict pedestrian crossing\nintentions by AVs in real-world urban scenarios. We offer two variants of\nPIP-Net designed for different camera mounts and setups. Leveraging both\nkinematic data and spatial features from the driving scene, the proposed model\nemploys a recurrent and temporal attention-based solution, outperforming\nstate-of-the-art performance. To enhance the visual representation of road\nusers and their proximity to the ego vehicle, we introduce a categorical depth\nfeature map, combined with a local motion flow feature, providing rich insights\ninto the scene dynamics. Additionally, we explore the impact of expanding the\ncamera's field of view, from one to three cameras surrounding the ego vehicle,\nleading to enhancement in the model's contextual perception. Depending on the\ntraffic scenario and road environment, the model excels in predicting\npedestrian crossing intentions up to 4 seconds in advance which is a\nbreakthrough in current research studies in pedestrian intention prediction.\nFinally, for the first time, we present the Urban-PIP dataset, a customised\npedestrian intention prediction dataset, with multi-camera annotations in\nreal-world automated driving scenarios.\n""]",Pedestrian Behavior Prediction for Autonomous Vehicles
502,501,11,501_developers_contributors_commits_projects,"['developers', 'contributors', 'commits', 'projects', 'maintainers', 'repositories', 'apis', 'contributor', 'backlog', 'analytics']","['contributors', 'projects', 'maintainers', 'abandonment', 'developers', 'bot', 'bugs', 'contributor', 'bug', 'project']","['developers', 'contributors', 'commits', 'repositories', 'apis', 'backlog', 'analytics', 'latencies', 'bitbucket', 'prs']","['  The success of a Pull Request (PR) depends on the responsiveness of the\nmaintainers and the contributor during the review process. Being aware of the\nexpected waiting times can lead to better interactions and managed expectations\nfor both the maintainers and the contributor. In this paper, we propose a\nmachine-learning approach to predict the first response latency of the\nmaintainers following the submission of a PR, and the first response latency of\nthe contributor after receiving the first response from the maintainers. We\ncurate a dataset of 20 large and popular open-source projects on GitHub and\nextract 21 features to characterize projects, contributors, PRs, and review\nprocesses. Using these features, we then evaluate seven types of classifiers to\nidentify the best-performing models. We also conduct permutation feature\nimportance and SHAP analyses to understand the importance and the impact of\ndifferent features on the predicted response latencies. We find that our\nCatBoost models are the most effective for predicting the first response\nlatencies of both maintainers and contributors. We also observe that PRs\nsubmitted earlier in the week, containing an average number of commits, and\nwith concise descriptions are more likely to receive faster first responses\nfrom the maintainers. Similarly, PRs with a lower first response latency from\nmaintainers, that received the first response of maintainers earlier in the\nweek, and containing an average number of commits tend to receive faster first\nresponses from the contributors. Additionally, contributors with a higher\nacceptance rate and a history of timely responses in the project are likely to\nboth obtain and provide faster first responses. Moreover, we show the\neffectiveness of our approach in a cross-project setting.\n', '  Pull Requests (PRs) that are neither progressed nor resolved clutter the list\nof PRs, making it difficult for the maintainers to manage and prioritize\nunresolved PRs. To automatically track, follow up, and close such inactive PRs,\nStale bot was introduced by GitHub. Despite its increasing adoption, there are\nongoing debates on whether using Stale bot alleviates or exacerbates the\nproblem of inactive PRs. To better understand if and how Stale bot helps\nprojects in their pull-based development workflow, we perform an empirical\nstudy of 20 large and popular open-source projects. We find that Stale bot can\nhelp deal with a backlog of unresolved PRs as the projects closed more PRs\nwithin the first few months of adoption. Moreover, Stale bot can help improve\nthe efficiency of the PR review process as the projects reviewed PRs that ended\nup merged and resolved PRs that ended up closed faster after the adoption.\nHowever, Stale bot can also negatively affect the contributors as the projects\nexperienced a considerable decrease in their number of active contributors\nafter the adoption. Therefore, relying solely on Stale bot to deal with\ninactive PRs may lead to decreased community engagement and an increased\nprobability of contributor abandonment.\n', ""  Pull-based development has enabled numerous volunteers to contribute to\nopen-source projects with fewer barriers. Nevertheless, a considerable amount\nof pull requests (PRs) with valid contributions are abandoned by their\ncontributors, wasting the effort and time put in by both the contributors and\nmaintainers. To better understand the underlying dynamics of\ncontributor-abandoned PRs, we conduct a mixed-methods study using both\nquantitative and qualitative methods. We curate a dataset consisting of 265,325\nPRs including 4,450 abandoned ones from ten popular and mature GitHub projects\nand measure 16 features characterizing PRs, contributors, review processes, and\nprojects. Using statistical and machine learning techniques, we find that\ncomplex PRs, novice contributors, and lengthy reviews have a higher probability\nof abandonment and the rate of PR abandonment fluctuates alongside the\nprojects' maturity or workload. To identify why contributors abandon their PRs,\nwe also manually examine a random sample of 354 abandoned PRs. We observe that\nthe most frequent abandonment reasons are related to the obstacles faced by\ncontributors, followed by the hurdles imposed by maintainers during the review\nprocess. Finally, we survey the top core maintainers of the studied projects to\nunderstand their perspectives on dealing with PR abandonment and on our\nfindings.\n""]",Open-Source Project Collaboration Dynamics
503,502,11,502_denoising_denoisers_denoiser_denoised,"['denoising', 'denoisers', 'denoiser', 'denoised', 'noise2noise', 'pixels', 'noisy', 'supervised', 'noise', 'images']","['denoising', 'noise', 'image', 'denoisers', 'supervised', 'filter', 'denoiser', 'pixels', 'self', 'corners']","['denoisers', 'noise2noise', 'pixels', 'supervised', 'dncnn', 'priors', 'blind', 'bm3d', 'n2n', 'nonlocal']","['  Recently, the mainstream practice for training low-light raw image denoising\nmethods has shifted towards employing synthetic data. Noise modeling, which\nfocuses on characterizing the noise distribution of real-world sensors,\nprofoundly influences the effectiveness and practicality of synthetic data.\nCurrently, physics-based noise modeling struggles to characterize the entire\nreal noise distribution, while learning-based noise modeling impractically\ndepends on paired real data. In this paper, we propose a novel strategy:\nlearning the noise model from dark frames instead of paired real data, to break\ndown the data dependency. Based on this strategy, we introduce an efficient\nphysics-guided noise neural proxy (PNNP) to approximate the real-world sensor\nnoise model. Specifically, we integrate physical priors into neural proxies and\nintroduce three efficient techniques: physics-guided noise decoupling (PND),\nphysics-guided proxy model (PPM), and differentiable distribution loss (DDL).\nPND decouples the dark frame into different components and handles different\nlevels of noise flexibly, which reduces the complexity of noise modeling. PPM\nincorporates physical priors to constrain the generated noise, which promotes\nthe accuracy of noise modeling. DDL provides explicit and reliable supervision\nfor noise distribution, which promotes the precision of noise modeling. PNNP\nexhibits powerful potential in characterizing the real noise distribution.\nExtensive experiments on public datasets demonstrate superior performance in\npractical low-light raw image denoising. The code will be available at\n\\url{https://github.com/fenghansen/PNNP}.\n', '  Due to the high flexibility and remarkable performance, low-rank\napproximation methods has been widely studied for color image denoising.\nHowever, those methods mostly ignore either the cross-channel difference or the\nspatial variation of noise, which limits their capacity in real world color\nimage denoising. To overcome those drawbacks, this paper is proposed to denoise\ncolor images with a double-weighted truncated nuclear norm minus truncated\nFrobenius norm minimization (DtNFM) method. Through exploiting the nonlocal\nself-similarity of the noisy image, the similar structures are gathered and a\nseries of similar patch matrices are constructed. For each group, the DtNFM\nmodel is conducted for estimating its denoised version. The denoised image\nwould be obtained by concatenating all the denoised patch matrices. The\nproposed DtNFM model has two merits. First, it models and utilizes both the\ncross-channel difference and the spatial variation of noise. This provides\nsufficient flexibility for handling the complex distribution of noise in real\nworld images. Second, the proposed DtNFM model provides a close approximation\nto the underlying clean matrix since it can treat different rank components\nflexibly. To solve the problem resulted from DtNFM model, an accurate and\neffective algorithm is proposed by exploiting the framework of the alternating\ndirection method of multipliers (ADMM). The generated subproblems are discussed\nin detail. And their global optima can be easily obtained in closed-form.\nRigorous mathematical derivation proves that the solution sequences generated\nby the algorithm converge to a single critical point. Extensive experiments on\nsynthetic and real noise datasets demonstrate that the proposed method\noutperforms many state-of-the-art color image denoising methods.\n', ""  Noise is ubiquitous during image acquisition. Sufficient denoising is often\nan important first step for image processing. In recent decades, deep neural\nnetworks (DNNs) have been widely used for image denoising. Most DNN-based image\ndenoising methods require a large-scale dataset or focus on supervised\nsettings, in which single/pairs of clean images or a set of noisy images are\nrequired. This poses a significant burden on the image acquisition process.\nMoreover, denoisers trained on datasets of limited scale may incur\nover-fitting. To mitigate these issues, we introduce a new self-supervised\nframework for image denoising based on the Tucker low-rank tensor\napproximation. With the proposed design, we are able to characterize our\ndenoiser with fewer parameters and train it based on a single image, which\nconsiderably improves the model's generalizability and reduces the cost of data\nacquisition. Extensive experiments on both synthetic and real-world noisy\nimages have been conducted. Empirical results show that our proposed method\noutperforms existing non-learning-based methods (e.g., low-pass filter,\nnon-local mean), single-image unsupervised denoisers (e.g., DIP, NN+BM3D)\nevaluated on both in-sample and out-sample datasets. The proposed method even\nachieves comparable performances with some supervised methods (e.g., DnCNN).\n""]",Image Denoising Techniques
504,503,11,503_scattering_regularization_neural_inversions,"['scattering', 'regularization', 'neural', 'inversions', 'inverse', 'inversion', 'cnn', 'imaging', 'shapenet', 'deep']","['inverse', 'scattering', 'inversion', 'imaging', 'invertible', 'geophysical', 'inversions', 'regularization', 'aperture', 'electromagnetic']","['scattering', 'regularization', 'inversions', 'imaging', 'shapenet', 'deep', 'sonar', 'reconstruct', 'forward', 'oceanography']","['  Regularization is critical for solving ill-posed geophysical inverse\nproblems. Explicit regularization is often used, but there are opportunities to\nexplore the implicit regularization effects that are inherent in a Neural\nNetwork structure. Researchers have discovered that the Convolutional Neural\nNetwork (CNN) architecture inherently enforces a regularization that is\nadvantageous for addressing diverse inverse problems in computer vision,\nincluding de-noising and in-painting. In this study, we examine the\napplicability of this implicit regularization to geophysical inversions. The\nCNN maps an arbitrary vector to the model space. The predicted subsurface model\nis then fed into a forward numerical simulation to generate corresponding\npredicted measurements. Subsequently, the objective function value is computed\nby comparing these predicted measurements with the observed measurements. The\nbackpropagation algorithm is employed to update the trainable parameters of the\nCNN during the inversion. Note that the CNN in our proposed method does not\nrequire training before the inversion, rather, the CNN weights are estimated in\nthe inversion process, hence this is a test-time learning (TTL) approach. In\nthis study, we choose to focus on the Direct Current (DC) resistivity inverse\nproblem, which is representative of typical Tikhonov-style geophysical\ninversions (e.g. gravity, electromagnetic, etc.), to test our hypothesis. The\nexperimental results demonstrate that the implicit regularization can be useful\nin some DC resistivity inversions. We also provide a discussion of the\npotential sources of this implicit regularization introduced from the CNN\narchitecture and discuss some practical guides for applying the proposed method\nto other geophysical methods.\n', '  In this paper, we consider a deep learning approach to the limited aperture\ninverse obstacle scattering problem. It is well known that traditional deep\nlearning relies solely on data, which may limit its performance for the inverse\nproblem when only indirect observation data and a physical model are available.\nA fundamental question arises in light of these limitations: is it possible to\nenable deep learning to work on inverse problems without labeled data and to be\naware of what it is learning? This work proposes a deep decomposition method\n(DDM) for such purposes, which does not require ground truth labels. It\naccomplishes this by providing physical operators associated with the\nscattering model to the neural network architecture. Additionally, a deep\nlearning based data completion scheme is implemented in DDM to prevent\ndistorting the solution of the inverse problem for limited aperture data.\nFurthermore, apart from addressing the ill-posedness imposed by the inverse\nproblem itself, DDM is a physics-aware machine learning technique that can have\ninterpretability property. The convergence result of DDM is theoretically\nproven. Numerical experiments are presented to demonstrate the validity of the\nproposed DDM even when the incident and observation apertures are extremely\nlimited.\n', ""  Inverse scattering problems are inherently challenging, given the fact they\nare ill-posed and nonlinear. This paper presents a powerful deep learning-based\napproach that relies on generative adversarial networks to accurately and\nefficiently reconstruct randomly-shaped two-dimensional dielectric objects from\namplitudes of multi-frequency scattered electric fields. An adversarial\nautoencoder (AAE) is trained to learn to generate the scatterer's geometry from\na lower-dimensional latent representation constrained to adhere to the Gaussian\ndistribution. A cohesive inverse neural network (INN) framework is set up\ncomprising a sequence of appropriately designed dense layers, the\nalready-trained generator as well as a separately trained forward neural\nnetwork. The images reconstructed at the output of the inverse network are\nvalidated through comparison with outputs from the forward neural network,\naddressing the non-uniqueness challenge inherent to electromagnetic (EM)\nimaging problems. The trained INN demonstrates an enhanced robustness,\nevidenced by a mean binary cross-entropy (BCE) loss of $0.13$ and a structure\nsimilarity index (SSI) of $0.90$. The study not only demonstrates a significant\nreduction in computational load, but also marks a substantial improvement over\ntraditional objective-function-based methods. It contributes both to the fields\nof machine learning and EM imaging by offering a real-time quantitative imaging\napproach. The results obtained with the simulated data, for both training and\ntesting, yield promising results and may open new avenues for radio-frequency\ninverse imaging.\n""]","""Neural Networks for Inverse Scattering Problems"""
505,504,11,504_hashing_hash_descriptors_retrieval,"['hashing', 'hash', 'descriptors', 'retrieval', 'bucket', 'descriptor', 'buckets', 'embeddings', 'embedding', 'images']","['hashing', 'hash', 'quantization', 'bucket', 'sign', 'retrieval', 'codes', 'binary', 'search', 'image']","['hashing', 'bucket', 'descriptor', 'embeddings', 'similarity', 'storage', 'deep', 'codes', 'quantization', 'distillation']","['  Due to its low storage cost and fast query speed, hashing has been widely\nused in large-scale image retrieval tasks. Hash bucket search returns data\npoints within a given Hamming radius to each query, which can enable search at\na constant or sub-linear time cost. However, existing hashing methods cannot\nachieve satisfactory retrieval performance for hash bucket search in complex\nscenarios, since they learn only one hash code for each image. More\nspecifically, by using one hash code to represent one image, existing methods\nmight fail to put similar image pairs to the buckets with a small Hamming\ndistance to the query when the semantic information of images is complex. As a\nresult, a large number of hash buckets need to be visited for retrieving\nsimilar images, based on the learned codes. This will deteriorate the\nefficiency of hash bucket search. In this paper, we propose a novel hashing\nframework, called multiple code hashing (MCH), to improve the performance of\nhash bucket search. The main idea of MCH is to learn multiple hash codes for\neach image, with each code representing a different region of the image.\nFurthermore, we propose a deep reinforcement learning algorithm to learn the\nparameters in MCH. To the best of our knowledge, this is the first work that\nproposes to learn multiple hash codes for each image in image retrieval.\nExperiments demonstrate that MCH can achieve a significant improvement in hash\nbucket search, compared with existing methods that learn only one hash code for\neach image.\n', '  Unsupervised semantic hashing has emerged as an indispensable technique for\nfast image search, which aims to convert images into binary hash codes without\nrelying on labels. Recent advancements in the field demonstrate that employing\nlarge-scale backbones (e.g., ViT) in unsupervised semantic hashing models can\nyield substantial improvements. However, the inference delay has become\nincreasingly difficult to overlook. Knowledge distillation provides a means for\npractical model compression to alleviate this delay. Nevertheless, the\nprevailing knowledge distillation approaches are not explicitly designed for\nsemantic hashing. They ignore the unique search paradigm of semantic hashing,\nthe inherent necessities of the distillation process, and the property of hash\ncodes. In this paper, we propose an innovative Bit-mask Robust Contrastive\nknowledge Distillation (BRCD) method, specifically devised for the distillation\nof semantic hashing models. To ensure the effectiveness of two kinds of search\nparadigms in the context of semantic hashing, BRCD first aligns the semantic\nspaces between the teacher and student models through a contrastive knowledge\ndistillation objective. Additionally, to eliminate noisy augmentations and\nensure robust optimization, a cluster-based method within the knowledge\ndistillation process is introduced. Furthermore, through a bit-level analysis,\nwe uncover the presence of redundancy bits resulting from the bit independence\nproperty. To mitigate these effects, we introduce a bit mask mechanism in our\nknowledge distillation objective. Finally, extensive experiments not only\nshowcase the noteworthy performance of our BRCD method in comparison to other\nknowledge distillation methods but also substantiate the generality of our\nmethods across diverse semantic hashing models and backbones. The code for BRCD\nis available at https://github.com/hly1998/BRCD.\n', '  Leveraging supervised information can lead to superior retrieval performance\nin the image hashing domain but the performance degrades significantly without\nenough labeled data. One effective solution to boost performance is to employ\ngenerative models, such as Generative Adversarial Networks (GANs), to generate\nsynthetic data in an image hashing model. However, GAN-based methods are\ndifficult to train, which prevents the hashing approaches from jointly training\nthe generative models and the hash functions. This limitation results in\nsub-optimal retrieval performance. To overcome this limitation, we propose a\nnovel framework, the generative cooperative hashing network, which is based on\nenergy-based cooperative learning. This framework jointly learns a powerful\ngenerative representation of the data and a robust hash function via two\ncomponents: a top-down contrastive pair generator that synthesizes contrastive\nimages and a bottom-up multipurpose descriptor that simultaneously represents\nthe images from multiple perspectives, including probability density, hash\ncode, latent code, and category. The two components are jointly learned via a\nnovel likelihood-based cooperative learning scheme. We conduct experiments on\nseveral real-world datasets and show that the proposed method outperforms the\ncompeting hashing supervised methods, achieving up to 10\\% relative improvement\nover the current state-of-the-art supervised hashing methods, and exhibits a\nsignificantly better performance in out-of-distribution retrieval.\n']",Image Hashing for Efficient Retrieval
506,505,11,505_gans_gan_generative_cyclegan,"['gans', 'gan', 'generative', 'cyclegan', 'codegan', 'adversarial', 'ccgan', 'stylegan', 'ccgans', 'asymmetricgan']","['discriminator', 'generator', 'generative', 'image', 'unsupervised', 'adversarial', 'stability', 'images', 'collapse', 'asymmetric']","['gans', 'ccgan', 'asymmetricgan', 'learninginvision', 'inpainting', 'colorization', 'forward', 'moverseai', 'mode', 'realworld']","[""  Generative Adversarial Networks (GANs) significantly advanced image\ngeneration but their performance heavily depends on abundant training data. In\nscenarios with limited data, GANs often struggle with discriminator overfitting\nand unstable training. Batch Normalization (BN), despite being known for\nenhancing generalization and training stability, has rarely been used in the\ndiscriminator of Data-Efficient GANs. Our work addresses this gap by\nidentifying a critical flaw in BN: the tendency for gradient explosion during\nthe centering and scaling steps. To tackle this issue, we present CHAIN\n(lipsCHitz continuity constrAIned Normalization), which replaces the\nconventional centering step with zero-mean regularization and integrates a\nLipschitz continuity constraint in the scaling step. CHAIN further enhances GAN\ntraining by adaptively interpolating the normalized and unnormalized features,\neffectively avoiding discriminator overfitting. Our theoretical analyses firmly\nestablishes CHAIN's effectiveness in reducing gradients in latent features and\nweights, improving stability and generalization in GAN training. Empirical\nevidence supports our theory. CHAIN achieves state-of-the-art results in\ndata-limited scenarios on CIFAR-10/100, ImageNet, five low-shot and seven\nhigh-resolution few-shot image datasets. Code:\nhttps://github.com/MaxwellYaoNi/CHAIN\n"", ""  Generative adversarial networks (GANs) learn a target probability\ndistribution by optimizing a generator and a discriminator with minimax\nobjectives. This paper addresses the question of whether such optimization\nactually provides the generator with gradients that make its distribution close\nto the target distribution. We derive metrizable conditions, sufficient\nconditions for the discriminator to serve as the distance between the\ndistributions by connecting the GAN formulation with the concept of sliced\noptimal transport. Furthermore, by leveraging these theoretical results, we\npropose a novel GAN training scheme, called slicing adversarial network (SAN).\nWith only simple modifications, a broad class of existing GANs can be converted\nto SANs. Experiments on synthetic and image datasets support our theoretical\nresults and the SAN's effectiveness as compared to usual GANs. Furthermore, we\nalso apply SAN to StyleGAN-XL, which leads to state-of-the-art FID score\namongst GANs for class conditional generation on ImageNet 256$\\times$256. Our\nimplementation is available on https://ytakida.github.io/san.\n"", ""  Continuous Conditional Generative Adversarial Networks (CcGANs) enable\ngenerative modeling conditional on continuous scalar variables (termed\nregression labels). However, they can produce subpar fake images due to limited\ntraining data. Although Negative Data Augmentation (NDA) effectively enhances\nunconditional and class-conditional GANs by introducing anomalies into real\ntraining images, guiding the GANs away from low-quality outputs, its impact on\nCcGANs is limited, as it fails to replicate negative samples that may occur\nduring the CcGAN sampling. We present a novel NDA approach called Dual-NDA\nspecifically tailored for CcGANs to address this problem. Dual-NDA employs two\ntypes of negative samples: visually unrealistic images generated from a\npre-trained CcGAN and label-inconsistent images created by manipulating real\nimages' labels. Leveraging these negative samples, we introduce a novel\ndiscriminator objective alongside a modified CcGAN training algorithm.\nEmpirical analysis on UTKFace and Steering Angle reveals that Dual-NDA\nconsistently enhances the visual fidelity and label consistency of fake images\ngenerated by CcGANs, exhibiting a substantial performance gain over the vanilla\nNDA. Moreover, by applying Dual-NDA, CcGANs demonstrate a remarkable\nadvancement beyond the capabilities of state-of-the-art conditional GANs and\ndiffusion models, establishing a new pinnacle of performance. Our codes can be\nfound at https://github.com/UBCDingXin/Dual-NDA.\n""]",Generative Adversarial Networks (GANs)
507,506,11,506_climate_climateq_climatebert_nlp,"['climate', 'climateq', 'climatebert', 'nlp', 'climateli', 'climatepolicyradar', 'ecoverse', 'narratives', 'policymakers', 'topics']","['climate', 'change', 'crisis', 'targets', 'economic', 'national', 'narratives', 'demand', 'related', 'impacts']","['climate', 'nlp', 'ecoverse', 'narratives', 'policymakers', 'topics', 'environmentalists', 'extracting', 'biodiversity', 'geographies']","[""  Quantified policy targets are a fundamental element of climate policy,\ntypically characterised by domain-specific and technical language. Current\nmethods for curating comprehensive views of global climate policy targets\nentail significant manual effort. At present there are few scalable methods for\nextracting climate targets from national laws or policies, which limits\npolicymakers' and researchers' ability to (1) assess private and public sector\nalignment with global goals and (2) inform policy decisions. In this paper we\npresent an approach for extracting mentions of climate targets from national\nlaws and policies. We create an expert-annotated dataset identifying three\ncategories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewable\nenergy targets)) and train a classifier to reliably identify them in text. We\ninvestigate bias and equity impacts related to our model and identify specific\nyears and country names as problematic features. Finally, we investigate the\ncharacteristics of the dataset produced by running this classifier on the\nClimate Policy Radar (CPR) dataset of global national climate laws and policies\nand UNFCCC submissions, highlighting the potential of automated and scalable\ndata collection for existing climate policy databases and supporting further\nresearch. Our work represents a significant upgrade in the accessibility of\nthese key climate policy elements for policymakers and researchers. We publish\nour model at https://huggingface.co/ClimatePolicyRadar/national-climate-targets\nand related dataset at\nhttps://huggingface.co/datasets/ClimatePolicyRadar/national-climate-targets.\n"", ""  Climate change poses grave challenges, demanding widespread understanding and\nlow-carbon lifestyle awareness. Large language models (LLMs) offer a powerful\ntool to address this crisis, yet comprehensive evaluations of their\nclimate-crisis knowledge are lacking. This paper proposes an automated\nevaluation framework to assess climate-crisis knowledge within LLMs. We adopt a\nhybrid approach for data acquisition, combining data synthesis and manual\ncollection, to compile a diverse set of questions encompassing various aspects\nof climate change. Utilizing prompt engineering based on the compiled\nquestions, we evaluate the model's knowledge by analyzing its generated\nanswers. Furthermore, we introduce a comprehensive set of metrics to assess\nclimate-crisis knowledge, encompassing indicators from 10 distinct\nperspectives. These metrics provide a multifaceted evaluation, enabling a\nnuanced understanding of the LLMs' climate crisis comprehension. The\nexperimental results demonstrate the efficacy of our proposed method. In our\nevaluation utilizing diverse high-performing LLMs, we discovered that while\nLLMs possess considerable climate-related knowledge, there are shortcomings in\nterms of timeliness, indicating a need for continuous updating and refinement\nof their climate-related content.\n"", ""  In this study, we propose a methodology to extract, index, and visualize\n``climate change narratives'' (stories about the connection between causal and\nconsequential events related to climate change). We use two natural language\nprocessing methods, BERT (Bidirectional Encoder Representations from\nTransformers) and causal extraction, to textually analyze newspaper articles on\nclimate change to extract ``climate change narratives.'' The novelty of the\nmethodology could extract and quantify the causal relationships assumed by the\nnewspaper's writers. Looking at the extracted climate change narratives over\ntime, we find that since 2018, an increasing number of narratives suggest the\nimpact of the development of climate change policy discussion and the\nimplementation of climate change-related policies on corporate behaviors,\nmacroeconomics, and price dynamics. We also observed the recent emergence of\nnarratives focusing on the linkages between climate change-related policies and\nmonetary policy. Furthermore, there is a growing awareness of the negative\nimpacts of natural disasters (e.g., abnormal weather and severe floods) related\nto climate change on economic activities, and this issue might be perceived as\na new challenge for companies and governments. The methodology of this study is\nexpected to be applied to a wide range of fields, as it can analyze causal\nrelationships among various economic topics, including analysis of inflation\nexpectation or monetary policy communication strategy.\n""]",Climate Change Policy and Research
508,507,11,507_bayesflow_mlmc_posteriors_posterior,"['bayesflow', 'mlmc', 'posteriors', 'posterior', 'likelihoods', 'bayesian', 'estimating', 'likelihood', 'simulations', 'inference']","['posterior', 'inference', 'likelihood', 'intractable', 'simulation', 'simulations', 'estimators', 'posteriors', 'summary', 'components']","['bayesflow', 'mlmc', 'posteriors', 'estimating', 'simulators', 'mrna', 'normalizing', 'parameters', 'carlo', 'sbmi']","['  Amortized Bayesian inference trains neural networks to solve stochastic\ninference problems using model simulations, thereby making it possible to\nrapidly perform Bayesian inference for any newly observed data. However,\ncurrent simulation-based amortized inference methods are simulation-hungry and\ninflexible: They require the specification of a fixed parametric prior,\nsimulator, and inference tasks ahead of time. Here, we present a new amortized\ninference method -- the Simformer -- which overcomes these limitations. By\ntraining a probabilistic diffusion model with transformer architectures, the\nSimformer outperforms current state-of-the-art amortized inference approaches\non benchmark tasks and is substantially more flexible: It can be applied to\nmodels with function-valued parameters, it can handle inference scenarios with\nmissing or unstructured data, and it can sample arbitrary conditionals of the\njoint distribution of parameters and data, including both posterior and\nlikelihood. We showcase the performance and flexibility of the Simformer on\nsimulators from ecology, epidemiology, and neuroscience, and demonstrate that\nit opens up new possibilities and application domains for amortized Bayesian\ninference on simulation-based models.\n', '  Bayesian inference for complex models with an intractable likelihood can be\ntackled using algorithms performing many calls to computer simulators. These\napproaches are collectively known as ""simulation-based inference"" (SBI). Recent\nSBI methods have made use of neural networks (NN) to provide approximate, yet\nexpressive constructs for the unavailable likelihood function and the posterior\ndistribution. However, the trade-off between accuracy and computational demand\nleaves much space for improvement. In this work, we propose an alternative that\nprovides both approximations to the likelihood and the posterior distribution,\nusing structured mixtures of probability distributions. Our approach produces\naccurate posterior inference when compared to state-of-the-art NN-based SBI\nmethods, even for multimodal posteriors, while exhibiting a much smaller\ncomputational footprint. We illustrate our results on several benchmark models\nfrom the SBI literature and on a biological model of the translation kinetics\nafter mRNA transfection.\n', '  Simulation based inference (SBI) methods enable the estimation of posterior\ndistributions when the likelihood function is intractable, but where model\nsimulation is feasible. Popular neural approaches to SBI are the neural\nposterior estimator (NPE) and its sequential version (SNPE). These methods can\noutperform statistical SBI approaches such as approximate Bayesian computation\n(ABC), particularly for relatively small numbers of model simulations. However,\nwe show in this paper that the NPE methods are not guaranteed to be highly\naccurate, even on problems with low dimension. In such settings the posterior\ncannot be accurately trained over the prior predictive space, and even the\nsequential extension remains sub-optimal. To overcome this, we propose\npreconditioned NPE (PNPE) and its sequential version (PSNPE), which uses a\nshort run of ABC to effectively eliminate regions of parameter space that\nproduce large discrepancy between simulations and data and allow the posterior\nemulator to be more accurately trained. We present comprehensive empirical\nevidence that this melding of neural and statistical SBI methods improves\nperformance over a range of examples, including a motivating example involving\na complex agent-based model applied to real tumour growth data.\n']",Bayesian Inference Methods for Complex Models
509,508,10,508_statistical_metrics_experimentation_experimenters,"['statistical', 'metrics', 'experimentation', 'experimenters', 'experiments', 'testing', 'outliers', 'tests', 'confidence', 'metric']","['variance', 'experiment', 'width', 'duration', 'metrics', 'online', 'experimenters', 'effect', 'reduction', 'experiments']","['experimentation', 'outliers', 'retention', 'loglikehood', 'mlrate', 'nonparametric', 'duration', 'insignificantly', 'revenue', 'heteroskedasticity']","['  Online controlled experiments are a crucial tool to allow for confident\ndecision-making in technology companies. A North Star metric is defined (such\nas long-term revenue or user retention), and system variants that statistically\nsignificantly improve on this metric in an A/B-test can be considered superior.\nNorth Star metrics are typically delayed and insensitive. As a result, the cost\nof experimentation is high: experiments need to run for a long time, and even\nthen, type-II errors (i.e. false negatives) are prevalent.\n  We propose to tackle this by learning metrics from short-term signals that\ndirectly maximise the statistical power they harness with respect to the North\nStar. We show that existing approaches are prone to overfitting, in that higher\naverage metric sensitivity does not imply improved type-II errors, and propose\nto instead minimise the $p$-values a metric would have produced on a log of\npast experiments. We collect such datasets from two social media applications\nwith over 160 million Monthly Active Users each, totalling over 153 A/B-pairs.\nEmpirical results show that we are able to increase statistical power by up to\n78% when using our learnt metrics stand-alone, and by up to 210% when used in\ntandem with the North Star. Alternatively, we can obtain constant statistical\npower at a sample size that is down to 12% of what the North Star requires,\nsignificantly reducing the cost of experimentation.\n', ""  Online controlled experiments, colloquially known as A/B-tests, are the bread\nand butter of real-world recommender system evaluation. Typically, end-users\nare randomly assigned some system variant, and a plethora of metrics are then\ntracked, collected, and aggregated throughout the experiment. A North Star\nmetric (e.g. long-term growth or revenue) is used to assess which system\nvariant should be deemed superior. As a result, most collected metrics are\nsupporting in nature, and serve to either (i) provide an understanding of how\nthe experiment impacts user experience, or (ii) allow for confident\ndecision-making when the North Star metric moves insignificantly (i.e. a false\nnegative or type-II error). The latter is not straightforward: suppose a\ntreatment variant leads to fewer but longer sessions, with more views but fewer\nengagements; should this be considered a positive or negative outcome?\n  The question then becomes: how do we assess a supporting metric's utility\nwhen it comes to decision-making using A/B-testing? Online platforms typically\nrun dozens of experiments at any given time. This provides a wealth of\ninformation about interventions and treatment effects that can be used to\nevaluate metrics' utility for online evaluation. We propose to collect this\ninformation and leverage it to quantify type-I, type-II, and type-III errors\nfor the metrics of interest, alongside a distribution of measurements of their\nstatistical power (e.g. $z$-scores and $p$-values). We present results and\ninsights from building this pipeline at scale for two large-scale short-video\nplatforms: ShareChat and Moj; leveraging hundreds of past experiments to find\nonline metrics with high statistical power.\n"", '  In designing an online A/B experiment, it is crucial to select a sample size\nand duration that ensure the resulting confidence interval (CI) for the\ntreatment effect is the right width to detect an effect of meaningful magnitude\nwith sufficient statistical power without wasting resources. While the\nrelationship between sample size and CI width is well understood, the effect of\nexperiment duration on CI width remains less clear. This paper provides an\nanalytical formula for the width of a CI based on a ratio treatment effect\nestimator as a function of both sample size (N) and duration (T). The formula\nis derived from a mixed effects model with two variance components. One\ncomponent, referred to as the temporal variance, persists over time for\nexperiments where the same users are kept in the same experiment arm across\ndifferent days. The remaining error variance component, by contrast, decays to\nzero as T gets large. The formula we derive introduces a key parameter that we\ncall the user-specific temporal correlation (UTC), which quantifies the\nrelative sizes of the two variance components and can be estimated from\nhistorical experiments. Higher UTC indicates a slower decay in CI width over\ntime. On the other hand, when the UTC is 0 -- as for experiments where users\nshuffle in and out of the experiment across days -- the CI width decays at the\nstandard parametric 1/T rate. We also study how access to pre-period data for\nthe users in the experiment affects the CI width decay. We show our formula\nclosely explains CI widths on real A/B experiments at YouTube.\n']",Statistical Metrics for Experimentation
510,509,10,509_nlp_adapting_tasks_language,"['nlp', 'adapting', 'tasks', 'language', 'pretraining', 'paraphrases', 'tokenizer', 'curriculum', 'examples', 'task']","['curricula', 'priming', 'saving', 'specialised', 'shot', 'downstream', 'domain', 'fine', 'tokenizer', 'tuning']","['nlp', 'adapting', 'tasks', 'pretraining', 'paraphrases', 'tokenizer', 'curriculum', 'dataset', 'sequencing', 'priming']","['  Large Language Models (LLMs) have been observed to perform well on a wide\nrange of downstream tasks when fine-tuned on domain-specific data. However,\nsuch data may not be readily available in many applications, motivating\nzero-shot or few-shot approaches using domain-adjacent models. While several\nfine-tuned models for various tasks are available, finding an appropriate\ndomain-adjacent model for a given task is often not straight forward. In this\npaper, we study DAFT-E, a framework that utilizes an Ensemble of\nDomain-Adjacent Fine-Tuned Foundation Models for few-shot problems. We show\nthat for zero-shot problems, this ensembling method provides an accuracy\nperformance close to that of the single best model. With few-shot problems,\nthis performance improves further, at which point DEFT-E can outperform any\nsingle domain-adjacent model while requiring much less data for domain-specific\nfine-tuning.\n', '  Parameter-efficient (PE) methods (like Prompts or Adapters) for adapting\npre-trained language models (PLM) to downstream tasks have been popular\nrecently. However, hindrances still prevent these methods from reaching their\nfull potential. For example, two significant challenges are few-shot adaptation\nand cross-task generalization. To tackle these issues, we propose a general PE\npriming framework to enhance and explore the few-shot adaptation and\ngeneralization ability of PE methods. In this framework, PLMs are primed with\nPE methods for rapidly adapting to various target tasks. To evaluate the\ngeneralization ability of these PE methods, we conduct experiments on a\nfew-shot cross-domain benchmark containing 160 diverse NLP tasks. Our\nexperiment not only reveals the best priming strategy but also verifies that\npriming facilitates the adaptation to target tasks.\n', ""  Pretrained Large Language Models (LLM) such as ChatGPT, Claude, etc. have\ndemonstrated strong capabilities in various fields of natural language\ngeneration. However, there are still many problems when using LLM in\nspecialized domain-specific fields. When using generative AI to process\ndownstream tasks, a common approach is to add new knowledge (e.g., private\ndomain knowledge, cutting-edge information) to a pretrained model through\ncontinued training or fine-tuning. However, whether there is a universal\nparadigm for domain adaptation training is still an open question. In this\narticle, we proposed Information Gain Optimized Tokenizer (IGOT), which\nanalyzes the special token set of downstream tasks, constructs a new subset\nusing heuristic function $\\phi$ with the special token and its information\ngain, to build new domain-specific tokenizer, and continues pretraining on the\ndownstream task data. We explored the many positive effects of this method's\ncustomized tokenizer on domain-adaptive pretraining and verified this method\ncan perform better than the ordinary method of just collecting data and\nfine-tuning. Based on our experiment, the continued pretraining process of IGOT\nwith LLaMA-7B achieved 11.9\\% token saving, 12.2\\% training time saving, and\n5.8\\% maximum GPU VRAM usage saving, combined with the T5 model, we can even\nreach a 31.5\\% of training time saving, making porting general generative AI to\nspecific domains more effective than before. In domain-specific tasks,\nsupervised $IGOT_\\tau$ shows great performance on reducing both the convergence\nradius and convergence point during keep pretraining.\n""]",Adapting Large Language Models to Downstream NLP Tasks
511,510,10,510_distractors_distractor_answerability_exams,"['distractors', 'distractor', 'answerability', 'exams', 'students', 'graded', 'learner', 'comprehension', 'pedagogical', 'assessment']","['distractors', 'distractor', 'math', 'misconceptions', 'choice', 'educators', 'students', 'questions', 'generation', 'incorrect']","['distractors', 'answerability', 'exams', 'comprehension', 'questions', 'prompting', 'feedback', 'task', 'educators', 'mcq']","['  High-quality distractors are crucial to both the assessment and pedagogical\nvalue of multiple-choice questions (MCQs), where manually crafting ones that\nanticipate knowledge deficiencies or misconceptions among real students is\ndifficult. Meanwhile, automated distractor generation, even with the help of\nlarge language models (LLMs), remains challenging for subjects like math. It is\ncrucial to not only identify plausible distractors but also understand the\nerror behind them. In this paper, we introduce DiVERT (Distractor Generation\nwith Variational Errors Represented as Text), a novel variational approach that\nlearns an interpretable representation of errors behind distractors in math\nMCQs. Through experiments on a real-world math MCQ dataset with 1,434 questions\nused by hundreds of thousands of students, we show that DiVERT, despite using a\nbase open-source LLM with 7B parameters, outperforms state-of-the-art\napproaches using GPT-4o on downstream distractor generation. We also conduct a\nhuman evaluation with math educators and find that DiVERT leads to error labels\nthat are of comparable quality to human-authored ones.\n', '  Multiple-choice questions (MCQs) are ubiquitous in almost all levels of\neducation since they are easy to administer, grade, and are a reliable format\nin assessments and practices. One of the most important aspects of MCQs is the\ndistractors, i.e., incorrect options that are designed to target common errors\nor misconceptions among real students. To date, the task of crafting\nhigh-quality distractors largely remains a labor and time-intensive process for\nteachers and learning content designers, which has limited scalability. In this\nwork, we study the task of automated distractor generation in the domain of\nmath MCQs and explore a wide variety of large language model (LLM)-based\napproaches, from in-context learning to fine-tuning. We conduct extensive\nexperiments using a real-world math MCQ dataset and find that although LLMs can\ngenerate some mathematically valid distractors, they are less adept at\nanticipating common errors or misconceptions among real students.\n', '  Multiple-choice questions (MCQs) are ubiquitous in almost all levels of\neducation since they are easy to administer, grade, and are a reliable form of\nassessment. An important aspect of MCQs is the distractors, i.e., incorrect\noptions that are designed to target specific misconceptions or insufficient\nknowledge among students. To date, the task of crafting high-quality\ndistractors has largely remained a labor-intensive process for teachers and\nlearning content designers, which has limited scalability. In this work, we\nexplore the task of automated distractor and corresponding feedback message\ngeneration in math MCQs using large language models. We establish a formulation\nof these two tasks and propose a simple, in-context learning-based solution.\nMoreover, we propose generative AI-based metrics for evaluating the quality of\nthe feedback messages. We conduct extensive experiments on these tasks using a\nreal-world MCQ dataset. Our findings suggest that there is a lot of room for\nimprovement in automated distractor and feedback generation; based on these\nfindings, we outline several directions for future work.\n']",Automated Distractor Generation for Multiple-Choice Questions
512,511,10,511_learns_deepmind_reinforcement_learning,"['learns', 'deepmind', 'reinforcement', 'learning', 'atari', 'visual', 'representations', 'encoder', 'vision', 'atari100k']","['visual', 'imagination', 'environment', 'patches', 'world', 'reinforcement', 'dynamics', 'representations', 'elements', 'reconstruction']","['learns', 'deepmind', 'reinforcement', 'representations', 'atari100k', 'simulator', 'modeling', 'agent', 'dreamer', 'pixel']","['  Recently, various pre-training methods have been introduced in vision-based\nReinforcement Learning (RL). However, their generalization ability remains\nunclear due to evaluations being limited to in-distribution environments and\nnon-unified experimental setups. To address this, we introduce the Atari\nPre-training Benchmark (Atari-PB), which pre-trains a ResNet-50 model on 10\nmillion transitions from 50 Atari games and evaluates it across diverse\nenvironment distributions. Our experiments show that pre-training objectives\nfocused on learning task-agnostic features (e.g., identifying objects and\nunderstanding temporal dynamics) enhance generalization across different\nenvironments. In contrast, objectives focused on learning task-specific\nknowledge (e.g., identifying agents and fitting reward functions) improve\nperformance in environments similar to the pre-training dataset but not in\nvaried ones. We publicize our codes, datasets, and model checkpoints at\nhttps://github.com/dojeon-ai/Atari-PB.\n', '  We propose Value Explicit Pretraining (VEP), a method that learns\ngeneralizable representations for transfer reinforcement learning. VEP enables\nlearning of new tasks that share similar objectives as previously learned\ntasks, by learning an encoder for objective-conditioned representations,\nirrespective of appearance changes and environment dynamics. To pre-train the\nencoder from a sequence of observations, we use a self-supervised contrastive\nloss that results in learning temporally smooth representations. VEP learns to\nrelate states across different tasks based on the Bellman return estimate that\nis reflective of task progress. Experiments using a realistic navigation\nsimulator and Atari benchmark show that the pretrained encoder produced by our\nmethod outperforms current SoTA pretraining methods on the ability to\ngeneralize to unseen tasks. VEP achieves up to a 2 times improvement in rewards\non Atari and visual navigation, and up to a 3 times improvement in sample\nefficiency. For videos of policy performance visit our\nhttps://sites.google.com/view/value-explicit-pretraining/\n', '  One of the biggest challenges to modern deep reinforcement learning (DRL)\nalgorithms is sample efficiency. Many approaches learn a world model in order\nto train an agent entirely in imagination, eliminating the need for direct\nenvironment interaction during training. However, these methods often suffer\nfrom either a lack of imagination accuracy, exploration capabilities, or\nruntime efficiency. We propose Hieros, a hierarchical policy that learns time\nabstracted world representations and imagines trajectories at multiple time\nscales in latent space. Hieros uses an S5 layer-based world model, which\npredicts next world states in parallel during training and iteratively during\nenvironment interaction. Due to the special properties of S5 layers, our method\ncan train in parallel and predict next world states iteratively during\nimagination. This allows for more efficient training than RNN-based world\nmodels and more efficient imagination than Transformer-based world models.\n  We show that our approach outperforms the state of the art in terms of mean\nand median normalized human score on the Atari 100k benchmark, and that our\nproposed world model is able to predict complex dynamics very accurately. We\nalso show that Hieros displays superior exploration capabilities compared to\nexisting approaches.\n']",Reinforcement Learning with Pre-Training Methods
513,512,10,512_radar_radars_lidar_radarocc,"['radar', 'radars', 'lidar', 'radarocc', 'tracking', 'detection', 'clouds', 'ranging', 'maps', 'doppler']","['radar', 'lidar', 'surround', 'spectra', 'driving', 'tracking', 'object', 'camera', 'sensors', 'weather']","['radars', 'lidar', 'clouds', 'ranging', 'maps', 'doppler', 'driving', 'cameras', 'adas', 'fusion']","['  Fusing Radar and Lidar sensor data can fully utilize their complementary\nadvantages and provide more accurate reconstruction of the surrounding for\nautonomous driving systems. Surround Radar/Lidar can provide 360-degree view\nsampling with the minimal cost, which are promising sensing hardware solutions\nfor autonomous driving systems. However, due to the intrinsic physical\nconstraints, the rotating speed of surround Radar, and thus the frequency to\ngenerate Radar data frames, is much lower than surround Lidar. Existing\nRadar/Lidar fusion methods have to work at the low frequency of surround Radar,\nwhich cannot meet the high responsiveness requirement of autonomous driving\nsystems.This paper develops techniques to fuse surround Radar/Lidar with\nworking frequency only limited by the faster surround Lidar instead of the\nslower surround Radar, based on the state-of-the-art object detection model\nMVDNet. The basic idea of our approach is simple: we let MVDNet work with\ntemporally unaligned data from Radar/Lidar, so that fusion can take place at\nany time when a new Lidar data frame arrives, instead of waiting for the slow\nRadar data frame. However, directly applying MVDNet to temporally unaligned\nRadar/Lidar data greatly degrades its object detection accuracy. The key\ninformation revealed in this paper is that we can achieve high output frequency\nwith little accuracy loss by enhancing the training procedure to explore the\ntemporal redundancy in MVDNet so that it can tolerate the temporal unalignment\nof input data. We explore several different ways of training enhancement and\ncompare them quantitatively with experiments.\n', '  This paper presents a novel deep-learning-based approach to improve\nlocalizing radar measurements against lidar maps. Although the state of the art\nfor localization is matching lidar data to lidar maps, radar has been\nconsidered as a promising alternative. This is largely due to radar being more\nresilient against adverse weather such as precipitation and heavy fog. To make\nuse of existing high-quality lidar maps, while maintaining performance in\nadverse weather, it is of interest to match radar data to lidar maps. However,\nowing in part to the unique artefacts present in radar measurements,\nradar-lidar localization has struggled to achieve comparable performance to\nlidar-lidar systems, preventing it from being viable for autonomous driving.\nThis work builds on an ICP-based radar-lidar localization system by including a\nlearned preprocessing step that weights radar points based on high-level scan\ninformation. Combining a proven analytical approach with a learned weight\nreduces localization errors in radar-lidar ICP results run on real-world\nautonomous driving data by up to 54.94% in translation and 68.39% in rotation,\nwhile maintaining interpretability and robustness.\n', '  With the rapid advancements of sensor technology and deep learning,\nautonomous driving systems are providing safe and efficient access to\nintelligent vehicles as well as intelligent transportation. Among these\nequipped sensors, the radar sensor plays a crucial role in providing robust\nperception information in diverse environmental conditions. This review focuses\non exploring different radar data representations utilized in autonomous\ndriving systems. Firstly, we introduce the capabilities and limitations of the\nradar sensor by examining the working principles of radar perception and signal\nprocessing of radar measurements. Then, we delve into the generation process of\nfive radar representations, including the ADC signal, radar tensor, point\ncloud, grid map, and micro-Doppler signature. For each radar representation, we\nexamine the related datasets, methods, advantages and limitations. Furthermore,\nwe discuss the challenges faced in these data representations and propose\npotential research directions. Above all, this comprehensive review offers an\nin-depth insight into how these representations enhance autonomous system\ncapabilities, providing guidance for radar perception researchers. To\nfacilitate retrieval and comparison of different data representations, datasets\nand methods, we provide an interactive website at\nhttps://radar-camera-fusion.github.io/radar.\n']",Radar and Lidar Sensor Fusion for Autonomous Driving
514,513,10,513_drones_drone_unmanned_uavs,"['drones', 'drone', 'unmanned', 'uavs', 'vision', 'quadrotor', 'flown', 'autonomous', 'dronet', 'sensing']","['drones', 'nano', 'navigation', 'rotor', 'drone', 'ultra', 'quadrotor', 'onboard', 'vision', 'perception']","['drones', 'uavs', 'quadrotor', 'onboard', 'actuators', 'chip', 'navigation', 'dnns', 'mav', 'perception']","['  Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning\n(TinyML), such as nano-drones, are becoming an increasingly attractive\ntechnology. Their small form factor (i.e., ~10cm diameter) ensures vast\napplicability, ranging from the exploration of narrow disaster scenarios to\nsafe human-robot interaction. Simple electronics make these CPSes inexpensive,\nbut strongly limit the computational, memory, and sensing resources available\non board. In real-world applications, these limitations are further exacerbated\nby domain shift. This fundamental machine learning problem implies that model\nperception performance drops when moving from the training domain to a\ndifferent deployment one. To cope with and mitigate this general problem, we\npresent a novel on-device fine-tuning approach that relies only on the limited\nultra-low power resources available aboard nano-drones. Then, to overcome the\nlack of ground-truth training labels aboard our CPS, we also employ a\nself-supervised method based on ego-motion consistency. Albeit our work builds\non top of a specific real-world vision-based human pose estimation task, it is\nwidely applicable for many embedded TinyML use cases. Our 512-image on-device\ntraining procedure is fully deployed aboard an ultra-low power GWT GAP9\nSystem-on-Chip and requires only 1MB of memory while consuming as low as 19mW\nor running in just 510ms (at 38mW). Finally, we demonstrate the benefits of our\non-device learning approach by field-testing our closed-loop CPS, showing a\nreduction in horizontal position error of up to 26% vs. a non-fine-tuned\nstate-of-the-art baseline. In the most challenging never-seen-before\nenvironment, our on-device learning procedure makes the difference between\nsucceeding or failing the mission.\n', '  Pocket-sized autonomous nano-drones can revolutionize many robotic use cases,\nsuch as visual inspection in narrow, constrained spaces, and ensure safer\nhuman-robot interaction due to their tiny form factor and weight -- i.e., tens\nof grams. This compelling vision is challenged by the high level of\nintelligence needed aboard, which clashes against the limited computational and\nstorage resources available on PULP (parallel-ultra-low-power) MCU class\nnavigation and mission controllers that can be hosted aboard. This work moves\nfrom PULP-Dronet, a State-of-the-Art convolutional neural network for\nautonomous navigation on nano-drones. We introduce Tiny-PULP-Dronet: a novel\nmethodology to squeeze by more than one order of magnitude model size (50x\nfewer parameters), and number of operations (27x less multiply-and-accumulate)\nrequired to run inference with similar flight performance as PULP-Dronet. This\nmassive reduction paves the way towards affordable multi-tasking on\nnano-drones, a fundamental requirement for achieving high-level intelligence.\n', ""  Sub-10cm diameter nano-drones are gaining momentum thanks to their\napplicability in scenarios prevented to bigger flying drones, such as in narrow\nenvironments and close to humans. However, their tiny form factor also brings\ntheir major drawback: ultra-constrained memory and processors for the onboard\nexecution of their perception pipelines. Therefore, lightweight deep\nlearning-based approaches are becoming increasingly popular, stressing how\ncomputational efficiency and energy-saving are paramount as they can make the\ndifference between a fully working closed-loop system and a failing one. In\nthis work, to maximize the exploitation of the ultra-limited resources aboard\nnano-drones, we present a novel adaptive deep learning-based mechanism for the\nefficient execution of a vision-based human pose estimation task. We leverage\ntwo State-of-the-Art (SoA) convolutional neural networks (CNNs) with different\nregression performance vs. computational costs trade-offs. By combining these\nCNNs with three novel adaptation strategies based on the output's temporal\nconsistency and on auxiliary tasks to swap the CNN being executed proactively,\nwe present six different systems. On a real-world dataset and the actual\nnano-drone hardware, our best-performing system, compared to executing only the\nbigger and most accurate SoA model, shows 28% latency reduction while keeping\nthe same mean absolute error (MAE), 3% MAE reduction while being iso-latency,\nand the absolute peak performance, i.e., 6% better than SoA model.\n""]",Autonomous Drones and UAVs
515,514,10,514_audioset_audio_spectrograms_spectrogram,"['audioset', 'audio', 'spectrograms', 'spectrogram', 'supervised', 'acoustic', 'auditory', 'speaker', 'sound', 'classification']","['audio', 'spectrogram', 'headphones', 'sounds', 'resolution', 'transformers', 'self', 'size', 'acoustic', 'transformer']","['audioset', 'spectrograms', 'acoustic', 'classification', 'speech', 'representations', 'data2vec', 'ssamba', 'alarm', 'aum']","['  Despite its widespread adoption as the prominent neural architecture, the\nTransformer has spurred several independent lines of work to address its\nlimitations. One such approach is selective state space models, which have\ndemonstrated promising results for language modelling. However, their\nfeasibility for learning self-supervised, general-purpose audio representations\nis yet to be investigated. This work proposes Audio Mamba, a selective state\nspace model for learning general-purpose audio representations from randomly\nmasked spectrogram patches through self-supervision. Empirical results on ten\ndiverse audio recognition downstream tasks show that the proposed models,\npretrained on the AudioSet dataset, consistently outperform comparable\nself-supervised audio spectrogram transformer (SSAST) baselines by a\nconsiderable margin and demonstrate better performance in dataset size,\nsequence length and model size comparisons.\n', ""  Transformers have revolutionized deep learning across various tasks,\nincluding audio representation learning, due to their powerful modeling\ncapabilities. However, they often suffer from quadratic complexity in both GPU\nmemory usage and computational inference time, affecting their efficiency.\nRecently, state space models (SSMs) like Mamba have emerged as a promising\nalternative, offering a more efficient approach by avoiding these complexities.\nGiven these advantages, we explore the potential of SSM-based models in audio\ntasks. In this paper, we introduce Self-Supervised Audio Mamba (SSAMBA), the\nfirst self-supervised, attention-free, and SSM-based model for audio\nrepresentation learning. SSAMBA leverages the bidirectional Mamba to capture\ncomplex audio patterns effectively. We incorporate a self-supervised\npretraining framework that optimizes both discriminative and generative\nobjectives, enabling the model to learn robust audio representations from\nlarge-scale, unlabeled datasets. We evaluated SSAMBA on various tasks such as\naudio classification, keyword spotting, and speaker identification. Our results\ndemonstrate that SSAMBA outperforms the Self-Supervised Audio Spectrogram\nTransformer (SSAST) in most tasks. Notably, SSAMBA is approximately 92.7%\nfaster in batch inference speed and 95.4% more memory-efficient than SSAST for\nthe tiny model size with an input token size of 22k. These efficiency gains,\ncombined with superior performance, underscore the effectiveness of SSAMBA's\narchitectural innovation, making it a compelling choice for a wide range of\naudio processing applications.\n"", '  Audio tagging is an important task of mapping audio samples to their\ncorresponding categories. Recently endeavours that exploit transformer models\nin this field have achieved great success. However, the quadratic\nself-attention cost limits the scaling of audio transformer models and further\nconstrains the development of more universal audio models. In this paper, we\nattempt to solve this problem by proposing Audio Mamba, a self-attention-free\napproach that captures long audio spectrogram dependency with state space\nmodels. Our experimental results on two audio-tagging datasets demonstrate the\nparameter efficiency of Audio Mamba, it achieves comparable results to SOTA\naudio spectrogram transformers with one third parameters.\n']",Audio Representation Learning with Spectrograms
516,515,10,515_loras_fedmem_llm_fedllm,"['loras', 'fedmem', 'llm', 'fedllm', 'backpropagation', 'federated', 'llms', 'privacy', 'memory', 'learner']","['gradients', 'communication', 'clients', 'heterogeneity', 'tuning', 'server', 'tune', 'fine', 'assisted', 'organizations']","['loras', 'backpropagation', 'llms', 'memory', 'fedlora', 'personalization', 'textsc', 'plugins', 'activations', 'provider']","['  The success of current Large-Language Models (LLMs) hinges on extensive\ntraining data that is collected and stored centrally, called Centralized\nLearning (CL). However, such a collection manner poses a privacy threat, and\none potential solution is Federated Learning (FL), which transfers gradients,\nnot raw data, among clients. Unlike traditional networks, FL for LLMs incurs\nsignificant communication costs due to their tremendous parameters. This study\nintroduces an innovative approach to compress gradients to improve\ncommunication efficiency during LLM FL, formulating the new FL pipeline named\nCG-FedLLM. This approach integrates an encoder on the client side to acquire\nthe compressed gradient features and a decoder on the server side to\nreconstruct the gradients. We also developed a novel training strategy that\ncomprises Temporal-ensemble Gradient-Aware Pre-training (TGAP) to identify\ncharacteristic gradients of the target model and Federated AutoEncoder-Involved\nFine-tuning (FAF) to compress gradients adaptively. Extensive experiments\nconfirm that our approach reduces communication costs and improves performance\n(e.g., average 3 points increment compared with traditional CL- and FL-based\nfine-tuning with LlaMA on a well-recognized benchmark, C-Eval). This\nimprovement is because our encoder-decoder, trained via TGAP and FAF, can\nfilter gradients while selectively preserving critical features. Furthermore,\nwe present a series of experimental analyses focusing on the signal-to-noise\nratio, compression rate, and robustness within this privacy-centric framework,\nproviding insight into developing more efficient and secure LLMs.\n', ""  Finetuning large language models (LLMs) in federated learning (FL) settings\nhas become important as it allows resource-constrained devices to finetune a\nmodel using private data. However, finetuning LLMs using backpropagation\nrequires excessive memory (especially from intermediate activations) for\nresource-constrained devices. While Forward-mode Auto-Differentiation (AD) can\nreduce memory footprint from activations, we observe that directly applying it\nto LLM finetuning results in slow convergence and poor accuracy. This work\nintroduces Spry, an FL algorithm that splits trainable weights of an LLM among\nparticipating clients, such that each client computes gradients using\nForward-mode AD that are closer estimates of the true gradients. Spry achieves\na low memory footprint, high accuracy, and fast convergence. We theoretically\nshow that the global gradients in Spry are unbiased estimates of true global\ngradients for homogeneous data distributions across clients, while\nheterogeneity increases bias of the estimates. We also derive Spry's\nconvergence rate, showing that the gradients decrease inversely proportional to\nthe number of FL rounds, indicating the convergence up to the limits of\nheterogeneity. Empirically, Spry reduces the memory footprint during training\nby 1.4-7.1$\\times$ in contrast to backpropagation, while reaching comparable\naccuracy, across a wide range of language tasks, models, and FL settings. Spry\nreduces the convergence time by 1.2-20.3$\\times$ and achieves 5.2-13.5\\% higher\naccuracy against state-of-the-art zero-order methods. When finetuning Llama2-7B\nwith LoRA, compared to the peak memory usage of 33.9GB of backpropagation, Spry\nonly consumes 6.2GB of peak memory. For OPT13B, the reduction is from 76.5GB to\n10.8GB. Spry makes feasible previously impossible FL deployments on commodity\nmobile and edge devices. Source code is available at\nhttps://github.com/Astuary/Spry.\n"", ""  Low-rank adaptation (LoRA) is a natural method for finetuning in\ncommunication-constrained machine learning settings such as cross-device\nfederated learning. Prior work that has studied LoRA in the context of\nfederated learning has focused on improving LoRA's robustness to heterogeneity\nand privacy. In this work, we instead consider techniques for further improving\ncommunication-efficiency in federated LoRA. Unfortunately, we show that\ncentralized ML methods that improve the efficiency of LoRA through unstructured\npruning do not transfer well to federated settings. We instead study a simple\napproach, \\textbf{FLASC}, that applies sparsity to LoRA during communication\nwhile allowing clients to locally fine-tune the entire LoRA module. Across four\ncommon federated learning tasks, we demonstrate that this method matches the\nperformance of dense LoRA with up to $10\\times$ less communication.\nAdditionally, despite being designed primarily to target communication, we find\nthat this approach has benefits in terms of heterogeneity and privacy relative\nto existing approaches tailored to these specific concerns. Overall, our work\nhighlights the importance of considering system-specific constraints when\ndeveloping communication-efficient finetuning approaches, and serves as a\nsimple and competitive baseline for future work in federated finetuning.\n""]",Federated Learning for Large Language Models
517,516,10,516_nationalities_cultural_cultures_language,"['nationalities', 'cultural', 'cultures', 'language', 'culturally', 'multicultural', 'culture', 'bias', 'biases', 'nationality']","['cultural', 'debiasing', 'nationality', 'country', 'biases', 'alignment', 'values', 'countries', 'culture', 'dimensions']","['nationalities', 'culturally', 'discourses', 'annotators', 'debiasing', 'korean', 'prejudices', 'underrepresentation', 'chatgpt', 'scrutiny']","[""  Large Language Models (LLMs) attempt to imitate human behavior by responding\nto humans in a way that pleases them, including by adhering to their values.\nHowever, humans come from diverse cultures with different values. It is\ncritical to understand whether LLMs showcase different values to the user based\non the stereotypical values of a user's known country. We prompt different LLMs\nwith a series of advice requests based on 5 Hofstede Cultural Dimensions -- a\nquantifiable way of representing the values of a country. Throughout each\nprompt, we incorporate personas representing 36 different countries and,\nseparately, languages predominantly tied to each country to analyze the\nconsistency in the LLMs' cultural understanding. Through our analysis of the\nresponses, we found that LLMs can differentiate between one side of a value and\nanother, as well as understand that countries have differing values, but will\nnot always uphold the values when giving advice, and fail to understand the\nneed to answer differently based on different cultural values. Rooted in these\nfindings, we present recommendations for training value-aligned and culturally\nsensitive LLMs. More importantly, the methodology and the framework developed\nhere can help further understand and mitigate culture and language alignment\nissues with LLMs.\n"", ""  The deployment of large language models (LLMs) raises concerns regarding\ntheir cultural misalignment and potential ramifications on individuals and\nsocieties with diverse cultural backgrounds. While the discourse has focused\nmainly on political and social biases, our research proposes a Cultural\nAlignment Test (Hoftede's CAT) to quantify cultural alignment using Hofstede's\ncultural dimension framework, which offers an explanatory cross-cultural\ncomparison through the latent variable analysis. We apply our approach to\nquantitatively evaluate LLMs, namely Llama 2, GPT-3.5, and GPT-4, against the\ncultural dimensions of regions like the United States, China, and Arab\ncountries, using different prompting styles and exploring the effects of\nlanguage-specific fine-tuning on the models' behavioural tendencies and\ncultural values. Our results quantify the cultural alignment of LLMs and reveal\nthe difference between LLMs in explanatory cultural dimensions. Our study\ndemonstrates that while all LLMs struggle to grasp cultural values, GPT-4 shows\na unique capability to adapt to cultural nuances, particularly in Chinese\nsettings. However, it faces challenges with American and Arab cultures. The\nresearch also highlights that fine-tuning LLama 2 models with different\nlanguages changes their responses to cultural questions, emphasizing the need\nfor culturally diverse development in AI for worldwide acceptance and ethical\nuse. For more details or to contribute to this research, visit our GitHub page\nhttps://github.com/reemim/Hofstedes_CAT/\n"", ""  As the scaling of Large Language Models (LLMs) has dramatically enhanced\ntheir capabilities, there has been a growing focus on the alignment problem to\nensure their responsible and ethical use. While existing alignment efforts\npredominantly concentrate on universal values such as the HHH principle, the\naspect of culture, which is inherently pluralistic and diverse, has not\nreceived adequate attention. This work introduces a new benchmark, CDEval,\naimed at evaluating the cultural dimensions of LLMs. CDEval is constructed by\nincorporating both GPT-4's automated generation and human verification,\ncovering six cultural dimensions across seven domains. Our comprehensive\nexperiments provide intriguing insights into the culture of mainstream LLMs,\nhighlighting both consistencies and variations across different dimensions and\ndomains. The findings underscore the importance of integrating cultural\nconsiderations in LLM development, particularly for applications in diverse\ncultural settings. Through CDEval, we aim to broaden the horizon of LLM\nalignment research by including cultural dimensions, thus providing a more\nholistic framework for the future development and evaluation of LLMs. This\nbenchmark serves as a valuable resource for cultural studies in LLMs, paving\nthe way for more culturally aware and sensitive models.\n""]",Cultural Sensitivity in Large Language Models
518,517,10,517_scheduling_prediction_predictions_queueing,"['scheduling', 'prediction', 'predictions', 'queueing', 'queues', 'algorithms', 'predicted', 'queue', 'algorithmic', 'priority']","['predictions', 'jobs', 'skip', 'queues', 'list', 'priority', 'scheduling', 'prediction', 'job', 'guarantees']","['scheduling', 'predictions', 'queueing', 'algorithmic', 'optimally', 'caching', 'stochastically', 'lists', 'priorities', 'skippredict']","[""  Online decision-makers often obtain predictions on future variables, such as\narrivals, demands, inventories, and so on. These predictions can be generated\nfrom simple forecasting algorithms for univariate time-series, all the way to\nstate-of-the-art machine learning models that leverage multiple time-series and\nadditional feature information. However, the prediction accuracy is unknown to\ndecision-makers a priori, hence blindly following the predictions can be\nharmful. In this paper, we address this problem by developing algorithms that\nutilize predictions in a manner that is robust to the unknown prediction\naccuracy.\n  We consider the Online Resource Allocation Problem, a generic model for\nonline decision-making, in which a limited amount of resources may be used to\nsatisfy a sequence of arriving requests. Prior work has characterized the best\nachievable performances when the arrivals are either generated stochastically\n(i.i.d.) or completely adversarially, and shown that algorithms exist which\nmatch these bounds under both arrival models, without ``knowing'' the\nunderlying model. To this backdrop, we introduce predictions in the form of\nshadow prices on each type of resource. Prediction accuracy is naturally\ndefined to be the distance between the predictions and the actual shadow\nprices.\n  We tightly characterize, via a formal lower bound, the extent to which any\nalgorithm can optimally leverage predictions (that is, to ``follow'' the\npredictions when accurate, and ``ignore'' them when inaccurate) without knowing\nthe prediction accuracy or the underlying arrival model. Our main contribution\nis then an algorithm which achieves this lower bound. Finally, we empirically\nvalidate our algorithm with a large-scale experiment on real data from the\nretailer H&M.\n"", ""  ML-augmented algorithms utilize predictions to achieve performance beyond\ntheir worst-case bounds. Producing these predictions might be a costly\noperation -- this motivated Im et al. '22 to introduce the study of algorithms\nwhich use predictions parsimoniously. We design parsimonious algorithms for\ncaching and MTS with action predictions, proposed by Antoniadis et al. '20,\nfocusing on the parameters of consistency (performance with perfect\npredictions) and smoothness (dependence of their performance on the prediction\nerror). Our algorithm for caching is 1-consistent, robust, and its smoothness\ndeteriorates with the decreasing number of available predictions. We propose an\nalgorithm for general MTS whose consistency and smoothness both scale linearly\nwith the decreasing number of predictions. Without the restriction on the\nnumber of available predictions, both algorithms match the earlier guarantees\nachieved by Antoniadis et al. '20.\n"", '  In light of recent work on scheduling with predicted job sizes, we consider\nthe effect of the cost of predictions in queueing systems, removing the\nassumption in prior research that predictions are external to the system\'s\nresources and/or cost-free. In particular, we introduce a novel approach to\nutilizing predictions, SkipPredict, designed to address their inherent cost.\nRather than uniformly applying predictions to all jobs, we propose a tailored\napproach that categorizes jobs based on their prediction requirements. To\nachieve this, we employ one-bit ""cheap predictions"" to classify jobs as either\nshort or long. SkipPredict prioritizes predicted short jobs over long jobs, and\nfor the latter, SkipPredict applies a second round of more detailed ""expensive\npredictions"" to approximate Shortest Remaining Processing Time for these jobs.\nOur analysis takes into account the cost of prediction. We examine the effect\nof this cost for two distinct models. In the external cost model, predictions\nare generated by some external method without impacting job service times but\nincur a cost. In the server time cost model, predictions themselves require\nserver processing time, and are scheduled on the same server as the jobs.\n']","""Scheduling and Queueing with Predictions"""
519,518,10,518_programming_solvers_solver_optimization,"['programming', 'solvers', 'solver', 'optimization', 'program', 'iteratively', 'code', 'optllm', 'language', 'tutorials']","['problems', 'programming', 'program', 'mathematical', 'optimization', 'solving', 'examples', 'solvers', 'synthesis', 'hindsight']","['programming', 'solvers', 'optimization', 'optllm', 'codeit', 'formulations', 'refinement', 'nlp4lp', 'procedurally', 'deepseek']","['  Optimization problems are pervasive in sectors from manufacturing and\ndistribution to healthcare. However, most such problems are still solved\nheuristically by hand rather than optimally by state-of-the-art solvers because\nthe expertise required to formulate and solve these problems limits the\nwidespread adoption of optimization tools and techniques. This paper introduces\nOptiMUS, a Large Language Model (LLM)-based agent designed to formulate and\nsolve (mixed integer) linear programming problems from their natural language\ndescriptions. OptiMUS can develop mathematical models, write and debug solver\ncode, evaluate the generated solutions, and improve its model and code based on\nthese evaluations. OptiMUS utilizes a modular structure to process problems,\nallowing it to handle problems with long descriptions and complex data without\nlong prompts. Experiments demonstrate that OptiMUS outperforms existing\nstate-of-the-art methods on easy datasets by more than $20\\%$ and on hard\ndatasets (including a new dataset, NLP4LP, released with this paper that\nfeatures long and complex problems) by more than $30\\%$.\n', ""  Large language models (LLMs) have exhibited their problem-solving ability in\nmathematical reasoning. Solving realistic optimization (OPT) problems in\nindustrial application scenarios requires advanced and applied math ability.\nHowever, current OPT benchmarks that merely solve linear programming are far\nfrom complex realistic situations. In this work, we propose E-OPT, a benchmark\nfor end-to-end optimization problem-solving with human-readable inputs and\noutputs. E-OPT contains rich optimization problems, including linear/nonlinear\nprogramming with/without table data, which can comprehensively evaluate LLMs'\nsolving ability. In our benchmark, LLMs are required to correctly understand\nthe problem in E-OPT and call code solver to get precise numerical answers.\nFurthermore, to alleviate the data scarcity for optimization problems, and to\nbridge the gap between open-source LLMs on a small scale (e.g., Llama-2-7b and\nLlama-3-8b) and closed-source LLMs (e.g., GPT-4), we further propose a novel\ndata synthesis method namely ReSocratic. Unlike general data synthesis methods\nthat proceed from questions to answers, ReSocratic first incrementally\nsynthesizes optimization scenarios with mathematical formulations step by step\nand then back-translates the generated scenarios into questions. In such a way,\nwe construct the ReSocratic-29k dataset from a small seed sample pool with the\npowerful open-source large model DeepSeek-V2. To demonstrate the effectiveness\nof ReSocratic, we conduct supervised fine-tuning with ReSocratic-29k on\nmultiple open-source models. The results show that Llama3-8b is significantly\nimproved from 13.6% to 51.7% on E-OPT, while DeepSeek-V2 reaches 61.0%,\napproaching 65.5% of GPT-4.\n"", '  Optimization problems are pervasive in sectors from manufacturing and\ndistribution to healthcare. However, most such problems are still solved\nheuristically by hand rather than optimally by state-of-the art solvers because\nthe expertise required to formulate and solve these problems limits the\nwidespread adoption of optimization tools and techniques. We introduce a Large\nLanguage Model (LLM)-based system designed to formulate and solve (mixed\ninteger) linear programming problems from their natural language descriptions.\nOur system is capable of developing mathematical models, writing and debugging\nsolver code, evaluating the generated solutions, and improving efficiency and\ncorrectness of its model and code based on these evaluations. OptiMUS-0.3\nutilizes a modular structure to process problems, allowing it to handle\nproblems with long descriptions and complex data without long prompts.\nExperiments demonstrate that OptiMUS-0.3 outperforms existing state-of-the-art\nmethods on easy datasets by more than 12% and on hard datasets (including a new\ndataset, NLP4LP, released with this paper that features long and complex\nproblems) by more than 8%.\n']",Optimization Problem Solving with Large Language Models
520,519,10,519_modeling_industrial_deep_flow,"['modeling', 'industrial', 'deep', 'flow', 'stochastic', 'manufacturing', 'sensors', 'industries', 'sensor', 'sensing']","['soft', 'wells', 'sensor', 'sensing', 'sensors', 'unit', 'industrial', 'wastewater', 'flow', 'control']","['modeling', 'industrial', 'flow', 'stochastic', 'sensor', 'backpropagate', 'knowledge', 'oil', 'toolchains', 'sdk']","['  The modeling of multistage manufacturing systems (MMSs) has attracted\nincreased attention from both academia and industry. Recent advancements in\ndeep learning methods provide an opportunity to accomplish this task with\nreduced cost and expertise. This study introduces a stochastic deep Koopman\n(SDK) framework to model the complex behavior of MMSs. Specifically, we present\na novel application of Koopman operators to propagate critical quality\ninformation extracted by variational autoencoders. Through this framework, we\ncan effectively capture the general nonlinear evolution of product quality\nusing a transferred linear representation, thus enhancing the interpretability\nof the data-driven model. To evaluate the performance of the SDK framework, we\ncarried out a comparative study on an open-source dataset. The main findings of\nthis paper are as follows. Our results indicate that SDK surpasses other\npopular data-driven models in accuracy when predicting stagewise product\nquality within the MMS. Furthermore, the unique linear propagation property in\nthe stochastic latent space of SDK enables traceability for quality evolution\nthroughout the process, thereby facilitating the design of root cause analysis\nschemes. Notably, the proposed framework requires minimal knowledge of the\nunderlying physics of production lines. It serves as a virtual metrology tool\nthat can be applied to various MMSs, contributing to the ultimate goal of Zero\nDefect Manufacturing.\n', '  In many industrial processes, an apparent lack of data limits the development\nof data-driven soft sensors. There are, however, often opportunities to learn\nstronger models by being more data-efficient. To achieve this, one can leverage\nknowledge about the data from which the soft sensor is learned. Taking\nadvantage of properties frequently possessed by industrial data, we introduce a\ndeep latent variable model for semi-supervised multi-unit soft sensing. This\nhierarchical, generative model is able to jointly model different units, as\nwell as learning from both labeled and unlabeled data.\n  An empirical study of multi-unit soft sensing is conducted using two\ndatasets: a synthetic dataset of single-phase fluid flow, and a large, real\ndataset of multi-phase flow in oil and gas wells. We show that by combining\nsemi-supervised and multi-task learning, the proposed model achieves superior\nresults, outperforming current leading methods for this soft sensing problem.\nWe also show that when a model has been trained on a multi-unit dataset, it may\nbe finetuned to previously unseen units using only a handful of data points. In\nthis finetuning procedure, unlabeled data improve soft sensor performance;\nremarkably, this is true even when no labeled data are available.\n', '  Recent literature has explored various ways to improve soft sensors by\nutilizing learning algorithms with transferability. A performance gain is\ngenerally attained when knowledge is transferred among strongly related soft\nsensor learning tasks. A particularly relevant case for transferability is when\ndeveloping soft sensors of the same type for similar, but physically different\nprocesses or units. Then, the data from each unit presents a soft sensor\nlearning task, and it is reasonable to expect strongly related tasks. Applying\nmethods that exploit transferability in this setting leads to what we call\nmulti-unit soft sensing.\n  This paper formulates multi-unit soft sensing as a probabilistic,\nhierarchical model, which we implement using a deep neural network. The\nlearning capabilities of the model are studied empirically on a large-scale\nindustrial case by developing virtual flow meters (a type of soft sensor) for\n80 petroleum wells. We investigate how the model generalizes with the number of\nwells/units. Interestingly, we demonstrate that multi-unit models learned from\ndata from many wells, permit few-shot learning of virtual flow meters for new\nwells. Surprisingly, regarding the difficulty of the tasks, few-shot learning\non 1-3 data points often leads to high performance on new wells.\n']",Industrial Process Modeling and Sensing with Deep Learning
521,520,10,520_fingerprint_fingerprints_shoeprints_palmprints,"['fingerprint', 'fingerprints', 'shoeprints', 'palmprints', 'shoeprint', 'biometrics', 'palmprint', 'biometric', 'genprint', 'inception']","['fingerprint', 'shoeprint', 'morphing', 'prints', 'signature', 'biometric', 'palmprint', 'spoof', 'fingerprints', 'identities']","['fingerprints', 'shoeprints', 'biometric', 'gans', 'spoofing', 'counterfeit', 'signatures', 'uniqueness', 'difffinger', 'identity']","['  In fingerprint matching, fixed-length descriptors generally offer greater\nefficiency compared to minutiae set, but the recognition accuracy is not as\ngood as that of the latter. Although much progress has been made in deep\nlearning based fixed-length descriptors recently, they often fall short when\ndealing with incomplete or partial fingerprints, diverse fingerprint poses, and\nsignificant background noise. In this paper, we propose a three-dimensional\nrepresentation called Fixed-length Dense Descriptor (FDD) for efficient\nfingerprint matching. FDD features great spatial properties, enabling it to\ncapture the spatial relationships of the original fingerprints, thereby\nenhancing interpretability and robustness. Our experiments on various\nfingerprint datasets reveal that FDD outperforms other fixed-length\ndescriptors, especially in matching fingerprints of different areas,\ncross-modal fingerprint matching, and fingerprint matching with background\nnoise.\n', ""  We present novel approaches involving generative adversarial networks and\ndiffusion models in order to synthesize high quality, live and spoof\nfingerprint images while preserving features such as uniqueness and diversity.\nWe generate live fingerprints from noise with a variety of methods, and we use\nimage translation techniques to translate live fingerprint images to spoof. To\ngenerate different types of spoof images based on limited training data we\nincorporate style transfer techniques through a cycle autoencoder equipped with\na Wasserstein metric along with Gradient Penalty (CycleWGAN-GP) in order to\navoid mode collapse and instability. We find that when the spoof training data\nincludes distinct spoof characteristics, it leads to improved live-to-spoof\ntranslation. We assess the diversity and realism of the generated live\nfingerprint images mainly through the Fr\\'echet Inception Distance (FID) and\nthe False Acceptance Rate (FAR). Our best diffusion model achieved a FID of\n15.78. The comparable WGAN-GP model achieved slightly higher FID while\nperforming better in the uniqueness assessment due to a slightly lower FAR when\nmatched against the training data, indicating better creativity. Moreover, we\ngive example images showing that a DDPM model clearly can generate realistic\nfingerprint images.\n"", '  The utilization of synthetic data for fingerprint recognition has garnered\nincreased attention due to its potential to alleviate privacy concerns\nsurrounding sensitive biometric data. However, current methods for generating\nfingerprints have limitations in creating impressions of the same finger with\nuseful intra-class variations. To tackle this challenge, we present GenPrint, a\nframework to produce fingerprint images of various types while maintaining\nidentity and offering humanly understandable control over different appearance\nfactors such as fingerprint class, acquisition type, sensor device, and quality\nlevel. Unlike previous fingerprint generation approaches, GenPrint is not\nconfined to replicating style characteristics from the training dataset alone:\nit enables the generation of novel styles from unseen devices without requiring\nadditional fine-tuning. To accomplish these objectives, we developed GenPrint\nusing latent diffusion models with multimodal conditions (text and image) for\nconsistent generation of style and identity. Our experiments leverage a variety\nof publicly available datasets for training and evaluation. Results demonstrate\nthe benefits of GenPrint in terms of identity preservation, explainable\ncontrol, and universality of generated images. Importantly, the\nGenPrint-generated images yield comparable or even superior accuracy to models\ntrained solely on real data and further enhances performance when augmenting\nthe diversity of existing real fingerprint datasets.\n']",Biometric Identification and Fingerprint Recognition
