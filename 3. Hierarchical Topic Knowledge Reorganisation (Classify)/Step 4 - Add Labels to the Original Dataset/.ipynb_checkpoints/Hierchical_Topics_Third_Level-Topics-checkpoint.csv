Cluster_Label,Second_Level_Topic_Label,Representation,Representative_Docs,Highest_Topic_Label,Representative_Document
1,Advances in Large Language Models ; Personalized Recommendation Systems with Advanced Modeling Techniques,"['retrieval', 'search', 'relevance', 'recall', 'retrieved', 'queries', 'semantic', 'ranking', 'corpus', 'indexing'] , ['reasoning', 'thinking', 'prompting', 'deductive', 'inference', 'prompts', 'tasks', 'steps', 'task', 'logic'] , ['multimodal', 'captioning', 'visual', 'mllm', 'modality', 'text', 'mllms', 'textual', 'language', 'comprehension'] , ['reward', 'rewards', 'reinforcement', 'supervised', 'learning', 'trained', 'language', 'preference', 'sampling', 'regularization'] ; ['recommender', 'recommenders', 'personalized', 'recommendation', 'recommendations', 'factorization', 'collaborative', 'embeddings', 'attention', 'embedding']","['  Large Language Models (LLMs) excel in various language tasks but they often\ngenerate incorrect information, a phenomenon known as ""hallucinations"".\nRetrieval-Augmented Generation (RAG) aims to mitigate this by using document\nretrieval for accurate responses. However, RAG still faces hallucinations due\nto vague queries. This study aims to improve RAG by optimizing query generation\nwith a query-document alignment score, refining queries using LLMs for better\nprecision and efficiency of document retrieval. Experiments have shown that our\napproach improves document retrieval, resulting in an average accuracy gain of\n1.6%.\n', ""  Retrieval-Augmented Generation (RAG) has recently emerged as a method to\nextend beyond the pre-trained knowledge of Large Language Models by augmenting\nthe original prompt with relevant passages or documents retrieved by an\nInformation Retrieval (IR) system. RAG has become increasingly important for\nGenerative AI solutions, especially in enterprise settings or in any domain in\nwhich knowledge is constantly refreshed and cannot be memorized in the LLM. We\nargue here that the retrieval component of RAG systems, be it dense or sparse,\ndeserves increased attention from the research community, and accordingly, we\nconduct the first comprehensive and systematic examination of the retrieval\nstrategy of RAG systems. We focus, in particular, on the type of passages IR\nsystems within a RAG solution should retrieve. Our analysis considers multiple\nfactors, such as the relevance of the passages included in the prompt context,\ntheir position, and their number. One counter-intuitive finding of this work is\nthat the retriever's highest-scoring documents that are not directly relevant\nto the query (e.g., do not contain the answer) negatively impact the\neffectiveness of the LLM. Even more surprising, we discovered that adding\nrandom documents in the prompt improves the LLM accuracy by up to 35%. These\nresults highlight the need to investigate the appropriate strategies when\nintegrating retrieval with LLMs, thereby laying the groundwork for future\nresearch in this area.\n"", '  Retrieval-Augmented Generation (RAG) has recently demonstrated the\nperformance of Large Language Models (LLMs) in the knowledge-intensive tasks\nsuch as Question-Answering (QA). RAG expands the query context by incorporating\nexternal knowledge bases to enhance the response accuracy. However, it would be\ninefficient to access LLMs multiple times for each query and unreliable to\nretrieve all the relevant documents by a single query. We have found that even\nthough there is low relevance between some critical documents and query, it is\npossible to retrieve the remaining documents by combining parts of the\ndocuments with the query. To mine the relevance, a two-stage retrieval\nframework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is\nproposed to improve document retrieval recall and the accuracy of answers while\nmaintaining efficiency. Additionally, a compact classifier is applied to two\ndifferent selection strategies to determine the contribution of the retrieved\ndocuments to answering the query and retrieve the relatively relevant\ndocuments. Meanwhile, DR-RAG call the LLMs only once, which significantly\nimproves the efficiency of the experiment. The experimental results on\nmulti-hop QA datasets show that DR-RAG can significantly improve the accuracy\nof the answers and achieve new progress in QA systems.\n'] , ['  Chain-of-thought (CoT) prompting can guide language models to engage in\ncomplex multi-step reasoning. The quality of provided demonstrations\nsignificantly impacts the success of downstream inference tasks. While existing\nautomated methods prioritize accuracy and semantics in these demonstrations, we\nshow that the underlying reasoning patterns play a more crucial role in such\ntasks. In this paper, we propose Pattern-Aware CoT, a prompting method that\nconsiders the diversity of demonstration patterns. By incorporating patterns\nsuch as step length and reasoning process within intermediate steps, PA-CoT\neffectively mitigates the issue of bias induced by demonstrations and enables\nbetter generalization to diverse scenarios. We conduct experiments on nine\nreasoning benchmark tasks using two open-source LLMs. The results show that our\nmethod substantially enhances reasoning performance and exhibits robustness to\nerrors. The code will be made publicly available.\n', ""  Chain of Thought (CoT) is significant in improving the reasoning abilities of\nlarge language models (LLMs). However, the correlation between the\neffectiveness of CoT and the length of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we have conducted several empirical\nexperiments to explore the relations. Specifically, we design experiments that\nexpand and compress the rationale reasoning steps within CoT demonstrations\nwhile keeping all other factors constant. We have the following key findings.\nFirst, the results indicate that lengthening the reasoning steps in prompts,\neven without adding new information into the prompt, considerably enhances\nLLMs' reasoning abilities across multiple datasets. Alternatively, shortening\nthe reasoning steps, even while preserving the key information, significantly\ndiminishes the reasoning abilities of models. This finding highlights the\nimportance of the number of steps in CoT prompts and provides practical\nguidance to make better use of LLMs' potential in complex problem-solving\nscenarios. Second, we also investigated the relationship between the\nperformance of CoT and the rationales used in demonstrations. Surprisingly, the\nresult shows that even incorrect rationales can yield favorable outcomes if\nthey maintain the requisite length of inference. Third, we observed that the\nadvantages of increasing reasoning steps are task-dependent: simpler tasks\nrequire fewer steps, whereas complex tasks gain significantly from longer\ninference sequences. The code is available at\nhttps://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models\n"", ""  Large Language Models (LLMs) have showcased impressive reasoning\ncapabilities, particularly when guided by specifically designed prompts in\ncomplex reasoning tasks such as math word problems. These models typically\nsolve tasks using a chain-of-thought approach, which not only bolsters their\nreasoning abilities but also provides valuable insights into their\nproblem-solving process. However, there is still significant room for enhancing\nthe reasoning abilities of LLMs. Some studies suggest that the integration of\nan LLM output verifier can boost reasoning accuracy without necessitating\nadditional model training. In this paper, we follow these studies and introduce\na novel graph-based method to further augment the reasoning capabilities of\nLLMs. We posit that multiple solutions to a reasoning task, generated by an\nLLM, can be represented as a reasoning graph due to the logical connections\nbetween intermediate steps from different reasoning paths. Therefore, we\npropose the Reasoning Graph Verifier (GraphReason) to analyze and verify the\nsolutions generated by LLMs. By evaluating these graphs, models can yield more\naccurate and reliable results.Our experimental results show that our\ngraph-based verification method not only significantly enhances the reasoning\nabilities of LLMs but also outperforms existing verifier methods in terms of\nimproving these models' reasoning performance.\n""] , [""  Multimodal Large Language Models (MLLMs) have recently achieved promising\nzero-shot accuracy on visual question answering (VQA) -- a fundamental task\naffecting various downstream applications and domains. Given the great\npotential for the broad use of these models, it is important to investigate\ntheir limitations in dealing with different image and question properties. In\nthis work, we investigate whether MLLMs can perceive small details as well as\nlarge details in images. In particular, we show that their zero-shot accuracy\nin answering visual questions is very sensitive to the size of the visual\nsubject of the question, declining up to 46% with size. Furthermore, we show\nthat this effect is causal by observing that human visual cropping can\nsignificantly mitigate their sensitivity to size. Inspired by the usefulness of\nhuman cropping, we then propose five automatic visual cropping methods --\nleveraging either external localization models or the decision process of the\ngiven MLLM itself -- as inference time mechanisms to improve the zero-shot\nperformance of MLLMs. We study their effectiveness on four popular VQA\ndatasets, and a subset of the VQAv2 dataset tailored towards fine visual\ndetails. Our findings suggest that MLLMs should be used with caution in\ndetail-sensitive VQA applications, and that visual cropping is a promising\ndirection to improve their zero-shot performance. To facilitate further\ninvestigation of MLLMs' behaviors, our code and data are publicly released.\n"", '  Recent advancements in Chain-of-Thought (CoT) and related rationale-based\nworks have significantly improved the performance of Large Language Models\n(LLMs) in complex reasoning tasks. With the evolution of Multimodal Large\nLanguage Models (MLLMs), enhancing their capability to tackle complex\nmultimodal reasoning problems is a crucial frontier. However, incorporating\nmultimodal rationales in CoT has yet to be thoroughly investigated. We propose\nthe Image-of-Thought (IoT) prompting method, which helps MLLMs to extract\nvisual rationales step-by-step. Specifically, IoT prompting can automatically\ndesign critical visual information extraction operations based on the input\nimages and questions. Each step of visual information refinement identifies\nspecific visual rationales that support answers to complex visual reasoning\nquestions. Beyond the textual CoT, IoT simultaneously utilizes visual and\ntextual rationales to help MLLMs understand complex multimodal information. IoT\nprompting has improved zero-shot visual reasoning performance across various\nvisual understanding tasks in different MLLMs. Moreover, the step-by-step\nvisual feature explanations generated by IoT prompting elucidate the visual\nreasoning process, aiding in analyzing the cognitive processes of large\nmultimodal models\n', '  Connecting text and visual modalities plays an essential role in generative\nintelligence. For this reason, inspired by the success of large language\nmodels, significant research efforts are being devoted to the development of\nMultimodal Large Language Models (MLLMs). These models can seamlessly integrate\nvisual and textual modalities, while providing a dialogue-based interface and\ninstruction-following capabilities. In this paper, we provide a comprehensive\nreview of recent visual-based MLLMs, analyzing their architectural choices,\nmultimodal alignment strategies, and training techniques. We also conduct a\ndetailed analysis of these models across a wide range of tasks, including\nvisual grounding, image generation and editing, visual understanding, and\ndomain-specific applications. Additionally, we compile and describe training\ndatasets and evaluation benchmarks, conducting comparisons among existing\nmodels in terms of performance and computational requirements. Overall, this\nsurvey offers a comprehensive overview of the current state of the art, laying\nthe groundwork for future MLLMs.\n'] , ['  Language model (LM) post-training (or alignment) involves maximizing a reward\nfunction that is derived from preference annotations. Direct Preference\nOptimization (DPO) is a popular offline alignment method that trains a policy\ndirectly on preference data without the need to train a reward model or apply\nreinforcement learning. However, typical preference datasets have only a\nsingle, or at most a few, annotation per preference pair, which causes DPO to\noverconfidently assign rewards that trend towards infinite magnitude. This\nfrequently leads to degenerate policies, sometimes causing even the\nprobabilities of the preferred generations to go to zero. In this work, we\nanalyze this phenomenon and propose distillation to get a better proxy for the\ntrue preference distribution over generation pairs: we train the LM to produce\nprobabilities that match the distribution induced by a reward model trained on\nthe preference data. Moreover, to account for uncertainty in the reward model\nwe are distilling from, we optimize against a family of reward models that, as\na whole, is likely to include at least one reasonable proxy for the preference\ndistribution. Our results show that distilling from such a family of reward\nmodels leads to improved robustness to distribution shift in preference\nannotations, while preserving the simple supervised nature of DPO.\n', '  RLHF has emerged as a pivotal step in aligning language models with human\nobjectives and values. It typically involves learning a reward model from human\npreference data and then using reinforcement learning to update the generative\nmodel accordingly. Conversely, Direct Preference Optimization (DPO) directly\noptimizes the generative model with preference data, skipping reinforcement\nlearning. However, both RLHF and DPO assume uniform preferences, overlooking\nthe reality of diverse human annotators. This paper presents a new method to\nalign generative models with varied human preferences. We propose an\nExpectation-Maximization adaptation to DPO, generating a mixture of models\nbased on latent preference types of the annotators. We then introduce a min-max\nregret ensemble learning model to produce a single generative method to\nminimize worst-case regret among annotator subgroups with similar latent\nfactors. Our algorithms leverage the simplicity of DPO while accommodating\ndiverse preferences. Experimental results validate the effectiveness of our\napproach in producing equitable generative policies.\n', '  The success of reinforcement learning from human feedback (RLHF) in language\nmodel alignment is strongly dependent on the quality of the underlying reward\nmodel. In this paper, we present a novel approach to improve reward model\nquality by generating synthetic preference data, thereby augmenting the\ntraining dataset with on-policy, high-quality preference pairs. Motivated by\nthe promising results of Best-of-N sampling strategies in language model\ntraining, we extend their application to reward model training. This results in\na self-training strategy to generate preference pairs by selecting the best and\nworst candidates in a pool of responses to a given query. Empirically, we find\nthat this approach improves the performance of any reward model, with an effect\ncomparable to the addition of a similar quantity of human preference data. This\nwork opens up new avenues of research for improving RLHF for language model\nalignment, by offering synthetic preference generation as a solution to reward\nmodeling challenges.\n'] ; ['  Contemporary recommender systems predominantly rely on collaborative\nfiltering techniques, employing ID-embedding to capture latent associations\namong users and items. However, this approach overlooks the wealth of semantic\ninformation embedded within textual descriptions of items, leading to\nsuboptimal performance in cold-start scenarios and long-tail user\nrecommendations. Leveraging the capabilities of Large Language Models (LLMs)\npretrained on massive text corpus presents a promising avenue for enhancing\nrecommender systems by integrating open-world domain knowledge. In this paper,\nwe propose an Llm-driven knowlEdge Adaptive RecommeNdation (LEARN) framework\nthat synergizes open-world knowledge with collaborative knowledge. We address\ncomputational complexity concerns by utilizing pretrained LLMs as item encoders\nand freezing LLM parameters to avoid catastrophic forgetting and preserve\nopen-world knowledge. To bridge the gap between the open-world and\ncollaborative domains, we design a twin-tower structure supervised by the\nrecommendation task and tailored for practical industrial application. Through\noffline experiments on the large-scale industrial dataset and online\nexperiments on A/B tests, we demonstrate the efficacy of our approach.\n', '  Many existing industrial recommender systems are sensitive to the patterns of\nuser-item engagement. Light users, who interact less frequently, correspond to\na data sparsity problem, making it difficult for the system to accurately learn\nand represent their preferences. On the other hand, heavy users with rich\ninteraction history often demonstrate a variety of niche interests that are\nhard to be precisely captured under the standard ""user-item"" similarity\nmeasurement. Moreover, implementing these systems in an industrial environment\nnecessitates that they are resource-efficient and scalable to process web-scale\ndata under strict latency constraints. In this paper, we address these\nchallenges by introducing an intermediate ""interest"" layer between users and\nitems. We propose a novel approach that efficiently constructs user interest\nand facilitates low computational cost inference by clustering engagement\ngraphs and incorporating user-interest attention. This method enhances the\nunderstanding of light users\' preferences by linking them with heavy users. By\nintegrating user-interest attention, our approach allows a more personalized\nsimilarity metric, adept at capturing the complex dynamics of user-item\ninteractions. The use of interest as an intermediary layer fosters a balance\nbetween scalability and expressiveness in the model. Evaluations on two public\ndatasets reveal that our method not only achieves improved recommendation\nperformance but also demonstrates enhanced computational efficiency compared to\nitem-level attention models. Our approach has also been deployed in multiple\nproducts at Meta, facilitating short-form video related recommendation.\n', '  The explainability of recommendation systems is crucial for enhancing user\ntrust and satisfaction. Leveraging large language models (LLMs) offers new\nopportunities for comprehensive recommendation logic generation. However, in\nexisting related studies, fine-tuning LLM models for recommendation tasks\nincurs high computational costs and alignment issues with existing systems,\nlimiting the application potential of proven proprietary/closed-source LLM\nmodels, such as GPT-4. In this work, our proposed effective strategy LANE\naligns LLMs with online recommendation systems without additional LLMs tuning,\nreducing costs and improving explainability. This innovative approach addresses\nkey challenges in integrating language models with recommendation systems while\nfully utilizing the capabilities of powerful proprietary models. Specifically,\nour strategy operates through several key components: semantic embedding, user\nmulti-preference extraction using zero-shot prompting, semantic alignment, and\nexplainable recommendation generation using Chain of Thought (CoT) prompting.\nBy embedding item titles instead of IDs and utilizing multi-head attention\nmechanisms, our approach aligns the semantic features of user preferences with\nthose of candidate items, ensuring coherent and user-aligned recommendations.\nSufficient experimental results including performance comparison, questionnaire\nvoting, and visualization cases prove that our method can not only ensure\nrecommendation performance, but also provide easy-to-understand and reasonable\nrecommendation logic.\n']",Advances in Artificial Intelligence and Machine Learning,Personalized Recommendation Systems with Advanced Modeling Techniques
2,Molecular Modeling for Drug Discovery,"['molecular', 'molecule', 'molecules', 'ligands', 'proteins', 'ligand', 'protein', 'models', 'modeling', 'discovery']","['  Generating molecules that bind to specific proteins is an important but\nchallenging task in drug discovery. Previous works usually generate atoms in an\nauto-regressive way, where element types and 3D coordinates of atoms are\ngenerated one by one. However, in real-world molecular systems, the\ninteractions among atoms in an entire molecule are global, leading to the\nenergy function pair-coupled among atoms. With such energy-based consideration,\nthe modeling of probability should be based on joint distributions, rather than\nsequentially conditional ones. Thus, the unnatural sequentially auto-regressive\nmodeling of molecule generation is likely to violate the physical rules, thus\nresulting in poor properties of the generated molecules. In this work, a\ngenerative diffusion model for molecular 3D structures based on target proteins\nas contextual constraints is established, at a full-atom level in a\nnon-autoregressive way. Given a designated 3D protein binding site, our model\nlearns the generative process that denoises both element types and 3D\ncoordinates of an entire molecule, with an equivariant network. Experimentally,\nthe proposed method shows competitive performance compared with prevailing\nworks in terms of high affinity with proteins and appropriate molecule sizes as\nwell as other drug properties such as drug-likeness of the generated molecules.\n', ""  Diffusion models have emerged as powerful tools for molecular generation,\nparticularly in the context of 3D molecular structures. Inspired by\nnon-equilibrium statistical physics, these models can generate 3D molecular\nstructures with specific properties or requirements crucial to drug discovery.\nDiffusion models were particularly successful at learning 3D molecular\ngeometries' complex probability distributions and their corresponding chemical\nand physical properties through forward and reverse diffusion processes. This\nreview focuses on the technical implementation of diffusion models tailored for\n3D molecular generation. It compares the performance, evaluation methods, and\nimplementation details of various diffusion models used for molecular\ngeneration tasks. We cover strategies for atom and bond representation,\narchitectures of reverse diffusion denoising networks, and challenges\nassociated with generating stable 3D molecular structures. This review also\nexplores the applications of diffusion models in $\\textit{de novo}$ drug design\nand related areas of computational chemistry, such as structure-based drug\ndesign, including target-specific molecular generation, molecular docking, and\nmolecular dynamics of protein-ligand complexes. We also cover conditional\ngeneration on physical properties, conformation generation, and fragment-based\ndrug design. By summarizing the state-of-the-art diffusion models for 3D\nmolecular generation, this review sheds light on their role in advancing drug\ndiscovery as well as their current limitations.\n"", ""  Molecular representation learning is pivotal for various molecular property\nprediction tasks related to drug discovery. Robust and accurate benchmarks are\nessential for refining and validating current methods. Existing molecular\nproperty benchmarks derived from wet experiments, however, face limitations\nsuch as data volume constraints, unbalanced label distribution, and noisy\nlabels. To address these issues, we construct a large-scale and precise\nmolecular representation dataset of approximately 140,000 small molecules,\nmeticulously designed to capture an extensive array of chemical, physical, and\nbiological properties, derived through a robust computational ligand-target\nbinding analysis pipeline. We conduct extensive experiments on various deep\nlearning models, demonstrating that our dataset offers significant\nphysicochemical interpretability to guide model development and design.\nNotably, the dataset's properties are linked to binding affinity metrics,\nproviding additional insights into model performance in drug-target interaction\ntasks. We believe this dataset will serve as a more accurate and reliable\nbenchmark for molecular representation learning, thereby expediting progress in\nthe field of artificial intelligence-driven drug discovery.\n""]",Artificial Intelligence in Molecular Science,Molecular Modeling for Drug Discovery
3,Natural Language Processing in Clinical Text Analysis,"['nlp', 'text', 'annotated', 'clinical', 'medical', 'patients', 'medicine', 'hospital', 'evaluation', 'clinicians']","['  In studies that rely on data from electronic health records (EHRs),\nunstructured text data such as clinical progress notes offer a rich source of\ninformation about patient characteristics and care that may be missing from\nstructured data. Despite the prevalence of text in clinical research, these\ndata are often ignored for the purposes of quantitative analysis due their\ncomplexity. This paper presents a unified framework for leveraging text data to\nsupport causal inference with electronic health data at multiple stages of\nanalysis. In particular, we consider how natural language processing and\nstatistical text analysis can be combined with standard inferential techniques\nto address common challenges due to missing data, confounding bias, and\ntreatment effect heterogeneity. Through an application to a recent EHR study\ninvestigating the effects of a non-randomized medical intervention on patient\noutcomes, we show how incorporating text data in a traditional matching\nanalysis can help strengthen the validity of an estimated treatment effect and\nidentify patient subgroups that may benefit most from treatment. We believe\nthese methods have the potential to expand the scope of secondary analysis of\nclinical data to domains where structured EHR data is limited, such as in\ndeveloping countries. To this end, we provide code and open-source replication\nmaterials to encourage adoption and broader exploration of these techniques in\nclinical research.\n', '  Analyzing vast textual data and summarizing key information from electronic\nhealth records imposes a substantial burden on how clinicians allocate their\ntime. Although large language models (LLMs) have shown promise in natural\nlanguage processing (NLP), their effectiveness on a diverse range of clinical\nsummarization tasks remains unproven. In this study, we apply adaptation\nmethods to eight LLMs, spanning four distinct clinical summarization tasks:\nradiology reports, patient questions, progress notes, and doctor-patient\ndialogue. Quantitative assessments with syntactic, semantic, and conceptual NLP\nmetrics reveal trade-offs between models and adaptation methods. A clinical\nreader study with ten physicians evaluates summary completeness, correctness,\nand conciseness; in a majority of cases, summaries from our best adapted LLMs\nare either equivalent (45%) or superior (36%) compared to summaries from\nmedical experts. The ensuing safety analysis highlights challenges faced by\nboth LLMs and medical experts, as we connect errors to potential medical harm\nand categorize types of fabricated information. Our research provides evidence\nof LLMs outperforming medical experts in clinical text summarization across\nmultiple tasks. This suggests that integrating LLMs into clinical workflows\ncould alleviate documentation burden, allowing clinicians to focus more on\npatient care.\n', '  Large language models (LLMs) have emerged as powerful tools with\ntransformative potential across numerous domains, including healthcare and\nmedicine. In the medical domain, LLMs hold promise for tasks ranging from\nclinical decision support to patient education. However, evaluating the\nperformance of LLMs in medical contexts presents unique challenges due to the\ncomplex and critical nature of medical information. This paper provides a\ncomprehensive overview of the landscape of medical LLM evaluation, synthesizing\ninsights from existing studies and highlighting evaluation data sources, task\nscenarios, and evaluation methods. Additionally, it identifies key challenges\nand opportunities in medical LLM evaluation, emphasizing the need for continued\nresearch and innovation to ensure the responsible integration of LLMs into\nclinical practice.\n']",Natural Language Processing in Healthcare,Natural Language Processing in Clinical Text Analysis
4,Machine Learning and Neural Networks ; Deep Learning Applications and Challenges ; Machine Learning Optimization Techniques,"['quantum', 'qubits', 'qubit', 'qcnns', 'qcnn', 'qnns', 'grover', 'qiskit', 'qnn', 'entanglement'] , ['networks', 'neural', 'neurons', 'regularization', 'approximation', 'gradient', 'relu', 'layers', 'network', 'layer'] , ['pdes', 'pde', 'learning', 'neural', 'solvers', 'pinn', 'nonlinear', 'pinns', 'solver', 'networks'] , ['graphs', 'subgraph', 'networks', 'graph', 'subgraphs', 'nodes', 'node', 'neural', 'gnn', 'gnns'] ; ['cnn', 'cnns', 'segmentation', 'segmentations', 'segmenting', 'supervised', 'mri', 'imaging', 'deep', 'classification'] , ['forecasting', 'forecasts', 'forecast', 'lstm', 'prediction', 'predictive', 'future', 'datasets', 'models', 'seasonal'] , ['adversarial', 'adversarially', 'adversary', 'imagenet', 'robustness', 'trained', 'robust', 'overfitting', 'defenses', 'attacks'] ; ['sgd', 'optimizers', 'optimizer', 'gradient', 'gradients', 'stochastic', 'optimization', 'adaptive', 'hessian', 'minimization'] , ['bandit', 'bandits', 'optimal', 'regret', 'reward', 'optimality', 'exploitation', 'rewards', 'exploration', 'incentive']","['  Our primary objective is to conduct a brief survey of various classical and\nquantum neural net sequence models, which includes self-attention and recurrent\nneural networks, with a focus on recent quantum approaches proposed to work\nwith near-term quantum devices, while exploring some basic enhancements for\nthese quantum models. We re-implement a key representative set of these\nexisting methods, adapting an image classification approach using quantum\nself-attention to create a quantum hybrid transformer that works for text and\nimage classification, and applying quantum self-attention and quantum recurrent\nneural networks to natural language processing tasks. We also explore different\nencoding techniques and introduce positional encoding into quantum\nself-attention neural networks leading to improved accuracy and faster\nconvergence in text and image classification experiments. This paper also\nperforms a comparative analysis of classical self-attention models and their\nquantum counterparts, helping shed light on the differences in these models and\ntheir performance.\n', '  In this work, quantum transformers are designed and analysed in detail by\nextending the state-of-the-art classical transformer neural network\narchitectures known to be very performant in natural language processing and\nimage analysis. Building upon the previous work, which uses parametrised\nquantum circuits for data loading and orthogonal neural layers, we introduce\nthree types of quantum transformers for training and inference, including a\nquantum transformer based on compound matrices, which guarantees a theoretical\nadvantage of the quantum attention mechanism compared to their classical\ncounterpart both in terms of asymptotic run time and the number of model\nparameters. These quantum architectures can be built using shallow quantum\ncircuits and produce qualitatively different classification models. The three\nproposed quantum attention layers vary on the spectrum between closely\nfollowing the classical transformers and exhibiting more quantum\ncharacteristics. As building blocks of the quantum transformer, we propose a\nnovel method for loading a matrix as quantum states as well as two new\ntrainable quantum orthogonal layers adaptable to different levels of\nconnectivity and quality of quantum computers. We performed extensive\nsimulations of the quantum transformers on standard medical image datasets that\nshowed competitively, and at times better performance compared to the classical\nbenchmarks, including the best-in-class classical vision transformers. The\nquantum transformers we trained on these small-scale datasets require fewer\nparameters compared to standard classical benchmarks. Finally, we implemented\nour quantum transformers on superconducting quantum computers and obtained\nencouraging results for up to six qubit experiments.\n', '  In the processing of quantum computation, analyzing and learning the pattern\nof the quantum data are essential for many tasks. Quantum machine learning\nalgorithms can not only deal with the quantum states generated in the preceding\nquantum procedures, but also the quantum registers encoding classical problems.\nIn this work, we experimentally demonstrate the anomaly detection of quantum\nstates encoding audio samples with a three-qubit quantum processor consisting\nof solid-state spins in diamond. By training the quantum machine with a few\nnormal samples, the quantum machine can detect the anomaly samples with a\nminimum error rate of 15.4%. These results show the power of quantum anomaly\ndetection in dealing with machine learning tasks and the potential to detect\nabnormal output of quantum devices.\n'] , ['  We prove a large deviation principle for deep neural networks with Gaussian\nweights and (at most linearly growing) activation functions. This generalises\nearlier work, in which bounded and continuous activation functions were\nconsidered. In practice, linearly growing activation functions such as ReLU are\nmost commonly used. We furthermore simplify previous expressions for the rate\nfunction and a give power-series expansions for the ReLU case.\n', '  We study the approximation capacity of some variation spaces corresponding to\nshallow ReLU$^k$ neural networks. It is shown that sufficiently smooth\nfunctions are contained in these spaces with finite variation norms. For\nfunctions with less smoothness, the approximation rates in terms of the\nvariation norm are established. Using these results, we are able to prove the\noptimal approximation rates in terms of the number of neurons for shallow\nReLU$^k$ neural networks. It is also shown how these results can be used to\nderive approximation bounds for deep neural networks and convolutional neural\nnetworks (CNNs). As applications, we study convergence rates for nonparametric\nregression using three ReLU neural network models: shallow neural network,\nover-parameterized neural network, and CNN. In particular, we show that shallow\nneural networks can achieve the minimax optimal rates for learning H\\""older\nfunctions, which complements recent results for deep neural networks. It is\nalso proven that over-parameterized (deep or shallow) neural networks can\nachieve nearly optimal rates for nonparametric regression.\n', '  We investigate the expressivity and learning dynamics of bias-free ReLU\nnetworks. We firstly show that two-layer bias-free ReLU networks have limited\nexpressivity: the only odd function two-layer bias-free ReLU networks can\nexpress is a linear one. We then show that, under symmetry conditions on the\ndata, these networks have the same learning dynamics as linear networks. This\nallows us to give closed-form time-course solutions to certain two-layer\nbias-free ReLU networks, which has not been done for nonlinear networks outside\nthe lazy learning regime. While deep bias-free ReLU networks are more\nexpressive than their two-layer counterparts, they still share a number of\nsimilarities with deep linear networks. These similarities enable us to\nleverage insights from linear networks, leading to a novel understanding of\nbias-free ReLU networks. Overall, our results show that some properties\nestablished for bias-free ReLU networks arise due to equivalence to linear\nnetworks, and suggest that including bias or considering asymmetric data are\navenues to engage with nonlinear behaviors.\n'] , ['  Learning and solving governing equations of a physical system, represented by\npartial differential equations (PDEs), from data is a central challenge in a\nvariety of areas of science and engineering. Traditional numerical methods for\nsolving PDEs can be computationally expensive for complex systems and require\nthe complete PDEs of the physical system. On the other hand, current\ndata-driven machine learning methods require a large amount of data to learn a\nsurrogate model of the PDE solution operator, which could be impractical. Here,\nwe propose the first solution operator learning method that only requires one\nPDE solution, i.e., one-shot learning. By leveraging the principle of locality\nof PDEs, we consider small local domains instead of the entire computational\ndomain and define a local solution operator. The local solution operator is\nthen trained using a neural network, and utilized to predict the solution of a\nnew input function via mesh-based fixed-point iteration (FPI), meshfree\nlocal-solution-operator informed neural network (LOINN) or\nlocal-solution-operator informed neural network with correction (cLOINN). We\ntest our method on diverse PDEs, including linear or nonlinear PDEs, PDEs\ndefined on complex geometries, and PDE systems, demonstrating the effectiveness\nand generalization capabilities of our method across these varied scenarios.\n', '  Physics-informed neural networks (PINNs) have attracted significant attention\nfor solving partial differential equations (PDEs) in recent years because they\nalleviate the curse of dimensionality that appears in traditional methods.\nHowever, the most disadvantage of PINNs is that one neural network corresponds\nto one PDE. In practice, we usually need to solve a class of PDEs, not just\none. With the explosive growth of deep learning, many useful techniques in\ngeneral deep learning tasks are also suitable for PINNs. Transfer learning\nmethods may reduce the cost for PINNs in solving a class of PDEs. In this\npaper, we proposed a transfer learning method of PINNs via keeping singular\nvectors and optimizing singular values (namely SVD-PINNs). Numerical\nexperiments on high dimensional PDEs (10-d linear parabolic equations and 10-d\nAllen-Cahn equations) show that SVD-PINNs work for solving a class of PDEs with\ndifferent but close right-hand-side functions.\n', '  Physics-informed neural networks (PINNs) have recently emerged as a promising\nway to compute the solutions of partial differential equations (PDEs) using\ndeep neural networks. However, despite their significant success in various\nfields, it remains unclear in many aspects how to effectively train PINNs if\nthe solutions of PDEs exhibit stiff behaviors or high frequencies. In this\npaper, we propose a new method for training PINNs using variable-scaling\ntechniques. This method is simple and it can be applied to a wide range of\nproblems including PDEs with rapidly-varying solutions. Throughout various\nnumerical experiments, we will demonstrate the effectiveness of the proposed\nmethod for these problems and confirm that it can significantly improve the\ntraining efficiency and performance of PINNs. Furthermore, based on the\nanalysis of the neural tangent kernel (NTK), we will provide theoretical\nevidence for this phenomenon and show that our methods can indeed improve the\nperformance of PINNs.\n'] , ['  Graph neural networks (GNNs) have become the \\textit{de facto} standard for\nrepresentational learning in graphs, and have achieved state-of-the-art\nperformance in many graph-related tasks; however, it has been shown that the\nexpressive power of standard GNNs are equivalent maximally to 1-dimensional\nWeisfeiler-Lehman (1-WL) Test. Recently, there is a line of works aiming to\nenhance the expressive power of graph neural networks. One line of such works\naim at developing $K$-hop message-passing GNNs where node representation is\nupdated by aggregating information from not only direct neighbors but all\nneighbors within $K$-hop of the node. Another line of works leverages subgraph\ninformation to enhance the expressive power which is proven to be strictly more\npowerful than 1-WL test. In this work, we discuss the limitation of $K$-hop\nmessage-passing GNNs and propose \\textit{substructure encoding function} to\nuplift the expressive power of any $K$-hop message-passing GNN. We further\ninject contextualized substructure information to enhance the expressiveness of\n$K$-hop message-passing GNNs. Our method is provably more powerful than\nprevious works on $K$-hop graph neural networks and 1-WL subgraph GNNs, which\nis a specific type of subgraph based GNN models, and not less powerful than\n3-WL. Empirically, our proposed method set new state-of-the-art performance or\nachieves comparable performance for a variety of datasets. Our code is\navailable at \\url{https://github.com/tianyao-aka/Expresive_K_hop_GNNs}.\n', ""  Graph Neural Network (GNN), with the main idea of encoding graph structure\ninformation of graphs by propagation and aggregation, has developed rapidly. It\nachieved excellent performance in representation learning of multiple types of\ngraphs such as homogeneous graphs, heterogeneous graphs, and more complex\ngraphs like knowledge graphs. However, merely stacking GNN layers may not\nimprove the model's performance and can even be detrimental. For the phenomenon\nof performance degradation in deep GNNs, we propose a new perspective. Unlike\nthe popular explanations of over-smoothing or over-squashing, we think the\nissue arises from the interference of low-quality node representations during\nmessage propagation. We introduce a simple and general method, SF-GNN, to\naddress this problem. In SF-GNN, we define two representations for each node,\none is the node representation that represents the feature of the node itself,\nand the other is the message representation specifically for propagating\nmessages to neighbor nodes. A self-filter module evaluates the quality of the\nnode representation and decides whether to integrate it into the message\npropagation based on this quality assessment. Experiments on node\nclassification tasks for both homogeneous and heterogeneous graphs, as well as\nlink prediction tasks on knowledge graphs, demonstrate that our method can be\napplied to various GNN models and outperforms state-of-the-art baseline methods\nin addressing deep GNN degradation.\n"", '  Graph neural networks (GNNs) have achieved state-of-the-art performance in\ngraph representation learning. Message passing neural networks, which learn\nrepresentations through recursively aggregating information from each node and\nits neighbors, are among the most commonly-used GNNs. However, a wealth of\nstructural information of individual nodes and full graphs is often ignored in\nsuch process, which restricts the expressive power of GNNs. Various graph data\naugmentation methods that enable the message passing with richer structure\nknowledge have been introduced as one main way to tackle this issue, but they\nare often focused on individual structure features and difficult to scale up\nwith more structure features. In this work we propose a novel approach, namely\ncollective structure knowledge-augmented graph neural network (CoS-GNN), in\nwhich a new message passing method is introduced to allow GNNs to harness a\ndiverse set of node- and graph-level structure features, together with original\nnode features/attributes, in augmented graphs. In doing so, our approach\nlargely improves the structural knowledge modeling of GNNs in both node and\ngraph levels, resulting in substantially improved graph representations. This\nis justified by extensive empirical results where CoS-GNN outperforms\nstate-of-the-art models in various graph-level learning tasks, including graph\nclassification, anomaly detection, and out-of-distribution generalization.\n'] ; ['  Brain tumors remain a critical global health challenge, necessitating\nadvancements in diagnostic techniques and treatment methodologies. A tumor or\nits recurrence often needs to be identified in imaging studies and\ndifferentiated from normal brain tissue. In response to the growing need for\nage-specific segmentation models, particularly for pediatric patients, this\nstudy explores the deployment of deep learning techniques using magnetic\nresonance imaging (MRI) modalities. By introducing a novel ensemble approach\nusing ONet and modified versions of UNet, coupled with innovative loss\nfunctions, this study achieves a precise segmentation model for the BraTS-PEDs\n2023 Challenge. Data augmentation, including both single and composite\ntransformations, ensures model robustness and accuracy across different\nscanning protocols. The ensemble strategy, integrating the ONet and UNet\nmodels, shows greater effectiveness in capturing specific features and modeling\ndiverse aspects of the MRI images which result in lesion wise Dice scores of\n0.52, 0.72 and 0.78 on unseen validation data and scores of 0.55, 0.70, 0.79 on\nfinal testing data for the ""enhancing tumor"", ""tumor core"" and ""whole tumor""\nlabels respectively. Visual comparisons further confirm the superiority of the\nensemble method in accurate tumor region coverage. The results indicate that\nthis advanced ensemble approach, building upon the unique strengths of\nindividual models, offers promising prospects for enhanced diagnostic accuracy\nand effective treatment planning and monitoring for brain tumors in pediatric\nbrains.\n', '  Brain tumor is a life-threatening problem and hampers the normal functioning\nof the human body. The average five-year relative survival rate for malignant\nbrain tumors is 35.6 percent. For proper diagnosis and efficient treatment\nplanning, it is necessary to detect the brain tumor in early stages. Due to\nadvancement in medical imaging technology, the brain images are taken in\ndifferent modalities. The ability to extract relevant characteristics from\nmagnetic resonance imaging (MRI) scans is a crucial step for brain tumor\nclassifiers. Several studies have proposed various strategies to extract\nrelevant features from different modalities of MRI to predict the growth of\nabnormal tumors. Most techniques used conventional methods of image processing\nfor feature extraction and machine learning for classification. More recently,\nthe use of deep learning algorithms in medical imaging has resulted in\nsignificant improvements in the classification and diagnosis of brain tumors.\nSince tumors are located at different regions of the brain, localizing the\ntumor and classifying it to a particular category is a challenging task. The\nobjective of this project is to develop a predictive system for brain tumor\ndetection using machine learning(ensembling).\n', '  Medical image segmentation of anatomical structures and pathology is crucial\nin modern clinical diagnosis, disease study, and treatment planning. To date,\ngreat progress has been made in deep learning-based segmentation techniques,\nbut most methods still lack data efficiency, generalizability, and\ninteractability. Consequently, the development of new, precise segmentation\nmethods that demand fewer labeled datasets is of utmost importance in medical\nimage analysis. Recently, the emergence of foundation models, such as CLIP and\nSegment-Anything-Model (SAM), with comprehensive cross-domain representation\nopened the door for interactive and universal image segmentation. However,\nexploration of these models for data-efficient medical image segmentation is\nstill limited, but is highly necessary. In this paper, we propose a novel\nframework, called MedCLIP-SAM that combines CLIP and SAM models to generate\nsegmentation of clinical scans using text prompts in both zero-shot and weakly\nsupervised settings. To achieve this, we employed a new Decoupled Hard Negative\nNoise Contrastive Estimation (DHN-NCE) loss to fine-tune the BiomedCLIP model\nand the recent gScoreCAM to generate prompts to obtain segmentation masks from\nSAM in a zero-shot setting. Additionally, we explored the use of zero-shot\nsegmentation labels in a weakly supervised paradigm to improve the segmentation\nquality further. By extensively testing three diverse segmentation tasks and\nmedical image modalities (breast tumor ultrasound, brain tumor MRI, and lung\nX-ray), our proposed framework has demonstrated excellent accuracy. Code is\navailable at https://github.com/HealthX-Lab/MedCLIP-SAM.\n'] , ['  Time series forecasting is an important task in many fields ranging from\nsupply chain management to weather forecasting. Recently, Transformer neural\nnetwork architectures have shown promising results in forecasting on common\ntime series benchmark datasets. However, application to supply chain demand\nforecasting, which can have challenging characteristics such as sparsity and\ncross-series effects, has been limited.\n  In this work, we explore the application of Transformer-based models to\nsupply chain demand forecasting. In particular, we develop a new\nTransformer-based forecasting approach using a shared, multi-task per-time\nseries network with an initial component applying attention across time series,\nto capture interactions and help address sparsity. We provide a case study\napplying our approach to successfully improve demand prediction for a medical\ndevice manufacturing company. To further validate our approach, we also apply\nit to public demand forecasting datasets as well and demonstrate competitive to\nsuperior performance compared to a variety of baseline and state-of-the-art\nforecast methods across the private and public datasets.\n', '  The rapid development of time series forecasting research has brought many\ndeep learning-based modules in this field. However, despite the increasing\namount of new forecasting architectures, it is still unclear if we have\nleveraged the full potential of these existing modules within a properly\ndesigned architecture. In this work, we propose a novel hierarchical neural\narchitecture search approach for time series forecasting tasks. With the design\nof a hierarchical search space, we incorporate many architecture types designed\nfor forecasting tasks and allow for the efficient combination of different\nforecasting architecture modules. Results on long-term-time-series-forecasting\ntasks show that our approach can search for lightweight high-performing\nforecasting architectures across different forecasting tasks.\n', '  Large language models (LLMs) are being applied to time series tasks,\nparticularly time series forecasting. However, are language models actually\nuseful for time series? After a series of ablation studies on three recent and\npopular LLM-based time series forecasting methods, we find that removing the\nLLM component or replacing it with a basic attention layer does not degrade the\nforecasting results -- in most cases the results even improved. We also find\nthat despite their significant computational cost, pretrained LLMs do no better\nthan models trained from scratch, do not represent the sequential dependencies\nin time series, and do not assist in few-shot settings. Additionally, we\nexplore time series encoders and reveal that patching and attention structures\nperform similarly to state-of-the-art LLM-based forecasters.\n'] , ['  Deep neural networks have been successfully applied in various machine\nlearning tasks. However, studies show that neural networks are susceptible to\nadversarial attacks. This exposes a potential threat to neural network-based\nintelligent systems. We observe that the probability of the correct result\noutputted by the neural network increases by applying small first-order\nperturbations generated for non-predicted class labels to adversarial examples.\nBased on this observation, we propose a method for counteracting adversarial\nperturbations to improve adversarial robustness. In the proposed method, we\nrandomly select a number of class labels and generate small first-order\nperturbations for these selected labels. The generated perturbations are added\ntogether and then clamped onto a specified space. The obtained perturbation is\nfinally added to the adversarial example to counteract the adversarial\nperturbation contained in the example. The proposed method is applied at\ninference time and does not require retraining or finetuning the model. We\nexperimentally validate the proposed method on CIFAR-10 and CIFAR-100. The\nresults demonstrate that our method effectively improves the defense\nperformance of several transformation-based defense methods, especially against\nstrong adversarial examples generated using more iterations.\n', ""  Deep neural networks (DNNs) are easily fooled by adversarial perturbations\nthat are imperceptible to humans. Adversarial training, a process where\nadversarial examples are added to the training set, is the current\nstate-of-the-art defense against adversarial attacks, but it lowers the model's\naccuracy on clean inputs, is computationally expensive, and offers less\nrobustness to natural noise. In contrast, energy-based models (EBMs), which\nwere designed for efficient implementation in neuromorphic hardware and\nphysical systems, incorporate feedback connections from each layer to the\nprevious layer, yielding a recurrent, deep-attractor architecture which we\nhypothesize should make them naturally robust. Our work is the first to explore\nthe robustness of EBMs to both natural corruptions and adversarial attacks,\nwhich we do using the CIFAR-10 and CIFAR-100 datasets. We demonstrate that EBMs\nare more robust than transformers and display comparable robustness to\nadversarially-trained DNNs on gradient-based (white-box) attacks, query-based\n(black-box) attacks, and natural perturbations without sacrificing clean\naccuracy, and without the need for adversarial training or additional training\ntechniques.\n"", '  As deep learning (DL) models are increasingly being integrated into our\neveryday lives, ensuring their safety by making them robust against adversarial\nattacks has become increasingly critical. DL models have been found to be\nsusceptible to adversarial attacks which can be achieved by introducing small,\ntargeted perturbations to disrupt the input data. Adversarial training has been\npresented as a mitigation strategy which can result in more robust models. This\nadversarial robustness comes with additional computational costs required to\ndesign adversarial attacks during training. The two objectives -- adversarial\nrobustness and computational efficiency -- then appear to be in conflict of\neach other. In this work, we explore the effects of two different model\ncompression methods -- structured weight pruning and quantization -- on\nadversarial robustness. We specifically explore the effects of fine-tuning on\ncompressed models, and present the trade-off between standard fine-tuning and\nadversarial fine-tuning. Our results show that compression does not inherently\nlead to loss in model robustness and adversarial fine-tuning of a compressed\nmodel can yield large improvement to the robustness performance of models. We\npresent experiments on two benchmark datasets showing that adversarial\nfine-tuning of compressed models can achieve robustness performance comparable\nto adversarially trained models, while also improving computational efficiency.\n'] ; ['  While stochastic gradient descent (SGD) can use various learning rates, such\nas constant or diminishing rates, the previous numerical results showed that\nSGD performs better than other deep learning optimizers using when it uses\nlearning rates given by line search methods. In this paper, we perform a\nconvergence analysis on SGD with a learning rate given by an Armijo line search\nfor nonconvex optimization indicating that the upper bound of the expectation\nof the squared norm of the full gradient becomes small when the number of steps\nand the batch size are large. Next, we show that, for SGD with the\nArmijo-line-search learning rate, the number of steps needed for nonconvex\noptimization is a monotone decreasing convex function of the batch size; that\nis, the number of steps needed for nonconvex optimization decreases as the\nbatch size increases. Furthermore, we show that the stochastic first-order\noracle (SFO) complexity, which is the stochastic gradient computation cost, is\na convex function of the batch size; that is, there exists a critical batch\nsize that minimizes the SFO complexity. Finally, we provide numerical results\nthat support our theoretical results. The numerical results indicate that the\nnumber of steps needed for training deep neural networks decreases as the batch\nsize increases and that there exist the critical batch sizes that can be\nestimated from the theoretical results.\n', '  It is known that the standard stochastic gradient descent (SGD) optimization\nmethod, as well as accelerated and adaptive SGD optimization methods such as\nthe Adam optimizer fail to converge if the learning rates do not converge to\nzero (as, for example, in the situation of constant learning rates). Numerical\nsimulations often use human-tuned deterministic learning rate schedules or\nsmall constant learning rates. The default learning rate schedules for SGD\noptimization methods in machine learning implementation frameworks such as\nTensorFlow and Pytorch are constant learning rates. In this work we propose and\nstudy a learning-rate-adaptive approach for SGD optimization methods in which\nthe learning rate is adjusted based on empirical estimates for the values of\nthe objective function of the considered optimization problem (the function\nthat one intends to minimize). In particular, we propose a\nlearning-rate-adaptive variant of the Adam optimizer and implement it in case\nof several neural network learning problems, particularly, in the context of\ndeep learning approximation methods for partial differential equations such as\ndeep Kolmogorov methods, physics-informed neural networks, and deep Ritz\nmethods. In each of the presented learning problems the proposed\nlearning-rate-adaptive variant of the Adam optimizer faster reduces the value\nof the objective function than the Adam optimizer with the default learning\nrate. For a simple class of quadratic minimization problems we also rigorously\nprove that a learning-rate-adaptive variant of the SGD optimization method\nconverges to the minimizer of the considered minimization problem. Our\nconvergence proof is based on an analysis of the laws of invariant measures of\nthe SGD method as well as on a more general convergence analysis for SGD with\nrandom but predictable learning rates which we develop in this work.\n', '  For nonconvex objective functions, including deep neural networks, stochastic\ngradient descent (SGD) with momentum has fast convergence and excellent\ngeneralizability, but a theoretical explanation for this is lacking. In\ncontrast to previous studies that defined the stochastic noise that occurs\nduring optimization as the variance of the stochastic gradient, we define it as\nthe gap between the search direction of the optimizer and the steepest descent\ndirection and show that its level dominates generalizability of the model. We\nalso show that the stochastic noise in SGD with momentum smoothes the objective\nfunction, the degree of which is determined by the learning rate, the batch\nsize, the momentum factor, the variance of the stochastic gradient, and the\nupper bound of the gradient norm. By numerically deriving the stochastic noise\nlevel in SGD and SGD with momentum, we provide theoretical findings that help\nexplain the training dynamics of SGD with momentum, which were not explained by\nprevious studies on convergence and stability. We also provide experimental\nresults supporting our assertion that model generalizability depends on the\nstochastic noise level.\n'] , ['  Fast changing states or volatile environments pose a significant challenge to\nonline optimization, which needs to perform rapid adaptation under limited\nobservation. In this paper, we give query and regret optimal bandit algorithms\nunder the strict notion of strongly adaptive regret, which measures the maximum\nregret over any contiguous interval $I$. Due to its worst-case nature, there is\nan almost-linear $\\Omega(|I|^{1-\\epsilon})$ regret lower bound, when only one\nquery per round is allowed [Daniely el al, ICML 2015]. Surprisingly, with just\ntwo queries per round, we give Strongly Adaptive Bandit Learner (StABL) that\nachieves $\\tilde{O}(\\sqrt{n|I|})$ adaptive regret for multi-armed bandits with\n$n$ arms. The bound is tight and cannot be improved in general. Our algorithm\nleverages a multiplicative update scheme of varying stepsizes and a carefully\nchosen observation distribution to control the variance. Furthermore, we extend\nour results and provide optimal algorithms in the bandit convex optimization\nsetting. Finally, we empirically demonstrate the superior performance of our\nalgorithms under volatile environments and for downstream tasks, such as\nalgorithm selection for hyperparameter optimization.\n', ""  Contextual dueling bandit is used to model the bandit problems, where a\nlearner's goal is to find the best arm for a given context using observed noisy\npreference feedback over the selected arms for the past contexts. However,\nexisting algorithms assume the reward function is linear, which can be complex\nand non-linear in many real-life applications like online recommendations or\nranking web search results. To overcome this challenge, we use a neural network\nto estimate the reward function using preference feedback for the previously\nselected arms. We propose upper confidence bound- and Thompson sampling-based\nalgorithms with sub-linear regret guarantees that efficiently select arms in\neach round. We then extend our theoretical results to contextual bandit\nproblems with binary feedback, which is in itself a non-trivial contribution.\nExperimental results on the problem instances derived from synthetic datasets\ncorroborate our theoretical results.\n"", '  We present substantial evidence demonstrating the benefits of integrating\nLarge Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.\nContextual bandits have been widely used in recommendation systems to generate\npersonalized suggestions based on user-specific contexts. We show that LLMs,\npre-trained on extensive corpora rich in human knowledge and preferences, can\nsimulate human behaviours well enough to jump-start contextual multi-armed\nbandits to reduce online learning regret. We propose an initialization\nalgorithm for contextual bandits by prompting LLMs to produce a pre-training\ndataset of approximate human preferences for the bandit. This significantly\nreduces online learning regret and data-gathering costs for training such\nmodels. Our approach is validated empirically through two sets of experiments\nwith different bandit setups: one which utilizes LLMs to serve as an oracle and\na real-world experiment utilizing data from a conjoint survey experiment.\n']",Machine Learning and Artificial Intelligence,Machine Learning and Neural Networks
5,Robot Manipulation and Learning ; Video and Image Editing with Generative Models,"['robotic', 'grasping', 'robot', 'robotics', 'robots', 'grasp', 'teleoperation', 'scenes', 'actions', 'pose'] ; ['editing', 'attention', 'images', 'videos', 'generative', 'visual', 'scene', 'image', 'inpainting', 'stylization']","['  We present a new reproducible benchmark for evaluating robot manipulation in\nthe real world, specifically focusing on pick-and-place. Our benchmark uses the\nYCB objects, a commonly used dataset in the robotics community, to ensure that\nour results are comparable to other studies. Additionally, the benchmark is\ndesigned to be easily reproducible in the real world, making it accessible to\nresearchers and practitioners. We also provide our experimental results and\nanalyzes for model-based and model-free 6D robotic grasping on the benchmark,\nwhere representative algorithms are evaluated for object perception, grasping\nplanning, and motion planning. We believe that our benchmark will be a valuable\ntool for advancing the field of robot manipulation. By providing a standardized\nevaluation framework, researchers can more easily compare different techniques\nand algorithms, leading to faster progress in developing robot manipulation\nmethods.\n', '  Robot learning of manipulation skills is hindered by the scarcity of diverse,\nunbiased datasets. While curated datasets can help, challenges remain in\ngeneralizability and real-world transfer. Meanwhile, large-scale ""in-the-wild""\nvideo datasets have driven progress in computer vision through self-supervised\ntechniques. Translating this to robotics, recent works have explored learning\nmanipulation skills by passively watching abundant videos sourced online.\nShowing promising results, such video-based learning paradigms provide scalable\nsupervision while reducing dataset bias. This survey reviews foundations such\nas video feature representation learning techniques, object affordance\nunderstanding, 3D hand/body modeling, and large-scale robot resources, as well\nas emerging techniques for acquiring robot manipulation skills from\nuncontrolled video demonstrations. We discuss how learning only from observing\nlarge-scale human videos can enhance generalization and sample efficiency for\nrobotic manipulation. The survey summarizes video-based learning approaches,\nanalyses their benefits over standard datasets, survey metrics, and benchmarks,\nand discusses open challenges and future directions in this nascent domain at\nthe intersection of computer vision, natural language processing, and robot\nlearning.\n', ""  In collaborative human-robot manipulation, a robot must predict human intents\nand adapt its actions accordingly to smoothly execute tasks. However, the\nhuman's intent in turn depends on actions the robot takes, creating a\nchicken-or-egg problem. Prior methods ignore such inter-dependency and instead\ntrain marginal intent prediction models independent of robot actions. This is\nbecause training conditional models is hard given a lack of paired human-robot\ninteraction datasets. Can we instead leverage large-scale human-human\ninteraction data that is more easily accessible? Our key insight is to exploit\na correspondence between human and robot actions that enables transfer learning\nfrom human-human to human-robot data. We propose a novel architecture,\nInteRACT, that pre-trains a conditional intent prediction model on large\nhuman-human datasets and fine-tunes on a small human-robot dataset. We evaluate\non a set of real-world collaborative human-robot manipulation tasks and show\nthat our conditional model improves over various marginal baselines. We also\nintroduce new techniques to tele-operate a 7-DoF robot arm and collect a\ndiverse range of human-robot collaborative manipulation data, which we\nopen-source.\n""] ; ['  With recent advances in image and video diffusion models for content\ncreation, a plethora of techniques have been proposed for customizing their\ngenerated content. In particular, manipulating the cross-attention layers of\nText-to-Image (T2I) diffusion models has shown great promise in controlling the\nshape and location of objects in the scene. Transferring image-editing\ntechniques to the video domain, however, is extremely challenging as object\nmotion and temporal consistency are difficult to capture accurately. In this\nwork, we take a first look at the role of cross-attention in Text-to-Video\n(T2V) diffusion models for zero-shot video editing. While one-shot models have\nshown potential in controlling motion and camera movement, we demonstrate\nzero-shot control over object shape, position and movement in T2V models. We\nshow that despite the limitations of current T2V models, cross-attention\nguidance can be a promising approach for editing videos.\n', '  While text-to-image models have achieved impressive capabilities in image\ngeneration and editing, their application across various modalities often\nnecessitates training separate models. Inspired by existing method of single\nimage editing with self attention injection and video editing with shared\nattention, we propose a novel unified editing framework that combines the\nstrengths of both approaches by utilizing only a basic 2D image text-to-image\n(T2I) diffusion model. Specifically, we design a sampling method that\nfacilitates editing consecutive images while maintaining semantic consistency\nutilizing shared self-attention features during both reference and consecutive\nimage sampling processes. Experimental results confirm that our method enables\nediting across diverse modalities including 3D scenes, videos, and panorama\nimages.\n', '  Diffusion models have made tremendous progress in text-driven image and video\ngeneration. Now text-to-image foundation models are widely applied to various\ndownstream image synthesis tasks, such as controllable image generation and\nimage editing, while downstream video synthesis tasks are less explored for\nseveral reasons. First, it requires huge memory and computation overhead to\ntrain a video generation foundation model. Even with video foundation models,\nadditional costly training is still required for downstream video synthesis\ntasks. Second, although some works extend image diffusion models into videos in\na training-free manner, temporal consistency cannot be well preserved. Finally,\nthese adaption methods are specifically designed for one task and fail to\ngeneralize to different tasks. To mitigate these issues, we propose a\ntraining-free general-purpose video synthesis framework, coined as {\\bf\nBIVDiff}, via bridging specific image diffusion models and general\ntext-to-video foundation diffusion models. Specifically, we first use a\nspecific image diffusion model (e.g., ControlNet and Instruct Pix2Pix) for\nframe-wise video generation, then perform Mixed Inversion on the generated\nvideo, and finally input the inverted latents into the video diffusion models\n(e.g., VidRD and ZeroScope) for temporal smoothing. This decoupled framework\nenables flexible image model selection for different purposes with strong task\ngeneralization and high efficiency. To validate the effectiveness and general\nuse of BIVDiff, we perform a wide range of video synthesis tasks, including\ncontrollable video generation, video editing, video inpainting, and\noutpainting.\n']",Artificial Intelligence for Visual and Robotic Applications,Robot Manipulation and Learning
6,Causal Analysis and Modeling,"['causal', 'causality', 'causally', 'inference', 'confounders', 'observational', 'discovery', 'identifiability', 'identifiable', 'unobserved']","['  Dynamic structural causal models (SCMs) are a powerful framework for\nreasoning in dynamic systems about direct effects which measure how a change in\none variable affects another variable while holding all other variables\nconstant. The causal relations in a dynamic structural causal model can be\nqualitatively represented with an acyclic full-time causal graph. Assuming\nlinearity and no hidden confounding and given the full-time causal graph, the\ndirect causal effect is always identifiable. However, in many application such\na graph is not available for various reasons but nevertheless experts have\naccess to the summary causal graph of the full-time causal graph which\nrepresents causal relations between time series while omitting temporal\ninformation and allowing cycles. This paper presents a complete identifiability\nresult which characterizes all cases for which the direct effect is graphically\nidentifiable from a summary causal graph and gives two sound finite adjustment\nsets that can be used to estimate the direct effect whenever it is\nidentifiable.\n', '  The ability to understand causality from data is one of the major milestones\nof human-level intelligence. Causal Discovery (CD) algorithms can identify the\ncause-effect relationships among the variables of a system from related\nobservational data with certain assumptions. Over the years, several methods\nhave been developed primarily based on the statistical properties of data to\nuncover the underlying causal mechanism. In this study, we present an extensive\ndiscussion on the methods designed to perform causal discovery from both\nindependent and identically distributed (I.I.D.) data and time series data. For\nthis purpose, we first introduce the common terminologies used in causal\ndiscovery literature and then provide a comprehensive discussion of the\nalgorithms designed to identify causal relations in different settings. We\nfurther discuss some of the benchmark datasets available for evaluating the\nalgorithmic performance, off-the-shelf tools or software packages to perform\ncausal discovery readily, and the common metrics used to evaluate these\nmethods. We also evaluate some widely used causal discovery algorithms on\nmultiple benchmark datasets and compare their performances. Finally, we\nconclude by discussing the research challenges and the applications of causal\ndiscovery algorithms in multiple areas of interest.\n', ""  Causal discovery from observational data holds great promise, but existing\nmethods rely on strong assumptions about the underlying causal structure, often\nrequiring full observability of all relevant variables. We tackle these\nchallenges by leveraging the score function $\\nabla \\log p(X)$ of observed\nvariables for causal discovery and propose the following contributions. First,\nwe generalize the existing results of identifiability with the score to\nadditive noise models with minimal requirements on the causal mechanisms.\nSecond, we establish conditions for inferring causal relations from the score\neven in the presence of hidden variables; this result is two-faced: we\ndemonstrate the score's potential as an alternative to conditional independence\ntests to infer the equivalence class of causal graphs with hidden variables,\nand we provide the necessary conditions for identifying direct causes in latent\nvariable models. Building on these insights, we propose a flexible algorithm\nfor causal discovery across linear, nonlinear, and latent variable models,\nwhich we empirically validate.\n""]",Causal Analysis and Modeling,Causal Analysis and Modeling
7,Autonomous Vehicle Safety on Highways,"['highway', 'driving', 'autonomous', 'traffic', 'vehicles', 'lane', 'road', 'vehicle', 'planning', 'cars']","[""  In recent years, the expansion of internet technology and advancements in\nautomation have brought significant attention to autonomous driving technology.\nMajor automobile manufacturers, including Volvo, Mercedes-Benz, and Tesla, have\nprogressively introduced products ranging from assisted-driving vehicles to\nsemi-autonomous vehicles. However, this period has also witnessed several\ntraffic safety incidents involving self-driving vehicles. For instance, in\nMarch 2016, a Google self-driving car was involved in a minor collision with a\nbus. At the time of the accident, the autonomous vehicle was attempting to\nmerge into the right lane but failed to dynamically respond to the real-time\nenvironmental information during the lane change. It incorrectly assumed that\nthe approaching bus would slow down to avoid it, leading to a low-speed\ncollision with the bus. This incident highlights the current technological\nshortcomings and safety concerns associated with autonomous lane-changing\nbehavior, despite the rapid advancements in autonomous driving technology.\nLane-changing is among the most common and hazardous behaviors in highway\ndriving, significantly impacting traffic safety and flow. Therefore,\nlane-changing is crucial for traffic safety, and accurately predicting drivers'\nlane change intentions can markedly enhance driving safety. This paper\nintroduces a deep learning-based prediction method for autonomous driving lane\nchange behavior, aiming to facilitate safe lane changes and thereby improve\nroad safety.\n"", '  Autonomous driving technology can improve traffic safety and reduce traffic\naccidents. In addition, it improves traffic flow, reduces congestion, saves\nenergy and increases travel efficiency. In the relatively mature automatic\ndriving technology, the automatic driving function is divided into several\nmodules: perception, decision-making, planning and control, and a reasonable\ndivision of labor can improve the stability of the system. Therefore,\nautonomous vehicles need to have the ability to predict the trajectory of\nsurrounding vehicles in order to make reasonable decision planning and safety\nmeasures to improve driving safety. By using deep learning method, a\nsafety-sensitive deep learning model based on short term memory (LSTM) network\nis proposed. This model can alleviate the shortcomings of current automatic\ndriving trajectory planning, and the output trajectory not only ensures high\naccuracy but also improves safety. The cell state simulation algorithm\nsimulates the trackability of the trajectory generated by this model. The\nresearch results show that compared with the traditional model-based method,\nthe trajectory prediction method based on LSTM network has obvious advantages\nin predicting the trajectory in the long time domain. The intention recognition\nmodule considering interactive information has higher prediction and accuracy,\nand the algorithm results show that the trajectory is very smooth based on the\npremise of safe prediction and efficient lane change. And autonomous vehicles\ncan efficiently and safely complete lane changes.\n', '  Autonomous vehicles (AVs) have the potential to prevent accidents caused by\ndrivers errors and reduce road traffic risks. Due to the nature of heavy\nvehicles, whose collisions cause more serious crashes, the weights of vehicles\nneed to be considered when making driving strategies aimed at reducing the\npotential risks and their consequences in the context of autonomous driving.\nThis study develops an autonomous driving strategy based on risk anticipation,\nconsidering the weights of surrounding vehicles and using hierarchical deep\nreinforcement learning. A risk indicator integrating surrounding vehicles\nweights, based on the risk field theory, is proposed and incorporated into\nautonomous driving decisions. A hybrid action space is designed to allow for\nleft lane changes, right lane changes and car-following, which enables AVs to\nact more freely and realistically whenever possible. To solve the above hybrid\ndecision-making problem, a hierarchical proximal policy optimization (HPPO)\nalgorithm with an attention mechanism (AT-HPPO) is developed, providing great\nadvantages in maintaining stable performance with high robustness and\ngeneralization. An indicator, potential collision energy in conflicts (PCEC),\nis newly proposed to evaluate the performance of the developed AV driving\nstrategy from the perspective of the consequences of potential accidents. The\nperformance evaluation results in simulation and dataset demonstrate that our\nmodel provides driving strategies that reduce both the likelihood and\nconsequences of potential accidents, at the same time maintaining driving\nefficiency. The developed method is especially meaningful for AVs driving on\nhighways, where heavy vehicles make up a high proportion of the traffic.\n']",Autonomous Vehicle Safety,Autonomous Vehicle Safety on Highways
